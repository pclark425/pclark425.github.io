<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2685 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2685</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2685</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-713b604fb9cdd6631074cbd6bf36db029031992e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/713b604fb9cdd6631074cbd6bf36db029031992e" target="_blank">Large Language Models are Zero Shot Hypothesis Proposers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The findings of this paper strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
                <p><strong>Paper Abstract:</strong> Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2685.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2685.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained transformer-based language models (e.g., GPT-3.5, GPT-4, Llama series, WizardLM) used in this work to generate scientific hypotheses, to be evaluated in zero-shot, few-shot, and fine-tuned settings across biomedical background+hypothesis pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large Language Models (GPT-3.5, GPT-4, Llama variants, WizardLM, Vicuna, Openchat, MedAlpaca, PMC-LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pretrained transformer LLMs were prompted (zero-shot and few-shot) or fine-tuned on background->hypothesis pairs to produce natural-language hypothesis proposals. The paper frames hypothesis generation as a text-completion task optimizing P(y_t | y_<t, I, X). Models evaluated include API-based (gpt-3.5-turbo, gpt-4 references), general instructed LLMs (Vicuna, Llama-2-70b-chat, WizardLM series, Openchat), and domain-adapted medical models (MedAlpaca-13B, PMC-LLaMA-13B). Fine-tuning (full-parameter) was applied to WizardLM-13B-V1.2 on the BHP dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Text completion conditioned on: instruction I + background X. Prompts used in zero-shot, few-shot (5-shot) and full fine-tuning; few-shot examples selected randomly or via similarity retrieval. Objective: maximize sequence probability over generated hypothesis tokens. Multi-agent role-play and tool-augmented retrieval were also used to produce diverse candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty measured by a 0–3 score (part of four metrics: novelty, relevance, significance, verifiability) assigned by ChatGPT and by human annotators; also by comparison to 'golden' hypotheses and whether generated hypotheses matched unseen literature (temporal split).</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility assessed via relevance/significance/verifiability 0–3 scores from ChatGPT and human evaluation; word-overlap metrics (BLEU/ROUGE) used when golden hypotheses exist though noted insufficient alone.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Observed trade-off: zero-shot prompting tended to increase novelty while few-shot and retrieval-based in-context examples increased verifiability (plausibility) but reduced novelty; fine-tuning increased some overlap metrics but could reduce novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Automatic: BLEU and ROUGE for overlap with golden hypotheses. Human/LLM evaluation: four metrics scored 0–3 (Novelty, Relevance, Significance, Verifiability). Aggregate reported as Avg score. Coherence (Pearson/Spearman) between ChatGPT and human scores computed.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Validation against published, unseen biomedical literature (temporal split—training before Jan 2023, unseen test from Aug 2023) to identify generated hypotheses that were subsequently present/validated in the literature; additionally ChatGPT-based scoring and human expert evaluation (three biomedical annotators on 100 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Dataset (BHP) constructed from biomedical papers with explicit publication-date split to control visibility; saved seen/unseen splits; training/fine-tuning hyperparameters (e.g., WizardLM-13B-V1.2 fine-tune: 3 epochs, batch size 8, max length 2048, lr 3e-5) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding attempts via retrieval from PubMed (Engineer role) and use of tool-augmented approaches (ReAct, OpenAI function calling) to fetch evidence; temporal filtering of search results to control for test-visibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Post-generation evaluation by ChatGPT and human experts (novelty/relevance/significance/verifiability) to flag low-verifiability or implausible claims; discussion refers to literature surveys on hallucination but no automatic hallucination detector is implemented beyond evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Qualitative: authors manipulate operational uncertainty (zero-shot vs few-shot, multi-agent role interactions) as an experimental variable; no formal probabilistic UQ method (e.g., Bayesian or ensembling) is implemented or reported.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BHP (Background and Hypothesis Pairs) dataset: training literature before Jan 2023, seen test, unseen test sampled from August 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative results (from tables): gpt-3.5-turbo (0-shot) on unseen: BLEU=15.52, ROUGE=26.48, Avg (four-metric ChatGPT eval)=1.90; Llama-2-70b-chat (0-shot) unseen BLEU=11.44, ROUGE=22.04, Avg=2.04; WizardLM-13B-V1.2 (fine-tuned) unseen BLEU=19.73, ROUGE=27.58, Avg=1.79. Multi-agent + tools increased Avg ChatGPT eval from baseline 1.92 to ~2.07 in experiments (see multi-agent entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared zero-shot, few-shot, and fine-tuned variants; found zero-shot often produced higher novelty than few-shot; fine-tuning (WizardLM-13B-V1.2) improved overlap metrics and sometimes overall performance vs some baselines; multi-agent role-play improved Avg ChatGPT eval over single-agent baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Authors report that LLMs generated hypotheses that were not in the training data but matched/uncovered hypotheses present in the unseen 2023 literature (i.e., 'untrained yet validated hypotheses'); no new wet-lab experimental discoveries generated/validated by authors beyond literature matching.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No formal uncertainty quantification implemented; tool-augmented retrieval provided limited improvement and models struggled with thought-action-observation loops; few-shot/in-context examples often reduced novelty; evaluation remains dependent on ChatGPT/human scoring and word-overlap metrics which have limited coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2685.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT API model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>API-based generative LLM (OpenAI) used both as a hypothesis proposer (generation) and as an annotator/evaluator in experiments (zero-shot and few-shot prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used with zero-shot and few-shot prompts to generate hypotheses from biomedical backgrounds; also used as an annotator to score hypotheses along four metrics and to provide step-by-step explanations for scores.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (API)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompt-based text completion (zero-shot and 5-shot) conditioned on background text; also tested with similarity-retrieved few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Evaluated by ChatGPT (when used as annotator another instance) and by human experts using the four 0–3 metrics; novelty numerical scores reported (e.g., gpt-3.5-turbo (0-shot) Avg=1.90).</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Scored via relevance/significance/verifiability 0–3 by ChatGPT and humans.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Zero-shot favored novelty; few-shot increased verifiability for this model as observed in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>BLEU, ROUGE when golden exists; four-metric evaluation (novelty, relevance, significance, verifiability) scored 0–3.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Match to unseen literature (temporal validation) and human expert evaluation; ChatGPT-based scoring used to assess quality.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Not specifically implemented beyond grounding attempts via tool use in other experiments; model relied on internal knowledge for zero-shot outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Post-hoc scoring by ChatGPT/human evaluators flagged verifiability concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BHP (Background and Hypothesis Pairs) dataset (unseen test Aug 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example: gpt-3.5-turbo (8-shot) seen BLEU=13.93 ROUGE=25.32; unseen BLEU=15.52 ROUGE=26.48; ChatGPT Eval Avg=1.90 (0–3 scale aggregated). Human eval Avg ~2.02 with Pearson=0.87 Spearman=0.78 correlation to ChatGPT evaluations (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to many open-source LLMs and fine-tuned models; performance roughly competitive in Avg evaluation but fine-tuned SFT models could achieve higher BLEU/ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Produced hypotheses that matched unseen literature examples in some cases (reported as 'untrained yet validated hypotheses').</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Provenance of pretraining data unknown; potential contamination and inability to strictly guarantee zero-shot for all API-based models; no internal UQ reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2685.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT used as annotator/evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (gpt-3.5/gpt-4 family) used to rate generated hypotheses on four metrics (novelty, relevance, significance, verifiability, each 0–3) and to provide step-by-step explanations; its scores were compared to human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT-based Evaluation (GPT as annotator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ChatGPT was prompted to score hypotheses against background texts on four dimensions (0–3) and to justify scores with explanations; used to scale automatic evaluation and compared to human annotator scores (Pearson/Spearman correlations reported).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine (evaluation of biomedical hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Assigns novelty scores (0–3) with explanatory rationale for each hypothesis relative to the provided background.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Assigns relevance, significance, and verifiability scores (0–3) using provided background and known evidence; provides stepwise reasoning supporting scores.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Four metrics (Novelty, Relevance, Significance, Verifiability), each 0–3; aggregated Avg reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Correlation of ChatGPT scores with human expert scores (Pearson/Spearman > 0.7 across evaluated models) used to validate ChatGPT's utility as an annotator.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Used to surface verifiability concerns in outputs (low verifiability scores flagged potential hallucinations).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Evaluations performed on BHP unseen test samples (100 examples used in human eval; same used for ChatGPT eval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ChatGPT evaluations showed strong correlation with human annotations: Pearson and Spearman coefficients >0.7 for all models in Table 2 (e.g., Llama-2-70b-chat(0-shot) Pearson=0.89 Spearman=0.84). ChatGPT Avg scores reported per model (range ~1.41–2.20).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ChatGPT evaluation correlated well with human expert ratings and outperformed word-overlap metrics (BLEU/ROUGE) in agreement with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on LLM evaluator which itself may hallucinate or be biased; human evaluation still needed as gold standard; the metric scale coarse (0–3) and may not capture all quality aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2685.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Multi-Agent Cooperative Framework (Analyst / Engineer / Scientist / Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A role-based multi-agent system implemented with LLMs where Analyst extracts key elements, Engineer retrieves and organizes external evidence (PubMed), Scientist synthesizes hypotheses, and Critic evaluates and refines them iteratively; optionally includes human-in-the-loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM Multi-Agent Cooperative Framework (Analyst, Engineer, Scientist, Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A structured pipeline emulating scientific collaboration: Analyst distills background to keywords/topics; Engineer performs targeted searches (PubMed) and compiles evidence (using ReAct or OpenAI function calling for tool use); Scientist integrates Engineer outputs with background to propose hypotheses; Critic evaluates hypotheses for coherence/novelty/verifiability and loops back for refinement. The system may run fully automated or include human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based, multi-agent, retrieval-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Iterative role-play: Analyst->Engineer retrieval->Scientist synthesis->Critic critique; multi-round interactions produce refined hypotheses. Tool integration (PubMed searches) grounds outputs in literature while Critic enforces a verification loop.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Critic role performs internal assessment; external scoring performed by ChatGPT/human raters with the 4-metric schema; novelty indirectly increased by role diversity and uncertainty introduced by interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Critic examines evidence compiled by Engineer; verifiability metric and human/ChatGPT scoring used to estimate plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Authors report that multi-agent interactions introduce uncertainty that increased novelty and overall Avg evaluation scores; tool use increases verifiability but can reduce novelty, reflecting a trade-off between groundedness and creative exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Same four metrics (Novelty, Relevance, Significance, Verifiability) plus BLEU/ROUGE when golden present; aggregate Avg reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Generated hypotheses compared to unseen literature and evaluated by ChatGPT and human experts; multi-agent outputs reported to have higher Avg ChatGPT eval than single-agent baseline (see performance_metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Tool searches filtered to exclude literature after Jan 2023 to maintain test invisibility; multi-agent role definitions and interaction protocols described.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Engineer role performs PubMed retrieval to ground facts; temporal filtering to prevent leakage of unseen-test materials into retrieval; use of ReAct or OpenAI function calling to fetch evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Critic and downstream ChatGPT/human evaluations used to flag low-verifiability statements; iterative critique loop aimed to reduce unsupported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Uncertainty handled qualitatively by manipulating prompt/context (e.g., zero-shot vs few-shot) and by role-based interactions; authors note that 'increasing uncertainty' (less constraining context) improves candidate diversity but no quantitative UQ provided.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BHP dataset (unseen test Aug 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3 (multi-agent results): baseline (single-agent) Avg ChatGPT Eval=1.92; Multi-agent without tools Avg=2.09 (Novelty=1.35, Relevance=2.85, Significance=2.05, Verifiability=2.10); Multi-agent with tools Avg=2.07 (Novelty=1.52, Relevance=2.66, Significance=2.06, Verifiability=2.05).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Multi-agent system improved Avg ChatGPT evaluation scores relative to single-agent baseline (from ~1.92 to ~2.07–2.09), indicating better overall judged hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Multi-agent runs produced hypotheses that authors report matched evidence in unseen literature; no new experimental wet-lab discoveries performed by authors beyond literature matching.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tool-augmented retrieval provided limited gains; LLMs struggled with effective thought-action-observation loops (ReAct implementation underperformed); uncertainty handling is qualitative and lacks formal UQ; role decomposition and tool orchestration require careful design to avoid reduced novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2685.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method combining chain-of-thought style reasoning with environment actions (tool calls) so models think before acting and use observations to guide further actions; implemented here to allow LLMs to decide when to call PubMed and use retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct (reasoning + action paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instruction to LLMs to 'think' (produce reasoning) and then 'act' (invoke tools/search), receive observations, and continue reasoning based on feedback. Used to enable the Engineer agent to decide when/how to call PubMed and use returned evidence when proposing hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM reasoning + tool-use pattern</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine (used as a retrieval/action policy)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Enables a think-act-observe loop: model contemplates the background, invokes PubMed search when needed, inspects results, and refines hypothesis drafts accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Evaluated using same four metrics and BLEU/ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Outputs assessed with ChatGPT/human evaluations and compared against other tool-use approach (OpenAI function calling).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Intended to reduce unsupported claims by grounding actions in retrieved evidence; in practice observed limited effect.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BHP dataset (used in experiments reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In Table 3, ReAct-based tool use (row 2^a) produced BLEU=14.94, ROUGE=24.16, ChatGPT Eval Avg=1.56 (lower than baseline in multi-agent settings), indicating ReAct underperformed relative to multi-agent without tools and to OpenAI function calling in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ReAct with simple tool use performed worse than the single-agent baseline in some experiments and worse than the OpenAI function-calling implementation for tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors observed LLMs struggled to identify useful information and to effectively perform the think-act-observe loop; ReAct underperformed compared to other tool-calling strategies in their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2685.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Function Calling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Function Calling / API-based tool invocation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI API feature for specifying callable functions (e.g., search functions) enabling the model to invoke external tools through structured function calls; used here to perform PubMed searches and retrieve evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAI Function Calling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The authors implemented tool use via OpenAI's function-calling interface to let the model perform publication searches (PubMed) programmatically and receive structured results for hypothesis grounding and verifiability checks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>API-based tool-invocation / retrieval-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Model issues function calls to query publications; retrieved results are used by the Scientist agent to craft hypotheses and by the Critic to check verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Used retrieved literature to increase verifiability (plausibility) of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Function-calling retrieval tended to raise verifiability but could reduce novelty as outputs become more grounded in retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Evaluated using BLEU/ROUGE and the four ChatGPT/human metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compared to ReAct-based tool use and to no-tool baselines; evaluated via ChatGPT/human scoring and matching to unseen literature.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Search results filtered to exclude literature published after Jan 2023 to maintain unseen-test control.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding via retrieved PubMed results intended to reduce unsupported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Verifiability scores from ChatGPT/humans used to identify unsupported claims after generation.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BHP dataset (used for unseen evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In Table 3 OpenAI function-calling based tool use (row 2^b) showed BLEU=15.87, ROUGE=24.94, ChatGPT Eval Avg=1.49 (in reported experiments this approach did not outperform multi-agent configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performed slightly differently than ReAct; authors report ReAct performed worse than function calling for simple tool use, but overall tool use provided limited gains compared to multi-agent role decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Despite structured tool-calls, retrieved evidence often provided limited benefit for improving hypothesis generation quality in reported experiments; models still struggled to correctly identify and synthesize useful evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2685.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolFormer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced method for teaching LMs to use external tools via self-supervised fine-tuning; cited as inspiration for OpenAI function-calling experiments but not directly applied in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ToolFormer (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as related work: approach that fine-tunes LMs to learn when and how to call external APIs/tools (e.g., search, calculator) in a self-supervised manner. The paper references ToolFormer to motivate function-calling experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>tool-augmented, self-supervised fine-tuning (retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Proposed in original work to reduce hallucination by grounding via tools (paper only referenced here).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2685.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reasoning prompting technique that elicits intermediate chain-of-thought style explanations from LLMs to improve multi-step reasoning; referenced as conceptual basis and predecessor to ReAct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Provides intermediate reasoning traces in LLM outputs to support multi-step inference; cited as background and foundation for ReAct and other deliberative prompting approaches used conceptually in multi-agent and tool-use experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>prompting / reasoning technique</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not directly used as an isolated method here, but informs ReAct and multi-step reasoning prompting in agent implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2685.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BHP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Background and Hypothesis Pairs (BHP) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical dataset constructed by the authors of background knowledge and corresponding hypothesis pairs, split by publication date to enforce unseen-test conditions for LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BHP (Background and Hypothesis Pairs) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Dataset built from biomedical literature (Jan 2000–Sep 2023), pipeline: compose paper set, summarize with chatgpt-turbo-3.5, generate background-hypothesis pairs, filter, and split by publication date. Training includes literature before Jan 2023 (seen), unseen test sampled from Aug 2023 to ensure temporal novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark dataset</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Temporal split by publication date; construction steps based on Self-Instruct pipeline documented; topic distributions and meta-info provided in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>This entity is the benchmark dataset used in all experiments (BHP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Unseen test limited to 200 pairs from August 2023; small unseen test size relative to seen set; reliance on ChatGPT summarization in data generation pipeline may introduce biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2685.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardLM-13B-V1.2 (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardLM-13B-V1.2 (Supervised Fine-Tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instructed LLM (Llama-2 based) fine-tuned by the authors on the BHP background-hypothesis pairs (full-parameter fine-tuning) and evaluated as a hypothesis proposer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WizardLM-13B-V1.2 (fine-tuned on BHP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>WizardLM-13B-V1.2 (Llama-2-based) was fine-tuned end-to-end on the BHP background->hypothesis pairs for three epochs (batch size 8, max seq len 2048, lr=3e-5) with early stopping. Used to evaluate the effect of instruction tuning / domain adaptation on hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based, fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Full-parameter supervised fine-tuning on background-hypothesis pairs, then generation with prompts similar to training style.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Evaluated via BLEU/ROUGE overlap with golden hypotheses and ChatGPT/human four-metric scoring; authors note SFT can increase overlap but may reduce novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Assessed by ChatGPT/human metrics; SFT increased verifiability vs zero-shot in some comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Fine-tuning improved overlap/word-matching (plausibility signals) but sometimes lowered novelty compared to zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>BLEU/ROUGE for overlap; ChatGPT/human 4-metric scores (0–3).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Performance measured on seen and unseen test sets; outputs compared to unseen literature and human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Fine-tuning hyperparameters and dataset splits reported; checkpoints selected by early stopping on seen test.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human/ChatGPT verifiability scores flag unsupported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Trained/evaluated on BHP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>WizardLM-13B-V1.2 (SFT) seen BLEU=19.13 ROUGE=27.35; unseen BLEU=19.73 ROUGE=27.58; ChatGPT Eval Avg=1.79 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>After fine-tuning, WizardLM-13B-V1.2 surpassed some models on BLEU/ROUGE and rivaled API-based gpt-3.5 in some metrics but did not uniformly dominate human/ChatGPT Avg scores.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Fine-tuned model produced hypotheses matching unseen literature in some cases (reported), but no novel experimental discoveries beyond literature matching.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Fine-tuning increases memorization/overlap and may reduce generative novelty; potential domain overfitting to training hypothesis styles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2685.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MedAlpaca / PMC-LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-adapted medical LLMs (MedAlpaca-13B, PMC-LLaMA-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLMs adapted to the medical domain via domain-specific corpora (medical books, PMC papers, medical dialogs) evaluated for hypothesis generation and domain adaptation effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MedAlpaca-13B / PMC-LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MedAlpaca-13B and PMC-LLaMA-13B are Llama-based models adapted/finetuned on medical corpora; evaluated to examine whether domain adaptation improves hypothesis generation/verifiability. MedAlpaca showed low Avg ChatGPT eval in some settings; PMC-LLaMA improved BLEU/ROUGE but ChatGPT metrics gave mixed gains.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based, domain-adapted</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Zero-shot and few-shot prompting after domain-adaptive fine-tuning; few-shot retrieval also tested.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>ChatGPT/human 4-metric evaluation; domain models sometimes produced lower novelty but higher overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Measured via verifiability scores and BLEU/ROUGE overlap to golden hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Domain adaptation increased word-overlap performance (plausibility signals) but had minimal effect on ChatGPT-evaluated novelty; tradeoff observed.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>BLEU, ROUGE, ChatGPT/human 4-metric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluation on BHP seen/unseen splits with ChatGPT and human assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human/ChatGPT scoring for verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BHP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples from Table 1: MedAlpaca-13B (0-shot) unseen BLEU=5.82 ROUGE=20.49, ChatGPT Avg=0.99; PMC-LLaMA-13B (0-shot) unseen BLEU=22.37 ROUGE=40.45, ChatGPT Avg=1.41 (but human/ChatGPT assessments differ).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Domain adaptation improved BLEU/ROUGE in some cases (PMC-LLaMA) but ChatGPT's four-metric evaluations did not always show significant gains, highlighting metric discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Domain adaptation advantages depend on metric; BLEU/ROUGE may overstate improvements while semantic evaluations by ChatGPT/humans show smaller effect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2685.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2685.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BHP Eval Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Novelty/Relevance/Significance/Verifiability evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four-dimension evaluation scheme devised by the authors to judge generated hypotheses: each dimension scored 0–3 by ChatGPT and humans to assess quality beyond word-overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Four-metric Hypothesis Evaluation (Novelty, Relevance, Significance, Verifiability)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A manual and automatic evaluation suite where generated hypotheses are scored 0–3 on novelty (newness relative to background), relevance (pertinence), significance (scientific importance), and verifiability (evidence-supported), accompanied by step-by-step explanations when scored by ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>evaluation metrics / human + LLM annotation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>medicine / biomedicine (applied to BHP generations)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty scored 0–3 by annotators (ChatGPT/human) based on whether hypothesis adds new claims beyond background.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility inferred from relevance/significance/verifiability scores; verifiability specifically judges whether claims can be supported by evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Used to measure trade-offs: authors observed zero-shot increased novelty but decreased verifiability; few-shot increased verifiability but lowered novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Explicit metrics: Novelty, Relevance, Significance, Verifiability (each 0–3); also BLEU/ROUGE for overlap when golden exists.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Correlation analysis between ChatGPT scores and human scores (Pearson/Spearman) to validate ChatGPT evaluator reliability; correlations >0.7 reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Scoring rubric and use of 3 biomedical human annotators for a 100-sample subset described.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Low verifiability scores used as proxy to flag potential hallucinations or unsupported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to BHP generated outputs (unseen test subset) during experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Strong correlation between ChatGPT and human evaluations: Pearson/Spearman >0.7 across models (Table 2). Word-overlap metrics showed poor correlation with human scores (low Pearson/Spearman).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Coarse 0–3 scale; human evaluation expensive so only applied to selected top models and 100 unseen examples; verifiability is subjective and dependent on retrievable literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero Shot Hypothesis Proposers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment <em>(Rating: 2)</em></li>
                <li>ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks <em>(Rating: 1)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2685",
    "paper_id": "paper-713b604fb9cdd6631074cbd6bf36db029031992e",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "LLMs",
            "name_full": "Large Language Models",
            "brief_description": "Pretrained transformer-based language models (e.g., GPT-3.5, GPT-4, Llama series, WizardLM) used in this work to generate scientific hypotheses, to be evaluated in zero-shot, few-shot, and fine-tuned settings across biomedical background+hypothesis pairs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Large Language Models (GPT-3.5, GPT-4, Llama variants, WizardLM, Vicuna, Openchat, MedAlpaca, PMC-LLaMA)",
            "system_description": "Pretrained transformer LLMs were prompted (zero-shot and few-shot) or fine-tuned on background-&gt;hypothesis pairs to produce natural-language hypothesis proposals. The paper frames hypothesis generation as a text-completion task optimizing P(y_t | y_&lt;t, I, X). Models evaluated include API-based (gpt-3.5-turbo, gpt-4 references), general instructed LLMs (Vicuna, Llama-2-70b-chat, WizardLM series, Openchat), and domain-adapted medical models (MedAlpaca-13B, PMC-LLaMA-13B). Fine-tuning (full-parameter) was applied to WizardLM-13B-V1.2 on the BHP dataset.",
            "system_type": "LLM-based",
            "scientific_domain": "medicine / biomedicine",
            "hypothesis_generation_method": "Text completion conditioned on: instruction I + background X. Prompts used in zero-shot, few-shot (5-shot) and full fine-tuning; few-shot examples selected randomly or via similarity retrieval. Objective: maximize sequence probability over generated hypothesis tokens. Multi-agent role-play and tool-augmented retrieval were also used to produce diverse candidate hypotheses.",
            "novelty_assessment_method": "Novelty measured by a 0–3 score (part of four metrics: novelty, relevance, significance, verifiability) assigned by ChatGPT and by human annotators; also by comparison to 'golden' hypotheses and whether generated hypotheses matched unseen literature (temporal split).",
            "plausibility_assessment_method": "Plausibility assessed via relevance/significance/verifiability 0–3 scores from ChatGPT and human evaluation; word-overlap metrics (BLEU/ROUGE) used when golden hypotheses exist though noted insufficient alone.",
            "novelty_plausibility_balance": "Observed trade-off: zero-shot prompting tended to increase novelty while few-shot and retrieval-based in-context examples increased verifiability (plausibility) but reduced novelty; fine-tuning increased some overlap metrics but could reduce novelty.",
            "hypothesis_quality_metrics": "Automatic: BLEU and ROUGE for overlap with golden hypotheses. Human/LLM evaluation: four metrics scored 0–3 (Novelty, Relevance, Significance, Verifiability). Aggregate reported as Avg score. Coherence (Pearson/Spearman) between ChatGPT and human scores computed.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Validation against published, unseen biomedical literature (temporal split—training before Jan 2023, unseen test from Aug 2023) to identify generated hypotheses that were subsequently present/validated in the literature; additionally ChatGPT-based scoring and human expert evaluation (three biomedical annotators on 100 examples).",
            "reproducibility_measures": "Dataset (BHP) constructed from biomedical papers with explicit publication-date split to control visibility; saved seen/unseen splits; training/fine-tuning hyperparameters (e.g., WizardLM-13B-V1.2 fine-tune: 3 epochs, batch size 8, max length 2048, lr 3e-5) reported.",
            "hallucination_prevention_method": "Grounding attempts via retrieval from PubMed (Engineer role) and use of tool-augmented approaches (ReAct, OpenAI function calling) to fetch evidence; temporal filtering of search results to control for test-visibility.",
            "hallucination_detection_method": "Post-generation evaluation by ChatGPT and human experts (novelty/relevance/significance/verifiability) to flag low-verifiability or implausible claims; discussion refers to literature surveys on hallucination but no automatic hallucination detector is implemented beyond evaluation.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Qualitative: authors manipulate operational uncertainty (zero-shot vs few-shot, multi-agent role interactions) as an experimental variable; no formal probabilistic UQ method (e.g., Bayesian or ensembling) is implemented or reported.",
            "benchmark_dataset": "BHP (Background and Hypothesis Pairs) dataset: training literature before Jan 2023, seen test, unseen test sampled from August 2023.",
            "performance_metrics": "Representative results (from tables): gpt-3.5-turbo (0-shot) on unseen: BLEU=15.52, ROUGE=26.48, Avg (four-metric ChatGPT eval)=1.90; Llama-2-70b-chat (0-shot) unseen BLEU=11.44, ROUGE=22.04, Avg=2.04; WizardLM-13B-V1.2 (fine-tuned) unseen BLEU=19.73, ROUGE=27.58, Avg=1.79. Multi-agent + tools increased Avg ChatGPT eval from baseline 1.92 to ~2.07 in experiments (see multi-agent entry).",
            "comparison_with_baseline": "Compared zero-shot, few-shot, and fine-tuned variants; found zero-shot often produced higher novelty than few-shot; fine-tuning (WizardLM-13B-V1.2) improved overlap metrics and sometimes overall performance vs some baselines; multi-agent role-play improved Avg ChatGPT eval over single-agent baseline.",
            "validated_on_real_science": true,
            "novel_discoveries": "Authors report that LLMs generated hypotheses that were not in the training data but matched/uncovered hypotheses present in the unseen 2023 literature (i.e., 'untrained yet validated hypotheses'); no new wet-lab experimental discoveries generated/validated by authors beyond literature matching.",
            "limitations": "No formal uncertainty quantification implemented; tool-augmented retrieval provided limited improvement and models struggled with thought-action-observation loops; few-shot/in-context examples often reduced novelty; evaluation remains dependent on ChatGPT/human scoring and word-overlap metrics which have limited coverage.",
            "uuid": "e2685.0",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "gpt-3.5-turbo (ChatGPT API model)",
            "brief_description": "API-based generative LLM (OpenAI) used both as a hypothesis proposer (generation) and as an annotator/evaluator in experiments (zero-shot and few-shot prompts).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "gpt-3.5-turbo",
            "system_description": "Used with zero-shot and few-shot prompts to generate hypotheses from biomedical backgrounds; also used as an annotator to score hypotheses along four metrics and to provide step-by-step explanations for scores.",
            "system_type": "LLM-based (API)",
            "scientific_domain": "medicine / biomedicine",
            "hypothesis_generation_method": "Prompt-based text completion (zero-shot and 5-shot) conditioned on background text; also tested with similarity-retrieved few-shot examples.",
            "novelty_assessment_method": "Evaluated by ChatGPT (when used as annotator another instance) and by human experts using the four 0–3 metrics; novelty numerical scores reported (e.g., gpt-3.5-turbo (0-shot) Avg=1.90).",
            "plausibility_assessment_method": "Scored via relevance/significance/verifiability 0–3 by ChatGPT and humans.",
            "novelty_plausibility_balance": "Zero-shot favored novelty; few-shot increased verifiability for this model as observed in experiments.",
            "hypothesis_quality_metrics": "BLEU, ROUGE when golden exists; four-metric evaluation (novelty, relevance, significance, verifiability) scored 0–3.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Match to unseen literature (temporal validation) and human expert evaluation; ChatGPT-based scoring used to assess quality.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Not specifically implemented beyond grounding attempts via tool use in other experiments; model relied on internal knowledge for zero-shot outputs.",
            "hallucination_detection_method": "Post-hoc scoring by ChatGPT/human evaluators flagged verifiability concerns.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "BHP (Background and Hypothesis Pairs) dataset (unseen test Aug 2023)",
            "performance_metrics": "Example: gpt-3.5-turbo (8-shot) seen BLEU=13.93 ROUGE=25.32; unseen BLEU=15.52 ROUGE=26.48; ChatGPT Eval Avg=1.90 (0–3 scale aggregated). Human eval Avg ~2.02 with Pearson=0.87 Spearman=0.78 correlation to ChatGPT evaluations (Table 2).",
            "comparison_with_baseline": "Compared to many open-source LLMs and fine-tuned models; performance roughly competitive in Avg evaluation but fine-tuned SFT models could achieve higher BLEU/ROUGE.",
            "validated_on_real_science": true,
            "novel_discoveries": "Produced hypotheses that matched unseen literature examples in some cases (reported as 'untrained yet validated hypotheses').",
            "limitations": "Provenance of pretraining data unknown; potential contamination and inability to strictly guarantee zero-shot for all API-based models; no internal UQ reported.",
            "uuid": "e2685.1",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChatGPT-eval",
            "name_full": "ChatGPT used as annotator/evaluator",
            "brief_description": "ChatGPT (gpt-3.5/gpt-4 family) used to rate generated hypotheses on four metrics (novelty, relevance, significance, verifiability, each 0–3) and to provide step-by-step explanations; its scores were compared to human evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT-based Evaluation (GPT as annotator)",
            "system_description": "ChatGPT was prompted to score hypotheses against background texts on four dimensions (0–3) and to justify scores with explanations; used to scale automatic evaluation and compared to human annotator scores (Pearson/Spearman correlations reported).",
            "system_type": "LLM-based evaluator",
            "scientific_domain": "medicine / biomedicine (evaluation of biomedical hypothesis generation)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": "Assigns novelty scores (0–3) with explanatory rationale for each hypothesis relative to the provided background.",
            "plausibility_assessment_method": "Assigns relevance, significance, and verifiability scores (0–3) using provided background and known evidence; provides stepwise reasoning supporting scores.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Four metrics (Novelty, Relevance, Significance, Verifiability), each 0–3; aggregated Avg reported.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Correlation of ChatGPT scores with human expert scores (Pearson/Spearman &gt; 0.7 across evaluated models) used to validate ChatGPT's utility as an annotator.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Used to surface verifiability concerns in outputs (low verifiability scores flagged potential hallucinations).",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Evaluations performed on BHP unseen test samples (100 examples used in human eval; same used for ChatGPT eval).",
            "performance_metrics": "ChatGPT evaluations showed strong correlation with human annotations: Pearson and Spearman coefficients &gt;0.7 for all models in Table 2 (e.g., Llama-2-70b-chat(0-shot) Pearson=0.89 Spearman=0.84). ChatGPT Avg scores reported per model (range ~1.41–2.20).",
            "comparison_with_baseline": "ChatGPT evaluation correlated well with human expert ratings and outperformed word-overlap metrics (BLEU/ROUGE) in agreement with human judgments.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Relies on LLM evaluator which itself may hallucinate or be biased; human evaluation still needed as gold standard; the metric scale coarse (0–3) and may not capture all quality aspects.",
            "uuid": "e2685.2",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Multi-Agent Framework",
            "name_full": "LLM-based Multi-Agent Cooperative Framework (Analyst / Engineer / Scientist / Critic)",
            "brief_description": "A role-based multi-agent system implemented with LLMs where Analyst extracts key elements, Engineer retrieves and organizes external evidence (PubMed), Scientist synthesizes hypotheses, and Critic evaluates and refines them iteratively; optionally includes human-in-the-loop.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM Multi-Agent Cooperative Framework (Analyst, Engineer, Scientist, Critic)",
            "system_description": "A structured pipeline emulating scientific collaboration: Analyst distills background to keywords/topics; Engineer performs targeted searches (PubMed) and compiles evidence (using ReAct or OpenAI function calling for tool use); Scientist integrates Engineer outputs with background to propose hypotheses; Critic evaluates hypotheses for coherence/novelty/verifiability and loops back for refinement. The system may run fully automated or include human oversight.",
            "system_type": "LLM-based, multi-agent, retrieval-augmented",
            "scientific_domain": "medicine / biomedicine",
            "hypothesis_generation_method": "Iterative role-play: Analyst-&gt;Engineer retrieval-&gt;Scientist synthesis-&gt;Critic critique; multi-round interactions produce refined hypotheses. Tool integration (PubMed searches) grounds outputs in literature while Critic enforces a verification loop.",
            "novelty_assessment_method": "Critic role performs internal assessment; external scoring performed by ChatGPT/human raters with the 4-metric schema; novelty indirectly increased by role diversity and uncertainty introduced by interaction.",
            "plausibility_assessment_method": "Critic examines evidence compiled by Engineer; verifiability metric and human/ChatGPT scoring used to estimate plausibility.",
            "novelty_plausibility_balance": "Authors report that multi-agent interactions introduce uncertainty that increased novelty and overall Avg evaluation scores; tool use increases verifiability but can reduce novelty, reflecting a trade-off between groundedness and creative exploration.",
            "hypothesis_quality_metrics": "Same four metrics (Novelty, Relevance, Significance, Verifiability) plus BLEU/ROUGE when golden present; aggregate Avg reported.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Generated hypotheses compared to unseen literature and evaluated by ChatGPT and human experts; multi-agent outputs reported to have higher Avg ChatGPT eval than single-agent baseline (see performance_metrics).",
            "reproducibility_measures": "Tool searches filtered to exclude literature after Jan 2023 to maintain test invisibility; multi-agent role definitions and interaction protocols described.",
            "hallucination_prevention_method": "Engineer role performs PubMed retrieval to ground facts; temporal filtering to prevent leakage of unseen-test materials into retrieval; use of ReAct or OpenAI function calling to fetch evidence.",
            "hallucination_detection_method": "Critic and downstream ChatGPT/human evaluations used to flag low-verifiability statements; iterative critique loop aimed to reduce unsupported claims.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Uncertainty handled qualitatively by manipulating prompt/context (e.g., zero-shot vs few-shot) and by role-based interactions; authors note that 'increasing uncertainty' (less constraining context) improves candidate diversity but no quantitative UQ provided.",
            "benchmark_dataset": "BHP dataset (unseen test Aug 2023)",
            "performance_metrics": "Table 3 (multi-agent results): baseline (single-agent) Avg ChatGPT Eval=1.92; Multi-agent without tools Avg=2.09 (Novelty=1.35, Relevance=2.85, Significance=2.05, Verifiability=2.10); Multi-agent with tools Avg=2.07 (Novelty=1.52, Relevance=2.66, Significance=2.06, Verifiability=2.05).",
            "comparison_with_baseline": "Multi-agent system improved Avg ChatGPT evaluation scores relative to single-agent baseline (from ~1.92 to ~2.07–2.09), indicating better overall judged hypothesis quality.",
            "validated_on_real_science": true,
            "novel_discoveries": "Multi-agent runs produced hypotheses that authors report matched evidence in unseen literature; no new experimental wet-lab discoveries performed by authors beyond literature matching.",
            "limitations": "Tool-augmented retrieval provided limited gains; LLMs struggled with effective thought-action-observation loops (ReAct implementation underperformed); uncertainty handling is qualitative and lacks formal UQ; role decomposition and tool orchestration require careful design to avoid reduced novelty.",
            "uuid": "e2685.3",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "brief_description": "A method combining chain-of-thought style reasoning with environment actions (tool calls) so models think before acting and use observations to guide further actions; implemented here to allow LLMs to decide when to call PubMed and use retrieved evidence.",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "mention_or_use": "use",
            "system_name": "ReAct (reasoning + action paradigm)",
            "system_description": "Instruction to LLMs to 'think' (produce reasoning) and then 'act' (invoke tools/search), receive observations, and continue reasoning based on feedback. Used to enable the Engineer agent to decide when/how to call PubMed and use returned evidence when proposing hypotheses.",
            "system_type": "LLM reasoning + tool-use pattern",
            "scientific_domain": "medicine / biomedicine (used as a retrieval/action policy)",
            "hypothesis_generation_method": "Enables a think-act-observe loop: model contemplates the background, invokes PubMed search when needed, inspects results, and refines hypothesis drafts accordingly.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Evaluated using same four metrics and BLEU/ROUGE.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Outputs assessed with ChatGPT/human evaluations and compared against other tool-use approach (OpenAI function calling).",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Intended to reduce unsupported claims by grounding actions in retrieved evidence; in practice observed limited effect.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "BHP dataset (used in experiments reported in Table 3)",
            "performance_metrics": "In Table 3, ReAct-based tool use (row 2^a) produced BLEU=14.94, ROUGE=24.16, ChatGPT Eval Avg=1.56 (lower than baseline in multi-agent settings), indicating ReAct underperformed relative to multi-agent without tools and to OpenAI function calling in these experiments.",
            "comparison_with_baseline": "ReAct with simple tool use performed worse than the single-agent baseline in some experiments and worse than the OpenAI function-calling implementation for tool use.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Authors observed LLMs struggled to identify useful information and to effectively perform the think-act-observe loop; ReAct underperformed compared to other tool-calling strategies in their setup.",
            "uuid": "e2685.4",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "OpenAI Function Calling",
            "name_full": "OpenAI Function Calling / API-based tool invocation",
            "brief_description": "OpenAI API feature for specifying callable functions (e.g., search functions) enabling the model to invoke external tools through structured function calls; used here to perform PubMed searches and retrieve evidence.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "OpenAI Function Calling",
            "system_description": "The authors implemented tool use via OpenAI's function-calling interface to let the model perform publication searches (PubMed) programmatically and receive structured results for hypothesis grounding and verifiability checks.",
            "system_type": "API-based tool-invocation / retrieval-augmented",
            "scientific_domain": "medicine / biomedicine",
            "hypothesis_generation_method": "Model issues function calls to query publications; retrieved results are used by the Scientist agent to craft hypotheses and by the Critic to check verifiability.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Used retrieved literature to increase verifiability (plausibility) of generated hypotheses.",
            "novelty_plausibility_balance": "Function-calling retrieval tended to raise verifiability but could reduce novelty as outputs become more grounded in retrieved documents.",
            "hypothesis_quality_metrics": "Evaluated using BLEU/ROUGE and the four ChatGPT/human metrics.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compared to ReAct-based tool use and to no-tool baselines; evaluated via ChatGPT/human scoring and matching to unseen literature.",
            "reproducibility_measures": "Search results filtered to exclude literature published after Jan 2023 to maintain unseen-test control.",
            "hallucination_prevention_method": "Grounding via retrieved PubMed results intended to reduce unsupported claims.",
            "hallucination_detection_method": "Verifiability scores from ChatGPT/humans used to identify unsupported claims after generation.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "BHP dataset (used for unseen evaluation)",
            "performance_metrics": "In Table 3 OpenAI function-calling based tool use (row 2^b) showed BLEU=15.87, ROUGE=24.94, ChatGPT Eval Avg=1.49 (in reported experiments this approach did not outperform multi-agent configurations).",
            "comparison_with_baseline": "Performed slightly differently than ReAct; authors report ReAct performed worse than function calling for simple tool use, but overall tool use provided limited gains compared to multi-agent role decomposition.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Despite structured tool-calls, retrieved evidence often provided limited benefit for improving hypothesis generation quality in reported experiments; models still struggled to correctly identify and synthesize useful evidence.",
            "uuid": "e2685.5",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ToolFormer",
            "name_full": "ToolFormer: Language Models Can Teach Themselves to Use Tools",
            "brief_description": "A referenced method for teaching LMs to use external tools via self-supervised fine-tuning; cited as inspiration for OpenAI function-calling experiments but not directly applied in the paper's experiments.",
            "citation_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "mention_or_use": "mention",
            "system_name": "ToolFormer (referenced)",
            "system_description": "Cited as related work: approach that fine-tunes LMs to learn when and how to call external APIs/tools (e.g., search, calculator) in a self-supervised manner. The paper references ToolFormer to motivate function-calling experiments.",
            "system_type": "tool-augmented, self-supervised fine-tuning (retrieval-augmented)",
            "scientific_domain": "",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Proposed in original work to reduce hallucination by grounding via tools (paper only referenced here).",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": null,
            "uuid": "e2685.6",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "Reasoning prompting technique that elicits intermediate chain-of-thought style explanations from LLMs to improve multi-step reasoning; referenced as conceptual basis and predecessor to ReAct.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "system_name": "Chain-of-Thought prompting",
            "system_description": "Provides intermediate reasoning traces in LLM outputs to support multi-step inference; cited as background and foundation for ReAct and other deliberative prompting approaches used conceptually in multi-agent and tool-use experiments.",
            "system_type": "prompting / reasoning technique",
            "scientific_domain": null,
            "hypothesis_generation_method": "Not directly used as an isolated method here, but informs ReAct and multi-step reasoning prompting in agent implementations.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": null,
            "uuid": "e2685.7",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "BHP dataset",
            "name_full": "Background and Hypothesis Pairs (BHP) dataset",
            "brief_description": "A biomedical dataset constructed by the authors of background knowledge and corresponding hypothesis pairs, split by publication date to enforce unseen-test conditions for LLM evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BHP (Background and Hypothesis Pairs) dataset",
            "system_description": "Dataset built from biomedical literature (Jan 2000–Sep 2023), pipeline: compose paper set, summarize with chatgpt-turbo-3.5, generate background-hypothesis pairs, filter, and split by publication date. Training includes literature before Jan 2023 (seen), unseen test sampled from Aug 2023 to ensure temporal novelty.",
            "system_type": "benchmark dataset",
            "scientific_domain": "medicine / biomedicine",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": "Temporal split by publication date; construction steps based on Self-Instruct pipeline documented; topic distributions and meta-info provided in Appendix.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "This entity is the benchmark dataset used in all experiments (BHP).",
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Unseen test limited to 200 pairs from August 2023; small unseen test size relative to seen set; reliance on ChatGPT summarization in data generation pipeline may introduce biases.",
            "uuid": "e2685.8",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "WizardLM-13B-V1.2 (SFT)",
            "name_full": "WizardLM-13B-V1.2 (Supervised Fine-Tuned)",
            "brief_description": "An instructed LLM (Llama-2 based) fine-tuned by the authors on the BHP background-hypothesis pairs (full-parameter fine-tuning) and evaluated as a hypothesis proposer.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "WizardLM-13B-V1.2 (fine-tuned on BHP)",
            "system_description": "WizardLM-13B-V1.2 (Llama-2-based) was fine-tuned end-to-end on the BHP background-&gt;hypothesis pairs for three epochs (batch size 8, max seq len 2048, lr=3e-5) with early stopping. Used to evaluate the effect of instruction tuning / domain adaptation on hypothesis generation.",
            "system_type": "LLM-based, fine-tuned",
            "scientific_domain": "medicine / biomedicine",
            "hypothesis_generation_method": "Full-parameter supervised fine-tuning on background-hypothesis pairs, then generation with prompts similar to training style.",
            "novelty_assessment_method": "Evaluated via BLEU/ROUGE overlap with golden hypotheses and ChatGPT/human four-metric scoring; authors note SFT can increase overlap but may reduce novelty.",
            "plausibility_assessment_method": "Assessed by ChatGPT/human metrics; SFT increased verifiability vs zero-shot in some comparisons.",
            "novelty_plausibility_balance": "Fine-tuning improved overlap/word-matching (plausibility signals) but sometimes lowered novelty compared to zero-shot settings.",
            "hypothesis_quality_metrics": "BLEU/ROUGE for overlap; ChatGPT/human 4-metric scores (0–3).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Performance measured on seen and unseen test sets; outputs compared to unseen literature and human annotations.",
            "reproducibility_measures": "Fine-tuning hyperparameters and dataset splits reported; checkpoints selected by early stopping on seen test.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Human/ChatGPT verifiability scores flag unsupported claims.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Trained/evaluated on BHP",
            "performance_metrics": "WizardLM-13B-V1.2 (SFT) seen BLEU=19.13 ROUGE=27.35; unseen BLEU=19.73 ROUGE=27.58; ChatGPT Eval Avg=1.79 (Table 1).",
            "comparison_with_baseline": "After fine-tuning, WizardLM-13B-V1.2 surpassed some models on BLEU/ROUGE and rivaled API-based gpt-3.5 in some metrics but did not uniformly dominate human/ChatGPT Avg scores.",
            "validated_on_real_science": true,
            "novel_discoveries": "Fine-tuned model produced hypotheses matching unseen literature in some cases (reported), but no novel experimental discoveries beyond literature matching.",
            "limitations": "Fine-tuning increases memorization/overlap and may reduce generative novelty; potential domain overfitting to training hypothesis styles.",
            "uuid": "e2685.9",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MedAlpaca / PMC-LLaMA",
            "name_full": "Domain-adapted medical LLMs (MedAlpaca-13B, PMC-LLaMA-13B)",
            "brief_description": "Open-source LLMs adapted to the medical domain via domain-specific corpora (medical books, PMC papers, medical dialogs) evaluated for hypothesis generation and domain adaptation effects.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "MedAlpaca-13B / PMC-LLaMA-13B",
            "system_description": "MedAlpaca-13B and PMC-LLaMA-13B are Llama-based models adapted/finetuned on medical corpora; evaluated to examine whether domain adaptation improves hypothesis generation/verifiability. MedAlpaca showed low Avg ChatGPT eval in some settings; PMC-LLaMA improved BLEU/ROUGE but ChatGPT metrics gave mixed gains.",
            "system_type": "LLM-based, domain-adapted",
            "scientific_domain": "medicine / biomedicine",
            "hypothesis_generation_method": "Zero-shot and few-shot prompting after domain-adaptive fine-tuning; few-shot retrieval also tested.",
            "novelty_assessment_method": "ChatGPT/human 4-metric evaluation; domain models sometimes produced lower novelty but higher overlap.",
            "plausibility_assessment_method": "Measured via verifiability scores and BLEU/ROUGE overlap to golden hypotheses.",
            "novelty_plausibility_balance": "Domain adaptation increased word-overlap performance (plausibility signals) but had minimal effect on ChatGPT-evaluated novelty; tradeoff observed.",
            "hypothesis_quality_metrics": "BLEU, ROUGE, ChatGPT/human 4-metric scores.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluation on BHP seen/unseen splits with ChatGPT and human assessments.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Human/ChatGPT scoring for verifiability.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "BHP dataset",
            "performance_metrics": "Examples from Table 1: MedAlpaca-13B (0-shot) unseen BLEU=5.82 ROUGE=20.49, ChatGPT Avg=0.99; PMC-LLaMA-13B (0-shot) unseen BLEU=22.37 ROUGE=40.45, ChatGPT Avg=1.41 (but human/ChatGPT assessments differ).",
            "comparison_with_baseline": "Domain adaptation improved BLEU/ROUGE in some cases (PMC-LLaMA) but ChatGPT's four-metric evaluations did not always show significant gains, highlighting metric discrepancies.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Domain adaptation advantages depend on metric; BLEU/ROUGE may overstate improvements while semantic evaluations by ChatGPT/humans show smaller effect.",
            "uuid": "e2685.10",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "BHP Eval Metrics",
            "name_full": "Novelty/Relevance/Significance/Verifiability evaluation metrics",
            "brief_description": "Four-dimension evaluation scheme devised by the authors to judge generated hypotheses: each dimension scored 0–3 by ChatGPT and humans to assess quality beyond word-overlap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Four-metric Hypothesis Evaluation (Novelty, Relevance, Significance, Verifiability)",
            "system_description": "A manual and automatic evaluation suite where generated hypotheses are scored 0–3 on novelty (newness relative to background), relevance (pertinence), significance (scientific importance), and verifiability (evidence-supported), accompanied by step-by-step explanations when scored by ChatGPT.",
            "system_type": "evaluation metrics / human + LLM annotation",
            "scientific_domain": "medicine / biomedicine (applied to BHP generations)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": "Novelty scored 0–3 by annotators (ChatGPT/human) based on whether hypothesis adds new claims beyond background.",
            "plausibility_assessment_method": "Plausibility inferred from relevance/significance/verifiability scores; verifiability specifically judges whether claims can be supported by evidence.",
            "novelty_plausibility_balance": "Used to measure trade-offs: authors observed zero-shot increased novelty but decreased verifiability; few-shot increased verifiability but lowered novelty.",
            "hypothesis_quality_metrics": "Explicit metrics: Novelty, Relevance, Significance, Verifiability (each 0–3); also BLEU/ROUGE for overlap when golden exists.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Correlation analysis between ChatGPT scores and human scores (Pearson/Spearman) to validate ChatGPT evaluator reliability; correlations &gt;0.7 reported.",
            "reproducibility_measures": "Scoring rubric and use of 3 biomedical human annotators for a 100-sample subset described.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Low verifiability scores used as proxy to flag potential hallucinations or unsupported claims.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied to BHP generated outputs (unseen test subset) during experiments.",
            "performance_metrics": "Strong correlation between ChatGPT and human evaluations: Pearson/Spearman &gt;0.7 across models (Table 2). Word-overlap metrics showed poor correlation with human scores (low Pearson/Spearman).",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Coarse 0–3 scale; human evaluation expensive so only applied to selected top models and 100 unseen examples; verifiability is subjective and dependent on retrievable literature.",
            "uuid": "e2685.11",
            "source_info": {
                "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "rating": 2
        },
        {
            "paper_title": "ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks",
            "rating": 1
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 1
        }
    ],
    "cost": 0.0296775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Zero Shot Hypothesis Proposers</h1>
<p>Biqing Qi ${ }^{1,2,3 <em>}$<br>qibiqing7@gmail.com<br>Kaiyan Zhang ${ }^{1 </em>}$<br>zhang-ky22@mails.tsinghua.edu.cn<br>Haoxiang Li ${ }^{1}$<br>kai Tian ${ }^{1}$<br>Sihang Zeng ${ }^{4}$<br>hx-li20@mails.tsinghua.edu.cn tk23@mails.tsinghua.edu.cn zengsh@uw.edu<br>Zhang-Ren Chen ${ }^{5}$ Jin-Fang Hu ${ }^{5}$！ Bowen Zhou ${ }^{1,2}$！<br>chenzhangren@ncu.edu.cn hujinfang333@126.com zhoubowen@tsinghua.edu.cn<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Frontis.AI ${ }^{3}$ Harbin Institute of Technology<br>${ }^{4}$ University of Washington ${ }^{5}$ The First Affiliated Hospital of Nanchang University</p>
<h4>Abstract</h4>
<p>Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.</p>
<h2>1 Introduction</h2>
<p>"When nothing is sure, everything is possible." - Margaret Drabble</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The pursuit of knowledge discovery stands as a cornerstone of human progress, driving innovation, and shaping our understanding of the world [30, 28]. However, in recent times, the process of knowledge discovery has encountered formidable challenges, characterized by serendipity and sluggishness. As the volume of data and literature continues to expand at an unprecedented rate, the ability to distill high-value insights and gain profound understanding from this wealth of information has become increasingly daunting[28]. Silos of information have erected themselves between disciplines, impeding the crucial cross-pollination of ideas and insights that could propel discovery to new heights. Yet, amidst these challenges, there is a glimmer of hope. The advent of large-scale models (LLMs), possessing the capacity to harness a vast reservoir of world knowledge and span multiple domains, holds promise in revolutionizing the landscape of knowledge discovery. These models present an opportunity to break down the barriers between disciplines, enabling researchers to traverse the expansive sea of information with ease and efficiency. Central to the process of knowledge discovery lies the formulation of sound hypotheses [42, 31, 1, 38]. However, a glaring gap persists in the arsenal of tools available to formally explore and evaluate hypotheses. While literature is replete with discussions on validation, it often overlooks the critical aspect of generating novel hypotheses.</p>
<p>In light of these challenges and opportunities, this paper delves into the current state of knowledge discovery, examining the hurdles posed by information explosion and disciplinary isolation. It explores the potential transformative role of LLMs in bridging these gaps, ultimately emphasizing the pivotal role of hypothesis generation in the knowledge discovery process. Furthermore, it highlights the pressing need for tools and methodologies to facilitate hypothesis generation, thus propelling knowledge discovery into a new era of efficiency and innovation [13]. Currently, both ChatGPT and GPT-4 undergo extensive pre-training on vast datasets and possess the capability of continuous updates. However, ensuring strict traceability of data sources becomes a challenging task, limiting our ability to explore zero-shot hypothesis generation.</p>
<p>The past literatures have explored scenarios of problem discovery, yet rigorous experimental designs to investigate whether LLMs can effectively propose genuine problems under zero-shot conditions remain lacking. To tackle this issue, we assemble a dataset of biomedicine literature spanning from January 2000 to September 2023. This dataset is partitioned into training and testing sets, with the training set exclusively containing literature published before January 2023. We construct an unseen test set using literature from August 2023 and ensure that the evaluated LLMs have been trained on corpora before that date. Additionally, we devise a multi-intelligent collaborative framework that incorporates search tools and role-playing to delve deeper into and uncover the potential for hypothesis generation.</p>
<p>Through experiments and analyses as shown in Figure 1, we draw the following findings: 1) LLMs surprisingly generate hypotheses that are untrained yet validated when tested against literature. 2) Increasing uncertainty levels can benefit by diversifying candidate generation and potentially enhancing zero-shot hypothesis generation capabilities. For instance, introducing heightened uncertainty through collaborative multi-agent approaches significantly improves the model’s ability to generalize in zero-shot scenarios. However, integrating subsequent few-shot enhancements and using additional tools may reduce the model’s proficiency in generating hypotheses. This phenomenon is likely due to the reduction of uncertainty, limiting the model’s space for hypothesis generation. Consequently, it lacks consistent positive effects, underscoring the need for careful consideration of the type of external knowledge employed. The above findings also support the notion: "When nothing is sure, everything is possible." Specifically, our contributions are as follows:</p>
<p>1) To rigorously validate the zero-shot and few-shot hypothesis generation potential of LLMs, we construct temporal biomedical instruction data and devised novel and effective experiments for</p>
<p>comprehensive analysis and evaluation. To the best of our knowledge, this is the first work that formally designs experiments to investigate the zero shot hypothesis generation capacity of LLMs.
2) Through validation across different models and various scenario dimensions, we surprisingly find that LLMs possess rudimentary higher-order knowledge reasoning capabilities and can propose new hypothesis statements. This provides new empirical insights and pathways for knowledge discovery.
3) For a comprehensive review of the generated hypotheses, we design metrics across four dimensions for both ChatGPT-based and human evaluations. The correlation scores between ChatGPT evaluations and manual results indicate that LLMs also play a significant role in hypothesis evaluations.
4) To efficiently explore and further harness the capability of hypothesis generation, we introduce a multi-agent system based on LLMs. Through efficient collaboration among multiple models and tool utilization, we analyze the factors influencing hypothesis generation by LLMs.</p>
<h1>2 Process of Scientific Discovery</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The iterative experimental loop of scientific discovery: observations and data accumulated from past experiments are analyzed and used to generate new hypotheses, and in turn new experiments that will yield new data to continue to cycle. In this paper, we mainly focus on investigating whether LLMs have the zero shot generalization ability to generate new hypotheses.</p>
<p>Scientific discovery involves key components, each crucial for advancing our understanding of the natural world: data analysis, hypothesis formulation, experiment design, execution, and observation and reflection [13] as shown in Figure 2.</p>
<p>1) Data Analysis: Foundational in the scientific process, it entails collecting and examining data to discern patterns and anomalies, extracting insights through statistical techniques and visualization. It initiates scientific inquiry, guiding further exploration. 2) Generating Hypotheses: Among these components, hypothesis formulation is pivotal. It entails crafting informed guesses to explain observed phenomena. Hypotheses serve as guiding frameworks, directing and focusing research by articulating specific relationships and outcomes for experimental exploration. 3) Experiment Design: Once a hypothesis is set, designing experiments becomes essential to rigorously test its validity. This involves defining variables, specifying control groups, and outlining methods and procedures. Well-designed experiments ensure objective hypothesis testing and yield meaningful, informative results. 4) Experiment Execution: Meticulous execution of designed experiments and data collection are critical. Researchers adhere precisely to experimental protocols, recording observations, measurements, and unexpected findings. Integrity in execution ensures reliable, reproducible outcomes. 5) Accumulating Observations: After experiments, scientists engage in observation and reflection. They analyze collected data to determine if results support or refute the initial hypothesis. If unsupported, hypotheses may be revised or new ones formulated based on findings. Observation and reflection permit iterative refinement of scientific understanding.
Hypothesis Pioneers Pathways: Guiding Knowledge Discovery. While all components are essential, hypothesis formulation holds a unique position. It drives the scientific endeavor, guiding research question selection, experiment design, and data analysis. Well-constructed hypotheses not only provide direction but also lay the foundation for meaningful scientific discoveries by posing rigorously testable questions. Hypothesis formulation serves as the intellectual anchor steering scientific investigation and ultimately advancing knowledge.</p>
<p>3 Can LLMs Truly Generate Zero-Shot Hypotheses?</p>
<p>In this section, we outline the methodology employed for a thorough assessment of LLMs’ capacity to generate hypotheses under zero-shot conditions. To accomplish this, we begin by defining the problem of hypothesis generation in zero-shot settings. Next, we elucidate the process of dataset construction within the biomedical domain. Finally, we undertake comprehensive experiments to evaluate various instructed models across multiple dimensions, aiming to explore the factors influencing the ability of LLMs to propose improved hypotheses.</p>
<h3>3.1 Problem Definition</h3>
<p>Following the scientific discovery process outlined in Section 2, hypothesis generation typically occurs after thorough literature analysis and examination of specific phenomena. To enhance evaluation effectiveness, we formalize this process as a text completion task. Given dataset $D$, an instruction $I$, and text pairs $(X_{i},Y_{i})<em i="i">{i=1}^{n}$ containing background knowledge and corresponding hypotheses, extracted from medical papers, our objective is to assess model $M$ by having it generate hypotheses based on the task instruction and background knowledge, i.e., $M(I,X</em>$. The objective function is formulated as:})=Y_{i}$, for each $i$ $\in{1,..,,n</p>
<p>$y^{*}=\underset{y_{1},...,y_{n}}{\arg\max}\prod_{t=1}^{n}{P(y_{t}|y_{1},...,y_{t-1},I,X)}.$</p>
<h3>3.2 Dataset Construction</h3>
<p>In this section, we detail the process of constructing datasets and ensuring the robustness of our evaluation. Prevalent LLMs, like Llama and ChatGPT, face challenges in tracing the origin of their knowledge due to continuous self-updating. To address this, we propose a novel approach to assess LLMs’ hypothesis generation. Recognizing their potential impact on public domain data, we reconstruct a new biomedical literature dataset based on publication dates.</p>
<p>As depicted in Figure 3, we designated the year 2023 as the cut-off point. Our training dataset comprises literature published before January 2023, while the test dataset comprises literature published after January 2023, forming pairs of data with background knowledge and hypothesis proposals. Due to the emergence of more advanced LLMs, our evaluations focus exclusively on the unseen test set, featuring literature published in August 2023. We selected instructed models fine-tuned before August 2023 for both evaluation and fine-tuning testing. In our experimental setup, we implemented stringent measures to ensure the models had no prior exposure to the test data, affirming the validity of our experiments. We strictly follow the standard pipeline as outlined in Self-Instruct [32] for our data generation process, encompassing four key steps: 1) Compose the paper set based on the topic and content of the literature. 2) Utilize chatgpt-turbo-3.5 to summarize the literature knowledge. 3) Generate background knowledge-assume pairs. 4) Filter low-quality data. 5) Split the dataset according to publication time.</p>
<h3>3.3 Datast Analysis</h3>
<p>In this section, we provide a comprehensive overview of the constructed dataset, encompassing details about the data acquisition strategy, dataset size, visibility control measures, distribution by year and month, as well as topic distribution.</p>
<p>We have created two datasets to maintain control over the visibility of hypotheses: 1) Seen dataset This dataset comprises 2700 background and hypothesis pairs sourced from literature published before January 2023. This dataset was partitioned into training (2500) and validation (200) subsets (as well as seen test set). It is consistent with the corpus that the LLMs have been exposed to. 2) Unseen dataset The unseen dataset consists of 200 pairs extracted from papers published in August 2023, which the LLMs have not encountered during training and are used for testing purposes.</p>
<p>We also provide publication date and topic distribution of constructed dataset in Appendix B.1.</p>
<h1>3.4 Experiment Setup</h1>
<p>In this section, we introduce experimental settings for hypothesis generation and evaluation.
Models For a fair comparison, we exclusively evaluate LLMs trained on corpora before March 2023 to ensure the test set remains unseen. We consider three categories of models in total: 1) API-based LLMs: this is mainly ChatGPT. 2) General domain instructed LLMs: These models consist of open-source models that have undergone fine-tuning based on Llama using general domain instructions. We primarily choose the top-tier models based on their performance rankings on the Alpaca Eval Leaderboard ${ }^{3}$. 3) Specific domain instructed LLMs: These include PMC-LLaMA [35], and MedAlpaca [10]. These models are trained on a variety of sources in medicine domain, such as medical books, PMC papers, medical dialogs, and others. We provide detailed meta-information for various models, including their training data sources and publication dates, in Appendix B.2.
Prompts To ensure a consistent output format across different models, we create prompts in two formats: zero-shot and few-shot examples. In our experiments, we adopt a 5-shot format, selecting examples from the training set before January 2023 using both randomly sampled and similarity retrieval methods. We provide illustrations of zero-shot and few-shot prompts in Appendix E.
Finetuning To assess the hypothesis generation capability beyond zero-shot scenarios, we identify the top-performing open-source models through few-shot evaluation. We proceed to fine-tune the full parameters of WizardLM-13B-V1.2 with the background and hypothesis pairs. The fine-tuning process consists of three epochs, employing a batch size of 8 , a maximum sequence length of 2048 tokens, and a learning rate set at $3 \mathrm{e}-5$. We implement early stopping and select the best checkpoints based on their performance on the seen test dataset.
Evaluation Given the disparities between the hypothesis generation task and traditional text generation tasks liking machine translation and summarization, with the former being more challenging and often involving uncertainty that extends beyond established ground truth, we approach our evaluation from two primary perspectives: conducting evaluations with and without golden hypotheses. In evaluations with golden hypotheses, we employ standard text generation metrics, including BLEU and ROUGE in evaluate library ${ }^{4}$, to assess word overlap between the generated outputs and the ground truth. The vastness of the hypothesis space renders it difficult to comprehensively assess the quality of generated hypotheses using word overlap metrics alone. To provide a more comprehensive evaluation of the generated hypotheses from multiple facets, we have thoughtfully devised four metrics: novelty, relevance, significance, and verifiability. Inspired by recent research that highlights ChatGPT as proficient annotators [8, 16], demonstrating a strong correlation with human ratings, we employ ChatGPT for further evaluation. In detail, we request ChatGPT to evaluate both the generated scientific hypotheses and the provided background across these aspects. The scoring scale ranges from 0 to 3 , where a higher score indicates superior results. Additionally, we solicit ChatGPT to furnish a step-by-step explanation to substantiate the assigned score. Moreover, we conduct human evaluation based on the four metrics for the top-tier models identified in the automatic evaluation in Section 3.5, and we provide a detailed description of this process in Section 3.6.</p>
<h3>3.5 Experiment Results</h3>
<p>This section presents the results of hypothesis generation across various models in both zero-shot and few-shot settings. We primarily analyze the results from two perspectives: the impact of the zero-shot setting and the influence of introducing external knowledge on hypothesis generation.</p>
<h3>3.5.1 Impact of zero-shot settings</h3>
<p>The results presented in Table 1 demonstrate the significant impact of zero-shot settings in improving hypothesis generation, particularly in terms of fostering high novelty. We analyze these results from two key perspectives as following.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results of various LLMs: We assess instructed models using zero-shot and few-shot format prompts to generate constrained outputs. To provide a comprehensive assessment, we calculate the average scores for novelty, relevance, significance, and verifiability, denoted as Avg. Results marked with an asterisk (*) indicate that the few-shot prompts are constructed by retrieving samples from the training set that are similar to the background of inputs. To facilitate better comparison, we highlight the highest and sub-high score with both bold and underline formatting under each category.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>Seen</th>
<th></th>
<th></th>
<th></th>
<th>Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>BLEU</td>
<td>ROUGE</td>
<td>BLEU</td>
<td>ROUGE</td>
<td>Novelty</td>
<td>Relevance</td>
<td>Significance</td>
<td>Verifiability</td>
</tr>
<tr>
<td>API-based</td>
<td>gpt-3.5-turbot(8-shot)</td>
<td>13.93</td>
<td>25.32</td>
<td>15.52</td>
<td>26.48</td>
<td>1.42</td>
<td>2.63</td>
<td>1.58</td>
<td>1.97</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbot(5-shot)</td>
<td>16.47</td>
<td>27.07</td>
<td>16.49</td>
<td>26.96</td>
<td>1.22</td>
<td>2.57</td>
<td>1.84</td>
<td>2.03</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbot(5-shot)*</td>
<td>17.33</td>
<td>27.28</td>
<td>17.71</td>
<td>27.53</td>
<td>1.02</td>
<td>2.61</td>
<td>1.85</td>
<td>2.36</td>
</tr>
<tr>
<td>General</td>
<td>Vicuna-33b-v1.3(0-shot)</td>
<td>13.97</td>
<td>24.43</td>
<td>13.66</td>
<td>23.43</td>
<td>1.67</td>
<td>2.55</td>
<td>2.04</td>
<td>1.84</td>
</tr>
<tr>
<td></td>
<td>Vicuna-33b-v1.3(5-shot)</td>
<td>11.23</td>
<td>22.54</td>
<td>11.49</td>
<td>22.68</td>
<td>1.60</td>
<td>2.40</td>
<td>1.67</td>
<td>1.90</td>
</tr>
<tr>
<td></td>
<td>Vicuna-33b-v1.3(5-shot)*</td>
<td>12.78</td>
<td>24.11</td>
<td>13.12</td>
<td>23.66</td>
<td>1.19</td>
<td>2.71</td>
<td>2.00</td>
<td>2.17</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(0-shot)</td>
<td>10.95</td>
<td>21.56</td>
<td>11.44</td>
<td>22.04</td>
<td>1.86</td>
<td>2.41</td>
<td>1.91</td>
<td>1.98</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)</td>
<td>8.17</td>
<td>21.09</td>
<td>7.63</td>
<td>20.70</td>
<td>1.95</td>
<td>2.38</td>
<td>2.06</td>
<td>2.22</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)*</td>
<td>8.40</td>
<td>21.65</td>
<td>9.66</td>
<td>22.43</td>
<td>1.43</td>
<td>2.50</td>
<td>1.94</td>
<td>2.15</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(0-shot)</td>
<td>11.91</td>
<td>23.35</td>
<td>12.03</td>
<td>23.55</td>
<td>1.62</td>
<td>2.55</td>
<td>1.90</td>
<td>1.90</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(5-shot)</td>
<td>14.00</td>
<td>24.30</td>
<td>13.82</td>
<td>24.38</td>
<td>1.33</td>
<td>2.54</td>
<td>1.81</td>
<td>2.23</td>
</tr>
<tr>
<td></td>
<td>WizardLM-13B-V1.2(5-shot)*</td>
<td>14.96</td>
<td>25.66</td>
<td>15.26</td>
<td>25.78</td>
<td>1.06</td>
<td>2.64</td>
<td>1.73</td>
<td>2.14</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(0-shot)</td>
<td>13.45</td>
<td>24.12</td>
<td>14.25</td>
<td>25.05</td>
<td>1.57</td>
<td>2.45</td>
<td>1.74</td>
<td>1.89</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)</td>
<td>14.04</td>
<td>24.59</td>
<td>13.78</td>
<td>24.28</td>
<td>1.17</td>
<td>2.61</td>
<td>2.12</td>
<td>2.14</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)*</td>
<td>14.46</td>
<td>24.78</td>
<td>15.26</td>
<td>25.56</td>
<td>0.97</td>
<td>2.67</td>
<td>1.85</td>
<td>1.99</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(0-shot)</td>
<td>8.79</td>
<td>22.71</td>
<td>8.38</td>
<td>21.48</td>
<td>1.58</td>
<td>2.51</td>
<td>1.70</td>
<td>2.05</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(5-shot)</td>
<td>12.46</td>
<td>23.60</td>
<td>12.58</td>
<td>24.21</td>
<td>1.06</td>
<td>2.64</td>
<td>2.09</td>
<td>2.20</td>
</tr>
<tr>
<td></td>
<td>Openchat-v3.2-super(5-shot)*</td>
<td>12.37</td>
<td>23.93</td>
<td>12.88</td>
<td>24.78</td>
<td>1.16</td>
<td>2.76</td>
<td>2.10</td>
<td>2.23</td>
</tr>
<tr>
<td>Medicine</td>
<td>MedAlpaca-13B(0-shot)</td>
<td>6.10</td>
<td>22.07</td>
<td>5.82</td>
<td>20.49</td>
<td>0.55</td>
<td>1.17</td>
<td>1.17</td>
<td>1.06</td>
</tr>
<tr>
<td></td>
<td>MedAlpaca-13B(5-shot)</td>
<td>0.99</td>
<td>3.84</td>
<td>1.08</td>
<td>3.84</td>
<td>0.98</td>
<td>1.32</td>
<td>1.32</td>
<td>1.49</td>
</tr>
<tr>
<td></td>
<td>MedAlpaca-13B(5-shot)*</td>
<td>4.60</td>
<td>9.36</td>
<td>4.50</td>
<td>9.07</td>
<td>1.09</td>
<td>1.40</td>
<td>1.20</td>
<td>1.53</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(0-shot)</td>
<td>22.89</td>
<td>40.36</td>
<td>22.37</td>
<td>40.45</td>
<td>0.76</td>
<td>1.94</td>
<td>1.42</td>
<td>1.52</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)</td>
<td>1.36</td>
<td>4.83</td>
<td>1.41</td>
<td>4.78</td>
<td>1.13</td>
<td>1.45</td>
<td>1.26</td>
<td>0.88</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)*</td>
<td>6.21</td>
<td>12.39</td>
<td>6.16</td>
<td>12.13</td>
<td>1.73</td>
<td>2.17</td>
<td>1.88</td>
<td>2.09</td>
</tr>
<tr>
<td>SFT</td>
<td>WizardLM-13B-V1.2</td>
<td>19.13</td>
<td>27.35</td>
<td>19.73</td>
<td>27.58</td>
<td>0.97</td>
<td>2.55</td>
<td>1.38</td>
<td>2.26</td>
</tr>
</tbody>
</table>
<p>Zero-shot Outperforms Few-shot. Our findings indicate that, for extra large models like Llama-2-70b-chat and WizardLM-70B-V1.0, zero-shot performance surpasses that of the fewshot setting, where few-shot examples are obtained by randomly sampling. This suggests that the capacity of hypothesis generation is limited by the inclusion of few-shot examples, and models exhibit stronger abilities in a zero-shot setting.
Outperforming the Unseen Compared to the Seen Test Set. Despite the visibility of literature published before 2022 in the pre-training corpus of most LLMs, we have categorized the test set into "seen" and "unseen." Typically, LLMs may excel in the "seen" test set due to the potential memorization of hypotheses present in the pre-training corpus, resulting in higher performance compared to the "unseen" test set. However, our results indicate that LLMs tend to perform better on the "unseen" test set. We speculate that this is because the complexity of hypothesis generation may hinder LLMs from effectively leveraging the dark knowledge in their parameters.</p>
<h1>3.5.2 Influence of external knowledge</h1>
<p>Based on the results, we observe that the introduction of external knowledge, such as few-shot examples, domain adaptation, and instruction fine-tuning, does not consistently enhance the ability of hypothesis proposing.
Few-Shot Examples Enhance Verifiability but Decrease Novelty. In comparison to zero-shot settings, models using few-shot prompts benefit from the provided examples, resulting in very high matching rates. Regarding word overlap metrics, including BLEU and ROUGE, most models, especially WizardLM series models, and Openchat-v3.2-super, show improved performance when provided with in-context examples, with retrieved examples being particularly beneficial for their generations. However, it's important to note that these few-shot prompts significantly increase verifiability while simultaneously leading to lower levels of novelty compared to zero-shot results.
Randomly Sampled Few-Shot Examples vs. Similarity Retrieval. Given that randomly sampled in-context examples often differ from the provided background in terms of topics or domains, this can potentially confuse LLMs and lead to decreased performance. In our pursuit of further exploration into the hypothesis generation capabilities of LLMs, we retrieve examples from the training dataset based on their similarity to the given background. The results indicate that similarity retrieval can further enhance performance.</p>
<p>Instruction Tuning Enhances LLM Performance. Following fine-tuning on a dataset comprising background and hypothesis pairs, WizardLM-13B-V1.2 attains superior performance, surpassing even gpt-3.5-turbo and WizardLM-70B-V1.0. This finding underscores that domain adaptation remains a valuable approach to enhance the hypothesis generation capabilities of LLMs. It not only leads to greater resource efficiency but also supports the promotion of privacy in a localized context.
Impact of Domain Adaptation on Hypothesis Generation. We have also conducted an analysis of the influence of fine-tuning for domain adaptation on hypothesis generation. In this comparison, we utilize instructed models adapted to the field of medicine. The results obtained from MedAplaca and PMC-LLaMA indicate that domain adaptation can significantly improve word overlap performance. However, the metrics derived from ChatGPT suggest that domain adaptation has only a minimal effect on hypothesis generation. This discrepancy between word overlap metrics and ChatGPT's evaluation highlights the need for more comprehensive and unified metrics in the context of hypothesis generation tasks.</p>
<h1>3.6 Human Evaluation and Case Study</h1>
<p>In this section, we conduct a human evaluation to assess the generated hypotheses and calculate coherence scores to compare them with ChatGPT evaluation scores, guiding further evaluation efforts.</p>
<h3>3.6.1 Evaluation Settings</h3>
<p>Evaluation Metrics To comprehensively evaluate the generations manually and simultaneously assess the quality of ChatGPT's evaluations, we continue to utilize the four metrics outlined in Section 3.4, which encompass novelty, relevance, significance, and verifiability. The range of each metric remains from 0 to 3 , with higher values indicating better performance. Additionally, we calculate coherence scores between human evaluations and ChatGPT evaluations.
Selection of Models Given the constraints associated with the cost of human evaluation, our primary objective is to assess whether LLMs can produce valuable hypotheses, rather than striving for state-of-the-art performance. As a result, we exclusively perform human evaluation on the outputs generated by the LLM that ranks highest in performance based on automatic metrics and ChatGPT evaluation. Furthermore, we aim to encompass a variety of prompts and models in our evaluation. The final models selected for human evaluation are detailed in Table 2.
Evaluation Details We randomly selected 100 examples from the unseen test set and had three evaluators with a biomedical background assign scores for each metric to each model.</p>
<h3>3.6.2 Evaluation Results</h3>
<p>As depicted in Table 2, the human evaluations exhibit a strong correlation with ChatGPT's evaluations, with Pearson and Spearman coefficients exceeding 0.7 for all models. These results strongly support our earlier findings regarding the influence of zero-shot learning and external knowledge. This reinforces our assertion that large language models can effectively propose hypotheses and significantly contribute to scientific discovery. For additional insights, we present correlation coefficients between word overlap scores and manual scores in the Appendix C, revealing lower coherence and highlighting the need for more advanced evaluation metrics.
We also conduct a case study that showcases the hypotheses generated by various models and includes examples of step-by-step evaluations by ChatGPT. Details can be found in Appendix D.</p>
<h2>4 Can agent collaboration enhance LLMs' zero-shot generalization?</h2>
<p>In this section, we will strive to enhance the ability of LLMs in hypothesis generation through multi-agent collaboration and the use of tools. Our objective is to improve hypothesis efficiency by employing multi-agent collaboration, simulating real-world research scenarios. To begin, we introduce the conceptual system of multi-agent collaboration for hypothesis generation, drawing inspiration from scientific research. Subsequently, we present the role design and the tools use in this context. Finally, we present preliminary validated results of the multi-agent system using our proposed BHP dataset.</p>
<p>Table 2: This table presents the results of human evaluation. The Avg Coefficient are used to assess the correlation between the average scores obtained from ChatGPT and those from human evaluation.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>ChatGPT</th>
<th>Human Eval</th>
<th></th>
<th></th>
<th></th>
<th>Avg Coefficient</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Eval.Avg</td>
<td>Novelty</td>
<td>Relevance</td>
<td>Significance</td>
<td>Verifiability</td>
<td>Avg</td>
<td>Pearson</td>
<td>Spearman</td>
</tr>
<tr>
<td>API-based</td>
<td>gpt-3.5-turbo(0-shot)</td>
<td>1.90</td>
<td>1.54</td>
<td>2.69</td>
<td>1.77</td>
<td>2.08</td>
<td>2.02</td>
<td>0.87</td>
<td>0.78</td>
</tr>
<tr>
<td></td>
<td>gpt-3.5-turbo(5-shot)*</td>
<td>1.96</td>
<td>1.31</td>
<td>2.62</td>
<td>2.08</td>
<td>2.62</td>
<td>2.15</td>
<td>0.80</td>
<td>0.78</td>
</tr>
<tr>
<td>General</td>
<td>Llama-2-70b-chat(0-shot)</td>
<td>2.04</td>
<td>1.77</td>
<td>2.23</td>
<td>1.92</td>
<td>1.92</td>
<td>1.96</td>
<td>0.89</td>
<td>0.84</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)</td>
<td>2.20</td>
<td>2.15</td>
<td>2.77</td>
<td>2.08</td>
<td>2.31</td>
<td>2.33</td>
<td>0.96</td>
<td>0.90</td>
</tr>
<tr>
<td></td>
<td>Llama-2-70b-chat(5-shot)*</td>
<td>2.01</td>
<td>1.38</td>
<td>2.62</td>
<td>2.31</td>
<td>2.00</td>
<td>2.08</td>
<td>0.97</td>
<td>0.94</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(0-shot)</td>
<td>1.91</td>
<td>1.38</td>
<td>2.31</td>
<td>1.54</td>
<td>2.00</td>
<td>1.81</td>
<td>0.90</td>
<td>0.75</td>
</tr>
<tr>
<td></td>
<td>WizardLM-70B-V1.0(5-shot)</td>
<td>2.01</td>
<td>1.15</td>
<td>2.69</td>
<td>2.46</td>
<td>1.77</td>
<td>2.02</td>
<td>0.85</td>
<td>0.89</td>
</tr>
<tr>
<td>Medicine</td>
<td>PMC-LLaMA-13B(0-shot)</td>
<td>1.41</td>
<td>1.00</td>
<td>2.62</td>
<td>1.92</td>
<td>2.00</td>
<td>1.88</td>
<td>0.73</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td>PMC-LLaMA-13B(5-shot)*</td>
<td>1.97</td>
<td>1.85</td>
<td>2.23</td>
<td>1.92</td>
<td>1.69</td>
<td>1.92</td>
<td>0.95</td>
<td>0.94</td>
</tr>
<tr>
<td>SFT</td>
<td>WizardLM-13BV1.2</td>
<td>1.79</td>
<td>0.85</td>
<td>2.77</td>
<td>1.23</td>
<td>2.23</td>
<td>1.77</td>
<td>0.83</td>
<td>0.85</td>
</tr>
</tbody>
</table>
<h1>4.1 Multi-agent Framework</h1>
<p>Inspired by the structured methodology detailed in Section 2, we introduce a comprehensive framework tailored for hypothesis formulation. This framework encapsulates a multi-agent system where each agent assumes a distinct role, mirroring the collaborative nature of scientific endeavors. Through a symbiotic and iterative process, these agents collaborate to craft hypotheses that are not only grounded in existing knowledge but also pave the way for novel insights. By emulating the essence of scientific discovery, our framework strives to produce hypotheses that are both innovative and scientifically robust. As depicted in Figure 4, we have partitioned the framework into five components, encompassing four automated agents and the option for human involvement within the loop.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: The conceptual system of multi-agent collaboration for hypothesis generation. The overall prototyping process is illustrated below, allowing users to choose optional involvement. We offer core role descriptions of multi-agents and the fully automated system above.</p>
<p>Role Design In our proposed multi-agent framework, each component plays a distinct and pivotal role. The Analyst serves as the foundation, meticulously extracting and defining core elements from the research background. Its primary objective is to interpret the literature, distilling it into keywords or topics that subsequently guide the Engineer's search efforts. The Engineer, leveraging these keywords, embarks on a mission to retrieve and organize pertinent information. They meticulously plan and execute detailed searches, ensuring that the findings are compiled in a structured manner. This organized materials then lands in the domain of the Scientist, whose objective is to weave together the Engineer's findings with the original research background. Through careful interpretation, the Scientist crafts a hypothesis that is both grounded in existing knowledge and offers a fresh perspective. However, before this hypothesis is finalized, it undergoes scrutiny by the Critic. The Critic's role</p>
<p>is paramount in ensuring the hypothesis's robustness, coherence, and novelty. They evaluate the hypothesis against the backdrop of the research background, ensuring it stands up to academic rigor. Feedback from the Critic, if necessary, loops back to refine the hypothesis or prompts the Analyst for further insights, creating a cyclical and iterative process of refinement.</p>
<p>Tool Use To explore external knowledge beyond the inherent dark knowledge within LLMs, we integrate the Engineer agent with search engines, mainly PubMed ${ }^{5}$. Similarly, to control the visibility of the unseen test dataset, we filter and exclude literature published after January 2023 from the search results. We carry out tool use experiments using ReAct [40] and OpenAI function calling. ReAct is a method that extends the concept of Chain of Thought (CoT) [34], involving thinking before taking action and subsequently making observations based on feedback from the environment. In our experiments, we instruct the LLMs to initially contemplate the provided background information and then make a decision regarding whether to utilize tools. Upon receiving feedback from the tools, the LLMs are expected to identify supporting evidence in the results or potentially make further tool requests. The LLMs are responsible for concluding the hypothesis generation process and summarizing the hypotheses independently. In the case of OpenAI function calling, we directly specify tools for publication searching and transmit them to OpenAI APIs. This process is roughly implemented through fine-tuning, as described in ToolFormer [24].</p>
<h1>4.2 Experiment Results</h1>
<p>Our primary focus is to investigate the impact of tool use and multi-agent collaboration on hypothesis generation. We present the experimental results in Table 3. Based on the results, we summarize our findings from two perspectives: tool use and role-playing.</p>
<p>Influence of Tool Use Based on our results, we observe that tool use has minimal impact on improving the hypothesis generation ability of LLMs. This observation aligns with the findings presented in the previous sections regarding the analysis of external knowledge. Notably, the ReAct-based method performs worse than OpenAI function calling. It is also evident that LLMs struggle to identify useful information and exhibit weaknesses in the thought-action-observation process, even when utilizing the official interface from OpenAI. Hypothesis generation is indeed a challenging task that necessitates iterative discussions and the exchange of ideas among various individuals.</p>
<p>Multi-Agent Collaboration In addition to tool use, our findings suggest that the division of labor and interaction among multi-agents can significantly enhance the model's capability to propose hypotheses by introducing uncertainty. This mirrors the dynamics of real-world scientific research, where hypotheses are formulated through iterative discussions and refutations. Additionally, it is worth noting that tool use can further enhance the performance of the multi-agent framework.</p>
<p>Table 3: Results of individual agents and multi-agent systems, both with and without the use of tools, on the unseen test dataset. The results demonstrate that both multi-agent systems and the utilization of tools enhance the ability of LLMs in hypothesis generation. Among the various types of models, both $2^{a}$ and $2^{b}$ are evaluated with tool use. The difference between them lies in their implementations: ReAct [40] and OpenAI function calling ${ }^{6}$, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Influence Factor</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Automatic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4 Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-agent</td>
<td style="text-align: center;">Tool use</td>
<td style="text-align: center;">BLUE</td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">Novelty</td>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">Significance</td>
<td style="text-align: center;">Verifiability</td>
<td style="text-align: center;">Avg</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.52</td>
<td style="text-align: center;">26.48</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">1.92</td>
</tr>
<tr>
<td style="text-align: center;">$2^{a}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">14.94</td>
<td style="text-align: center;">24.16</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">2.42</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">1.87</td>
<td style="text-align: center;">1.56</td>
</tr>
<tr>
<td style="text-align: center;">$2^{b}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">15.87</td>
<td style="text-align: center;">24.94</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">1.49</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.71</td>
<td style="text-align: center;">22.11</td>
<td style="text-align: center;">1.35</td>
<td style="text-align: center;">2.85</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">2.10</td>
<td style="text-align: center;">2.09</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">22.04</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">2.07</td>
</tr>
</tbody>
</table>
<h2>5 Conclusion</h2>
<p>From the hypothesis-proposer perspective, we investigated LLMs' zero-shot generalisation ability in scientific research. Specifically, we first build a comprehensive corpus based on biomedical literature,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>split by publication date, including background knowledge and hypothesis pairs. This corpus is then used as a basis for fine-tuning LLMs, leading to the generation of the LLM. To further analysis and enhance the capabilities of the hypothesis proposer, we introduce a LLM-based multi-agent collaboration system. Experimental results show that fine-tuned LLMs of various sizes can propose new hypotheses that did not appear in the training data but can be confirmed by the test literature, with performance comparable to ChatGPT and in some cases even better. Notably, our study revealed that introducing uncertainty into processes and operations enhances zero-shot generalization capabilities. These findings confirm the potential of LLMs to propose new hypotheses and offers hope for future unlocked scientific discovery. In future work, we will focus on optimizing models and generating hypotheses guided by effective uncertainty assessment metrics.</p>
<h1>Acknowledgements</h1>
<p>We extend our gratitude to the anonymous reviewers for their insightful feedback.</p>
<h2>References</h2>
<p>[1] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.
[2] Andres M. Bran, Sam Cox, Andrew D. White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools, June 2023. arXiv:2304.05376.
[3] Boxi Cao, Hongyu Lin, Xianpei Han, and Le Sun. The Life Cycle of Knowledge in Big Language Models: A Survey, March 2023. arXiv:2303.07616 [cs].
[4] Zhuo Chang, Jing Zhang, Yilun Liu, Huajian Gao, and Guang-Kui Xu. New Mechanical Markers for Tracking the Progression of Myocardial Infarction. Nano Letters, 23(16):7350-7357, August 2023.
[5] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen. Asking Before Action: Gather Information in Embodied Decision Making with Language Models, May 2023. arXiv:2305.15695 [cs].
[6] Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Empowering Private Tutoring by Chaining Large Language Models, September 2023. arXiv:2309.08112 [cs] version: 1.
[7] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback, May 2023. arXiv:2305.10142 [cs].
[8] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. ChatGPT Outperforms CrowdWorkers for Text-Annotation Tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, July 2023. arXiv:2303.15056 [cs].
[9] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. MindAgent: Emergent Gaming Interaction, September 2023. arXiv:2309.09971 [cs] version: 1.
[10] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno K Bressem. Medalpaca-an open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023.
[11] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework, August 2023. arXiv:2308.00352 [cs].
[12] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory, June 2023. arXiv:2306.03901 [cs].</p>
<p>[13] Moksh Jain, Tristan Deleu, Jason Hartford, Cheng-Hao Liu, Alex Hernandez-Garcia, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. Digital Discovery, 2(3):557-577, 2023.
[14] Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, and Jie Fu. Think Before You Act: Decision Transformers with Internal Working Memory, May 2023. 0 citations (Semantic Scholar/arXiv) [2023-05-30] arXiv:2305.16338 [cs].
[15] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society, March 2023. arXiv:2303.17760 [cs].
[16] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment, May 2023. arXiv:2303.16634 [cs].
[17] Philipp Maas, Frank Carey, Chris Wheeler, Edward Saatchi, Pete Billington, and Jessica Yaffa Shamash. SHOW-1 and Showrunner Agents in Multi-Agent Simulations. arXiv preprint, 2023.
[18] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback, June 2022. arXiv:2112.09332.
[19] Aaron Parisi, Yao Zhao, and Noah Fiedel. TALM: Tool Augmented Language Models, May 2022. arXiv:2205.12255 [cs].
[20] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative Agents: Interactive Simulacra of Human Behavior, August 2023. arXiv:2304.03442 [cs].
[21] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large Language Model Connected with Massive APIs, May 2023. arXiv:2305.15334 [cs].
[22] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative Agents for Software Development, August 2023. arXiv:2307.07924 [cs].
[23] Vipula Rawte, Amit Sheth, and Amitava Das. A Survey of Hallucination in Large Foundation Models, September 2023. arXiv:2309.05922 [cs].
[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach Themselves to Use Tools, February 2023. arXiv:2302.04761.
[25] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, May 2023. arXiv:2303.17580 [cs].
[26] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, June 2023. arXiv:2303.11366 [cs].
[27] Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs, June 2023. arXiv:2306.06624 [cs].
[28] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.
[29] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the Planning Abilities of Large Language Models - A Critical Investigation, May 2023. arXiv:2305.15771 [cs].</p>
<p>[30] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, 2023.
[31] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Learning to generate novel scientific directions with contextualized literature-based discovery. arXiv preprint arXiv:2305.14259, 2023.
[32] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
[33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, January 2023. arXiv:2201.11903 [cs].
[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.
[35] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmcllama: Towards building open-source language models for medicine, 2023.
[36] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The Rise and Potential of Large Language Model Based Agents: A Survey, September 2023. arXiv:2309.07864 [cs].
[37] Hui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions, June 2023. arXiv:2306.02224.
[38] Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726, 2023.
[39] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models, May 2023. arXiv:2305.10601 [cs].
[40] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models, March 2023. arXiv:2210.03629.
[41] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models, September 2023. arXiv:2309.01219 [cs].
[42] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. arXiv preprint arXiv:2302.14233, 2023.
[43] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory, June 2023. arXiv:2305.17144 [cs].
[44] Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou. Pad: Program-aided distillation specializes large models in reasoning. arXiv preprint arXiv:2305.13888, 2023.</p>
<h1>A Related Works</h1>
<h2>A. 1 Data-Driven Scientific Discovery</h2>
<p>Data-driven knowledge discovery research within LLM is relatively limited, with the current focus primarily on dataset construction and task-driven design. In this context, [42] proposed a dataset for investigating the transition from goals to discoveries. However, it should be noted that accurate discoveries within this dataset are not recent. [31] introduced a method for automatically collecting and constructing publication data, along with a proposal for a hypothesis generation approach in the natural language processing (NLP) domain. However, this method requires prior human knowledge, explicit context, and is not an automated process. It's worth noting that their data was constructed from literature before 2021 from the ACL collection, implying that the information may already exist in open-source models like chatGPT and LLAMA. Furthermore, [31] focused on integrating computational tools in the field of chemistry, primarily analyzing the capabilities of LLMs in using integrated tools but neglecting the ability for zero-shot generalization in chemistry reactions. [1] delved more into the abilities of LLMs regarding planning and conducting experiments but did not consider proposing new hypotheses. [38] introduced a new task for open-domain hypothesis induction and created a dataset comprising 50 articles from social science journals. Additionally, they developed a multi-module system for exploring feedback mechanisms. However, all of the above-mentioned literature lacks strict guarantees on the visibility of test data to models, thereby limiting our exploration of the zero-shot generalization capability of LLMs through learning from existing knowledge to propose new hypothesis. Unlike existing works, we have designed datasets based on publication dates, which can easily ensure a strict independence between test data and LLMs.</p>
<h2>A. 2 LLM-driven Autonomous Agents</h2>
<p>Large language models demonstrate exceptional capabilities in tasks such as question answering, program coding, and instruction following. However, they still confront significant challenges related to factual hallucination [41, 23], knowledge outdated [3], and interactions with real-world. To address these challenges, recent research has explored enhancing LLMs by incorporating tools such as search engines [18, 19], calculators [24], code interpreter [44], RESTful APIs [27, 21] and others. The integration of LLMs with tool use, also known as LLM-driven autonomous agents (LAAs), has attracted substantial public attention. These agents are equipped with reasoning [33, 39], planning [25, 29], decision-making [37, 14, 5], and long-term memory capabilities [43, 12], and they are constructed upon the foundation of LLMs. LAAs can autonomously plan sub-goals for complex tasks, execute actions, obtain feedback from the environment, and adjust their behaviors to adapt [40, 36, 26]. LAAs have demonstrated significant potential in addressing complex real-world tasks, including software development [22, 11], drama creation [17], course design [6], chemistry experiments [2] and more. Furthermore, multi-agent collaboration plays a significant role in LAA applications, allowing agents to collaborate and interact to solve problems through various roleplaying scenarios [20, 7, 9, 15]. To the best of our knowledge, there is still a dearth of exploration regarding the use of agents, particularly multi-agents, for scientific discovery. In this paper, our objective is to undertake a preliminary effort to enhance the hypothesis proposing capability of LLMs by harnessing tools and multiple agents, along with conducting an analysis of influencing factors.</p>
<h2>B Implementation Details</h2>
<p>In this section, we delve into further implementation details of our experiments, including information about the constructed dataset and open-source models.</p>
<h2>B. 1 Details of Dataset</h2>
<p>Distribution of Training and Test Sets. We present the publication dates and topic distributions of the various datasets for comparison, as illustrated in Figure 5, where we utilize Nomic Atlas ${ }^{7}$ to visualize the topic distribution of abstracts in both the training and test datasets.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Distribution of the background and hypothesis pairs (BHP) dataset: In the left panel, we present the publication distribution by year for the training and seen test datasets, indicating a steady increase year by year until January 2023. In the center panel, we depict the publication distribution by month for the unseen test dataset, which was sampled from August 2023 and emphasizes the latter part of the month. The right panel displays the distribution of keywords in abstracts from the training, seen test, and unseen test datasets, represented by blue, yellow, and red, respectively.</p>
<h1>B. 2 Details of Models</h1>
<p>We present the meta-information of the open-source models used in our experiments, as shown in Table 4. We have gathered data regarding their pre-training, supervised learning corpus, and release dates to ensure the non-visibility of the unseen test data.</p>
<p>Table 4: To further ensure the non-visibility of the test data, we provide an overview of the related literature corpus within the training set of various LLMs, accompanied by their respective publication dates. The data marked with $\left(^{*}\right)$ is the data generated by people talking to ChatGPT. Our date marking is consistent with ChatGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Base Model</th>
<th style="text-align: center;">SFT Data (Y/M)</th>
<th style="text-align: center;">Released</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">API-based</td>
<td style="text-align: center;">gpt-3.5-turbo (0-shot)</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2022/12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-3.5-turbo (5-shot)</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2022/12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-4*</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/06</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Vicuna-33b-v1.3</td>
<td style="text-align: center;">Llama-1</td>
<td style="text-align: center;">ShareGPT (Unknown)</td>
<td style="text-align: center;">2023/06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-7b-chat</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-13b-chat</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Alpaca and ShareGPT (2023/06)</td>
<td style="text-align: center;">2023/07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Alpaca and ShareGPT (2023/06)</td>
<td style="text-align: center;">2023/08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">openchat-v3.2-super</td>
<td style="text-align: center;">Llama-2</td>
<td style="text-align: center;">Sharegpt4 Dataset (2023/06)</td>
<td style="text-align: center;">2023/09</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">MedAlpaca-13B</td>
<td style="text-align: center;">Llama-1*</td>
<td style="text-align: center;">Mixture (2023/03)</td>
<td style="text-align: center;">2023/03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatDoctor*</td>
<td style="text-align: center;">Llama-1*</td>
<td style="text-align: center;">Mixture (2023/04)</td>
<td style="text-align: center;">2023/04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PMC-LLaMA-13B</td>
<td style="text-align: center;">Llama-2*</td>
<td style="text-align: center;">Mixture (2023/04)</td>
<td style="text-align: center;">2023/08*</td>
</tr>
</tbody>
</table>
<h2>C Additional Results</h2>
<p>We have included additional results from human evaluations in Table 5, primarily focusing on correlation scores between word overlap metrics and manual evaluations. Note that we continue to use the same samples used in human evaluation to compute BLEU and ROUGE-L for a fair comparison. We calculate the Pearson and Spearman coefficients between each automatic metric and the average human score. These results reveal that word overlap metrics, such as BLEU and ROUGE-L, exhibit notably lower correlation with manual scores. While BLEU and ROUGE-L may have a high correlation with relevance metrics, they are weak in providing a comprehensive evaluation of the generations. Conversely, evaluations conducted by ChatGPT demonstrate higher correlation with human evaluations, as illustrated in Table 2. However, there is still a significant need to explore advanced metrics, particularly automated ones, in the context of scientific discovery.</p>
<p>Table 5: The table illustrates the correlations between automatic metrics and human evaluations. We annotate the Pearson and Spearman scores after each correlation score, denoting them as $r$ and $\rho$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Word Overlap</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">Human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU $(r / \rho)$</td>
<td style="text-align: center;">ROUGE-L $(r / \rho)$</td>
<td style="text-align: center;">$\operatorname{Avg}(r / \rho)$</td>
<td style="text-align: center;">$\operatorname{Avg}(r / \rho)$</td>
</tr>
<tr>
<td style="text-align: center;">API-based</td>
<td style="text-align: center;">gpt-3.5-turbo(0-shot)</td>
<td style="text-align: center;">16.59(0.03/0.01)</td>
<td style="text-align: center;">29.87(-0.04/-0.05)</td>
<td style="text-align: center;">1.90(0.87/0.78)</td>
<td style="text-align: center;">2.02(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-3.5-turbo(5-shot)*</td>
<td style="text-align: center;">14.99(-0.09/0.12)</td>
<td style="text-align: center;">27.51(-0.33/-0.35)</td>
<td style="text-align: center;">1.96(0.80/0.78)</td>
<td style="text-align: center;">2.15(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Llama-2-70b-chat(0-shot)</td>
<td style="text-align: center;">9.64(-0.21/-0.20)</td>
<td style="text-align: center;">22.17(-0.31/-0.28)</td>
<td style="text-align: center;">2.04(0.89/0.84)</td>
<td style="text-align: center;">1.96(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat(5-shot)</td>
<td style="text-align: center;">9.42(-0.58/-0.65)</td>
<td style="text-align: center;">20.59(-0.47/-0.42)</td>
<td style="text-align: center;">2.20(0.96/0.90)</td>
<td style="text-align: center;">2.33(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-2-70b-chat(5-shot)*</td>
<td style="text-align: center;">9.60(-0.16/-0.10)</td>
<td style="text-align: center;">19.99(-0.15/-0.17)</td>
<td style="text-align: center;">2.01(0.97/0.94)</td>
<td style="text-align: center;">2.08(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0(0-shot)</td>
<td style="text-align: center;">11.42(0.21/0.36)</td>
<td style="text-align: center;">24.11(0.29/0.49)</td>
<td style="text-align: center;">1.91(0.90/0.75)</td>
<td style="text-align: center;">1.81(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WizardLM-70B-V1.0(5-shot)</td>
<td style="text-align: center;">9.86(-0.28/-0.37)</td>
<td style="text-align: center;">23.52(-0.17/-0.24)</td>
<td style="text-align: center;">2.01(0.85/0.89)</td>
<td style="text-align: center;">2.02(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">PMC-LLaMA-13B(0-shot)</td>
<td style="text-align: center;">8.19(0.32/0.39)</td>
<td style="text-align: center;">21.85(0.18/0.27)</td>
<td style="text-align: center;">1.41(0.73/0.73)</td>
<td style="text-align: center;">1.88(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PMC-LLaMA-13B(5-shot)*</td>
<td style="text-align: center;">5.52(0.06/-0.01)</td>
<td style="text-align: center;">13.64(0.26/0.23)</td>
<td style="text-align: center;">1.97(0.95/0.94)</td>
<td style="text-align: center;">1.92(1.00/1.00)</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">21.48(-0.00/0.00)</td>
<td style="text-align: center;">27.83(0.17/0.27)</td>
<td style="text-align: center;">1.79(0.83/0.85)</td>
<td style="text-align: center;">1.77(1.00/1.00)</td>
</tr>
</tbody>
</table>
<h1>D Case Study</h1>
<p>In this section, we present several generated hypotheses from various models and provide examples of the evaluation process, step by step, using ChatGPT.</p>
<h2>D. 1 Generated Hypothesis</h2>
<p>We compare the generated hypotheses of different LLMs selected in human evaluation. The selected medicine literature was published in August 2023 [4], which proposed the power law index as an early marker of myocardial infarction. As shown in Table 6, some responses like gpt-3.5-turbo (0-shot) propose this hypothesis from zero, based only on the background. This indicates that LLMs have great potential in proposing highly novel and verifiable hypotheses and could play a significant role in scientific discovery. Furthermore, we showcase the proposed hypotheses in a multi-agent collaboration involving three roles: Analyst, Scientist, and Critic, working together to generate hypotheses across 6 rounds, as shown in Table 7.</p>
<p>Table 6: This table illustrates the hypothesis proposals from different LLMs.</p>
<h2>InSTRUCTED LLMS CASE</h2>
<h2>Background:</h2>
<p>(1) In the early stage of MI, $\alpha$ short and $\alpha$ long can track and distinguish MI samples from healthy control samples with an AUC $&gt;0.7$.
(2) In the mid-stage and late-stage of MI, E2, E3, and Etotal exhibit excellent discriminating performance with an AUC $&gt;0.6$.
(3) All cardiac tissues display a universal two-stage power-law rheology at different time scales, independent of their disease status.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The sequential order of MI-related changes in cardiac tissues starts from the cytoplasmic level and progresses to the cellular and tissue levels.
(2) Pathological changes at subcellular, cellular, and tissue levels are accompanied by mechanical alterations.
(3) The proposed mechanical markers have diagnostic potential for assessing the progression of MI and studying other cardiovascular diseases.</p>
<h2>gpt-3.5-turbo (0-shot):</h2>
<p>(1) The combination of $\alpha$ short and $\alpha$ long can be used as a reliable biomarker for early detection and differentiation of myocardial infarction (MI) from healthy controls.
(2) E2, E3, and Etotal can serve as effective biomarkers for distinguishing between mid-stage and late-stage MI patients with high accuracy.
(3) The universal two-stage power-law rheology observed in cardiac tissues may play a crucial role in maintaining tissue integrity and function, regardless of the presence or absence of disease.</p>
<h1>gpt-3.5-turbo (5-shot):</h1>
<p>(1) The levels of $\alpha$ short and $\alpha$ long may serve as potential biomarkers for early detection and diagnosis of myocardial infarction (MI).
(2) E2, E3, and Etotal may have utility as biomarkers for assessing the progression and severity of MI in the mid and late stages.
(3) The universal two-stage power-law rheology observed in cardiac tissues suggests a fundamental biomechanical principle underlying tissue behavior, regardless of disease status.</p>
<h2>Llama-2-70b-chat (0-shot):</h2>
<p>(1) E2, E3, and Etotal can also track and distinguish MI samples from healthy control samples in the early stage of MI, with an AUC $&gt;0.7$, similar to $\alpha$ short and $\alpha$ long.
(2) The universal two-stage power-law rheology of cardiac tissues is related to the alteration of the extracellular matrix and the degradation of collagen in the early stage of MI, which can be detected by E2, E3, and Etotal.
(3) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI is affected by the changes in the levels of collagen and proteoglycans in the extracellular matrix, which can be evaluated by measuring the mechanical properties of cardiac tissues using a uniaxial tensiometer.</p>
<h2>Llama-2-70b-chat (5-shot):</h2>
<p>(1) The discriminating performance of $\alpha$ short and $\alpha$ long in the early stage of MI is attributed to their ability to capture the subtle changes in cardiac tissue mechanics caused by the onset of fibrosis.
(2) E2, E3, and Etotal exhibit excellent discriminating performance in the mid-stage and late-stage of MI due to their sensitivity to changes in collagen content and cross-linking, which are hallmarks of fibrosis progression.
(3) The universal two-stage power-law rheology observed in all cardiac tissues, independent of disease status, suggests a common mechanism of mechanical regulation that can be exploited for early disease detection and monitoring.</p>
<h2>Llama-2-70b-chat (5-shot)*:</h2>
<p>(1) The $\alpha$ short and $\alpha$ long parameters can detect changes in myocardial stiffness in the early stages of myocardial infarction (MI).
(2) E2, E3, and Etotal can differentiate between healthy controls and MI samples in the mid and late stages.
(3) The universal two-stage power-law rheology in cardiac tissues may be useful in developing novel therapeutic approaches for MI.</p>
<h2>WizardLM-70B-V1.0 (0-shot):</h2>
<p>(1) $\alpha$ short and $\alpha$ long can track and distinguish MI samples from healthy control samples in all stages of MI with an AUC $&gt;0.7$.
(2) E2, E3, and Etotal exhibit excellent discriminating performance in the early stage of MI with an AUC $&gt;0.6$.
(3) The two-stage power-law rheology observed in all cardiac tissues is related to the progression of MI.</p>
<h2>WizardLM-70B-V1.0 (5-shot):</h2>
<p>(1) The discriminating performance of $\alpha$ short and $\alpha$ long in the early stage of MI may be attributed to their sensitivity to early molecular changes in the myocardium.
(2) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI may be due to their ability to reflect the progression of myocardial damage and remodeling.</p>
<p>(3) The universal two-stage power-law rheology observed in all cardiac tissues may be a fundamental property of cardiac tissue biomechanics, potentially useful for developing diagnostic and therapeutic strategies for MI.</p>
<h1>PMC-LLaMA-13B (0-shot):</h1>
<p>(1) $\alpha$ short and $\alpha$ long are effective biomarkers for distinguishing MI samples from healthy control samples both in the early stage and the late stage of MI.
(2) E2 and E3 (Etotal) are likely to serve as effective biomarkers for discriminating mid-stage or late-stage MI samples from healthy control samples.
(3) Cardiac tissues display a universal two-stage power-law rheology that is independent of their disease status.</p>
<h2>PMC-LLaMA-13B (5-shot):</h2>
<p>(1) Further comprehensive validation of the proposed framework is needed to elucidate the pathological changes in MI patients.
(2) Consideration of multiple markers is important.
(3) The influence of various external factors should be incorporated for accurate interpretation of the mechanical markers.</p>
<h2>WizardLM-13B-V1.2 (SFT):</h2>
<p>(1) The combination of $\alpha$ short and $\alpha$ long features can provide a more robust identification of MI samples.
(2) E2, E3, and Etotal features can serve as independent predictors of MI.
(3) The universal two-stage power-law rheology observed in all cardiac tissues may have diagnostic implications.</p>
<p>Table 7: This table illustrates a case from multi-agent collaboration on hypothesis proposing.</p>
<h2>Multi-Agent Case</h2>
<h2>Background:</h2>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.
(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.
(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.
(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.
(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<h2>Generated Hypothesis:</h2>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.</p>
<p>(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h1>Dialogue process of multi-agent collaboration.</h1>
<p>(Round 1) Analyst: Based on the provided research background, I have identified the following keywords and topics:</p>
<p>Keywords:</p>
<ul>
<li>Digital electrophysiology</li>
<li>Telemonitoring</li>
<li>Cardiac implantable electronic devices (CIEDs)</li>
<li>Wearable devices</li>
<li>Cardiac rhythm monitoring</li>
<li>Photoplethysmography</li>
</ul>
<p>Topics:</p>
<ol>
<li>Advancements in digital electrophysiology and telemonitoring of CIEDs</li>
<li>Benefits of telemonitoring in improving patient outcomes</li>
<li>Accuracy and reliability of wearable devices for cardiac rhythm monitoring using photoplethysmography</li>
</ol>
<p>Preliminary insights:</p>
<ul>
<li>The field of digital electrophysiology has seen significant development in the past 25 years, particularly in the areas of telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring.</li>
<li>Telemonitoring of CIEDs has been shown to have positive impacts, such as reducing the time to detect clinically actionable events and improving patient outcomes.</li>
<li>Wearable devices, including smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</li>
</ul>
<p>Based on these insights, it can be hypothesized that the advancements in telemonitoring and wearable devices have led to improved cardiac rhythm monitoring and patient outcomes. However, further analysis and collaboration with the Scientist and Critic are needed to refine and validate this hypothesis.
(Round 2) Scientist: Based on the insights provided by the Analyst and the research background, I propose the following hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes and more accurate detection of clinically actionable events.</p>
<p>This hypothesis suggests that the development and implementation of telemonitoring technologies for CIEDs, along with the increasing accuracy and reliability of wearable devices using photoplethysmography, have positively impacted patient outcomes by reducing the time to detect clinically actionable events. By continuously monitoring cardiac rhythm, these advancements have the potential to provide timely interventions and improve overall patient care.</p>
<p>The hypothesis is grounded in the existing literature, which highlights the benefits of telemonitoring in improving patient outcomes and the increasing accuracy of wearable devices for cardiac rhythm monitoring. However, further collaboration with the Critic is necessary to evaluate the novelty, coherence, and scientific validity of the hypothesis.</p>
<p>(Round 3) Critic: The hypothesis proposed by the Scientist is well-aligned with the research background and the insights provided by the Analyst. It effectively captures the essence of the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring. The hypothesis also highlights the potential positive impact on patient outcomes and the detection of clinically actionable events.</p>
<p>However, there are a few areas that could be further refined or expanded upon in the hypothesis:</p>
<ol>
<li><strong>Specific outcomes</strong>: The hypothesis mentions ""improved patient outcomes," but it would be beneficial to specify the specific outcomes that are expected to improve. For example, are we referring to reduced hospitalizations, improved quality of life, or decreased mortality rates?</li>
<li>${ }^{<em> </em>}$ Mechanisms**: The hypothesis briefly mentions the potential for timely interventions and improved patient care, but it would be helpful to elaborate on the specific mechanisms through which the advancements in telemonitoring and wearable devices are expected to achieve these outcomes. Are there specific interventions or treatments that can be administered based on the data collected from these devices?</li>
<li>${ }^{<em> </em>}$ Scientific validity**: While the hypothesis is grounded in the existing literature, it would be valuable to provide some references or studies that support the claims made in the hypothesis. This would enhance the scientific validity of the hypothesis and demonstrate that it is not just speculative but based on empirical evidence.</li>
</ol>
<p>To refine the hypothesis, I would recommend the Scientist to consider incorporating these suggestions and providing more specific details and references to support the claims made. This will strengthen the coherence and scientific validity of the hypothesis.</p>
<p>Analyst, based on the feedback provided, please re-evaluate the research background and provide any additional insights or interpretations that could further refine the hypothesis.
(Round 4) Analyst: Based on the feedback from the Critic, I have re-evaluated the research background and identified additional insights:</p>
<ol>
<li><strong>Specific outcomes</strong>: The research background mentions that telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes. These outcomes could include early detection of arrhythmias, reduced hospitalizations due to cardiac events, improved quality of life, and decreased mortality rates. It would be beneficial to specify one or more of these specific outcomes in the hypothesis.</li>
<li>${ }^{<em> </em>}$ Mechanisms**: The advancements in telemonitoring and wearable devices can potentially lead to improved patient outcomes through several mechanisms. For example, continuous monitoring of cardiac rhythm can enable early detection of abnormal rhythms, allowing for timely interventions such as medication adjustments or electrical cardioversion. Additionally, the data collected from these devices can provide valuable insights for personalized treatment plans and remote patient management. Including these specific mechanisms in the hypothesis would enhance its coherence.</li>
<li>${ }^{<em> </em>}$ Scientific validity**: To strengthen the scientific validity of the hypothesis, it would be valuable to reference specific studies or research that support the claims made. For example, citing studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events or improving patient outcomes would provide empirical evidence to support the hypothesis.</li>
</ol>
<p>Incorporating these additional insights and suggestions, the refined hypothesis could be:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis takes into account the specific outcomes, mechanisms, and scientific validity, as suggested by the Critic. It provides a more comprehensive and well-supported statement that captures the essence of the research background and the collaborative insights.
(Round 5) Scientist: Based on the additional insights and suggestions provided by the Analyst and the Critic, I propose the following refined hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis incorporates the specific outcomes mentioned in the research background, such as reduced hospitalizations, improved quality of life, and decreased mortality rates. It also expands on the mechanisms through which these advancements are expected to achieve these outcomes, including early detection of abnormal rhythms, timely interventions, and personalized treatment plans. Additionally, the hypothesis acknowledges the importance of scientific validity by referencing studies that support the claims made.</p>
<p>The refined hypothesis is both innovative and scientifically grounded, as it builds upon the existing literature and incorporates the collaborative insights provided by the Analyst and the Critic. It provides a logical and well-supported statement that can be further evaluated and tested.
(Round 6) Critic: The refined hypothesis proposed by the Scientist is well-structured and addresses the feedback provided by the Critic. It incorporates specific outcomes, mechanisms, and scientific validity, making it a comprehensive and scientifically robust statement.</p>
<p>Final Answer:
(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.
(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h1>D. 2 Evaluation Examples</h1>
<p>We provide evaluation results conducted by ChatGPT in Table 8. ChatGPT offers a step-by-step explanation for each metric, ultimately providing independent scores in integer format.</p>
<p>Table 8: This table illustrates a case for hypothesis evaluation using ChatGPT.</p>
<h2>CASE OF ChatGPT as ANNOTATOR</h2>
<h2>Background:</h2>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.
(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.
(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<h2>Golden Hypothesis:</h2>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.
(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.
(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<h2>Generated Hypothesis:</h2>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.
(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.
(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement. It can now be further evaluated and tested in future research.</p>
<h2>Evaluation Results</h2>
<p>Novelty:
Explanation: The generated scientific hypothesis builds upon the background information by stating that the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes. It further suggests that these advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans. This hypothesis goes beyond the background by proposing specific mechanisms through which the advancements in telemonitoring and wearable devices contribute to improved patient outcomes. It also mentions the supporting evidence from studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes. Overall, the hypothesis provides new insights and expands upon the existing knowledge in the field.
Score: 2</p>
<h2>Relevance:</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/nomic-ai/nomic&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
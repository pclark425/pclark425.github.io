<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-982 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-982</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-982</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-ced1c3a3775959903b9a47e5276264c57933ea69</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ced1c3a3775959903b9a47e5276264c57933ea69" target="_blank">Invariant Causal Imitation Learning for Generalizable Policies</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Invariant Causal Imitation Learning (ICIL), a novel technique in which a feature representation that is invariant across domains is learned, is proposed on the basis of which an imitation policy is learned that matches expert behavior.</p>
                <p><strong>Paper Abstract:</strong> Consider learning an imitation policy on the basis of demonstrated behavior from multiple environments, with an eye towards deployment in an unseen environment. Since the observable features from each setting may be different, directly learning individual policies as mappings from features to actions is prone to spurious correlations -- and may not generalize well. However, the expert's policy is often a function of a shared latent structure underlying those observable features that is invariant across settings. By leveraging data from multiple environments, we propose Invariant Causal Imitation Learning (ICIL), a novel technique in which we learn a feature representation that is invariant across domains, on the basis of which we learn an imitation policy that matches expert behavior. To cope with transition dynamics mismatch, ICIL learns a shared representation of causal features (for all training environments), that is disentangled from the specific representations of noise variables (for each of those environments). Moreover, to ensure that the learned policy matches the observation distribution of the expert's policy, ICIL estimates the energy of the expert's observations and uses a regularization term that minimizes the imitator policy's next state energy. Experimentally, we compare our methods against several benchmarks in control and healthcare tasks and show its effectiveness in learning imitation policies capable of generalizing to unseen environments.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e982.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e982.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Causal Imitation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch imitation-learning method that decomposes observations into an invariant causal state and environment-specific noise, enforces independence between them, preserves dynamics via learned transition models, and regularizes the imitator to stay in the expert's support using an energy-based model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Imitation Learning (ICIL)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ICIL learns a shared encoder phi(x; theta_s) that maps observations to an invariant causal state s and environment-specific encoders mu^e(x; theta_eta^e) that map to noise variables eta^e. It enforces invariance of s across training environments by adversarially maximizing the entropy of an environment classifier on s, enforces dynamics-preservation by learning state and noise transition models g_s and g_eta^e and a decoder psi to reconstruct next observations, and enforces marginal independence between s and eta^e by minimizing their mutual information using MINE. To keep the learned policy within the expert occupancy, ICIL trains an energy-based model (EBM) of expert observations (via persistent contrastive divergence / Langevin sampling) and adds a regularizer that minimizes the energy of the next-state that would result from following the imitator policy (i.e., penalize high-energy/out-of-support next states). The imitation policy is trained by behaviour cloning on s plus the energy-based next-state penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline multi-environment benchmarks: OpenAI Gym control tasks (Acrobot, CartPole, LunarLander, BeamRider) and MIMIC-III ICU trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Strictly offline / batch setting with demonstrations collected from multiple training environments (constructed by adding spurious noise or domain shifts); environments are non-interactive for the learner at training time (no online experimentation), but are standard RL control simulators or static observational healthcare trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Representation disentanglement + invariance: (1) explicit factorization into invariant state s and environment-specific noise eta^e; (2) adversarial domain-invariance on s (maximize env-classifier entropy); (3) minimize mutual information between s and eta^e via MINE to remove residual dependence; (4) reconstructive/dynamics losses to ensure s contains causal dynamics and eta^e captures nuisances; (5) energy-based next-state penalty to avoid out-of-support (potentially spurious) behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant / environment-specific noise variables (distractors), selection bias manifested as spurious correlations between observed features and expert actions, dynamics mismatch of nuisance variables across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Dependence between candidate state and noise is measured via mutual information estimated by MINE; environment predictability of s is measured with an environment classifier (high predictability indicates non-invariance); out-of-support next states are detected as high energy under an EBM trained on expert observations.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>No explicit per-variable weight; influence of spurious signals is reduced by (a) projecting inputs into s and eta and minimizing MI (removing shared signal), (b) adversarially forcing s to be non-predictive of environment, and (c) penalizing high-energy next states so the policy avoids transitions that would rely on spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Implicit refutation via invariant representation learning: if a candidate feature is spuriously predictive, it will either be captured in eta^e (and thus not used by policy) or be removed from s by the invariance and MI penalties; additionally, next-state high-energy penalties discourage policies that rely on spurious cues that produce out-of-support transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Consistently outperforms baseline imitation methods on domain-generalization tests in OpenAI Gym (plots show higher normalized returns across varying numbers of trajectories) and substantially outperforms baselines on MIMIC-III action-matching: ACC = 0.855 ± 0.003, AUC = 0.856 ± 0.004, APR = 0.789 ± 0.004.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines (example on MIMIC-III): BC ACC = 0.783 ± 0.001, AUC = 0.762 ± 0.002, APR = 0.692 ± 0.001; EDM ACC = 0.786 ± 0.003, AUC = 0.741 ± 0.011, APR = 0.682 ± 0.005. ICIL shows a marked improvement over these non-robust methods.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Experiments used multiple settings (control tasks: 3 noise variables by default; robustness ablation used 3, 6, 9, 12; MIMIC augmentation used 20 noise variables), i.e., '3-12 (control), 20 (MIMIC)'.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposing observations into invariant causal features and environment-specific noise, enforcing independence via MINE, and enforcing invariance via adversarial entropy are critical to discard spurious correlations and generalize to unseen domains; adding an EBM-based next-state energy penalty further reduces compounding error in the strictly batch setting. Ablations show the invariance loss is the single most important component; ICIL remains robust as the number of noise variables increases while BC and EDM degrade.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e982.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e982.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRM-v1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization (practical IRM-v1 objective)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-domain representation-learning principle that seeks representations admitting a classifier that is simultaneously optimal across environments; IRM-v1 is a practical proxy objective used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant risk minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IRM-v1 (Invariant Risk Minimization - practical penalty)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IRM proposes to find a representation Phi such that there exists a classifier w that is optimal for all environments; IRM-v1 uses a penalty term that enforces the gradient of the risk w.r.t. classifier parameters to be small across environments (practical surrogate) and is appended to the training objective to bias toward invariant predictors that avoid using environment-specific spurious correlates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied here as an augmentation to strictly-batch imitation baselines evaluated on OpenAI Gym control tasks and MIMIC-III</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same strictly offline demonstration datasets from multiple training environments; not interactive for the learner during training.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Invariant predictor enforcement across multiple environments via gradient-based penalty (IRM-v1); encourages learned representation to ignore features whose predictive power varies across environments (i.e., spurious features).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-specific spurious correlates / distribution shifts across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Not explicit detection; uses cross-environment risk/gradient inconsistency as signal that a feature is non-invariant.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit: penalizes predictors that rely on features producing varying gradients across environments, thereby discouraging use of spurious features.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By requiring a single classifier to be optimal across environments, IRM attempts to rule out predictors that depend on environment-specific associations; no explicit intervention/refutation step in the implementation used here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When used to augment several batch-imitation algorithms (BC-IRM, RCAL-IRM, VDICE-IRM, EDM-IRM), IRM-v1 generally did not improve generalization and often produced more unstable training in the authors' experiments; small numeric differences: BC-IRM ACC on MIMIC = 0.791 vs BC = 0.783 (slight increase), but overall IRM-augmented baselines remained inferior to ICIL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline behaviour cloning and other imitation baselines without IRM: e.g., BC ACC = 0.783 ± 0.001, VDICE ACC = 0.794 ± 0.001, EDM ACC = 0.786 ± 0.003 (MIMIC-III).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Directly applying IRM-v1 to batch imitation objectives in this sequential setting was insufficient: it did not reliably yield policies that generalize across environments and led to unstable training when combined with standard batch imitation algorithms in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e982.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e982.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mutual Information Neural Estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network-based estimator of mutual information used to measure and minimize dependence between learned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mutual information neural estimation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MINE (Mutual Information Neural Estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>MINE uses a parametrized statistics network T_theta to form a variational lower bound on mutual information I(U;V) via the Donsker-Varadhan representation: I >= E_joint[T] - log(E_prod[e^{T}]). The parameters of T are trained to maximize this lower bound; here, I(phi(x), mu(x)) is estimated and minimized (by gradient descent on encoders, gradient ascent on T) to encourage independence of invariant state s and noise eta^e.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied inside ICIL on offline datasets (OpenAI Gym tasks and MIMIC trajectories) to enforce representation independence</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Strictly batch, multi-environment observational datasets; no active experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Dependence estimation and removal: MINE detects dependence between s and eta and is used as a differentiable loss to drive representations apart (minimize MI), thereby assigning spurious variation to eta instead of s.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant/noise variables that correlate with actions or with causal features (spurious associations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Estimates mutual information between candidate invariant representation and noise representation via a trained statistics network; high estimated MI indicates spurious/shared signal.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not a reweighting per se — by minimizing the MI estimate the encoder parameters are updated to remove shared information (effectively downweighting spurious information in s).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Reduces/eliminates statistical dependence between s and eta to refute that candidate features in s are spuriously related; this is an implicit refutation via representation disentanglement rather than explicit hypothesis testing.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Contributes to ICIL's improved generalization; ablations show removing the MI loss degrades test performance (CartPole ablation reported). No standalone performance numbers reported for MINE alone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MINE provides a practical, differentiable means to detect and minimize dependence between putative causal features and nuisance variables in representation learning; in ICIL minimizing MI is essential to ensure the invariant representation is not contaminated by environment-specific spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e982.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e982.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EBM / EDM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Energy-Based Model / Energy-based Distribution Matching (EDM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An energy-based density model of expert observations used either as a standalone distribution-matching imitation method (EDM) or inside ICIL to estimate expert occupancy and penalize out-of-support next states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Strictly batch imitation learning by energy-based distribution matching</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Energy-Based Model (EBM) / Energy-based Distribution Matching (EDM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>EBMs parametrize an unnormalized density rho_D(x) proportional to exp(-E_theta(x)); training uses contrastive divergence / persistent CD with Langevin dynamics to push down energy on expert samples and up on model samples. EDM jointly learns an EBM of state distributions and a policy to match expert occupancy in the offline setting. In ICIL, an EBM trained on expert observations is used to compute energy of predicted next observations and the imitator is penalized by the next-state energy.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used on offline OpenAI Gym tasks and MIMIC-III data; EDM evaluated as a baseline and EBM used inside ICIL</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline demonstrations; no interactive sampling for training the policy (but EBM sampling uses Langevin MCMC internally).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Support-based regularization: by modeling expert state density, high-energy (low-density) states—often arising from policies exploiting spurious cues—are penalized, indirectly discouraging behaviors that rely on spurious correlations that lead out-of-support.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Out-of-support transitions caused by policies depending on spurious features; distributional shift relative to expert occupancy.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>High energy under the EBM indicates an observation not supported by expert data (i.e., potentially arising from spurious-driven behavior). EBM is trained via persistent contrastive divergence and samples via Langevin dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Policy objective includes a term minimizing expected next-state energy (lower energy = higher reward); this downweights policies that would produce out-of-support (spurious-driven) next states.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>No explicit refutation of a causal hypothesis; refutation is implicit by penalizing behavior that leads to improbable states under the expert distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>As a baseline on MIMIC-III: EDM ACC = 0.786 ± 0.003, AUC = 0.741 ± 0.011, APR = 0.682 ± 0.005; in ICIL the EBM-based next-state penalty improved robustness when combined with invariant representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>EDM-IRM (EDM augmented with IRMv1) performed worse on some metrics (EDM-IRM AUC = 0.717 ± 0.015) indicating IRM augmentation did not help EDM in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>EBMs provide a practical way to estimate expert occupancy in the strictly offline setting and to penalize out-of-support next states; when combined with invariant representation learning (as in ICIL) they help reduce compounding errors and improve generalization, whereas using them alone (EDM baseline) is insufficient to handle environment-specific spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e982.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e982.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial domain classifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial environment classifier (domain-invariance via classifier entropy maximization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that enforces learned representation invariance across environments by adversarial training: a classifier predicts environment from representation and the encoder is trained to maximize the classifier's entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial discriminative domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Adversarial environment classifier (entropy maximization)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train an environment classifier c_s on the shared representation s to predict environment label; to make s invariant, ICIL maximizes the entropy of c_s(phi(x)) (adversarial objective) so that s contains no environment-identifying information. Practically implemented by alternating updates: classifier trained to predict env, encoder trained to maximize classifier entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied inside ICIL on offline multi-environment datasets (control tasks and MIMIC-III)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch demonstrations from multiple training environments with different interventions on nuisance variables; learner cannot interact with environments during training.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Domain-invariance enforcement: features informative about environment (likely nuisance/distractor features) are discouraged from appearing in invariant state representation by adversarial entropy maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-specific nuisance variables and spurious correlations that vary across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>If the environment classifier can predict environment from s (low entropy), then s contains environment-specific information—this detects remaining spurious signal.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Encoder is trained to maximize classifier entropy, thereby reducing representation sensitivity to environment-specific distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>No formal statistical refutation; invariance enforcement operationally removes environment-specific predictors from s.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Part of ICIL's overall gains; ablations removing invariance loss strongly degrade performance, indicating this adversarial invariance component is crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adversarial invariance on the state representation is essential to separate causal features from environment-specific distractors; the invariance loss was the most critical single component in ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e982.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e982.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal confusion (de Haan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal confusion in imitation learning (de Haan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work highlighting that policies learned by imitation can latch onto spurious correlates (causal confusion) and advocating conditioning on causal parents to avoid learning spurious associations, typically requiring interventions or queries to the expert.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal confusion in imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal-parent conditioning to avoid causal confusion</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Proposes identifying and conditioning on causal parents of actions to avoid policies exploiting spurious correlates; typically requires ability to query/exert interventions on the environment or access to additional experiments, which is not available in a strictly batch setting.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Single-environment imitation-learning settings (original work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Often assumes access to the ability to intervene or query the expert / environment during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection to identify causal parents (often via interventions or active queries) and conditioning policy on those parents.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlates that induce causal confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Interventions / querying expert to determine causal parents and rule out spurious correlates.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Exclude non-causal features from policy conditioning (i.e., variable selection).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Use interventions/queries to refute that a feature is causal if the predictive relationship breaks under intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Interventions or querying the demonstrator to resolve causal ambiguity (not applicable in batch-only setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Identifying causal parents prevents imitation policies from exploiting spurious correlates, but methods often require interventions or queries which are infeasible in a strictly offline/batch imitation setting; ICIL addresses this by using multiple passive environments instead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e982.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e982.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invariant causal prediction for block MDPs (Zhang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant causal prediction for block MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent work that learns representations shared across domains in block MDPs to identify causal ancestors of the reward, improving RL generalization across environments with differing nuisances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant causal prediction for block mdps</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant causal prediction in block MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Learns a representation shared across domains such that a predictor (for reward or value) is invariant, using multiple domains produced by interventions; focuses on reward-based RL and identification of causal ancestors of reward.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Block MDPs (simulated RL environments with distinct observation nuisances across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive RL environments with access to reward signals and the ability to collect rollouts; not restricted to strictly offline/batch data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Invariant representation learning across domains to separate causal state from nuisance factors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Observation nuisances that vary across domains, irrelevant for reward/action decision.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Multi-domain consistency of predictors; invariance across interventions indicates causal features.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Representation learning that allocates nuisance variation to non-invariant part, reducing influence on learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Uses cross-domain invariance criteria; explicit interventions in generating domains support refutation of spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows that multi-domain invariance can identify causal state factors in block MDPs and improve RL generalization when reward is available; referenced as related work and a conceptual precursor to ICIL's multi-environment invariance approach but not directly applicable to strictly offline imitation without rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e982.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e982.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Resolving spurious correlations via interventions (Volodin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Resolving spurious correlations in causal models of environments via interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A work proposing interventions in environments to resolve spurious correlations in causal models and thereby improve robustness of learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Resolving spurious correlations in causal models of environments via interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Intervention-based resolution of spurious correlations</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses explicit interventions on environment variables to break spurious associations and identify causal relationships, enabling causal-model-based corrections in learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated environments where interventions can be executed</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive/experimental settings where one can intervene to change variables and observe causal effects; not a strictly batch-only setting.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Active interventions to break spurious associations and identify causal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations induced by environment-specific mechanisms or confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Observe changes in conditional distributions under interventions; lack of invariance indicates spuriousness.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not applicable directly — interventions are used to identify and remove spurious edges or reweight model factors post-intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Interventions that change the generating mechanism of a variable can refute spurious causal links when predictive relationships disappear.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Select and perform interventions designed to alter suspected nuisance variables and observe resulting changes in downstream distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interventions are a principled way to resolve spurious correlations in environment models, but require interactive access to environments; cited by the authors as part of the broader literature on causality and spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e982.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e982.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal transfer (Etesami & Geiger)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal transfer for imitation learning and decision making under sensor-shift</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method addressing sensor-shift (distributional sensor changes) and hidden confounding in imitation/decision making by learning transfers robust to such shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal transfer for imitation learning and decision making under sensor-shift</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal transfer under sensor-shift</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Addresses hidden confounding and sensor distribution shift by leveraging causal assumptions/transfer techniques to produce policies robust to changes in sensor distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Settings with sensor shifts and potential hidden confounders (method paper context)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Typically offline or partially observable settings where sensors change between environments; may involve simulated or logged data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Causal transfer techniques; methods to account for sensor shifts and hidden confounders (details in original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounding, sensor-induced spurious associations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Not detailed in this ICIL paper; original work addresses detection/adjustment via causal-transfer methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Approaches to mitigate sensor-shift effects (original paper required for specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Methodology aims to identify and correct for sensor-shift/confounding rather than purely refute via interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work tackling hidden confounding / sensor-shift in imitation and decision making; emphasizes differences from ICIL which assumes no hidden confounders and uses multiple passive training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Causal Imitation Learning for Generalizable Policies', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Invariant risk minimization <em>(Rating: 2)</em></li>
                <li>Invariant causal prediction for block mdps <em>(Rating: 2)</em></li>
                <li>Causal confusion in imitation learning <em>(Rating: 2)</em></li>
                <li>Mutual information neural estimation <em>(Rating: 2)</em></li>
                <li>Strictly batch imitation learning by energy-based distribution matching <em>(Rating: 2)</em></li>
                <li>Resolving spurious correlations in causal models of environments via interventions <em>(Rating: 1)</em></li>
                <li>Causal transfer for imitation learning and decision making under sensor-shift <em>(Rating: 1)</em></li>
                <li>Adversarial discriminative domain adaptation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-982",
    "paper_id": "paper-ced1c3a3775959903b9a47e5276264c57933ea69",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "ICIL",
            "name_full": "Invariant Causal Imitation Learning",
            "brief_description": "A batch imitation-learning method that decomposes observations into an invariant causal state and environment-specific noise, enforces independence between them, preserves dynamics via learned transition models, and regularizes the imitator to stay in the expert's support using an energy-based model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Invariant Causal Imitation Learning (ICIL)",
            "method_description": "ICIL learns a shared encoder phi(x; theta_s) that maps observations to an invariant causal state s and environment-specific encoders mu^e(x; theta_eta^e) that map to noise variables eta^e. It enforces invariance of s across training environments by adversarially maximizing the entropy of an environment classifier on s, enforces dynamics-preservation by learning state and noise transition models g_s and g_eta^e and a decoder psi to reconstruct next observations, and enforces marginal independence between s and eta^e by minimizing their mutual information using MINE. To keep the learned policy within the expert occupancy, ICIL trains an energy-based model (EBM) of expert observations (via persistent contrastive divergence / Langevin sampling) and adds a regularizer that minimizes the energy of the next-state that would result from following the imitator policy (i.e., penalize high-energy/out-of-support next states). The imitation policy is trained by behaviour cloning on s plus the energy-based next-state penalty.",
            "environment_name": "Offline multi-environment benchmarks: OpenAI Gym control tasks (Acrobot, CartPole, LunarLander, BeamRider) and MIMIC-III ICU trajectories",
            "environment_description": "Strictly offline / batch setting with demonstrations collected from multiple training environments (constructed by adding spurious noise or domain shifts); environments are non-interactive for the learner at training time (no online experimentation), but are standard RL control simulators or static observational healthcare trajectories.",
            "handles_distractors": true,
            "distractor_handling_technique": "Representation disentanglement + invariance: (1) explicit factorization into invariant state s and environment-specific noise eta^e; (2) adversarial domain-invariance on s (maximize env-classifier entropy); (3) minimize mutual information between s and eta^e via MINE to remove residual dependence; (4) reconstructive/dynamics losses to ensure s contains causal dynamics and eta^e captures nuisances; (5) energy-based next-state penalty to avoid out-of-support (potentially spurious) behavior.",
            "spurious_signal_types": "Irrelevant / environment-specific noise variables (distractors), selection bias manifested as spurious correlations between observed features and expert actions, dynamics mismatch of nuisance variables across environments.",
            "detection_method": "Dependence between candidate state and noise is measured via mutual information estimated by MINE; environment predictability of s is measured with an environment classifier (high predictability indicates non-invariance); out-of-support next states are detected as high energy under an EBM trained on expert observations.",
            "downweighting_method": "No explicit per-variable weight; influence of spurious signals is reduced by (a) projecting inputs into s and eta and minimizing MI (removing shared signal), (b) adversarially forcing s to be non-predictive of environment, and (c) penalizing high-energy next states so the policy avoids transitions that would rely on spurious correlations.",
            "refutation_method": "Implicit refutation via invariant representation learning: if a candidate feature is spuriously predictive, it will either be captured in eta^e (and thus not used by policy) or be removed from s by the invariance and MI penalties; additionally, next-state high-energy penalties discourage policies that rely on spurious cues that produce out-of-support transitions.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Consistently outperforms baseline imitation methods on domain-generalization tests in OpenAI Gym (plots show higher normalized returns across varying numbers of trajectories) and substantially outperforms baselines on MIMIC-III action-matching: ACC = 0.855 ± 0.003, AUC = 0.856 ± 0.004, APR = 0.789 ± 0.004.",
            "performance_without_robustness": "Baselines (example on MIMIC-III): BC ACC = 0.783 ± 0.001, AUC = 0.762 ± 0.002, APR = 0.692 ± 0.001; EDM ACC = 0.786 ± 0.003, AUC = 0.741 ± 0.011, APR = 0.682 ± 0.005. ICIL shows a marked improvement over these non-robust methods.",
            "has_ablation_study": true,
            "number_of_distractors": "Experiments used multiple settings (control tasks: 3 noise variables by default; robustness ablation used 3, 6, 9, 12; MIMIC augmentation used 20 noise variables), i.e., '3-12 (control), 20 (MIMIC)'.",
            "key_findings": "Decomposing observations into invariant causal features and environment-specific noise, enforcing independence via MINE, and enforcing invariance via adversarial entropy are critical to discard spurious correlations and generalize to unseen domains; adding an EBM-based next-state energy penalty further reduces compounding error in the strictly batch setting. Ablations show the invariance loss is the single most important component; ICIL remains robust as the number of noise variables increases while BC and EDM degrade.",
            "uuid": "e982.0",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "IRM-v1",
            "name_full": "Invariant Risk Minimization (practical IRM-v1 objective)",
            "brief_description": "A multi-domain representation-learning principle that seeks representations admitting a classifier that is simultaneously optimal across environments; IRM-v1 is a practical proxy objective used in experiments.",
            "citation_title": "Invariant risk minimization",
            "mention_or_use": "use",
            "method_name": "IRM-v1 (Invariant Risk Minimization - practical penalty)",
            "method_description": "IRM proposes to find a representation Phi such that there exists a classifier w that is optimal for all environments; IRM-v1 uses a penalty term that enforces the gradient of the risk w.r.t. classifier parameters to be small across environments (practical surrogate) and is appended to the training objective to bias toward invariant predictors that avoid using environment-specific spurious correlates.",
            "environment_name": "Applied here as an augmentation to strictly-batch imitation baselines evaluated on OpenAI Gym control tasks and MIMIC-III",
            "environment_description": "Same strictly offline demonstration datasets from multiple training environments; not interactive for the learner during training.",
            "handles_distractors": true,
            "distractor_handling_technique": "Invariant predictor enforcement across multiple environments via gradient-based penalty (IRM-v1); encourages learned representation to ignore features whose predictive power varies across environments (i.e., spurious features).",
            "spurious_signal_types": "Environment-specific spurious correlates / distribution shifts across domains.",
            "detection_method": "Not explicit detection; uses cross-environment risk/gradient inconsistency as signal that a feature is non-invariant.",
            "downweighting_method": "Implicit: penalizes predictors that rely on features producing varying gradients across environments, thereby discouraging use of spurious features.",
            "refutation_method": "By requiring a single classifier to be optimal across environments, IRM attempts to rule out predictors that depend on environment-specific associations; no explicit intervention/refutation step in the implementation used here.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When used to augment several batch-imitation algorithms (BC-IRM, RCAL-IRM, VDICE-IRM, EDM-IRM), IRM-v1 generally did not improve generalization and often produced more unstable training in the authors' experiments; small numeric differences: BC-IRM ACC on MIMIC = 0.791 vs BC = 0.783 (slight increase), but overall IRM-augmented baselines remained inferior to ICIL.",
            "performance_without_robustness": "Baseline behaviour cloning and other imitation baselines without IRM: e.g., BC ACC = 0.783 ± 0.001, VDICE ACC = 0.794 ± 0.001, EDM ACC = 0.786 ± 0.003 (MIMIC-III).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Directly applying IRM-v1 to batch imitation objectives in this sequential setting was insufficient: it did not reliably yield policies that generalize across environments and led to unstable training when combined with standard batch imitation algorithms in these experiments.",
            "uuid": "e982.1",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MINE",
            "name_full": "Mutual Information Neural Estimation",
            "brief_description": "A neural-network-based estimator of mutual information used to measure and minimize dependence between learned representations.",
            "citation_title": "Mutual information neural estimation",
            "mention_or_use": "use",
            "method_name": "MINE (Mutual Information Neural Estimation)",
            "method_description": "MINE uses a parametrized statistics network T_theta to form a variational lower bound on mutual information I(U;V) via the Donsker-Varadhan representation: I &gt;= E_joint[T] - log(E_prod[e^{T}]). The parameters of T are trained to maximize this lower bound; here, I(phi(x), mu(x)) is estimated and minimized (by gradient descent on encoders, gradient ascent on T) to encourage independence of invariant state s and noise eta^e.",
            "environment_name": "Applied inside ICIL on offline datasets (OpenAI Gym tasks and MIMIC trajectories) to enforce representation independence",
            "environment_description": "Strictly batch, multi-environment observational datasets; no active experimentation.",
            "handles_distractors": true,
            "distractor_handling_technique": "Dependence estimation and removal: MINE detects dependence between s and eta and is used as a differentiable loss to drive representations apart (minimize MI), thereby assigning spurious variation to eta instead of s.",
            "spurious_signal_types": "Irrelevant/noise variables that correlate with actions or with causal features (spurious associations).",
            "detection_method": "Estimates mutual information between candidate invariant representation and noise representation via a trained statistics network; high estimated MI indicates spurious/shared signal.",
            "downweighting_method": "Not a reweighting per se — by minimizing the MI estimate the encoder parameters are updated to remove shared information (effectively downweighting spurious information in s).",
            "refutation_method": "Reduces/eliminates statistical dependence between s and eta to refute that candidate features in s are spuriously related; this is an implicit refutation via representation disentanglement rather than explicit hypothesis testing.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Contributes to ICIL's improved generalization; ablations show removing the MI loss degrades test performance (CartPole ablation reported). No standalone performance numbers reported for MINE alone.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "MINE provides a practical, differentiable means to detect and minimize dependence between putative causal features and nuisance variables in representation learning; in ICIL minimizing MI is essential to ensure the invariant representation is not contaminated by environment-specific spurious signals.",
            "uuid": "e982.2",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "EBM / EDM",
            "name_full": "Energy-Based Model / Energy-based Distribution Matching (EDM)",
            "brief_description": "An energy-based density model of expert observations used either as a standalone distribution-matching imitation method (EDM) or inside ICIL to estimate expert occupancy and penalize out-of-support next states.",
            "citation_title": "Strictly batch imitation learning by energy-based distribution matching",
            "mention_or_use": "use",
            "method_name": "Energy-Based Model (EBM) / Energy-based Distribution Matching (EDM)",
            "method_description": "EBMs parametrize an unnormalized density rho_D(x) proportional to exp(-E_theta(x)); training uses contrastive divergence / persistent CD with Langevin dynamics to push down energy on expert samples and up on model samples. EDM jointly learns an EBM of state distributions and a policy to match expert occupancy in the offline setting. In ICIL, an EBM trained on expert observations is used to compute energy of predicted next observations and the imitator is penalized by the next-state energy.",
            "environment_name": "Used on offline OpenAI Gym tasks and MIMIC-III data; EDM evaluated as a baseline and EBM used inside ICIL",
            "environment_description": "Offline demonstrations; no interactive sampling for training the policy (but EBM sampling uses Langevin MCMC internally).",
            "handles_distractors": true,
            "distractor_handling_technique": "Support-based regularization: by modeling expert state density, high-energy (low-density) states—often arising from policies exploiting spurious cues—are penalized, indirectly discouraging behaviors that rely on spurious correlations that lead out-of-support.",
            "spurious_signal_types": "Out-of-support transitions caused by policies depending on spurious features; distributional shift relative to expert occupancy.",
            "detection_method": "High energy under the EBM indicates an observation not supported by expert data (i.e., potentially arising from spurious-driven behavior). EBM is trained via persistent contrastive divergence and samples via Langevin dynamics.",
            "downweighting_method": "Policy objective includes a term minimizing expected next-state energy (lower energy = higher reward); this downweights policies that would produce out-of-support (spurious-driven) next states.",
            "refutation_method": "No explicit refutation of a causal hypothesis; refutation is implicit by penalizing behavior that leads to improbable states under the expert distribution.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "As a baseline on MIMIC-III: EDM ACC = 0.786 ± 0.003, AUC = 0.741 ± 0.011, APR = 0.682 ± 0.005; in ICIL the EBM-based next-state penalty improved robustness when combined with invariant representation learning.",
            "performance_without_robustness": "EDM-IRM (EDM augmented with IRMv1) performed worse on some metrics (EDM-IRM AUC = 0.717 ± 0.015) indicating IRM augmentation did not help EDM in these experiments.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "EBMs provide a practical way to estimate expert occupancy in the strictly offline setting and to penalize out-of-support next states; when combined with invariant representation learning (as in ICIL) they help reduce compounding errors and improve generalization, whereas using them alone (EDM baseline) is insufficient to handle environment-specific spurious correlations.",
            "uuid": "e982.3",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Adversarial domain classifier",
            "name_full": "Adversarial environment classifier (domain-invariance via classifier entropy maximization)",
            "brief_description": "An approach that enforces learned representation invariance across environments by adversarial training: a classifier predicts environment from representation and the encoder is trained to maximize the classifier's entropy.",
            "citation_title": "Adversarial discriminative domain adaptation",
            "mention_or_use": "use",
            "method_name": "Adversarial environment classifier (entropy maximization)",
            "method_description": "Train an environment classifier c_s on the shared representation s to predict environment label; to make s invariant, ICIL maximizes the entropy of c_s(phi(x)) (adversarial objective) so that s contains no environment-identifying information. Practically implemented by alternating updates: classifier trained to predict env, encoder trained to maximize classifier entropy.",
            "environment_name": "Applied inside ICIL on offline multi-environment datasets (control tasks and MIMIC-III)",
            "environment_description": "Batch demonstrations from multiple training environments with different interventions on nuisance variables; learner cannot interact with environments during training.",
            "handles_distractors": true,
            "distractor_handling_technique": "Domain-invariance enforcement: features informative about environment (likely nuisance/distractor features) are discouraged from appearing in invariant state representation by adversarial entropy maximization.",
            "spurious_signal_types": "Environment-specific nuisance variables and spurious correlations that vary across domains.",
            "detection_method": "If the environment classifier can predict environment from s (low entropy), then s contains environment-specific information—this detects remaining spurious signal.",
            "downweighting_method": "Encoder is trained to maximize classifier entropy, thereby reducing representation sensitivity to environment-specific distractors.",
            "refutation_method": "No formal statistical refutation; invariance enforcement operationally removes environment-specific predictors from s.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Part of ICIL's overall gains; ablations removing invariance loss strongly degrade performance, indicating this adversarial invariance component is crucial.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Adversarial invariance on the state representation is essential to separate causal features from environment-specific distractors; the invariance loss was the most critical single component in ablation studies.",
            "uuid": "e982.4",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Causal confusion (de Haan et al.)",
            "name_full": "Causal confusion in imitation learning (de Haan et al.)",
            "brief_description": "Work highlighting that policies learned by imitation can latch onto spurious correlates (causal confusion) and advocating conditioning on causal parents to avoid learning spurious associations, typically requiring interventions or queries to the expert.",
            "citation_title": "Causal confusion in imitation learning",
            "mention_or_use": "mention",
            "method_name": "Causal-parent conditioning to avoid causal confusion",
            "method_description": "Proposes identifying and conditioning on causal parents of actions to avoid policies exploiting spurious correlates; typically requires ability to query/exert interventions on the environment or access to additional experiments, which is not available in a strictly batch setting.",
            "environment_name": "Single-environment imitation-learning settings (original work)",
            "environment_description": "Often assumes access to the ability to intervene or query the expert / environment during learning.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection to identify causal parents (often via interventions or active queries) and conditioning policy on those parents.",
            "spurious_signal_types": "Spurious correlates that induce causal confusion.",
            "detection_method": "Interventions / querying expert to determine causal parents and rule out spurious correlates.",
            "downweighting_method": "Exclude non-causal features from policy conditioning (i.e., variable selection).",
            "refutation_method": "Use interventions/queries to refute that a feature is causal if the predictive relationship breaks under intervention.",
            "uses_active_learning": true,
            "inquiry_strategy": "Interventions or querying the demonstrator to resolve causal ambiguity (not applicable in batch-only setting).",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Identifying causal parents prevents imitation policies from exploiting spurious correlates, but methods often require interventions or queries which are infeasible in a strictly offline/batch imitation setting; ICIL addresses this by using multiple passive environments instead.",
            "uuid": "e982.5",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Invariant causal prediction for block MDPs (Zhang et al.)",
            "name_full": "Invariant causal prediction for block MDPs",
            "brief_description": "A recent work that learns representations shared across domains in block MDPs to identify causal ancestors of the reward, improving RL generalization across environments with differing nuisances.",
            "citation_title": "Invariant causal prediction for block mdps",
            "mention_or_use": "mention",
            "method_name": "Invariant causal prediction in block MDPs",
            "method_description": "Learns a representation shared across domains such that a predictor (for reward or value) is invariant, using multiple domains produced by interventions; focuses on reward-based RL and identification of causal ancestors of reward.",
            "environment_name": "Block MDPs (simulated RL environments with distinct observation nuisances across domains)",
            "environment_description": "Interactive RL environments with access to reward signals and the ability to collect rollouts; not restricted to strictly offline/batch data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Invariant representation learning across domains to separate causal state from nuisance factors.",
            "spurious_signal_types": "Observation nuisances that vary across domains, irrelevant for reward/action decision.",
            "detection_method": "Multi-domain consistency of predictors; invariance across interventions indicates causal features.",
            "downweighting_method": "Representation learning that allocates nuisance variation to non-invariant part, reducing influence on learned policy.",
            "refutation_method": "Uses cross-domain invariance criteria; explicit interventions in generating domains support refutation of spurious predictors.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Shows that multi-domain invariance can identify causal state factors in block MDPs and improve RL generalization when reward is available; referenced as related work and a conceptual precursor to ICIL's multi-environment invariance approach but not directly applicable to strictly offline imitation without rewards.",
            "uuid": "e982.6",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Resolving spurious correlations via interventions (Volodin et al.)",
            "name_full": "Resolving spurious correlations in causal models of environments via interventions",
            "brief_description": "A work proposing interventions in environments to resolve spurious correlations in causal models and thereby improve robustness of learned policies.",
            "citation_title": "Resolving spurious correlations in causal models of environments via interventions",
            "mention_or_use": "mention",
            "method_name": "Intervention-based resolution of spurious correlations",
            "method_description": "Uses explicit interventions on environment variables to break spurious associations and identify causal relationships, enabling causal-model-based corrections in learned policies.",
            "environment_name": "Simulated environments where interventions can be executed",
            "environment_description": "Interactive/experimental settings where one can intervene to change variables and observe causal effects; not a strictly batch-only setting.",
            "handles_distractors": true,
            "distractor_handling_technique": "Active interventions to break spurious associations and identify causal structure.",
            "spurious_signal_types": "Spurious correlations induced by environment-specific mechanisms or confounding.",
            "detection_method": "Observe changes in conditional distributions under interventions; lack of invariance indicates spuriousness.",
            "downweighting_method": "Not applicable directly — interventions are used to identify and remove spurious edges or reweight model factors post-intervention.",
            "refutation_method": "Interventions that change the generating mechanism of a variable can refute spurious causal links when predictive relationships disappear.",
            "uses_active_learning": true,
            "inquiry_strategy": "Select and perform interventions designed to alter suspected nuisance variables and observe resulting changes in downstream distributions.",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Interventions are a principled way to resolve spurious correlations in environment models, but require interactive access to environments; cited by the authors as part of the broader literature on causality and spurious correlations.",
            "uuid": "e982.7",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Causal transfer (Etesami & Geiger)",
            "name_full": "Causal transfer for imitation learning and decision making under sensor-shift",
            "brief_description": "A method addressing sensor-shift (distributional sensor changes) and hidden confounding in imitation/decision making by learning transfers robust to such shifts.",
            "citation_title": "Causal transfer for imitation learning and decision making under sensor-shift",
            "mention_or_use": "mention",
            "method_name": "Causal transfer under sensor-shift",
            "method_description": "Addresses hidden confounding and sensor distribution shift by leveraging causal assumptions/transfer techniques to produce policies robust to changes in sensor distributions.",
            "environment_name": "Settings with sensor shifts and potential hidden confounders (method paper context)",
            "environment_description": "Typically offline or partially observable settings where sensors change between environments; may involve simulated or logged data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Causal transfer techniques; methods to account for sensor shifts and hidden confounders (details in original paper).",
            "spurious_signal_types": "Hidden confounding, sensor-induced spurious associations.",
            "detection_method": "Not detailed in this ICIL paper; original work addresses detection/adjustment via causal-transfer methodology.",
            "downweighting_method": "Approaches to mitigate sensor-shift effects (original paper required for specifics).",
            "refutation_method": "Methodology aims to identify and correct for sensor-shift/confounding rather than purely refute via interventions.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as related work tackling hidden confounding / sensor-shift in imitation and decision making; emphasizes differences from ICIL which assumes no hidden confounders and uses multiple passive training environments.",
            "uuid": "e982.8",
            "source_info": {
                "paper_title": "Invariant Causal Imitation Learning for Generalizable Policies",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Invariant risk minimization",
            "rating": 2
        },
        {
            "paper_title": "Invariant causal prediction for block mdps",
            "rating": 2
        },
        {
            "paper_title": "Causal confusion in imitation learning",
            "rating": 2
        },
        {
            "paper_title": "Mutual information neural estimation",
            "rating": 2
        },
        {
            "paper_title": "Strictly batch imitation learning by energy-based distribution matching",
            "rating": 2
        },
        {
            "paper_title": "Resolving spurious correlations in causal models of environments via interventions",
            "rating": 1
        },
        {
            "paper_title": "Causal transfer for imitation learning and decision making under sensor-shift",
            "rating": 1
        },
        {
            "paper_title": "Adversarial discriminative domain adaptation",
            "rating": 1
        }
    ],
    "cost": 0.02233175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Invariant Causal Imitation Learning for Generalizable Policies</h1>
<p>Ioana Bica*<br>University of Oxford, Oxford, UK<br>The Alan Turing Institute, London, UK<br>ioana.bica@eng.ox.ac.uk</p>
<p>Daniel Jarrett ${ }^{*}$<br>University of Cambridge, Cambridge, UK<br>daniel.jarrett@maths.cam.ac.uk</p>
<p>Mihaela van der Schaar<br>University of Cambridge, Cambridge, UK<br>University of California, Los Angeles, USA<br>The Alan Turing Institute, London, UK<br>mv472@cam.ac.uk</p>
<h4>Abstract</h4>
<p>Consider learning an imitation policy on the basis of demonstrated behavior from multiple environments, with an eye towards deployment in an unseen environment. Since the observable features from each setting may be different, directly learning individual policies as mappings from features to actions is prone to spurious correla-tions-and may not generalize well. However, the expert's policy is often a function of a shared latent structure underlying those observable features that is invariant across settings. By leveraging data from multiple environments, we propose Invariant Causal Imitation Learning (ICIL), a novel technique in which we learn a feature representation that is invariant across domains, on the basis of which we learn an imitation policy that matches expert behavior. To cope with transition dynamics mismatch, ICIL learns a shared representation of causal features (for all training environments), that is independent from the specific representations of noise variables (for each of those environments). Moreover, to ensure that the learned policy matches the observation distribution of the expert's policy, ICIL estimates the energy of the expert's observations and uses a regularization term that minimizes the imitator policy's next state energy. Experimentally, we compare our methods against several benchmarks in control and healthcare tasks and show its effectiveness in learning imitation policies capable of generalizing to unseen environments.</p>
<h2>1 Introduction</h2>
<p>Strictly batch imitation learning aims to learn a policy that directly mimics the behaviour of experts, for which we only have access to a set of demonstrations: logged trajectories of observations and actions following the expert's policy [1-3]. We cannot interact online with the environment, let alone query the expert any further, nor do we have reward signals for supervision. This setting is relevant in real-world scenarios where live experimentation is risky or costly-such as healthcare and education.</p>
<p>Our aim is to learn an imitation policy in the strictly batch setting that faithfully matches the expert behaviour, while at the same time is able to generalize to unseen environments. In healthcare, learning a generalizable behaviour policy that could achieve expert performance in new environments is an important goal: As a means of providing clinical decision support, it could serve as an "individualized"</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Causal diagram for the structure of environments. Expert demonstrations contain information about observations $x_{t}$ and actions $a_{t}$. We assume that observations are decomposable into (1) state representations $s_{t}$ that consist of the causal parents of the actions, and (2) noise representations $\eta_{t}$ that encapsulate any spurious correlations with the actions. To allow for dynamics mismatch, the transitions between the noise representations are specific to each environment. We want to recover the invariant state representation $s_{t}$ such that the learned policy $\pi(\cdot|s_{t})$ generalizes well to new environments.</p>
<p>clinical guideline for actions that can be taken for different patients—especially in a hospital, region, or patient demographic from which we have no access to data during training. In this endeavor, a principal challenge is that the sets of expert demonstrations that we have access to may contain variables that induce selection bias, or are otherwise spuriously correlated with the expert's actions [4–7]. Directly learning an imitation policy from such data may lead to learning those spurious associations, thereby failing to generalize to unseen environments, and perpetuating any biases in the expert's behaviour.</p>
<p>However, in general it is likely that the expert's actions are only causally affected by a subset of the observed variables or by a shared latent structure [8, 9]. For instance, when imitating ideal driving behaviour, the background scenery might change, but the actions should only depend on car and road features. Another example includes the case when the lightning conditions in a room are changing, but physical dynamics of the environment are staying the same [7]. By leveraging expert trajectories from multiple different environments, our aim is to uncover this shared latent structure that causally determines expert actions, which allows us to eliminate the spurious associations and biases. In this way, the learnt policy will better be able to generalize to any unseen environments that share the same latent structure as those used for training.</p>
<p>As illustrated in Figure 1, we assume access to observations and actions from the expert's policy in the different environments <em>e</em>. The observations are functions of noise factors $\eta^{e}$ (which may differ across environments) and shared latent state representations <em>s</em> (which is invariant across environments)—that encapsulate the causal parents of the expert's actions. Note that the observed features for an environment may simply be the union of $\eta^{e}$ and <em>s</em>, but they may also be any non-linear transformation of them. We shall operate in the setting where there are no hidden confounders, i.e. that we observe all variables that are affecting the expert's actions (and the next states that result from these actions).</p>
<p>In addition to spurious correlations, another difficulty stems from learning to imitate sequential behavior in the strictly batch setting itself: While behaviour cloning [10] provides an intrinsically batch solution, it ignores important information contained in the expert's roll-out distribution, and the learned policy may drift from the support of the distribution of states visited by the expert [11, 12].</p>
<p>Contributions: In this paper, we introduce <em>Invariant Causal Imitation Learning</em> (ICIL), a novel method that learns a causal representation of the expert's actions—which is used to build a generalizable imitation policy that matches the expert's behaviour. ICIL operates in the strictly batch setting and does not assume access to data from the target environments. By leveraging expert demonstrations from multiple different training environments, ICIL learns an (shared) invariant causal representation as well as an (environment-specific) noise representation. This accommodates dynamics mismatch across environments, while allowing the imitation policy to be learned by conditioning on the invariant causal representations. First, to satisfy the causal relationships in Figure 1, ICIL learns dynamics preserving representations and ensures that the learnt causal and noise representations are marginally independent by minimizing their mutual information. Second, to encourage the learnt imitation policy to stay within the support of the distribution of states visited by the expert's policy, ICIL estimates the energy of the expert's observations and uses a regularization term that minimizes the imitator policy's next state energy. Third, we evaluate ICIL against benchmarks for batch imitation learning in control and healthcare environments. We also empirically investigate directly using ideas from invariant risk minimization [6] to augment the loss function of existing batch imitation learning methods, and benchmark against their ability to generalize across environments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Environment Offline</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dynamics mismatch (hidden confounders)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">State-distribution matching target trajectories</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Non-invariant</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { ㅇ } \ &amp; \text { ㅇ } \end{aligned}$</td>
<td style="text-align: center;">Pomerleau [10]</td>
<td style="text-align: center;">Model-free</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ho \&amp; Ermon [15]</td>
<td style="text-align: center;">Model-free</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Model rollouts</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Kostrikov et al. [2]</td>
<td style="text-align: center;">Model-free</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Adversarial off-policy matching</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">de Haan et al. [8]</td>
<td style="text-align: center;">Model-free</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { ㅇ } \ &amp; \text { ㅇ } \end{aligned}$</td>
<td style="text-align: center;">Lu et al. [16]</td>
<td style="text-align: center;">Model-free</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Model rollouts</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Kim et al. [17]</td>
<td style="text-align: center;">Model-based</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Model rollouts</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Etsami et al. [18]</td>
<td style="text-align: center;">Model-free</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">IRM</td>
<td style="text-align: center;">Arjovsky et al. [6]</td>
<td style="text-align: center;">Model-free</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;">ICIL (Ours)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Model-based</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Energy-based</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of our proposed method with related works. ICIL operates in the strictly batch setting, allows for dynamics mismatch, does not require access to target trajectories, and incentivizes the imitation policy to stay in the support of the expert's distribution via energy-based regularization.</p>
<h1>2 Related Works</h1>
<p>We tackle the problem of learning generalizable policies in an offline setting using ideas from causal inference. As such, our work straddles the intersection of research in (1) strictly batch imitation, (2) invariant representation learning, and-more broadly-(3) causality in sequential decision-making.
Strictly Batch Imitation Learning: The simplest approach to imitation learning in the batch setting is behaviour cloning [10] which uses standard supervised learning techniques to learn an imitation policy that minimizes the negative log-likelihood of the observed demonstrator actions. However, behaviour cloning suffers from distributional shift as the learnt imitation policy cannot recover if it reaches a state out-of the distribution of the expert demonstrations [11-14]. To overcome this problem, $[1,14]$ propose incorporate dynamics-awareness by adding regularization to behaviour cloning by using norm-based penalties on the sparsity of implied rewards. Alternatively, [2] uses a distribution matching approach and propose an offline objective for estimating the distribution ratio of the imitator policy and the expert policy, while [3] jointly learn a policy function together with an energy-based model of the state distribution. However, none of the existing approaches consider the problem of generalization across environments and learning policies robust to spurious correlations.
Invariant Risk Minimization: In the supervised learning setting, Invariant Risk Minimization (IRM) [6] leverages data from multiple domains to learn a data representation that elicits an invariant predictor across the different environments. The training data from each environment corresponds to different interventions on the data generating process. Given data from several training environments, the IRM objective aims to find a representation such that there exists a classifier that is optimal across all training domains, i.e. that minimizes the empirical risk in each domain. This represents a challenging, bi-level optimization, and [6] propose the IRM-v1 objective which is a practical version to optimize. Through this optimization, the IRM objective should learn a predictor that only uses the causal parents of the target variable and that is thus invariant across environments. However, directly using IRM for our sequential problem setting is not desirable, since it does not take into account the effect of each action on the subsequent states. Nonetheless, we empirically investigate augmenting existing methods for batch imitation to use the IRM-v1 objective in conjunction with their defined imitation risk, and verify whether they are able to generalize across environments. In our experiments we observe that, in general, directly applying the IRM objective in this manner is not good enough.
Generalization in Imitation Learning: The problem of domain adaptation and transfer learning for the imitation learning setting has been tackled by several works so far. However, while they consider problems of dynamics-, embodiment-, and/or viewpoint-mismatch between the imitator and expert, existing methods assume access to demonstrations from the target environment [17, 19, 20], assume access to online interaction or simulators in the different environments [16], or focus on the different problem of hidden confounding [18, 21]. Another line of work that is related is learning from demonstrations and meta-learning. While works in meta-learning also aims to generalize learnt policies to new-tasks, they require access to one or more expert trajectories from the new task [22-27].
Causality in Imitation Learning and Reinforcement Learning: Several ideas from causality have been used to improve imitation learning and generalization in reinforcement learning. The idea of conditioning the imitation policy on the causal parents has been employed by [8] to avoid the problem of 'causal confusion' when learning a policy for the single environment setting. However, [8] requires</p>
<p>queering the expert or being able to perform interventions in the environment this is not possible in the batch setting. Similarly, [9, 28] also learn causal relationships between the observations, actions and rewards by performing/simulating the effect of interventions in the environment. Alternatively, [29] use ideas from Invariant Risk Minimization [6, 30] to learn optimal reinforcement learning policies that generalize across domains. Perhaps the most similar setting to ours is the one in [7] which studies the problem of generalization in reinforcement learning and also learn a representation that is shared across the domains. However, unlike our imitation setting, they assume access to a known reward signal, and focus on learning the causal ancestors of that reward to improve reinforcement learning [31].</p>
<p>To the best of our knowledge, we are the first to tackle the problem of learning generalizable imitation policies in the strictly batch setting. Table 1 summarizes main differences with relevant related works.</p>
<h1>3 Problem Formalism</h1>
<h3>3.1 Imitation Learning</h3>
<p>We work in the standard Markov decision process (MDP) setting: Let an environment be given by $e=(\mathcal{X}, \mathcal{A}, T, r, \gamma)$, with observations $x \in \mathcal{X}$, actions $a \in \mathcal{A}$, transition function $T \in \Delta(\mathcal{X})^{\mathcal{X} \times \mathcal{A}}$, reward function $r \in \mathbb{R}^{\mathcal{X} \times \mathcal{A}}$, and discount factor $\gamma$. Let $\pi \in \Delta(\mathcal{A})^{\mathcal{X}}$ be a policy with the induced occupancy measure $\rho_{\pi}(x, a)=(1-\gamma) \sum_{t=0}^{\infty} \gamma^{t} p\left(x_{t}=x, a_{t}=a \mid x_{t} \sim T\left(\cdot \mid x_{t-1}, a_{t-1}\right), a_{t} \sim \pi\left(\cdot \mid x_{t}\right)\right)$ of observations and actions, and let $\rho_{\pi}(x)=\sum_{a \in \mathcal{A}} \rho_{\pi}(x, a)$ be the observation occupancy measure.</p>
<p>Unlike in the reinforcement learning setting, where the aim is to learn a policy $\pi(\cdot \mid x)$ that maximizes the cumulative sum of some known reward signal, in imitation learning the reward is neither known nor observed. Instead, we only have access to a dataset of trajectories $\mathcal{D}=\left{\tau_{i}\right}<em D="D">{i=1}^{N}$ from a demonstrator policy $\pi</em>\right)}$, where each trajectory $\tau \sim \pi_{D}=\left(x_{t}, a_{t}, x_{t+1<em t="t">{t=0, \ldots}$ consists of a sequence of observation, action, next observation tuples that are sampled as $a</em>\right)$. The goal of imitation learning is to seek an imitation policy $\pi$ that minimizes the following risk:} \sim \pi_{D}\left(\cdot \mid x_{t}\right)$ and $x_{t+1} \sim T\left(\cdot \mid x_{t}, a_{t</p>
<p>$$
R(\pi)=\mathcal{L}\left(\pi, \pi_{D}\right)
$$</p>
<p>where $\mathcal{L}$ is a choice of loss function. Now, if we were in the online setting, we would have access to the environment (or a simulator), with which we can interactively perform distribution matching by minimizing the divergence between the expert's state occupancy $\rho_{D}$ and the imitator's state occupancy $\rho_{\pi}[15,32-34]$. One example is to use the (forward) KL divergence: $\mathcal{L}\left(\pi, \pi_{D}\right)=D_{K L}\left(\rho_{D} | \rho_{\pi}\right)$ [34]. However, in the offline setting we have no further access to the environment. As noted above, the simplest solution is behaviour cloning (BC) [10, 35, 36], which minimizes the negative log-likelihood of the demonstrator's actions. However, by disregarding the distribution of the expert's observations, imitation policies learnt by BC often result in compounding error when deployed in practice [11-14].</p>
<h3>3.2 Imitation Learning from Multiple Environments</h3>
<p>Consider a family of environments $\mathcal{M}=\left{\left(\mathcal{X}^{e}, \mathcal{A}, T^{e}, r^{e}, \gamma\right) \mid e \in \mathcal{E}\right}$ with observations $x^{e} \in \mathcal{X}^{e}$, actions $a \in \mathcal{A}$, transition function $T^{e} \in \Delta(\mathcal{X})^{\mathcal{X} \times \mathcal{A}}$, reward function $r^{e} \in \mathbb{R}^{\mathcal{X} \times \mathcal{A}}$, and discount factor $\gamma$. This is the primary setting that we shall operate in. Note that the action space and discount factor do not change between environments. For notational simplicity, when considering the union over environments, we shall drop the index $e$. We assume offline access to a dataset of recorded trajectories from the expert policy $\pi_{D}$ in a set of training environments $\mathcal{E}<em i="i">{\text {train }} \subset \mathcal{E}, \mathcal{D}=\left{\left{\left(\tau</em>\right}}^{e<em e="e">{i=1}^{N</em>}} \mid e \in \mathcal{E<em D="D">{\text {train }}\right}\right.$. Each trajectory $\tau^{e} \sim \pi</em>\right)}=\left(x_{t}^{e}, a_{t}, x_{t+1}^{e<em t="t">{t=0, \ldots}$ consists of a sequence of environment specific observations, expert actions and next observations sampled as $a</em>\right)$.
In the presence of multiple environments, our goal is to learn a policy $\pi \in \Delta(\mathcal{A})^{\mathcal{X}}$ that matches the expert behaviour in all possible environments $\mathcal{E}$ that share a certain structure for the observations and the transition dynamics. In particular, this involves finding a policy that generalizes well across these related environments $e \in \mathcal{E}$-that is, the policy should ideally minimize the imitation risk across them:} \sim \pi_{D}\left(\cdot \mid x_{t}^{e}\right)$ and $x_{t+1}^{e} \sim T^{e}\left(\cdot \mid x_{t}^{e}, a_{t</p>
<p>$$
\max <em D="D">{e \in \mathcal{E}} R^{e}(\pi)=\mathcal{L}^{e}\left(\pi, \pi</em>\right)
$$</p>
<p>where each $\mathcal{L}^{e}$ explicitly depends on the characteristics of the environment $e$. Note that since we know nothing specific about $\mathcal{E}$, it is difficult to optimize for this directly. That said, if we make mild assumptions about the "relatedness" of these environments, we can learn policies that generalize well.</p>
<p>Structure of Observations: First, we assume there is a shared latent structure underlying the observations from different environments—on which the expert policy depends. Finding such a structure would let us discard irrelevant factors as inputs to the learnt policy, improving generalization [6, 7, 37]:
Assumption 3.1. (Shared Latent Structure) Consider decomposing the observations $x^{e}\in\mathcal{X}^{e}$ in each environment $e \in \mathcal{E}$ into two components: an invariant representation $s \in \mathcal{S}$ and noise terms $\eta^{e} \in \mathcal{Z}^{e}$ (i.e. spurious correlations), such that $x^{e}=q\left(s, \eta^{e}\right)$ for some invertible transformation $q: \mathcal{S} \times \mathcal{Z}^{e} \rightarrow \mathcal{X}^{e}$. There exists some $q$ such that $\pi_{D}$ only depends on $s$, and space $\mathcal{S}$ is non-empty.</p>
<p>In other words, we assume that the demonstrator's policy $\pi_{D}$ depends only on information that is shared across the environments, i.e. the state variables $s$ are the causal parents of the expert action $a \sim \pi_{D}(\cdot \mid s)$. As illustrated in Figure 1, the state variables and the noise terms are responsible for generating the patient observations, but the policy depends only on the state variables. Thus, we allow different environments to have different $p(x)$ marginals (as well as different $p(a \mid x)$ ). This allows environments to have different structure. The only requirement is that the environments are the same as far as the task is concerned. This means that there exists some $\mathcal{S}$ such that $p(s)$ marginals should be the same (as well as $p(a \mid s)$ ). Learning such a representation that is invariant satisfies the standard Environment Invariance Constraint [38]. While this set-up is similar to the one in [7], a crucial difference is that we have no access to any reward functions whatsoever, and that we must learn an imitation policy in a strictly batch setting.
Note that the latent structure induced by the state variables is shared across the different environments. This means that the transition dynamics for the state representation $p\left(s_{t+1} \mid s_{t}, a_{t}\right)$ remain invariant across the environments. On the other hand, as different environments may be characterized by different types of spurious correlations, to allow for flexibility in their structure and evolution, we consider that the transition dynamics of the noise terms $p^{e}\left(\eta_{t+1}^{e} \mid \eta_{t}^{e}, a_{t}\right)$ are specific to each environment. Our goal, then, is to learn a generalizable policy $\pi$-that is, one that depends only on $s$.
Structure of Environments: Second, to learn a policy that depends only on $s$, we must assume that the available training environments are actually different, so that we can learn the invariant state representation using the data from these environments and separate it from the noise representation:
Assumption 3.2. (Environment Interventions) Each available training environment e corresponds to a hard [39] or soft [40] intervention on one or more dimensions of that environment's observation space (where these dimensions do not constitute any causal parents of the demonstrator's actions).</p>
<p>To ensure that a generalizable policy actually exists, Assumption 3.1 requires that $\mathcal{S}$ be non-empty across all environments. Here, to ensure that the space $\mathcal{S}$ can actually be learned, Assumption 3.2 requires that $\mathcal{Z}$ be non-empty across the training environments. Note that we require that the interventions inducing the different environments not be on the causal parents of the action, such that Assumption 3.1 is not violated.
Overall, in our setting $p(x)$ and $p(a \mid x)$ can differ between the multiple environments. However, Assumption 3.1 enforces that the environments and tasks are the same modulo noise, i.e. that there exists some non-empty $\mathcal{S}$ such that $p(s)$ and $p(a \mid s)$ are the same between them. In other words, we have a set of environments that are different (i.e. the dynamics of $x_{t}$ are different), but the task being performed by the agent is the same (i.e. the dynamics of $s_{t}$ are the same). This setting applies to the case when lightning conditions in a room are changing, but physical dynamics of the environment are staying the same [7] or when weather conditions are changing, but driving behaviour and dynamics are staying the same. We provide additional explanations and definitions of environment interventions in Appendix A.
Figure 2 shows a simple example where each observation $x_{t}$ represents a union of the causal parents of the action (state variables) $s_{t}=\left{x_{t}^{1}, x_{t}^{2}\right}$ and the spurious correlations (noise variables) $\eta_{t}=\left{x_{t}^{3}\right}$. To satisfy Assumption 3.2, the different environments need to correspond to interventions on $x_{t}^{3}$. And to satisfy Assumption 3.1,
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Causal diagram illustrating temporal dependencies between causal parents of action $\left{x_{t}^{1}, x_{t}^{2}\right}$ and noise variables $\left{x_{t}^{3}\right}$. Different environments are induced by different interventions on the noise variables.</p>
<p>$x_{t}^{1}$ and $x_{t}^{2}$ must not be intervened on. Our aim is to find a representation $s$ of the causal parents $\left{x_{t}^{1}, x_{t}^{2}\right}$ of the actions, as well as the mapping between them and the actions $a_{t}$, that mimics the expert's policy.</p>
<p>Finally, similarly to [7], we also assume that the observations $x_{t}$ at timestep $t$ can only affect the actions $a_{t}$ and the observations at the next timestep $t+1$ :
Assumption 3.3. (Temporal Causal Mechanism) Let $x^{i}$ and $x^{j}$ be any two components of the observation $x$ at timestep $t$. Then:</p>
<p>$$
x_{t+1}^{i} \Perp x_{t+1}^{j} \mid x_{t}, a_{t}
$$</p>
<p>Note that Assumption 3.3 simply serves to place us within the standard MDP setting: It ensures Markovianity of the temporal transitions, that only the observations $x_{t}$ at time $t$ will contain the causal parents of the action $a_{t}$, and that $x_{t}$ and $a_{t}$ are the only factors that determine the next observation $x_{t+1}$.</p>
<h1>4 Invariant Causal Imitation Learning for Domain Generalization</h1>
<p>The goal of our Invariant Causal Imitation Learning (ICIL) algorithm is to learn a representation of the state variables $s$ that is invariant across domains, and an imitation policy $\pi$ that depends on this causal representation and matches the demonstrator's behaviour. We operate in the strictly batch setting, and our aim is for $\pi$ to generalize to unseen environments $e \in \mathcal{E}$ given the above structural assumptions.</p>
<h3>4.1 Learning Invariant Causal Representations</h3>
<p>To achieve our goal, we decompose the observations $x_{t}^{e}$ in each environment $e$ into a representation $s_{t}=\phi\left(x_{t}^{e} ; \theta_{s}\right)$ for the causal features of the action $a_{t}$, and another representation $\eta_{t}^{e}=\mu^{e}\left(x_{t}^{e} ; \theta_{\eta}^{e}\right)$ for the noise variables, where $\theta_{s}$ and $\theta_{\eta}^{e}$ are the learnable parameters of $\phi$ and $\eta$. Since the causal parents of the action are invariant across the environments, the state representation model $\phi: \mathcal{X} \rightarrow \mathcal{S}$ is the same across all environments. On the other hand, $\mu^{e}: \mathcal{X} \rightarrow \mathcal{Z}^{e}$ is environment-specific in order to allow for dynamics mismatch of the noise variables between the different environments.</p>
<p>In order to satisfy the causal diagram in Figure 1 and to learn a minimal causal representation, we need the following conditions to be satisfied: (1) $s_{t}$ should be invariant across the environments, (2) $s_{t}$ and $\eta_{t}^{e}$ should be dynamics-preserving, and (3) $s_{t}$ and $\eta_{t}^{e}$ should be independent from each other.
Firstly, to fulfill condition (1) we train an environment classifier on the shared state representation $c_{s}: \mathcal{S} \rightarrow \mid \mathcal{E}<em c="c">{\text {train }} \mid$, parameterized by $\theta</em>\right)\right)$. This gives us the following practical loss function:}$ using the cross-entropy loss. Similarly to [7], in order to build a state representation that is invariant across domains, we use an adversarial loss [41] that maximizes the entropy of the classifier: $H\left(c_{s}\left(\phi\left(x_{t} ; \theta_{s}\right) ; \theta_{c</p>
<p>$$
\mathcal{L}<em s="s">{\text {inv }}\left(\theta</em>}\right)=\sum_{e \in \mathcal{E<em x__t="x_{t">{\text {train }}} \mathbb{E}</em>\right)\right)
$$}^{e} \sim \rho_{D}^{e}}-H\left(c_{s}\left(\phi\left(x_{t}^{e} ; \theta_{s}\right) ; \theta_{c</p>
<p>Out of all possible representations that are invariant, we specifically seek one that also preserves the transition dynamics, fulfilling condition (2). To ensure that the state and noise representations are dynamics-preserving, we also learn the transition dynamics for the state variables $g_{s}: \mathcal{S} \times \mathcal{A}: \rightarrow \mathcal{S}$, such that $\hat{s}<em s="s">{t+1}=g</em>}\left(s_{t}, a_{t} ; \theta_{g_{s}}\right)$ and the environment specific transition dynamics for the noise variables: $g_{\eta}^{e}: \mathcal{Z}^{e} \times \mathcal{A} \rightarrow \mathcal{Z}^{e}$, such that $\hat{\eta<em _eta="\eta">{t+1}^{e}=g</em>}^{e}\left(\eta_{t}^{e}, a_{t} ; \theta_{g_{\eta}}^{e}\right)$. To reconstruct $x_{t+1}$ we also learn $\psi$ : $\mathcal{S} \times \mathcal{Z}^{e} \rightarrow \mathcal{X}$ such that $\hat{x<em t_1="t+1">{t+1}^{e}=\psi\left(s</em>\right)$. This yields the following practical loss function:}, \eta_{t+1}^{e} ; \theta_{\psi</p>
<p>$$
\mathcal{L}<em s="s">{\text {dyn }}\left(\theta</em>\right}}, \theta_{g_{s}},\left{\theta_{\eta}^{e}, \theta_{g_{\eta}}^{e<em _text="\text" _train="{train">{e \in \mathcal{E}</em>}}}, \theta_{\psi}\right)=\sum_{e \in \mathcal{E<em x__t_1="x_{t+1">{\text {train }}} \mathbb{E}</em>
$$}^{e}, a_{t}, x_{t+1}^{e} \sim \rho_{D}^{e}}\left|x_{t+1}^{e}-\hat{x}_{t+1}^{e}\right|^{2</p>
<p>Note that while an alternative approach could consider directly building an invertible mapping from $x^{e}$ to $\left(s, \eta^{e}\right)$, the motivation for decoding $s_{t+1}$ and $\eta_{t+1}^{e}$ into $\hat{x}_{t+1}^{e}$ is twofold. In addition to learning dynamics-preserving representations, as we will see in Section 4.2, this also allows us to compute the energy of the next state obtained by following the imitation policy and enforcing this to be similar to the distribution of states visited by the expert's policy.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Block diagram of our model. ICIL decomposes the observations $x_{t}^{e}$ into an invariant causal representation $s_t$ and an environment specific noise representation $\eta^e$. To obtain an invariant representation, we maximize entropy of an environment classifier that receives as input $s_t$ ($L_{inv}$). Moreover, the state and noise representations are learnt to be dynamics preserving by minimizing the prediction error of the next observation ($L_{dyn}$) and independent by minimizing their mutual information ($L_{mi}$). We learn a generalizable imitation policy that is conditioned on the invariant causal representation ($L_{\pi}$) and to ensure that the learnt policy matches the distribution of the expert's observations, we minimize the imitator's policy next state energy ($L_{energy}$).</p>
<p>Finally, to ensure that the state representation and the noise representation are marginally independent per condition (3), we minimize the mutual information between them. We use the Mutual Information Neural Estimation (MINE) framework [42], which provides a way for estimating the mutual information using neural networks. In particular, MINE uses a neural information measure $I(U, V)$ to approximate the mutual information between random variables $U$ and $V$. Let $T_{\theta_m}$ be a statistics network parametrized by $\theta_m$. MINE estimates $I(U, V)$ by ascending the gradient of the following:</p>
<p>$$
I(U, V) = \sup_{\theta_m} \mathbb{E}<em UV="UV">{p</em>}^{(n)}} [T_{\theta_m}] - \log(\mathbb{E<em UV="UV">{p</em>
$$}^{(n)} \otimes P_{V}^{(n)}} [e^{T_{\theta_m}}]) = \sup_{\theta_m} I(U, V; \theta_m) \tag{6</p>
<p>where $\mathbb{P}<em U="U">{UV}$ is the joint measure of $(U, V)$ and $\mathbb{P}</em>} = \int_{V} dP_{UV}$, $\mathbb{P<em U="U">{V} = \int</em>$ denotes the empirical distribution associated with $n$ i.i.d samples. As noted in [42], the neural information measure $I(U, V)$ can approximate the mutual information with arbitrary accuracy. We therefore add the following practical loss function to our optimization objective, which seeks to minimize the mutual information between the state representation and noise representation:} dP_{UV}$ are the marginal distributions. $\mathbb{P}^{(n)</p>
<p>$$
\mathcal{L}<em _eta="\eta">{mi}(\theta_s, {\theta^e</em>}<em train="train">{e \in \mathcal{E}</em>}}) = \sum_{e \in \mathcal{E<em x_e__t="x^e_{t">{train}} \mathbb{E}</em>
$$} \sim \rho^e_{D}} I(\phi(x^e_t; \theta_s), \mu(x^e_t; \theta^e_{\eta}); \theta_m) \tag{7</p>
<p>Note that the parameters $\theta_m$ of the statistics network $T_{\theta_m}$ used for computing the mutual information are updated through gradient ascent on $I(U, V; \theta_m)$.</p>
<h3>4.2 Matching Expert Behaviour in a Strictly Batch Setting</h3>
<p>On the basis of the causal representation $s$, we shall learn a generalizable policy $\pi$ (parameterized by $\theta_\pi$) in the strictly batch setting, such that it matches the demonstrator's behaviour. To begin, we first condition $\pi$ on the representation $s_t$ and minimize the negative log-likelihood of the expert's actions:</p>
<p>$$
\mathcal{L}<em>{\pi}(\theta</em>\pi, \theta_s) = \sum_{e \in \mathcal{E}<em _rho_e_D="\rho^e_D" _sim="\sim" a_t="a_t" x_e_t_="x^e_t,">{train}} -\mathbb{E}</em>
$$} \log \pi(a_t \mid \phi(x^e_t; \theta_s); \theta_\pi) \tag{8</p>
<p>However, having only this objective corresponds to performing behaviour cloning, which has well-known limitations [11–14]. To mitigate compounding error, we want some form of added regularization to incentivize the imitation policy to stay within the distribution of the expert's observations.</p>
<p>In the online setting, a popular approach is to make sure that the rollout distribution of the imitating policy matches the rollout distribution of the expert's policy—for instance, by minimizing some form</p>
<p>of divergence between their induced occupancy measures. However, this requires interactive access to the real environment or simulator to perform rollouts of intermediate policies—which is not possible in our setting. Instead, we propose a method that takes advantage of the learnt transition dynamics. For any current observation $x_{t}\sim\rho_{D}$, we shall encourage the next observation $\bar{x}<em t="t">{t+1}$ obtained by following the imitation policy $\bar{a}</em>)$ to remain within the occupancy measure of the expert.}\sim\pi(\cdot \mid x_{t</p>
<p>Consider approximating the expert’s occupancy measure using an Energy Based Model (EBM) such that $\rho_{D}(x)=\frac{\exp(-E_{\bar{\theta}}(x))}{Z(\bar{\theta})}$ where the function $E_{\bar{\theta}}(x):\mathcal{X}\rightarrow\mathbb{R}$ is the energy function and $Z(\bar{\theta})=\int_{x}-E_{\bar{\theta}}(x)dx$ is the partition function. We parameterize $E_{\bar{\theta}}$ by a neural network. It is not possible to train the EBM directly through maximum likelihood because $Z(\bar{\theta})$ involves integrating over the entire input domain of $x$ which is impractical. Instead, we use contrastive divergence to pre-train the energy function $E_{\bar{\theta}}$ [43,44]. Contrastive divergence lowers the energy of the observations coming from the expert’s occupancy distribution and increases the energy of the observations outside of the expert’s occupancy distribution. Refer to Appendix B for details on how we train the EBM.</p>
<p>To incentive the imitation policy to stay within the distribution of the expert’s observations, we train it to minimize the energy of the next observation obtained by following $\pi$ given the current observation:</p>
<p>This effectively assigns a high “reward” to the imitation policy for staying within high-density areas of the expert’s occupancy measure, and a low “reward” for straying from it. This can be seen as an adaptation of online imitation methods [45,46] where the expectation would be instead over $x_{t}\sim\rho_{\pi}$.</p>
<p>We illustrate in Figure 3 all of the components of the our ICIL model. Further details and the full algorithm for optimizing ICIL can be found in Appendix C.</p>
<h2>5 Experiments</h2>
<p>We perform experiments on OpenAI gym tasks [47] and on an ICU dataset from the MIMIC III database [48]. In both cases, we generate data from multiple domains by augmenting the feature space with noise variables (spurious correlations).</p>
<p>Benchmarks We compare ICIL against standard methods for strictly batch imitation learning: Behaviour Cloning (BC) [10]; Reward-regularized Classification for Apprenticeship Learning (RCAL), which incorporates dynamics-awareness through a sparsity regularization on the implied rewards [14]; ValueDICE (VDICE) [2], which uses an off-policy objective to estimate distribution ratios needed for distribution matching; as well as Energy-based Distribution Matching (EDM) [3], which jointly learns the imitator policy with an energy model of its state distributions. These methods seek to find a policy that approximately matches the expert’s behaviour from a single environment, and were not designed with generalization in mind. Hence we augment these benchmark by using the IRMv1 objective [6] in conjunction with their originally defined imitation risk to obtain the additional benchmarks: BC-IRM, RCAL-IRM, VDICE-IRM, and EDM-IRM. More details about how we used the invariance-based penalty from IRM [6] to augment these existing methods such that they may learn generalizable policies can be found in Appendix D. Implementation details about all benchmarks and the hyperparameter settings used can be found in Appendix E.</p>
<h3>5.1 Evaluation on OpenAI Gym</h3>
<p>We perform experiments on the following control tasks from OpenAI gym [47]: Acrobot [49], Cartpole [50], LunarLander [47] and BeamRider [51]. For each task, we use pre-trained RL agents from RL Baselines Zoo [52] and Stable OpenAI Baselines [53] to obtain expert policies. We then follow an approach similar to the one in [7] to obtain datasets with demonstrations from the expert in two different environments. In particular, for Acrobot [49], Cartpole [50] and LunarLander [47] we add spurious correlations to the state space of each control task and an environment identifier. The</p>
<p>[1] The code for ICIL can be found at https://github.com/vanderschaarlab/mlforhealthlabpub and at https://github.com/ioanabica/Invariant-Causal-Imitation-Learning.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Evaluation on OpenAI gym environments. $x$-axis indicates the number of trajectories (in ${1,5,10,15,20}$ ) with expert demonstrations from each training environment given as input to each benchmark and $y$-axis represents the average return of the learnt imitation policy on the test environments, scaled between 1 (expert performance) and 0 (random policy performance).
spurious correlations in each environment are different multiplicative factors of a subset of variables in the original state space. The invariant causal state is represented by the original variables in the state space of each control task. We train the benchmarks on demonstrations from two environments with $1 \times$ and $2 \times$ multiplicative factors for the spurious correlations and we test on an environments with multiplicative factors sampled from $\mathcal{U}(-1,1)$. For BeamRider [51], similarly to [7], different camera angles are used for the training and testing environments. In particular, we use two training environments where the game frames are rotated by 10 degrees to the left and to the right respectively, while the test environment has no rotation. The rotation is applied to the entire frame for all trajectories in each environment. However, note that, despite the rotation, the dynamics for the state variables and how they influence the action stay the same. Further details about the train and test environments can be found in Appendix E.</p>
<p>We vary the number of demonstrated trajectories from each environment that we give as input to each benchmark and we evaluate them on the average return obtained by deploying the learnt imitation policies on the test environment. Figure 4 shows the mean results and standard errors obtained across 10 runs where for each run we train the benchmarks on different trajectories from the train environments and we evaluate on a test environment with newly sampled multiplicative factors for computing the spurious correlations. We notice that our method consistently outperforms the benchmarks and is capable of generalizing better to the unseen target environments. Moreover, we generally found that using the IRMv1 objective [6] together with existing methods for strictly batch imitation learning did not improve performance and resulted in more unstable training. For additional results, see Appendix F where we perform ablation studies to investigate the impact of the different terms in the loss function used to train ICIL on overall performance, compare performance on train vs. test environments and also evaluate robustness to increasing the size of the spurious correlations.</p>
<h1>5.2 Evaluation on MIMIC-III</h1>
<p>We also perform experiments on a healthcare dataset with Intensive Care Unit (ICU) patients extracted from the Medical Information Mart for Intensive Care (MIMIC-III) database [48]. The dataset consists of trajectories of clinical measurements (e.g. heart rate, respiratory rate) recorded every hour. The aim is to learn a generalizable policy for the action of putting patients on the mechanical ventilator.</p>
<p>We define three environments, two for training and one for testing, each consisting of 2000 independent patient trajectories from MIMIC-III. We augment the original feature space by adding spurious correlations (noise variables) that are the same as the expert actions with probabilities $p=0.1$ and $p=0.2$ in the training environments and with probability $p=0.8$ in the testing environment.</p>
<p>In a real setting, such spurious correlations are commonplace. For instance, consider some hospitals (i.e. training environments) where selection bias is present, such that patients with a certain otherwise irrelevant comorbidity happen to receive a treatment more often [54-57]. However, learning an imitation policy that takes into account such a comorbidity when assigning the patient's treatment would fail to generalize to hospitals where fewer patients suffer from this comorbidity but should still receive the treatments. More details about the dataset can be found in Appendix E.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Mechanical ventilator</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Benchmark</td>
<td style="text-align: center;">ACC</td>
<td style="text-align: center;">AUC</td>
<td style="text-align: center;">APR</td>
</tr>
<tr>
<td style="text-align: left;">BC</td>
<td style="text-align: center;">$0.783 \pm 0.001$</td>
<td style="text-align: center;">$0.762 \pm 0.002$</td>
<td style="text-align: center;">$0.692 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: left;">RCAL</td>
<td style="text-align: center;">$0.790 \pm 0.002$</td>
<td style="text-align: center;">$0.771 \pm 0.002$</td>
<td style="text-align: center;">$0.697 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: left;">VDICE</td>
<td style="text-align: center;">$0.794 \pm 0.001$</td>
<td style="text-align: center;">$0.784 \pm 0.001$</td>
<td style="text-align: center;">$0.716 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: left;">EDM</td>
<td style="text-align: center;">$0.786 \pm 0.003$</td>
<td style="text-align: center;">$0.741 \pm 0.011$</td>
<td style="text-align: center;">$0.682 \pm 0.005$</td>
</tr>
<tr>
<td style="text-align: left;">BC-IRM</td>
<td style="text-align: center;">$0.791 \pm 0.002$</td>
<td style="text-align: center;">$0.767 \pm 0.003$</td>
<td style="text-align: center;">$0.696 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: left;">RCAL-IRM</td>
<td style="text-align: center;">$0.789 \pm 0.002$</td>
<td style="text-align: center;">$0.766 \pm 0.003$</td>
<td style="text-align: center;">$0.694 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">VDICE-IRM</td>
<td style="text-align: center;">$0.766 \pm 0.001$</td>
<td style="text-align: center;">$0.730 \pm 0.001$</td>
<td style="text-align: center;">$0.694 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: left;">EDM-IRM</td>
<td style="text-align: center;">$0.781 \pm 0.004$</td>
<td style="text-align: center;">$0.717 \pm 0.015$</td>
<td style="text-align: center;">$0.673 \pm 0.007$</td>
</tr>
<tr>
<td style="text-align: left;">ICIL</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 5} \pm \mathbf{0 . 0 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 6} \pm \mathbf{0 . 0 0 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 9} \pm \mathbf{0 . 0 0 4}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation on MIMIC-III in terms of action-matching. We compare the actions selected by the benchmark imitation policies with the ones from the clinical expert policy in the test environment and report the accuracy (ACC), the area under the receiving operator characteristic curve (AUC) and the area under the the precision-recall curve (APR).</p>
<p>Since MIMIC-III is an entirely offline dataset, it is not possible to compute average returns for running the policies in the test environment. Instead, we evaluate the benchmarks in terms of action matching on the test environment. We report in table 2 the mean accuracy (ACC), the mean area under the receiving operator characteristic curve (AUC), the mean area under the the precision-recall curve (APR) and their standard deviations over 10 runs. We notice that ICIL learns a policy that best discards the spurious correlations present in the training environment to learn a generalizable policy for putting patients on the mechanical ventilator that best matches the expert's actions on the test environment.</p>
<h1>6 Discussion</h1>
<p>In this paper, we tackle the problem of learning generalizable imitation policies in the strictly batch setting. Our ICIL model leverages ideas from causality and learns an invariant state representation that minimizes the presence of spurious correlations. By conditioning the imitation policy on this state representation, we obtain a policy that generalizes to environments with the same shared latent structure, but with different noise distribution and dynamics. ICIL also matches expert behaviour by incentivizing the learnt imitation policy to stay within the expert's observations distribution.</p>
<p>In terms of limitations, we believe that future work should consider providing theoretical insights and error bounds on the generalization error. In addition, to be able to learn an invariant state representation, our method requires demonstrated trajectories from at least two training environments with different interventions on the noise variables (spurious correlations), and the method cannot be used if such data is not available in practice. Finally, we bear in mind that-as with any other imitation learning method that aims to match the expert's policy-ICIL can have potential negative societal impacts if the expert's policy is flawed in the first place. Thus, in sensitive applications such as clinical decision support, care must be taken to prevent potentially negative feedback loops.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the reviewers for their valuable feedback. The research presented in this paper was supported by The Alan Turing Institute, under the EPSRC grant EP/N510129/1, by Alzheimer's Research UK (ARUK), by the US Office of Naval Research (ONR), and by the National Science Foundation (NSF) under grant number 1722516.</p>
<h2>References</h2>
<p>[1] Bilal Piot, Matthieu Geist, and Olivier Pietquin. Bridging the gap between imitation learning and inverse reinforcement learning. IEEE transactions on neural networks and learning systems, 28(8):1814-1826, 2016.</p>
<p>[2] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching. arXiv preprint arXiv:1912.05032, 2019.
[3] Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. Strictly batch imitation learning by energy-based distribution matching. Advances in Neural Information Processing Systems, 2020.
[4] Amy Zhang, Yuxin Wu, and Joelle Pineau. Natural environment benchmarks for reinforcement learning. arXiv preprint arXiv:1811.06032, 2018.
[5] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521-1528. IEEE, 2011.
[6] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.
[7] Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup. Invariant causal prediction for block mdps. In International Conference on Machine Learning, pages 11214-11224. PMLR, 2020.
[8] Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. arXiv preprint arXiv:1905.11979, 2019.
[9] Timothy E Lee, Jialiang Zhao, Amrita S Sawhney, Siddharth Girdhar, and Oliver Kroemer. Causal reasoning in simulation for structure and transfer learning of robot manipulation policies. arXiv preprint arXiv:2103.16772, 2021.
[10] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural computation, 3(1):88-97, 1991.
[11] Stéphane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 661-668. JMLR Workshop and Conference Proceedings, 2010.
[12] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.
[13] Francisco S Melo and Manuel Lopes. Learning from demonstration using mdp induced metrics. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 385-401. Springer, 2010.
[14] Bilal Piot, Matthieu Geist, and Olivier Pietquin. Boosted and reward-regularized classification for apprenticeship learning. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pages 1249-1256, 2014.
[15] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. arXiv preprint arXiv:1606.03476, 2016.
[16] Yiren Lu and Jonathan Tompson. Adail: Adaptive adversarial imitation learning. arXiv preprint arXiv:2008.12647, 2020.
[17] Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, and Stefano Ermon. Domain adaptive imitation learning. In International Conference on Machine Learning, pages 5286-5295. PMLR, 2020.
[18] Jalal Etesami and Philipp Geiger. Causal transfer for imitation learning and decision making under sensor-shift. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10118-10125, 2020.
[19] Pierre Sermanet, Corey Lynch, Jasmine Hsu, and Sergey Levine. Time-contrastive networks: Self-supervised learning from multi-view observation. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 486-487. IEEE, 2017.
[20] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1118-1125. IEEE, 2018.
[21] Junzhe Zhang, Daniel Kumor, and Elias Bareinboim. Causal imitation learning with unobserved confounders. Advances in Neural Information Processing Systems, 33, 2020.</p>
<p>[22] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126-1135. PMLR, 2017.
[23] Yan Duan, Marcin Andrychowicz, Bradly C Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. arXiv preprint arXiv:1703.07326, 2017.
[24] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta-learning. In Conference on Robot Learning, pages 357-368. PMLR, 2017.
[25] Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018.
[26] Stephen James, Michael Bloesch, and Andrew J Davison. Task-embedded control networks for few-shot imitation learning. In Conference on Robot Learning, pages 783-795. PMLR, 2018.
[27] Pratyusha Sharma, Deepak Pathak, and Abhinav Gupta. Third-person visual imitation learning via decoupled hierarchical controller. arXiv preprint arXiv:1911.09676, 2019.
[28] Sergei Volodin, Nevan Wichers, and Jeremy Nixon. Resolving spurious correlations in causal models of environments via interventions. arXiv preprint arXiv:2002.05217, 2020.
[29] Anoopkumar Sonar, Vincent Pacelli, and Anirudha Majumdar. Invariant policy optimization: Towards stronger generalization in reinforcement learning. arXiv preprint arXiv:2006.01096, 2020.
[30] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In International Conference on Machine Learning, pages 145-155. PMLR, 2020.
[31] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861-1870. PMLR, 2018.
[32] Nir Baram, Oron Anschel, and Shie Mannor. Model-based adversarial imitation learning. arXiv preprint arXiv:1612.02179, 2016.
[33] Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016.
[34] Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization perspective on imitation learning methods. In Conference on Robot Learning, pages 1259-1277. PMLR, 2020.
[35] Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence 15, pages 103-129, 1995.
[36] Umar Syed and Robert E Schapire. A reduction from apprenticeship learning to classification. Advances in neural information processing systems, 23:2253-2261, 2010.
[37] Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical Methodology), pages 947-1012, 2016.
[38] Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In International Conference on Machine Learning, pages 2189-2200. PMLR, 2021.
[39] Judea Pearl. Causality. Cambridge university press, 2009.
[40] Frederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosophy of science, 74(5):981-995, 2007.
[41] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $7167-7176,2017$.
[42] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning, pages 531-540. PMLR, 2018.</p>
<p>[43] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771-1800, 2002.
[44] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. 2019.
[45] Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement learning with sparse rewards. arXiv preprint arXiv:1905.11108, 2019.
[46] Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning. arXiv preprint arXiv:2004.09395, 2020.
[47] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[48] Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1-9, 2016.
[49] Alborz Geramifard, Christoph Dann, Robert H Klein, William Dabney, and Jonathan P How. Rlpy: a value-function-based reinforcement learning framework for education and research. J. Mach. Learn. Res., 16(1):1573-1578, 2015.
[50] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834-846, 1983.
[51] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.
[52] Antonin Raffin. Rl baselines zoo. https://github.com/araffin/rl-baselines-zoo, 2018.
[53] Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https: //github.com/hill-a/stable-baselines, 2018.
[54] Chloe E Taylor, Helen Jones, Mohammad Zaregarizi, Nigel T Cable, Keith P George, and Greg Atkinson. Blood pressure status and post-exercise hypotension: an example of a spurious correlation in hypertension research? Journal of human hypertension, 24(9):585-592, 2010.
[55] Thomas Desautels, Ritankar Das, Jacob Calvert, Monica Trivedi, Charlotte Summers, David J Wales, and Ari Ercole. Prediction of early unplanned intensive care unit readmission in a uk tertiary care hospital: a cross-sectional machine learning approach. BMJ open, 7(9), 2017.
[56] Andrea Soo, Danny J Zuege, Gordon H Fick, Daniel J Niven, Luc R Berthiaume, Henry T Stelfox, and Christopher J Doig. Describing organ dysfunction in the intensive care unit: a cohort study of 20,000 patients. Critical Care, 23(1):1-15, 2019.
[57] Adarsh Subbaswamy, Roy Adams, and Suchi Saria. Evaluating model robustness and stability to dataset shift. In International Conference on Artificial Intelligence and Statistics, pages 2611-2619. PMLR, 2021.
[58] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681-688. Citeseer, 2011.
[59] Daniel Jarrett and Mihaela van der Schaar. Inverse active sensing: Modeling and understanding timely decision-making. International Conference on Machine Learning (ICML), 2020.</p>
<h1>A Structure of environment and interventions</h1>
<p>Similarly to [6], we consider that all environments have the same underlying Structural Causal Model (SCM) and that the different environments correspond to different interventions on the SCM. We provide here the formal definition for SCMs and interventions.
Definition A.1. (Structural Causal Model) [6]: A structural causal model (SCM) $\mathcal{C}=(S, N)$ governing the random vector $X=\left(X_{1}, \ldots X_{m}\right)$ is a collection $S$ of $m$ assignments:</p>
<p>$$
S_{j}: X_{j} \leftarrow f_{j}\left(\operatorname{Pa}\left(X_{j}\right), N_{j}\right), \text { for } j=1, \ldots m
$$</p>
<p>where $\operatorname{Pa}\left(X_{j}\right) \subseteq\left{X_{1}, X_{2}, \ldots X_{m}\right} /\left{X_{j}\right}$ are the parents of $X_{j}$ and the $N_{j}$ are the independent noise variables. We say that $X_{i}$ causes $X_{j}$ if $X_{i} \in \operatorname{Pa}\left(X_{j}\right)$.
Definition A.2. (Intervention) [6]: Consider a SCM $\mathcal{C}=(S, N)$. An intervention $e$ on $\mathcal{C}$ consists of replacing one or several of its structural equations to obtain an intervened SCM $\mathcal{C}^{e}=\left(S^{e}, N^{e}\right)$ with structural equations:</p>
<p>$$
S_{j}^{e}: X_{j}^{e} \leftarrow f_{j}\left(\operatorname{Pa}\left(X_{j}^{e}\right), N_{j}^{e}\right), \text { for } j=1, \ldots m
$$</p>
<p>The variable $X^{e}$ is intervened on if $S_{i} \neq S_{i}^{e}$ or $N_{i} \neq N_{i}^{e}$.
In our setting, the variables forming the SCM are the different observations and actions at each timestep. Moreover, Assumption 3.3 that ensures the Markovianity of the temporal transitions restrict the relationships that can be present in the SCM. In addition, Assumption 3.2 requires that the interventions to not be on the causal parents of the action.
Soft interventions [40] do not remove any edges in the causal graph induced by the SCM, but instead modify the conditional probability distributions of the variables intervened on. On the other hand, hard interventions [39] on a variable remove all incoming edges from the parents of that variables.</p>
<h1>B Train energy based model</h1>
<p>We train the energy-based model for the expert demonstrations using Persistent Contrastive Divergence [44]. To sample from the energy-based model, we use Markov Chain Monte Carlo using Langevin Dynamics [58]. Algorithm 1 outlines the method used to learn the energy-based model $E_{\bar{\theta}}$ of the expert observations. See [44] for details on training EBMs in this manner.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Learning</span><span class="w"> </span><span class="n">energy</span><span class="o">-</span><span class="n">based</span><span class="w"> </span><span class="n">model</span><span class="w"> </span>\<span class="p">(</span><span class="n">E_</span><span class="p">{</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">expert</span><span class="w"> </span><span class="n">demonstrations</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">Dataset</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">expert</span><span class="w"> </span><span class="n">demonstrations</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span>\<span class="p">(</span><span class="n">K</span>\<span class="p">),</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">size</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">alpha</span>\<span class="p">),</span><span class="w"> </span><span class="n">noise</span><span class="w"> </span><span class="n">variance</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">sigma</span>\<span class="p">),</span>
<span class="w">        </span><span class="n">Mini</span><span class="o">-</span><span class="n">batch</span><span class="w"> </span><span class="n">size</span><span class="w"> </span>\<span class="p">(</span><span class="n">N</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="p">:</span><span class="w"> </span><span class="n">energy</span><span class="o">-</span><span class="n">based</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">converged</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span>\<span class="p">(</span><span class="n">N</span>\<span class="p">)</span><span class="w"> </span><span class="n">positive</span><span class="w"> </span><span class="n">samples</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">expert</span><span class="w"> </span><span class="n">demonstrations</span><span class="w"> </span>\<span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">}</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="n">negative</span><span class="w"> </span><span class="n">samples</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span>\<span class="p">(</span><span class="mi">95</span><span class="w"> </span>\<span class="o">%</span>\<span class="p">)</span><span class="w"> </span><span class="n">probability</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">U</span><span class="p">}(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">otherwise</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span><span class="n">step</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span><span class="n">K</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">triangleright</span>\<span class="p">)</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span><span class="n">via</span><span class="w"> </span><span class="n">Langevin</span><span class="w"> </span><span class="n">dynamics</span>
<span class="w">            </span>\<span class="p">(</span>\<span class="n">hat</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">k</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">hat</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="o">-</span>\<span class="n">alpha</span><span class="w"> </span>\<span class="n">cdot</span><span class="w"> </span>\<span class="n">nabla_</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="w"> </span><span class="n">E_</span><span class="p">{</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">hat</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">+</span>\<span class="n">omega</span>\<span class="p">),</span><span class="w"> </span><span class="n">where</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">omega</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">N</span><span class="p">}(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span>\<span class="n">sigma</span><span class="p">),</span><span class="w"> </span>\<span class="n">forall</span><span class="w"> </span><span class="n">i</span><span class="w"> </span>\<span class="ow">in</span>\<span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="w"> </span><span class="n">N</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="o">=</span>\<span class="n">Omega</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">K</span><span class="p">}</span>\<span class="n">right</span><span class="p">),</span><span class="w"> </span>\<span class="n">forall</span><span class="w"> </span><span class="n">i</span><span class="w"> </span>\<span class="ow">in</span>\<span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="w"> </span><span class="n">N</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">triangleright</span><span class="w"> </span>\<span class="n">Omega</span>\<span class="p">)</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">stop</span><span class="w"> </span><span class="n">gradient</span><span class="w"> </span><span class="n">operator</span>
<span class="w">        </span><span class="n">Contrastive</span><span class="w"> </span><span class="n">divergence</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">C</span><span class="w"> </span><span class="n">D</span><span class="p">}</span><span class="o">=</span>\<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="n">N</span><span class="p">}</span><span class="w"> </span>\<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="n">E_</span><span class="p">{</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">-</span><span class="n">E_</span><span class="p">{</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">Regularization</span><span class="w"> </span><span class="n">loss</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">R</span><span class="w"> </span><span class="n">G</span><span class="p">}</span><span class="o">=</span>\<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="n">N</span><span class="p">}</span><span class="w"> </span>\<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span><span class="n">E_</span><span class="p">{</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">+</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="o">+</span><span class="n">E_</span><span class="p">{</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">backpropagating</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">nabla_</span><span class="p">{</span>\<span class="n">bar</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">C</span><span class="w"> </span><span class="n">D</span><span class="p">}</span><span class="o">+</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">R</span><span class="w"> </span><span class="n">G</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">Add</span><span class="w"> </span><span class="n">samples</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">buffer</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="w"> </span>\<span class="n">cup</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">N</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<p>For the Acrobot [49], CartPole [50], LunarLander [47] control tasks and the experiments on the healthcare dataset extracted from MIMIC III [48], we use a neural network with 2 fully-connected hidden layers of size 64 and with ReLU activation to define $E_{\bar{\theta}}$. For Acrobot [49], CartPole [50], LunarLander [47], we set the hyperparameters to number of steps $K=100$, step size $\alpha=0.01$, noise variance $\sigma=0.01$ and mini-batch size $N=64$. We optimize $\bar{\theta}$ by using the Adam optimizer for 1000 training iterations with the learning rate set to 0.001 . For the experiments on the healthcare dataset extracted from MIMIC III [48], we use the following hyperparameters for the number of steps $K=50$, step size $\alpha=0.01$, noise variance $\sigma=0.01$ and mini-batch size $N=128$. We optimize $\bar{\theta}$ by using the Adam optimizer for 1000 training iterations with the learning rate set to 0.0005 .
To define $E_{\bar{\theta}}$ for the BeamRider Atari environment [51] we use a convolutional neural network with 3 convolutional layers with 32-64-64 filters, followed by a fully connected layer of size 64, with all layers followed by ReLU activations. The hyperparameters are set as follows: number of steps $K=100$, step size $\alpha=0.01$, noise variance $\sigma=0.01$ and mini-batch size $N=64 . \bar{\theta}$ is optimized by using the Adam optimizer for 1000 training iterations with the learning rate set to 0.001 .</p>
<h1>C Algorithm</h1>
<p>Algorithm 2 provides the pseudo-code for training Invariant Causal Imitation Learning (ICIL).</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Invariant</span><span class="w"> </span><span class="n">Causal</span><span class="w"> </span><span class="n">Imitation</span><span class="w"> </span><span class="n">Learning</span>
<span class="w">    </span><span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="n">Dataset</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">expert</span><span class="w"> </span><span class="n">demonstrations</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">D</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">learning</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">lambda</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">mini</span><span class="o">-</span><span class="n">batch</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">N</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">Initialize</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">theta_</span><span class="err">{</span><span class="n">s</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{</span><span class="n">g_</span><span class="err">{</span><span class="n">s</span><span class="err">}}</span><span class="p">,</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="n">theta_</span><span class="err">{</span><span class="mi">0</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">e</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{</span><span class="n">g_</span><span class="err">{</span><span class="n">u</span><span class="err">}}</span><span class="o">^</span><span class="err">{</span><span class="n">e</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">e</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">E</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">train</span><span class="w"> </span><span class="err">}}}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{\</span><span class="n">psi</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{</span><span class="n">c</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{\</span><span class="nf">pi</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">theta_</span><span class="err">{</span><span class="n">m</span><span class="err">}\</span><span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">converged</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">mini</span><span class="o">-</span><span class="n">batch</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">demonstrations</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">e_</span><span class="err">{</span><span class="n">i</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">a_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">e_</span><span class="err">{</span><span class="n">i</span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">sim</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">D</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">permutation</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">[</span><span class="n">N</span><span class="o">]=</span><span class="err">\{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="w"> </span><span class="n">N</span><span class="err">\}\</span><span class="p">)</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">uniform</span><span class="w"> </span><span class="n">distribution</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">set</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="ow">all</span>
<span class="w">    </span><span class="n">permutations</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S_</span><span class="err">{</span><span class="n">N</span><span class="err">}:</span><span class="w"> </span><span class="err">\</span><span class="n">kappa</span><span class="w"> </span><span class="err">\</span><span class="n">sim</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">U</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">S_</span><span class="err">{</span><span class="n">N</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="n">ICIL</span><span class="w"> </span><span class="k">update</span><span class="err">:</span>
<span class="w">        </span><span class="n">Invariance</span><span class="w"> </span><span class="nl">loss</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">L</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">inv</span><span class="w"> </span><span class="err">}}</span><span class="o">=</span><span class="err">\</span><span class="n">frac</span><span class="err">{</span><span class="mi">1</span><span class="err">}{</span><span class="n">N</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">sum_</span><span class="err">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">N</span><span class="err">}</span><span class="o">-</span><span class="n">H</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">c_</span><span class="err">{</span><span class="n">s</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">phi</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">e_</span><span class="err">{</span><span class="n">i</span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="n">Transition</span><span class="w"> </span><span class="n">dynamics</span><span class="w"> </span><span class="nl">loss</span><span class="p">:</span>
</code></pre></div>

<p>$$
\mathcal{L}<em i="1">{d y n}=\frac{1}{N} \sum</em>
$$}^{N}\left|x_{i+1}^{e_{i}}-\psi\left(g_{s}\left(\phi\left(x_{i}^{e_{i}}\right), a_{i}\right), g_{q}^{e_{i}}\left(\mu^{e_{i}}\left(x_{i}^{e_{i}}\right), a_{i}\right)\right)\right|^{2</p>
<p>9: Mutual information loss:</p>
<p>$$
\mathcal{L}<em i="1">{m i}=\sum</em> \right)\right)\right)
$$}^{N} T_{\theta_{m}}\left(\phi\left(x_{i}^{e_{i}}\right), \mu\left(x_{i}^{e_{i}}\right)\right)-\log \left(\sum_{i=1}^{N} \exp T_{\theta_{m}}\left(\phi\left(x_{i}^{e_{i}}\right), \mu\left(x_{\kappa(i)}^{e_{i(i)}</p>
<p>10: Policy loss:</p>
<p>$$
\mathcal{L}<em i="1">{\pi}=\frac{1}{N} \sum</em>\right)
$$}^{N} \operatorname{Cross} \text { entropy }\left(\pi\left(\cdot \mid \phi\left(x_{i}^{e_{i}}\right)\right), a_{i</p>
<p>11: for $i=1 \ldots N$ do
12: $\quad \bar{a}<em i="i">{i} \sim \operatorname{Gumbel} \operatorname{Softmax}\left(\pi\left(\cdot \mid \phi\left(x</em>\right)\right)\right)$
13: end for
14: Next state energy loss:}^{e_{i}</p>
<p>$$
\mathcal{L}<em i="1">{\text {energy }}=\frac{1}{N} \sum</em>}^{N} E_{\bar{\theta}}\left(\psi\left(g_{s}\left(\phi\left(x_{i}^{e_{i}}\right), \bar{a<em 0="0">{i}\right), g</em>\right)\right)\right)
$$}^{e_{i}}\left(\mu^{e_{i}}\left(x_{i}^{e_{i}}\right), \bar{a}_{i</p>
<p>15: Parameters update:
16: $\quad \theta_{s} \leftarrow \theta_{s}-\lambda \nabla_{\theta_{s}}\left(\mathcal{L}<em d="d" n="n" y="y">{\text {inv }}+\mathcal{L}</em>}+\mathcal{L<em _pi="\pi">{m i}+\mathcal{L}</em>\right)$
17: $\quad \theta_{g_{s}} \leftarrow \theta_{g_{s}}-\lambda \nabla_{\theta_{s}} \mathcal{L}<em _psi="\psi">{d y n}$
18: $\theta</em>} \leftarrow \theta_{\psi}-\lambda \nabla_{\theta_{\psi}} \mathcal{L<em _text="\text" _train="{train">{d y n}$
19: for $e \in \mathcal{E}</em>$ do
20: $\quad \theta_{q}^{e} \leftarrow \theta_{q}^{e}-\lambda \nabla_{\theta_{q}^{e}}\left(\mathcal{L}}<em i="i" m="m">{d y n}+\mathcal{L}</em>\right)$
21: $\quad \theta_{g_{q}}^{e} \leftarrow \theta_{g_{q}}^{e}-\lambda \nabla_{\theta_{g_{q}}^{e}} \mathcal{L}<em _pi="\pi">{d y n}$
22: end for
23: $\theta</em>} \leftarrow \theta_{\pi}-\lambda \nabla_{\theta_{\pi}}\left(\mathcal{L<em _energy="{energy" _text="\text">{\pi}+\mathcal{L}</em>\right)$
24: Environment classifier update: Used to define the invariance loss.
25: $\quad \mathcal{L}}<em i="1">{c}=\frac{1}{N} \sum</em>\right)\right.$
26: $\quad \theta_{c} \leftarrow \theta_{c}-\lambda \nabla_{\theta_{c}} \mathcal{L}}^{N} \operatorname{Cross} \operatorname{entropy}\left(c_{s}\left(\phi\left(x_{i}^{e_{i}}\right), e_{i<em _theta__m="\theta_{m">{c}$
27: Mutual information (MINE) update: $\triangleright$ Used to define the mutual information loss.
28: Update $T</em>}}$ by ascending the gradient of the mutual information loss: $\theta_{m} \leftarrow \theta_{m}+\nabla_{\theta_{m}} \mathcal{L<em s="s">{m i}$
29: end while
30: Output: Learnt parameters $\theta</em>\right}}, \theta_{g_{s}},\left{\theta_{q}^{e}, \theta_{g_{q}}^{e<em _text="\text" _train="{train">{e \in \mathcal{E}</em>$}}}, \theta_{\psi}, \theta_{c}, \theta_{\pi}, \theta_{m</p>
<h1>D Causal features for imitation using invariant risk minimization</h1>
<p>In the supervised learning setting, Invariant Risk Minimization (IRM) [6] leverage data from multiple domains to learn a data representation $\Phi: \mathcal{X} \rightarrow \mathcal{H}$ that elicits an invariant predictor $w: \mathcal{H} \rightarrow \mathcal{Y}$ across the different environments, where $\mathcal{X}$ and $\mathcal{Y}$ are the input and output spaces respectively. The training data from each environment $e \in \mathcal{E}$ corresponds to different interventions on the data generating process and $R^{e}$ corresponds to the empirical risk of the classifier in each domain. We use the following formal definition from [6] to describe the characteristics we want from the learnt representation.
Definition D.1. [6]: We say that a data representation $\Phi: \mathcal{X} \rightarrow \mathcal{H}$ elicits an invariant predictor across environments $\mathcal{E}$ if there is a classifier $w: \mathcal{H} \rightarrow \mathcal{A}$ simultaneously optimal for all environments, that is, $w=\arg \min _{\bar{w}: \mathcal{H} \rightarrow \mathcal{A}} R^{e}(\bar{w} \circ \Phi)$, for all $e \in \mathcal{E}$.</p>
<p>Given data from training environments $\mathcal{E}_{\text {train }}$, the IRM objective aims to find a representation $\Phi$ such that there exists a classifier $w$ that is optimal across all training domains:</p>
<p>$$
\min <em _in="\in" _mathcal_E="\mathcal{E" e="e">{\Phi: \mathcal{X} \rightarrow \mathcal{H}, w: \mathcal{H} \rightarrow \mathcal{Y}} \sum</em><em _bar_w="\bar{w">{\text {train }}} R^{e}(\bar{w} \circ \Phi) \text { subject to } w \in \arg \min </em>
$$}: \mathcal{H} \rightarrow \mathcal{A}} R^{e}(\bar{w} \circ \Phi), \forall e \in \mathcal{E}_{\text {train }</p>
<p>This represents a challenging, bi-level optimization and [6] propose the IRM-v1 objective which is a practical version to optimize:</p>
<p>$$
\min <em _in="\in" _mathcal_E="\mathcal{E" e="e">{\phi: \mathcal{X} \rightarrow \mathcal{H}} \sum</em><em _penalty="{penalty" _text="\text">{\text {train }}} R^{e}(\Phi)+\lambda</em>(w \cdot \Phi)\right|
$$}} \cdot\left|\nabla_{w \mid w=1.0} R^{e</p>
<p>Through this optimization, the IRM objective should learn a predictor that only uses the causal parents of the target variable and that is thus invariant across environments. In the supervised setting considered by IRM [6], $R^{e}$ is the risk of the classifier in environment $e$. For classification and regression problems, this can represent for instance the cross-entropy loss.
Imitation learning risk: We propose extending IRM to the imitation learning setting by using instead an imitation risk $R^{e}$ as described in equation 2. Moreover, in this setting, our aim is to find an invariant policy $\pi$ across the different environments (instead of the invariant classifier $w$ ). The risk $R^{e}$ will therefore be specific to the imitation learning algorithm used. For instance, in behaviour cloning (BC) [10], for categorical actions $R_{\mathrm{BC}}^{e}=$ Cross entropy $\left(\pi\left(\cdot \mid x_{t}\right), a_{t}\right)$. Alternatively, ValueDice(VDICE) [2] minimizes the disparity between the occupancy measure of the expert policy vs the imitator policy $R_{\text {VDICE }}^{e}=D_{K L}\left(\rho_{D}^{e} | \rho_{\pi}\right)$. We use the IRMv1 objective in conjunction with the following imitation learning algorithms Behaviour Cloning (BC) [10], Reward-regularized Classification for Apprenticeship Learning (RCAL) [14], ValueDice(VDICE) [2] and Energy-based Distribution Matching (EDM) [3].</p>
<table>
<thead>
<tr>
<th>Environments</th>
<th>Original Obs. Space</th>
<th>Action Space</th>
<th>Demonstrator</th>
<th>Random Perf.</th>
<th>Demonstrator Perf.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Acrobot-v1</td>
<td>Continuous (6)</td>
<td>Discrete (3)</td>
<td>PPO2 Agent</td>
<td>$-439.92 \pm 13.14$</td>
<td>$-87.32 \pm 12.02$</td>
</tr>
<tr>
<td>CartPole-v1</td>
<td>Continuous (4)</td>
<td>Discrete (2)</td>
<td>DQN Agent</td>
<td>$19.12 \pm 1.76$</td>
<td>$500.00 \pm 0.00$</td>
</tr>
<tr>
<td>LunarLander-v2</td>
<td>Continuous (8)</td>
<td>Discrete (4)</td>
<td>PPO2 Agent</td>
<td>$-452.22 \pm 61.24$</td>
<td>$271.71 \pm 17.88$</td>
</tr>
<tr>
<td>BeamRider-v4</td>
<td>Continuous</td>
<td>Discrete (9)</td>
<td>PPO2 Agent</td>
<td>$754.84 \pm 214.85$</td>
<td>$1623.80 \pm 482.27$</td>
</tr>
<tr>
<td>MIMIC-III</td>
<td>(210 $\times 160 \times 3)$</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: Environment details. The random and demonstrator performances are averaged over 1,000 episodes roll-outs.</p>
<h1>E Experimental details</h1>
<h2>E. 1 Environments details</h2>
<p>We use the following control tasks from OpenAI gym for experiments [47]: Acrobot [49], Cartpole [50], Lunar Lander [47] and BeamRider [51]. For each task, we use pre-trained RL agents from RL Baselines Zoo [52] and Stable OpenAI Baselines [53] to obtain expert policies. We provide in Table 3 details about the different environments used including the size of the observation and action space, the agent used as demonstrator, the demonstrator's performance (average return) as well as the performance of an agent randomly selecting actions. The performance on the OpenAI gym tasks is measured in terms of average return of running the agent in the environment.</p>
<p>Note that the expert uses the original observation space for each task. To create the different environments with spurious correlations used to train and test the imitation learning benchmarks on Acrobot [49], Cartpole [50] and Lunar Lander [47], we augment the observation space as follows. We add 3 noise variables that are different multiplicative factors of the last 3 variables in the original state space. For Acrobot, these are cosine of the second rotational angle and the two joint angular velocities, for CartPole these are Cart Velocity, Pole Angle, Pole Angular Velocity and for LunarLander these are Lander angular velocity, leg 1 ground contact and leg 2 ground contact. The invariant causal state is represented by the original variables in the state space of each control task. We train the benchmarks on demonstrations from two environments with $1 \times$ and $2 \times$ multiplicative factors for the spurious correlations and we test on an environments with multiplicative factors sampled from $\mathcal{U}(-1,1)$.</p>
<p>Alternatively, for BeamRider [51], we create the different environments for training and testing by using different camera angles, similarly to [7]. More specifically, we use 2 training environments where the game frames are rotated by 10 degrees to the left and to the right respectively and a test environment that does not have any rotation.</p>
<p>For each OpenAI Gym task, and for each benchmark, we obtain datasets with $N_{\text {traj }} \in$ ${1,5,10,15,20}$ demonstrated trajectories from each of the two training environments. We evaluate each benchmark in the test environment by computing the average return over 300 episodes roll-outs. We repeat each experiments 10 times, each time sampling different expert demonstrations for training.</p>
<p>In addition, we also use a dataset from the Medical Information Mart for Intensive Care (MIMIC-III) database [48]. For each patient, we extract 52 clinical covariates including vital signs (e.g. respiratory rate, heart rate, temperature, O2 saturation) and lab test (e.g. glucose, hemoglobin, magnesium, potassium, platelet count, white blood cell count) that are aggregated every hour during their ICU stay. We consider patient trajectories that are up to 24 hours. Moreover, we concatenate the last 4 hours to build the observations received by each imitation learning algorithm. The expert in this case is the doctor and we consider as action the ventilator support. We consider three environment each with 2000 independent patient trajectories from MIMIC III. Two of the environments are used for training and one for testing. We augment the original feature space by adding 20 spurious correlations (noise variables) that are the same as the expert actions with probabilities $p=0.1$ and $p=0.2$ in the training environments and with probability $p=0.8$ in the testing environment.</p>
<h1>E. 2 Implementation details</h1>
<p>Similarly to [3] and for a fair comparison, whenever possible, we use the same policy network architecture for all imitation learning benchmarks. For Acrobot [49], Cartpole [50], Lunar Lander [47] and MIMIC-III [48] we use a policy network consisting of two fully-connected hidden layers with 64 units each and with ELU activation. Alternatively, for BeamRider [51], we use as the policy network a convolutional neural network with 3 convolutional layers consisting of 32-64-64 filters, followed by a fully connected layer of size 64, with all layers followed by ReLU activations.
We consider discrete actions in all environments; thus, the output layer of the policy network has the same number of dimensions as the action space. For all the different environments used for evaluation we optimize the parameters using the Adam Optimizer for 10k iterations with learning rate $\lambda=0.001$ and batch size 64 [3]. Moreover, we use the publicly available code for the different benchmarks used and other than the standardized policy network, we keep the optimal hyperparameters in the original implementations.
The experiments were run on a system with 6CPUs, an Nvidia K80 Tesla GPU and 56GB of RAM.
Invariant Causal Imitation Learning: uses a policy network as described above and neural network architectures with two fully connected hidden layers with 64 units and with ELU activation for each of $\phi, \mu^{e}, g_{s}, g_{\eta}, \psi, c_{s}$ and $T_{\theta_{m}}$ in the Acrobot [49], Cartpole [50], Lunar Lander [47] and MIMIC-III [48] experiments. On the other hand, for the BeamRider environment [51], we use a convolutional neural network with 3 convolutional layers consisting of 32-64-64 filters, followed by a fully connected layer of size 64, with all layers followed by ReLU activations for $\phi$ and $\mu^{e}$. Moreover, we use neural network architectures with two fully connected hidden layers with 64 units and with ELU activation for $g_{s}, g_{\eta}, c_{s}, T_{\theta_{m}}$ and for the policy network $\pi$. Finally, for $\psi$ we use a neural network with 2 fully connected layers of 64 and $64 \times 7 \times 7$ hidden units followed by 3 transposed convolution layers consisting of 64-64-32 filters.
Behaviour cloning (BC): We implement behaviour cloning by using a policy network architecture as described above. We train the model using cross entropy loss and we optimize it as described above. We use the same hyperparameters for BC-IRM but instead use IRM-v1 objective.
Reward-regularized Classification for Apprenticeship Learning (RCAL): We implement RCAL by adding a sparsity-based loss on the implied rewards [14] and we set the sparsity-based regularization coefficient to 0.01 . We use the same policy network architecture for optimization procedure as described above. Moreover, we use the same hyperparameters for RCAL-IRM but instead use IRM-v1 objective.
Energy-based Distribution Matching (EDM): We use the publicly available implementation for EDM [59] from here: https://github.com/vanderschaarlab/mlforhealthlabpub. We use the same policy network and optimization procedure as above, which corresponds to the ones used in [59]. Moreover, following the implementation details provided in [59] we set the joint EBM training hyperparameters to noise coefficient $\sigma=0.01$, buffer size $\kappa=10000$, length $l=20$, re-initialization $\delta=0.05$ and SGLD step size $\alpha=0.01$. For EDM-IRM we use the same hyperparameters, but instead optimize the IRM-v1 objective.
ValueDice (VDICE): We use the publicly available implementation for VDICE [2] from https : //github.com/google-research/google-research/tree/master/value_dice. However, to adapt the model to discrete actions we modify the last layer of the actor network to use Gumbelsoftmax. For Acrobot [49], Cartpole [50], Lunar Lander [47] and MIMIC-III [48], the actor and discriminator network architecture used have two fully connected hidden layers with 64 units and ReLU activation. Conversely, for BeamRider [51] the actor and discriminator networks consist of 3 convolutional layers consisting of 32-64-64 filters, followed by a fully connected layer of size 64, with all layers followed by ReLU activations. As described in [2], orthogonal regularization is used for the actor and a learning rate of 0.00001 . The discriminator uses a learning rate of 0.001 . For VDICE-IRM we use the same hyperparameters, but instead optimize the IRM-v1 objective.</p>
<h1>F Additional experiments</h1>
<h2>F. 1 Ablations</h2>
<p>To understand the impact of the different components in the overall loss function used to train ICIL, we performed an ablation experiment on the CartPole [50] control task from OpenAI gym [47]. We follow the same training and testing set-up described in Section 5.1.
Let $L=L_{\pi}+L_{\text {dyn }}+L_{\text {inv }}+L_{\text {mi }}+L_{\text {energy }}$ be the full loss function used for training ICIL. Refer to Section 4.1 for details of how each component in $L$ is defined. The results in Figure 5 illustrate the impact of removing different terms from this loss function on overall performance (average return of the learnt imitation policy in the test environment). The average return is scaled between 1 (expert performance) and 0 (random policy performance). The setting of only using $L_{\pi}$ corresponds to the Behaviour Cloning (BC) benchmark.</p>
<p>We notice that while each term in the loss $L$ used to train ICIL is important for the overall performance, the loss term $L_{\text {inv }}$ which ensures that the state representation is invariant across environments plays the most significant role on the performance in the test environment. This is due to the fact that $L_{\text {inv }}$ is crucial for learning the shared latent structure across the different environments that consists of the causal parents of the actions.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Evaluating the impact of the different loss components on overall performance for the CartPole control task. $x$-axis indicates the number of trajectories (in ${1,5,10,15,20}$ ) with expert demonstrations from each training environment given as input to each ablated version of ICIL and $y$-axis represents average return of the learnt imitation policy on the test environments, scaled between 1 (expert performance) and 0 (random policy performance).</p>
<h2>F. 2 Train vs. test performance</h2>
<p>We report here the evaluation metrics for both training and testing environments on the CartPole [50] control task. We follow the same experimental set-up described in Section 5.1. In Figure 6 we report the performance of ICIL and BC when evaluated both on 300 new episode roll-outs from one of the environments used for training as well as when evaluated on a test environment with different multiplicative factors for the noise variables. We notice that the imitation policy learnt by BC relies on the spurious correlations and thus fails to generalize beyond the environments it has been exposed to. On the other hand, ICIL learns an imitation policy that correctly depends on the state variables, which are the ones that are being shared across the different environments.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Train vs. test performance on CartPole. $x$-axis indicates the number of trajectories (in ${1,5,10,15,20}$ ) with expert demonstrations from each training environment given as input to each benchmark and $y$-axis represents average return of the learnt imitation policy when evaluated on both the train and test environments, scaled between 1 (expert performance) and 0 (random policy performance).</p>
<h1>F. 3 Robustness to increasing the size of spurious correlations</h1>
<p>Finally, we investigate the robustness of the different imitation learning methods to increasing the size of the spurious correlations (i.e. the number of noise variables in each environment). In this case, we consider again the CartPole [50] control task and the setting where 5 trajectories from each training environment are given as input to each benchmark during training. The spurious correlations in each environment are different multiplicative factors of the last 3 variables in the original state space of the control task. We follow the same set-up described in Appendix E for setting the multiplicative factors for the train and test environments. Figure 7 illustrates the performance of ICIL, BC and EDM when increasing the number of noise variables used for the different environments. We notice that ICIL is robust to having more spurious correlations, while the performance of BC and EDM degrades more significantly in the case where the observations from the expert demonstrations have a large number of noise variables.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Robustness to increasing the number of noise variables. $x$-axis indicates the number of noise variables (in ${3,6,9,12}$ ) that are part of the observations in each environment and $y$-axis represents average return of the learnt imitation policy on the test environments, scaled between 1 (expert performance) and 0 (random policy performance).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
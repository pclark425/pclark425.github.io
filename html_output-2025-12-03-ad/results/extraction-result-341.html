<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-341 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-341</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-341</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-264146047</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.10103v1.pdf" target="_blank">Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning</a></p>
                <p><strong>Paper Abstract:</strong> Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics -- e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. Therefore, in this paper we study how the ``semantic guesswork'' produced by language models can be utilized as a guiding heuristic for planning algorithms. Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e341.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e341.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LFG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Frontier Guide</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses ungrounded large language models as a semantic heuristic to score candidate frontier subgoals (textualized by a vision-language model) and bias a conventional planner for goal-directed exploration in novel environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified large language model (API; e.g., GPT-family / PaLM / Claude)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs are queried via API with in-context examples and chain-of-thought prompts; the LLM is not trained or fine-tuned in this work. The system polls the LLM multiple times (sampling) under both positive and negative prompts to empirically estimate likelihoods that textual subgoal descriptors will lead to a language-specified goal. These LLM scores are combined with geometric/topological map information as a heuristic for a search-based planner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Object-goal navigation (ObjectNav) / goal-directed exploration</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agent receives a natural-language goal (e.g., 'find the toilet') and must explore an unfamiliar indoor environment (simulated HM3D and real apartments/cafeteria) to find an instance of the queried object. The agent maintains episodic memory as either a 2D metric semantic map or a topological map of visual observations with object labels; frontiers (unexplored boundary points) are proposed and scored to select the next subgoal for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation (goal-directed visual exploration / ObjectNav)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (room-adjacency priors, object co-occurrence and typical object-room relationships); procedural knowledge is used only implicitly as a heuristic for planning rather than explicit action sequences</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (LLM semantic priors) combined with in-context examples and prompting; perceptual grounding via a vision-language model (VLM) that converts visual object clusters into textual subgoal descriptors</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>in-context prompting with structured examples, chain-of-thought (CoT) prompting, positive and negative prompts, and sampling/polling to produce empirical likelihood estimates</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit semantic priors encoded in LLM weights and expressed as natural-language narratives/reasoning; subgoals are represented as textual descriptors produced by a VLM; episodic map stored externally as metric or topological map (explicit spatial representation) and combined with LLM-derived scalar heuristic scores</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (percentage of episodes that find the goal) and SPL (Success weighted by Path Length)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On the HM3D ObjectNav benchmark LFG achieved 68.9% success and 36.0 SPL (simulated agent with ground-truth semantic detections). In real-world topological experiments LFG outperformed a greedy-LLM baseline by ~16% absolute in success rate (specific real-world absolute numbers depend on environment and task).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively uses object-relational priors and coarse spatial/layout priors (e.g., kitchen appliances co-occurring; dining area near kitchen; bathroom near bedroom) to prioritize frontiers that are semantically more likely to lead to the queried object, reducing exploration time; negative prompting allowed it to de-prioritize frontiers unlikely to contain the goal.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>The LLM narratives are ungrounded and can be arbitrarily wrong about a specific environment's layout (hallucinated room relationships or incorrect object placements), causing poor frontier ranking if relied on alone; greedy use of LLM plans fails more often. LLMs do not encode precise geometry, obstacle awareness, or reliable multi-step action sequences tied to the observed environment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines include unguided Frontier-Based Exploration (FBE) (61.1% success, 34.0 SPL), greedy LLM methods (54.4% success), L3MVN (62.4% success), and several learning-based agents (DD-PPO: ~27.9% success reported in table). LFG (68.9% success, 36.0 SPL) outperforms other LLM-based baselines and most learning-based baselines reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key ablations on HM3D: removing chain-of-thought prompting (No CoT) reduced success from 68.9% to 62.3% (−6.6%); using only positive prompts reduced to 64.2% (−4.7%); replacing polling scoring with random scoring gave 61.1% (−7.8%); using logprobs instead of polling gave 60.4% (−8.5%); an 'Ask' variant scored 62.4% (−6.5%). These show polling, CoT, and positive+negative prompts are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs store broad semantic and object-relational priors (co-occurrence and room-adjacency knowledge) that can be extracted via prompting (CoT, positive/negative prompts) and used as a robust heuristic to bias exploration, even though the LLM lacks direct sensory grounding. Polling-based empirical likelihood estimation + CoT yields reliable scores; using LLM outputs as heuristics (not direct plans) lets a conventional planner recover from LLM errors and improves navigation performance in novel indoor environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e341.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e341.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polling + CoT scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Polling-based likelihood estimation with chain-of-thought prompting (positive & negative prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to extract task-relevant likelihoods from an ungrounded LLM by repeatedly sampling structured answers under both 'which subgoal is most likely' and 'which is least likely' prompts, using chain-of-thought to improve consistency, then forming empirical positive and negative scores to rank textual subgoal descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM (API; examples mentioned: OpenAI ChatGPT/GPT-3.5/GPT-4, PaLM, Claude - noted as APIs that often do not expose logprobs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>No model weights or sizes are specified; the approach is model-agnostic and designed to work with large API-hosted LLMs that support sampling but may not expose token log-probabilities. The method leverages CoT and structured answer formats to force the model to select from a pre-defined set of textual subgoal descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subgoal scoring for heuristic planning in goal-directed exploration</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a set of textual subgoal descriptors (object clusters from the episodic map) and a language goal, repeatedly sample the LLM under positive and negative prompts to obtain counts/frequencies of chosen subgoals; use these empirical frequencies (positive minus negative) as a probabilistic score p(s_i, q, M) estimating how likely each frontier will lead to the goal.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation (heuristic estimation for exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial priors (semantic guesswork about which object clusters or rooms are likely/ unlikely to contain the queried object); not geometric detail</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>implicit knowledge in LLM pre-training on text corpora and in-context examples provided in the prompt; no fine-tuning on embodied data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>chain-of-thought prompting, structured in-context examples, positive and negative prompts, repeated sampling / polling to estimate frequencies</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit semantic priors in LLM weights expressed as natural-language reasoning sequences; empirical scalar likelihoods derived from sampling frequency represent the extracted knowledge for downstream planners</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Contribution to downstream planner performance (success rate, SPL) and ablation impact on those metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Inclusion of polling+CoT in scoring increased HM3D success from ablated variants (e.g., No CoT: 62.3% vs full 68.9%). Using polling rather than logprobs improved success by ~7–8% absolute in reported ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Produces consistent and interpretable justifications (CoT) that capture common-sense object-room relationships and object co-occurrence patterns, enabling robust ranking among semantically-distinct subgoal candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When LLM priors mismatch actual layout, sampled narratives can still favor incorrect frontiers; sampling is slow (multiple API calls) and depends on LLM stochasticity; tokenization/logprob approaches were less robust than polling and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to naive logprob scoring (where logprobs are used directly) and greedy LLM plan selection, polling+CoT gave substantially better downstream navigation (logprobs ablation: −8.5% success; greedy LLM: −14.5% vs full LFG).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>See LFG ablations: removing CoT (−6.6%), using only positives (−4.7%), replacing polling/logprobs (logprobs gave −8.5%), showing polling+CoT with positive+negative prompts is the most effective configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Empirical polling of structured LLM outputs under CoT and both positive/negative prompts yields more reliable, interpretable, and useful task-relevant likelihoods than relying on raw token logprobs or greedy plan outputs. These extracted likelihoods successfully capture object-relational priors that can be converted into scalar heuristics for embodied planners even though the LLM has no direct sensory grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e341.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e341.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM grounding (used in LFG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-Language Model grounding of subgoal descriptors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model (or ground-truth semantic labels in simulation) converts visual observations around frontier points into compact textual subgoal descriptors (object clusters) that are provided to the LLM for semantic scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>vision-language model (unspecified; DETIC referenced as a general VLM; ground-truth semantics used in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In sim experiments, the system uses ground-truth semantic labels; in real-world experiments an off-the-shelf object detector (20k-class detector referenced) annotates n_v = 4 omnidirectional views and objects are clustered into textual descriptors for each frontier.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Perceptual grounding of frontier subgoals</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>At each frontier, objects detected by the VLM are aggregated into clusters (agglo clustering for geometric maps or view-based grouping for topological maps), producing a short textual list of objects that describe that frontier, which is then used as the candidate s_i for LLM scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception + grounding for navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (object lists, categories) used to infer likely room semantics / spatial priors</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained VLM / object detector; ground-truth labels in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>object detection followed by agglomerative clustering or view-based grouping to create textual descriptors</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit textual object-cluster descriptors derived from visual detections; these descriptors feed the LLM which holds implicit priors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream navigation success and SPL when combined with LLM scoring</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using accurate semantic descriptors (ground-truth) in simulation allowed LFG to demonstrate its heuristic benefits (68.9% success). Real-world performance depends on detector quality; whitelist of ~20k classes used to improve prompt focus.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Accurate object clusters allowed LLM to reason about room types (e.g., 'refrigerator + microwave + sink' -> kitchen) and thus direct exploration efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Noisy or erroneous detections can mislead LLM scoring; however, planner fallback (FBE) can recover in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Simulation used perfect perception (ground-truth) to isolate heuristic effect; real-world used detector with a whitelist to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper reports that perception quality affects outcomes; simulation with ground-truth semantic maps shows stronger benefit of LFG than if perception is noisy (qualitative examples shown).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantic grounding via a VLM (or ground-truth labels) is necessary to convert visual frontiers into the textual inputs the LLM needs; the combination of explicit perceptual descriptors and implicit LLM priors is what enables semantic guesswork to guide exploration without the LLM having direct sensory access.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LM-nav: Robotic navigation with large pretrained models of language, vision, and action <em>(Rating: 2)</em></li>
                <li>L3MVN: Leveraging large language models for visual target navigation <em>(Rating: 2)</em></li>
                <li>Can an Embodied Agent Find Your "Catshaped Mug"? LLM-Based Zero-Shot Object Navigation <em>(Rating: 2)</em></li>
                <li>Grounded decoding: Guiding text generation with grounded models for robot control <em>(Rating: 1)</em></li>
                <li>PALM-E: An embodied multimodal language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-341",
    "paper_id": "paper-264146047",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "LFG",
            "name_full": "Language Frontier Guide",
            "brief_description": "A method that uses ungrounded large language models as a semantic heuristic to score candidate frontier subgoals (textualized by a vision-language model) and bias a conventional planner for goal-directed exploration in novel environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "unspecified large language model (API; e.g., GPT-family / PaLM / Claude)",
            "model_size": null,
            "model_description": "LLMs are queried via API with in-context examples and chain-of-thought prompts; the LLM is not trained or fine-tuned in this work. The system polls the LLM multiple times (sampling) under both positive and negative prompts to empirically estimate likelihoods that textual subgoal descriptors will lead to a language-specified goal. These LLM scores are combined with geometric/topological map information as a heuristic for a search-based planner.",
            "task_name": "Object-goal navigation (ObjectNav) / goal-directed exploration",
            "task_description": "Agent receives a natural-language goal (e.g., 'find the toilet') and must explore an unfamiliar indoor environment (simulated HM3D and real apartments/cafeteria) to find an instance of the queried object. The agent maintains episodic memory as either a 2D metric semantic map or a topological map of visual observations with object labels; frontiers (unexplored boundary points) are proposed and scored to select the next subgoal for navigation.",
            "task_type": "navigation (goal-directed visual exploration / ObjectNav)",
            "knowledge_type": "spatial + object-relational (room-adjacency priors, object co-occurrence and typical object-room relationships); procedural knowledge is used only implicitly as a heuristic for planning rather than explicit action sequences",
            "knowledge_source": "pre-training on large text corpora (LLM semantic priors) combined with in-context examples and prompting; perceptual grounding via a vision-language model (VLM) that converts visual object clusters into textual subgoal descriptors",
            "has_direct_sensory_input": false,
            "elicitation_method": "in-context prompting with structured examples, chain-of-thought (CoT) prompting, positive and negative prompts, and sampling/polling to produce empirical likelihood estimates",
            "knowledge_representation": "implicit semantic priors encoded in LLM weights and expressed as natural-language narratives/reasoning; subgoals are represented as textual descriptors produced by a VLM; episodic map stored externally as metric or topological map (explicit spatial representation) and combined with LLM-derived scalar heuristic scores",
            "performance_metric": "Success rate (percentage of episodes that find the goal) and SPL (Success weighted by Path Length)",
            "performance_result": "On the HM3D ObjectNav benchmark LFG achieved 68.9% success and 36.0 SPL (simulated agent with ground-truth semantic detections). In real-world topological experiments LFG outperformed a greedy-LLM baseline by ~16% absolute in success rate (specific real-world absolute numbers depend on environment and task).",
            "success_patterns": "Effectively uses object-relational priors and coarse spatial/layout priors (e.g., kitchen appliances co-occurring; dining area near kitchen; bathroom near bedroom) to prioritize frontiers that are semantically more likely to lead to the queried object, reducing exploration time; negative prompting allowed it to de-prioritize frontiers unlikely to contain the goal.",
            "failure_patterns": "The LLM narratives are ungrounded and can be arbitrarily wrong about a specific environment's layout (hallucinated room relationships or incorrect object placements), causing poor frontier ranking if relied on alone; greedy use of LLM plans fails more often. LLMs do not encode precise geometry, obstacle awareness, or reliable multi-step action sequences tied to the observed environment.",
            "baseline_comparison": "Baselines include unguided Frontier-Based Exploration (FBE) (61.1% success, 34.0 SPL), greedy LLM methods (54.4% success), L3MVN (62.4% success), and several learning-based agents (DD-PPO: ~27.9% success reported in table). LFG (68.9% success, 36.0 SPL) outperforms other LLM-based baselines and most learning-based baselines reported in the paper.",
            "ablation_results": "Key ablations on HM3D: removing chain-of-thought prompting (No CoT) reduced success from 68.9% to 62.3% (−6.6%); using only positive prompts reduced to 64.2% (−4.7%); replacing polling scoring with random scoring gave 61.1% (−7.8%); using logprobs instead of polling gave 60.4% (−8.5%); an 'Ask' variant scored 62.4% (−6.5%). These show polling, CoT, and positive+negative prompts are critical.",
            "key_findings": "LLMs store broad semantic and object-relational priors (co-occurrence and room-adjacency knowledge) that can be extracted via prompting (CoT, positive/negative prompts) and used as a robust heuristic to bias exploration, even though the LLM lacks direct sensory grounding. Polling-based empirical likelihood estimation + CoT yields reliable scores; using LLM outputs as heuristics (not direct plans) lets a conventional planner recover from LLM errors and improves navigation performance in novel indoor environments.",
            "uuid": "e341.0",
            "source_info": {
                "paper_title": "Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Polling + CoT scoring",
            "name_full": "Polling-based likelihood estimation with chain-of-thought prompting (positive & negative prompts)",
            "brief_description": "A technique to extract task-relevant likelihoods from an ungrounded LLM by repeatedly sampling structured answers under both 'which subgoal is most likely' and 'which is least likely' prompts, using chain-of-thought to improve consistency, then forming empirical positive and negative scores to rank textual subgoal descriptors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "unspecified LLM (API; examples mentioned: OpenAI ChatGPT/GPT-3.5/GPT-4, PaLM, Claude - noted as APIs that often do not expose logprobs)",
            "model_size": null,
            "model_description": "No model weights or sizes are specified; the approach is model-agnostic and designed to work with large API-hosted LLMs that support sampling but may not expose token log-probabilities. The method leverages CoT and structured answer formats to force the model to select from a pre-defined set of textual subgoal descriptors.",
            "task_name": "Subgoal scoring for heuristic planning in goal-directed exploration",
            "task_description": "Given a set of textual subgoal descriptors (object clusters from the episodic map) and a language goal, repeatedly sample the LLM under positive and negative prompts to obtain counts/frequencies of chosen subgoals; use these empirical frequencies (positive minus negative) as a probabilistic score p(s_i, q, M) estimating how likely each frontier will lead to the goal.",
            "task_type": "navigation (heuristic estimation for exploration)",
            "knowledge_type": "object-relational + spatial priors (semantic guesswork about which object clusters or rooms are likely/ unlikely to contain the queried object); not geometric detail",
            "knowledge_source": "implicit knowledge in LLM pre-training on text corpora and in-context examples provided in the prompt; no fine-tuning on embodied data",
            "has_direct_sensory_input": false,
            "elicitation_method": "chain-of-thought prompting, structured in-context examples, positive and negative prompts, repeated sampling / polling to estimate frequencies",
            "knowledge_representation": "Implicit semantic priors in LLM weights expressed as natural-language reasoning sequences; empirical scalar likelihoods derived from sampling frequency represent the extracted knowledge for downstream planners",
            "performance_metric": "Contribution to downstream planner performance (success rate, SPL) and ablation impact on those metrics",
            "performance_result": "Inclusion of polling+CoT in scoring increased HM3D success from ablated variants (e.g., No CoT: 62.3% vs full 68.9%). Using polling rather than logprobs improved success by ~7–8% absolute in reported ablations.",
            "success_patterns": "Produces consistent and interpretable justifications (CoT) that capture common-sense object-room relationships and object co-occurrence patterns, enabling robust ranking among semantically-distinct subgoal candidates.",
            "failure_patterns": "When LLM priors mismatch actual layout, sampled narratives can still favor incorrect frontiers; sampling is slow (multiple API calls) and depends on LLM stochasticity; tokenization/logprob approaches were less robust than polling and CoT.",
            "baseline_comparison": "Compared to naive logprob scoring (where logprobs are used directly) and greedy LLM plan selection, polling+CoT gave substantially better downstream navigation (logprobs ablation: −8.5% success; greedy LLM: −14.5% vs full LFG).",
            "ablation_results": "See LFG ablations: removing CoT (−6.6%), using only positives (−4.7%), replacing polling/logprobs (logprobs gave −8.5%), showing polling+CoT with positive+negative prompts is the most effective configuration.",
            "key_findings": "Empirical polling of structured LLM outputs under CoT and both positive/negative prompts yields more reliable, interpretable, and useful task-relevant likelihoods than relying on raw token logprobs or greedy plan outputs. These extracted likelihoods successfully capture object-relational priors that can be converted into scalar heuristics for embodied planners even though the LLM has no direct sensory grounding.",
            "uuid": "e341.1",
            "source_info": {
                "paper_title": "Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "VLM grounding (used in LFG)",
            "name_full": "Vision-Language Model grounding of subgoal descriptors",
            "brief_description": "A vision-language model (or ground-truth semantic labels in simulation) converts visual observations around frontier points into compact textual subgoal descriptors (object clusters) that are provided to the LLM for semantic scoring.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "vision-language model (unspecified; DETIC referenced as a general VLM; ground-truth semantics used in simulation)",
            "model_size": null,
            "model_description": "In sim experiments, the system uses ground-truth semantic labels; in real-world experiments an off-the-shelf object detector (20k-class detector referenced) annotates n_v = 4 omnidirectional views and objects are clustered into textual descriptors for each frontier.",
            "task_name": "Perceptual grounding of frontier subgoals",
            "task_description": "At each frontier, objects detected by the VLM are aggregated into clusters (agglo clustering for geometric maps or view-based grouping for topological maps), producing a short textual list of objects that describe that frontier, which is then used as the candidate s_i for LLM scoring.",
            "task_type": "perception + grounding for navigation",
            "knowledge_type": "object-relational (object lists, categories) used to infer likely room semantics / spatial priors",
            "knowledge_source": "pre-trained VLM / object detector; ground-truth labels in simulation",
            "has_direct_sensory_input": true,
            "elicitation_method": "object detection followed by agglomerative clustering or view-based grouping to create textual descriptors",
            "knowledge_representation": "explicit textual object-cluster descriptors derived from visual detections; these descriptors feed the LLM which holds implicit priors",
            "performance_metric": "Downstream navigation success and SPL when combined with LLM scoring",
            "performance_result": "Using accurate semantic descriptors (ground-truth) in simulation allowed LFG to demonstrate its heuristic benefits (68.9% success). Real-world performance depends on detector quality; whitelist of ~20k classes used to improve prompt focus.",
            "success_patterns": "Accurate object clusters allowed LLM to reason about room types (e.g., 'refrigerator + microwave + sink' -&gt; kitchen) and thus direct exploration efficiently.",
            "failure_patterns": "Noisy or erroneous detections can mislead LLM scoring; however, planner fallback (FBE) can recover in many cases.",
            "baseline_comparison": "Simulation used perfect perception (ground-truth) to isolate heuristic effect; real-world used detector with a whitelist to reduce noise.",
            "ablation_results": "Paper reports that perception quality affects outcomes; simulation with ground-truth semantic maps shows stronger benefit of LFG than if perception is noisy (qualitative examples shown).",
            "key_findings": "Semantic grounding via a VLM (or ground-truth labels) is necessary to convert visual frontiers into the textual inputs the LLM needs; the combination of explicit perceptual descriptors and implicit LLM priors is what enables semantic guesswork to guide exploration without the LLM having direct sensory access.",
            "uuid": "e341.2",
            "source_info": {
                "paper_title": "Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LM-nav: Robotic navigation with large pretrained models of language, vision, and action",
            "rating": 2,
            "sanitized_title": "lmnav_robotic_navigation_with_large_pretrained_models_of_language_vision_and_action"
        },
        {
            "paper_title": "L3MVN: Leveraging large language models for visual target navigation",
            "rating": 2,
            "sanitized_title": "l3mvn_leveraging_large_language_models_for_visual_target_navigation"
        },
        {
            "paper_title": "Can an Embodied Agent Find Your \"Catshaped Mug\"? LLM-Based Zero-Shot Object Navigation",
            "rating": 2,
            "sanitized_title": "can_an_embodied_agent_find_your_catshaped_mug_llmbased_zeroshot_object_navigation"
        },
        {
            "paper_title": "Grounded decoding: Guiding text generation with grounded models for robot control",
            "rating": 1,
            "sanitized_title": "grounded_decoding_guiding_text_generation_with_grounded_models_for_robot_control"
        },
        {
            "paper_title": "PALM-E: An embodied multimodal language model",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        }
    ],
    "cost": 0.01298325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning
16 Oct 2023</p>
<p>Dhruv Shah 
Michael Equi 
Blazej Osinski 
University of Warsaw</p>
<p>Fei Xia 
Brian Ichter 
Sergey Levine 
U C Berkeley 
Google Deepmind </p>
<p>CoRL 2023)
AtlantaUSA</p>
<p>Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning
16 Oct 2023886A5CA0C6755B40105B77EFBB01DDE0arXiv:2310.10103v1[cs.RO]navigationlanguage modelsplanningsemantic scene understanding
Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration.Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics -e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth.Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong.Therefore, in this paper we study how the "semantic guesswork" produced by language models can be utilized as a guiding heuristic for planning algorithms.Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps.We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.</p>
<p>Introduction</p>
<p>Navigation in complex open-world environments is conventionally viewed as the largely geometric problem of determining collision-free paths that traverse the environment from one location to another.However, real-world environments possess semantics.Imagine navigating an airport to get to a terminal: your prior knowledge about the way such buildings are constructed provides extensive guidance, even if this particular airport is unfamiliar to you.Large language models (LLMs) and various language embedding techniques have been studied extensively as ways to interpret the semantics in user-specified instructions (e.g., parsing "go to the television in the living room" and grounding it in a specific spatial location), but such models can provide much more assistance in robotic navigation scenarios by capturing rich semantic knowledge about the world.For instance, when looking for a spoon in an unseen house, the LLM can produce a "narrative" explaining why going towards a dishwasher may eventually lead you to find the spoon, and that the robot should prioritize that direction.This is similar to how a person might imagine different ways that the available subgoals might lie on the path to the goal, and start exploring the one for which this "narrative" seems most realistic.However, since language models are not grounded in the real world, such models do not know the spatial layout of the robot's surroundings (e.g., there is a couch that the robot Query: Find the gas stove.LLM as Planner LFG "Look for the gas stove in the kitchen."</p>
<p>Go Straight</p>
<p>"I see a refrigerator and microwave in front of me.These appliances are usually in the kitchen, and gas stoves are in the kitchen.</p>
<p>Let's explore in this direction!""I see a door to my left that looks like a bedroom.People don't keep gas stoves in the bedroom.</p>
<p>Let's avoid this region." needs to circumnavigate).To utilize the semantic knowledge in language models to aid in embodied tasks, we should not just blindly follow the language model suggestions, but instead use them as proposals or navigational heuristics.In this paper, we study how that might be accomplished.</p>
<p>We study this idea in the context of visual navigation, where a robot is tasked with reaching a goal denoted by a natural language query q (see Fig. 1) in a novel environment using visual observations.The robot has no prior experience in the target environment, and must explore the environment to look for the goal.While narratives generated by an LLM may not be sufficient for navigation by themselves, they contain useful cues that can be used to inform or guide the behavior of the underlying navigation stack for the language navigation task (e.g., by choosing between collisionfree subgoal proposals that avoid the couch and lead to the ice tray).We show that this idea can be combined with frontier-based exploration, where the robot maintains a set of unvisited locations at its frontier, grounds them in text using a vision-language model (VLM), and scores the unvisited subgoals by using LLM reasoning.</p>
<p>We propose Language Frontier Guide, or LFG, a method for leveraging the reasoning capabilities of LLMs to produce a search heuristic for guiding exploration of previously unseen real-world environments, combining the strengths of search-based planning with LLM reasoning.LFG is agnostic of the memory representation and planning framework, and can be combined with both (i) a geometric navigation pipeline, building a metric map of the environment for planning and using a hand-designed controller, as well as (ii) a learning-based navigation pipeline, building a topological map for planning and using a learned control policy, yielding a versatile system for navigating to open-vocabulary natural language goals.Our experiments show that LFG can identify and predict simple patterns in previously unseen environments to accelerate goal-directed exploration.We show that LFG outperforms other LLM-based approaches for semantic goal-finding in challenging real-world environments and on the Habitat ObjectNav benchmark.</p>
<p>Related Work</p>
<p>Vision-based navigation: Navigation is conventionally approached as a largely geometric problem, where the aim is to map an environment and use that map to find a path to a goal location [1].</p>
<p>Learning-based approaches can exploit patterns in the training environments, particularly by learning vision-based navigation strategies through reinforcement or imitation [2][3][4][5][6][7].Our work is also related to PONI [7], which uses a learned potential function to prioritize frontier points to explore; instead, we use a language model to rank these points.Notably, these methods do not benefit from prior semantic knowledge (e.g., from the web), and must rely entirely on patterns discovered from offline or online navigational data.Our aim is specifically to bring semantic knowledge into navigation, to enable robots to more effectively search for a goal in a new environment.</p>
<p>Semantic knowledge-guided navigation: Prior knowledge about the semantics of indoor environments can provide significantly richer guidance.With the advent of effective open-vocabulary vision models [8,9], some works have recently explored incorporating their semantic knowledge into models for navigation and other robotic tasks with the express aim of improving performance at instruction following [10][11][12][13][14].In general within robotics, such methods have either utilized pre-trained vision-language representations [15][16][17], or used language models directly to make decisions [18][19][20][21][22][23].Our aim is somewhat different: while we also focus on language-specified goals, we are primarily concerned with utilizing the semantics in pre-trained language models to help a robot figure out how to actually reach the goal, rather than utilizing the language models to more effectively interpret a language instruction.While language models can output reasonable substeps for temporally extended tasks in some settings [24,25], there is contradictory evidence about their ability to actually plan [26], and because they are unaware of the observations and layout in a particular environment, their "plans" depend entirely on the context that is provided to them.In contrast to prior work, our approach does not rely on the language model producing a good plan, but merely a heuristic that can bias a dedicated planner to reach a goal more effectively.In this way, we use the language models more to produce suggestions rather than actual plans.</p>
<p>LLM-guided navigation: Some works have sought to combine predictions from language models with either planning or probabilistic inference [14,27], so as to not rely entirely on forward prediction from the language model to take actions.However, these methods are more aimed at filtering out infeasible decisions, for example by disallowing actions that a robot is incapable of performing, and still focus largely on being able to interpret and process instructions, rather than using the language model as a source of semantic hints.In contrast, by incorporating language model suggestions as heuristics into a heuristic planner, our approach can completely override the language model predictions if they are incorrect, while still making use of them if they point the way to the goal.</p>
<p>Another branch of recent research [28][29][30] has taken a different approach to ground language models, by making it possible for them to read in image observations directly.While this represents a promising alternative approach to make language models more useful for embodied decision making, we believe it is largely orthogonal and complementary to our work: although vision-language models can produce more grounded inferences about the actions a robot should take, they are still limited only to guessing when placed in unfamiliar environments.Therefore, although we use ungrounded language-only models in our evaluation, we expect that our method could be combined with vision-language models easily, and would provide complementary benefits.</p>
<p>Problem Formulation and Overview</p>
<p>Our objective is to design a high-level planner that takes as input a natural language query q (e.g., "find the bedside table "), explores the environment in search of the queried object, and commands a low-level policy to control a robotic agent.To do this, we maintain an episodic memory of the environment M in the form of either (i) a 2D map of the environment, where grid cells contain information about occupancy and semantic labels, or (ii) a topological map of the environment, where nodes contain images captured by the robot and corresponding object labels.One way to solve this task is Frontier-Based Exploration (FBE) [31], where a robot maintains a set of unexplored frontiers in it's memory, and explores randomly to reach the goal.Can we do better with access to LLMs?</p>
<p>We distill the language-guided exploration task to a heuristic-based search problem, where the robot must propose unvisited subgoals or waypoints, score them, and then use a search algorithm (e.g., A*) to plan a path to the goal.Thus, at the core of LFG is the task of scoring subgoal proposals.Formally, let's assume we have the task by query q, a partially explored environment stored in M, and a set S of n textual subgoal proposals s 1 , s 2 , . . ., s n (e.g., "a corner with a dishwasher and refrigerator", "a hallway with a door", etc.).Our goal is to score these subgoal proposals with p(s i , q, M), the probability that the candidate s i ∈ S will lead to the goal q given the current state of the environment, described through M.</p>
<p>We posit that we can leverage the semantic reasoning capabilities of LLMs by prompting them to construct narratives about which semantic regions of the environment are most (and least) likely to lead to the goal.While the narrative itself might be ungrounded, since the LLM knows very little about the environment, reasoning over objects and semantic regions of the environment often generalizes very broadly.For example, even without seeing a new apartment, a human would guess that the dining area is close to the kitchen.Hence, rather than directly using LLM scores for planning [23,25], we incorporate them as a goal-directed heuristic to inform the search process.This has two distinct advantages: (i) when the LLM is right, it nudges the search towards the goal, and when it is wrong (or uncertain), we can still default to the underlying FBE algorithm, allowing recovery from LLM failures, and (ii) it allows us to combine the signal from LLMs with other scores that may be more grounded, e.g.distance to subgoals, making the system more versatile.</p>
<p>LFG: Scoring Subgoals by Polling LLMs</p>
<p>Our aim in this section is to derive a scoring function from LLMs that takes a textual description of subgoal candidates s i and the goal query q as inputs, and predicts task-relevant probability p(s i , q, M), conditioned on the episodic memory M. While we may obtain this from next-token likelihoods (or "logprobs"), they do not represent the desired task-relevant probability p(s i , q, M), and fail to assign similar scores, say, to different subgoals that are semantically similar but have different tokenizations (see our experiments in Section 6 for a comparison).Furthermore, most capable LLMs of today are available through APIs that do not expose the ability to query logprobs. 1nd lastly, even if reliable logprobs were available, they are incompatible with chain-of-thought prompting [32], which we find to be crucial to success in spatial reasoning.</p>
<p>To overcome these challenges, LFG uses a novel approach to extract task-relevant likelihoods from LLMs.Given candidate subgoal images, LFG uses a VLM to obtain a textual subgoal desriptor s i , which must be scored with the LLM.LFG polls the LLMs by sampling the most likely subgoal n s times, conditioned on a task-relevant prompt.We then use these samples to empirically estimate the likelihood of each subgoal.To get informative and robust likelihood estimates, we use a chain-ofthought prompting (CoT) technique [32], to improve the quality and interpretability of the scores, and use a combination of positive and negative prompts to gather unbiased likelihood estimates.Figure 2 outlines our scoring technique, with the full prompt provided in Appendix B. We now describe the details of our scoring technique.</p>
<p>Structured query: We rely on in-context learning by providing an example of a structured queryresponse pair to the LLM, and ask it to pick the most likely subgoal that satisfies the query.To sample a subgoal from S using a language model, we prompt it to generate a structured response, ending with ''Answer: i''.This structure allows us to always sample a valid subgoal, without having to ground LLM generations in the environment [24].</p>
<p>Positives and negatives: We find that only using positive prompts (e.g., "which subgoal is most likely to reach the goal") leads to likelihood estimates being uninformative for cases where the LLM is not confident about any subgoal.To overcome this, we also use negative prompts (e.g., "which subgoal is least likely to be relevant for the goal"), which allows us to score subgoals by eliminating/downweighting subgoals that are clearly irrelevant.We then use the difference between the positive and negative likelihoods to rank subgoals.</p>
<p>Goal Category</p>
<p>Sink</p>
<p>Language Model Scoring Heuristic-based Exploration Policy "avoid the couch" "explore near oven" Algorithm 1: Scoring Subgoals with LFG Data: Subgoal descriptors {l i ∀s i ∈ S}
1 pPrompt ← PositivePrompt({l i }) 2 nPrompt ← NegativePrompt({l i }) 3 pSamples ← [sampleLLM(pPrompt) * n s ] 4 nSamples ← [sampleLLM(nPrompt) * n s ] 5 pScores ← sum(pSamples) / n s 6 nScores ← sum(nSamples) / n s 7 return pScores, nScores
Chain-of-thought prompting: A crucial component of getting interpretable and reliable likelihood estimates is to encourage the LLM to justify its choice by chain-of-thought prompting.As demonstrated in prior works, CoT elicits interpretability and reasoning capabilities in LLMs, and while we don't explicitly use the generated reasonings in our approach (great future work direction), we find that CoT improves the quality and consistency of the likelihood estimates.Additionally, it also helps maintain interpretability, to better understand why the LFG-equipped agent takes certain decisions.</p>
<p>In summary, we score subgoals by sampling the LLM multiple times and empirically estimating the likelihood of each subgoal.We use a combination of positive and negative prompts to get unbiased likelihood estimates, and use chain-of-thought prompting to improve the quality and interpretability of the scores (Figure 2).We will now discuss how these scores can be incorporated into a navigation system as search heuristics.</p>
<p>LLM Heuristics for Goal-Directed Exploration</p>
<p>Given the LLM scoring pipeline outlined in the previous section, our key insight is that we can incorporate these scores in a search-based planning pipeline to heuristically guide the search process.We instantiate LFG using frontier-based exploration (FBE) and LLM scores generated via polling.FBE: This method maintains a "map" of the seen parts of the environment, which may be geometric [33] or topological [34], and a frontier separating it from the unexplored parts.By navigating to the nearest point of the frontier, the robot explores new areas of the environment until it finds the goal object or completes exploration without finding it.A standard FBE implementation is presented in Algorithm 2 inblack text.The robot maintains either a 2D metric map of its surroundings, or a topological map whose nodes are comprised of the robot's visual observations and edges represent paths taken in the environment.Additionally, we also store semantic labels corresponding to objects detected in the robot's observations, which are used to ground the observations in text.</p>
<p>At a fixed re-planning rate, the high-level planner computes its frontier f i (Line 10), and picks the frontier point that is closest to the current location, i.e., maximizing the distance score (Line 16), and then navigates to this frontier (Line 21).At any point in this process, if the agent's semantic detector detects an object of the same category as the query q, it navigates directly to this object and the trajectory ends.</p>
<p>Incorporating LLM scores: Our method, LFG, extends FBE by using an additional search heuristic obtained by polling LLMs for semantic "scores".The modifications to FBE are marked in purple in Algorithm 2. After enumerating the frontiers, LFG uses the semantic labels from a VLM [35] to ground subgoal images at each frontier f i (Line 11).These images are converted into textual strings, and form the subgoal candidates s i that can be scored using Algorithm 1.We associate each frontier point f i with the nearest object cluster c i (Line 17), and compute LLM scores for each point as follows:
h(f i , q) = w p • LLM pos (c i ) − w n • LLM neg (c i ) − dist(f i , p),(1)
where p is the current position of the agent, and w p , w n are hyperparameters (see Appendix A.1).</p>
<p>We then choose the frontier with the highest score to be the next subgoal (Line 21), navigate to it using a local controller, and repeat the planning process.Algorithm 2 outlines the general recipe for integrating LLM scores as a planning heuristic.Please see Appendix A for specific instantiations of this system with geometric and topological maps, and more details about the referenced subroutines.</p>
<p>Algorithm 2: Instantiating LFG for Goal-Directed Exploration Data: o 0 , Goal language query q 1 subgoal ← None</p>
<p>System Evaluation</p>
<p>We now evaluate the performance of LFG for the task of goal-directed exploration in real-world environments, and benchmark its performance against baselines.We instantiate two systems with LFG: a real-world system that uses a topological map and a learned control policy, and a simulated agent that uses a geometric map and a deterministic control policy.Our experiments show that both these systems outperform existing LLM-based exploration algorithms by a wide margin, owing to the high quality scores incorporated as search heuristics.</p>
<p>Query: Find the potted plant.</p>
<p>LLM: "plants are not typically found in bedrooms or around furniture, so we should avoid cluster 1, 2, and 3" Query: Find a Toilet LLM: "toilets are not typically found in bedrooms kitchens, but it is more likely that a bathroom is near a bedroom so we should explore the bedroom first" In this apartment experiment, LFG believes that a bathroom is more likely to be found near a bedroom rather than a kitchen, and guides the robot towards the bedroom, successfully reaching the goal.</p>
<p>Agent succeeds!</p>
<p>Benchmarking ObjectNav Performance</p>
<p>We benchmark the performance of LFG for the task of object-goal navigation on the Habitat Ob-jectNav Challenge [36], where the agent is placed into a simulated environment with photo-realistic graphics, and is tasked with finding a query object from one of 10 categories (e.g., "toilet", "bed", "couch" etc.).The simulated agent has access to egocentric RGBD observations and accurate pose information.We run 10 evaluation episodes per scene and report two metrics: the average success rate, and success weighted by optimal path length (SPL), the default metrics for the benchmark.Since LFG requires no training, we do not use the training scenes from HM3D.</p>
<p>We compare to three classes of published baselines: (i) learning-based baselines that learn navigation behavior from demonstrations or online experience in the training scenes [37] on up to 2.5B frames of experience, (ii) search-based baselines [33,38], and (iii) LLM-based baselines that do not use the training data directly, but leverage the semantic knowledge inside foundation models to guide embodied tasks [18,39].</p>
<p>Evaluating LFG on the HM3D benchmark, we find that it significantly outperforms search and LLMbased baselines (Table 1).Greedy LLM struggles on the task due to several LLM planning failures, causing the episodes to fail.LFG significantly outperforms the vanilla FBE baseline by leveraging semantic priors from LLMs to score subgoals intelligently.Comparing to learning-based baselines, we find that LFG outperforms most of them and closely matches the state-of-the-art on the task, proving the competence of our polling and heuristic approach.Figure 4 shows an example of the LFG agent successfully reaching the goal by using chain-of-thought and negative prompting.</p>
<p>L3MVN [39], which uses a combination of LLMs and search, performs slightly better than FBE, but is unable to fully leverage the semantics in LLMs.While being similar to LFG, it suffers from two key limitations: (i) it uses a small language model (GPT-2), which likely does not contain strong semantic priors for the agent to leverage, and (ii) it uses a simple likelihood-based scoring scheme, which we show below is not very effective.Another closely related work, LGX [18], uses a variant of greedy LLM scoring, and hence fails to perform reliably on the benchmark.</p>
<p>Probing deeper into the strong performance of LFG, we ablated various components of our scoring pipeline and studied the change in performance.Note that LGX (Greedy) and L3MVN (No CoT, Logprobs) can be seen as ablations of LFG.Table 2 shows that modifying both the prompting and scoring mechanisms used by LFG lead to large drops in performance.Most notably, scoring via polling (+7.8%) and CoT (+6.6%) are both essential to the strong performance of LFG.Furthermore, we find that using only positive prompts also hurts performance (−4.7%).Popular approaches for using LLMs for planning are significantly outperformed by LFG: Greedy (−14.5%) and Logprobs (−8.5%).Figure 4 shows an example of the LFG agent successfully reaching the goal by using CoT and negative prompting.</p>
<p>Setup: For these experiments, we mimic the semantic mapping pipeline of the best-performing baseline on the benchmark [33,38], and integrate LFG with the geometric map.The simulated agent builds a 2D semantic map of its environment, where grid cells represent both occupancy and semantic labels corresponding to objects detected by the agent.Prior work has shown that stateof-the-art vision models, such as DETIC, work poorly in simulation due to rendering artifacts [33]; hence, we use ground-truth semantic information for all simulated baselines to analyze navigation performance under perfect perception.</p>
<p>Real-world Exploration with LFG</p>
<p>To show the versatility of the LFG scoring framework, we further integrated it with a heuristicbased exploration framework that uses topological graphs for episodic memory [34].We compare two published baselines: a language-agnostic FBE baseline [40], and an LLM-based baseline that uses the language model to greedily pick the frontier [18].</p>
<p>We evaluate this system in two challenging real-world environments: a cluttered cafeteria and an apartment building (shown in Figures 3 and 5).In each environment, the robot is tasked to reach an object described by a textual string (e.g., "kitchen sink" or "oven"), and we measure the success rate and efficiency of reaching the goal.Episodes that take longer than 30 minutes are marked as failure.</p>
<p>While we only tested our system with goal strings corresponding to the 20,000 classes supported by our object detector [35], this can be extended to more flexible goal specifications with the rapid progress in vision-language models.</p>
<p>We find that the complexity of real-world environments causes the language-agnostic FBE baseline to time out, i.e., the robot is unable to reach the goal by randomly exploring the environment.Both LLM baselines are able to leverage the stored semantic knowledge to guide the exploration in novel environments, but LFG achieves 16% better performance.Figure 5 shows an example rollout in a real apartment, where the robot uses LFG to reach the toilet successfully.</p>
<p>Setup: We instantiate LFG in the real-world using a previously published topological navigation framework [34] that builds a topological map of the environment, where nodes correspond to the robot's visual observations and edges correspond to paths traversed in the environment.This system relies on omnidirectional RGB observations and does not attempt to make a dense geometric map of the environment.To obtain "semantic frontiers" from the omnidirectional camera, we generate n v = 4 views and run an off-the-shelf object detector [35] to generate rich semantic labels describing objects in these views.The robot maintains a topological graph of these views and semantic labels, and picks the frontier view with the highest score (Algorithm 2, Line 21) according to LFG.The robot then uses a Transformer-based policy [6,41] to reach this subgoal.For more implementation details, see Appendix A.3.</p>
<p>Method</p>
<p>Success SPL Data DD-PPO [37] 27.9 14.2 2.5B FBE [33] 61.1 34.0 0 SemExp [38] 63.1 0.29 10M OVRL-v2 [42] 64.7 28.1 12M</p>
<p>Greedy LLM [18] 54.4 26.9 0 L3MVN [39] 62.4 0 LFG (Ours) 68.9 36.0 0</p>
<p>Discussion</p>
<p>We presented LFG, a method for utilizing language models for semantic guesswork to help navigate to goals in new and unfamiliar environments.The central idea in our work is that, while language models can bring to bear rich semantic understanding, their ungrounded inferences about how to perform navigational tasks are better used as suggestions and heuristics rather than plans.We formulate a way to derive a heuristic score from language models that we can then incorporate into a planning algorithm, and use this heuristic planner to reach goals in new environments more effectively.This way of using language models benefits from their inferences when they are correct, and reverts to a more conventional unguided search when they are not.</p>
<p>Limitations and future work: While our experiments provide a validation of our key hypothesis, they have a number of limitations.First, we only test in indoor environments in both sim and real yet the role of semantics in navigation likely differs drastically across domains -e.g., navigating a forest might implicate semantics very differently than navigating an apartment building.Exploring the applicability of semantics derived from language models in other settings would be another promising and exciting direction for future work.Second, we acknowledge that multiple requests to cloud-hosted LLMs with CoT is slow and requires an internet connection, severely limiting the extent of real-world deployment of the proposed method.We hope that ongoing advancements in quantizing LLMs for edge deployment and fast inference will address this limitation.</p>
<p>•</p>
<p>• pillow</p>
<p>Low-level Policy: The low-level policy running on the robot is the NoMaD goal-conditioned diffusion policy trained to avoid obstacles during exploration and determine which frontiers can be explored further [41].</p>
<p>High-level Planning: For real-world experiments, we follow the setup of ViKiNG [34], where the agent runs a simple frontier-based exploration algorithm and incorporates the LLM scores as goaldirected heuristics to pick the best subgoal frontier.For simulation experiments, we use a geometric map coupled with frontier-based exploration, following the setup of Chaplot et al. [38].Algorithms 3 and 4 summarize the high-level planning module in both cases.</p>
<p>Algorithm 3: Instantiating LFG with Topological Mapping</p>
<p>Data: o 0 , Goal language query q 1 subgoal ← None 2 while not done do A.4 More Experiment Rollouts Figure 6 shows an example where the negative scoring is essential to LFG's success.Figures 7  and 8 show examples of LFG deployed in a previously unseen apartment and an office building, successfully exploring the environments to find an oven and a kitchen sink.</p>
<p>Agent succeeds!</p>
<p>B Prompts B.1 Positive Prompt</p>
<p>You are a robot exploring an environment for the first time .</p>
<p>You will be given an object to look for and should provide guidance of where to explore based on a series of observations .Observations will be given as a list of object clusters numbered 1 to N .</p>
<p>Your job is to provide guidance about where we should explore next .For example if we are in a house and looking for a tv  Reasoning : Knifes are typically kept in the kitchen and a sink , microwave , and refrigerator are commonly found in kitchens .Therefore we should check the cluster that is likely to be a kitchen first .Answer : 3 Other considerations 1. Disregard the frequency of the objects listed on each line .</p>
<p>If there are multiple of the same item in a cluster it will only be listed once in that cluster .2. You will only be given a list of common items found in the environment .You will not be given room labels .Use your best judgement when determining what room a cluster of objects is likely to belong to .</p>
<p>B.2 Negative Prompt</p>
<p>You are a robot exploring an environment for the first time .</p>
<p>You will be given an object to look for and should provide guidance of where to explore based on a series of observations .Observations will be given as a list of object clusters numbered 1 to N .</p>
<p>Your job is to provide guidance about where we should not waste time exploring .For example if we are in a house and looking for a tv we should not waste time looking in the bathroom .It is your job to point this out .</p>
<p>You should always provide reasoning along with a number identifying where we should not explore .If there are multiple of the same item in a cluster it will only be listed once in that cluster .2. You will only be given a list of common items found in the environment .You will not be given room labels .Use your best judgement when determining what room a cluster of objects is likely to belong to .</p>
<p>Figure 1 :
1
Figure 1: In constrast to methods that use LLM plans directly, Language Frontier Guide (LFG) uses a language model to score subgoal candidates, and uses these scores to guide a heuristic-based planner.</p>
<p>IFigure 2 :
2
Figure 2: LFG scores subgoals with an empirical estimate of the likelihoods by sampling an LLM ns times with both positive and negative prompts, and uses chain-of-thought to obtain reliable scores.These scores are used by a high-level planner as heuristics to guide search.For full prompts, see Appendix B.</p>
<p>Figure 3 :
3
Figure 3: Overview of LFG for language-guided exploration.Based on the pose and observations, LFG builds an episodic memory (topological or metric), which is used by the heuristic-based exploration policy to rank adjacent clusters, or subgoal frontiers.Navigation to the subgoal frontier is completed by a low-level policy.</p>
<p>Figure 4 :
4
Figure 4: Qualitative example of a negative score influencing the agent's decision.LFG discourages the agent from exploring the bedroom and living room, leading to fast convergence toward the goal, whereas FBE fails.The CoT reasoning given by the LLM is shown in purple, justifying its score.</p>
<p>Figure 5 :
5
Figure 5: Qualitative example of LFG in real.LFG reasons about floor plans in the environment it is searching.In this apartment experiment, LFG believes that a bathroom is more likely to be found near a bedroom rather than a kitchen, and guides the robot towards the bedroom, successfully reaching the goal.</p>
<p>3 o 5 if q in frontierPoints then 6 turnTowardGoal 8 if
3568
t ← getObservation() 4 frontierPoints ← mappingModule(o t ) numSteps % τ == 0 then 9 location ← getCurrentLocation() 10 LLM pos , LLM neg ← scoreFrontiers(frontierPoints) scores ← [] 11 for point in frontier do 12 distance ← distTo(location, point) 13 scores[point] ← w p • LLM pos [i] -w n • LLM neg [i] -distance 14 subgoal ← argmax(scores) 15 numSteps ← numSteps +1 16 goTo(subGoal)</p>
<p>Figure 6 :
6
Figure 6: Tolerance to LLM failures.An example rollout of LFG compensating for LLM failure.FBE takes over in this case and eventually succeeds, whereas the Greedy agent fails.</p>
<p>Figure 7 :
7
Figure7: LFG in an unseen apartment.The robot starts in the same starting location and environment as 5, and is tasked with finding an oven.LFG guides the robot towards the kitchen appliances, rather than the bedroom door, and successfully leads to the oven.</p>
<p>Figure 8 :
8
Figure 8: LFG in an unseen office building.The agent looks for a sink in an open-plan office building.Despite erroneous detections, the robot continues exploring the environment, with LFG guiding it towards frontiers containing appliances found in a cafe.The robot successfully finds the sink despite imperfect detections.we should explore areas that typically have tv ' s such as bedrooms and living rooms .You should always provide reasoning along with a number identifying where we should explore .If there are multiple right answers you should separate them with commas .Always include Reasoning : &lt; your reasoning &gt; and Answer : &lt; your answer ( s ) &gt;.If there are no suitable answers leave the space afters Answer : blank .Example User : I observe the following clusters of objects while exploring a house :</p>
<p>Table 1 :
1
LFG outperforms all LLM-based baselines on HM3D ObjectNav benchmark, and can achieve close to SOTA performance without any pre-training.
MethodSuccess∆LFG (Full)68.9-PromptingNo CoT62.3(6.6)Only Positives64.2(4.7)ScoringRandom61.1(7.8)Logprobs60.4(8.5)Ask62.4(6.5)</p>
<p>Table 2 :
2
We find that CoT prompting with positives and negatives, combined with polling, are essential to achieve the best performance.</p>
<p>table •
•• sofa bed• monitor(computerbathtub• bedequipment)• bath towel• chandelier• computer monitor• urinal • toilet • toilet tissue • refrigerator• ottoman • dresser • curtain• computer keyboard • laptop computer • desk • stool• automatic washer• shower curtain• hand towel• washbasin• trash can• shampoo• dishwasher• garbage• soap• television set• cabinet• drawer• sofa• file cabinet</p>
<p>Algorithm 4 :
4
Instantiating LFG with Geometric Mapping Data: o 0 , Goal language query q 1 subgoal ← None 2 while not done do
3o t ← getObservation()4obstacleMap, semanticMap ← mappingModule(o t [depth], o t [semantic])5if q in semanticMap then6subGoal ← getLocation(semanticMap, q)7else8if numSteps % τ == 0 then// replanning9location ← getCurrentLocation()10frontier ← getFrontier(obstacleMap)11objects ← parseObjects(semanticMap)12objectClusters ← clusterObjects(objects)13LLM pos , LLM neg ← ScoreSubgoals(objectClusters)14scores ← []15for point in frontier do16distance ← distTo(location, point)
17 scores[point] ← -distance 18 closestCluster ← getClosestCluster(objectClusters, point) 19 i ← clusterID(closestCluster) 20 if dist(closestCluster, point) &lt; δ then // incorporate language scores 21 scores[point] ← w p • LLM pos [i] -w n • LLM neg [i] -distance 22 subgoal ← argmax(scores) 23 numSteps ← numSteps +1 24 goTo(subGoal) Query: Find the toilet.1.LLM finds a bed, increases score to explore nearby.2. No toilet found, LLM failure, FBE takes over.3. FBE finds toilet by continuing exploration.</p>
<p>If there are multiple right answers you should separate them with commas .Always include Reasoning : &lt; your reasoning &gt; and Answer : &lt; your answer ( s ) &gt;.If there are no suitable answers leave the space afters Answer : blank .Disregard the frequency of the objects listed on each line .
ExampleUser :I observe the following clusters of objects while exploring ahouse :1. sofa , tv , speaker2. desk , chair , computer3. sink , microwave , refrigeratorWhere should I avoid spending time searching if I am lookingfor a knife ?Assistant :Reasoning : Knifes are typically not kept in a living room oroffice space which is what the objects in 1 and 2 suggest .Therefore you should avoid looking in 1 and 2.Answer : 1 ,2Other considerations1.
Most notably, OpenAI's Chat API for GPT-3.5 and GPT-4, Google's PaLM API, and Anthropic's Claude API all do not support logprobs.
AcknowledgmentsThis research was partly supported by AFOSR FA9550-22-1-0273 and DARPA ANSR.The authors would like to thank Bangguo Yu, Vishnu Sashank Dorbala, Mukul Khanna, Theophile Gervet, and Chris Paxton, for their help in reproducing baselines.The authors would also like to thank Ajay Sridhar, for supporting real-world experiments, and Devendra Singh Chaplot, Jie Tan, Peng Xu, and Tingnan Zhang, for useful discussions in various stages of the project.A.3 Real World ResultsGenerating Prompts: For both topological and geometric maps we use hand engineered methods for clustering objects in ways that the LLM can efficiently reason over.For geometric maps we implement two functions: parseObjects and clusterObjects.In our implementation, parseObejcts filters the geometric map and identifies the cluster centers among each class.clusterObjects takes the cluster centers and performs agglomerative clustering with a threshold of 6 meters, which is roughly the size of one section of a standard house.For topological maps we rely on the configuration of the four cameras to automatically perform parsing and clustering.In our implementation all the objects detected in each frame from either the front, left, right, or rear facing cameras is considered a single cluster.Perception: For the hardware, we use a locobot base with a four HD logitech web cameras that are positioned at 90 degrees relative to each other.At each step of LFG each of four cameras is recorded and frames are semantically annotated.LFG directly uses these frames to determine if the robot should continue to move forward, turn left, turn right, or turn around a full 180 degrees.To improve the performance of our system we choose to whitelist a subset of the 20,000 classes.This reduces the size of the API calls to the language models and helps steer the LLM to focus on more useful information.Following is the complete whitelist used in our experiments:• toaster
Vision for mobile robot navigation: A survey. G N Desouza, A C Kak, IEEE transactions on pattern analysis and machine intelligence. 200224</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, A Van Den, Hengel, IEEE Conference on Computer Vision and Pattern Recognition. 2018</p>
<p>Semi-parametric topological memory for navigation. N Savinov, A Dosovitskiy, V Koltun, arXiv:1803.006532018arXiv preprint</p>
<p>Deep visual MPC-policy learning for navigation. N Hirose, F Xia, R Martín-Martín, A Sadeghian, S Savarese, IEEE Robotics and Automation Letters. 2019</p>
<p>GNM: A General Navigation Model to Drive Any Robot. D Shah, A Sridhar, A Bhorkar, N Hirose, S Levine, arXiV2022</p>
<p>ViNT: A Foundation Model for Visual Navigation. D Shah, A Sridhar, N Dashora, K Stachowicz, K Black, N Hirose, S Levine, 7th Annual Conference on Robot Learning (CoRL). 2023</p>
<p>Poni: Potential functions for objectgoal navigation with interaction-free learning. S K Ramakrishnan, D S Chaplot, Z Al-Halah, J Malik, K Grauman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>X Chen, X Wang, S Changpinyo, A Piergiovanni, P Padlewski, D Salz, S Goodman, A Grycner, B Mustafa, L Beyer, arXiv:2209.06794A jointly-scaled multilingual language-image model. 2022arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. 2021</p>
<p>Improving visionand-language navigation with image-text pairs from the web. A Majumdar, A Shrivastava, S Lee, P Anderson, D Parikh, D Batra, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part VI 16</p>
<p>Zson: Zero-shot objectgoal navigation using multimodal goal embeddings. A Majumdar, G Aggarwal, B Devnani, J Hoffman, D Batra, arXiv:2206.124032022arXiv preprint</p>
<p>B Chen, F Xia, B Ichter, K Rao, K Gopalakrishnan, M S Ryoo, A Stone, D Kappler, arXiv:2209.09874Open-vocabulary queryable scene representations for real world planning. 2022arXiv preprint</p>
<p>C Huang, O Mees, A Zeng, W Burgard, arXiv:2210.05714Visual language maps for robot navigation. 2022arXiv preprint</p>
<p>LM-nav: Robotic navigation with large pretrained models of language, vision, and action. D Shah, B Osinski, B Ichter, S Levine, Annual Conference on Robot Learning (CoRL). 202223</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Proceedings of the 5th Conference on Robot Learning (CoRL). the 5th Conference on Robot Learning (CoRL)2021</p>
<p>Clip-fields: Weakly supervised semantic fields for robotic memory. N M M Shafiullah, C Paxton, L Pinto, S Chintala, A Szlam, 2023</p>
<p>K Jatavallabhula, A Kuwajerwala, Q Gu, M Omama, T Chen, S Li, G Iyer, S Saryazdi, N Keetha, A Tewari, J Tenenbaum, C De Melo, M Krishna, L Paull, F Shkurti, A Torralba, arXiv, 2023. 3Conceptfusion: Open-set multimodal 3d mapping. </p>
<p>Can an Embodied Agent Find Your "Catshaped Mug"? LLM-Based Zero-Shot Object Navigation. V S Dorbala, J F J Mullen, D Manocha, 2023. 3, 789</p>
<p>Grounding language with visual affordances over unstructured data. O Mees, J Borja-Diaz, W Burgard, 2023</p>
<p>I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, Progprompt: Generating situated robot task plans using large language models. 2022</p>
<p>Y Xie, C Yu, T Zhu, J Bai, Z Gong, H Soh, arXiv:2302.05128Translating natural language to planning goals with large-language models. 2023arXiv preprint</p>
<p>Task and motion planning with large language models for object rearrangement. Y Ding, X Zhang, C Paxton, S Zhang, 2023</p>
<p>Text2motion: From natural language instructions to feasible plans. K Lin, C Agia, T Migimatsu, M Pavone, J Bohg, 202334</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning (ICML). </p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. B Ichter, A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, D Kalashnikov, S Levine, Y Lu, C Parada, K Rao, P Sermanet, A T Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, M Yan, N Brown, M Ahn, O Cortes, N Sievers, C Tan, S Xu, D Reyes, J Rettinghouse, J Quiambao, P Pastor, L Luu, K.-H Lee, Y Kuang, S Jesmonth, K Jeffrey, R J Ruano, J Hsu, K Gopalakrishnan, B David, A Zeng, C K Fu, Annual Conference on Robot Learning. </p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). K Valmeekam, A Olmo, S Sreedharan, S Kambhampati, 2023</p>
<p>Grounded decoding: Guiding text generation with grounded models for robot control. W Huang, F Xia, D Shah, D Driess, A Zeng, Y Lu, P Florence, I Mordatch, S Levine, K Hausman, B Ichter, 2023</p>
<p>Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, Vima, General robot manipulation with multimodal prompts. 2022arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. S Huang, L Dong, W Wang, Y Hao, S Singhal, S Ma, T Lv, L Cui, O K Mohammed, B Patra, Q Liu, K Aggarwal, Z Chi, J Bjorck, V Chaudhary, S Som, X Song, F Wei, 2023</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S M Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, Y Chebotar, P Sermanet, D Duckworth, S Levine, V Vanhoucke, K Hausman, M Toussaint, K Greff, A Zeng, I Mordatch, P Florence, 2023In arXiv preprint</p>
<p>A frontier-based approach for autonomous exploration. B Yamauchi, IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA). 1997</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Navigating to objects in the real world. T Gervet, S Chintala, D Batra, J Malik, D S Chaplot, Science Robotics. 2023. 5, 7, 8, 9</p>
<p>Viking: Vision-based kilometer-scale navigation with geographic hints. D Shah, S Levine, Robotics: Science and Systems (RSS). 2022814</p>
<p>Detecting twenty-thousand classes using image-level supervision. X Zhou, R Girdhar, A Joulin, P Krähenbühl, I Misra, 10.1007/978-3-031-20077-9_2117th European Conference on Computer Vision (ECCV). 62022</p>
<p>. K Yadav, S K Ramakrishnan, J Turner, A Gokaslan, O Maksymets, R Jain, R Ramrakhya, A X Chang, A Clegg, M Savva, E Undersander, D S Chaplot, D Batra, Habitat challenge. 2022</p>
<p>DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames. E Wijmans, A Kadian, A Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, International Conference on Learning Representations (ICLR). 202079</p>
<p>Semantic curiosity for active visual learning. D S Chaplot, H Jiang, S Gupta, A Gupta, ECCV. 2020. 7, 8, 914</p>
<p>L3mvn: Leveraging large language models for visual target navigation. B Yu, H Kasaei, M Cao, 202379</p>
<p>Rapid exploration for open-world navigation with latent goal models. D Shah, B Eysenbach, N Rhinehart, S Levine, 5th Annual Conference on Robot Learning. 2021</p>
<p>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration. A Sridhar, D Shah, C Glossop, S Levine, arXiv2023814</p>
<p>Ovrl-v2: A simple state-of-art baseline for imagenav and objectnav. K Yadav, A Majumdar, R Ramrakhya, N Yokoyama, A Baevski, Z Kira, O Maksymets, D Batra, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
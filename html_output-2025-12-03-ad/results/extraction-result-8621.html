<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8621 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8621</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8621</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267770010</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.13718v7.pdf" target="_blank">Exploring Self-supervised Logic-enhanced Training for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains. Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks. Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-the-art fine-tuning based models. To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning. We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion. LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2). Additionally, LogicLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model’s general language understanding capabilities.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8621.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8621.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter autoregressive transformer foundation model (LLaMA series) evaluated in this paper both as baseline and after LogicLLM self-supervised post-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive transformer family (LLaMA); in these experiments the 7B variant is continually trained with the LogicLLM objective (auto-regressive generation on logic-consistent pairs) and mixed with standard LM loss.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor and LogiQA-v2 (zero-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice reading-comprehension benchmarks requiring logical reasoning: ReClor (graduate exam style logical reasoning) and LogiQA-v2 (logical reasoning MRC/NLI).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Self-supervised LogicLLM meta-training: extract logically consistent direct/indirect relation pairs from Wikipedia, auto-regressive generation objective to reconstruct paired relation, counterfactual entity replacement augmentation, mixed LM loss to avoid forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline zero-shot accuracies: ReClor dev 30.2% / test 30.3%; LogiQA-v2 dev 27.4% / test 28.1%. After LogicLLM: ReClor dev 32.4% / test 31.0%; LogiQA-v2 dev 27.7% / test 28.6% (absolute gains small, ≈0.9–2.2 pts on ReClor).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to vanilla LLaMA-7B baseline (no LogicLLM). Contrastive variant (MERIt-style) not reported for 7B in main table; auto-regressive LogicLLM yields measurable but small gains for 7B vs larger models where gains are larger.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Small absolute improvements for small models; remains far below tuned fine-tuned SOTA on these benchmarks; performance sensitive to prompts/instructions; memorization risk if counterfactual augmentation omitted; positional bias and prompt randomness persist.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LogicLLM's auto-regressive, self-supervised meta-training can improve logical reasoning even for small LLMs but gains scale with model size; counterfactual augmentation and mixing LM loss are important to avoid memorization and forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8621.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8621.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (13 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter autoregressive transformer evaluated as baseline and after LogicLLM self-supervised logic meta-training, showing notable improvements on strict logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter variant of the LLaMA family; used for continual training with the LogicLLM objective (auto-regressive reconstruction of logic-consistent relation pairs) and compared with a contrastive (MERIt-style) variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor and LogiQA-v2 (zero-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning reading-comprehension datasets (ReClor, LogiQA-v2) that require multi-step/coherent relational reasoning in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LogicLLM auto-regressive self-supervised meta-training (Wikipedia relation pair extraction, counterfactual augmentation, mixed LM loss). Ablation includes contrastive (ctr) variant following MERIt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline zero-shot accuracies: ReClor dev 30.4% / test 33.5%; LogiQA-v2 dev 33.0% / test 32.1%. With LogicLLM (ar): ReClor dev 37.4% / test 36.3%; LogiQA-v2 dev 34.1% / test 34.0% (average improvements ≈3.2 points across splits).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms vanilla LLaMA-13B; LogicLLM (ar) substantially outperforms LogicLLM (ctr, contrastive) which yields minor/insignificant improvements (e.g., ctr: ReClor dev 33.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still below supervised fine-tuned SOTA on some benchmarks; some remaining sensitivity to prompt/instruction and exemplar selection; few-shot and CoT provide limited gains on strict logic tasks per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Auto-regressive self-supervised logic meta-training aligns better with in-context learning than contrastive objectives and yields meaningful gains at mid model scales; larger models show stronger emergent benefit from the same training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8621.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8621.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-33B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (33 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 33B-parameter LLaMA model evaluated with LogicLLM (using QLoRA for training) and showing the largest absolute improvements among LLaMA variants in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-33B (QLoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>33B-parameter LLaMA model; due to compute constraints trained with QLoRA low-rank adaptation; LogicLLM meta-training applied (with and without counterfactual augmentation ablations reported).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>33B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor and LogiQA-v2 (zero-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>ReClor and LogiQA-v2 multiple-choice datasets requiring logical inference and multi-hop relational reasoning in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LogicLLM auto-regressive meta-training (logic-consistent pair reconstruction), experiments report ablations: no augmentation, one augmentation, full augmentation; training on QLoRA for large model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline zero-shot accuracies: ReClor dev 45.2% / test 50.3%; LogiQA-v2 dev 41.2% / test 41.6%. With LogicLLM (no aug.) ReClor dev 49.4% / test 53.0%; with 1 aug. ReClor dev 50.8% / test 52.7%; final LogicLLM (†, with QLoRA & full aug.) ReClor dev 50.2% / test 54.4% and LogiQA-v2 dev 45.9% / test 42.6% (average gains ≈3.7 points on some splits).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially improves over vanilla LLaMA-33B. Ablation shows counterfactual augmentation further increases gains vs no-augmentation; auto-regressive objective outperforms contrastive.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Improvements on some metrics are marginal; low-rank adaptation (QLoRA) may restrict generalization; some test-split anomalies (e.g., LogiQA test decreased in some ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Larger models benefit more from LogicLLM meta-training; counterfactual augmentation is important to reduce memorization and amplify logic-generalization effects; QLoRA enables scaling but may constrain gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8621.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8621.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (3 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B-parameter instruction-tuned encoder-decoder T5 model (FLAN-T5 family) evaluated before and after LogicLLM post-training; LogicLLM is combined with FLAN instruction data in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>3B-parameter FLAN-T5 variant (instruction-tuned T5 family); in experiments LogicLLM training is applied and also combined with FLAN instruction-tuning data subset for multitask training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor and LogiQA-v2 (zero-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning reading comprehension benchmarks requiring relational and multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LogicLLM self-supervised meta-training (auto-regressive reconstruction of relation pairs) combined with instruction tuning (FLAN subset) in multitask training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline zero-shot accuracies: ReClor dev 54.6% / test 52.5%; LogiQA-v2 dev 48.7% / test 48.7%. With LogicLLM & FLAN: ReClor dev 55.8% / test 54.1%; LogiQA-v2 dev 50.8% / test 50.1% (absolute gains ≈1–2 pts).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improves over FLAN-T5 baseline; combined LogicLLM+instruction tuning yields additional gains vs instruction tuning alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>CoT/Chain-of-Thought generalization limitations observed in BBH/CoT settings; gains modest relative to larger models and supervised fine-tuned SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LogicLLM is compatible with instruction tuning on FLAN-T5; combining self-supervised logic meta-training with instruction data yields additive improvements on strict logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8621.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8621.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (11 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 11B-parameter instruction-tuned T5 model (FLAN-T5) that, after LogicLLM self-supervised meta-training combined with FLAN instruction data, attains results comparable to or exceeding ChatGPT on the evaluated logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>11B-parameter FLAN-T5 (instruction-tuned) model; in experiments LogicLLM meta-training is applied and merged with FLAN instruction-tuning data for multitask optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor and LogiQA-v2 (zero-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Reading-comprehension logical reasoning benchmarks requiring multi-step and relational reasoning in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Self-supervised LogicLLM (auto-regressive relation pair reconstruction) combined with FLAN instruction tuning in multitask training; counterfactual data augmentation and mixed LM loss used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline zero-shot accuracies: ReClor dev 57.4% / test 59.9%; LogiQA-v2 dev 55.3% / test 53.1%. With LogicLLM & FLAN: ReClor dev 61.2% / test 61.1%; LogiQA-v2 dev 56.0% / test 54.0%. Notably, FLAN-T5-11B w/ LogicLLM outperforms ChatGPT on ReClor dev by 4.8 pts and surpasses ChatGPT on LogiQA-v2 dev/test by ≈1.5/1.3 pts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms FLAN-T5 baseline and is competitive with ChatGPT (GPT-3.5) on these logical reasoning benchmarks without supervised fine-tuning on those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Some task categories (e.g., CoT-based BBH tasks) show mixed generalization; instruction-tuned models still exhibit prompt variability; scaling beyond 40B not exhaustively explored due to compute.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Self-supervised LogicLLM reliably enhances instruction-tuned FLAN-T5 at 11B scale, achieving ChatGPT-comparable or superior zero-shot performance on strict logical benchmarks; indicates logic prior can be learned without supervised labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8621.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8621.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (40 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40B-parameter open large language model (Falcon-40B) evaluated as baseline and after LogicLLM post-training (trained via QLoRA for larger models), showing consistent improvements on logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>40B-parameter language model (Falcon); in experiments LogicLLM is applied (training via QLoRA) as a proof that the method can benefit other large model families.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor and LogiQA-v2 (zero-shot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning datasets requiring discourse-level and relational inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LogicLLM auto-regressive self-supervised meta-training with counterfactual augmentation; training of large model variants used QLoRA adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline zero-shot accuracies: ReClor dev 38.4% / test 37.1%; LogiQA-v2 dev 35.9% / test 36.1%. With LogicLLM (QLoRA): ReClor dev 41.4% / test 43.0%; LogiQA-v2 dev 38.6% / test 37.2% (average improvements ≈3.2 points reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improves over Falcon-40B baseline; similar magnitude of gains to LLaMA-13B/33B after LogicLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Trained with QLoRA due to resource limits — low-rank adaptation may affect absolute gains; still below supervised fine-tuned SOTA on strict logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LogicLLM benefits extend beyond LLaMA and FLAN-T5 families; method is applicable to other large models with QLoRA-based fine/adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8621.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8621.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5-based ChatGPT model used as an off-the-shelf baseline for zero-shot and CoT evaluations on ReClor and LogiQA-v2; also used in data auto-verification experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 family model exposed via API and used as a strong baseline in zero-shot and chain-of-thought prompting evaluations; also used to evaluate logical consistency of constructed training pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (API model)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor and LogiQA-v2; also used to judge logical consistency of training data</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning benchmarks; additionally used as an automatic annotator to judge whether direct/indirect relation pairs are logically consistent (Yes/No).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot in-context prompting, Chain-of-Thought prompting (CoT) in ablation; used as a human-like rater for automatic verification of data consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline zero-shot accuracies: ReClor dev 56.6% / test 61.2%; LogiQA-v2 dev 54.5% / test 52.7%. CoT variants reported mixed effects (sometimes worse on LogiQA-v2 dev).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as a strong baseline; LogicLLM FLAN-T5-11B matches or exceeds ChatGPT on some splits (e.g., ReClor dev). CoT and few-shot improvements for ChatGPT are inconsistent for strict logic benchmarks per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Chain-of-Thought prompting sometimes degrades performance on strict logical benchmarks; exemplar selection and prompt variability substantially affect outcomes; struggles to learn reasoning structure from few exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Even powerful instruction-tuned API models like ChatGPT do not always reliably benefit from CoT or few-shot for strict logical reasoning tasks; LogicLLM can reach or surpass ChatGPT-level performance on some logic benchmarks without supervised labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8621.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8621.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 model used in this paper for automatic evaluation (data verification) of logically consistent examples extracted from Wikipedia; used only as an annotator/validator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-capability API model from OpenAI; leveraged here to automatically assess whether extracted direct/indirect relation pairs (and counterfactual/anonymized variants) are logically consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (API model)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Automatic verification of logically consistent/counterfactual training pairs</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary (Yes/No) judgment whether two sentences describing relations between an entity pair are logically consistent (fuzzy/logical consistency in natural language).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompt-based automatic evaluation on a random sample (1,000 examples) across normal, counterfactual, and anonymized settings to estimate the fraction of constructed pairs judged logically consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-4 judged that over 70% of the counterfactual corpus remained logically consistent; anonymization increased judged consistency for counterfactual examples by ~23 percentage points (per paper text; exact table values not reproduced in body).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared judgments with ChatGPT; GPT-4's judgments were less perturbed by anonymization and indicated higher consistency rates than ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Automatic judgments are model-dependent; anonymization and entity-background removal can significantly change model judgments, indicating weakness in causal relation identification by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GPT-4's higher judged consistency supports the data-construction assumption (direct/indirect relations are often logically consistent), but variation across annotator models reveals limitations of using LLMs for auto-labelling and the fuzzy nature of logical consistency in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8621.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8621.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicLLM (Self-supervised Logic-enhanced Meta-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully self-supervised framework introduced in this paper that constructs logic-consistent relation pairs from raw text and uses an auto-regressive objective plus counterfactual augmentation to inject logical reasoning prior into LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring Self-supervised Logic-enhanced Training for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogicLLM (training framework applied to LLaMA, FLAN-T5, Falcon)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method: extract directly-mentioned (single-step) and indirectly-composed (multi-hop) relation descriptions for the same entity pair from Wikipedia; construct paired examples; train LLMs autoregressively to generate one view given the other; use counterfactual entity replacement and mix LM loss to avoid memorization/forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applied to models from 3B to 40B (experiments: 3B, 7B, 11B, 13B, 33B, 40B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Applied evaluation on ReClor, LogiQA-v2; additional evaluation on RACE, MMLU, BIG-Bench-Hard (BBH)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks probe discourse-level logical reasoning (ReClor, LogiQA-v2) and broader language understanding/complex reasoning (RACE, MMLU, BBH).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Self-supervised: (1) logic-consistent data construction via entity-pair relation extraction, (2) counterfactual augmentation by entity replacement, (3) auto-regressive training objective reconstructing paired relations, with concurrent standard LM loss; ablations vs contrastive (MERIt-style) training included.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across multiple model families, LogicLLM produced consistent zero-shot improvements on ReClor and LogiQA-v2 (examples: LLaMA-13B +3.2 pts avg, LLaMA-33B +3.7 pts avg, Falcon-40B +3.2 pts avg, FLAN-T5-11B improved to 61.2% dev ReClor and 56.0% dev LogiQA-v2). FLAN-T5-11B + LogicLLM matched/exceeded ChatGPT on some splits.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Auto-regressive LogicLLM consistently outperforms contrastive (MERIt-style) variants in these LLM in-context settings; counterfactual augmentation outperforms no-augmentation ablation; LogicLLM + instruction tuning yields additive gains over instruction-tuning-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Logical consistency is fuzzy in natural language (not strictly provable); heuristics to build negatives for contrastive learning are noisy; model performance remains sensitive to prompt/instruction variability; CoT and few-shot exemplars give limited improvement for strict logic tasks; scaling beyond 40B and compute-limited training are constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Self-supervised logical prior can be injected into LLMs via reconstructive, autoregressive objectives built from naturally-occurring direct/indirect relation pairs plus counterfactual augmentation; this aligns well with in-context learning and token-level supervision and improves strict logical reasoning without degrading broader language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MERIt: Meta-path guided contrastive learning for logical reasoning <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>Llama: Open and efficient foundation language models <em>(Rating: 2)</em></li>
                <li>The flan collection: Designing data and methods for effective instruction tuning <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
                <li>Chain-of-thought hub <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8621",
    "paper_id": "paper-267770010",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA (7 billion parameter variant)",
            "brief_description": "A 7B-parameter autoregressive transformer foundation model (LLaMA series) evaluated in this paper both as baseline and after LogicLLM self-supervised post-training.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_description": "Open-source autoregressive transformer family (LLaMA); in these experiments the 7B variant is continually trained with the LogicLLM objective (auto-regressive generation on logic-consistent pairs) and mixed with standard LM loss.",
            "model_size": "7B",
            "reasoning_task_name": "ReClor and LogiQA-v2 (zero-shot evaluation)",
            "reasoning_task_description": "Multiple-choice reading-comprehension benchmarks requiring logical reasoning: ReClor (graduate exam style logical reasoning) and LogiQA-v2 (logical reasoning MRC/NLI).",
            "method_or_approach": "Self-supervised LogicLLM meta-training: extract logically consistent direct/indirect relation pairs from Wikipedia, auto-regressive generation objective to reconstruct paired relation, counterfactual entity replacement augmentation, mixed LM loss to avoid forgetting.",
            "performance": "Baseline zero-shot accuracies: ReClor dev 30.2% / test 30.3%; LogiQA-v2 dev 27.4% / test 28.1%. After LogicLLM: ReClor dev 32.4% / test 31.0%; LogiQA-v2 dev 27.7% / test 28.6% (absolute gains small, ≈0.9–2.2 pts on ReClor).",
            "baseline_comparison": "Compared to vanilla LLaMA-7B baseline (no LogicLLM). Contrastive variant (MERIt-style) not reported for 7B in main table; auto-regressive LogicLLM yields measurable but small gains for 7B vs larger models where gains are larger.",
            "limitations_or_failures": "Small absolute improvements for small models; remains far below tuned fine-tuned SOTA on these benchmarks; performance sensitive to prompts/instructions; memorization risk if counterfactual augmentation omitted; positional bias and prompt randomness persist.",
            "insights_or_conclusions": "LogicLLM's auto-regressive, self-supervised meta-training can improve logical reasoning even for small LLMs but gains scale with model size; counterfactual augmentation and mixing LM loss are important to avoid memorization and forgetting.",
            "uuid": "e8621.0",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA-13B",
            "name_full": "LLaMA (13 billion parameter variant)",
            "brief_description": "A 13B-parameter autoregressive transformer evaluated as baseline and after LogicLLM self-supervised logic meta-training, showing notable improvements on strict logical benchmarks.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B",
            "model_description": "13B-parameter variant of the LLaMA family; used for continual training with the LogicLLM objective (auto-regressive reconstruction of logic-consistent relation pairs) and compared with a contrastive (MERIt-style) variant.",
            "model_size": "13B",
            "reasoning_task_name": "ReClor and LogiQA-v2 (zero-shot evaluation)",
            "reasoning_task_description": "Multiple-choice logical reasoning reading-comprehension datasets (ReClor, LogiQA-v2) that require multi-step/coherent relational reasoning in natural language.",
            "method_or_approach": "LogicLLM auto-regressive self-supervised meta-training (Wikipedia relation pair extraction, counterfactual augmentation, mixed LM loss). Ablation includes contrastive (ctr) variant following MERIt.",
            "performance": "Baseline zero-shot accuracies: ReClor dev 30.4% / test 33.5%; LogiQA-v2 dev 33.0% / test 32.1%. With LogicLLM (ar): ReClor dev 37.4% / test 36.3%; LogiQA-v2 dev 34.1% / test 34.0% (average improvements ≈3.2 points across splits).",
            "baseline_comparison": "Outperforms vanilla LLaMA-13B; LogicLLM (ar) substantially outperforms LogicLLM (ctr, contrastive) which yields minor/insignificant improvements (e.g., ctr: ReClor dev 33.4%).",
            "limitations_or_failures": "Still below supervised fine-tuned SOTA on some benchmarks; some remaining sensitivity to prompt/instruction and exemplar selection; few-shot and CoT provide limited gains on strict logic tasks per the paper.",
            "insights_or_conclusions": "Auto-regressive self-supervised logic meta-training aligns better with in-context learning than contrastive objectives and yields meaningful gains at mid model scales; larger models show stronger emergent benefit from the same training.",
            "uuid": "e8621.1",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA-33B",
            "name_full": "LLaMA (33 billion parameter variant)",
            "brief_description": "A 33B-parameter LLaMA model evaluated with LogicLLM (using QLoRA for training) and showing the largest absolute improvements among LLaMA variants in these experiments.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-33B (QLoRA)",
            "model_description": "33B-parameter LLaMA model; due to compute constraints trained with QLoRA low-rank adaptation; LogicLLM meta-training applied (with and without counterfactual augmentation ablations reported).",
            "model_size": "33B",
            "reasoning_task_name": "ReClor and LogiQA-v2 (zero-shot evaluation)",
            "reasoning_task_description": "ReClor and LogiQA-v2 multiple-choice datasets requiring logical inference and multi-hop relational reasoning in natural language.",
            "method_or_approach": "LogicLLM auto-regressive meta-training (logic-consistent pair reconstruction), experiments report ablations: no augmentation, one augmentation, full augmentation; training on QLoRA for large model.",
            "performance": "Baseline zero-shot accuracies: ReClor dev 45.2% / test 50.3%; LogiQA-v2 dev 41.2% / test 41.6%. With LogicLLM (no aug.) ReClor dev 49.4% / test 53.0%; with 1 aug. ReClor dev 50.8% / test 52.7%; final LogicLLM (†, with QLoRA & full aug.) ReClor dev 50.2% / test 54.4% and LogiQA-v2 dev 45.9% / test 42.6% (average gains ≈3.7 points on some splits).",
            "baseline_comparison": "Substantially improves over vanilla LLaMA-33B. Ablation shows counterfactual augmentation further increases gains vs no-augmentation; auto-regressive objective outperforms contrastive.",
            "limitations_or_failures": "Improvements on some metrics are marginal; low-rank adaptation (QLoRA) may restrict generalization; some test-split anomalies (e.g., LogiQA test decreased in some ablations).",
            "insights_or_conclusions": "Larger models benefit more from LogicLLM meta-training; counterfactual augmentation is important to reduce memorization and amplify logic-generalization effects; QLoRA enables scaling but may constrain gains.",
            "uuid": "e8621.2",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FLAN-T5-3B",
            "name_full": "FLAN-T5 (3 billion parameter variant)",
            "brief_description": "A 3B-parameter instruction-tuned encoder-decoder T5 model (FLAN-T5 family) evaluated before and after LogicLLM post-training; LogicLLM is combined with FLAN instruction data in some experiments.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "FLAN-T5-3B",
            "model_description": "3B-parameter FLAN-T5 variant (instruction-tuned T5 family); in experiments LogicLLM training is applied and also combined with FLAN instruction-tuning data subset for multitask training.",
            "model_size": "3B",
            "reasoning_task_name": "ReClor and LogiQA-v2 (zero-shot evaluation)",
            "reasoning_task_description": "Multiple-choice logical reasoning reading comprehension benchmarks requiring relational and multi-step reasoning.",
            "method_or_approach": "LogicLLM self-supervised meta-training (auto-regressive reconstruction of relation pairs) combined with instruction tuning (FLAN subset) in multitask training.",
            "performance": "Baseline zero-shot accuracies: ReClor dev 54.6% / test 52.5%; LogiQA-v2 dev 48.7% / test 48.7%. With LogicLLM & FLAN: ReClor dev 55.8% / test 54.1%; LogiQA-v2 dev 50.8% / test 50.1% (absolute gains ≈1–2 pts).",
            "baseline_comparison": "Improves over FLAN-T5 baseline; combined LogicLLM+instruction tuning yields additional gains vs instruction tuning alone.",
            "limitations_or_failures": "CoT/Chain-of-Thought generalization limitations observed in BBH/CoT settings; gains modest relative to larger models and supervised fine-tuned SOTA.",
            "insights_or_conclusions": "LogicLLM is compatible with instruction tuning on FLAN-T5; combining self-supervised logic meta-training with instruction data yields additive improvements on strict logical benchmarks.",
            "uuid": "e8621.3",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FLAN-T5-11B",
            "name_full": "FLAN-T5 (11 billion parameter variant)",
            "brief_description": "An 11B-parameter instruction-tuned T5 model (FLAN-T5) that, after LogicLLM self-supervised meta-training combined with FLAN instruction data, attains results comparable to or exceeding ChatGPT on the evaluated logical benchmarks.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "FLAN-T5-11B",
            "model_description": "11B-parameter FLAN-T5 (instruction-tuned) model; in experiments LogicLLM meta-training is applied and merged with FLAN instruction-tuning data for multitask optimization.",
            "model_size": "11B",
            "reasoning_task_name": "ReClor and LogiQA-v2 (zero-shot evaluation)",
            "reasoning_task_description": "Reading-comprehension logical reasoning benchmarks requiring multi-step and relational reasoning in natural language.",
            "method_or_approach": "Self-supervised LogicLLM (auto-regressive relation pair reconstruction) combined with FLAN instruction tuning in multitask training; counterfactual data augmentation and mixed LM loss used.",
            "performance": "Baseline zero-shot accuracies: ReClor dev 57.4% / test 59.9%; LogiQA-v2 dev 55.3% / test 53.1%. With LogicLLM & FLAN: ReClor dev 61.2% / test 61.1%; LogiQA-v2 dev 56.0% / test 54.0%. Notably, FLAN-T5-11B w/ LogicLLM outperforms ChatGPT on ReClor dev by 4.8 pts and surpasses ChatGPT on LogiQA-v2 dev/test by ≈1.5/1.3 pts.",
            "baseline_comparison": "Outperforms FLAN-T5 baseline and is competitive with ChatGPT (GPT-3.5) on these logical reasoning benchmarks without supervised fine-tuning on those tasks.",
            "limitations_or_failures": "Some task categories (e.g., CoT-based BBH tasks) show mixed generalization; instruction-tuned models still exhibit prompt variability; scaling beyond 40B not exhaustively explored due to compute.",
            "insights_or_conclusions": "Self-supervised LogicLLM reliably enhances instruction-tuned FLAN-T5 at 11B scale, achieving ChatGPT-comparable or superior zero-shot performance on strict logical benchmarks; indicates logic prior can be learned without supervised labels.",
            "uuid": "e8621.4",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Falcon-40B",
            "name_full": "Falcon (40 billion parameter variant)",
            "brief_description": "A 40B-parameter open large language model (Falcon-40B) evaluated as baseline and after LogicLLM post-training (trained via QLoRA for larger models), showing consistent improvements on logical benchmarks.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "Falcon-40B",
            "model_description": "40B-parameter language model (Falcon); in experiments LogicLLM is applied (training via QLoRA) as a proof that the method can benefit other large model families.",
            "model_size": "40B",
            "reasoning_task_name": "ReClor and LogiQA-v2 (zero-shot evaluation)",
            "reasoning_task_description": "Multiple-choice logical reasoning datasets requiring discourse-level and relational inference.",
            "method_or_approach": "LogicLLM auto-regressive self-supervised meta-training with counterfactual augmentation; training of large model variants used QLoRA adaptations.",
            "performance": "Baseline zero-shot accuracies: ReClor dev 38.4% / test 37.1%; LogiQA-v2 dev 35.9% / test 36.1%. With LogicLLM (QLoRA): ReClor dev 41.4% / test 43.0%; LogiQA-v2 dev 38.6% / test 37.2% (average improvements ≈3.2 points reported).",
            "baseline_comparison": "Improves over Falcon-40B baseline; similar magnitude of gains to LLaMA-13B/33B after LogicLLM.",
            "limitations_or_failures": "Trained with QLoRA due to resource limits — low-rank adaptation may affect absolute gains; still below supervised fine-tuned SOTA on strict logical reasoning tasks.",
            "insights_or_conclusions": "LogicLLM benefits extend beyond LLaMA and FLAN-T5 families; method is applicable to other large models with QLoRA-based fine/adaptation.",
            "uuid": "e8621.5",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT (GPT-3.5-turbo)",
            "name_full": "ChatGPT / GPT-3.5-turbo",
            "brief_description": "OpenAI's GPT-3.5-based ChatGPT model used as an off-the-shelf baseline for zero-shot and CoT evaluations on ReClor and LogiQA-v2; also used in data auto-verification experiments.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-turbo)",
            "model_description": "Instruction-tuned GPT-3.5 family model exposed via API and used as a strong baseline in zero-shot and chain-of-thought prompting evaluations; also used to evaluate logical consistency of constructed training pairs.",
            "model_size": "not specified (API model)",
            "reasoning_task_name": "ReClor and LogiQA-v2; also used to judge logical consistency of training data",
            "reasoning_task_description": "Multiple-choice logical reasoning benchmarks; additionally used as an automatic annotator to judge whether direct/indirect relation pairs are logically consistent (Yes/No).",
            "method_or_approach": "Zero-shot in-context prompting, Chain-of-Thought prompting (CoT) in ablation; used as a human-like rater for automatic verification of data consistency.",
            "performance": "Reported baseline zero-shot accuracies: ReClor dev 56.6% / test 61.2%; LogiQA-v2 dev 54.5% / test 52.7%. CoT variants reported mixed effects (sometimes worse on LogiQA-v2 dev).",
            "baseline_comparison": "Used as a strong baseline; LogicLLM FLAN-T5-11B matches or exceeds ChatGPT on some splits (e.g., ReClor dev). CoT and few-shot improvements for ChatGPT are inconsistent for strict logic benchmarks per paper.",
            "limitations_or_failures": "Chain-of-Thought prompting sometimes degrades performance on strict logical benchmarks; exemplar selection and prompt variability substantially affect outcomes; struggles to learn reasoning structure from few exemplars.",
            "insights_or_conclusions": "Even powerful instruction-tuned API models like ChatGPT do not always reliably benefit from CoT or few-shot for strict logical reasoning tasks; LogicLLM can reach or surpass ChatGPT-level performance on some logic benchmarks without supervised labels.",
            "uuid": "e8621.6",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "OpenAI's GPT-4 model used in this paper for automatic evaluation (data verification) of logically consistent examples extracted from Wikipedia; used only as an annotator/validator.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "High-capability API model from OpenAI; leveraged here to automatically assess whether extracted direct/indirect relation pairs (and counterfactual/anonymized variants) are logically consistent.",
            "model_size": "not specified (API model)",
            "reasoning_task_name": "Automatic verification of logically consistent/counterfactual training pairs",
            "reasoning_task_description": "Binary (Yes/No) judgment whether two sentences describing relations between an entity pair are logically consistent (fuzzy/logical consistency in natural language).",
            "method_or_approach": "Prompt-based automatic evaluation on a random sample (1,000 examples) across normal, counterfactual, and anonymized settings to estimate the fraction of constructed pairs judged logically consistent.",
            "performance": "GPT-4 judged that over 70% of the counterfactual corpus remained logically consistent; anonymization increased judged consistency for counterfactual examples by ~23 percentage points (per paper text; exact table values not reproduced in body).",
            "baseline_comparison": "Compared judgments with ChatGPT; GPT-4's judgments were less perturbed by anonymization and indicated higher consistency rates than ChatGPT.",
            "limitations_or_failures": "Automatic judgments are model-dependent; anonymization and entity-background removal can significantly change model judgments, indicating weakness in causal relation identification by LLMs.",
            "insights_or_conclusions": "GPT-4's higher judged consistency supports the data-construction assumption (direct/indirect relations are often logically consistent), but variation across annotator models reveals limitations of using LLMs for auto-labelling and the fuzzy nature of logical consistency in natural language.",
            "uuid": "e8621.7",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LogicLLM",
            "name_full": "LogicLLM (Self-supervised Logic-enhanced Meta-training)",
            "brief_description": "A fully self-supervised framework introduced in this paper that constructs logic-consistent relation pairs from raw text and uses an auto-regressive objective plus counterfactual augmentation to inject logical reasoning prior into LLMs.",
            "citation_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "mention_or_use": "use",
            "model_name": "LogicLLM (training framework applied to LLaMA, FLAN-T5, Falcon)",
            "model_description": "Method: extract directly-mentioned (single-step) and indirectly-composed (multi-hop) relation descriptions for the same entity pair from Wikipedia; construct paired examples; train LLMs autoregressively to generate one view given the other; use counterfactual entity replacement and mix LM loss to avoid memorization/forgetting.",
            "model_size": "applied to models from 3B to 40B (experiments: 3B, 7B, 11B, 13B, 33B, 40B)",
            "reasoning_task_name": "Applied evaluation on ReClor, LogiQA-v2; additional evaluation on RACE, MMLU, BIG-Bench-Hard (BBH)",
            "reasoning_task_description": "Benchmarks probe discourse-level logical reasoning (ReClor, LogiQA-v2) and broader language understanding/complex reasoning (RACE, MMLU, BBH).",
            "method_or_approach": "Self-supervised: (1) logic-consistent data construction via entity-pair relation extraction, (2) counterfactual augmentation by entity replacement, (3) auto-regressive training objective reconstructing paired relations, with concurrent standard LM loss; ablations vs contrastive (MERIt-style) training included.",
            "performance": "Across multiple model families, LogicLLM produced consistent zero-shot improvements on ReClor and LogiQA-v2 (examples: LLaMA-13B +3.2 pts avg, LLaMA-33B +3.7 pts avg, Falcon-40B +3.2 pts avg, FLAN-T5-11B improved to 61.2% dev ReClor and 56.0% dev LogiQA-v2). FLAN-T5-11B + LogicLLM matched/exceeded ChatGPT on some splits.",
            "baseline_comparison": "Auto-regressive LogicLLM consistently outperforms contrastive (MERIt-style) variants in these LLM in-context settings; counterfactual augmentation outperforms no-augmentation ablation; LogicLLM + instruction tuning yields additive gains over instruction-tuning-only baselines.",
            "limitations_or_failures": "Logical consistency is fuzzy in natural language (not strictly provable); heuristics to build negatives for contrastive learning are noisy; model performance remains sensitive to prompt/instruction variability; CoT and few-shot exemplars give limited improvement for strict logic tasks; scaling beyond 40B and compute-limited training are constraints.",
            "insights_or_conclusions": "Self-supervised logical prior can be injected into LLMs via reconstructive, autoregressive objectives built from naturally-occurring direct/indirect relation pairs plus counterfactual augmentation; this aligns well with in-context learning and token-level supervision and improves strict logical reasoning without degrading broader language understanding.",
            "uuid": "e8621.8",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MERIt: Meta-path guided contrastive learning for logical reasoning",
            "rating": 2,
            "sanitized_title": "merit_metapath_guided_contrastive_learning_for_logical_reasoning"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models",
            "rating": 2,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "The flan collection: Designing data and methods for effective instruction tuning",
            "rating": 2,
            "sanitized_title": "the_flan_collection_designing_data_and_methods_for_effective_instruction_tuning"
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Chain-of-thought hub",
            "rating": 1,
            "sanitized_title": "chainofthought_hub"
        }
    ],
    "cost": 0.019469,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring Self-supervised Logic-enhanced Training for Large Language Models
17 Jun 2024</p>
<p>Fangkai Jiao jiaofangkai@hotmail.com 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>Zhiyang Teng zhiyang.teng@ntu.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Bosheng Ding bosheng001@e.ntu.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Zhengyuan Liu liu_zhengyuan@i2r.a-star.edu.sg 
Institute for Infocomm Research (I</p>
<p>Nancy F Chen nfychen@i2r.a-star.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>Shafiq Joty sjoty@salesforce.com 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>A * Star 
Singapore 
Salesforce Research 
Exploring Self-supervised Logic-enhanced Training for Large Language Models
17 Jun 20247D83B3DA1754CBD33A7A9B2D40A54E77arXiv:2305.13718v7[cs.CL]
Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains.Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks.Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-theart fine-tuning based models.To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning.We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion.LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2).Additionally, Log-icLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model's general language understanding capabilities. 1</p>
<p>Introduction</p>
<p>Logical reasoning serves as a bedrock for negotiation, debate and writing, underpinning our ability to engage with complex cognitive tasks (Yu et al., 2020).An example of logic reasoning in natural language is shown in Figure 1.As the complexity of relations and expressions presented in this task defy straightforward conversion into symbolic or formal languages, perfecting logical reasoning within language models has proven to be a significant challenge (Zhong et al., 2021).Figure 1: An example logical reasoning task from LogiQA-v2 dataset (Liu et al., 2020).The relations between different constituents, e.g., agriculture and development of Andean society, include various predicates, and it is hard to be converted into logical form through either first-order logic or formal language.</p>
<p>Past attempts to incorporate logical reasoning into language models primarily focused on integrating knowledge about logic.For instance, Huang et al. (2021) employed graph neural networks to capture relational semantics, while Wang et al. (2022) used data augmentation to implement first-order logic.These techniques, however, are constrained by their need for extensive annotated training data, which hinders the model's ability to generalize across different tasks due to disparities in data distribution and optimization objectives.</p>
<p>Conversely, recent breakthroughs in Large Language Models (LLMs) like PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), Chat-GPT 2 , GPT-4 (OpenAI, 2023), and Bard 3 offer a promising alternative.These LLMs effectively encapsulate a vast array of knowledge and tackle diverse tasks with minimal specialization, guided by human instruction.Despite their potential, our experiments on logical reasoning benchmarks revealed deficiencies in their logical reasoning capabilities as shown later in our experiments.</p>
<p>Contemporary efforts to fortify LLMs' specific capabilities fall broadly into two categories.The first employs external tools or APIs (Schick et al., 2023;Mialon et al., 2023;Cheng et al., 2022;Gao et al., 2022;Chen et al., 2022), aiding LLMs in argument parsing and semantic understanding.Yet, these tools' utility for logical reasoning remains limited due to the absence of a symbolic language for problem descriptions.The second category, instruction tuning, relies on data augmentation or enriched human feedback but struggles due to the scarcity of task-specific data and high annotation costs (Ouyang et al., 2022;Xu et al., 2023).In this work, we pivot away from these traditional methods and introduce LogicLLM, which performs self-supervised logic-enhanced meta-training for LLMs.It tackles two primary challenges: 1) synthesising logic-consistent data from raw texts ensuring fully self-supervised training, and 2) effectively incorporating logic prior into LLMs while preventing learning problems, such as memorization, forgetting and generalization.</p>
<p>To tackle the first challenge, LogicLLM emphasizes the necessity of understanding and exploiting fuzzy logical consistency.As mentioned previously, strict formal logic is often absent in natural language, we instead treat the relational consistency between different perspectives of relational expressions as an approximation to fuzzy logic consistency 4 .In fact, ensuring logical consistency in a discourse is a key requirement for text coherence and effective information conveyance (Jurafsky and Martin, 2009).We devise a method that inspects the implicit intra-sentence relation of entity pairs at the discourse level to extract logically consistent examples from Wikipedia articles (Figure 2).Specifically, we posit that direct and indirect relations of an anchor entity pair should be logically consistent, as they are derived from the "same" context.For the second challenge, LogicLLM adopts an auto-regressive objective optimizing on the logically consistent relation instances directly to make it seamlessly adapt to its pretraining objective.It tasks the model with gen-erating the alternative perspective (indirect or direct) given a direct or indirect description of the anchor entity pair.We further employ counterfactual data augmentation through entity replacement to enforce relation-centric reasoning, which not only avoids the model's tendency to merely recall results from memory but also ensures the preservation of the logic-enhanced aspect of the learning process.</p>
<p>LogicLLM is task-agnostic and does not require any annotations, making it adaptable to various logical reasoning tasks.We have conducted experiments across two distinct LLM series, FLAN-T5 (Longpre et al., 2023) and LLaMA (Touvron et al., 2023), encompassing a variety of parameter sizes.These experiments are designed to investigate two main questions: (1) Can the logical reasoning capabilities be exclusively improved through self-supervised meta-training for LLMs, thereby circumventing the need for task-specific supervised fine-tuning?(2) How does the logicenhanced meta training affect the LLM's language understanding capabilities, i.e., does it suffer from forgetting or generalization issues?</p>
<p>In response to the first question, our findings suggest that LLMs trained with the LogicLLM objective demonstrate superior performance on logical reasoning benchmarks, eliminating the need for further fine-tuning.Our LogicLLM based on FLAN-T5-11B attain comparable results to Chat-GPT on two logic reasoning benchmarks, Re-Clor (Yu et al., 2020) and LogiQA-v2 (Liu et al., 2022a), highlighting the feasibility of enhancing logical reasoning abilities through self-supervised training alone.</p>
<p>Regarding the second question, our evaluations with LLaMA-based models on three general language understanding benchmarks -RACE (Lai et al., 2017), MMLU (Hendrycks et al., 2021) and BIG-Bench-Hard (BBH) (Suzgun et al., 2022), confirm that the enhanced logical reasoning capabilities do not compromise the model's overall language understanding on MMLU and BBH.In fact, the learned logic ability appears to boost the model's performance in RACE.</p>
<p>Related Work</p>
<p>Large Language Models</p>
<p>In recent years, Large Language Models with incontext learning have emerged as a groundbreaking paradigm in the field of NLP.Unlike the traditional fine-tuning approach, in-context learning leverages natural language instructions or a small number of annotated examples as demonstrations to predict responses for new instances.This unique approach empowers LLMs to serve as a versatile tool for handling multiple tasks without requiring task-specific training.However, recent evaluations of LLMs (Qin et al., 2023;Bang et al., 2023;Jiao et al., 2023;Laskar et al., 2023;Wang et al., 2023a) have revealed a limitation in their ability to learn complex skills like logic and planning through language modeling alone.To address this, even the training of GPT-4 has incorporated labeled matching datasets to enhance its performance in solving math word problems (OpenAI, 2023).Nevertheless, due to the vast amount of data used in pre-training LLMs, annotated data for specific capabilities may be severely undersampled, and the cost of obtaining annotations should not be overlooked.Therefore, it remains crucial to develop various self-supervised or weaklysupervised training methods that do not rely on human annotation.These approaches are essential for constructing more robust and versatile LLMs that can perform a wider range of tasks with higher proficiency and lower resource.</p>
<p>Reasoning in Natural Language</p>
<p>Previous research aimed at natural language reasoning tasks can be broadly classified into three categories.The first category involves explicit prior knowledge, such as discourse structure or linguistic knowledge, to model implicit reasoning processes (Gao et al., 2020;Huang et al., 2021).The second category is neural-symbolic reasoning, where variables are first parsed, and then predefined programs are executed to obtain final results (Wang et al., 2022;Zhong et al., 2021).However, a significant challenge with these methods is the requirement of a robust semantic parser and a self-contained symbolic system for extracting variables or arguments, which is impractical for logic reasoning based on natural language.The third category encompasses methods that focus on general domain pre-training for reasoning via denoising auto-encoding (Jiao et al., 2021;Deng et al., 2021;Liu et al., 2022b).Nevertheless, restricted by the poor task generalization of discriminative models with few parameters, these methods are still in demand of task-specific fine-tuning to activate learned knowledge.</p>
<p>Our approach in this paper falls within the third category, which improves the efforts of MERIt (Jiao et al., 2022) by transforming it into auto-regressive framework to better align the nature of LLMs as generative model.We also drop the usage of knowledge graph enabling enhancing the logic of LLMs through purely self-supervised learning.</p>
<p>LogicLLM</p>
<p>Figure 2 shows the framework of LogicLLM.It involves three main steps: 1) Logic-consistent Data Construction (Section 3.1), which synthesises the logic-consistent data using relation discrimination between entity pairs; 2) Counterfactual Data Augmentation (Section 3.2), which augments the logic-consistent training data by entity sampling and replacement; 3) LLM Training (Section 3.3), which performs continual training of LLMs using the training data generated by the previous two steps.</p>
<p>Logically consistent Data Construction</p>
<p>Ensuring logical consistency in discourse and pragmatics is a fundamental prerequisite for natural language to effectively convey information and maintain coherence.Consequently, logically consistent data is prevalent in text documents and various techniques can be applied to extract them.In this study, we implement this by inspecting intrasentence relation of entity pairs at the discourse level to extract logically consistent examples from Wikipedia.</p>
<p>Direct relation Given an arbitrary paragraph and an anchor entity pair ⟨ e i , e j ⟩, we assume there exists an implicit relation s k between ⟨ e i , e j ⟩ if one sentence directly mentioning them can be found.This comes from the distant supervision (Mintz et al., 2009) and has been employed and extended in self-supervised training by previous work (Deng et al., 2021).For example, the instance ① in Figure 2 is a direct relation.To this end, we simply treat ⟨ e i , s k , e j ⟩ as the direct relation triplet for further data construction.</p>
<p>Indirect relation Entities e i and e j can be indirectly connected through multiple sentences within the input paragraph.</p>
<p>In such situations, we identify a chain of triplets, such as ⟨e i , s i+1 , e i+1 , • • • , s j , e j ⟩, which represents an indirect relation between the entity pair ⟨ e i , e j ⟩ through the relation composition of serial relation triplets ⟨ e i , s i+1 , e i+1 ⟩, ⟨ e i+1 , s i+2 , e i+2 ⟩, • • • , Logical consistency Intuitively, the direct and indirect relations between ⟨ e i , e j ⟩ should be logically consistent since they are derived from same context and describing the same entity pairs.Instances ① and ② in Figure 2 exemplify logically consistent relations.By establishing implicit connections between single-step and multihop reasoning, LLMs gain the ability to understand relation composition process between s k and ⟨s i+1 , s i+2 , • • • , s j−1 ⟩.This capability consequently enhances the LLMs' logical reasoning abilities.</p>
<p>To retrieve logically consistent relation pairs, we follow a two-step process.First, we recognize all entities within each paragraph via distant annotation from WikiData (Wang et al., 2021).And secondly, we enumerate every possible entity pair and search for a series of sentences and check if both direct and indirect relations can be extracted.</p>
<p>Counterfactual Data Augmentation</p>
<p>The work we have described in Section 3.1 produces logically consistent data that correlates entities and relations within reasoning paths.To enhance entity-irrelevant reasoning and ensure LLM focuses more on the process of relational composition rather than the entities themselves, we have additionally introduced counterfactual data augmentation.This approach, similar to the method suggested by Jiao et al. (2022), includes the random replacement of entities.</p>
<p>To create counterfactual examples of ⟨ e i , e j ⟩ within paragraph P , we initially select a random paragraph, denoted as Q, from a separate document.Subsequently, we sample a new set of entities, such as e a , e a+1 , • • • , e b from Q.The head and tail entities in the original relation instances of ⟨ e i , e j ⟩ are then substituted by these randomly sampled entities, maintaining the relationships unchanged.For instance, after substituting e i and e j with e a and e b , ③ and ④ become the counterfactual augmentations of ① and ②, respectively.In our research, we postulate that the logic-consistency between s k and s i+1 , e i+1 , s i+2 , • • • , s j−1 remains undisturbed in the counterfactual examples.This assertion is based on the idea that logical relationships within a paragraph's context are primarily driven by shared entities and their interconnections rather than the specific entities themselves.</p>
<p>Training Objective</p>
<p>During the training phase, we apply continual training to LLMs using logic-consistent data.Drawing inspiration from the success of in-context learning, we treat one relation from a logicconsistent relation pair as the in-context example and task the LLM with generating the other relation.As depicted in Figure 2, using the logicconsistent pair ⟨①, ②⟩ as an example, when ① is given as the conditional input, the LLM is expected to produce ② as the output, and vice versa.This process intuitively forces the LLM to reason the logic-consistent connections between the input and output relations since they are from the same context and the entity pairs of ① and ② are both e i and e j .</p>
<p>Formally, we denote the data extracted from Section 3.1 and Section 3.2 as
D = {⟨R 1 i , R 2 i ⟩} N i=1
, where N represents the number of training examples, and ⟨R 1 i , R 2 i ⟩ is the i-th logic-consistent record.Here, R 1 i refers to the direct relation-related instance, while R 2 i represents the instance with an indirect relation.The goal of LLM training is to minimize the negative log-likelihood function as follows:
L logic = − N i=1 [log P (R 1 i |R 2 i ) + log P (R 2 i |R 1 i )] = − N i=1 [ |R 1 i | j=1 log P (R 1 i,j |R 1 i,&lt;j , R 2 i ) + |R 2 i | j=1 log P (R 2 i,j |R 2 i,&lt;j , R 1 i )],(1)
where R 1 i,j , R 2 i,j denotes the j-th token of R 1 i and R 2 i , respectively.Furthermore, we incorporate the another causal language modeling loss L lm to mitigate the catastrophic forgetting problem.Both L lm and L logic are implemented as auto-regressive decoding.The only difference is that they sample from different data source.L lm continuously samples data from the subset of training corpus used during the laststage pre-training, i.e., Wikipedia paragraphs for LLaMA series models, and FLAN-collection-v2 for FLAN-T5 series models.Therefore, the over-all training objective is defined as:
L = L logic + L lm .
(2)</p>
<p>During training, for each forward-backward, we randomly sample two mini-batches with the same size from the datasets for logic-enhanced training and language modeling, respectively, and merge them into a single one.</p>
<p>Experiment</p>
<p>We integrate our pre-training approach into two prominent LLMs: LLaMA (Touvron et al., 2023) and FLAN-T5 (Wei et al., 2022a).These models boast parameter sizes ranging from 3 billion to 30 billion.To thoroughly evaluate the capability of LLMs from various angles, we have carefully selected five datasets representing three distinct categories.ReClor (Yu et al., 2020) and LogiQA-V2 (Liu et al., 2020) are two logical reasoning benchmarks sourced respectively from standardized graduate admission examinations and logical examination papers intended for reading comprehension.RACE (Lai et al., 2017) is a reading comprehension task that assesses general reasoning abilities.MMLU (Hendrycks et al., 2021) is used for measuring the learned knowledge and massive multitask language understanding, and BIG-Bench-Hard (BBH) (Suzgun et al., 2022) is a collection of multiple challenging tasks where LLMs fall behind human being.By employing MMLU and BBH, we aim to verify whether the logic-oriented meta-training negatively impacts the models' ability to generalize across a wide range of tasks.Due to space limitation, more implementation details can be found in Appendix A.</p>
<p>5 Results and Analysis</p>
<p>Logical Reasoning</p>
<p>Table 1 shows the results on ReClor and LogiQA-v2 under zero-shot setting.From the table we can find that the performance of LLaMA-based models is notably lower compared to ChatGPT.By training LLaMA models with LogicLLM, we observe significant enhancement in their zero-shot logical reasoning capabilities.For instance, on LLaMA-13B and LLaMA-33B, the average improvements across the four dataset splits are 3.2 and 3.7 points, respectively.The benefits are more substantial than those observed in the 7B models (0.9 points), which aligns with the findings  (Dettmers et al., 2023).on emergent abilities (Wei et al., 2022b).This could be attributed to the fact that larger models possess stronger generalization abilities and better apply their learned capabilities to different tasks.We also conducted experiments on Falcon-40B (Penedo et al., 2023), and found that LogicLLM brings an average improvement of 3.2 points.</p>
<p>Consistent with LLaMA-based models, we can draw similar conclusions for those based on FLAN-T5, where logic-oriented meta-training also yields improvements for both FLAN-T5-3B and FLAN-T5-11B.For FLAN-T5-11B, our model achieves accuracies of 61.2 and 61.1 on the development and test sets of ReClor, respectively.On the development and test sets of LogiQA-v2, our logic-oriented FLAN-T5-11B model achieves accuracies of 56.0 and 54.0, respectively.Notably, on the development set of ReClor, our logic-oriented FLAN-T5-11B model outperforms ChatGPT by a significant margin of 4.8 accuracy points.Similarly, on the development and test sets of LogiQA-v2, our logic-oriented FLAN-T5-11B model surpasses ChatGPT by 1.5 and 1.3 accuracy points, respectively.These overall results indicate that instruction tuning on multiple supervised datasets, such as the FLAN collection, can still be improved for learning logic.We hypothesize that this may be attributed to the sparsity of reasoningrelevant data in the entire collection and the conflicts between different tasks.</p>
<p>Hybrid Reasoning and Application</p>
<p>In addition to logical reasoning in text, we are also curious about whether logic-enhanced training contributes to general language understanding (RACE), and maintain the general capabilities on massive knowledge based tasks (MMLU).To investigate this, we evaluate the performance of the enhanced LLaMA models on these two datasets.</p>
<p>As shown in Table 2, from 7B to 33B, Logi-cLLM can consistently improve the performance on RACE, except the one of LLaMA-33B w/ Log-icLLMon the test set.Specifically, LLaMA-7B w/ LogicLLM obtain around 4.2 absolute improvements, and LLaMA-13B w/ LogicLLM achieves 1.5 improvements, which has verified that the logic-enhanced training is also beneficial to general reasoning and reading comprehension.Additionally, we find that LogicLLM can also benefits the massive multitask language understanding (MMLU) on LLaMA-7B and 13B.We find that the improvements of both RACE and MMLU on LLaMA-33B are marginal, probably because lowrank adaptation have restricted the generalization.</p>
<p>Pre-training Strategy</p>
<p>LogicLLM draws inspiration from the contrastive learning framework for logical reasoning, i.e., MERIt, which has demonstrated its efficacy in fine-tuning based approaches.As mentioned earlier, we hypothesize that contrastive learning may be inadequate for LLM with in-context learning.To validate this assumption, we examine the effects of contrastive learning (ctr) and auto-regressive generation (ar).In the case of contrastive learning, we adopt the methodology of MERIt to construct logically inconsistent instances and optimize the model by maximizing the distance between logically consistent instances and the inconsistent counterparts.Referring to the table, it can be observed that LogicLLM (ctr) fails to yield significant improvements compared to LLaMA-13B, except for the dev set of Re-Clor.Conversely, the auto-regressive models consistently outperform both the baseline models and the contrastive methods by considerable margins across all dataset splits.We propose two primary reasons to explain the superiority of autoregressive models over the contrastive approach.</p>
<p>First, the heuristic construction process for negative candidates used in contrastive learning fails to identify true contradictory relations, resulting in randomly chosen negative samples that lack logically opposite relationships with the positive instances.To this end, the contrastive learning process can degrade into a positive-only optimization process, which is similar to auto-regressive learning but receives less token-level supervision.</p>
<p>Second, the divergence between the training objectives of contrastive learning and auto-regressive generation undermines the model's ability to effectively do in-context reasoning.Contrastive learning primarily focuses on discriminating positive pairs from negative pairs based on a global semantic perspective.Auto-regressive models, on the other hand, accumulate their ability through local token prediction.During inference, LLMs are expected to understand instruction, and jointly consider the logical relations between different hypothesises within single input.By placing emphasis on fine-grained relations, the auto-regressive objective can better support in-context learning, enabling the model to grasp the nuanced connections and reasoning processes required for logical understanding.</p>
<p>Moreover, the auto-regressive objective signifi- Table 4: Ablation study to explore if LogicLLM can be combined with instruction tuning.For FLAN-T5 , we use the subset of FLAN collection.For LLaMA, we introduce GPT4All (Anand et al., 2023).cantly reduces computation costs during training by eliminating the need for negative candidates encoding.The streamlining of training process leads to more efficient and resource-friendly training without sacrificing performance.We also add another experiment by adjusting the ratio between counterfactual data and the normal ones as 1:1, and the comparison reveal that mixing more counterfactual data can also benefit the performance, which could be especially useful for low-resource domain, like finance and multi-lingual LLMs.</p>
<p>In summary, considering the advantages in both performance and training cost, the auto-regressive variant proves to be a superior choice for incorporating logic reasoning into LLMs.</p>
<p>Factors Relevant to Logic Prior</p>
<p>In Table 3, we also present the ablation results on LLaMA-33B when the counterfactual data augmentation strategy is omitted.Without the inclusion of counterfactual data, LogicLLM degrades into a conditional generative task that can be solved through memorization, as each sample has its own prototypes within Wikipedia.</p>
<p>As indicated in the table, even without the augmentation (no aug.), LogicLLM still contributes to the enhancement of logical reasoning abilities, albeit with more limited improvements.However, the introduction of counterfactual data augmentation to eliminate memorization effects can further amplify the benefits.The overall experimental results point out that relation construction serves as effective supervision signal for introducing logic prior.We leave the work about developing novel techniques to prevent memorization but less involve factual noise as future work.</p>
<p>Compatibility with Instruction Tuning</p>
<p>Instruction tuning has served as a critical step to make LLMs better in following human instruction, and/or generating with less toxic.In this section, we hope to study if LogicLLM can be well integrated with supervised instruction tuning so that LogicLLM has the potential to serve as a basic approach to train logic-enhanced foundation model before building applications.For FLAN-T5, we directly use the same subset of FLAN collection with our approach as the instruction tuning data.For LLaMA models, we introduce GPT4All (Anand et al., 2023) data for extra supervision.During training, we simply sum the loss of instruction tuning and LogicLLM in multitask training manner to keep the same data ratio.</p>
<p>As shown in Table 4, on most dataset splits, LogicLLM can achieve additional improvements compared with the instruction tuning-only baselines.Specifically, we find that the improvements are more significant on ReClor that those on LogiQA-v2.One possible reason is that the language style in LogiQA-v2 is more close to formal language, leaving a gap with the natural user questions.</p>
<p>Data Assumption Auto-Verification</p>
<p>In order to verify the rationality of our assumption that the direct and indirect relations are logically consistent, we employ ChatGPT and GPT-4 for automatic evaluations.Specifically, we randomly sample 1,000 examples from the development set for our pre-training with the ratio of normal data and counterfactual ones as 1:1.For each data pair, we ask ChatGPT/GPT-4 to determine if the relation between the target entities are logically consistent.The prompt we used is shown in Appendix E. We have involved four different settings.Beside the normal data and the counterfactual ones, we have also applied anonymization (Qiu et al., 2020) to them to decouple the background knowledge from entity.Specifically, the target entities are replaced with [X] and [Y], and for counterfactual data, the other replaced entities during data augmentation are not further anonymized.Some cases can also be found in Appendix E for clearer understanding.</p>
<p>Our results are shown in Tabel 5, from which we can observe that: (1) for normal data, Chat-GPT and GPT-4 deem that the logically consistent data occupie high ratios, which has initially verified the rationality of our data construction assumption.</p>
<p>(2) For counterfactual data, the ratios significantly decrease.Yet, in the view of GPT-4, there is still more than 70% of logically consistent data in the whole corpus.(3) When combined with entity anonymization, the ratios become much higher for counterfactual data, i.e., nearly 15% absolute improvements for ChatGPT and 23% for GPT-4.Besides, the ratio of normal data decreases significantly for ChatGPT, but is less perturbed for GPT-4.The observation further demonstrates that most counterfactual data should also hold the assumption since the anonymization only remove the backgrounds of entities, yet leaving the context as original.And the great variation brought by counterfactual data augmentation also reveals the potential weakness of current LLMs on identifying the true causal relations.</p>
<p>Robustness</p>
<p>By training LLMs on logic-consistent data and counterfactual augmentations, they are exposed to a wide range of input variations.This exposure helps them become less sensitive to minor perturbations such as shuffling of input options.To determine the robustness of LogicLLM , we conducted experiments on LogiQA-v2 using models of varying sizes.We shuffled the input order of different options and reperformed the inference process.</p>
<p>Figure 3 illustrates the findings of our experiments.We observed that LLaMA exhibited higher variance across different input option orders, as indicated by the greater spread in results.The circular outlier values that indicate specific input orders causing significant variations, leading to substantially higher or lower performance results.Our observation is consistent with the recent findings of Wang et al. (2023b), suggesting that the normal LLMs heavily suffer from position bias.In contrast, when LLaMA is enhanced with Logi-cLLM, it achieves more stable performance across different parameter sizes.Moreover, the averaged performance of LLaMA w/ LogicLLM is significantly superior to that of LLaMA alone.These results show that LogicLLM produces consistent and improved results compared to traditional LLMs, demonstrating the value of incorporating logic-enhanced training techniques into LLMs.</p>
<p>Training Quality Analysis</p>
<p>In order to analyze the quality of our metatraining, we have constructed a test set using the framework of MERIt (Jiao et al., 2022), which contains both logically consistent and inconsistent data.We have measured the log-likelihood on each sample as illustrated by Equation 1, and report the averaged results in Figure 4.</p>
<p>As shown in the figure, for logically consistent data, LogicLLM significantly reduced the negative log-likelihood.Moreover, the 7B-based model with LogicLLM surpasses the performance of LLaMA-13B.Notably, the disparity between the negative log-likelihood of logically consistent and inconsistent instances is further amplified, highlighting the effectiveness of LogicLLM in logical relation reconstruction.Furthermore, our experiments suggest a decrease in the negative log-likelihood for logically inconsistent data.This observation exposes a weakness in the contrastive learning-based method, i.e., MERIt, wherein the heuristic process for generating negative candidates introduces considerable noise.Consequently, some negative instances may not genuinely present contradictory logical relations.</p>
<p>Conclusion</p>
<p>In this paper, we have explored the feasibility and effectiveness of enhancing logical reasoning of LLMs via purely self-supervised training.We evaluate the performance based on two LLM series, i.e., FLAN-T5 and LLaMA.The experimental results on two logical reasoning benchmarks, LogiQA-v2 and ReClor, demonstrate the effectiveness of our method.And the performance on RACE, MMLU and Big-Bench-Hard have also verified that the framework do not hurt the generalization of LLMs.Finally, we have analyzed the factors relevant to logic during training, and the compability with supervised instruction tuning.We hope the analysis could bring new insights to future research.</p>
<p>A Implementation Details</p>
<p>A.1 LLM Prompting</p>
<p>In order to evaluate the generalization capabilities of LLMs across different tasks after post-training, we adopt a prompting-based approach.Here, the input to the LLMs is structured as Instruction [Exemplars] Task input.The instruction is tailored to the specific task at hand, while exemplars are utilized only in a few-shot setting.Each exemplar comprises both the task input and its corresponding output.For tasks such as multiplechoice question answering, the task input is a concatenation of the context, the question, and all potential options.The correct option index is used as the output.Besides, in a Chain-of-Thought (CoT) setting, we include a reasoning process formulated in natural language between the task input and output.</p>
<p>A.2 Data</p>
<p>We have constructed our self-supervised logicenhanced training data from Wikipedia, where we directly used the paragraph corpus pre-processed by Qin et al. (2021).We have constructed around 200K logically consistent sample pairs.After that, we further performed counterfactual data augmentation with the ratio of 1:3, and finally induced 800K training sample pairs in total.The data construction process mainly follows the original setting of Jiao et al. (2022)  we have dropped the negative candidates since we employed auto-regressive training.</p>
<p>For language modeling, we employed different dataset with respect to the data used in their last stage training.For FLAN-T5 series models, we used the subset of FLAN-collection-v2 (Longpre et al., 2023); while for LLaMA series models, we used the same Wikipedia paragraphs from the corpus of Qin et al. (2021).</p>
<p>A.3 Hyper-parameters of Training</p>
<p>During the pre-training process, we set the batch size to 4,096, which is implemented using gradient accumulation.The maximum sequence length is truncated at 1,024 for the FLAN collection and 512 for the MERIt corpus.For the FLAN-T5 series models, we conduct training steps for 200 iterations, while for the LLaMA series models, we perform training steps for 500 iterations.The learning rates are set as follows: 1e-4 for FLAN-T5-3B, 5e-5 for FLAN-T5-11B, 1e-5 for LLaMA-7B, and 5e-6 for LLaMA-13B.To carry out the training process, we utilize 8 NVIDIA A100 80G GPUs.However, due to hardware limitations, models larger than 13B are trained using QLoRA (Dettmers et al., 2023), a low-rank adaptation approach specifically designed for quantized LLMs.We follow the setting used in QLoRA with α as 16 and r as 64.All linear layers are used for adaptation and the LoRA dropout is 0.05.The learning rate for LLaMA-33B and Falcon-40B is set as 5e-4.</p>
<p>A.4 Evaluation</p>
<p>To ensure a fair comparison, we maintain consistency across different models for each dataset.This involves using identical instructions and fewshot samples.We use accuracy as the evaluation metric across all experiments.The prompts for different dataset can be found in Appendix D.</p>
<p>B Interpretation for Different Results on RACE</p>
<p>In this section, we will discuss the different results on RACE between ours and those reported by the original paper of LLaMA.Specifically, Touvron et al. (2023) do not report the weighted results, so we convert them by ourselves.The results are shown in Table 7. From the table we can find that only LLaMA-7B cannot match the performance reported by the authors.On LLaMA-13B and LLaMA-33B, our reproduced accuracies are much higher than the reported ones, which can help address the concern of unfair comparison, and demonstrate the effectiveness of our proposed LogicLLM.</p>
<p>C Logic-enhanced Meta-training for Complex Task Understanding</p>
<p>We evaluated the performance of logic-enhanced pre-trained models on BIG-Bench-Hard, a benchmark comprising challenging tasks where human performance surpasses that of LLMs.Table 8 presents the results achieved by the LLaMA and FLAN-T5 models under three evaluation settings: zero-shot, direct few-shot, and CoT.</p>
<p>In the zero-shot setting, our logic-enhanced meta-training significantly improves all four investigated models.For instance, the zero-shot accuracies of LLaMA-13B and FLAN-T5-T5-11B are 25.0% and 38.0%, respectively.When combined with the LogicLLM model, the accuracy scores of LLaMA-13B and FLAN-T5-11B improve to 26.3% and 44.1%, respectively.Some tasks included in BBH require free-form answers thus we cannot evaluate the models by selecting the candidate with lowest perplexity or log likelihood.Instead, we need to follow the evaluation of API-based models, which employs regularization expression to capture the answer from the response.However, smaller language models, especially those without being instruction tuned, fail to accept diverse instruction, and generate structured response.As a result, the absolute performance under zero-setting setting of LLaMA-based models are relatively limited.</p>
<p>On the other hand, the direct few-shot results outperform the zero-shot results in three out of four models, with the exception of FLAN-T5-11B.Similarly, logic-enhanced meta-training boosts the performance of models, except for FLAN-T5-11B.In the CoT setting, our method further enhances the performances of LLaMA-13B and FLAN-T5-3B.However, the best direct few-shot and CoT results (42.6% and 40.9%, respectively) are both inferior to the best zeroshot result (44.1%).Notably, the CoT results on FLAN-T5-3B are significantly worse than the zero-shot and direct few-shot results.These observations suggest the potential drawback that learning CoT from annotated training data, i.e., FLAN collection, has difficulty in generalizing to different task categories, for example, learning CoT from math word problem solving and solving logical puzzles.We provide further discussion on these findings in Appendix G.</p>
<p>D Prompt Template</p>
<p>E.2 Normal Version</p>
<p>[User]:</p>
<p>Determine whether the relation between "Everdingen" and "Sweden" in the given two sentences are logically consistent.The output should either be Yes or No.</p>
<p>[ChatGPT]:</p>
<p>Yes.The output should either be Yes or No.</p>
<p>E.3 Counterfactual Version</p>
<p>[ChatGPT]:</p>
<p>No.</p>
<p>Entity replacement:</p>
<p>• Everdingen → Nicholas Roerich;</p>
<p>• Sweden → Master;</p>
<p>• Norwegian (connecting entity) → Canal del Dique;   The output should either be Yes or No.</p>
<p>E.4 Anonymized Version</p>
<p>[ChatGPT]:</p>
<p>Yes.</p>
<p>F Discussion about Different Perspectives of Logical Reasoning</p>
<p>In our opinion, logic can be reflected through multiple aspects.Here, we use a simple logic rule to discuss the different perspectives:
(α → β) ∧ (β → γ) ↔ α → γ.(3)
The above equation shows the simplest case of first-order logic reasoning, where α, β and γ are different variables, and ∧ is logical and.We can also introduce the necessary logical connectives in natural language to make it easier for understanding:
IF α → β AND β → γ, THEN α → γ. (4)
It should be noted that, in symbolic logic, we often ignore the actual meaning of relations.However, we can always find a path, i.e., a series of relation triplets from knowledge graph to transform the above symbolic form into natural language based logical reasoning process:
IF α r 1 −→ β AND β r 2 −→ γ, THEN α r 3 −→ γ.
(5) One example here can be: r 1 refers to is the father of, r 2 refers to is the mother of, and r 3 refers to is the grandpa of.</p>
<p>From the above discussion, we can conclude that (1) logical connectives focus on discourselevel connections, (2) symbolic logic can be viewed as the simplified version of logical reasoning in natural language, where we focus more on the formal rules of atomic logic operations, and (3) relational reasoning concentrates on the actual logic operations built on world knowledge.Both of what we have discussed in the paper and the reviewers have mentioned in comments, i.e., logical connectives, are indeed different perspectives of logical reasoning.They do not contradict to each other, and discussing them separately is beneficial to make the problem easier.Besides, there are also several studies also discuss logical reasoning from the relational reasoning perspective (Wong et al., 2023;Xu et al., 2021;Zeng et al., 2021;Wang et al., 2022).And Figure 1 also shows the case emphasizing relational reasoning.</p>
<p>G Weakness of LLMs on Logical Reasoning</p>
<p>Table 9 showcases the evaluation results of LLMs' performance in both few-shot and CoT settings.</p>
<p>The intermediate reasoning process is automatically generated by ChatGPT using the prompt "Let's think step by step."In the case of zeroshot CoT, we include the suffix prompt "So the answer is" to guide the models in summarizing and concluding the answer.For few-shot CoT, the reasoning process is initially generated for each sample in the training set.Subsequently, we retain the samples where the final prediction is correct, following the steps outlined in zero-shot CoT.During testing, we randomly select samples from the retained candidates, as well as the automatically generated CoT, to serve as exemplars.However, our observations indicate that both few-shot learning and the use of CoT do not significantly improve the models' performance.For example, ChatGPT w/ CoT performs much worse than that without CoT on the development set of LogiQA-v2.One potential reason for this is that the selected samples differ substantially from the target example.To investigate further, we incorporate reasoning category information during exemplar selection.In LogiQA-V2, each question is annotated with a reasoning category, such as categorical reasoning, sufficient conditional reasoning, or necessary conditional reasoning.For few-shot CoT prompting, we only consider candidates that share at least two common reasoning categories.This particular variant is denoted as "ChatGPT w/ CoT + Cate." in the table.</p>
<p>Despite these efforts, we find that carefully selecting prompting exemplars only provides limited improvement.The results indicate that LLMs struggle to comprehend the reasoning structure from a limited number of observed examples.Consequently, they face challenges in effectively learning the mapping between input-label and input-rationale-label.Additionally, as shown in Table 1, we observe that LogicLLM also contributes minimally to addressing this issue.We recognize the need for further investigation in this area and leave it as a potential avenue for future research.</p>
<p>In 1644 ,Figure 2 :
16442
Figure 2: The LogicLLM framework.P and Q are two arbitrary paragraphs from Wikipedia.In Step 1, we extract intra-sentence relations ①: ⟨ e i , s k , e j ⟩, and the compositions of them ②: ⟨e i , s i+1 , e i+1 , • • • , s j , e j ⟩ from P for an entity pair ⟨ e i , e j ⟩; ① and ② are direct and indirection relations, respectively.Here s k is a relation, represented by the sentence that mentions ⟨ e i , e j ⟩. ① and ② are viewed as logically consistent since both of them describe the "same" relation between ⟨ e i , e j ⟩ from different view.In Part I of the figure, e i refers to Everdigen and e j represents Sweden.The intermediate entity is Norwegian here.The direct relation on the left says that Everdigen has traveled to Sweden, and the indirect relation implies the fact that Everdigen has probably visited Sweden as well as its nearby area, otherwise he could not complete the sketches of Norwegian, demonstrating the fuzzy logic consistency with high probability.Step 2 is the process of counterfactual data augmentation, where counterfactual relation composition is generated by random entity replacement.③ and ④ are the counterfactual augmentations of ① and ②, respectively.Finally, in Step 3, the LLM is optimized to generate direct/indirect relations with their logically consistent indirect/direct counterparts as inputs.Here, ①→ ②, ②→ ①, ③→ ④, and ④→ ③ are considered.</p>
<p>Figure 3 :
3
Figure 3: Results of 5 experiments with different option input orders across different model sizes on the test set of LogiQA-v2.Brown circular marker: outlier, green triangle: arithmetic mean value.</p>
<p>Figure 4 :
4
Figure 4: The averaged log-likelihood value of different models on the self-constructed logically consistent and inconsistent instances, respectively.w/ L. refers to the models augmented with LogicLLM.</p>
<p>of Frans Post, Everdingen took advantage of this mishap by making sketches of the Norwegian landscape, which would have seemed very exotic to his Dutch countrymen.His annotated drawings document visits to the south -east Norwegian coast and to Bohusland and the Göteborg area in western Sweden.Sentence 2: In 1644 Everdingen travelled to Norway and Sweden, a trip that was to have profound consequences on his art.</p>
<p>Roerich travelled toNorway and Master , a trip that was to have profound consequences on his art .</p>
<p>[</p>
<p>User]: Determine whether the relation between "[X]" and "[Y]" in the given two sentences are logically consistent.</p>
<p>of Frans Post, [X] took advantage of this mishap by making sketches of the Canal del Dique landscape , which would have seemed very exotic to his Dutch countrymen.His annotated drawings document visits to the south -east Canal del Dique coast and to Bohusland and the Göteborg area in western [Y].Sentence 2: In 1644 [X] travelled to Norway and [Y], a trip that was to have profound consequences on his art .</p>
<p>Table 1 :
1
The results on logical reasoning benchmarks.Better results are annotated in bold.† refers that the corresponding model is trained through QLoRA
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.ChatGPT56.6 61.2 54.5 52.7LLaMA-7B30.2 30.3 27.4 28.1w/ LogicLLM32.4 31.0 27.7 28.6LLaMA-13B30.4 33.5 33.0 32.1w/ LogicLLM37.4 36.3 34.1 34.0LLaMA-33B45.2 50.3 41.2 41.6w/ LogicLLM  †50.2 54.4 45.9 42.6Falcon-40B38.4 37.1 35.9 36.1w/ LogicLLM  †41.4 43.0 38.6 37.2FLAN-T5-3B54.6 52.5 48.7 48.7w/ LogicLLM &amp; FLAN 55.8 54.1 50.8 50.1FLAN-T5-11B57.4 59.9 55.3 53.1w/ LogicLLM &amp; FLAN 61.2 61.1 56.0 54.0</p>
<p>Table 2 :
2
The results of LLaMA models on RACE and MMLU.† means training through QLoRA.
RACEMMLUModel / DatasetDevTest 0-shot 5-shotAcc. Acc.Acc.Acc.LLaMA-7B31.3 32.333.336.2w/ LogicLLM37.3 37.934.636.6LLaMA-13B55.8 54.541.146.7w/ LogicLLM57.7 55.643.347.3LLaMA-33B68.4 68.154.358.3w/ LogicLLM  † 68.8 68.154.458.3ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.LLaMA-13B30.4 33.5 33.0 32.1w/ LogicLLM (ctr)33.4 33.3 33.1 32.7w/ LogicLLM (ar)37.4 36.3 34.1 34.0LLaMA-33B45.2 50.3 41.2 41.6w/ LogicLLM  † (no aug.) 49.4 53.0 44.2 40.8w/ LogicLLM  † (1 aug.)50.8 52.7 45.6 41.5w/ LogicLLM  †50.2 54.4 45.9 42.6</p>
<p>Table 3 :
3
The effect of different training objectives.Ctr refers contrastive learning and ar means the autoregressive variant.no aug.means the counterfactual data augmentation is removed from the Logi-cLLM framework.</p>
<p>† means that the model is trained with QLoRA.</p>
<p>Table 5 :
5
The ratio of consistent data deemed by Chat-GPT and GPT-4.Anony.refers to anonymization and C.F. is the simplification of Counterfactual.</p>
<p>Table 6 :
6
(Jiao et al., 2022)y of LLMs, i.e., Chat-GPT (GPT-3.5-turbo)andLLaMA,and existing stateof-the-art methods(Jiao et al., 2022)on logical reasoning benchmarks.The evaluation of LLMs follows zeroshot in-context learning setting, where the models are expected to decode the answer based on the given instruction, context, and question.
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.RoBERTa-L.62.655.659.857.0MERIt (RoBERTa-L)69.461.662.659.3MERIt (DeBERTa-XXL) 80.678.1--LLaMA-7B28.828.324.423.7LLaMA-13B31.634.431.631.1LLaMA-33B45.250.341.241.6GPT-3.5-turbo56.661.254.552.7w/ CoT58.857.7-53.1</p>
<p>Table 7 :
7
except two differences.First, we remove the usage of knowledge graph for relation annotation to enable fully self-supervision and simplify the construction workflow.Secondly, The comparison on RACE dataset between our reproduced results and those reported by the opriginal paper of LLaMA.
High Middle WeightedLLaMA-7B46.961.151.0LLaMA-7B (Ours)--32.3LLaMA-13B47.261.651.4LLaMA-13B (Ours)--54.5LLaMA-33B48.364.152.9LLaMA-33B (Ours)--68.1</p>
<p>Table 8 :
8
The accuracy of LLaMA and FLAN-T5 based models on BIG-Bench-Hard.Direct refer to few-shot setting through direct prompting, where only the final answer is given.Instead, in CoT setting, the reasoning process is also concatenated.The exemplars used for direct few-shot prompting and CoT prompting are consistent in each task, which are officially provided.
Model / DatasetZero-shot Direct CoTLLaMA-7B24.930.4 27.0w/ LogicLLM25.230.8 25.9LLaMA-13B25.034.7 32.3w/ LogicLLM26.335.0 33.9FLAN-T5-3B38.040.2 35.1w/ LogicLLM &amp; FLAN40.541.2 36.7FLAN-T5-11B43.042.6 40.9w/ LogicLLM &amp; FLAN44.136.2 40.2</p>
<p>Table 9 :
9
The results on logical reasoning benchmarks with enhanced Chain-of-Thought prompting.
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.zero-shotChatGPT56.6 61.2 54.5 52.7w/ CoT58.8 57.7 54.5 53.15-shotChatGPT61.0 63.0 55.1 54.5w/ CoT62.0 62.5 47.6 55.6w/ CoT + Cate. N/A N/A 55.8 55.0
In this paper, we will use the term logical consistency to represent consistency in fuzzy logic for simplification, which is further described by relational consistency. This means that the relationship between a logically consistent data pair has a higher degree of logical consistency but cannot be strictly proved considering the diverse expressions of relations.
In practice, we find 87% indirect relations are composed of two relation triplets, 12% contain three triplets, and less than 1% have more than 4 triplets. This prevents the logical consistency be weakened by long context.
AcknowledgementsThis research is supported by the Ministry of Education, Singapore, under its Science of Learning Grant (award ID: MOE-MOESOL2021-0006). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the Ministry of Education, Singapore.Besides, we sincerely appreciate the valuable comments from all the reviewers to help us make the paper polished.We also greatly thank to Chengwei Qin and Professor Aixin Sun for their kind suggestions.code and models are released at https://github.com/SparkJiao/LogicLLM.LimitationsIn this paper, we have explored the feasibility to introduce logical reasoning capability into LLMs via purely self-supervised meta-training.Though the results have demonstrated significant improvements on logical reasoning benchmarks, there are also some limitations: Randomness from Diverse Prompt/Instruction.In our experiments, we find that the performance of LLMs, especially those never optimized by instruction tuning, is varying to different prompts.We try to reduce the variance by (1) using simpler prompt (as shown in Section D or (2) using the released prompt by commonly accepted benchmark or leaderboard, e.g., MMLU, Big-Bench-Hard and Chain-of-Thought Hub(Fu et al., 2023).Nevertheless, this still cannot entirely keep the certainty of the experimental results.Non-uniform Evaluation Strategy.Currently, there is no de facto technical standard for LLMs evaluation.Some work just let language models generate the response and match the content.However, this can be unfair for non-instructiontuned models since they often cannot generate meaningful and complete sentences, especially those under 13 billion parameters.Scaling.Due to the resource limitation, we can only scale the method into models with 40 billion parameters under the help of low-rank adaptation.
Gpt4all: Training an assistant-style chatbot with large scale data distillation. Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar, 2023from gpt-3.5-turbo</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 10.48550/arXiv.2302.04023CoRR, abs/2302.040232023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 10.48550/arXiv.2211.12588CoRR, abs/2211.125882022</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, 10.48550/arXiv.2210.02875CoRR, abs/2210.028752022</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, 10.48550/arXiv.2204.02311David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</p>
<p>Reasonbert: Pre-trained to reason with distant supervision. Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, Huan Sun, 10.18653/v1/2021.emnlp-main.494EMNLP. ACL2021</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, CoRR, abs/2305.143142023</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, 10.48550/arXiv.2305.17306CoRR, abs/2305.173062023</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 10.48550/arXiv.2211.10435CoRR, abs/2211.104352022</p>
<p>Discern: Discourseaware entailment reasoning network for conversational machine reading. Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq R Joty, C H Steven, Caiming Hoi, Irwin Xiong, Michael R King, Lyu, 10.18653/v1/2020.emnlp-main.191EMNLP. ACL2020</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, ICLR. OpenReview2021</p>
<p>DAGN: discourse-aware graph network for logical reasoning. Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, Xiaodan Liang, 10.18653/v1/2021.naacl-main.467NAACL-HLT. ACL2021</p>
<p>REPT: bridging language models and machine reading comprehension via retrieval-based pre-training. Fangkai Jiao, Yangyang Guo, Yilin Niu, Feng Ji, Feng-Lin Li, Liqiang Nie, 10.18653/v1/2021.findings-acl.13Findings of ACL/IJCNLP. ACL2021</p>
<p>Merit: Meta-path guided contrastive learning for logical reasoning. Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie, 10.18653/v1/2022.findings-acl.276Findings of ACL. ACL2022</p>
<p>Is chatgpt A good translator? A preliminary study. Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing Wang, Zhaopeng Tu, 10.48550/arXiv.2301.08745CoRR, abs/2301.087452023</p>
<p>series in artificial intelligence. Daniel Jurafsky, James H Martin, Speech and language processing. Pearson Education International20092pearson international edition] edition</p>
<p>RACE: large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard H Hovy, 10.18653/v1/d17-1082EMNLP. ACL2017</p>
<p>A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang, ACL. ACL. 2023</p>
<p>Hanmeng Liu, Jian Liu, Leyang Cui, Nan Duan, Ming Zhou, Yue Zhang, Logiqa2.0 datasetlogical reasoning in mrc and nli tasks. TASLP2022a</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501IJCAI. 2020</p>
<p>Knowledge based multilingual language model. Linlin Liu, Xin Li, Ruidan He, Lidong Bing, R Shafiq, Luo Joty, Si, EMNLP. ACL2022b</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts, 10.48550/arXiv.2301.13688CoRR, abs/2301.136882023</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 10.48550/arXiv.2302.07842CoRR, abs/2302.078422023</p>
<p>Distant supervision for relation extraction without labeled data. Mike Mintz, Steven Bills, Rion Snow, Daniel Jurafsky, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLPACL2009</p>
<p>OpenAI. 2023. Gpt-4 technical report. Preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, 10.48550/arXiv.2306.01116CoRR, abs/2306.01116Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro CappelliGuilherme Penedo2022</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, 10.48550/arXiv.2302.06476CoRR, abs/2302.064762023</p>
<p>ERICA: Improving entity and relation understanding for pre-trained language models via contrastive learning. Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie Huang, Maosong Sun, Jie Zhou, 10.18653/v1/2021.acl-long.260ACL/IJCNLP. ACL2021</p>
<p>GCC: graph contrastive coding for graph neural network pre-training. Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, Jie Tang, 10.1145/3394486.3403168KDD. ACM2020</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 10.48550/arXiv.2302.04761CoRR, abs/2302.047612023</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 10.48550/arXiv.2210.09261CoRR, abs/2210.092612022</p>
<p>Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, CoRR, abs/2302.139712023</p>
<p>Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning. Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, Ai Ti Aw, Nancy F Chen, CoRR, abs/2309.047662023a</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, 10.48550/arXiv.2305.17926CoRR, abs/2305.179262023b</p>
<p>Logic-driven context extension and data augmentation for logical reasoning of text. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan, 10.18653/v1/2022.findings-acl.127ACL. ACL2022</p>
<p>KEPLER: A unified model for knowledge embedding and pre-trained language representation. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang, 10.1162/tacl_a_00360TACL. 92021</p>
<p>2022a. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, ICLR. OpenReviewM. Dai, and Quoc V. Le.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, 10.48550/arXiv.2206.07682CoRR, abs/2206.07682Emergent abilities of large language models. 2022b</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, K Vikash, Jacob Mansinghka, Joshua B Andreas, Tenenbaum, 10.48550/arXiv.2306.12672CoRR, abs/2306.126722023</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, CoRR, abs/2304.122442023</p>
<p>Discriminative reasoning for document-level relation extraction. Wang Xu, Kehai Chen, Tiejun Zhao, 10.18653/v1/2021.findings-acl.144Findings of ACL. ACL2021</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, In ICLR. Open-Review. 2020</p>
<p>SIRE: separate intra-and inter-sentential reasoning for document-level relation extraction. Shuang Zeng, Yuting Wu, Baobao Chang, Findings of ACL. ACL2021</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, CoRR, abs/2104.06598AR-LSAT: investigating analytical reasoning of text. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7684 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7684</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7684</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‚Äëtuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267770712</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.13750v1.pdf" target="_blank">Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph</a></p>
                <p><strong>Paper Abstract:</strong> Recommendation systems are widely used in e-commerce websites and online platforms to address information overload. However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions. Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment. To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec). It introduces an entity extractor that extracts unified concept terms from item and user information. To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies. The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph. Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model using real complementary exposure-click samples. Extensive experiments conducted on three industry datasets demonstrate the significant performance improvement of our model compared to existing approaches. Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items. In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7684.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7684.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‚Äëtuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-KERec LLM Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Complementary Relation Extraction for Knowledge Graph Construction (LLM-KERec)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses large language models (primarily Claude 2, with comparisons to ChatGPT-3.5 and ChatGLM2) via prompt engineering to classify whether a pair of item/entity concepts are complementary; positive classifications are used to build a complementary knowledge graph whose edges are later weighted and corrected by real exposure-click data and a learned E-E-I model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Claude 2 (primary); comparisons reported for ChatGPT-3.5 and ChatGLM2</td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>Prompt engineering with few-shot examples and stepwise/chain-of-thought style reasoning prompts; prompts iteratively refined via manual annotation; paper mentions parameter-efficient adaptation (LoRA) as a candidate approach but does not report a deployed fine-tune.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not scholarly papers ‚Äî industrial Alipay data: an Entity Dict (tens of thousands of entities created by experts, updated weekly) derived from item metadata and user billing/behavior logs; entity pairs constructed by segmented combination (popular/very-popular/unpopular heuristics) yielding thousands of candidate pairs; evaluation used three Alipay marketing datasets (Dataset A/B/C) containing tens to hundreds of millions of users/exposures (see Table 1). Authors sampled thousands (and a 1,000-pair subset for model comparison) of entity pairs for manual annotation to refine prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM classification via prompt engineering: provide each entity pair and instruct the LLM to determine whether B is likely purchased shortly after A, include few-shot examples and ask for step-by-step reasoning and evidence; output is Y/N plus rationale; positive outputs are used to create edges in a complementary knowledge graph (entity->entity); iterative manual annotation used to refine prompts; LoRA and prompt-tuning are mentioned as relevant techniques but not reported as core deployed extraction steps.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Symbolic/relational representation as a knowledge graph: binary/complementary relation edges (Y/N) with accompanying textual reasoning; later converted into numeric edge weights and used with an E-E-I (entity-entity-item) scoring model (scores used as recall/factors in ranking). No algebraic quantitative laws or equations are distilled.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Alipay industrial datasets A (Super 567), B (Consumer Channel), C (Payment Result Page) for offline and online evaluation; additionally a manually annotated sample of 1,000 randomly sampled complementary entity pairs used to compare LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Offline: AUC for click and conversion prediction. Online: #Click, #Conversion (#Conv), and GMV (Gross Merchandise Volume). Human evaluation: 5-level manual relevance scoring of sampled entity pairs and a weighted mean score. Statistical tests: paired t-tests for significance on offline AUC comparisons; online A/B tests with significance reported (p ‚â™ 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Representative reported results: Offline ‚Äî for a DNN baseline block, LLM-KERec achieved click AUC 0.62882 and conversion AUC 0.82460 (Table 2) outperforming the plain DNN baseline (click AUC 0.61182, conv 0.75844). Online A/B ‚Äî relative improvements reported vs baseline: #Conv +6.24% (Dataset A), GMV +6.45% (Dataset B), #Conv +10.07% (Dataset C). Manual LLM comparison (1,000 pairs): mean human judgment scores ‚Äî Claude 2: 4.056, ChatGPT-3.5: 3.685, ChatGLM2: 3.584 (5-point scale).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Multiple standard recommendation baselines (DNN, Wide&Deep, DCN, ESMM, PLE, Masknet), sometimes augmented with an item-item (i-i) graph; example: DNN baseline click AUC 0.61182 / conv AUC 0.75844 vs LLM-KERec 0.62882 / 0.82460 (see Table 2). Online baseline metrics are presented in absolute counts (obfuscated for confidentiality) and relative % improvements are reported for LLM-KERec.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human manual annotation of sampled LLM outputs (1,000 pairs) for quality assessment and prompt refinement; offline evaluation on held-out test splits using AUC; ablation studies removing recall or ranking modules to quantify contribution; large-scale online A/B tests (10% traffic split) measuring clicks, conversions, GMV and reporting significance; paired t-tests for offline significance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors explicitly note the following limitations: (1) The LLM sometimes produces imaginative or spurious justifications (examples shown) leading to false-positive complementary links, so manual sampling/annotation and downstream reweighting (E-E-I model trained on real exposure-click data) are required; (2) LLM inference is slower and resource intensive, so the LLM is used offline to build/update the complementary graph rather than executed per-request in production; (3) LLMs cannot reliably estimate preference strength (magnitude) ‚Äî only presence/absence of complementary relation ‚Äî requiring a learned weighting model to correct edge strengths; (4) dependence on prompt design and manual annotation; (5) commercial data confidentiality prevents some result disclosure.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Although this paper uses LLMs to infer semantic/associative relations from an industrial corpus, it does not attempt to distill numeric/analytic quantitative laws, equations, or functional forms from scholarly literature or experimental datasets; the distilled artifacts are relational (graph edges and scores) rather than algebraic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Low-Rank Adaptation of Large Language Models <em>(Rating: 2)</em></li>
                <li>Language Models Are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>The Power of Scale for Parameter-Efficient Prompt Tuning <em>(Rating: 2)</em></li>
                <li>GLM-130B: An open bilingual pre-trained model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7684",
    "paper_id": "paper-267770712",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [
        {
            "name_short": "LLM-KERec LLM Extraction",
            "name_full": "LLM-based Complementary Relation Extraction for Knowledge Graph Construction (LLM-KERec)",
            "brief_description": "This paper uses large language models (primarily Claude 2, with comparisons to ChatGPT-3.5 and ChatGLM2) via prompt engineering to classify whether a pair of item/entity concepts are complementary; positive classifications are used to build a complementary knowledge graph whose edges are later weighted and corrected by real exposure-click data and a learned E-E-I model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph",
            "llm_name": "Claude 2 (primary); comparisons reported for ChatGPT-3.5 and ChatGLM2",
            "llm_type": "Prompt engineering with few-shot examples and stepwise/chain-of-thought style reasoning prompts; prompts iteratively refined via manual annotation; paper mentions parameter-efficient adaptation (LoRA) as a candidate approach but does not report a deployed fine-tune.",
            "input_corpus_description": "Not scholarly papers ‚Äî industrial Alipay data: an Entity Dict (tens of thousands of entities created by experts, updated weekly) derived from item metadata and user billing/behavior logs; entity pairs constructed by segmented combination (popular/very-popular/unpopular heuristics) yielding thousands of candidate pairs; evaluation used three Alipay marketing datasets (Dataset A/B/C) containing tens to hundreds of millions of users/exposures (see Table 1). Authors sampled thousands (and a 1,000-pair subset for model comparison) of entity pairs for manual annotation to refine prompts.",
            "extraction_method": "LLM classification via prompt engineering: provide each entity pair and instruct the LLM to determine whether B is likely purchased shortly after A, include few-shot examples and ask for step-by-step reasoning and evidence; output is Y/N plus rationale; positive outputs are used to create edges in a complementary knowledge graph (entity-&gt;entity); iterative manual annotation used to refine prompts; LoRA and prompt-tuning are mentioned as relevant techniques but not reported as core deployed extraction steps.",
            "law_type": null,
            "law_representation": "Symbolic/relational representation as a knowledge graph: binary/complementary relation edges (Y/N) with accompanying textual reasoning; later converted into numeric edge weights and used with an E-E-I (entity-entity-item) scoring model (scores used as recall/factors in ranking). No algebraic quantitative laws or equations are distilled.",
            "evaluation_dataset": "Alipay industrial datasets A (Super 567), B (Consumer Channel), C (Payment Result Page) for offline and online evaluation; additionally a manually annotated sample of 1,000 randomly sampled complementary entity pairs used to compare LLM outputs.",
            "evaluation_metrics": "Offline: AUC for click and conversion prediction. Online: #Click, #Conversion (#Conv), and GMV (Gross Merchandise Volume). Human evaluation: 5-level manual relevance scoring of sampled entity pairs and a weighted mean score. Statistical tests: paired t-tests for significance on offline AUC comparisons; online A/B tests with significance reported (p ‚â™ 0.05).",
            "performance_results": "Representative reported results: Offline ‚Äî for a DNN baseline block, LLM-KERec achieved click AUC 0.62882 and conversion AUC 0.82460 (Table 2) outperforming the plain DNN baseline (click AUC 0.61182, conv 0.75844). Online A/B ‚Äî relative improvements reported vs baseline: #Conv +6.24% (Dataset A), GMV +6.45% (Dataset B), #Conv +10.07% (Dataset C). Manual LLM comparison (1,000 pairs): mean human judgment scores ‚Äî Claude 2: 4.056, ChatGPT-3.5: 3.685, ChatGLM2: 3.584 (5-point scale).",
            "baseline_comparison": true,
            "baseline_performance": "Multiple standard recommendation baselines (DNN, Wide&Deep, DCN, ESMM, PLE, Masknet), sometimes augmented with an item-item (i-i) graph; example: DNN baseline click AUC 0.61182 / conv AUC 0.75844 vs LLM-KERec 0.62882 / 0.82460 (see Table 2). Online baseline metrics are presented in absolute counts (obfuscated for confidentiality) and relative % improvements are reported for LLM-KERec.",
            "validation_method": "Human manual annotation of sampled LLM outputs (1,000 pairs) for quality assessment and prompt refinement; offline evaluation on held-out test splits using AUC; ablation studies removing recall or ranking modules to quantify contribution; large-scale online A/B tests (10% traffic split) measuring clicks, conversions, GMV and reporting significance; paired t-tests for offline significance.",
            "limitations": "Authors explicitly note the following limitations: (1) The LLM sometimes produces imaginative or spurious justifications (examples shown) leading to false-positive complementary links, so manual sampling/annotation and downstream reweighting (E-E-I model trained on real exposure-click data) are required; (2) LLM inference is slower and resource intensive, so the LLM is used offline to build/update the complementary graph rather than executed per-request in production; (3) LLMs cannot reliably estimate preference strength (magnitude) ‚Äî only presence/absence of complementary relation ‚Äî requiring a learned weighting model to correct edge strengths; (4) dependence on prompt design and manual annotation; (5) commercial data confidentiality prevents some result disclosure.",
            "additional_notes": "Although this paper uses LLMs to infer semantic/associative relations from an industrial corpus, it does not attempt to distill numeric/analytic quantitative laws, equations, or functional forms from scholarly literature or experimental datasets; the distilled artifacts are relational (graph edges and scores) rather than algebraic laws.",
            "uuid": "e7684.0",
            "source_info": {
                "paper_title": "Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Low-Rank Adaptation of Large Language Models",
            "rating": 2,
            "sanitized_title": "lowrank_adaptation_of_large_language_models"
        },
        {
            "paper_title": "Language Models Are Few-Shot Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "rating": 2,
            "sanitized_title": "the_power_of_scale_for_parameterefficient_prompt_tuning"
        },
        {
            "paper_title": "GLM-130B: An open bilingual pre-trained model",
            "rating": 1,
            "sanitized_title": "glm130b_an_open_bilingual_pretrained_model"
        }
    ],
    "cost": 0.01387675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph
21 Feb 2024</p>
<p>Qian Zhao 
Hao Qian qianhao.qh@antgroup.com 
Gong-DuoZiqi Liu ziqiliu@antgroup.com 
Duo Zhang 
Lihong Gu lihong.glh@antgroup.com 
Lihong Zhang 
2024 Gu </p>
<p>Ant Group Shanghai
China</p>
<p>Ant Group Shanghai
China</p>
<p>Ant Group Hangzhou
China</p>
<p>Ant Group Hangzhou
China</p>
<p>Ant Group Hangzhou
China</p>
<p>Conference acronym 'XX
August 25-292024BarcelonaSpain</p>
<p>Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph
21 Feb 20244C43399129B2834EAA82C37EFC79519EarXiv:2402.13750v1[cs.IR]‚Ä¢ Information systems ‚Üí Recommender systemsLearning to rankData mining Recommendation system, Large language model, Knowledge graph Recall Module Coarse-Ranking Model Recent Bills Request Item Infomation Traditional Recommendation Module LLM-based Complementary Knowledge Enhancement
Recommendation systems are widely used in e-commerce websites and online platforms to address information overload.However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions.Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggles to adapt to new items and the evolving e-commerce environment.To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec).It introduces an entity extractor that extracts unified concept terms from items and user information.To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies.The large language model determines complementary relationships in each entity pair, and constructs a complementary knowledge graph.Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model by using real complementary exposure-click samples.We conduct extensive experiments on three industry datasets.The results demonstrate significant performance improvement of our model compared to existing approaches.Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items.In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape.</p>
<p>INTRODUCTION</p>
<p>The Recommendation System (RS) has been widely used in online service platforms (e.g., Amazon and Taobao) as an effective tool for alleviating information overload.The primary objective of RS is to infer user preferences from their past behaviors, recommend the most suitable items that align with their interest.Hence, existing recommendation systems are mostly trained based on historical exposure and click logs.</p>
<p>Hereby, we summarize the existing recommendation tasks as the combination of following sub-tasks: 1) Recommend substitutive items based on the exposure or click feedback from users.2) Recommend complementary items based on the conversion feedback from users.3) Conduct traffic exploration or business intervention to explore users' other potential interests.Traditional deep Click-Through Rate (CTR) prediction models [8,14,20,21] equipped with well-designed feature interaction techniques through deep neural networks have been widely applied to tackle these sub-tasks in major e-commerce systems.These methods provide personality in RS via extracting user preference from historical exposure-click samples.Despite achieving notable performance improvements in RS, we argue they still suffer from the following two major challenges in real-world scenarios.1) These models rely heavily on exposed samples and user feedback, which limits the performance of RS in cold-start scenarios and makes it difficult to cope with the continuous emergence of new items.2) The sparsity of user interaction samples results in existing CTR models being more effective in recommending substitutes (sub-task 1) than complementary items (sub-task 2).While models based on expert-crafted complementary rules or knowledge graphs can aid in recommending complementary items, they are not a sustainable solution in the ever-evolving landscape of e-commerce due to efficiency and expenditure challenges.Therefore, it's indispensable to incorporate efficient knowledge and Large Language Model (LLM) as the carrier of human reasoning and logic to improve the performance of RS [1,4,5,19].However, in RS, due to the difficulty of large-scale deployment and long inference time of the large language model, it has only been used as a tool for text embedding in previous work [2,9], making it difficult to fully utilize its powerful reasoning ability.</p>
<p>In the light of the above limitations and challenges, we propose a novel LLM-KERec for recommendation.Our method combines the efficient collaborative signal processing capability of traditional models with large language models and complementary graph to help users quickly find their preferred items.This method not only reduces the homogeneity of traditional model recommendation results, but also improves overall click-through and conversion rates.Specifically, we first use our designed entity extractor to extract unified concept terms (referred to as entities) from the information of all items and user billing information.Next, we generate entity pairs based on the popularity of entities and carefully designed strategies.Then we construct a complementary graph based on a large language model, where each edge in the graph represents a complementary purchasing relationship between corresponding entities.Finally, we launch a new complementary recall module and train the E-E-I weight decision model through real exposure click samples.This model will apply the edge weights of the graph corrected by real feedback to the fine-ranking layer model to achieve recommendation of complementary items.It is worth mentioning that both the entity extractor and complementary graph are periodically updated to adapt to new items and the changing ecommerce environment.The main contributions of this paper can be summarized as follows:</p>
<p>‚Ä¢ For the first time, we utilize the inference ability of large language models as a medium to improve the scenario preference when recommending items to each user, achieving large-scale application of large language model in industrial scenarios.‚Ä¢ Our method continuously adjusts the weights of graph edges based on real exposure samples of complementary item pairs, addressing the language model's weakness in determining user preference strength.</p>
<p>‚Ä¢ Extensive experiments are conducted on three industry scenarios, demonstrating our approach is consistently better than a number of competitive baselines.</p>
<p>SYSTEM OVERVIEW</p>
<p>In this section, we present the overview of the LLM-KERec System, including Traditional Recommendation Module and LLM-based Complementary Knowledge Enhancement, shown in Fig. 1.</p>
<p>Traditional Recommendation Module</p>
<p>In the traditional recommendation architecture, when a user opens an application, the application will automatically send a request to server.This process follows these steps: 1) The server triggers the recall module based on the user's request information, including popular item recall, LBS recall, personalized recall, etc.The recall module returns a large number of candidate items.2) These candidate items are then input into the coarse-ranking model for filtering.The coarse-ranking model produces a smaller set of candidate items.3) Finally, the fine-ranking model and re-ranking model are used to make the final decision on the display order of these items.Additionally, manual intervention may occur at each step, such as assigning weights to the items for publication.</p>
<p>The fine-ranking model and re-ranking model are typically trained using historical exposure and click logs.As a result, existing recommendation models often prioritize recommending similar items based on positive user feedback.This poses a challenge when it comes to providing reliable recommendations for supplementary items that have potential reasoning behind them, such as suggesting complementary item B after a user has purchased item A.</p>
<p>LLM-based Complementary Knowledge Enhancement</p>
<p>In this paper, the LLM-KERec system maintains the ability to efficiently process a large number of collaborative signals of the existing recommendation system.It also overcomes above challenge through the LLM-based Complementary Knowledge Enhancement Module.To establish connections between different content in Alipay, LLM-KERec creates a unified entity (category) system for users' billing behaviors and all items.Each item or bill is classified into a unique entity, which serves as a bridge between various contents.Utilizing world knowledge and commonsense knowledge, we employ a large language model to determine the existence of a complementary relationship between two entities and construct a complementary graph.The nodes of this graph are all entities, while the edges indicate the complementary relationship between the corresponding entities.Subsequently, using the real exposure and click feedback of complementary items, we train an entity-entity-item (E-E-I) weight decision model.This model is then used to inject knowledge into the ranking model.By adopting this approach, we can provide personalized recommendations for both favorite items and complementary items.This solution has been successfully implemented in Alipay marketing scenarios, and experimental results have demonstrated its effectiveness.</p>
<p>DIVING INTO THE LLM-KEREC SYSTEM</p>
<p>In this section, we will zoom into each module in LLM-KERec System.</p>
<p>Entity Extractor</p>
<p>3.1.1Entity Dict.In real-world applications, like Alipay, users' behaviors span across various scenarios, each with diverse content.</p>
<p>To align information and knowledge from these diverse sources, it is crucial to establish a unified association pattern.This is where our Entity Dict comes into play, serving as a bridge for different content types.In the Entity Dict, each entity represents a specific concept, such as "phone" or "cola".Our dedicated group of experts meticulously designed the Entity Dict, incorporating tens of thousands of entities.Importantly, the Entity Dict is regularly updated every week to ensure its adaptability to new items and content.</p>
<p>Extracting Entities.</p>
<p>Building upon the Entity Dict, our focus shifts to extracting entities from various user behaviors within Alipay, including bills, visit logs, and the entity information of items in marketing scenarios.This extraction process can be viewed as a Named Entity Recognition (NER) task, which has been extensively studied in the field of Natural Language Processing (NLP) [13,15,23].To perform entity extraction, we utilize the BERT-CRF model.This model combines the transfer capabilities of BERT [4] with the structured predictions of CRF [11].The BERT-CRF model enables us to accurately extract entities from user behaviors in Alipay.</p>
<p>In the LLM-based Complementary Knowledge Enhancement, our primary objective is to establish connections between user purchase behaviors and the items to be recommended.To achieve this, we extract entities from each user's recent bills, forming their recent entity transaction sequence.Furthermore, we extract entities from item information and assign a unique entity as the item's category.The detailed procedure is illustrated in Fig. 2.</p>
<p>Complementary Graph Construction</p>
<p>We utilize the results of the entity extractor to construct a complementary graph, which helps us gain insights into users' purchasing patterns.Specifically, we aim to understand which item B (e.g., paper towels) users typically buy after purchasing item A (e.g., utensils) by leveraging natural language understanding and commonsense reasoning.The construction of the complementary graph involves two main steps:   prompt engineering and the utilization of a large language model, we perform reasoning tasks to extract meaningful insights from the data.</p>
<p>Entity Pair Construction.</p>
<p>Firstly, it is important to recognize that certain items have complementary relationships with specific concepts, and these concepts often encompass more specific items.</p>
<p>In industrial e-commerce scenarios, where the number of items can reach millions or even more, there are only a few thousand concept categories.By using concepts as entities instead of individual items, computational resources can be significantly conserved.In Section 3.1, we have already assigned unique entities to all items using the entity extractor.To construct entity pairs, a straightforward approach would involve taking elements from a set containing  entities and combining them pairwise, resulting in
ùëõ (ùëõ‚àí1)2
candidate entity pairs.However, this method is not cost-effective due to the slower inference speed of downstream large language models.Additionally, in real-world scenarios, there is often a long-tail distribution where a few entities are frequently purchased while the majority of entities are rarely consumed (as depicted in Fig. 3).Focusing solely on tail entity combinations makes it challenging to improve the overall performance of the recommendation system.</p>
<p>To tackle this challenge, we have devised a cost-effective segmented combination strategy as follows: 1) Initially, we sort entities in descending order based on metrics like total conversions and clicks.This allows us to classify them into three categories: extremely popular, popular, and unpopular entities.2) We focus on constructing entity pairs exclusively within the popular entities.This approach enhances the performance and coverage specifically for popular items.3) Additionally, we construct entity pairs that include both extremely popular and unpopular entities.This ensures comprehensive coverage in the complementary graph for unpopular items.By merging and eliminating duplicates from all entity pairs, we obtain the final output.This segmented combination strategy ensures reliable support for downstream modules while minimizing resource waste.</p>
<p>3.2.2Large Language Model.Large language models have garnered significant attention from researchers due to their remarkable understanding and reasoning abilities in natural language processing.A specific research direction, exemplified by methods like Prompt-Turing [12] and LoRA [7], explores prompt engineering techniques based on these large language models.In these approaches, researchers can obtain desired answers from the large language model by providing a simple task description and a small number of examples.By fine-tuning the model efficiently using techniques like LoRA [7] based on annotated samples, researchers can enhance its support for the current task.In this study, we also leverage the capabilities of large language models to determine the existence of a complementary relationship in an entity pair.Specifically, we utilize Claude 2 1 as the underlying language model and thoughtfully design reliable prompts to guide the model in conducting a step-by-step analysis and providing dependable reasoning evidence.The ultimate goal is to enhance the interpretability of the reasoning results.Upon completing the reasoning process, we sample thousands of examples for manual annotation and continuously refine the prompts to attain an acceptable level of accuracy in the reasoning outcomes.</p>
<p>The prompts we have designed encompass various aspects, including: 1) Description of the input data format, where each line consists of two entities representing real-world concepts.2) Task description, which involves determining whether there is a likelihood of a person purchasing entity B shortly after purchasing entity A. 3) Provide multiple data examples and their corresponding reasons.For instance, we provide examples like the complementary relationship between bread and milk, as they form a popular breakfast combination.Conversely, we highlight that there is no complementary relationship between a phone and milk, as they are unrelated.4) Explanation of the output format, which includes a concise description of the purposes of the two entities, whether a complementary relationship exists between them, and a detailed explanation.Ultimately, the answer is denoted as either Y or N.Moreover, we have explored methods such as ChatGPT 3.5 2 and ChatGLM 2 [5,22].A comprehensive comparison between these methods can be found in Section 4.5.</p>
<p>Automatic Update Strategy.</p>
<p>In a real e-commerce environment, users and merchants continually rely on each other's cognitive updates and mutually promote one another.This means that the popularity of entities is not static.For instance, certain merchants may employ marketing strategies to rapidly gain public attention for their products, and over time, older products may be phased out.To address this dynamic nature of popularity, we have implemented an automatic daily schedule for constructing the incremental complementary graph.By promptly recognizing such changes and updating our complementary graph accordingly, we can ensure the effective and sustained operation of the entire system.This proactive approach is crucial for maintaining optimal system performance in the long run.</p>
<p>E-E-I weight decision model</p>
<p>At present, we have successfully linked each user's recent bills and each item to entities in the complementary graph.Our objective is to recommend complementary items (entity2) based on user bill (entity1), where relation entity1-entity2 exist in the complementary graph.However, due to the limited ability of LLM to accurately assess user preferences, we require an E-E-I (entity1-entity2-item) weight decision model to effectively accomplish this task.</p>
<p>Model Overview.</p>
<p>Intuitively, the success of the LLM-KERec System relies heavily on the construction of a high-quality E-E-I weight decision model.Therefore, we propose a Two-stage Complementary Knowledge Enhancement Procedure, which consists of the Ranking Stage and the Integration Stage, as shown in Fig. 4. In the following sections, we will take a closer look at each well-designed stage.</p>
<p>Ranking Stage.</p>
<p>As shown in Fig. 4(a.0), our model adopts a dual-tower architecture, where the outputs of the two towers represent the representations of the complementary item and bill entity, respectively.The dot product of these outputs serves as the preference level indicator.For the representation of item, we can extract a rich set of features from the database, including basic features, statistical features, and interaction features, etc.However, for the entity representation, we face a challenge as we lack specific information to describe them, aside from a pre-assigned ID.To overcome this limitation, we employ Graph Neural Network [10] and Contrastive Learning to representative entity from two distinct perspectives: the first-order substitutable view and the second-order complementary view.The Ranking Stage can be further subdivided into the following modules: Graph Construction.Graph Neural Networks (GNNs) has demonstrated promising results for recommender systems, as they can effectively leverage high-order relationship.These methods represent interaction data as graphs, such as the user-item interaction graph, and iteratively propagate neighborhood information to learn effective node representations.Similarly, as show in Fig. 4(a.1),we have designed the following edge relationships around how to better represent entity: 1) Establish edges for click behaviors between user nodes and item nodes.2) Establish edges for dependency relationships between item nodes and entity nodes.3) Establish edges for complementary relationships between entity nodes and entity nodes.</p>
<p>Given the user set U = {}, the item set I = {} and the entity set E = {}.The number of nodes is  = |U| + |I| + |E |.Our method formulate the available data as a user-item-entity graph G = (V, A), where V = U ‚à™ I ‚à™ E and A ‚àà R √ó is the adjacent matrix.
G g = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q o I b N 8 E i u C o z U t R l q R s 3 Q g v 2 A e 1 Y M m m m D c 1 k h i S j l K H / 4 c a F I u 7 E n / A L 3 L n x W 8 y 0 X W j r g c D h n H u 5 J 8 e L O F P a t r + s z N L y y u p a d j 2 3 s b m 1 v Z P f 3 W u o M J a E 1 k n I Q 9 n y s K K c C V r X T H P a i i T F g c d p 0 x t e p n 7 z j k r F Q n G j R x F 1 A 9 w X z G c E a y P d d g K s B w T z 5 L o 6 7 j r d f M E u 2 h O g R e L M S K F 8 U P t m b 5 W P a j f / 2 e m F J A 6 o 0 I R j p d q O H W k 3 w V I z w u k 4 1 4 k V j T A Z 4 j 5 t G y p w Q J W b T F K P 0 b F R e s g P p X l C o 4 n 6 e y P B g V K j w D O T a U o 1 7 6 X i f 1 4 7 1 v 6 F m z A R x Z o K M j 3 k x x z p E K U V o B 6 T l G g + M g Q T y U x W R A Z Y Y q J N U T l T g j P / 5 U X S O C 0 6 Z 8 V S z b R R g S m y c A h H c A I O n E M Z r q A K d S A g 4 Q G e 4 N m 6 t x 6 t F + t 1 O p q x Z j v 7 8 A f W + w 8 0 7 5 Y H &lt; / l a t e x i t &gt; MP1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r F b 2 N W 4 M 0 v Z l f k X T l 8 V k U S + b x H s = " &gt; A A A B 9 X i c b V D L S g M x F L 1 T X 7 W + q o I b N 8 E i u C o z R d R l q R s 3 Q g v 2 A e 1 Y M m m m D c 1 k h i S j l K H / 4 c a F I u 7 E n / A L 3 L n x W 8 y 0 X W j r g c D h n H u 5 J 8 e L O F P a t r + s z N L y y u p a d j 2 3 s b m 1 v Z P f 3 W u o M J a E 1 k n I Q 9 n y s K K c C V r X T H P a i i T F g c d p 0 x t e p n 7 z j k r F Q n G j R x F 1 A 9 w X z G c E a y P d d g K s B w T z 5 L o 6 7 p a 6 + Y J d t C d A i 8 S Z k U L 5 o P b N 3 i o f 1 W 7 + s 9 M L S R x Q o Q n H S r U d O 9 J u g q V m h N N x r h M r G m E y x H 3 a N l T g g C o 3 m a Q e o 2 O j 9 J A f S v O E R h P 1 9 0 a C A 6 V G g W c m 0 5 R q 3 k v F / 7 x 2 r P 0 L N 2 E i i j U V Z H r I j z n S I U o r Q D 0 m K d F 8 Z A g m k p m s i A y w x E S b o n K m B G f + y 4 u k U S o 6 Z 8 X T m m m j A l N k 4 R C O 4 A Q c O I c y X E E V 6 k B A w g M 8 w b N 1 b z 1 a L 9 b r d D R j z X b 2 4 Q + s 9 x 8 2 c 5 Y I &lt; / l a t e x i t &gt; MP2 meta-path &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r y g x I Z l l N 3 R 8 A w a q S B h 4 R 1 0 K g d 4 = " &gt; A A A B + X i c b V D L S s N A F L 2 p r 1 p f U c G N m 2 A R X J V E R F 2 W u n H h o g X 7 g D a E y X T S D p 1 M w s y k U E L + x I 0 L R Q R X / o J f 4 M 6 N 3 + K k 7 U J b D w w c z r m X e + b 4 M a N S 2 f a X U V h Z X V v f K G 6 W t r Z 3 d v f M / Y O W j B K B S R N H L B I d H 0 n C K C d N R R U j n V g Q F P q M t P 3 R T e 6 3 x 0 R I G v F 7 N Y m J G 6 I B p w H F S G n J M 8 1 e i N Q Q I 5 b e Z V 6 K W e a Z Z b t i T 2 E t E 2 d O y t W j x j d 9 q 3 3 U P f O z 1 4 9 w E h K u M E N S d h 0 7 V m 6 K h K K Y k a z U S y S J E R 6 h A e l q y l F I p J t O k 2 f W q V b 6 V h A J / b i y p u r v j R S F U k 5 C X 0 / m O e W i l 4 v / e d 1 E B d d u S n m c K M L x 7 F C Q M E t F V l 6 D 1 a e C Y M U m m i A s q M 5 q 4 S E S C C t d V k m X 4 C x + e Z m 0 z i v O Z e W i o d u o w Q x F O I Y T O A M H r q A K t 1 C H J m A Y w w M 8 w b O R G o / G i / E 6 G y 0 Y 8 5 1 D + A P j / Q f v s J e R &lt; / l a t e x i t &gt; L cl graph feature &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p K P n 1 W a R O 4 a 1 E m / t + I r 1 g C / I V K A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / a j 1 6 W S y C p 5 K U o h 4 L X j x W t L X Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 n G l o + N U M W y z W M S q G 1 C N g k t s G 2 4 E d h O F N A o E P g a T m 7 n / + I R K 8 1 g + m G m C f k R H k o e c U W O l e x z U B + W q W 3 M X I O v E y 0 k V c r Q G 5 a / + M G Z p h N I w Q b X u e W 5 i / I w q w 5 n A W a m f a k w o m 9 A R 9 i y V N E L t Z 4 t T Z + T c K k M S x s q W N G S/ I V K A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / a j 1 6 W S y C p 5 K U o h 4 L X j x W t L X Q h r L Z T t q l m 0 3 Y 3 Q g l 9 C d 4 8 a C I V 3 + R N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 n G l o + N U M W y z W M S q G 1 C N g k t s G 2 4 E d h O F N A o E P g a T m 7 n / + I R K 8 1 g + m G m C f k R H k o e c U W O l e x z U B + W q W 3 M X I O v E y 0 k V c r Q G 5 a / + M G Z p h N I w Q b XF o V 6 J t + L K P X F 0 + 1 6 W e J s b O H Q c = " &gt; A A A B 9 X i c b V D J S g N B E O 2 J W x y 3 q E c v j U H w F G Z E 1 I s Y 9 O I x g l k g G z 2 d n q R J T 8 / Q X a P G Y f 7 D i w c X v P o Z 3 r 2 I f 2 N n O W j i g 4 L H e 1 V U 1 f M i w T U 4 z r e V m Z t f W F z K L t s r q 2 v r G 7 n N r Y o O Y 0 V Z m Y Y i V D W P a C a 4 Z G X g I F g t U o w E n m B V r 3 8 x 9 K s 3 T G k e y m s Y R K w Z k K 7 k P q c E j N R q A L s D z 0 / u 0 1 b i p + 1 c 3 i k 4 I + B Z 4 k 5 I / u z D P o 1 e v u x S O / f Z 6 I Q 0 D p g E K o j W d d e J o J k Q B Z w K l t q N W L O I 0 D 7 p s r q h k g R M N 5 P R 1 S n e M 0 o H + 6 E y J Q G P 1 N 8 T C Q m 0 H g S e 6 Q w I 9 P S 0 N x T / 8 + o x + C f N h M s o B i b p e J E f C w w h H k a A O 1 w x C m J g C K G K m 1 s x 7 R F F K J i g b B O C O / 3 y L K k c F N y j w u G V k y + e o z G y a A f t o n 3 k o m N U R J e o h M q I I o U e 0 B N 6 t m 6 t R + v V e h u 3 Z q z J z D b 6 A + v 9 B 9 U + l m M = &lt; / l a t e x i t &gt; z f
&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c s Q l q y U L b 3 q o R P 1 r p e n j i      First-order Substitutable View.In order to model substitutable relationships, we consider two different sources of information for each entity:
V P k n h Q = " &gt; A A A B 9 X i c b V D J S g N B E O 2 J W x y 3 q E c v j U H w F G Z E 1 I s Y 9 O I x g l k g G z 2 d n q R J T 8 / Q X a P G Y f 7 D i w c X v P o Z 3 r 2 I f 2 N n O W j i g 4 L H e 1 V U 1 f M i w T U 4 z r e V m Z t f W F z K L t s r q 2 v r G 7 n N r Y o O Y 0 V Z m Y Y i V D W P a C a 4 Z G X g I F g t U o w E n m B V r 3 8 x 9 K s 3 T G k e y m s Y R K w Z k K 7 k P q c E j N R q A L s D z 0 / u 0 1 a i 0 3 Y u 7 x S c E f A s c S c k f / Z h n 0 Y v X 3 a p n f t s d E I a B 0 w C F U T r u u t E 0 E y I A k 4 F S + 1 G r F l E a J 9 0 W d 1 Q S Q K m m 8 n o 6 h T v G a W D / V C Z k o B H 6 u + J h A R a D w L P d A Y E e n r a G 4 r / e f U Y / J N m w m U U A 5 N 0 v M i P B Y Y Q D y P A H a 4 Y B T E w h F D F z a 2 Y 9 o g i F E x Q t g n B n X 5 5 l l Q O C u 5 R 4 f D K z R f P 0 R h Z t I N 2 0 T 5 y 0 T E q o k t U Q m V E k U I P 6 A k 9 W 7 f W o / V q v Y 1 b M 9 Z k Z h vx W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J a P Z p q g H 9 G R 5 C F n 1 F j p g Q + 8 Q b n i V t 0 F y D r x c l K B H M 1 B + a s / j F k a o T R M U K 1 7 n p s Y P 6 P K c C Z w V u q n G h P K J n S E P U s l j V D 7 2 e L U G b m w y p C E s b I l D V m o v y c y G m k 9 j Q L b G V E z 1 q v e X P z P 6 6 U m v P E z L p P U o G T L R W E q i I n J / G 8 y 5 A q Z E V N L K F P c 3 k r Y m C r K j E 2 n Z E P w V l 9 e J + 2 r q l e v 1 u 5 r l Y a X x 1 G E M z i H S / D g G h p w B 0 1 o A Y M R P M M r v D n C e XV t + r O Q V a J l 5 M K 5 G j 2 y 1 + 9 Q c z S C K V h g m r d 9 d z E + B l V h j O B 0 1 I v 1 Z h Q N q Z D 7 F o q a Y T a z + a n T s m Z V Q Y k j J U t a c h c / T 2 R 0 U j r S R T Y z o i a k V 7 2 Z u J / X j c 1 4 Z W f c Z m k B i V b L A p T Q U x M Z n + T A V f I j J h Y Q p n i 9 l b C R l R R Z m w 6 J R u C t / z y K m n X q t 5 F t X 5 XD / Z U B w V q W p f 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 0 q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B q w 8 G H u / N M D M v S A T X x n W / n M L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 T h V D J s s F r H q B F S j 4 B K b h h u B n U Q h j Q K B 7 W B 8 M / P b j 6 g 0 j + W D m S T o R 3 Q o e c g Z N V a 6 5 / 3 z f r n i V t 0 5 y F / i 5 a Q C O R r 9 8 m d v E L M 0 Q m m Y o F p 3 P T c x f k a V 4 U z g t N R L N S a U j e k Q u 5 Z K G q H 2 s / m p U 3 J i l Q E J Y 2 V L G j J X f 0 5 k N N J 6 E g W 2 M 6 J m p J e 9 m f i f 1 0 1 N e O V n X C a p Q c k W i 8 J U E B O T 2 d 9 k w B U y I y a W U K a 4 v Z W w E V W U G Z t O y Y b g L b / 8 l 7 T O q t 5 F t X Z X q 9 S v 8 z i K c A T H cV t + r O Q V a J l 5 M K 5 G j 2 y 1 + 9 Q c z S C K V h g m r d 9 d z E + B l V h j O B 0 1 I v 1 Z h Q N q Z D 7 F o q a Y T a z + a n T s m Z V Q Y k j J U t a c h c / T 2 R 0 U j r S R T Y z o i a k V 7 2 Z u J / X j c 1 4 Z W f c Z m k B i V b L A p T Q U x M Z n + T A V f I j J h Y Q p n i 9 l b C R l R R Z m w 6 J R u C t / z y K m n X q t 5 F t X 5 XD / Z U B w V q W p f 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 0 q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B q w 8 G H u / N M D M v S A T X x n W / n M L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 T h V D J s s F r H q B F S j 4 B K b h h u B n U Q h j Q K B 7 W B 8 M / P b j 6 g 0 j + W D m S T o R 3 Q o e c g Z N V a 6 5 / 3 z f r n i V t 0 5 y F / i 5 a Q C O R r 9 8 m d v E L M 0 Q m m Y o F p 3 P T c x f k a V 4 U z g t N R L N S a U j e k Q u 5 Z K G q H 2 s / m p U 3 J i l Q E J Y 2 V L G j J X f 0 5 k N N J 6 E g W 2 M 6 J m p J e 9 m f i f 1 0 1 N e O V n X C a p Q c k W i 8 J U E B O T 2 d 9 k w B U y I y a W U K a 4 v Z W w E V W U G Z t O y Y b g L b / 8 l 7 T O q t 5 F t X Z X q 9 S v 8 z i K c A T H c
(1) From an item sub-perspective, we need to explore the common features of items that have a dependency relationship on the current entity.(2) Similarly, from an user sub-perspective, we need to explore the common features of the users who frequently click on the current entity.Specifically, we aggregate information using the Graph Attention Network (GAT), denoted by h ‚Ä≤  =   (h, , N  ;   ).Here, h represents the embeddings of all nodes,  denotes current node index, N  is neighbors of node ,   is the network parameters, and function  (‚Ä¢, ‚Ä¢, ‚Ä¢; ‚Ä¢) is defined as:
h ‚Ä≤ ùëñ = ‚àëÔ∏Å ùëó ‚àà N ùëñ ùúé ùõº ùëñ ùëó W 1 h ùëó ,(1)
where    is defined as:
ùõº ùëñ ùëó = exp LeakyReLU W 2 [W 1 h ùëñ ||W 1 h ùëó ] ùëò ‚àà N ùëñ exp LeakyReLU W 2 [W 1 h ùëñ ||W 1 h ùëò ] ,(2)
where h  represents the embeddings of node ,
W 1 ‚àà R ùëë √óùëë and W 2 ‚àà R 2ùëë arez ùëì ùëñ = Attention ùëì ùë° (h, ùëñ, N ùëñ ‚à© I; ùúÉ 1 ), ùëì ùë° (h, ùëñ, N ùëñ ‚à© U; ùúÉ 1 ) .(3
) As shown in the Fig. 4(a.2),where  1 and  3 are aggregated to  1 on the item side, and  1 and  2 are aggregated to  1 on the user side.</p>
<p>Second-order Complementary View.In the modeling of complementary relationships, we also consider two different sources of information for each entity:</p>
<p>(1) From the complementary graph, we design a meta path MP 1 : item (database) -&gt; entity (graph) -&gt; entity (bill), represents the collection of item features complementary to the current entity from the perspective of semantic reasoning.(2) From the user' daily behaviors, we also design a meta path Similarly, we obtain the representation of entity node  through Eq. 4:
z ùë† ùëñ = Attention ùëì ùë° (h, ùëñ, N ùëñ ‚àíM P 1 ; ùúÉ 2 ), ùëì ùë° (h, ùëñ, N ùëñ ‚àíM P 2 ; ùúÉ 2 ) ,(4)
where N  ‚àíM P 1 and N  ‚àíM P 2 are the target node sets explored throught meta-path MP 1 and MP 2 , respectively, starting from entity node .As shown in the Fig. 4(a.3),where  4 and  5 are aggregated to  1 , and  3 is aggregated to  1 .</p>
<p>Contrasive Learning.z   and z   are aggregated through information from first-order substitutable view and second-order complementary view, respectively, representing the characterization of entity  from two independent and complementary perspectives.z   and z   are interrelated and complementary, as they can supervise each other in training process.Therefore, we utilize the contrastive loss, InfoNCE [17], to maximize the agreement of positive pairs and minimize that of negative pairs:
L ùëêùëô = ‚àëÔ∏Å ùëñ ‚àà E ‚àí log exp(ùë† (z ùëì ùëñ , z ùë† ùëñ )/ùúè) ùëó ‚àà E exp(ùë† (z ùëì ùëñ , z ùë† ùëó )/ùúè) ,(5)
where  (‚Ä¢) measures the similarity between two vectors, which is set as cosine similarity function;  is the hyper-parameter, known as the temperature in softmax.Finally, the representation of the node  is the weighted sum of z   and z   , which will be used for downstream recommended tasks.</p>
<p>Training Process.We leverage a multi-task training strategy to optimize the main E-E-I weight decision task and the auxiliary tasks including contrastive learning task and L2 normalization task jointly:
L = L ùëöùëéùëñùëõ + ùúÜ 1 L ùëêùëô + ùúÜ 2 ||Œò|| 2 ,(6)
where Œò is the set of model parameters,  1 ,  2 are hyperparameters to control the strengths of the diversity preserving loss.L  is the Cross Entropy Loss of the main E-E-I weight decision task.</p>
<p>Integration Stage.</p>
<p>To effectively and efficiently recommend items complemented by recent user bills to those with higher demand, we optimize both the recall module and the fine-ranking model, as shown in Fig. 4(b).Specifically, for the recall module, we added a new complementary recall route.To avoid excessive recall, we prepared a set of up to top-k newly recalled complementary items based on the scores from the E-E-I weight decision model and the recent bill entities retrieved from real-time requests by user ID.As for the fine-ranking model, during the training phase, we also introduce the E-E-I weight decision model to provide scores, entity embeddings, and item embeddings for current samples.The new recall module enables the downstream fine-ranking model to pay attention to complementary items, overcoming the limited input of complementary items caused by exposure bias in previous recommendation systems.The fine-ranking model combines the features of current complementary items and user profile behaviors to comprehensively and personalized sort candidate items.</p>
<p>EXPERIMENTS</p>
<p>To verify the effectiveness of the proposed LLM-KERec, we conduct extensive offline experiments utilizing the real industrial dataset procured from the Alipay online environment and report detailed analysis results.Moreover, we conduct online A/B tests in realworld marketing recommendation scenario to evaluate the performance of LLM-KERec in real industrial applications.</p>
<p>This section encompasses a series of experiments designed to answer the following key questions:</p>
<p>‚Ä¢ Q1: How does LLM-KERec perform when compared with other state-of-the-art (SOTA) baseline methods?(see Subsec-  them through purchases made on Alipay.Moreover, within the Consumer Channel, the APP directly showcases goods that align with users' potential interests, aiming to stimulate clicks and subsequent purchases.Each day, a substantial user base, amounting to tens of millions, is exposed to the assortment of coupons and goods available on Alipay.To conduct our study, we randomly selected some instances spanning various dates over a one-month duration.The primary objective underlying data optimization efforts is to augment user conversions.</p>
<p>These scenarios exhibit significant differences in terms of user population distribution, as well as user intentions and behaviors.They are further randomly divided into the disjoint training set, validation set, and test set.The statistics of these datasets are presented in Table 1.</p>
<p>Evaluation Metrics.</p>
<p>In order to assess the overall system performance, we employ AUC (Area Under Curve) as the evaluation metric for offline experiments.Despite the actual industrial scenario being a ranking scenario, we simplify the offline experiments by treating them as a binary classification problem during modeling.In this approach, the model produces a score indicating whether the user likes (clicks or converts) the recommended item.Therefore, AUC is utilized for offline evaluation purposes.For online experiments, we directly measure the quality of different models by counting the number of clicks and conversions made by real users in various experimental groups.Consequently, the experimental group exhibiting a higher number of recommended items clicked and converted by users signifies better model performance.</p>
<p>4.1.3Baselines.We choose the state-of-the-art recommendation system models as baselines for efficiency comparison.The baselines include DNN [6], Wide&amp;Deep [3], DCN [20], ESMM [16], PLE [18] and Masknet [21].In order to facilitate a more comprehensive comparison, we have incorporated the i-i graph into the baseline models, denoted as "+ ii graph".This adjustment has been implemented as our methodology capitalizes on graph-based techniques.The superior results are emphasized in bold, while the second-best results are denoted in underline.We utilize the symbol " ‚Ä† " to indicate that LLM-KERec exhibits a significant difference from the top-performing baseline, as determined by paired t-tests at a significance level of 0.01.Upon meticulous examination of the table, it is evident that LLM-KERec surpasses other methods in terms of AUC across the three datasets, exhibiting superior performance across the majority of experimental outcomes.</p>
<p>Offline Performance Comparison</p>
<p>Online Performance Comparison</p>
<p>To assess the effectiveness of LLM-KERec in real-world industrial scenarios, online A/B Tests were conducted across the three rec- To conduct the A/B Tests, 10 percent of the actual online traffic was allocated, with the testing traffic candidates assigned randomly and evenly to two experimental groups.LLM-KERec was compared against the online baseline approach, which represents the existing model version serving all online users.Over a period of one month, data on #Click, #Conv, and GMV were collected for the different experimental groups.The results of the online experiments are summarized in Table 3. Due to commercial confidentiality, specific figures are withheld and represented with the symbol " * * ".The percentage of relative improvement achieved by our method compared to the baseline is presented in the last row.The results demonstrate that our proposed LLM-KERec approach achieved a 6.24% increase and a 10.07%increase in #Conv for Dataset A and Dataset C, respectively.Additionally, a 6.45% increase in GMV was observed for Dataset B. The results of the A/B Test demonstrate the</p>
<p>Ablation Study</p>
<p>In order to comprehensively evaluate the impact of the U2E2I recall module and the E-E-I model ranking module on LLM-KERec, we conducted deeper ablation studies on Dataset A by selectively removing either the recall or ranking modules.The annotation w/o indicates the absence of the U2E2I recall module or the E-E-I model ranking module, while w/ signifies the inclusion of these modules.The results are show in Table 4 and final row represents the improvements achieved by retaining each respective module compared to removing it.The experimental findings presented in Table 4 demonstrate that both U2E2I recall and E-E-I model ranking modules contribute to an increase in clicks and conversions, thus affirming the effectiveness of our U2E2I recall module and E-E-I model.</p>
<p>Difference LLMs Comparison</p>
<p>In this subsection, we perform a comparative analysis of different large language models, namely ChatGPT, ChatGLM, and Claude.To assess their performance, we randomly selected 1,000 complementary entity pairs from the generated complementary graphs of these models.These entity pairs were manually evaluated and assigned scores based on their relevance.The scoring scale consists of five levels: 1-Completely unrelated, 2-Somewhat unrelated, 3-Uncertain, 4-Somewhat related, and 5-Completely related.The numbers of entity pairs falling into each of these five levels are reported in Table 5.We then calculate the weighted average of these entity pairs using the following formula: This calculation yields the final manual judgment score, which is presented as the last row in Table 5.Based on the manual judgment scores reported in Table 5, it is evident that the complementary entity pairs recommended by Claude exhibit a higher level of correlation.To provide a more comprehensive understanding, we also extract and present the instances of misjudgment made by ChatGPT and ChatGLM, where the models considered certain entity pairs as relevant, but they were manually determined as irrelevant.These instances are listed in Table 6.</p>
<p>An analysis of the table reveals that ChatGPT associates "Presbyopic Glasses" with "Makeup Remover" based on the reasoning that Makeup Remover needs to be carefully applied by hand, and using a Presbyopic Glass after makeup removal can provide enhanced observation of the facial skin condition.ChatGLM, on the other hand, links "Cake" with "Pajamas" by suggesting that people may wear pajamas while eating cakes at night.We consider these explanations provided by the language models to be excessively imaginative, as they forcefully establish connections between these entity pairs.Table 6: The instances of problematic complementary entity pairs generated by the large language models (LLMs) from their respective complementary graphs.</p>
<p>CONCLUSION</p>
<p>In this paper, we propose a novel LLM based Complementary Knowledge Enhanced Recommendation (LLM-KERec) System.It involves utilizing an entity extractor to extract unified concept terms from the information available for all items and user bills.</p>
<p>To construct a complementary graph, we initially generate entity pairs on their popularity and designed strategies.Next, we leverage a large language model to uncover existing complementary purchasing relationship between each entity pairs.Furthermore, we incorporate a new complementary recall module and train the E-E-I weight decision model to enhance the ranking model's knowledge and facilitate the recommendation of complementary items.</p>
<p>Comprehensive experiments demonstrate the effectiveness of our proposed LLM-KERec system.</p>
<p>Figure 1 :
1
Figure 1: Overall framework of our proposed LLM-KERec System.</p>
<p>Figure 2 :
2
Figure 2: Extracting entites from item information and user bills.</p>
<p>Figure 3 :
3
Figure 3: Long-tail distribution in Entity Dict.</p>
<p>t e x i t s h a 1 _ b a s e 6 4 = " P i + A c x M p S b w E y P C + g B r 7 9 x j 8 y</p>
<p>h / p 7 I a K T 1 N A p s Z 0 T N W K 9 6 c / E / r 5 e a 8 N r P u E x S g 5 I t F 4 W p I C Y m 8 7 / J k C t k R k w t o U x x e y t h Y 6 o o M z a d k g 3 B W 3 1 5 n X T q N e + y 1 r h r V J u V P I 4 i n M I Z X I A H V 9 C E W 2 h B G x i M 4 B l e 4 c 0 R z o v z 7 n w s W w t O P n M C f + B 8 / g D k s Y 1 n &lt; / l a t e x i t &gt; e2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p K P n 1 W a R O 4 a 1 E m / t + I r 1 g C</p>
<p>u e W 5 i / I w q w 5 n A W a m f a k w o m 9 A R 9 i y V N E L t Z 4 t T Z + T c K k M S x s q W N G S h / p 7 I a K T 1 N A p s Z 0 T N W K 9 6 c / E / r 5 e a 8 N r P u E x S g 5 I t F 4 W p I C Y m 8 7 / J k C t k R k w t o U x x e y t h Y 6 o o M z a d k g 3 B W 3 1 5 n X T q N e + y 1 r h r V J u V P I 4 i n M I Z X I A H V 9 C E W 2 h B G x i M 4 B l e 4 c 0 R z o v z 7 n w s W w t O P n M C f + B 8 / g D k s Y 1 n &lt; / l a t e x i t &gt; e2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q g</p>
<p>9 g f X + A + l P l n E = &lt; / l a t e x i t &gt; z s (a.0) Overview &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z O S R z 0 k b u 1 a 9 B w g Z 2 5 u H E R l J g k 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k V I 8 F L x 4 r 2 g 9 o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y</p>
<p>H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H / K Y j Y k = &lt; / l a t e x i t &gt; i1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 6 E 6 a b q e a L A + f M O S 1 w L N h C 5 i C Z o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K U Y 9 F L x 4 r 2 l p o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g b j m 5 n / + I R K 8 1 g + m E m C f k S H k o e c U W O l e 9 6 v 9 c s</p>
<p>r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O C / O u / O x a C 0 4 + c w x / I H z + Q P 5 O Y 2 b &lt; / l a t e x i t &gt; i2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I b A h 0 4 3 N v a I + 7 q y</p>
<p>A o e X E I d b q E B T W A w h C d 4 g V d H O M / O m / O + a C 0 4 + c w h / I L z 8 Q 3 6 v Y 2 c &lt; / l a t e x i t &gt; i3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g Q 7 3 + w s O Z g 8 F c A X n m 2 p m E b b o v a o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 w P u 1 f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P Y z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q n d Z r d 3 X K v W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g D 8 Q Y 2 d &lt; / l a t e x i t &gt; i4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q 4d A E 4 9 A r R Q a J 9 W h s W t D X A 7 S w i 4 = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i 8 e I 5 g H J E m Y n v c m Q 2 d l l Z l Y I S z 7 B i w d F v P p F 3 v w b J 8 k e N L G g o a j q p r s r S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 O / V b T 6 g 0 j + W j G S f o R 3 Q g e c g Z N V Z 6 4 L 2 L X r n i V t 0 Z y D L x c l K B H P V e + a v b j 1 k a o T R M U K 0 7 n p s Y P 6 P K c C Z w U u q m G h P K R n S A H U s l j V D 7 2 e z U C T m x S p + E s b I l D Z m p v y c y G m k 9 j g L b G V E z 1 I v e V P z P 6 6 Q m v P Y z L p P U o G T z R W E q i I n J 9 G / S 5 w q Z E W N L K F P c 3 k r Y k C r K j E 2 n Z E P w F l 9 e J s 2 z q n d Z P b 8 / r 9 R u 8 j i K c A T H c A o e X E E N 7 q A O D W A w g G d 4 h T d H O C / O u / Mx b y 0 4 + c w h / I H z + Q P 9 x Y 2 e &lt; / l a t e x i t &gt; i5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 o W 7 e f 0 Y 3 R r 3 x 8 5 y j o + x / i G f W Y = " &gt; A A A B 6 n i c b Z D L S g M x F I b P e K 3 1 V u v S T b A I r u q M i L q z 4 M Z l i / Y C 7 V A y 6 W k b m s k M S U Y o Q 9 9 A N y 4 U c e s T u R B 8 A Z / D 9 L L Q 1 h 8 C H / 9 / D j n n B L H g 2 r j u l 7 O 0 v L K 6 t p 7 Z y G 5 u b e / s 5 v b y N R 0 l i m G V R S J S j Y B q F F x i 1 X A j s B E r p G E g s B 4 M r s d 5 / R 6 V 5 p G 8 M 8 M Y / Z D 2 J O 9 y R o 2 1 b r H t t X M F t + h O R B b B m 0 H h 6 r v y + c B P o n I 7 9 9 H q R C w J U R o m q N Z N z 4 2 N n 1 J l O B M 4 y r Y S j T F l A 9 r D p k V J Q 9 R + O h l 1 R I 6 s 0 y H d S N k n D Z m 4 v z t S G m o 9 D A N b G V L T 1 / P Z 2 P w v a y a m e + m n X M a J Q c m m H 3 U T Q U x E x n u T D l f I j B h a o E x x O y t h f a o o M / Y 6 W X s E b 3 7 l R a i d F r 3 z 4 l n F L Z T y M F U G D u A Q j s G D C y j B D Z S h C g x 6 8 A j P 8 O I I 5 8 l 5 d d 6 m p U v O r G c f / s h 5 / w E Z A Z F B &lt; / l a t e x i t &gt; e1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z / G 7 E A E p 3 D 8 P t 1 v f g A i 3 O Z t a S q k = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K U Y 9 F L x 4 r 2 l p o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g b j m 5 n / + I R K 8 1 g + m E m C f k S H k o e c U W O l + 7 R f 6 5 c r b t W d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q DG c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w i s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u V b 2 L a v 2 u X m l c 5 3 E U 4 Q R O 4 R w 8 u I Q G 3 E I T W s B g C M / w C m + O cF 6 c d + d j 0 V p w 8 p l j + A P n 8 w c L k I 2 n &lt; / l a t e x i t &gt; u2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 o W 7 e f 0 Y 3 R r 3 x 8 5 y jo + x / i G f W Y = " &gt; A A A B 6 n i c b Z D L S g M x F I b P e K 3 1 V u v S T b A I r u q M i L q z 4 M Z l i / Y C 7 V A y 6 W k b m s k M S U Y o Q 9 9 A N y 4 U c e s T u R B 8 A Z / D 9 L L Q 1 h 8 C H / 9 / D j n n B L H g 2 r j u l 7 O 0 v L K 6 t p 7 Z y G 5 u b e / s 5 v b y N R 0 l i m G V R S J S j Y B q F F x i 1 X A j s B E r p G E g s B 4 M r s d 5 / R 6 V 5 p G 8 M 8 M Y / Z D 2 J O 9 y R o 2 1 b r H t t X M F t + h O R B b B m 0 H h 6 r v y + c B P o n I 7 9 9 H q R C w J U R o m q N Z N z 4 2 N n 1 J l O B M 4 y r Y S j T F l A 9 r D p k V J Q 9 R + O h l 1 R I 6 s 0 y H d S N k n D Z m 4 v z t S G m o 9 D A N b G V L T 1 / P Z 2 P w v a y a m e + m n X M a J Q c m m H 3 U T Q U x E x n u T D l f I j B h a o E x x O y t h f a o o M / Y 6 W X s E b 3 7 l R a i d F r 3 z 4 l n F L Z T y M F U G D u A Q j s G D Cy j B D Z S h C g x 6 8 A j P 8 O I I 5 8 l 5 d d 6 m p U v O r G c f / s h 5 / w E Z A Z F B &lt; / l a t e x i t &gt; e1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g Q 7 3 + w s O Z g 8 F c A X n m 2 p m E b b o v a o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 w P u 1 f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P Y z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q n d Z r d 3 X K v W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g D 8 Q Y 2 d &lt; / l a t e x i t &gt; i4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q 4 d A E 4 9 A r R Q a J 9 W h s W t D X A 7 S w i 4 = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i 8 e I 5 g H J E m Y n v c m Q 2 d l l Z l Y I S z 7 B i w d F v P p F 3 v w b J 8 k e N L G g o a j q p r s r S A T X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 O / V b T 6 g 0 j + W j G S f o R 3 Q g e c g Z N V Z 6 4 L 2 L X r n i V t 0 Z y D L x c l K B H P V e + a v b j 1 k a o T R M U K 0 7 n p s Y P 6 P K c C Z w U u q m G h P K R n S A H U s l j V D 7 2 e z U C T m x S p + E s b I l D Z m p v y c y G m k 9 j g L b G V E z 1 I v e V P z P 6 6 Q m v P Y z L p P U o G T z R W E q i I n J 9 G / S 5 w q Z E W N L K F P c 3 k r Y k C r K j E 2 n Z E P w F l 9 e J s 2 z q n d Z P b 8 / r 9 R u 8 j i K c A T H c A o e X E E N 7 q A O D W A w g G d 4 h T d H O C / O u / M x b y 0 4 + c w h / I H z + Q P 9 x Y 2 e &lt; / l a t e x i t &gt; i5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z O S R z 0 k b u 1 a 9 B w g Z 2 5 u H E R l J g k 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k V I 8 F L x 4 r 2 g 9 o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J a P Z p q g H 9 G R 5 C F n 1 F j p g Q + 8 Q b n i V t 0 F y D r x c l K B H M 1 B + a s / j F k a o T R M U K 1 7 n p s Y P 6 P K c C Z w V u q n G h P K J n S E P U s l j V D 7 2 e L U G b m w y p C E s b I l D V m o v y c y G m k 9 j Q L b G V E z 1 q v e X P z P 6 6 U m v P E z L p P U o G T L R W E q i I n J / G 8 y 5 A q Z E V N L K F P c 3 k r Y m C r K j E 2 n Z E P w V l 9 e J + 2 r q l e v 1 u 5 r l Y a X x 1 G E M z i H S / D g G h p w B 0 1 o A Y M R P M M r v D n C e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H / K Y j Y k = &lt; / l a t e x i t &gt; i1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I b A h 0 4 3 N v a I + 7 q y D / Z U B w V q W p f 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 0 q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B q w 8 G H u / N M D M v S A T X x n W / n M L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 T h V D J s s F r H q B F S j 4B K b h h u B n U Q h j Q K B 7 W B 8 M / P b j 6 g 0 j + W D m S T o R 3 Q o e c g Z N V a 6 5 / 3 z f r n i V t 0 5 y F / i 5 a Q C O R r 9 8 m d v E L M 0 Q m m Y o F p 3 P T c x f k a V 4 U z g t N R L N S a U j e k Q u 5 Z K G q H 2 s / m p U 3 J i l Q E J Y 2 V L G j J X f 0 5 k N N J 6 E g W 2 M 6 J m p J e 9 m f i f 1 0 1 N e O V n X C a p Q c k W i 8 J U E B O T 2 d 9 k w B U y I y a W U K a 4 v Z W w E V W U G Z t O y Y b g L b / 8 l 7 T O q t 5 F t X Z X q 9 S v 8 z i K c A T H c A o e X E I d b q E B T W A w h C d 4 g V d H O M / O m / O + a C 0 4 + c w h / I L z 8 Q 3 6 v Y 2 c &lt; / l a t e x i t &gt; i3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F u s J 3 M u + g n 0 d y Y F u N i G O G u z M L N M = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 k P a 9 f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P Y z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q n d Z r d 3 X K v W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 Rl e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g A K D I 2 m &lt; / l a t e x i t &gt; u1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z / G 7 E A E p 3 D 8 P t 1 v f g A i 3 O Z t a S q k = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K U Y 9 F L x 4 r 2 l p o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g b j m 5 n / + I R K 8 1 g + m E m C f k S H k o e c U W O l + 7 R f 6 5 c r b t W d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w i s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u V b 2 L a v 2 u X m l c 5 3 E U 4 Q R O 4 R w 8 u I Q G 3 E I T W s B g C M / w C m + O c F 6 c d + d j 0 V p w 8 p l j + A P n 8 w c L k I 2 n &lt; / l a t e x i t &gt; u2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F u s J 3 M u + g n 0 d y Y F u N i G O G u z M L N M = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k q M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 k P a 9 f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P Y z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 2 L q n d Z r d 3 X K v W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g A K D I 2 m &lt; / l a t e x i t &gt; u1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z / G 7 E A E p 3 D 8 P t 1 v f g A i 3 O Z t a S q k = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K U Y 9 F L x 4 r 2 l p o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g b j m 5 n / + I R K 8 1 g + m E m C f k S H k o e c U W O l + 7 R f 6 5 c r b t W d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w i s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u V b 2 L a v 2 u X m l c 5 3 E U 4 Q R O 4 R w 8 u I Q G 3 E I T W s B g C M / w C m + O c F 6 c d + d j 0 V p w 8 p l j + A P n 8 w c L k I 2 n &lt; / l a t e x i t &gt; u2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 6 E 6 a b q e a L A + f M O S 1 w L N h C 5 i C Z o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K U Y 9 F L x 4 r 2 l p o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g b j m 5 n / + I R K 8 1 g + m E m C f k S H k o e c U W O l e 9 6 v 9 c s</p>
<p>r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O C / O u / O x a C 0 4 + c w x / I H z + Q P 5 O Y 2 b &lt; / l a t e x i t &gt; i2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z / G 7 E A E p 3 D 8 P t 1 v f g A i 3 O Z t a S q k = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K U Y 9 F L x 4 r 2 l p o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o r e N U M W y x W M S q E 1 C N g k t s G W 4 E d h K F N A o E P g b j m 5 n / + I R K 8 1 g + m E m C f k S H k o e c U W O l + 7 R f 6 5 c r b t W d g 6 w S L y c V y N H s l 7 9 6 g 5 i l E U r D B N W 6 6 7 m J 8 T O q D G c C p 6 V e q j G h b E y H 2 L V U 0 g i 1 n 8 1 P n Z I z q w x I G C t b 0 p C 5 + n s i o 5 H W k y i w n R E 1 I 7 3 s z c T / v G 5 q w i s / 4 z J J D U q 2 W B S m g p i Y z P 4 m A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 0 S j Y E b / n l V d K u V b 2 L a v 2 u X m l c 5 3 E U 4 Q R O 4 R w 8 u I Q G 3 E I T W s B g C M / w C m + O c F 6 c d + d j 0 V p w 8 p l j + A P n 8 w c L k I 2 n &lt; / l a t e x i t &gt; u2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I b A h 0 4 3 N v a I + 7 q y</p>
<p>Figure 4 :
4
Figure 4: Overall framework of Two-stage Complementary Knowledge Enhancement Procedure.</p>
<p>MP 2 :
2
item1 (bill) -&gt; user -&gt; item2 (bill) -&gt; entity (bill), which indicates what items have been recently consumed by users who have consumed item2 in the short term.</p>
<p>tion 4.2) ‚Ä¢ Q2: How does LLM-KERec perform in real-world industrial applications?(see Subsection 4.3) ‚Ä¢ Q3: How do the distinct modules of LLM-KERec contribute to performance improvements?(see Subsection 4.4) ‚Ä¢ Q4: How do the different large language models impact the performance of LLM-KERec?(see Subsection 4.5) 4.1 Experimental Setups 4.1.1Datasets.This paper mainly focuses on recommendation in digital marketing scenarios, we utilize real-world industrial datasets 3 from Alipay.It includes three major marketing and recommendation scenarios within Alipay: Super 567 (Dataset A), Consumer Channel (Dataset B), and Payment Result Page (Dataset C).The Alipay application (APP) facilitates the presentation of numerous coupons to users through Super 567 and the Payment Result Page.The intention is to encourage user engagement by prompting them to click and collect these coupons, subsequently redeeming</p>
<p>1 √ó
1
Completely Unrelated Num + . . .+ 5 √ó Completely Related Num 1000 .</p>
<p>needs to be carefully applied by hand, and using a Presbyopic Glass after makeup removal can provide enhanced observation of the facial skin condition.ChatGLM 2 Cake, PajamasPeople may wear pajamas while eating cakes at night.</p>
<p>Figure 5 :
5
Figure 5: The relative improvement of conversion rate (CVR) for randomly sampled complementary pairs in LLM-KERec compared to the baseline.</p>
<p>trainable parameters,  (‚Ä¢) is a non-linearity activate function, LeakyReLU(‚Ä¢) is the LeakyReLU activate function and [‚Ä¢||‚Ä¢] is concatenate operation.Then we can fuse information from different sub-perspectives including user and item side, based on attention mechanism to obtain entity node  embedding:</p>
<p>Table 1 :
1
The statistics of three datasets.</p>
<h1>Users#Items #click#conversionDataset A 1552098484628500126780Dataset B 1301782376111 144821319437Dataset C 28361313 172786 15336011 143502</h1>
<p>Table 2
2
presents the AUC results of the offline performance comparison for all methods.The Click and Conv.columns indicate the click AUC and conversion AUC values for the three datasets, respectively.</p>
<p>Table 2 :
2
Offline performance comparison, with evaluation metrics including click AUC and conversion AUC(conv.).The best results are bolded and the second best results are underlined.
Dataset ADataset BDataset CClickConv.ClickConv.ClickConv.DNN0.611820.758440.775970.760920.860600.93010DNN + ii graph0.615800.806840.777510.731870.860610.93997LLM-KERec0.62882  ‚Ä† 0.82460  ‚Ä† 0.78523  ‚Ä† 0.76271  ‚Ä† 0.859720.94608Wnd0.605990.727510.777660.742430.860250.93069Wnd + ii graph0.628220.817820.775070.751970.860640.93384LLM-KERec0.63207  ‚Ä† 0.81896  ‚Ä† 0.77897  ‚Ä† 0.77140  ‚Ä† 0.860590.94064  ‚Ä†DCN0.624570.817600.780530.752310.849660.92919DCN + ii graph0.631210.804870.778024 0.752800.857060.93370LLM-KERec0.672840.82507  ‚Ä† 0.782850.76789  ‚Ä† 0.85732  ‚Ä† 0.94174  ‚Ä†ESMM0.612590.783660.765090.752460.850900.91357ESMM + ii graph0.619270.821000.779200.761360.853780.92483LLM-KERec0.62488  ‚Ä† 0.82239  ‚Ä† 0.78168  ‚Ä† 0.76263  ‚Ä† 0.85398  ‚Ä† 0.92832  ‚Ä†PLE0.606520.772820.771570.739860.856400.93391PLE + ii graph0.598700.801170.771850.738170.856720.93683LLM-KERec0.62576  ‚Ä† 0.82238  ‚Ä† 0.78636  ‚Ä† 0.74725  ‚Ä† 0.85681  ‚Ä† 0.93897  ‚Ä†Masknet0.593600.812630.691590.61034  ‚Ä† 0.827820.86086Masknet + ii graph 0.639980.8166  ‚Ä†0.720440.580090.828890.89715LLM-KERec0.65137  ‚Ä† 0.816310.72863  ‚Ä† 0.595340.84161  ‚Ä† 0.90086</p>
<p>Table 3 :
3
The overall online performance comparison, where #conv. is number of coupon conversion and GMV is Gross Merchandise Volume.Note that the improvements achieved by LLM-KERec are statistically significant (-value ‚â™ 0.05).
Dataset ADataset BDataset CMethods #Click #Conv. #Click GMV #Click #Conv.Baseline 3<em> </em>72<em> </em>03<em> </em>02<em> </em>17<em> </em>61<em> </em>1Ours3<em> </em>73<em> </em>63<em> </em>92<em> </em>27<em> </em>71<em> </em>2Improv. +2.67% +6.24% +6.18% +6.45% +4.39% +10.07%significant improvements achieved by our method in real-worldindustrial recommendation scenarios.</p>
<p>Table 4 :
4
The online ablation performance comparsion for Dataset C, where w/o and w/ represent without and with, respectively.
U2E2I recallE-E-I model for rankingMethod#click#conv.#click#conv.w/o7<em> </em>11<em> </em>14<em> </em>78<em> </em>5w/7<em> </em>71<em> </em>24<em> </em>58<em> </em>2Improv.+3.33%+2.95%+1.05%+0.59%</p>
<p>Table 5 :
5
Comparing the performance of complementary graph generated by different LLMs using five levels of manual annotation (randomly sampled 1000 entity pairs), a higher Mean Score indicates that the model's predictions are closer to human judgments.
ModelChatGLM 2 ChatGPT 3.5 Claude 2(1) Completely unrelated 191171109(2) Somewhat unrelated 402636(3) Uncertain145145127(4) Somewhat related242263146(5) Completely related382395582Mean Score3.5843.6854.056
https://www.anthropic.com/index/claude-2
https://openai.com/blog/chatgpt
The data set does not contain any Personal Identifiable Information (PII). The data set is desensitized and encrypted. Adequate data protection was carried out during the experiment to prevent the risk of data copy leakage, and the data set was destroyed after the experiment.
Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph.In Proceedings of 30TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (Conference acronym 'XX).ACM, New York, NY, USA, 9 pages.https://doi.org/XXXXXXX.XXXXXXX
Language Models Are Few-Shot Learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Proceedings of the 34th International Conference on Neural Information Processing Systems. the 34th International Conference on Neural Information Processing SystemsBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford; Vancouver, BC, Canada; Red Hook, NY, USACurran Associates Inc202020Ilya Sutskever, and Dario Amodei. Article 159, 25 pages</p>
<p>Joint text embedding for personalized content-based recommendation. Ting Chen, Liangjie Hong, Yue Shi, Yizhou Sun, arXiv:1706.010842017. 2017arXiv preprint</p>
<p>Wide &amp; deep learning for recommender systems. Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Proceedings of the 1st workshop on deep learning for recommender systems. the 1st workshop on deep learning for recommender systems2016</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Simon Haykin, Neural Networks: A Comprehensive Foundation. Prentice Hall PTR, USA19982nd ed.</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. Tongwen Huang, Zhiqi Zhang, Junlin Zhang, Proceedings of the 13th ACM Conference on Recommender Systems. the 13th ACM Conference on Recommender Systems2019</p>
<p>Personalized recommendation system based on knowledge embedding and historical behavior. Bei Hui, Lizong Zhang, Xue Zhou, Xiao Wen, Yuhui Nian, Applied Intelligence. 2022. 2022</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, arXiv:1609.029075th International Conference on Learning Representations, ICLR 2017 -Conference Track Proceedings. 2019. 2019</p>
<p>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. John D Lafferty, Andrew Mccallum, Fernando C N Pereira, Proceedings of the Eighteenth International Conference on Machine Learning (ICML '01). the Eighteenth International Conference on Machine Learning (ICML '01)San Francisco, CA, USAMorgan Kaufmann Publishers Inc2001</p>
<p>The Power of Scale for Parameter-Efficient Prompt Tuning. Brian Lester, Rami Al-Rfou, Noah Constant, 10.18653/v1/2021.emnlp-main.243Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>A Survey on Deep Learning for Named Entity Recognition. Jing Li, Aixin Sun, Jianglei Han, Chenliang Li, 10.1109/TKDE.2020.2981314IEEE Trans. on Knowl. and Data Eng. 3412022. jan 2022</p>
<p>xdeepfm: Combining explicit and implicit feature interactions for recommender systems. Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, Guangzhong Sun, Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018</p>
<p>An Encoding Strategy Based Word-Character LSTM for Chinese NER. Wei Liu, Tongge Xu, Qinghua Xu, Jiayu Song, Yueran Zu, 10.18653/v1/N19-1247Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Entire space multi-task model: An effective approach for estimating post-click conversion rate. Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, Kun Gai, The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval. 2018</p>
<p>Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. 2018. 2018arXiv preprint</p>
<p>Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. Hongyan Tang, Junning Liu, Ming Zhao, Xudong Gong, ; Rodrygo, L T Santos, Leandro Balby Marinho, Elizabeth M Daly, Li Chen, Kim Falk, 10.1145/3383313.3412236RecSys 2020: Fourteenth ACM Conference on Recommender Systems, Virtual Event. Noam Koenigstein, Edleno Silva De Moura, BrazilACM2020. September 22-26, 2020</p>
<p>Edouard Grave, and Guillaume Lample. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth√©e Lachaux, Baptiste Lacroix, Naman Rozi√®re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arxiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023</p>
<p>Deep &amp; cross network for ad click predictions. Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang, Proceedings of the ADKDD'17. the ADKDD'172017</p>
<p>MaskNet: Introducing feature-wise multiplication to CTR ranking models by instance-guided mask. Zhiqiang Wang, Qingyun She, Junlin Zhang, 2021. 2021</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022. 2022arXiv preprint</p>
<p>Chinese NER Using Lattice LSTM. Yue Zhang, Jie Yang, 10.18653/v1/P18-1144Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>            </div>
        </div>

    </div>
</body>
</html>
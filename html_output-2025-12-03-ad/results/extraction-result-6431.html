<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6431 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6431</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6431</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-268247700</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.01999v1.pdf" target="_blank">LLM-Oriented Retrieval Tuner</a></p>
                <p><strong>Paper Abstract:</strong> Dense Retrieval (DR) is now considered as a promising tool to enhance the memorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by incorporating external memories. However, due to the paradigm discrepancy between text generation of LLM and DR, it is still an open challenge to integrate the retrieval and generation tasks in a shared LLM. In this paper, we propose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which decouples DR capacity from base LLM and non-invasively coordinates the optimally aligned and uniform layers of the LLM towards a unified DR space, achieving an efficient and effective DR without tuning the LLM itself. The extensive experiments on six BEIR datasets show that our approach could achieve competitive zero-shot retrieval performance compared to a range of strong DR models while maintaining the generation ability of LLM.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6431.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6431.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMORT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Oriented Retrieval Tuner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight plugin tuner that sits on top of a frozen LLM and transforms two selected LLM layer representations (optimal alignment and optimal uniformity layers) into a unified dense retrieval space via layered bi-directional self- and cross-attention blocks, enabling the LLM to be used as an effective dense retriever without changing LLM parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LMORT (LLM-Oriented Retrieval Tuner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not a standalone generative agent but a retrieval-plugin that converts selected hidden-layer outputs of a frozen decoder-only LLM into dense vectors for an external memory (vector DB). It composes a few Transformer-like layers (self bi-attention on the chosen "alignment" layer, cross bi-attention attending to the chosen "uniformity" layer, and a feedforward), producing pooled embeddings used for nearest-neighbor retrieval. The tuner is trained while the base LLM stays frozen; retrieved passages can be used as external memory for downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>evaluated with GPT2-Large (0.75B), GPT2-XL (1.5B), GPT-j-6B (6B)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector-store dense retrieval (retrieval-augmented memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>mean-pooled dense vectors derived from selected LLM hidden layers (embeddings produced by LMORT output H_o)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>similarity search (dot-product / cosine) over the vector database of passage embeddings produced by LMORT; LMORT encodes queries and corpus passages into the same vector space and retrieval selects top-K by similarity</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot retrieval on six BEIR datasets (TREC-COVID, NFCorpus, FiQA, ArguAna, SciFact, SCIDOCS); training on MS MARCO for LMORT</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / memory augmentation for LLMs (zero-shot dense retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average NDCG@10: GPT-j-6B + LMORT (L=3) = 0.425; GPT2-XL + LMORT (L=5) = 0.355; GPT2-Large + LMORT (L=7) = 0.296 (NDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablations / degraded connections: connecting LMORT to worst A&U layers yields avg NDCG@10 of 0.324 (GPT-j-6B), 0.167 (GPT2-XL), 0.248 (GPT2-Large); removing cross-bi-attention (only self-attn) reduces average NDCG@10 (e.g., GPT-j-6B drops to 0.293 for some ablations). (Values from Table 2 comparisons.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NDCG@10</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>LMORT is far more parameter- and training-efficient than fine-tuning the LLM: standard LMORT uses ~13% of LLM parameters and ~14% of per-step training time vs full LLM fine-tune; a dimension-reduced LMORT uses ~2% of LLM parameters and ~4% of training time with only ~1% average NDCG@10 drop. Trade-off: combining alignment and uniformity into one space improves retrieval but sacrifices some of the optimal alignment/uniformity achievable in their separate LLM layers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LMORT still underperforms LLM-based full fine-tuning at the same scale; performance degrades substantially if incorrect alignment/uniformity layers are selected or cross-bi-attention is removed; requires access to LLM hidden states (so only applicable to open-source / introspectable LLMs); balancing alignment and uniformity in one layer comes at a cost (cannot simultaneously equal the separate-layer optima).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Si Sun, Hanqing Zhang, Zhiyuan Liu, Jie Bao, Dawei Song (2024). LLM-ORIENTED RETRIEVAL TUNER. arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Oriented Retrieval Tuner', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6431.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6431.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cpt-text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>cpt-text (contrastive pre-trained text embeddings / GPT-based retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that fine-tunes large GPT-style LLMs (e.g., GPT-3 variants) as dense retrievers to produce text embeddings used for retrieval, demonstrating strong zero-shot retrieval performance when large models are tuned as retrieval encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text and code embeddings by contrastive pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>cpt-text (LLM fine-tuned as retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Fine-tunes large decoder-only LLMs (e.g., GPT-3 family) to act as encoders that map queries and passages to dense vectors; those vectors are stored in an external vector index and retrieved via nearest-neighbor search (standard dense retrieval pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>reported in related work as large GPT-3 models (examples: GPT3-6B case cited as cpt-text-L in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector-store dense retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>dense sentence/passage embeddings produced by fine-tuned LLM</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>nearest-neighbor similarity search (dot product / cosine) over precomputed embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot retrieval (BEIR benchmark referenced) - reported for reference in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / memory augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 (for reference): cpt-text-L (GPT3-6B backbone) reported per-dataset scores: TREC-COVID 0.562, NFCorpus 0.380, FiQA 0.452, ArguAna 0.469, SciFact 0.744; some entries N/A in table. Metric: NDCG@10.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>NDCG@10</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Mentioned trade-offs in related work: directly fine-tuning large LLMs as retrievers is effective but costly (high training cost) and makes the LLM retrieval-specific (reduces compatibility with generation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Noted as costly in compute and less compatible with multi-purpose generation because retriever-specific fine-tuning changes the LLM; also large models required to obtain strong zero-shot retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Arvind Neelakantan et al. (2022). Text and code embeddings by contrastive pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Oriented Retrieval Tuner', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6431.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6431.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based autonomous agent (memory component)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based autonomous agent with external memory component</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general conceptual class of systems where a large language model operates as part of an agent architecture (planning, tools, action) and uses an explicit external memory to retain and recall long-term information; the paper discusses this as motivation for retrieval-augmented LLM memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-based autonomous agent (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent architectures that integrate an LLM for decision-making and generation together with components for planning, tools, action and an external memory; memory is used to store and retrieve information (e.g., via dense retrieval) to support long-range information retention and contextual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (concept applies across model sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory (general) – commonly instantiated as an external vector store / DR system</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>could be long-term stored text passages or their dense embeddings (paper references dense retrieval as a promising external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>typically retrieval (dense retrieval similarity search) integrated into agent loops to fetch relevant memories when composing model input</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>agent tasks / long-term memory support for generation, planning, long-text modeling and long-range conversations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper motivates external memory as important for long-term retention; notes trade-offs of existing LLM-based retrievers: fine-tuning LLMs for retrieval increases parameters and inference cost (every query must be re-encoded by retrieval-LLM) and reduces generality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes open challenges integrating retrieval and generation within a single unified LLM due to paradigm divergence (generation vs. contrastive embedding objectives); also practical constraint that LMORT-like approaches need access to LLM hidden states (limiting closed-source applicability).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Concept discussed and motivated in: Si Sun et al. (2024). LLM-ORIENTED RETRIEVAL TUNER. arXiv preprint. Related references: Nematzadeh et al. (2020) 'On memory in human and artificial language processing systems', Weng (2023) 'LLM powered autonomous agents'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Oriented Retrieval Tuner', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Text and code embeddings by contrastive pre-training <em>(Rating: 2)</em></li>
                <li>On memory in human and artificial language processing systems <em>(Rating: 2)</em></li>
                <li>SGPT: GPT sentence embeddings for semantic search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6431",
    "paper_id": "paper-268247700",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "LMORT",
            "name_full": "LLM-Oriented Retrieval Tuner",
            "brief_description": "A lightweight plugin tuner that sits on top of a frozen LLM and transforms two selected LLM layer representations (optimal alignment and optimal uniformity layers) into a unified dense retrieval space via layered bi-directional self- and cross-attention blocks, enabling the LLM to be used as an effective dense retriever without changing LLM parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LMORT (LLM-Oriented Retrieval Tuner)",
            "agent_description": "Not a standalone generative agent but a retrieval-plugin that converts selected hidden-layer outputs of a frozen decoder-only LLM into dense vectors for an external memory (vector DB). It composes a few Transformer-like layers (self bi-attention on the chosen \"alignment\" layer, cross bi-attention attending to the chosen \"uniformity\" layer, and a feedforward), producing pooled embeddings used for nearest-neighbor retrieval. The tuner is trained while the base LLM stays frozen; retrieved passages can be used as external memory for downstream generation.",
            "model_size": "evaluated with GPT2-Large (0.75B), GPT2-XL (1.5B), GPT-j-6B (6B)",
            "memory_used": true,
            "memory_type": "external vector-store dense retrieval (retrieval-augmented memory)",
            "memory_representation": "mean-pooled dense vectors derived from selected LLM hidden layers (embeddings produced by LMORT output H_o)",
            "memory_access_mechanism": "similarity search (dot-product / cosine) over the vector database of passage embeddings produced by LMORT; LMORT encodes queries and corpus passages into the same vector space and retrieval selects top-K by similarity",
            "task_name": "Zero-shot retrieval on six BEIR datasets (TREC-COVID, NFCorpus, FiQA, ArguAna, SciFact, SCIDOCS); training on MS MARCO for LMORT",
            "task_category": "retrieval / memory augmentation for LLMs (zero-shot dense retrieval)",
            "performance_with_memory": "Average NDCG@10: GPT-j-6B + LMORT (L=3) = 0.425; GPT2-XL + LMORT (L=5) = 0.355; GPT2-Large + LMORT (L=7) = 0.296 (NDCG@10)",
            "performance_without_memory": "Ablations / degraded connections: connecting LMORT to worst A&U layers yields avg NDCG@10 of 0.324 (GPT-j-6B), 0.167 (GPT2-XL), 0.248 (GPT2-Large); removing cross-bi-attention (only self-attn) reduces average NDCG@10 (e.g., GPT-j-6B drops to 0.293 for some ablations). (Values from Table 2 comparisons.)",
            "has_comparative_results": true,
            "performance_metric": "NDCG@10",
            "tradeoffs_reported": "LMORT is far more parameter- and training-efficient than fine-tuning the LLM: standard LMORT uses ~13% of LLM parameters and ~14% of per-step training time vs full LLM fine-tune; a dimension-reduced LMORT uses ~2% of LLM parameters and ~4% of training time with only ~1% average NDCG@10 drop. Trade-off: combining alignment and uniformity into one space improves retrieval but sacrifices some of the optimal alignment/uniformity achievable in their separate LLM layers.",
            "limitations_or_failure_cases": "LMORT still underperforms LLM-based full fine-tuning at the same scale; performance degrades substantially if incorrect alignment/uniformity layers are selected or cross-bi-attention is removed; requires access to LLM hidden states (so only applicable to open-source / introspectable LLMs); balancing alignment and uniformity in one layer comes at a cost (cannot simultaneously equal the separate-layer optima).",
            "citation": "Si Sun, Hanqing Zhang, Zhiyuan Liu, Jie Bao, Dawei Song (2024). LLM-ORIENTED RETRIEVAL TUNER. arXiv preprint.",
            "uuid": "e6431.0",
            "source_info": {
                "paper_title": "LLM-Oriented Retrieval Tuner",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "cpt-text",
            "name_full": "cpt-text (contrastive pre-trained text embeddings / GPT-based retriever)",
            "brief_description": "A prior approach that fine-tunes large GPT-style LLMs (e.g., GPT-3 variants) as dense retrievers to produce text embeddings used for retrieval, demonstrating strong zero-shot retrieval performance when large models are tuned as retrieval encoders.",
            "citation_title": "Text and code embeddings by contrastive pre-training",
            "mention_or_use": "mention",
            "agent_name": "cpt-text (LLM fine-tuned as retriever)",
            "agent_description": "Fine-tunes large decoder-only LLMs (e.g., GPT-3 family) to act as encoders that map queries and passages to dense vectors; those vectors are stored in an external vector index and retrieved via nearest-neighbor search (standard dense retrieval pipeline).",
            "model_size": "reported in related work as large GPT-3 models (examples: GPT3-6B case cited as cpt-text-L in Table 2)",
            "memory_used": true,
            "memory_type": "external vector-store dense retrieval",
            "memory_representation": "dense sentence/passage embeddings produced by fine-tuned LLM",
            "memory_access_mechanism": "nearest-neighbor similarity search (dot product / cosine) over precomputed embeddings",
            "task_name": "Zero-shot retrieval (BEIR benchmark referenced) - reported for reference in the paper",
            "task_category": "retrieval / memory augmentation",
            "performance_with_memory": "Table 2 (for reference): cpt-text-L (GPT3-6B backbone) reported per-dataset scores: TREC-COVID 0.562, NFCorpus 0.380, FiQA 0.452, ArguAna 0.469, SciFact 0.744; some entries N/A in table. Metric: NDCG@10.",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "NDCG@10",
            "tradeoffs_reported": "Mentioned trade-offs in related work: directly fine-tuning large LLMs as retrievers is effective but costly (high training cost) and makes the LLM retrieval-specific (reduces compatibility with generation tasks).",
            "limitations_or_failure_cases": "Noted as costly in compute and less compatible with multi-purpose generation because retriever-specific fine-tuning changes the LLM; also large models required to obtain strong zero-shot retrieval.",
            "citation": "Arvind Neelakantan et al. (2022). Text and code embeddings by contrastive pre-training.",
            "uuid": "e6431.1",
            "source_info": {
                "paper_title": "LLM-Oriented Retrieval Tuner",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LLM-based autonomous agent (memory component)",
            "name_full": "LLM-based autonomous agent with external memory component",
            "brief_description": "A general conceptual class of systems where a large language model operates as part of an agent architecture (planning, tools, action) and uses an explicit external memory to retain and recall long-term information; the paper discusses this as motivation for retrieval-augmented LLM memory.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "LLM-based autonomous agent (general concept)",
            "agent_description": "Agent architectures that integrate an LLM for decision-making and generation together with components for planning, tools, action and an external memory; memory is used to store and retrieve information (e.g., via dense retrieval) to support long-range information retention and contextual grounding.",
            "model_size": "not specified (concept applies across model sizes)",
            "memory_used": true,
            "memory_type": "external memory (general) – commonly instantiated as an external vector store / DR system",
            "memory_representation": "could be long-term stored text passages or their dense embeddings (paper references dense retrieval as a promising external memory)",
            "memory_access_mechanism": "typically retrieval (dense retrieval similarity search) integrated into agent loops to fetch relevant memories when composing model input",
            "task_name": null,
            "task_category": "agent tasks / long-term memory support for generation, planning, long-text modeling and long-range conversations",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper motivates external memory as important for long-term retention; notes trade-offs of existing LLM-based retrievers: fine-tuning LLMs for retrieval increases parameters and inference cost (every query must be re-encoded by retrieval-LLM) and reduces generality.",
            "limitations_or_failure_cases": "Paper notes open challenges integrating retrieval and generation within a single unified LLM due to paradigm divergence (generation vs. contrastive embedding objectives); also practical constraint that LMORT-like approaches need access to LLM hidden states (limiting closed-source applicability).",
            "citation": "Concept discussed and motivated in: Si Sun et al. (2024). LLM-ORIENTED RETRIEVAL TUNER. arXiv preprint. Related references: Nematzadeh et al. (2020) 'On memory in human and artificial language processing systems', Weng (2023) 'LLM powered autonomous agents'.",
            "uuid": "e6431.2",
            "source_info": {
                "paper_title": "LLM-Oriented Retrieval Tuner",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2,
            "sanitized_title": "dense_passage_retrieval_for_opendomain_question_answering"
        },
        {
            "paper_title": "Text and code embeddings by contrastive pre-training",
            "rating": 2,
            "sanitized_title": "text_and_code_embeddings_by_contrastive_pretraining"
        },
        {
            "paper_title": "On memory in human and artificial language processing systems",
            "rating": 2,
            "sanitized_title": "on_memory_in_human_and_artificial_language_processing_systems"
        },
        {
            "paper_title": "SGPT: GPT sentence embeddings for semantic search",
            "rating": 1,
            "sanitized_title": "sgpt_gpt_sentence_embeddings_for_semantic_search"
        }
    ],
    "cost": 0.0133195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-ORIENTED RETRIEVAL TUNER
4 Mar 2024</p>
<p>Si Sun 
Department of Electronic Engineering
Tsinghua University</p>
<p>Hanqing Zhang zhanghanqing@bit.edu.cn 
School of Computer Science &amp; Technology
Beijing Institute of Technology</p>
<p>Zhiyuan Liu 
Department of Computer Science &amp; Technology
Institute for AI
Tsinghua University</p>
<p>Jie Bao bao@tsinghua.edu.cn 
Department of Electronic Engineering
Tsinghua University</p>
<p>Dawei Song dwsong@bit.edu.cn 
School of Computer Science &amp; Technology
Beijing Institute of Technology</p>
<p>LLM-ORIENTED RETRIEVAL TUNER
4 Mar 202428984FA11E03BC6B3C62919B52D36560arXiv:2403.01999v1[cs.CL]
Dense Retrieval (DR) is now considered as a promising tool to enhance the memorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by incorporating external memories.However, due to the paradigm discrepancy between text generation of LLM and DR, it is still an open challenge to integrate the retrieval and generation tasks in a shared LLM.In this paper, we propose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which decouples DR capacity from base LLM and non-invasively coordinates the optimally aligned and uniform layers of the LLM towards a unified DR space, achieving an efficient and effective DR without tuning the LLM itself.The extensive experiments on six BEIR datasets show that our approach could achieve competitive zero-shot retrieval performance compared to a range of strong DR models while maintaining the generation ability of LLM.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023) have achieved significant success and shown impressive zero/few-shot generalization ability across a wide range of natural language processing tasks (Brown et al., 2020;Kojima et al., 2022).Recently, they are now being functioned as the backbone of autonomous agents consisting of planning, tools, action, and memory, and become a milestone towards Artificial General Agent (AGI) (Weng, 2023).</p>
<p>In a LLM-based autonomous agent, the inclusion of an external memory component, which can aid the agent in retaining and recalling information for a long time (Nematzadeh et al., 2020), holds significant importance.A promising avenue for augmenting LLM's long-term memory lies in dense retrieval (DR), which employs a representation model to map information into dense vectors, and allows efficient identification of relevant information from large-scale vector storage (Karpukhin et al., 2020;Xiong et al., 2021;Ni et al., 2022).</p>
<p>LLM-based retrieval, such as cpt-text (Neelakantan et al., 2022) with large GPT-3 models, presents a promising avenue for establishing external memory.They generally need to fine-tune the LLMs as a retrieval-specific representation models, which is always feasible but suboptimal.Specifically, LLMs maximize the likelihood of the succeeding properly generated token, building upon the history context text (Brown et al., 2020;Touvron et al., 2023).However, the DR task involves transforming existing text into a vector space, whereby text vectors embodying related semantics draw closer, while those representing distinct semantics are distanced further apart (Karpukhin et al., 2020;Wang &amp; Isola, 2020).The divergence paradigm between text generation and DR makes it difficult for LLM-based retriever/agent share a single LLM, resulting in additional model parameters (requiring a retrieval-specific LLM) and longer inference time (i.e., every query needs being re-encoded by the retrieval-LLM).Consequently, achieving compatibility between retrieval and text generation within a unified LLM remains a significant yet largely unresolved problem.</p>
<p>Considering the impressive zero-shot capabilities of LLM across various NLP tasks, we have every reason to support the hypothesis that the original representation of LLM (i.e., the output of a frozen LLM) contains sufficient semantic information for Dense Retrieval (DR), albeit not aligned : Layer-wise alignment and uniformity analysis in GPT-j-6B.The redder the color, the better the alignment and uniformity.Conversely, the bluer the color, the worse alignment and uniformity.The X-axis denotes six BEIR datasets and their average results.The Y-axis represents the layer number of GPT-j-6B (e.g., #1 is the first embedding layer and #29 is the last hidden layer).</p>
<p>with the DR space.Inspired by previous works (Wang &amp; Isola, 2020), we introduce the alignment and uniformity, which represent two important aspects to measure an ideal DR's vector space, to analyze LLM representation space layer by layer.Alignment favors the representation model that assign similar features to similar samples.Uniformity prefers a feature distribution that preserves maximal information for samples.As illustrated in Fig. 1, our analysis reveals that layers of LLM with better alignment tend to be less uniform, and conversely, layers with high uniformity exhibit weaker alignment.This observation suggests a more promising direction to solve those problems is to synthesize the alignment and uniformity of original LLM's representation space, instead of conducting retrieval-specific LLM tuning.</p>
<p>Motivated by the observations mentioned above, we propose an efficient LLM-Oriented Retrieval Tuner (LMORT), to drives the optimally aligned and uniform layers of the frozen LLM towards a unified DR space.Specifically, the LMORT tuner adapts a Transformer-like structure, comprising two carefully-crafted bidirectional attention operations in each layer.One attention utilizes selfattention to learn features starting from the LLM's optimal alignment (uniformity) layer, and the other cross-attention is operated on LLM's best uniformity (alignment), so as to simultaneously consider uniformity and alignment in a shot.Through the fine-tuning of LMORT with standard DR tasks, the alignment and uniformity properties of the frozen LLM seamlessly merge into a unified space conducive to effective retrieval.</p>
<p>We conduct extensive experiments on six zero-shot retrieval datasets from BEIR benchmark (Thakur et al., 2021), focusing on three LLMs, including GPT2-Large, XL (Radford et al., 2018), and GPTj-6B (Wang, 2021).LMORT demonstrates significant scalability, with its zero-shot retrieval performance improving by 13% as the size of LLM increases from Large to 6B.Even when compared to strong DR baselines with fine-tuned LLM, tuning just three-layer of LMORT yields competitive performance.Our analysis also indicates that LMORT's effectiveness lies in its resonable utilization of LLM alignment and uniformity, mitigating their original incompatibility.Furthermore, we evaluate LMORT's parameter and training efficiency.After dimensionality reduction, with only a marginal 1% performance decrease, LMORT significantly cuts down training parameters to 2% and training time to 4% compared to LLM-based DR fine-tuning.</p>
<p>RELATED WORK</p>
<p>Dense Retrieval (DR) based on Pre-trained Language Models (PLMs) entails the process of finetuning PLMs into dense representation models (Karpukhin et al., 2020;Ni et al., 2022;Neelakantan et al., 2022).Within this category, masked PLMs with bidirectional attention, such as BERT (Kenton &amp; Toutanova, 2019) and T5 (Raffel et al., 2020), have demonstrated substantial empirical advantages for retrieval tasks.Nevertheless, these benefits tend to diminish in zero-shot DR scenarios, particularly when the PLM is either inadequately sized or not meticulously trained (Thakur et al., 2021).</p>
<p>To enhance the zero-shot generalization capabilities of PLM-based DRs, research communities have explored a variety of training techniques, including the adoption of training data augmentation (Ma et al., 2021), refinement training strategies (Yu et al., 2022).Popular data augmentation techniques include the creation of weakly supervised data through text processes (Lee et al., 2019) such as span corruption (Izacard et al., 2022) and pseudo-query generation (Ma et al., 2021).In parallel, commonly employed training strategies contain retrieval-oriented pre-training (Gao &amp; Callan, 2022;Lu et al., 2021), training-negative iteration (Xiong et al., 2021;Si et al., 2022), and cross-encoder distillation (Ren et al., 2021b;Zhang et al., 2022).In addition to improving training techniques, recent work has shown that simply increasing the size of T5 to XXL achieves state-of-the-art performance on the zero-shot retrieval benchmark (Ni et al., 2022).</p>
<p>Recently, considering the powerful ability of decoder-only large language models (LLMs) on a wide range of NLP tasks (Brown et al., 2020;OpenAI, 2023), their potential for retrieval tasks has also been explored.Researchers find that simply increasing the size of the LLM can also significantly boost zero-shot retrieval performance (Muennighoff, 2022;Neelakantan et al., 2022).For instance, cpt-text (Neelakantan et al., 2022) fine-tunes the massive 175B GPT-3 (Brown et al., 2020) as a dense retriever, achieving superior results on many zero-shot retrieval datasets (Thakur et al., 2021).Although directly fine-tuning a larger LLMs is a simpler and effective approach, it comes with a higher training cost and limits the LLM to a retrieval-specific model, making it less compatible with other natural language processing and generation tasks.</p>
<p>In this paper, we employ a distinct approach by fine-tuning a lightweight LLM-oriented retrieval tuner, seamlessly integrated into the LLM without direct alterations to its internal parameters.This approach allows us to unlock the zero-shot retrieval capabilities of the LLM while preserving its versatile generalization abilities at a more cost-effective training expense.</p>
<p>LLM'S ZERO-SHOT DR CAPABILITY ANALYSIS</p>
<p>In this section, we first recaps the preliminary of Dense Retrieval (DR).Following that, we present the analysis findings regarding zero-shot DR capabilities.</p>
<p>PRELIMINARY OF DR</p>
<p>According to a query X q , DR aims to retrieve a set of relevant passages X + p from a large-scale corpus X p ∈ C. Specifically, the query X q and the passage X p can be encoded as dense representations:</p>
<p>x q = g(X q ; ϕ); x p = g(X p ; ϕ),</p>
<p>where g(•; ϕ) denotes the representation model with parameters ϕ.In this way, the whole corpus can be encoded as vector database V and retained for a long term with a limitless storage capacity.</p>
<p>Then the most related K passages can be retrieved by assessing the similarity between the query vector x q and each passage vectors x p , such as dot product and cosine similarity:</p>
<p>Top K xp∈V sim(x q , x p ; ϕ).</p>
<p>(2)</p>
<p>ZERO-SHOT DR CAPABILITY ANALYSIS</p>
<p>We carry out an analysis of frozen LLMs from the views of alignment and uniformity that are two crucial characteristics of an ideal DR space (Wang &amp; Isola, 2020).</p>
<p>Layer-wise Dense Representation.To evaluate the DR potential of LLMs, our initial step involves acquiring a dense representation of the input sequence through the LLM.Achieving this, we transform the hidden states from the LLM's final output layer into dense vectors through mean pooling, and this process is applied to all layers of the LLM.</p>
<p>Given an input sequence X = {x 1 , ..., x t , ..., x n }, the LLM processes the sequence X into a set of layered hidden states H l = {h l 1 , ..., h l t , ..., h l n }, where 1 ≤ l &lt; L and L is the total layer number of the LLM.Then the intermediate states H l of layer l can be pooled into a dense vector x l : H l ← LLM(X; ϕ llm ),
x l = f (H l ),(3)
where ϕ llm and f denotes the LLM's parameters and the mean pooling operation, respectively.Once the layer-wise dense representation is established, the alignment and uniformity of the representation space at each LLM layer can be evaluated.</p>
<p>Layer-wise Alignment Analysis.Alignment requires that two samples forming a positive pair should be mapped to nearby features in the representation space, i.e., the distance between positive samples is excepted to be closer.Formally, let be a positive pair (X, X + ) ∼ p pos , which is processed into LLM's layered hidden states H l X and H l X + (Eq.3), where 0 ≤ l &lt; L. Then each layer of hidden states H l (•) is then mapped into a representation vector f (•) through pooling and normalization operations.The alignment loss L align is introduced to measure the expected distance between positive pairs:
L align (f ) ≜ E (X,X + )∼ppos ||f (H l X ) − f (H l X + )|| 2 2 . (4)
Layer-wise Uniformity Analysis.Uniformity prefers representation vectors should be uniformly distributed on the normalized feature space, preserving as much information of the sample as possible.Similar to alignment analysis, we measure the uniformity loss of each LLM layer.The uniformity loss L uniform is defined as the expected pairwise potential of all sample pairs (X, Y )
i.i.d ∼ p data : L uniform (f ) ≜ log E (X,Y ) i.i.d ∼ pdata e −2||f (H l X )−f (H l Y )|| 2 2 . (5)
Layer-wise Analysis Results.We conduct the layer-wise alignment and uniformity analysis on three causal LLMs of different sizes: GPT2-Large (0.75B, 37 layers), GPT2-XL (1.5B, 49 layers) and GPT-j-6B (6B, 29 layers).The analysis data are six zero-shot retrieval datasets from the BEIR benchmark (Thakur et al., 2021) 1 : TREC-COVID, NFCorpus, FiQA, ArguAna, SciFact, and SCI-DOCS.In these datasets, the query and the relevant passage are regarded as positive pairs (X, X + ) in the alignment analysis.When evaluating uniformity, the pairwise pair (X, Y ) can be uniformly sampled from the query set and the passage set.</p>
<p>Figure 1 illustrates the alignment and uniformity losses computed from the representation space of each layer of GPT-j-6B on these datasets (more results are shown in Appendix C).The results suggest that the inherent nature of LLMs makes it challenging to simultaneously optimize alignment and uniformity within a single layer, as these two properties tend to be mutually exclusive.This observation becomes more pronounced for larger LLMs.Additionally, we also observe that lower layers exhibit better alignment, while higher layers tend to excel in uniformity.This finding is consistent with previous research (Sajjad et al., 2022): the LLM captures shallow concepts at the low layers, such as lexical n-grams.Meanwhile, the lexical overlap is an important feature of positive pairs (alignment); while top layers capture richer and more abstract information, such as morphology, semantics and syntax, revealing that higher layers preserve more information (uniformity).</p>
<p>Through the above analysis, we conclude that the representation spaces of frozen LLMs possess the alignment and uniformity characteristics necessary for an effective retrieval space.However, these two properties are distributed across different layers of the LLM.Consequently, these observations inspire the idea presented in Section 4, which aims to unleash the zero-shot DR capability of LLM, by tuning the alignment and uniformity of the LLM into a unified output representation space.</p>
<p>LLM-ORIENTED RETRIEVAL TUNER (LMORT)</p>
<p>Motivated by the insights discussed in Section 3, we intuitively propose a LLM-oriented retrieval tuner, namely LMORT, which non-invasively tunes the optimal alignment and uniformity layer of LLMs into a unified representation space to achieve a effective LLM-oriented retrieval.</p>
<p>LMORT'S ARCHITECTURE</p>
<p>Figure 2 illustrates the architecture of LMORT, which is a multi-layer architecture built on top of LLM's optimal alignment and uniformity layers.Each LMORT layer contains three carefullydesigned sub-layers.Next, we first introduce the selection of LLM's alignment and uniformity layers and then describe the details of LMORT.LLM's Align &amp; Uniform Layers.As per the analysis method introduced in Section 3, we select the alignment layer with the lowest alignment loss (Eq.4) and the uniform layer with the lowest uniformity loss (Eq.5).Specifically, given an input sequence X = {x 1 , ..., x t , ..., x n }, LLM processes the sequence X into a set of layered hidden states H l = {h l 1 , ..., h l t , ..., h l n }, where 0 ≤ l &lt; L. the LLM's align layer and uniform layer are denoted as H a and H u , respectively:
H a , H u ← LLM(X; ϕ llm ),(6)
LMORT's Layer-wise Structure.Each LMORT block consists of two carefully-designed bidirectional attention sub-layers (i.e., self bi-attention and cross bi-attention sub-layers) and one feedforward layer.Referring to vanilla Transformer blocks, residual connections surround each sublayer, and layer normalization follows.The two attention sub-layers are directed towards the LLM's align layer H a and uniform layer H u , respectively:</p>
<p>• Self Bi-Attention.The first attention sub-layer utilizes a bi-directional attention operation, whereby the attention matrices Q a , K a , and V a are all mapped from the LLM's align layer H a and each token at a given position interacts with tokens from all positions in the sequence.This attention mechanism facilitates the identification and capturing contextual features of sequence from LLM's alignment perspective:
Self-Attention(Q a , K a , V a ) = softmax Q a (K a ) T √ d k V a .(7)
• Cross Bi-Attention.The second attention sub-layer also employs bi-directional attention, but with a significant difference: The key K u and value V u is sourced from the LLM's uniform layer's output H u , while query Q s are obtained from the previous Self Bi-Attention's output.This design establishes an inner connection within LLM's align and uniform layers, enabling LMORT to narrow the gap between them:
Cross-Attention(Q s , K u , V u ) = softmax Q s (K u ) T √ d k V u . (8)
It should be noted that the connections between LMORT's two attention sub-layers and LLM's alignment and uniformity layers can be inter-changed.For instance, Self-Attention(Q u , K u , V u ) can also be applied to the uniform layer H u , while Cross-Attention(Q s , K a , V a ) can be directed towards the alignment layer H a .This connection mode is regarded as one of the hyper-parameters.</p>
<p>Lastly, LMORT's output representation H o is converted into a dense vector x through the mean pooling operation f (•): where θ represents the parameters of the LMORT.In this way, large-scale input sequences X can be encoded and stored as dense vectors V.
H o = LMORT(H a , H u ; θ), x = f (H o ),(9)</p>
<p>LMORT'S TRAINING</p>
<p>We employ the standard DR training method to fine-tune LMORT.Formally, let be a training query set Q and a corpus C, where each query X q is labeled with a set of relevant passages X + p ∈ C + Xq , and negative passages C − Xq sampled from the rest corpus C \ C + Xq .The learning objective can be formulated as optimizing parameters θ of LMORT, in such a way, positive pairs of the query and positive passages (X q , X + p ) have higher similarity than the negative ones (X q , X − p ):
θ * =
where the dense representation of query x q and passage x p +/− are encoded through LLM (Eq. 6) and LMORT (Eq.9).During training, we only tune the LMORT's parameters (θ) and freeze the all parameters of the LLM ( ϕ llm ), where the optimization gradient is not passed back to the LLM.</p>
<p>EXPERIMENTS</p>
<p>In this section, we begin by detailing the experimental setups, followed by conducting hyperparameter studies, comparisons, and ablation experiments on LMORT.Additionally, we perform an in-depth analysis of alignment, uniformity, efficiency, and scalability.</p>
<p>EXPERIMENTAL SETUPS</p>
<p>Evaluation Datasets.We employ six zero-shot retrieval datasets, the same analysis data in Section 3, as testing data.These datasets include TREC-COVID (Voorhees et al., 2021), NFCorpus (Boteva et al., 2016), FiQA (Maia et al., 2018), ArguAna (Wachsmuth et al., 2018), Sci-Fact (Wadden et al., 2020), and SCIDOCS (Cohan et al., 2020).We evaluate model performance using the standard NDCG@10 metric.For training, we leverage MS MARCO (Nguyen et al., 2016), which annotates about 500k web query-positive passages, and use training negatives released by sentence-transformers (Reimers &amp; Gurevych, 2019).More details presents in Appendix A and B.</p>
<p>LMORT Implementation.We select GPTs as the target LLMs, specifically containing GPT2-Large (0.75B), GPT2-XL (1.5B) and GPT-j-6B.For all our training runs, we maintain a batch size of 8, a learning rate of 5e-6, and train for a total of 3 epochs.We evaluate the models using the checkpoint from the last training step, without selecting checkpoints based on testing performance.</p>
<p>We employ a single RTX 3090 GPU (24GB) for GPT2-Large and XL, while GPT-j-6B utilizes four A100 GPUs (40GB).More implementation details are listed in Appendix A.</p>
<p>HYPER-PARAMETER STUDY</p>
<p>We explore three hyperparameters of LMORT: (1) Connection Mode: This pertains to how LLM's optimal alignment (A) and uniformity (U) layers are connected within LMORT.We test two connection methods: (A→U) self-attention to A and cross-attention to U; (U→A) self-attention to U and cross-attention to A. (2) Number of LMORT Layers: We determine the total number of LMORT layers to be utilized.(3) Performance-scaling with base-LLM size: We assess performance scalability under various base-LLMs with different parameter sizes.</p>
<p>Connection Mode.The optimal A&amp;U layer of LLMs is determined by the alignment and uniformity loss (Eq. 4 and Eq. 5).Therefore, we firstly assess these two losses for each layer of GPT2-Large, XL, and GPT-j-6B across six BEIR datasets.The resulting average losses are visualized in Fig. 3.We observe that the A layers differ among the three LLMs, but their U layers are consistently the final layers.Tab. 1 shows the results of specific layer selection and connection mode.As shown in Tab. 1, Large and 6B prefer the A→U connection, whereas XL is better suited for the U→A connection within LMORT.When LMORT is connected to the LLM layers with the worst alignment and uniformity, its retrieval performance significantly declines across all three LLM scenarios.This reveals the critical role of selecting and connecting LLM's A&amp;U layers for LMORT's effectiveness.</p>
<p>Number of LMORT Layers.We further conduct experiments with these LLMs to investigate how the number of LMORT layers affects their zero-shot retrieval performance.The results exhibited in Fig. 4 reveals that various LLMs show effective retrieval performance with a very few number of LMORT layers.Notably, larger LLMs require fewer LMORT layers, i.e., the optimal LMORT layer count is seven for GPT2-Large (L=7), five for GPT2-XL (L=5), and just three for GPT-j-6B (L=3).</p>
<p>Performance-scaling with base-LLM size.As LMORT transitions from GPT2-Large to XL and then GPT-j-6B, its retrieval performance consistently sees improvements of 6% and 7%, respectively.This significant performance-scaling capability highlight the effectiveness of the lightweight LMORT in unlocking the retrieval potential of LLMs without necessitating any tuning of the LLM.</p>
<p>COMPARISON &amp; ABLATION STUDY</p>
<p>Baselines.We experiment four different ablated versions of LMORT.First, (1) we shift LMORT from its best align and uniform layers to the worst A&amp;U layers of LLMs.(2&amp;3) Then, we remove LMORT's cross-bi-attention, only retaining the self-bi-attention to A or U. Since the the last layer of three LLMs consistently serves as U layers, self-attention to U is equivalent to applying self-attention on top of LLMs.(4) To provide a basis for comparison, we also test the effectiveness of applying self-attention to the embedding layer (E) of LLM, indicating LLMs have not been used.Notably, in GPT-j-6B, layer A is identical to layer E.</p>
<p>Apart from the ablated baselines, we also present results from four publicly classic baselines for comparison: BM25 (Yang et al., 2017), DPR (Karpukhin et al., 2020), GTR-XXL (Ni et al., 2022), and cpt-text-L (Neelakantan et al., 2022).BM25 is a sparse retriever, demonstrating strong performance in zero-shot retrieval tasks.On the other hand, DPR, GTR-XXL, and cpt-text-L are dense retrievers employing BERT-base, T5-XXL-encoder (4.5B), and GPT3-6B as their fine-tuning backbones, respectively.It is worth noting that the proposal of LMORT is to equip LLM with zero-shot retrieval capability without altering any internal states of LLMs, instead of achieving state-of-the-art on BEIR.Hence, we do not reference methods that achieve SOTA results through more sophisticated training techniques, even though these skills could theoretically be applied to LMORT.4: The average NDCG@10 results of LMORT with different layer number on three LLMs (GPT2-Large, GPT2-XL, GPT-j-6B).The X-axis means the total layer number of LMORT.The Yaxis denotes the average NDCG@10 scores of six BEIR datasets.Evaluation results.Tab. 2 presents the overall results.Among three different size of base-LLM settings, the performance of the four-ablated LMORT versions is significantly worse compared to the full version.Specifically, when LMORT is converted to the worst A&amp;U layer of LLMs, retrieval performance degrades by 5%, 19%, and 10% for GPT2-Large, XL, and GPT-j-6B, respectively.On the other hand, removing cross-bi-attention from LMORT and retaining only self-bi-attention leads to the largest drops in performance, with decreases of 3%, 6%, and 13% for Large, XL, and 6B, respectively.These results underscore the critical importance of selecting the A&amp;U layer of LLMs and their connections to the self/cross-attention of LMORT.</p>
<p>Compared to strong sparse and dense retrievers on BEIR, the outcome of LMORT is quite promising, considering that the base LLM remains entirely frozen, with only 3 plugin LMORT layers fine-tuned.LMORT initially falls behind when mounted on GPT2-Large.However, its performance greatly improves when transitioning to GPT2-XL, surpassing DPR.Furthermore, with the base LLM scaled up to 6B size, LMORT outperforms BM25 and DPR by 3% and 9%, respectively, trailing behind GTR-XXL by just 2%.As we known, GTR-XXL and cpt-text-L leverage additional training data, while LMORT trains solely with MARCO.We thus have a reasonable expectation that LMORT's performance can be further enhanced through the utilization of data augmentation techniques.</p>
<p>FURTHER ANALYSIS</p>
<p>In this sub-section, we further analyze LMORT's alignment and uniformity, and then quantify the parameter and training efficiency of LMORT.#O means the output layer of LMORT.#A and #U denotes the optimal alignment and uniformity layer of the LLM, respectively.The minimum the loss, the better the alignment and uniformity.</p>
<p>Conversely, the maximum the loss, the worse alignment and uniformity.LMORT's alignment &amp; uniformity.We analyze alignment and uniformity losses in the output (O) layer of LMORT (GPT-j-6B) and compare them to the optimal align (A) and uniform (U) layer of the LLM.These results shown in Fig. 5 highlight LMORT's ability to achieve a better balance between alignment and uniformity within the same representation space.However, this balance does come at some cost to optimal alignment and uniformity.How to simultaneously maintain/surpass the optimal A and U provided by LLM in a unified space, is a potential avenue for future research.</p>
<p>LMORT's parameter efficiency.Additionally, we conduct an assessment of LMORT's parameter size.Tab. 3 presents the results of the standard LMORT mounted on GPT-j-6B, which shares the same hidden vector dimensions as GPT-j-6B, and the dimension-reduced version of LMORT.The standard LMORT comprises only 13% of LLM's parameters, while the dimension-reduced version contains a mere 2% of LLM's parameters, with just a 1% drop in retrieval performance.</p>
<p>LMORT's training efficiency.We also analyze the training efficiency of LMORT.Specifically, we compare the cost time per training step between training LMORT and fine-tuning LLM on a single A100 GPU, where we set the batch size and number of positive and negative passages to one, and the input length to 32.The results of Tab. 3 show that standard LMORT only requires 14% of the time of each training step of directly fine-tuning LLM, and LMORT after dimensionality reduction (Appendix A), even reduces the training time to 4% of that of fine-tuning LLM.Such high training efficiency is due to the mechanism that LMORT avoids propagating gradients back to LLM.</p>
<p>LIMITATIONS AND FUTURE WORK</p>
<p>Limitations.LMORT still lags behind the retrieval performance achieved through LLM-based finetuning at the same scale.We believe this performance gap will narrow with the size of base-LLM.Moreover, it's important to emphasize that LMORT can only be used with open-source LLMs because it necessitates access to the LLM's hidden state.</p>
<p>Future work.LMORT offers a obvious advantage in its compatibility with LLM's retrieval and generation abilities.This makes it an suitable choice for memory-enhanced generation scene, e.g., dealing with long-text modeling and long-range conversations.LMORT can effectively store LLMprocessed information for long periods, facilitating quick retrieval when necessary.These retrieved memories can be seamlessly integrated into the latest modeling sequence, ensuring consistent longrange modeling.The application of LMORT will be left for future research.</p>
<p>CONCLUSION</p>
<p>In this paper, we initially conduct a layer-wise analysis on the representation space of the frozen LLM from the perspective of alignment and uniformity traits for DR, observing mutually exclusive nature between those two metrics.Subsequently, we further propose a novel tuner, namely LMORT, which strikes a trade-off between the optimal alignment and uniformity layers of LLM, establishing Training hyper-parameter.We outline all of our training hyperparameters in Tab. 4.During training LMORT (GPT-j-6B), we use DeepSpeed ZeRO-2 technology (Ren et al., 2021a) for gradient partitioning.Additionally, the corresponding code can also be found in the supplementary material.</p>
<p>Dimensionality reduction.In Sec 5.4, we simply employ two-layer MLPs to perform dimensionality reduction on LMORT (GPT-j-6B).The 4096-dimensional hidden states from the LLM's alignment and uniformity layer passes through two MLP layers with sizes of 8192 and 1024 dimensions, respectively.This results in a 4-fold reduction in dimensionality, reducing the inner dimension size of LMORT to 1024.Further implementation details can be located in the supplementary material.</p>
<p>B EVALUATION DETAILS</p>
<p>Evaluation datasets.Six BEIR datasets (Thakur et al., 2021) are used for zero-shot DR evaluation, i.e., TREC-COVID (Voorhees et al., 2021), NFCorpus (Boteva et al., 2016), FiQA-2018(Maia et al., 2018), ArguAna (Wachsmuth et al., 2018), SciFact (Wadden et al., 2020), and SCIDOCS (Cohan et al., 2020).Tab. 5 shows their statistics.The Normalised Cumulative Discount Gain (NDCG@10) score on the test set is used as the metric, which is consistent with prior work (Thakur et al., 2021).Evaluation hyper-parameters.We keep the same evaluation hyperparameter settings as utilized in previous research (Yu et al., 2022).Detailed hyperparameters can be found in its public evaluation script3 .During the evaluation of LMORT (GPT-j-6B), we adopt the DeepSpeed Zero-3 technology (Ren et al., 2021a) for parameter partitioning.6, 7, and 8 show in detail the layer-wise analysis results on the alignment loss and uniformity loss for GPT2-Large (37 layers), GPT2-XL (49 layers), and GPT-j-6B (29 layers), as described in Section 3. Lower loss values in these figures indicate a higher level of alignment and uniformity.</p>
<p>Figure 6: The layer-wise alignment and uniformity analysis of GPT2-Large on six BEIR datasets.</p>
<p>The minimum the loss, the better the alignment and uniformity.Conversely, the maximum the loss, the worse alignment and uniformity.</p>
<p>Figure 7: The layer-wise alignment and uniformity analysis of GPT2-XL on six BEIR datasets.The minimum the loss, the better the alignment and uniformity.Conversely, the maximum the loss, the worse alignment and uniformity.</p>
<p>Figure 8: The layer-wise alignment and uniformity analysis of GPT-j-6B on six BEIR datasets.The minimum the loss, the better the alignment and uniformity.Conversely, the maximum the loss, the worse alignment and uniformity.</p>
<p>The analysis code for estimating the layer-wise alignment loss and uniformity loss is available in the supplementary material.</p>
<p>FRYLG</p>
<p>Figure1: Layer-wise alignment and uniformity analysis in GPT-j-6B.The redder the color, the better the alignment and uniformity.Conversely, the bluer the color, the worse alignment and uniformity.The X-axis denotes six BEIR datasets and their average results.The Y-axis represents the layer number of GPT-j-6B (e.g., #1 is the first embedding layer and #29 is the last hidden layer).</p>
<p>Figure 2 :
2
Figure 2: Illustration of LLM-Oriented Retrieval Tuner (LMORT).The total layer number of LMORT is much less than that of the frozen LLM (M ≪ N ).</p>
<p>Figure 3 :
3
Figure3: The average results of layer-wise alignment and uniformity estimation on six BEIR datasets.The redder the color, the better the alignment and uniformity.Conversely, the bluer the color, the worse alignment and uniformity.The Y-axis represents the layer number of three GPTs.</p>
<p>Figure 5 :
5
Figure5: The alignment and uniformity analysis of LMORT (GPT-j-6B) on six BEIR datasets.#O means the output layer of LMORT.#A and #U denotes the optimal alignment and uniformity layer of the LLM, respectively.The minimum the loss, the better the alignment and uniformity.Conversely, the maximum the loss, the worse alignment and uniformity.</p>
<p>Figures</p>
<p>Figures 6, 7, and 8  show in detail the layer-wise analysis results on the alignment loss and uniformity loss for GPT2-Large (37 layers), GPT2-XL (49 layers), and GPT-j-6B (29 layers), as described in Section 3. Lower loss values in these figures indicate a higher level of alignment and uniformity.</p>
<p>Table 1 :
1
LLM's optimal alignment (A) and uniformity (U) layers and the average NDCG@10 scores of LMORT on six BEIR datasets.A→U denotes LMORT applies self-attention on A layer and cross-attention to U layer.U→A means using self-attention on U and cross-attention to A. worst A&amp;U denotes connecting LMORT to the worst A&amp;U layers.
LLMsA layer U layerLMORT (NDCG@10) A→U U→A worst A&amp;UGPT2-Large#36#370.296 0.2940.248GPT2-XL#4#490.342 0.3550.167GPT-j-6B#1#290.425 0.4170.324</p>
<p>Table 2 :
2
Overall NDCG@10 results on six BEIR datasets.L means the layer number of LMORT.A, U, and E denotes the alignment (A), uniformity (U), and embedding (E) layers of LLMs, respectively.
LLMsMethodsCOVID NFCorpus FiQA ArguAna SciFact SCIDOCS AVGLMORT (L=7)0.4550.2240.1640.3930.4510.0910.296self/cross-attn to worst A&amp;U0.4050.1890.1760.2350.3960.0850.248GPT2-Largeonly self-attn to A0.3710.2060.1430.3490.4480.0760.266(Fix)only self-attn to U (top LLM)0.3730.2030.1310.3760.4710.0760.272only self-attn to E (w/o LLM)0.4810.1750.1340.3160.3680.0900.261LMORT (L=5)0.6000.2360.2190.4280.5260.1220.355self/cross-attn to worst A&amp;U0.2870.1560.1140.0940.2810.0680.167GPT2-XLonly self-attn to A0.4990.2070.1650.3300.5240.1070.305(Fix)only self-attn to U (top LLM)0.4180.2230.1680.4260.4490.0880.295only self-attn to E (w/o LLM)0.4720.1710.1230.3210.4690.0870.274LMORT (L=3)0.7350.2800.2510.4760.6790.1260.425self/cross-attn to worst A&amp;U0.6980.2860.2290.4050.2050.1220.324GPT-j-6Bonly self-attn to A (w/o LLM)0.5160.1930.1440.3080.5030.0940.293(Fix)only self-attn to U (top LLM)0.7070.2900.2480.4320.6460.1150.406only self-attn to E (w/o LLM)0.5160.1930.1440.3080.5030.0940.293For Reference: Sparse Retrieval and Dense Retrieval (fine-tuning all parameters)BM250.6560.3250.2360.3150.6650.1580.393DPR (BERT-base)0.5880.2340.2060.3940.4940.1190.339GTR-XXL (T5-enc-4.5B)0.5010.3420.4670.5400.6620.1610.445cpt-text-L (GPT3-6B)0.5620.3800.4520.4690.744n.a.n.a.</p>
<p>Table 3 :
3
NDCG@10 results of the standard and dimension-reduced versions of LMORT (L=3), where LLM is GPT-j-6B.#D denotes the dimension size.#P denotes the ratio of parameters of LMORT to the base LLM.#T represents the ratio of the average time of each training step to that of directly fine-tuning the LLM.
LMORT#D #P #T COVID NFCorpus FiQA ArguAna SciFact SCIDOCS AVGstandard4096 13% 14%0.7350.2800.2510.4760.6790.1260.425dim-reduced 1024 2%4%0.7340.2800.2480.4270.6630.1220.412</p>
<p>(Nguyen et al., 2016)presentation for DR.Extensive experiments on six BEIR datasets show that LMORT could unlock zero-shot capacity of LLM and achieve competitive performance in terms of retrieval ability and parameter/training efficiency.The plugin paragram of LMORT could unify the DR and text generation in a shared LLM, providing a new alternative for memory-augmented LLM.We employ the MS MARCO passage ranking dataset(Nguyen et al., 2016)as the training data, which is curated from actual user search queries originating from the Bing search engine.Within MARCO, there are approximately 500k pairs of training queries and corresponding positive passages.The training negatives used are sampled from the collected negatives provided by sentence-transformer 2 .We have made the training negatives available in supplementary material.
A TRAINING DETAILSTraining dataset. Table 4: Hyper-parameter of training LMORT.HyperparametersGPT2-Large (Fix)GPT2-XL (Fix)GPT-j-6B (Fix)LMORT layer number753Batch size (query size)888Positive size per query111Negative size per query7715Max query length323232Max passage length128128128Learning rate5e-65e-65e-6OptimizerAdamWAdamWAdamWSchedulerWarmupDecayLR WarmupDecayLR WarmupDecayLRWarmup ratio0.10.10.1Training epoch333FP16✓✓✓DeepSpeed✘✘ZeRO-2</p>
<p>Table 5 :
5
Statistics of evaluation datasets.
DatasetTest Query CorpusTREC-COVID 50171332NFCorpus3233633FiQA-201864857638ArguAna14068674SciFact3005183SCIDOCS100025657
Due to the high cost of LLM-oriented evaluation, we have chosen six reasonably sized datasets for experiments from all 18 BEIR datasets. More details of these datasets can be found in Appendix B.
https://huggingface.co/datasets/sentence-transformers/ msmarco-hard-negatives
https://github.com/OpenMatch/OpenMatch/tree/master/scripts/BEIR</p>
<p>A full-text learning to rank dataset for medical information retrieval. Vera Boteva, Demian Gholipour Ghalandari, Artem Sokolov, Stefan Riezler, 10.1007/978-3-319-30671-1_58Advances in Information Retrieval -38th European Conference on IR Research ECIR. 2016</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>SPECTER: Document-level representation learning using citation-informed transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel Weld, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020Online</p>
<p>Unsupervised corpus aware language model pre-training for dense passage retrieval. Luyu Gao, Jamie Callan, 10.18653/v1/2022.acl-long.203Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandMay 22-27, 2022. 20221ACL 2022</p>
<p>Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, Transactions on Machine Learning Research. 2022</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT2019</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 2022</p>
<p>Latent retrieval for weakly supervised open domain question answering. Kenton Lee, Ming-Wei Chang, Kristina Toutanova, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Less is more: Pretrain a strong siamese encoder for dense text retrieval using a weak decoder. Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, Arnold Overwijk, 10.18653/v1/2021.emnlp-main.220Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021. 2021</p>
<p>Zero-shot neural passage retrieval via domain-targeted synthetic question generation. Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, Ryan Mcdonald, 10.18653/v1/2021.eacl-main.92Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain Volume2021</p>
<p>Www'18 open challenge: Financial opinion mining and question answering. Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross Mcdermott, Manel Zarrouk, Alexandra Balahur, 10.1145/3184558.3192301Companion Proceedings of the The Web Conference 2018. 2018</p>
<p>SGPT: GPT sentence embeddings for semantic search. Niklas Muennighoff, CoRR, abs/2202.089042022</p>
<p>Text and code embeddings by contrastive pre-training. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, arXiv:2201.100052022arXiv preprint</p>
<p>On memory in human and artificial language processing systems. Aida Nematzadeh, Sebastian Ruder, Dani Yogatama, Proceedings of ICLR Workshop on Bridging AI and Cognitive Science. ICLR Workshop on Bridging AI and Cognitive Science2020</p>
<p>A human generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, Marco, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016). CEUR Workshop Proceedings. CEUR-WS.org. the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)Barcelona, SpainDecember 9, 2016. 20161773</p>
<p>Large dual encoders are generalizable retrievers. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Y Vincent, Yi Zhao, Keith B Luan, Ming-Wei Hall, Chang, 10.18653/v1/2022.emnlp-main.669Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP. the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP2022</p>
<p>OpenAI. Gpt-4 technical report. 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2018</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 212020</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Zero-offload: Democratizing billion-scale model training. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He, 2021 USENIX Annual Technical Conference, USENIX ATC 2021. July 14-16, 2021. 2021aUSENIX Association</p>
<p>Rocketqav2: A joint training method for dense passage retrieval and passage reranking. Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, Ji-Rong Wen, 10.18653/v1/2021.emnlp-main.224Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP2021b</p>
<p>Analyzing encoded concepts in transformer language models. Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Rafae Khan, Jia Xu, 10.18653/v1/2022.naacl-main.225Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesJuly 10-15, 2022. 2022</p>
<p>Reduce catastrophic forgetting of dense retrieval training with teleportation negatives. Sun Si, Xiong Chenyan, Yu Yue, Overwijk Arnold, Liu Zhiyuan, Bao Jie, 10.18653/v1/2022.emnlp-main.445Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP. the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP2022</p>
<p>Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, Iryna Gurevych, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks2021</p>
<p>Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Trec-covid: Constructing a pandemic information retrieval test collection. Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, Lucy Lu, Wang , 10.1145/3451964.3451965SIGIR Forum. feb 202154</p>
<p>Retrieval of the best counterargument without prior topic knowledge. Henning Wachsmuth, Shahbaz Syed, Benno Stein, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 20181</p>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 2020</p>
<p>Ben Wang, Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX. 2021</p>
<p>Understanding contrastive representation learning through alignment and uniformity on the hypersphere. Tongzhou Wang, Phillip Isola, Proceedings of the 37th International Conference on Machine Learning, ICML 2020. the 37th International Conference on Machine Learning, ICML 2020PMLR13-18 July 2020. 2020</p>
<p>Llm powered autonomous agents. Lilian Weng, 2023</p>
<p>Approximate nearest neighbor negative contrastive learning for dense text retrieval. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N Bennett, Junaid Ahmed, Arnold Overwijk, Proceedings of International Conference on Learning Representations. International Conference on Learning Representations2021</p>
<p>Anserini: Enabling the use of lucene for information retrieval research. Peilin Yang, Hui Fang, Jimmy Lin, 10.1145/3077136.3080721Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17. the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17New York, NY, USAAssociation for Computing Machinery2017</p>
<p>Coco-dr: Combating distribution shifts in zero-shot dense retrieval with contrastive and distributionally robust learning. Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, Arnold Overwijk, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Adversarial retriever-ranker for dense text retrieval. Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, Weizhu Chen, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. April 25-29, 2022. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
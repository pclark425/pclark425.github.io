<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9851 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9851</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9851</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-394924896e24c9b086d96d0958dae07f54ff9452</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/394924896e24c9b086d96d0958dae07f54ff9452" target="_blank">Agent Laboratory: Using LLM Agents as Research Assistants</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work introduces Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process, and finds that it significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods.</p>
                <p><strong>Paper Abstract:</strong> Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9851.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9851.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentLab-LitRev</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Laboratory Literature Review (PhD agent + arXiv)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The literature review component of Agent Laboratory where a PhD agent queries the arXiv API, retrieves abstracts and full texts, iteratively summarizes and curates relevant papers to produce a curated review used by downstream agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Varied LLM backends used in Agent Laboratory (gpt-4o, o1-mini, o1-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM backends (commercial/packaged models) used as language agents in the pipeline; exact architectures and parameter counts are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Papers retrieved from arXiv via the arXiv API; the PhD agent issues queries, retrieves the top relevant abstracts (initially top-20) and can fetch full text for selected papers; iterative retrieval and selection are used to assemble a curated review.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>20</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>A human-provided research idea / initial query guiding arXiv searches; the PhD agent refines queries iteratively to identify relevant literature for the specified research idea.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-based summarization pipeline: the PhD agent issues arXiv API queries, performs three actions—summary (retrieve abstracts of top-20 relevant papers), full text retrieval (extract full content of specific papers), and add paper (incorporate selected summaries or full texts into curated review). The process is iterative: multiple queries, relevance evaluation of returned papers, and refinement until a specified number N (max) of texts is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Curated literature review composed of paper summaries and selected full-text extracts (structured summaries used as references for planning and experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A curated review consisting of the top ~20 paper abstracts and selected full-text summaries organized for downstream planning and experiment design (no verbatim long example provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Indirect evaluation via downstream task success and human assessments of final generated papers; runtime metrics include a literature-review phase success rate reported per backend (see paper runtime statistics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Phase success rates reported: literature review success rates of 60% (gpt-4o), 70% (o1-mini), and 80% (o1-preview). No direct intrinsic-quality metric for the literature review itself is reported beyond its contribution to downstream outputs and user feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Automated, iterative retrieval and curation from a large scholarly repository (arXiv); integrates directly into an end-to-end research pipeline; allows fetching of abstracts and full text and assembling a focused review tailored to a human-provided idea; compute-flexible (can run with different LLM backends).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Models sometimes struggled with instruction following during the literature review (e.g., repeatedly issuing summarize until phase termination); retrieved papers could hit model token limits; search queries could fail or require many retries before returning papers; N (the max number) unspecified and review quality not directly validated; literature-review failures contributed to lower overall pipeline performance for lower-performing backends.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Repeated use of the summarize command until termination (leading to failure); retrieved papers reaching token limits for some models; slow or failing arXiv searches (pre-timeout could take many tries); literature-review phase had the highest failure rate among phases (60–80% success depending on backend).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Laboratory: Using LLM Agents as Research Assistants', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9851.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9851.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>paper-solver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper-Solver (report synthesis with arXiv access and LLM reviewer loop)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized Agent Laboratory module that generates an initial LaTeX scaffold for a research paper, queries arXiv to gather references, iteratively edits the manuscript with compile checks, and uses an automated LLM reviewer to score and refine the draft.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Agent Laboratory LLM backends (gpt-4o, o1-mini, o1-preview) orchestrating paper-solver; the paper also leverages an adapted LLM-based automated reviewer (from Lu et al. (2024b)).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-driven iterative document generation and editing system: generates section-by-section LaTeX scaffold, performs arXiv retrieval during writing, applies targeted EDIT commands for line-level LaTeX modification, compiles LaTeX to verify integrity, and invokes an LLM-based reviewer for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Access to prior literature retrieved by the literature-review step and additional on-demand arXiv searches during scaffold building; uses the curated review and possibly expanded arXiv results to source citations and background material.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>The human-provided research idea and the curated literature review; paper-solver may issue additional arXiv queries specific to sections being written (e.g., related work or methods).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Iterative scaffold-and-refine synthesis: (A) generate initial LaTeX scaffold with placeholders for standard sections (Abstract, Introduction, Background, Related Work, Methods, Experimental Setup, Results, Discussion); (B) perform arXiv searches to pull in literature while filling sections; (C) apply line-level EDIT operations and compile LaTeX to ensure syntactic correctness; (D) run an LLM-based automated review (NeurIPS-style) to score and guide revisions; loop until the PhD agent decides to finalize.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured academic paper in LaTeX format (complete scaffold and filled sections), plus reviewer-like critique outputs used for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>The paper provides an example automated review (from o1-mini) with Strengths/Weaknesses, numerical NeurIPS-style scores (Originality, Quality, Clarity, Significance), questions, limitations, and an overall decision (example: 'Overall': 7, 'Decision': 'Accept').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Two-pronged evaluation: (1) automated LLM-based reviewers (adapted from Lu et al. (2024b)) applied during paper-solver iterations; (2) human NeurIPS-style reviewer evaluations and other human quality/usefulness ratings collected for final papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Automated reviewers rated generated papers relatively highly (average overall 6.1/10), but human reviewers rated them substantially lower (average overall 3.8/10), indicating a major overestimation by automated review. Human NeurIPS-style overall scores by backend averaged: 3.5/10 (gpt-4o), 3.8/10 (o1-mini), 4.0/10 (o1-preview). Co-pilot (human-in-loop) papers improved scores modestly but remained below accepted conference averages.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Produces full LaTeX-formatted drafts aligned to standard academic structures; integrates literature retrieval during writing; uses compile checks to ensure LaTeX validity; integrates automated review to drive iterative refinement; supports both autonomous and human co-pilot modes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Enforces a relatively fixed paper structure (may disallow novel section organizations); restricted to generating at most two figures (as configured in this work); struggles to find relevant papers via arXiv reliably at times; hallucination of experimental details observed especially with lower-performing backends; automated reviewer scores poorly align with human reviewers, limiting self-evaluation reliability; intended as a human-facing summary rather than a fully polished publishable paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hallucinated experimental details (e.g., fabricated hyperparameter settings and experimental runs); paper-solver often struggled with arXiv searches (could require many tries before success until a search time-limit was enforced); generated fewer/poorer quality figures than expected; fixed-structure limitation prevented alternative paper organizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Laboratory: Using LLM Agents as Research Assistants', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9851.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9851.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-Reviewer (adapted Lu2024b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapted LLM-based Automated Reviewer (from Lu et al. (2024b))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adapted automated review system used by paper-solver that employs an LLM to simulate NeurIPS-style peer review, producing numeric scores and textual reviews used both for paper refinement and for internal evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adapted LLM-based reviewer (original system from Lu et al. (2024b)); in this work it is invoked with the Agent Laboratory backends (e.g., o1-mini example shown).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM configured to simulate peer reviewers and produce NeurIPS-style review outputs (strengths/weaknesses, numeric scores across criteria such as Originality, Quality, Clarity, Significance, Soundness, Presentation, Contribution, and overall decision).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Individual generated paper drafts (LaTeX content) and the associated research plan/outputs; for calibration in prior work the system was evaluated on 500 ICLR 2022 papers from OpenReview.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>500</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>A generated manuscript (from paper-solver) presented for evaluation following NeurIPS review guidelines.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not a distillation per se; the system ingests a candidate manuscript and outputs a structured review and scalar scores following NeurIPS review criteria. It was calibrated/evaluated on a corpus of prior conference submissions to align reviewer behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured peer review: textual strengths/weaknesses, a list of reviewer questions, limitations, and numeric scores per NeurIPS criteria plus an overall recommendation/decision.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>"Strengths": [...], "Weaknesses": [...], "Originality": 3, "Quality": 4, "Clarity": 3, "Significance": 3, "Overall": 7, "Decision": "Accept" (example provided from an o1-mini automated review in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Calibrated/validated against human OpenReview/ICLR reviews: compared automated reviewer outputs versus ground-truth OpenReview reviewer outcomes on 500 ICLR 2022 papers and measured accuracy and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On the 500 ICLR 2022 papers, the automated reviewer achieved human-level accuracy (65% vs human 66%) and higher F1 (0.57 vs human 0.49) after calibration. However, when applied to Agent Laboratory generated papers, automated reviewer overall scores (avg 6.1/10) substantially overestimated human reviewer scores (avg 3.8/10), indicating misalignment in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Enables automated, repeatable NeurIPS-style critique for iterative refinement; demonstrated calibration success on a 500-paper ICLR dataset in prior work; produces structured feedback useful for guiding edits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tends to over-estimate quality on machine-generated papers in this study (systematic discrepancy vs human reviewers); reliance on surface cues may produce optimistic assessments not matched by human judgments; calibration on conference submissions does not fully generalize to AI-generated manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Significant divergence between automated reviewer scores and human reviewer scores for Agent Laboratory outputs (automated reviewers ~+2.3 points higher on overall NeurIPS score), leading to misleading self-assessment if used without human checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Laboratory: Using LLM Agents as Research Assistants', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9851.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9851.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent (Baek et al. 2024) - related LLM-agent research system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited prior system where LLM agents automatically generate research ideas, experimental methods, and designs and iteratively refine them using multiple reviewing agents aligned with human evaluation criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ResearchAgent (as described in Baek et al. (2024)); specific model details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An agent-based research automation system that uses LLMs to propose research ideas and plans and refines them via simulated reviewer feedback; details are provided in the cited work rather than in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; described generally as performing automated research ideation and experiment design.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Automated generation of novel research ideas and experiment designs (no single human-provided query required in that system, per description here).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Iterative idea generation and refinement using multiple reviewing agents that simulate peer discussion and leverage human-aligned evaluation criteria to improve outputs (per description in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated research ideas, experimental methods, and experiment designs; iterative improvements to proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Mentioned as using reviewing agents and iterative refinement; the present paper cites ResearchAgent demonstrating idea novelty but also notes limitations in feasibility/implementation (Si et al. (2024)).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Related work reports that LLMs can generate ideas judged more novel than humans (cited), but separate work (Si et al. (2024)) indicates weaknesses in feasibility and implementation details, suggesting complementarity with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Demonstrated capability to produce novel research ideas and iterative refinement via simulated reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reported weaknesses in feasibility and implementation detail (per Si et al. (2024)); not necessarily reliable as a sole replacement for human researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Feasibility and implementation gaps identified in follow-up analyses (no concrete failure examples detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Laboratory: Using LLM Agents as Research Assistants', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9851.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9851.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mle-solver (Kaggle distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mle-solver (Agent Laboratory experimentation module using distilled Kaggle knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized module in Agent Laboratory for autonomously generating, testing, and refining machine learning code; it ingests Kaggle dataset descriptions and distilled knowledge from Kaggle notebooks to construct solutions for MLE-Bench challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mle-solver orchestrated with Agent Laboratory LLM backends for code generation and an LLM reward model for program scoring (when configured); in MLE-Bench evaluation, dev-set scoring replaced the LLM reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An iterative program synthesis and evaluation loop: initialize a program, perform EDIT and REPLACE operations to generate new code files or modify line ranges, compile and attempt repairs up to N_rep tries, score programs (via an LLM reward model or dev-set evaluation), self-reflect on failures/successes, and maintain a top-program pool with batch-parallel exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Kaggle dataset descriptions, distilled content from Kaggle notebooks, and accessible train/dev splits provided as in-memory arrays for the solver to use when preparing data and training models.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Specific Kaggle competition/tasks from a subset of MLE-Bench (text and tabular low-complexity challenges); the solver receives dataset metadata and distilled notebook knowledge to guide modeling choices.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Distillation of practitioner knowledge from Kaggle notebooks: aggregated distilled knowledge (notebooks) plus dataset descriptions are provided as context; mle-solver synthesizes code via iterative program edits (EDIT/REPLACE), compiles and tests code, uses an evaluation function (LLM-based reward or dev-set scoring) to rank programs, and applies self-reflection to guide subsequent edits.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Executable machine-learning experiment code (Python scripts/notebooks) implementing data preparation, model training, and evaluation for Kaggle tasks; final high-scoring program submitted for benchmark scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Final code solutions that achieved medal rankings on MLE-Bench tasks (specific code not reproduced in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Benchmarked on a subset of 10 MLE-Bench Kaggle challenges: mle-solver runs produce submissions evaluated with Kaggle-style scoring and medal assignments; comparisons made against other solver systems (MLAB, OpenHands, AIDE).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>mle-solver obtained four medals across the evaluated subset (two gold, one silver, one bronze) and exceeded median human performance on 6/10 benchmarks. It outperformed MLAB and OpenHands on consistency and medal counts in the reported subset.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Consistent high-scoring solutions on evaluated Kaggle tasks; iterative generate-compile-test-repair loop with program-level scoring and self-reflection; maintains a top-program pool for stability and uses batch-parallel exploration to balance exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Observed tendencies include over-editing at top-of-file (line 0), generating Python exit() calls that prematurely terminate processes, occasional 0% accuracy solutions that remain uncorrected before solver exhaustion, and potential unsafe execution of subprocess.run() commands. The LLM reward-based scoring was sometimes replaced by dev-set scoring for objective benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Common failure modes listed in the paper include: mle-solver producing 0% accuracy for all tested methods on some runs; frequent edits concentrated at line 0; generation of exit() commands requiring manual interception; generated system commands (subprocess.run()) that could be risky; printed output causing LLM token-limit issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent Laboratory: Using LLM Agents as Research Assistants', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ResearchAgent <em>(Rating: 2)</em></li>
                <li>The AI Scientist <em>(Rating: 2)</em></li>
                <li>MLE-Bench <em>(Rating: 2)</em></li>
                <li>AIDE <em>(Rating: 2)</em></li>
                <li>OpenHands <em>(Rating: 1)</em></li>
                <li>MLAgentBench <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9851",
    "paper_id": "paper-394924896e24c9b086d96d0958dae07f54ff9452",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "AgentLab-LitRev",
            "name_full": "Agent Laboratory Literature Review (PhD agent + arXiv)",
            "brief_description": "The literature review component of Agent Laboratory where a PhD agent queries the arXiv API, retrieves abstracts and full texts, iteratively summarizes and curates relevant papers to produce a curated review used by downstream agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Varied LLM backends used in Agent Laboratory (gpt-4o, o1-mini, o1-preview)",
            "model_description": "Autoregressive transformer LLM backends (commercial/packaged models) used as language agents in the pipeline; exact architectures and parameter counts are not specified in this paper.",
            "model_size": null,
            "input_corpus_description": "Papers retrieved from arXiv via the arXiv API; the PhD agent issues queries, retrieves the top relevant abstracts (initially top-20) and can fetch full text for selected papers; iterative retrieval and selection are used to assemble a curated review.",
            "input_corpus_size": 20,
            "topic_query_description": "A human-provided research idea / initial query guiding arXiv searches; the PhD agent refines queries iteratively to identify relevant literature for the specified research idea.",
            "distillation_method": "Retrieval-based summarization pipeline: the PhD agent issues arXiv API queries, performs three actions—summary (retrieve abstracts of top-20 relevant papers), full text retrieval (extract full content of specific papers), and add paper (incorporate selected summaries or full texts into curated review). The process is iterative: multiple queries, relevance evaluation of returned papers, and refinement until a specified number N (max) of texts is reached.",
            "output_type": "Curated literature review composed of paper summaries and selected full-text extracts (structured summaries used as references for planning and experiments).",
            "output_example": "A curated review consisting of the top ~20 paper abstracts and selected full-text summaries organized for downstream planning and experiment design (no verbatim long example provided in the paper).",
            "evaluation_method": "Indirect evaluation via downstream task success and human assessments of final generated papers; runtime metrics include a literature-review phase success rate reported per backend (see paper runtime statistics).",
            "evaluation_results": "Phase success rates reported: literature review success rates of 60% (gpt-4o), 70% (o1-mini), and 80% (o1-preview). No direct intrinsic-quality metric for the literature review itself is reported beyond its contribution to downstream outputs and user feedback.",
            "strengths": "Automated, iterative retrieval and curation from a large scholarly repository (arXiv); integrates directly into an end-to-end research pipeline; allows fetching of abstracts and full text and assembling a focused review tailored to a human-provided idea; compute-flexible (can run with different LLM backends).",
            "limitations": "Models sometimes struggled with instruction following during the literature review (e.g., repeatedly issuing summarize until phase termination); retrieved papers could hit model token limits; search queries could fail or require many retries before returning papers; N (the max number) unspecified and review quality not directly validated; literature-review failures contributed to lower overall pipeline performance for lower-performing backends.",
            "failure_cases": "Repeated use of the summarize command until termination (leading to failure); retrieved papers reaching token limits for some models; slow or failing arXiv searches (pre-timeout could take many tries); literature-review phase had the highest failure rate among phases (60–80% success depending on backend).",
            "uuid": "e9851.0",
            "source_info": {
                "paper_title": "Agent Laboratory: Using LLM Agents as Research Assistants",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "paper-solver",
            "name_full": "Paper-Solver (report synthesis with arXiv access and LLM reviewer loop)",
            "brief_description": "A specialized Agent Laboratory module that generates an initial LaTeX scaffold for a research paper, queries arXiv to gather references, iteratively edits the manuscript with compile checks, and uses an automated LLM reviewer to score and refine the draft.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Agent Laboratory LLM backends (gpt-4o, o1-mini, o1-preview) orchestrating paper-solver; the paper also leverages an adapted LLM-based automated reviewer (from Lu et al. (2024b)).",
            "model_description": "LLM-driven iterative document generation and editing system: generates section-by-section LaTeX scaffold, performs arXiv retrieval during writing, applies targeted EDIT commands for line-level LaTeX modification, compiles LaTeX to verify integrity, and invokes an LLM-based reviewer for scoring.",
            "model_size": null,
            "input_corpus_description": "Access to prior literature retrieved by the literature-review step and additional on-demand arXiv searches during scaffold building; uses the curated review and possibly expanded arXiv results to source citations and background material.",
            "input_corpus_size": null,
            "topic_query_description": "The human-provided research idea and the curated literature review; paper-solver may issue additional arXiv queries specific to sections being written (e.g., related work or methods).",
            "distillation_method": "Iterative scaffold-and-refine synthesis: (A) generate initial LaTeX scaffold with placeholders for standard sections (Abstract, Introduction, Background, Related Work, Methods, Experimental Setup, Results, Discussion); (B) perform arXiv searches to pull in literature while filling sections; (C) apply line-level EDIT operations and compile LaTeX to ensure syntactic correctness; (D) run an LLM-based automated review (NeurIPS-style) to score and guide revisions; loop until the PhD agent decides to finalize.",
            "output_type": "Structured academic paper in LaTeX format (complete scaffold and filled sections), plus reviewer-like critique outputs used for refinement.",
            "output_example": "The paper provides an example automated review (from o1-mini) with Strengths/Weaknesses, numerical NeurIPS-style scores (Originality, Quality, Clarity, Significance), questions, limitations, and an overall decision (example: 'Overall': 7, 'Decision': 'Accept').",
            "evaluation_method": "Two-pronged evaluation: (1) automated LLM-based reviewers (adapted from Lu et al. (2024b)) applied during paper-solver iterations; (2) human NeurIPS-style reviewer evaluations and other human quality/usefulness ratings collected for final papers.",
            "evaluation_results": "Automated reviewers rated generated papers relatively highly (average overall 6.1/10), but human reviewers rated them substantially lower (average overall 3.8/10), indicating a major overestimation by automated review. Human NeurIPS-style overall scores by backend averaged: 3.5/10 (gpt-4o), 3.8/10 (o1-mini), 4.0/10 (o1-preview). Co-pilot (human-in-loop) papers improved scores modestly but remained below accepted conference averages.",
            "strengths": "Produces full LaTeX-formatted drafts aligned to standard academic structures; integrates literature retrieval during writing; uses compile checks to ensure LaTeX validity; integrates automated review to drive iterative refinement; supports both autonomous and human co-pilot modes.",
            "limitations": "Enforces a relatively fixed paper structure (may disallow novel section organizations); restricted to generating at most two figures (as configured in this work); struggles to find relevant papers via arXiv reliably at times; hallucination of experimental details observed especially with lower-performing backends; automated reviewer scores poorly align with human reviewers, limiting self-evaluation reliability; intended as a human-facing summary rather than a fully polished publishable paper.",
            "failure_cases": "Hallucinated experimental details (e.g., fabricated hyperparameter settings and experimental runs); paper-solver often struggled with arXiv searches (could require many tries before success until a search time-limit was enforced); generated fewer/poorer quality figures than expected; fixed-structure limitation prevented alternative paper organizations.",
            "uuid": "e9851.1",
            "source_info": {
                "paper_title": "Agent Laboratory: Using LLM Agents as Research Assistants",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Auto-Reviewer (adapted Lu2024b)",
            "name_full": "Adapted LLM-based Automated Reviewer (from Lu et al. (2024b))",
            "brief_description": "An adapted automated review system used by paper-solver that employs an LLM to simulate NeurIPS-style peer review, producing numeric scores and textual reviews used both for paper refinement and for internal evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Adapted LLM-based reviewer (original system from Lu et al. (2024b)); in this work it is invoked with the Agent Laboratory backends (e.g., o1-mini example shown).",
            "model_description": "LLM configured to simulate peer reviewers and produce NeurIPS-style review outputs (strengths/weaknesses, numeric scores across criteria such as Originality, Quality, Clarity, Significance, Soundness, Presentation, Contribution, and overall decision).",
            "model_size": null,
            "input_corpus_description": "Individual generated paper drafts (LaTeX content) and the associated research plan/outputs; for calibration in prior work the system was evaluated on 500 ICLR 2022 papers from OpenReview.",
            "input_corpus_size": 500,
            "topic_query_description": "A generated manuscript (from paper-solver) presented for evaluation following NeurIPS review guidelines.",
            "distillation_method": "Not a distillation per se; the system ingests a candidate manuscript and outputs a structured review and scalar scores following NeurIPS review criteria. It was calibrated/evaluated on a corpus of prior conference submissions to align reviewer behavior.",
            "output_type": "Structured peer review: textual strengths/weaknesses, a list of reviewer questions, limitations, and numeric scores per NeurIPS criteria plus an overall recommendation/decision.",
            "output_example": "\"Strengths\": [...], \"Weaknesses\": [...], \"Originality\": 3, \"Quality\": 4, \"Clarity\": 3, \"Significance\": 3, \"Overall\": 7, \"Decision\": \"Accept\" (example provided from an o1-mini automated review in the paper).",
            "evaluation_method": "Calibrated/validated against human OpenReview/ICLR reviews: compared automated reviewer outputs versus ground-truth OpenReview reviewer outcomes on 500 ICLR 2022 papers and measured accuracy and F1.",
            "evaluation_results": "On the 500 ICLR 2022 papers, the automated reviewer achieved human-level accuracy (65% vs human 66%) and higher F1 (0.57 vs human 0.49) after calibration. However, when applied to Agent Laboratory generated papers, automated reviewer overall scores (avg 6.1/10) substantially overestimated human reviewer scores (avg 3.8/10), indicating misalignment in this domain.",
            "strengths": "Enables automated, repeatable NeurIPS-style critique for iterative refinement; demonstrated calibration success on a 500-paper ICLR dataset in prior work; produces structured feedback useful for guiding edits.",
            "limitations": "Tends to over-estimate quality on machine-generated papers in this study (systematic discrepancy vs human reviewers); reliance on surface cues may produce optimistic assessments not matched by human judgments; calibration on conference submissions does not fully generalize to AI-generated manuscripts.",
            "failure_cases": "Significant divergence between automated reviewer scores and human reviewer scores for Agent Laboratory outputs (automated reviewers ~+2.3 points higher on overall NeurIPS score), leading to misleading self-assessment if used without human checks.",
            "uuid": "e9851.2",
            "source_info": {
                "paper_title": "Agent Laboratory: Using LLM Agents as Research Assistants",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ResearchAgent (related work)",
            "name_full": "ResearchAgent (Baek et al. 2024) - related LLM-agent research system",
            "brief_description": "A cited prior system where LLM agents automatically generate research ideas, experimental methods, and designs and iteratively refine them using multiple reviewing agents aligned with human evaluation criteria.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ResearchAgent (as described in Baek et al. (2024)); specific model details not provided in this paper.",
            "model_description": "An agent-based research automation system that uses LLMs to propose research ideas and plans and refines them via simulated reviewer feedback; details are provided in the cited work rather than in this paper.",
            "model_size": null,
            "input_corpus_description": "Not specified in this paper; described generally as performing automated research ideation and experiment design.",
            "input_corpus_size": null,
            "topic_query_description": "Automated generation of novel research ideas and experiment designs (no single human-provided query required in that system, per description here).",
            "distillation_method": "Iterative idea generation and refinement using multiple reviewing agents that simulate peer discussion and leverage human-aligned evaluation criteria to improve outputs (per description in related work).",
            "output_type": "Generated research ideas, experimental methods, and experiment designs; iterative improvements to proposals.",
            "output_example": null,
            "evaluation_method": "Mentioned as using reviewing agents and iterative refinement; the present paper cites ResearchAgent demonstrating idea novelty but also notes limitations in feasibility/implementation (Si et al. (2024)).",
            "evaluation_results": "Related work reports that LLMs can generate ideas judged more novel than humans (cited), but separate work (Si et al. (2024)) indicates weaknesses in feasibility and implementation details, suggesting complementarity with humans.",
            "strengths": "Demonstrated capability to produce novel research ideas and iterative refinement via simulated reviewers.",
            "limitations": "Reported weaknesses in feasibility and implementation detail (per Si et al. (2024)); not necessarily reliable as a sole replacement for human researchers.",
            "failure_cases": "Feasibility and implementation gaps identified in follow-up analyses (no concrete failure examples detailed in this paper).",
            "uuid": "e9851.3",
            "source_info": {
                "paper_title": "Agent Laboratory: Using LLM Agents as Research Assistants",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "mle-solver (Kaggle distillation)",
            "name_full": "mle-solver (Agent Laboratory experimentation module using distilled Kaggle knowledge)",
            "brief_description": "A specialized module in Agent Laboratory for autonomously generating, testing, and refining machine learning code; it ingests Kaggle dataset descriptions and distilled knowledge from Kaggle notebooks to construct solutions for MLE-Bench challenges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "mle-solver orchestrated with Agent Laboratory LLM backends for code generation and an LLM reward model for program scoring (when configured); in MLE-Bench evaluation, dev-set scoring replaced the LLM reward model.",
            "model_description": "An iterative program synthesis and evaluation loop: initialize a program, perform EDIT and REPLACE operations to generate new code files or modify line ranges, compile and attempt repairs up to N_rep tries, score programs (via an LLM reward model or dev-set evaluation), self-reflect on failures/successes, and maintain a top-program pool with batch-parallel exploration.",
            "model_size": null,
            "input_corpus_description": "Kaggle dataset descriptions, distilled content from Kaggle notebooks, and accessible train/dev splits provided as in-memory arrays for the solver to use when preparing data and training models.",
            "input_corpus_size": null,
            "topic_query_description": "Specific Kaggle competition/tasks from a subset of MLE-Bench (text and tabular low-complexity challenges); the solver receives dataset metadata and distilled notebook knowledge to guide modeling choices.",
            "distillation_method": "Distillation of practitioner knowledge from Kaggle notebooks: aggregated distilled knowledge (notebooks) plus dataset descriptions are provided as context; mle-solver synthesizes code via iterative program edits (EDIT/REPLACE), compiles and tests code, uses an evaluation function (LLM-based reward or dev-set scoring) to rank programs, and applies self-reflection to guide subsequent edits.",
            "output_type": "Executable machine-learning experiment code (Python scripts/notebooks) implementing data preparation, model training, and evaluation for Kaggle tasks; final high-scoring program submitted for benchmark scoring.",
            "output_example": "Final code solutions that achieved medal rankings on MLE-Bench tasks (specific code not reproduced in paper).",
            "evaluation_method": "Benchmarked on a subset of 10 MLE-Bench Kaggle challenges: mle-solver runs produce submissions evaluated with Kaggle-style scoring and medal assignments; comparisons made against other solver systems (MLAB, OpenHands, AIDE).",
            "evaluation_results": "mle-solver obtained four medals across the evaluated subset (two gold, one silver, one bronze) and exceeded median human performance on 6/10 benchmarks. It outperformed MLAB and OpenHands on consistency and medal counts in the reported subset.",
            "strengths": "Consistent high-scoring solutions on evaluated Kaggle tasks; iterative generate-compile-test-repair loop with program-level scoring and self-reflection; maintains a top-program pool for stability and uses batch-parallel exploration to balance exploration/exploitation.",
            "limitations": "Observed tendencies include over-editing at top-of-file (line 0), generating Python exit() calls that prematurely terminate processes, occasional 0% accuracy solutions that remain uncorrected before solver exhaustion, and potential unsafe execution of subprocess.run() commands. The LLM reward-based scoring was sometimes replaced by dev-set scoring for objective benchmarking.",
            "failure_cases": "Common failure modes listed in the paper include: mle-solver producing 0% accuracy for all tested methods on some runs; frequent edits concentrated at line 0; generation of exit() commands requiring manual interception; generated system commands (subprocess.run()) that could be risky; printed output causing LLM token-limit issues.",
            "uuid": "e9851.4",
            "source_info": {
                "paper_title": "Agent Laboratory: Using LLM Agents as Research Assistants",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ResearchAgent",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist",
            "rating": 2
        },
        {
            "paper_title": "MLE-Bench",
            "rating": 2
        },
        {
            "paper_title": "AIDE",
            "rating": 2
        },
        {
            "paper_title": "OpenHands",
            "rating": 1
        },
        {
            "paper_title": "MLAgentBench",
            "rating": 1
        }
    ],
    "cost": 0.0184015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Agent Laboratory: Using LLM Agents as Research Assistants</h1>
<p>Samuel Schmidgall ${ }^{1,2}$, Yusheng $\mathrm{Su}^{1}$, Ze Wang ${ }^{1}$, Ximeng Sun ${ }^{1}$, Jialian Wu ${ }^{1}$, Xiaodong Yu ${ }^{1}$, Jiang Liu ${ }^{1}$, Michael Moor $^{3}$, Zicheng Liu ${ }^{1}$ and Emad Barsoum ${ }^{1}$<br>${ }^{1}$ AMD, ${ }^{2}$ Johns Hopkins University, ${ }^{3}$ ETH Zurich</p>
<h4>Abstract</h4>
<p>Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages-literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an $84 \%$ decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.</p>
<p>(1) https://AgentLaboratory.github.io
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Agent Laboratory takes as input a human research idea and a set of notes, provides this to a pipeline of specialized LLM-driven agents, and produces a research report and code repository.</p>
<h1>1. Introduction</h1>
<p>Scientists frequently face constraints that limit the number of research ideas they can explore at any given time, resulting in ideas being prioritized based on predicted impact. While this process helps determine which concepts are worth investing time in and how best to allocate limited resources effectively, many high quality ideas remain unexplored. If the process of exploring ideas had less limitations, researchers would be able to investigate multiple concepts simultaneously, increasing the likelihood of scientific discovery.</p>
<p>In an effort to achieve this, recent work has explored the capability of LLMs to perform research ideation and automated paper generation, where LLM agents perform the role of human scientists (Baek et al. (2024); Ghafarollahi \&amp; Buehler (2024b); Lu et al. (2024a); Swanson et al. (2024)). The work of Baek et al. (2024) introduces ResearchAgent, which automatically generates research ideas, methods, and experiment designs, iteratively refining them through feedback from multiple reviewing agents that mirror peer discussions and leverage human-aligned evaluation criteria to improve the outputs. Lu et al. (2024a) explores fully automated paper generation, where The AI Scientist framework generates novel research ideas, writes code, conducts experiments, and creates a full scientific paper with an automated peer-review system to evaluate the work. Even though these works demonstrate that current LLMs can generate ideas judged to be more novel than those produced by human experts, Si et al. (2024) indicates that LLMs still exhibit weaknesses in feasibility and implementation details, suggesting a complementary rather than replacement role for LLMs in research. Therefore, we aim to design an autonomous agent pipeline that can assist humans toward implementing their own research ideas.</p>
<p>In this work, we introduce Agent Laboratory, an autonomous pipeline for accelerating the individual's ability to perform machine learning research. Unlike previous approaches, where agents participate in their own research ideation independent of human input (Baek et al. (2024); Lu et al. (2024b)), Agent Laboratory is designed to assist human scientists in executing their own research ideas using language agents. Agent Laboratory takes as input a human research idea and outputs a research report and code repository produced by autonomous language agents, allowing various levels of human involvement, where feedback can be provided at a frequency based on user preference. A detailed list of our contributions are provided below:</p>
<ol>
<li>We introduce Agent Laboratory, an open-source LLM agent framework for accelerating the individual's ability to perform research in machine learning. In order to accommodate all users, Agent Laboratory is compute flexible, where various levels of compute can be allocated based on the individual's access to compute resource (e.g., CPU, GPU, memory) and model inference budget.</li>
<li>Human evaluators rated papers generated using Agent Laboratory across experimental quality, report quality, and usefulness, showing that while the o1-preview backend was perceived as the most useful, o1-mini achieved the highest experimental quality scores, and gpt-40 was behind in all metrics.</li>
<li>NeurIPS-style evaluations showed that o1-preview performed best among backends, particularly in clarity and soundness, according to human reviewers. However, a clear gap emerged between human and automated evaluations, with automated scores significantly overestimating quality (6.1/10 vs. 3.8/10 overall). Similar discrepancies were seen across clarity and contribution metrics, suggesting the need for human feedback to complement automated evaluations for more accurate assessments of research quality.</li>
<li>Co-pilot mode in Agent Laboratory was evaluated on custom and preselected topics, showing higher overall scores compared to autonomous mode. Co-pilot papers also saw trade-offs</li>
</ol>
<p>in experimental quality and usefulness, reflecting challenges in aligning agent outputs with researcher intent.
5. The co-pilot feature in Agent Laboratory is overall found to have high utility and usability when rated by human users, with most participants deciding to continue usage after their experience
6. Detailed cost and inference time statistics, as well as the breakdown of cost per paper phase, are presented for different model back-ends, demonstrating that Agent Laboratory offers automatic research at a greatly reduced price compared with other works (only $\$ 2.33$ USD per paper with a gpt-4o backend).
7. State-of-the-art performance on a subset of MLE-Bench challenges using the proposed mle-solver, achieving higher consistency and scoring compared to other solvers, and earning more medals, including gold and silver, than MLAB, OpenHands, and AIDE.</p>
<p>We hope that this work takes a step toward accelerating scientific discovery in machine learning, allowing researchers to allocate more effort toward creative ideation and experiment design rather than low-level coding and writing.</p>
<h1>2. Background \&amp; Related Work</h1>
<p>Large language models The research agents in this paper are built on autoregressive large language models (LLMs), which are trained on extensive text corpora to predict conditional probabilities of token sequences, $p\left(x_{t} \mid x_{&lt;t} ; \theta\right)$, and generate text completions through sampling, where $x_{t} \sim \operatorname{softmax}\left(W \cdot h_{t}\right)$, with $h_{t}$ as the hidden state and $W$ as the learned weight matrix mapping to token probabilities. LLMs utilize transformer architectures (Vaswani (2017)) to capture long-range dependencies in text. These models, such as Claude (Anthropic (2024)), Llama (Dubey et al. (2024); Touvron et al. (2023a,b)), and ChatGPT (Achiam et al. (2023); Hurst et al. (2024); OpenAI (2022)), leverage vast datasets and scaling techniques, thus enabling them to perform a wide array of language-based tasks, such as translation, summarization, and reasoning, by generalizing patterns learned during pretraining to novel inputs Brown (2020).</p>
<p>LLM Agents While LLMs demonstrate strong understanding and reasoning abilities, they face challenges when executing tasks in real-world scenarios. To overcome these limitations, their capabilities are extended through structured frameworks, enabling them to autonomously and semi-autonomously perform task execution and semi-autonomously perform task execution (Chen et al. (2023b); Li et al. (2023); Qian et al. (2024); Wu et al. (2023)). These systems, referred to as agents, utilize techniques such as chain-of-thought prompting (Wei et al. (2022)), iterative refinement (Shinn et al. (2024)), self-improvement (Huang et al. (2022)), and external tool integration to execute complex workflows (Hao et al. (2024); Qin et al. (2023); Schick et al. (2023)). LLM agents have made remarkable progress in solving tasks of real-world significance, such as software engineering (Jimenez et al. (2023); Wang et al. (2024b); Yang et al. (2024)), cybersecurity (Abramovich et al. (2024); Fang et al. (2024); Wan et al. (2024)), and medical diagnosis (McDuff et al. (2023); Schmidgall et al. (2024); Tu et al. (2024)). There has also been progress in applying LLMs agents to embodied problems such as autonomous robotics (Black et al. (2024); Brohan et al. (2022, 2023); Kim et al. (2024)), web tasks (Deng et al. (2024); Gur et al. (2023); He et al. (2024); Putta et al. (2024); Shi et al. (2017)), and game playing (AL et al. (2024); Feng et al. (2024); Wang et al. (2023)). For a broader overview of LLM agents, refer to Wang et al. (2024a).</p>
<p>Automated machine learning Automated machine learning is an area of active research, with many approaches focused on using Kaggle, an online platform for machine learning competitions, as a benchmark for evaluating agent performance. Notable efforts include MLE-Bench (Chan et al. (2024)), DS-bench (Jing et al. (2024)), and MLAgentBench (Huang et al. (2024)) which propose using 75, 74, and 6 Kaggle challenges respectively as benchmarks to measure the abilities of ML agents in tasks such as data preparation, model development, and submission. Several ML "solvers" which can solve ML challenges have been introduced, such as AIDE (Schmidt et al. (2024)), CodeActAgent (referred to as "OpenHands") (Wang et al. (2024b)), and ResearchAgent (referred to as "MLAB") from MLAgentBench (Huang et al. (2024)) which automate feature implementation, bug fixing, and code refactoring with a high success rate. Agent K (Grosnit et al. (2024)) demonstrates the ability to solve Kaggle challenges at the human-level with a challenge URL provided as input.</p>
<p>AI in Scientific Discovery AI has been used to support scientific discovery across numerous disciplines for decades. For instance, AI has been used for discovery in mathematics (Romera-Paredes et al. (2024)), material science (Merchant et al. (2023); Pyzer-Knapp et al. (2022); Szymanski et al. (2023)), chemistry (Hayes et al. (2024); Jumper et al. (2021)), algorithm discovery (Fawzi et al. (2022)), and computational biology (Ding et al. (2024)). These approaches position AI as a tool rather than an agent performing research in autonomous research.</p>
<p>LLMs for research related tasks LLMs have demonstrated strong capabilities in diverse researchrelated tasks, such as code generation (Chen et al. (2021); Nijkamp et al. (2022)), end-to-end software development (Hai et al. (2024); Phan et al. (2024); Qian et al. (2023, 2024)), code generation for discovery (Chen et al. (2024b); Ghafarollahi \&amp; Buehler (2024a); Gu et al. (2024); Guo et al. (2024); Hu et al. (2024b); Ifargan et al. (2024); Majumder et al. (2024)), research question-answering (Chen et al. (2024a); Lála et al. (2023); Lin et al. (2024); Song et al. (2024)), research ideation (Baek et al. (2024); Ghafarollahi \&amp; Buehler (2024b); Li et al. (2024a); Si et al. (2024)), automated paper reviewing (D'Arcy et al. (2024); Liang et al. (2024); Lu et al. (2024b); Weng et al. (2024)), literature search (Ajith et al. (2024); Kang \&amp; Xiong (2024); Li et al. (2024b); Press et al. (2024)), and predicting the outcome of experiments (Ashokkumar et al. (2024); Lehr et al. (2024); Luo et al. (2024); Manning et al. (2024); Zhang et al. (2024b)). Although LLMs have made notable progress in solving the aforementioned tasks, ideation has struggled to progress, with some work showing that LLM ideation leads to greater novelty than humans (Si et al. (2024)), while others show reduced creativity (Chakrabarty et al. (2024)) and greater homogeneous effects (Anderson et al. (2024); Zhou et al. (2024)) that may limit creative discovery without human guidance.</p>
<p>Additionally, research on human-AI collaboration has reached mixed conclusions about the idea novelty (Ashkinaze et al. (2024); Liu et al. (2024); Padmakumar \&amp; He (2024)). These findings suggest that, with the current LLMs, the strongest research systems would combine human-guided ideation with LLM-based workflows.</p>
<p>LLMs for autonomous research Recent advancements in automated scientific workflows have focused on leveraging LLMs to emulate the process of research. Swanson et al. (2024) introduces a team of LLM agents working as scientists alongside a human researcher who provides high-level feedback, with the end result being novel nanobody binders aimed at addressing recent variants of SARS-CoV-2. ChemCrow (M. Bran et al. (2024)) and Coscientist (Boiko et al. (2023)) demonstrate the ability for autonomous ideation and experimentation in chemistry. ResearchAgent (Baek et al. (2024)) automates research idea generation, experiment design, and iterative refinement using feedback from reviewing agents aligned with human evaluation criterion. The AI Scientist (Lu et al. (2024a)) extends</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Agent Laboratory Workflow. This image illustrates the three primary phases of Agent Laboratory: Literature Review, Experimentation, and Report Writing, each featuring distinct tasks, tools, and human-agent roles. The pipeline integrates human input with LLM-driven agents, such as the PhD and Postdoc agents, which handle literature reviews, experimental planning, data preparation, and result interpretation. Specialized tools like mle-solver for experimentation and paper-solver for report generation automate tedious research tasks, enabling collaboration between human researchers and AI to produce high-quality research outputs.
this automation to encompass end-to-end scientific discovery, including coding, experiment execution, and automated peer review for manuscript generation. Despite these advancements, studies like Si et al. (2024) highlight limitations in the feasibility and implementation details of LLM ideation, indicating a complementary rather than replacement role for LLMs in autonomous research.</p>
<h1>3. Agent Laboratory</h1>
<p>Overview. Agent Laboratory begins with the independent collection and analysis of relevant research papers, progresses through collaborative planning and data preparation, and results in automated experimentation and comprehensive report generation. As shown in Figure 2, the overall workflow consists of three primary phases: (1) Literature Review, (2) Experimentation, and (3) Report Writing. In this section, we will introduce these phases in detail along with the corresponding involved agents. Furthermore, in Section 4, we will conduct qualitative and quantitative analyses to demonstrate the strengths of Agent Laboratory and its ability to generate research.</p>
<h3>3.1. Literature Review</h3>
<p>Literature Review. The literature review phase involves gathering and curating relevant research papers for the given research idea to provide references for subsequent stages. During this process, the PhD agent utilizes the arXiv API to retrieve related papers and performs three main actions: summary, full text, and add paper. The summary action retrieves abstracts of the top 20 papers relevant to the initial query produced by the agent. The full text action extracts the complete content of specific papers, and the add paper action incorporates selected summaries or full texts into the curated review. This process is iterative rather than a single-step operation, as the agent performs multiple queries, evaluates the relevance of each paper based on its content, and refines the</p>
<p>selection to build a comprehensive review. Once the specified number of relevant texts $(\mathrm{N}=\max )$ is reached via the add paper command, the curated review is finalized for use in subsequent phases.</p>
<h1>3.2. Experimentation</h1>
<p>Plan Formulation The plan formulation phase focuses on creating a detailed, actionable research plan based on the literature review and research goal. During this phase, the PhD and Postdoc agents collaborate through dialogue to specify how to achieve the research objective, detailing experimental components needed to complete the specified research idea such as which machine learning models to implement, which datasets to use, and the high-level steps of the experiment. Once a consensus is reached, the Postdoc agent submits this plan using the plan command, which serves as a set of instructions for subsequent subtasks.</p>
<p>Data Preparation. The goal of the data preparation phase is to write code that prepares data for running experiments, using the instructions from the plan formulation stage as a guideline. The ML Engineer agent executes code using Python command command and observes any printed output. The ML Engineer has access to HuggingFace datasets, searchable via the search HF command. After agreeing on the finalized data preparation code, the SW Engineer agent submits it using the submit code command. Before the final submission proceeds, the code is first passed through a Python compiler to ensure that there are no compilation issues. This process will be iteratively executed until the code is bug-free.</p>
<p>Running Experiments. In the running experiments phase, the ML Engineer agent focuses on implementing and executing the experimental plan formulated prior. This is facilitated by mle-solver, a specialized module designed to generate, test, and refine machine learning code autonomously. mle-solver begins by producing initial code based on the research plan and insights from the literature review. For the first mle-solver step, the program is empty and must generate a file from scratch, which is used as the top scoring program. The following processes describe the workflow of the mle-solver:
A. Command Execution. During the command execution phase, an initial program is sampled from a maintained set of top-performing programs, which is represented by a single file during initialization. The mle-solver iteratively refines this program through two operations, REPLACE and EDIT, to better align the output with experimental objectives. The EDIT operation identifies a range of lines, substituting the code between the specified line numbers with newly generated code. In contrast, the REPLACE operation generates a completely new Python file.
B. Code Execution. After a code command is executed, the new program is passed through a compiler to check for runtime errors. If it successfully compiles, a score is returned and the list of top programs is updated if the score is higher than the existing programs. If the code does not compile, the agent attempts to repair the code for $N_{\text {rep }}$ tries ( $N_{\text {rep }}=3$ in our experiments) before returning an error and moving on to a new code replacement.
C. Program Scoring. If a code succeeds in compilation, it is sent to a scoring function which determines if it is better than previously implemented experiment code. In order to obtain a program score, we implement a scoring function that uses an LLM reward model to assess the effectiveness of the ML code generated by mle-solver. The reward model, invoked as an LM, scores the program on a scale from 0 to 1 considering the outlined research plan, the produced code, and the observed output to determine how accurately the program adheres to</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Overview of the mle-solver workflow. This diagram details the iterative process used by the MLE-Solver to autonomously generate machine learning code. Beginning with external resources, the workflow integrates command execution (A), where new code is generated, followed by code execution (B) to compile and repair issues if needed. Program scoring (C) evaluates the generated code using a reward function, while self-reflection (D) helps refine future iterations based on results. Performance stabilization (E) ensures consistent outcomes by maintaining a pool of top-performing programs and iterative optimization.
the initial goals. A score of 1 is provided for results with high alignment and everything below on a spectrum of how closely the output and code matches the planning goals. This process is similar to existing methods for LLM reasoning tree search (Yao et al. (2024)), where instead of a series of reasoning steps being traversed using self-evaluated LLM scoring, the set of possible programs are being traversed (via EDIT and REPLACE commands) and the resulting program outcome is self-evaluated to determine if a program is worth building on. This is similar to the Solution Space Search of AIDE (Schmidt et al. (2024)), however their method was specifically designed for the Kaggle competitions and is simply extracting the accuracy rather than scoring the research code and outcomes.
D. Self Reflection. Whether the code succeeds or fails, a self-reflection is produced based on the experimental results or the encountered error signal (Renze \&amp; Guven (2024); Shinn et al. (2024)). Here, the mle-solver is prompted to reflect on the outcome of its actions. If the program failed to compile, the solver reflects on how to fix this issue in next iterations. If it successfuly compiles and returns a score, the solver will reflect on how to increase this score. These reflections are generated to improve future performance, ensuring that the system learns from errors, improving the quality and robustness of the generated code over iterative cycles.
E. Performance Stabilization To prevent performance drift, two mechanisms are implemented: top program sampling and batch-parallelization. In top program sampling, a collection of the highest-scoring programs is maintained, and one program is randomly sampled before executing a command, ensuring diversity while retaining quality. For batch-parallelization, each solver step involves making N modifications simultaneously, with the top modification selected to replace the lowest-scoring program in the top collection. These strategies use high-entropy sampling to modify the code, resulting in a balance between exploration of new solutions and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure $4 \mid$ Graphical outline of paper-solver. This diagram showcases the step-by-step process of generating and refining academic research reports using the Paper-Solver tool. The workflow starts with the creation of an initial report scaffold (A) by iteratively generating LaTeX-based sections, followed by updates to ensure structural completeness. (B) Research is performed through an Arxiv tool during relevant sections. In the Report Editing phase (C), the language model applies targeted edits to improve the document, with LaTeX compilation verifying the integrity of changes. Finally, the completed report undergoes a reward-based evaluation during the Paper Review phase (D), ensuring alignment with academic standards and research goals.
refinement of existing ones in order to maintain stable code modifications.</p>
<p>Results Interpretation. The goal of the results interpretation phase is to derive meaningful insights from experimental outcomes to inform the final report. The PhD and Postdoc agents discuss their understanding of the experimental results produced by mle-solver. Once they agree on a meaningful interpretation that could contribute to a compelling academic paper, the Postdoc agent submits it using the interpretation command, forming the basis for the report writing phase.</p>
<h1>3.3. Report Writing</h1>
<p>Report Writing. In the report writing phase, the PhD and Professor agent synthesize the research findings into a comprehensive academic report. This process is facilitated by a specialized module called paper-solver, which iteratively generates and refines the report. The paper-solver aims to act as a report generator, positioning the work that has been produced by previous stages of Agent Laboratory. paper-solver does not aim to entirely replace the academic paper-writing process, but rather to summarize the research that has been produced in a human-readable format so that the researcher using Agent Laboratory understands what has been accomplished. The output follows the standard structure of an academic paper, ensuring it meets conference submission requirements (for the paper scoring phase) while being clear and methodical. The following processes describe the workflow of paper-solver:
A. Initial Report Scaffold. The first task of the paper-solver is to generate an initial scaffold for the research paper. This scaffold outlines the document structure, dividing it into eight standardized sections: Abstract, Introduction, Background, Related Work, Methods, Experimental Setup, Results, and Discussion. During scaffold creation, placeholders are inserted for each section to categorize future content. This process establishes the framework for subsequent detailed text generation. The scaffold includes necessary formatting for LaTeX compilation, allowing the generated paper to be directly reviewed and refined. Special care is taken to ensure the scaffold aligns with academic conventions, such as appropriate section titles and placeholders that guide content development.</p>
<p>B. Arxiv Research. During the scaffold building phase, we allow the paper-solver access to arXiv which is accessible through the same interface as the earlier literature review phase. ArXiv is enabled to allow the solver to explore related literature on the subject it is writing on as well as finding papers to refer to, although it is not enforced. We note that the agent still has access to the original literature search, but has the opportunity to expand based on literature needed to write a particular paper section.
C. Report Editing. One the scaffold is built, the paper-solver uses specialized commands to iteratively refine the generated paper. The primary command are available for this stage is the EDIT command, which allows precise line-by-line modifications to the LaTeX code. This command enable dynamic adjustments to the content, ensuring alignment with the research plan, the clarity of arguments, and compliance with formatting standards. Before integrating edits, the system compiles the LaTeX to verify error-free functionality, thereby maintaining document integrity. Through iterative editing, the solver ensures the paper achieves the desired level of quality, cohesiveness, and depth required for academic acceptance.
D. Paper Review. For obtaining scores for papers during the paper-solver iterations, we leverage an adapted version of the automated review system developed in Lu et al. (2024b). This system works by using an LLM-based agent to simulate the scientific paper review process following the NeurIPS conference guidelines. When evaluated on 500 ICLR 2022 papers from the OpenReview dataset, the automated reviewer achieved human-level accuracy ( $65 \%$ compared to $66 \%$ for human reviewers) and surpassed human performance in F1 score ( 0.57 vs. 0.49 ) after calibration. An example review from one of our papers by o1-mini is provided below.</p>
<h1>Example Review ( o1-mini | Word Order Sensitivity )</h1>
<p>"Strengths": [
"Comprehensive experimental design and methodology.",
"Use of a well-known dataset (RACE) for evaluation.",
"Empirical validation of bias mitigation strategies.",
"Clear presentation of results and analysis."],
Weaknesses": [
"Limited exploration of additional bias mitigation techniques.",
"Lack of in-depth discussion on limitations
and societal impacts.",
"The originality could be enhanced by exploring novel
strategies."],
"Originality": 3, "Quality": 4, "Clarity": 3, "Significance": 3,
"Questions": [
"Have you considered exploring additional bias
mitigation techniques beyond majority voting and entropy-based
thresholding?",
"Can you provide more details on the potential societal impacts
of the model's sensitivity to option order?",
"What are the limitations of the current study, and how
might they be addressed in future work?"],
"Limitations": [
"The study is limited to the RACE dataset and may not generalize
to other datasets.",
"The bias mitigation strategies, while effective,
do not completely eliminate sensitivity to option order."],</p>
<div class="codehilite"><pre><span></span><code>&quot;Ethical Concerns&quot;: false,
&quot;Soundness&quot;: 3, &quot;Presentation&quot;: 3, &quot;Contribution&quot;: 3,
&quot;Overall&quot;: 7, &quot;Confidence&quot;: 4,
&quot;Decision&quot;: &quot;Accept&quot;
</code></pre></div>

<p>Paper Refinement. In the paper refinement phase, the PhD agent makes a decision on whether to make paper revisions or to determine that the paper is complete. The process begins with a set of three reviewer agents generating reviews that mimic feedback from NeurIPS peer reviewers, evaluating the report based on criteria such as originality, quality, clarity, and significance. Based on these scores, the PhD agent then decides whether to finalize the project or revisit earlier subtasks-such as planning, experimentation, or results interpretation-to address the feedback. This allows the agents to refine the research report until it meets sufficiently high standards, effectively simulating the real-world academic revision process.</p>
<h1>3.3.1. Autonomous versus Co-Pilot Mode:</h1>
<p>There are two ways in which Agent Laboratory can be operated: autonomous and co-pilot modes. In autonomous mode, there is no human involvement other than providing the initial research idea for agents to produce research for. Each subtask moves on to the next subtask sequentially upon completion. In co-pilot mode, in addition to providing the research idea, there is also a checkpoint at the end of each subtask, where a human is involved in reviewing the work produced by agents in that phase (e.g., the literature review summary or generated report). The human reviewer can either decide to proceed to the next subtask, or ask the agent to repeat the subtask while providing high level notes for the agent to improve its performance during the next attempt. For example, if the literature review phase did not include a specific paper or the experiments did not include a desired technique, the human reviewer would instruct the agent to include this.</p>
<h2>4. Results</h2>
<p>In this section, we present our main findings on the efficacy of Agent Laboratory to produce research. We begin our results by asking how human evaluators perceive papers generated by Agent Laboratory running in end-to-end autonomous mode across five topics. Next, we examine human evaluation when using Agent Laboratory in collaborative co-pilot mode from both allowing the researcher to choose any topic they want and from our set of preselected topics. We then provide a detailed runtime analysis including cost, average time, and success rate by various models. Finally, we conclude with an evaluation of the mle-solver in isolation on MLE-Bench, a set of real-world Kaggle challenges. The details of all surveys are provided in Appendix C.</p>
<h3>4.1. Evaluation of quality by language model</h3>
<p>Our first experiment aims to evaluate how human-evaluated quality varies across three axes: experiment quality, report quality, and usefulness. This evaluation was conducted by human participants using three different LLM backends: gpt-4o (Hurst et al. (2024)), o1-mini, and o1-preview (OpenAI (2024)). Research questions were selected from a set of 5 templates:</p>
<ol>
<li>Do language models exhibit cognitive biases, such as confirmation bias or anchoring bias?</li>
<li>Are image transformers more or less sensitive to pixel noise than convolutional networks?</li>
</ol>
<p>Average human evaluated score by Agent Laboratory base LLM</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">gpt-40</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">o1-mini</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">o1-preview</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Research Question</td>
<td style="text-align: center;">Research <br> Type</td>
<td style="text-align: center;">Experiment <br> Quality</td>
<td style="text-align: center;">Report <br> Quality</td>
<td style="text-align: center;">Usefulness</td>
<td style="text-align: center;">Experiment <br> Quality</td>
<td style="text-align: center;">Report <br> Quality</td>
<td style="text-align: center;">Usefulness</td>
<td style="text-align: center;">Experiment <br> Quality</td>
<td style="text-align: center;">Report <br> Quality</td>
<td style="text-align: center;">Usefulness</td>
</tr>
<tr>
<td style="text-align: center;">Are image transformers more or less sensitive to noise than convolutional networks?</td>
<td style="text-align: center;">Computer <br> Vision</td>
<td style="text-align: center;">$1.5 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
</tr>
<tr>
<td style="text-align: center;">Does gender affect the accuracy on of language models on answering gamlik questions?</td>
<td style="text-align: center;">NLP <br> [Social Sci]</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$5.0 / 5$</td>
</tr>
<tr>
<td style="text-align: center;">Do language models improve accuracy on MedQA when asked to perform differential diagnosis?</td>
<td style="text-align: center;">NLP <br> [Medical]</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
</tr>
<tr>
<td style="text-align: center;">Do language models exhibit cognitive biases similar to humans, such as anchoring bias?</td>
<td style="text-align: center;">NLP <br> [Cog Sci]</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$2.0 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
</tr>
<tr>
<td style="text-align: center;">Are language models sensitive to word order in multiple choice benchmarks?</td>
<td style="text-align: center;">NLP <br> [Core]</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
<td style="text-align: center;">$4.5 / 5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$2.6 / 5$</td>
<td style="text-align: center;">$3.0 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
<td style="text-align: center;">$3.2 / 5$</td>
<td style="text-align: center;">$3.2 / 5$</td>
<td style="text-align: center;">$4.3 / 5$</td>
<td style="text-align: center;">$2.9 / 5$</td>
<td style="text-align: center;">$3.4 / 5$</td>
<td style="text-align: center;">$4.4 / 5$</td>
</tr>
</tbody>
</table>
<p>Figure 5 | The average human evaluated scores of papers generated by Agent Laboratory in an autonomous mode based on a research question (left column) and LLM backend (top row). The bottom row shows the average score across all topics by LLM backend.
3. Do language models improve accuracy on MedQA when asked to perform differential diagnosis?
4. Are language models sensitive to word order in multiple choice benchmarks?
5. Does gender role play affect the accuracy on of language models on answering math questions?</p>
<p>These 5 questions across 3 LLM backends resulted in a total of 15 papers being written autonomously by Agent Laboratory without any human involvement. We then recruited 10 volunteer PhD students to review 3 randomly assigned papers each. These researchers rated the experimental quality, report quality, and usefulness of the generated outputs on a scale of 1 to 5 . The goal of this evaluation is to understand the differences in quality of produced research based on the three distinct LLM backbones, and to understand the usefulness of Agent Laboratory in autonomous mode. The details of the evaluation questions are provided here:</p>
<ul>
<li>Experimental Quality: What is your perception of the quality of the experimental results presented in this report?</li>
<li>Report Quality: What is your perception of the quality of the research report writing quality presented in this report?</li>
<li>Usefulness: What is your perception of the usefulness of an AI assistant tool that can generate the presented report autonomously?</li>
</ul>
<p>The results of this evaluation indicate variability in performance across different Agent Laboratory LLM backends (Figure 5). gpt-40 consistently achieved lower scores, with an average experimental quality rating of $2.6 / 5$, a report quality rating of $3.0 / 5$, and a usefulness rating of $4.0 / 5$. In contrast, o1-mini generally outperformed gpt-40 in experimental quality, with an average score of 3.2/5 (+0.6), while maintaining similar levels of report quality and usefulness at $3.2 / 5(+0.2)$ and $4.3 / 5(+0.3)$, respectively. o1-preview demonstrated the highest usefulness and report quality, averaging $4.4 / 5$ ( +0.4 from gpt-40 and +0.1 from o1-mini) and $3.4 / 5$ ( +0.4 from gpt-40 and +0.2 from o1-mini) respectively, though its experimental ratings were slightly lower than o1-mini at $2.9 / 5(+0.3$ from gpt-40 and -0.3 from o1-mini). While all backends perform comparably in terms of report and experimental quality, the o1-preview model was as the most useful for research assistance, suggesting that its outputs were better aligned with the expectations and needs of researchers.</p>
<p>From our results, the quality is demonstrated to vary based on the selected topic. We find that the overall highest average report quality to be $3.8 / 5$ and usefulness to be $4.5 / 5$ for the word order topic and the highest average experiment quality to be $3.2 / 5$ for the cognitive bias topic. Interestingly, we also find that word order has the lowest experiment quality at 2.7/5 along with the image noise topic. The image noise topic was demonstrated to have high variance based on the LLM backend, with an experiment quality score of $1.5 / 5$ for gpt-4o and a $4.0 / 5$ with o1-mini ( +2.5 point difference) and a usefulness score of $2.5 / 5$ for gpt-4o and a $4.5 / 5$ with o1-mini ( +2.0 point difference).</p>
<p>In summary, the evaluation of quality across LLM backends demonstrates clear differences in experimental quality, report quality, and usefulness. While o1-preview is consistently rated as the most useful for research assistance, o1-mini achieves the highest experimental quality scores, and gpt-4o is generally being outperformed in all areas. Topic-specific trends suggest there may exist variability in the performance of Agent Laboratory across difference areas of machine learning research and across backend models.</p>
<h1>4.1.1. Human reviewer scores by language model</h1>
<p>In addition to evaluating paper quality, we also asked human reviewers to assess papers generated by Agent Laboratory according to NeurIPS-style criteria, including quality, significance, clarity, soundness, presentation, and contribution as shown in Figure 6. We evaluated the same papers analyzed in Section 4.1 using the aforementioned metrics and conducted the comparison. We found that the average human scores for the three backends revealed differences in performance, with average overall ratings ranging from $3.5 / 10$ with gpt-4o, $3.8 / 10$ with o1-mini, and $4.0 / 10$ with o1-preview.</p>
<p>First, when evaluating quality we find that reviewers rated gpt-4o the lowest at 1.8/4, while o1-mini achieved the highest score of $2.3 / 4$, demonstrating relatively better technical soundness. In terms of significance, all three backends received similar scores between 2.2-2.5/4, indicating a modest contribution to advancing research goals. Clarity scores showed slight variability, with gpt-4o receiving $2.6 / 4$ and o1-mini falling slightly lower at $2.1 / 4$ (-0.5), reflecting differences in how well the papers were written. The soundness of the generated outputs, which assesses the robustness of claims, was rated highest for o1-preview at $2.2 / 4$, with o1-mini and gpt-4o at $1.8(-0.4)$ and 1.7. Presentation and contribution ratings followed similar trends, with the overall contribution score averaging $2.1 / 4$ across models, highlighting a need for improvement in the originality of the outputs.</p>
<p>These scores show a general trend where human reviewers identified o1-preview as producing slightly better-rounded outputs compared to other backends, though significant gaps remain in technical and methodological aspects across all models. We note that the average score of an accepted paper at NeurIPS is 5.9. In this regard, on average, papers produced in autonomous mode are below the acceptance threshold for top ML conferences. These results demonstrate that, in autonomous mode, there is a need for refinement of Agent Laboratory to meet human expectations for high-quality, impactful research papers.</p>
<p>Automated Reviews versus Human Reviews. We also explore to what extent the automated reviewer scores align with those of human reviewers. The alignment is graphically illustrated using both tabular data (for all scores) and violin plots (for overall scores) in Figure 6. Our findings suggest that automated reviewers demonstrate notable discrepancies across all metrics compared with human evaluators, with a tendency to highly over-estimate the contribution of self-evaluated work. While the automated reviewers gave an average overall above average NeurIPS paper score of 6.1/10, human reviewers provided a much lower average of $3.8 / 10$ ( -2.3 points). Similar gaps are observed for all</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Average Automated Reviewer NeurIPS scores in Agent Laboratory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Quality</th>
<th style="text-align: center;">Significance</th>
<th style="text-align: center;">Clarity</th>
<th style="text-align: center;">Soundness</th>
<th style="text-align: center;">Presentation</th>
<th style="text-align: center;">Contribution</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gpt-40</td>
<td style="text-align: center;">$3.1 / 4$</td>
<td style="text-align: center;">$3.1 / 4$</td>
<td style="text-align: center;">$3.6 / 4$</td>
<td style="text-align: center;">$2.9 / 4$</td>
<td style="text-align: center;">$3.4 / 4$</td>
<td style="text-align: center;">$3.1 / 4$</td>
<td style="text-align: center;">$6.2 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">o1-mini</td>
<td style="text-align: center;">$3.1 / 4$</td>
<td style="text-align: center;">$2.9 / 4$</td>
<td style="text-align: center;">$3.5 / 4$</td>
<td style="text-align: center;">$2.9 / 4$</td>
<td style="text-align: center;">$3.2 / 4$</td>
<td style="text-align: center;">$2.8 / 4$</td>
<td style="text-align: center;">$6.0 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">o1-preview</td>
<td style="text-align: center;">$3.0 / 4$</td>
<td style="text-align: center;">$2.7 / 4$</td>
<td style="text-align: center;">$3.7 / 4$</td>
<td style="text-align: center;">$3.0 / 4$</td>
<td style="text-align: center;">$3.1 / 4$</td>
<td style="text-align: center;">$2.7 / 4$</td>
<td style="text-align: center;">$5.9 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$3.1 / 4$</td>
<td style="text-align: center;">$2.9 / 4$</td>
<td style="text-align: center;">$3.6 / 4$</td>
<td style="text-align: center;">$2.9 / 4$</td>
<td style="text-align: center;">$3.2 / 4$</td>
<td style="text-align: center;">$2.9 / 4$</td>
<td style="text-align: center;">$6.1 / 10$</td>
</tr>
</tbody>
</table>
<p>Average Human Reviewer NeurIPS scores in Agent Laboratory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Quality</th>
<th style="text-align: center;">Significance</th>
<th style="text-align: center;">Clarity</th>
<th style="text-align: center;">Soundness</th>
<th style="text-align: center;">Presentation</th>
<th style="text-align: center;">Contribution</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gpt-40</td>
<td style="text-align: center;">$1.8 / 4$</td>
<td style="text-align: center;">$2.2 / 4$</td>
<td style="text-align: center;">$2.6 / 4$</td>
<td style="text-align: center;">$1.8 / 4$</td>
<td style="text-align: center;">$2.7 / 4$</td>
<td style="text-align: center;">$1.9 / 4$</td>
<td style="text-align: center;">$3.5 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">o1-mini</td>
<td style="text-align: center;">$2.3 / 4$</td>
<td style="text-align: center;">$2.2 / 4$</td>
<td style="text-align: center;">$2.1 / 4$</td>
<td style="text-align: center;">$1.7 / 4$</td>
<td style="text-align: center;">$2.4 / 4$</td>
<td style="text-align: center;">$2.2 / 4$</td>
<td style="text-align: center;">$3.8 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">o1-preview</td>
<td style="text-align: center;">$1.9 / 4$</td>
<td style="text-align: center;">$2.5 / 4$</td>
<td style="text-align: center;">$2.4 / 4$</td>
<td style="text-align: center;">$2.2 / 4$</td>
<td style="text-align: center;">$2.4 / 4$</td>
<td style="text-align: center;">$2.3 / 4$</td>
<td style="text-align: center;">$4.0 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$2.3 / 4$</td>
<td style="text-align: center;">$2.4 / 4$</td>
<td style="text-align: center;">$1.9 / 4$</td>
<td style="text-align: center;">$2.5 / 4$</td>
<td style="text-align: center;">$2.1 / 4$</td>
<td style="text-align: center;">$3.8 / 10$</td>
</tr>
</tbody>
</table>
<p>Figure 6 | Scores from NeurIPs-style evaluation of generated papers, including the criterion: quality, significance, clarity, soundness, presentation, and contribution. (top) Split-violin plot comparing the overall score distribution of automated reviewers (LLM scores, left half of violin) and human reviewers (right half of violin). Human scores are not predictive of automated reviewer scores, demonstrating an average of -2.3 points lower. (middle) Automated reviewer scores across NeurIPs-style criterion. (bottom) Human reviewer scores across NeurIPs-style criterion.</p>
<p>specific criteria, such as clarity and contribution, where automated reviewers rated clarity at 3.6/4 on average compared to $2.4 / 4$ by human evaluators. This pattern holds for all criterion. Previous work demonstrates high alignment with automated reviewers (Lu et al. (2024b)) and ICLR scores from OpenReview. However, with actual humans rating the generated papers, we find that automated reviews do not align closely with human reviews and are far from an average accepted paper at NeurIPS 2024, which stands at $5.85^{\circ}$ (our scores were -2.05 points lower on average). Our results demonstrate that it is important for human evaluations to be provided alongside automated reviewer scores in future works in order to obtain a better understanding of the quality of generated papers.</p>
<h1>4.2. Evaluation of co-pilot quality</h1>
<p>We next evaluate the use of Agent Laboratory in co-pilot mode, where a human researcher is providing feedback at the end of each subtask (see Section 3.3.1 for more details). We evaluate performance across two measures: (1) the quality of Agent Laboratory as a tool for assisting their research and (2) the quality of generated papers. We first ask researchers to co-pilot Agent Laboratory on a topic of their choice without limitations. We then ask researchers to select a topic from the 5 topics introduced in Section 4.1, resulting in a total of 2 papers per researcher which we refer to as custom and preselected papers respectively. After their papers are generated, we ask researchers to rate their experience using Agent Laboratory during the process of generating custom and preselected papers. We then ask them to self-evaluate the generated papers according to NeurIPS-style criterion. Finally, we ask external researchers to evaluate their paper comparing performance with Agent Laboratory in autonomous mode. All experiments used an ol-mini backbone for all phases except the literature review.</p>
<h3>4.2.1. Quality as a tool</h3>
<p>The evaluation of Agent Laboratory as a research tool focuses on understanding its effectiveness in assisting researchers during the co-pilot mode. After generating their papers, participants were asked to reflect on their experiences and assess the tool's utility, usability, and overall satisfaction. We begin our evaluation by asking the following questions:</p>
<ul>
<li>Utility: How useful is Agent Laboratory for assisting your research?</li>
<li>Continuation: How likely are you to continue using Agent Laboratory for research?</li>
<li>Satisfaction: How much did you enjoy using Agent Laboratory?</li>
<li>Usability: How easy was it for you to build a project using Agent Laboratory?</li>
</ul>
<p>The result of answering each question is a score from 1-5, where 1 indicates the lowest agreement and 5 indicates the highest. We find that the overall scores across all experiments are 3.5/5 for utility, $3.75 / 5$ for continuation, $3.63 / 5$ for satisfaction, and $4.0 / 5$ for usability (Figure 7). We also delineate average scores based on custom and preselected topics. For custom experiments, we find overall scores of $3.75 / 5$ for utility, $4.0 / 5$ for continuation, $3.75 / 5$ for satisfaction, and $3.75 / 5$ for usability. For preselected topics, we find overall scores of $3.25 / 5$ for utility, $3.5 / 5$ for continuation, $3.5 / 5$ for satisfaction, and 4.25 for usability. Ratings for preselected topics are lower across all measures compared with custom, except for usability which was -0.5 points lower. From preselected to custom, utility and continuation increased by +0.5 points and satisfaction increased by +0.25 points.</p>
<p>We also evaluated across the same questions reported in Section 4.1. We report an average experimental quality rating of $2.38 / 5$, a report quality rating of $3.13 / 5$, and a usefulness rating of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Quality Evaluation of Agent Laboratory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Utility</th>
<th style="text-align: center;">Continuation</th>
<th style="text-align: center;">Satisfaction</th>
<th style="text-align: center;">Usability</th>
<th style="text-align: center;">Experiment <br> Quality</th>
<th style="text-align: center;">Report <br> Quality</th>
<th style="text-align: center;">Usefulness</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Preselected Topics</td>
<td style="text-align: center;">$3.25 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.25 / 5$</td>
<td style="text-align: center;">$2.5 / 5$</td>
<td style="text-align: center;">$2.75 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
</tr>
<tr>
<td style="text-align: center;">Custom Topics</td>
<td style="text-align: center;">$3.75 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
<td style="text-align: center;">$3.75 / 5$</td>
<td style="text-align: center;">$3.75 / 5$</td>
<td style="text-align: center;">$2.25 / 5$</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$3.5 / 5$</td>
<td style="text-align: center;">$3.75 / 5$</td>
<td style="text-align: center;">$3.63 / 5$</td>
<td style="text-align: center;">$4.0 / 5$</td>
<td style="text-align: center;">$2.38 / 5$</td>
<td style="text-align: center;">$3.13 / 5$</td>
<td style="text-align: center;">$3.75 / 5$</td>
</tr>
</tbody>
</table>
<p>Average Self-Evaluation NeurIPS scores in Agent Laboratory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Quality</th>
<th style="text-align: center;">Significance</th>
<th style="text-align: center;">Clarity</th>
<th style="text-align: center;">Soundness</th>
<th style="text-align: center;">Presentation</th>
<th style="text-align: center;">Contribution</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Preselected Topics</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$2.75 / 4$</td>
<td style="text-align: center;">$2.25 / 4$</td>
<td style="text-align: center;">$3.0 / 4$</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$4.0 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">Custom Topics</td>
<td style="text-align: center;">$2.25 / 4$</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$3.0 / 4$</td>
<td style="text-align: center;">$2.25 / 4$</td>
<td style="text-align: center;">$2.75 / 4$</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$4.25 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$2.13 / 4$</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$2.88 / 4$</td>
<td style="text-align: center;">$2.25 / 4$</td>
<td style="text-align: center;">$2.88 / 4$</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$4.13 / 10$</td>
</tr>
</tbody>
</table>
<p>Average External Evaluation NeurIPS scores in Agent Laboratory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Quality</th>
<th style="text-align: center;">Significance</th>
<th style="text-align: center;">Clarity</th>
<th style="text-align: center;">Soundness</th>
<th style="text-align: center;">Presentation</th>
<th style="text-align: center;">Contribution</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Preselected Topics</td>
<td style="text-align: center;">$3.0 / 4$</td>
<td style="text-align: center;">$2.5 / 4$</td>
<td style="text-align: center;">$2.75 / 4$</td>
<td style="text-align: center;">$2.5 / 4$</td>
<td style="text-align: center;">$3.0 / 4$</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$4.5 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">Custom Topics</td>
<td style="text-align: center;">$2.5 / 4$</td>
<td style="text-align: center;">$2.0 / 4$</td>
<td style="text-align: center;">$2.5 / 4$</td>
<td style="text-align: center;">$2.25 / 4$</td>
<td style="text-align: center;">$2.75 / 4$</td>
<td style="text-align: center;">$2.25 / 4$</td>
<td style="text-align: center;">$4.25 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$2.75 / 4$</td>
<td style="text-align: center;">$2.25 / 4$</td>
<td style="text-align: center;">$2.63 / 4$</td>
<td style="text-align: center;">$2.38 / 4$</td>
<td style="text-align: center;">$2.88 / 4$</td>
<td style="text-align: center;">$2.13 / 4$</td>
<td style="text-align: center;">$4.38 / 10$</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$ Autonomous</td>
<td style="text-align: center;">+0.75</td>
<td style="text-align: center;">-0.05</td>
<td style="text-align: center;">+0.23</td>
<td style="text-align: center;">+0.48</td>
<td style="text-align: center;">+0.33</td>
<td style="text-align: center;">+0.03</td>
<td style="text-align: center;">+0.58</td>
</tr>
</tbody>
</table>
<p>Figure 7 | Co-pilot evaluation.
$3.75 / 5$. We find higher scores for custom topics across report quality with a rating of $3.5 / 5(+0.75)$ and a usefulness rating of $4.0 / 5(+0.5)$. For experiment quality, we find that preselected has +0.25 points higher with a score of $2.5 / 5$. Scores across all metrics rated lower when compared with the corresponding ol-mini autonomous evaluation results. While report quality was only rated -0.07 points lower, usefulness was rated -0.55 points lower and experiment quality was -0.82 points lower.</p>
<p>Finally, we opened an optional question for participants to provide feedback, which asks the following question: "How could Agent Laboratory be improved for your research?" For both custom and preselected topics we received a $75 \%$ response rate. From this feedback, there were suggestions for improving the Agent Laboratory interface (e.g., adding a GUI, better inspection of intermediate results), adding the option to incorporate more figures for the paper, and improving the literature review phase. We find that when compared to reviews of Agent Laboratory in autonomous mode from Section 4.1, human co-pilots rated report quality, usefulness, and experiment quality lower. From feedback provided by researchers, we find the reduction in scores is due to difficulty guiding the agents to execute their exact vision for the project. We discuss these limitations in greater detail in Section 5.</p>
<h1>4.2.2. Evaluation of co-pilot generated papers</h1>
<p>To assess the quality of papers generated by Agent Laboratory in co-pilot mode, we conduct evaluations using two approaches: (1) researchers self-assessed their generated papers based on NeurIPS-style criteria, and (2) external researchers provided evaluations of the same papers. This section aims to understand differences in scores from self-assessment and external assessment, as well as how assessments compare to Agent Laboratory in fully autonomous mode. We use the same NeurIPS criterion introduced in Section 4.1.1.</p>
<p>Self-evaluation. From the results of the self-evaluation (Figure 7), we found that the average overall score increased from evaluations provided to papers generated in autonomous mode, with autonomous papers having an overall average of $3.8 / 10$ and co-pilot papers at $4.13 / 10(+0.33)$. These scores even improved across the best autonomous backend, o1-preview, which averaged 4.0/10. Across individual criterion, scores increased for quality ( +0.13 ), clarity ( +0.48 ), soundness ( +0.35 ), and presentation ( +0.33 ), but decreased for significance and contribution. The scores that decreased were significance $(-0.3)$ and contribution $(-0.1)$.</p>
<p>External evaluation. We compare scores provided through self-evaluation with those provided by a set of external evaluators on the same papers (Figure 7). We find that average scores across most criteria, including quality, significance, clarity, soundness, presentation, and contribution, show an improvement in the external assessments, with an overall average of $4.38 / 10$, up from $4.13 / 10$ in self-evaluations. The most significant improvements were observed in quality ( +0.62 ), significance $(+0.25)$, and overall $(+0.25)$ scores, suggesting that external reviewers perceived the generated papers to be higher quality and more significant than the researchers who produced them. However, clarity scores decreased $(-0.25)$, indicating potential issues in the articulation of ideas that might have been overlooked during self-assessment. While presentation scores did not improve ( +0.0 ), soundness $(+0.13)$ and contribution $(+0.13)$ only increased slightly.</p>
<p>Notably, the external evaluations also reinforce differences between scores preselected and custom topics. Unlike with the self-evaluated papers, papers on preselected topics were rated slightly higher overall, with improvements observed across several metrics, particularly in quality ( +0.5 ) and significance ( +0.5 ). These findings suggest that self-evaluated reviewers perceive the work produced on their custom topic as higher quality compared to the work produced on preselected topics, whereas external evaluators find the opposite to be true.</p>
<p>Comparison with autonomous mode Comparing scores by external evaluators on autonomous and co-pilot papers (Figure 7), we find that the largest improvements were seen for quality, which increased by +0.75 , soundness, which improved by +0.48 , and the overall score, which improved by +0.58 . Moderate gains were also observed in clarity ( +0.23 ) and presentation ( +0.33 ). In contrast, some metrics showed minimal or no improvement. Significance declined slightly (-0.05), and contribution increased only marginally ( +0.03 ). Our results suggest that papers generated with human involvement overall are evaluated more highly than autonomously generated paper, with much of the focus of human involvement going toward making the paper more presentable (presentation and clarity) while there was less emphasis on improving experimental results (significance and contribution). Finally, we note that co-pilot overall scores, which average at 4.38 , are still -1.45 points below the average score of 5.85 for an accepted paper at NeurIPS 2024. Increasing the overall score to match conference standards will likely result by improving the contribution and significance of the paper results, which is consistently lower than other evaluation metrics.</p>
<h1>4.3. Runtime statistics</h1>
<p>Runtime statistics for Agent Laboratory are detailed to provide insight into the computational efficiency and monetary costs associated with different phases of its workflow. In this evaluation, both the time required per phase (measured in seconds) and the costs incurred (calculated in USD) were analyzed to better understand the performance of three model backends: gpt-4o, o1-mini, and o1-preview. These measurements were recorded for each subtask, including Literature Review, Plan Formulation, Data Preparation, Running Experiments, Results Interpretation, Report Writing, and Report Refinement.</p>
<p>Subtask Average Cost (\$US) in Agent Laboratory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Literature <br> Review</th>
<th style="text-align: center;">Plan <br> Formulation</th>
<th style="text-align: center;">Data <br> Preparation</th>
<th style="text-align: center;">Running <br> Experiments</th>
<th style="text-align: center;">Results <br> Interpretation</th>
<th style="text-align: center;">Report <br> Writing</th>
<th style="text-align: center;">Report <br> Refinement</th>
<th style="text-align: center;">Entire <br> Workflow</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gpt-40</td>
<td style="text-align: center;">$\$ 0.12$</td>
<td style="text-align: center;">$\$ 0.03$</td>
<td style="text-align: center;">$\$ 0.09$</td>
<td style="text-align: center;">$\$ 0.18$</td>
<td style="text-align: center;">$\$ 0.16$</td>
<td style="text-align: center;">$\$ 1.73$</td>
<td style="text-align: center;">$\$ 0.02$</td>
<td style="text-align: center;">$\$ 2.33$</td>
</tr>
<tr>
<td style="text-align: center;">o1-mini</td>
<td style="text-align: center;">$\$ 0.16$</td>
<td style="text-align: center;">$\$ 0.22$</td>
<td style="text-align: center;">$\$ 3.03$</td>
<td style="text-align: center;">$\$ 1.05$</td>
<td style="text-align: center;">$\$ 0.40$</td>
<td style="text-align: center;">$\$ 2.58$</td>
<td style="text-align: center;">$\$ 0.07$</td>
<td style="text-align: center;">$\$ 7.51$</td>
</tr>
<tr>
<td style="text-align: center;">o1-preview</td>
<td style="text-align: center;">$\$ 0.31$</td>
<td style="text-align: center;">$\$ 0.04$</td>
<td style="text-align: center;">$\$ 0.30$</td>
<td style="text-align: center;">$\$ 2.59$</td>
<td style="text-align: center;">$\$ 0.21$</td>
<td style="text-align: center;">$\$ 9.58$</td>
<td style="text-align: center;">$\$ 0.09$</td>
<td style="text-align: center;">$\$ 13.1$</td>
</tr>
</tbody>
</table>
<p>Subtask Average Time (seconds) in Agent Laboratory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Literature <br> Review</th>
<th style="text-align: center;">Plan <br> Formulation</th>
<th style="text-align: center;">Data <br> Preparation</th>
<th style="text-align: center;">Running <br> Experiments</th>
<th style="text-align: center;">Results <br> Interpretation</th>
<th style="text-align: center;">Report <br> Writing</th>
<th style="text-align: center;">Report <br> Refinement</th>
<th style="text-align: center;">Entire <br> Workflow</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gpt-40</td>
<td style="text-align: center;">92.9 s</td>
<td style="text-align: center;">23.3 s</td>
<td style="text-align: center;">37.1 s</td>
<td style="text-align: center;">417.8 s</td>
<td style="text-align: center;">21.5 s</td>
<td style="text-align: center;">572.5 s</td>
<td style="text-align: center;">16.8 s</td>
<td style="text-align: center;">1165.4 s</td>
</tr>
<tr>
<td style="text-align: center;">o1-mini</td>
<td style="text-align: center;">56.8 s</td>
<td style="text-align: center;">51.7 s</td>
<td style="text-align: center;">503.6 s</td>
<td style="text-align: center;">2082.5 s</td>
<td style="text-align: center;">73.3 s</td>
<td style="text-align: center;">827.7 s</td>
<td style="text-align: center;">21.2 s</td>
<td style="text-align: center;">3616.8 s</td>
</tr>
<tr>
<td style="text-align: center;">o1-preview</td>
<td style="text-align: center;">136.1 s</td>
<td style="text-align: center;">33.1 s</td>
<td style="text-align: center;">113.5 s</td>
<td style="text-align: center;">4036.2 s</td>
<td style="text-align: center;">28.3 s</td>
<td style="text-align: center;">1854.2 s</td>
<td style="text-align: center;">33.1 s</td>
<td style="text-align: center;">6201.3 s</td>
</tr>
</tbody>
</table>
<p>Subtask Success Rate in Agent Laboratory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Literature <br> Review</th>
<th style="text-align: center;">Plan <br> Formulation</th>
<th style="text-align: center;">Data <br> Preparation</th>
<th style="text-align: center;">Running <br> Experiments</th>
<th style="text-align: center;">Results <br> Interpretation</th>
<th style="text-align: center;">Report <br> Writing</th>
<th style="text-align: center;">Report <br> Refinement</th>
<th style="text-align: center;">Entire <br> Workflow</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gpt-40</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$94.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">o1-mini</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$92.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">o1-preview</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$95.7 \%$</td>
</tr>
</tbody>
</table>
<p>Figure 8 | Performance and Cost Evaluation. This table summarizes the runtime statistics, cost, and success rates of Agent Laboratory across its workflow phases using three different model backends: gpt-40, o1-mini, and o1-preview. The metrics include average cost per phase (in USD), average time per phase (in seconds), and success rates for each phase.</p>
<p>Inference time Across all models, gpt-40 exhibited the fastest execution times, completing the entire workflow in 1165.4 seconds, approximately 3.2 x faster than o 1 -mini and 5.3 x faster than o1-preview, which required 3616.8 seconds and 6201.3 seconds, respectively. In most subtasks, gpt-40 demonstrated superior speed, particularly in Running Experiments and Report Writing phases, where its times were significantly shorter than those of o1-mini and o1-preview. For instance, in Running Experiments, gpt-40 averaged 417.8 seconds, while o1-mini and o1-preview took 2082.5 seconds and 4036.2 seconds, respectively. Similarly, for Report Writing, gpt-40 completed the task in 572.5 seconds, compared to 827.7 seconds for o1-mini and 1854.2 seconds for o1-preview.</p>
<p>Inference cost Monetary costs per workflow were also substantially lower for gpt-40, which averaged just $\$ 2.33$ for the entire process. This is significantly more cost effective than previous autonomous research workflows (Lu et al. (2024b)), which cost around $\sim \$ 15$ ( 6.4 x more expensive) to complete using gpt-40. Other models in our workflow has a lower cost efficiency, such as o1-mini at $\$ 7.51$, and o1-preview at $\$ 13.10$, the latter being over 5.6 x more expensive than gpt-40. Among the individual subtasks, gpt-40 consistently had the lowest costs. For example, its costs for Data Preparation and Report Writing were $\$ 0.09$ and $\$ 1.73$, respectively, compared to $\$ 3.03$ and $\$ 2.58$ for o1-mini, and $\$ 0.30$ and $\$ 9.58$ for o1-preview.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9 | Average score of four methods (MLAB, OpenHands, AIDE, and mle-solver) on a subset of MLE-Bench.</p>
<p>Phase-level Observations From our observations at the phase-level, Literature Review was notably efficient for all models in terms of time and cost, with gpt-4o completing it in 92.9 seconds at a cost of $\$ 0.12$. Meanwhile, o1-mini completed this phase faster ( 56.8 seconds) but at a slightly higher cost (\$0.16). For Plan Formulation, gpt-4o was both the fastest ( 23.3 seconds) and the cheapest (\$0.03), followed closely by o1-preview in cost ( $\$ 0.04$ ) but not in speed ( 33.1 seconds). The most expensive phase across models was Report Writing, where costs were driven by the increased computational resources required for writing a long document. o1-preview incurred particularly high costs in this phase ( $\$ 9.58$ ) despite producing comparable outputs in terms of task success rates.</p>
<p>Success Rates Overall, every model exhibits reasonably high reliability, with o1-preview achieving the highest average subtask success rate ( $95.7 \%$ ) for the entire workflow. Both gpt-4o and o1-mini followed closely at $94.3 \%$ and $92.8 \%$. While most tasks had $100 \%$ success rate for each model, the literature review phase had a high rate of failure, at $60 \%, 70 \%$, and $80 \%$ for gpt-4o, o1-mini, and o1-preview respectively. The Data Preparation phase showed minor challenges, with o1-mini recording an $80 \%$ success rate in Data Preparation, compared to gpt-4o's $100 \%$ success rate and o1-preview at a $90 \%$ success rate.</p>
<h1>4.4. Evaluating mle-solver on MLE-Bench</h1>
<p>Evaluating the entire Agent Laboratory workflow does not contain much information about the ability of mle-solver specifically to solve individual ML problems. In order to evaluate mle-solver more objectively, we use a subset of 10 ML challenges from MLE-Bench (Chan et al. (2024)). MLEBench is a benchmark designed to assess the capability of agents in handling real-world ML tasks on Kaggle competitions. This benchmark compares agent performances with human baselines, scoring agents with Kaggle's medal system, and incorporating mechanisms to mitigate contamination and plagiarism risks. We include all challenges focusing on text and tabular data from the low complexity category of MLE-Bench. We provide as input to mle-solver the following: Kaggle dataset description, distilled knowledge from Kaggle notebooks, as well as an accessible train and dev set. Instead of using an LLM scoring function, the mle-solver score is evaluated on the dev set, which is a $20 \%$ random sample taken from the original training set, and the training set is represented by the other $80 \%$ split. All data (dev, test, train) is placed into arrays using the numpy library instead of providing</p>
<p>file locations in order to better emulate the data preparation phase. Once all mle-solver steps have concluded, the final code with the highest score is evaluated on the actual Kaggle test set and a benchmark score is recorded.</p>
<p>We compare average scores across several runs from three other methods: MLAB (Huang et al. (2024), gpt-4o backend), OpenHands (Wang et al. (2024b), gpt-4o backend), and AIDE (Schmidt et al. (2024), o1-preview backend). While mle-solver submitted valid solutions for all MLE-Bench challenges within two hours, prior methods often failed to submit, complicating scoring. We thus calculated average scores by excluding invalid submissions from other works and averaging valid ones. We find that Agent Laboratory's mle-solver is more consistently high scoring than other solvers, with mle-solver obtaining four medals (two gold, one silver, and one bronze) compared with OpenHands (gpt-4o) obtaining two medals (two gold), AIDE (o1-preview) obtaining two medals (one gold, one bronze) and MLAB obtaining zero medals. Additionally, mle-solver obtained above median human performance on six out of ten benchmarks, with AIDE obtaining five out of ten, OpenHands two out of ten, and MLAB zero out of ten. A detailed overview is provided in Figure 9.</p>
<h1>5. Limitations</h1>
<p>While our results suggest that Agent Laboratory demonstrates strong performance as a research tool, we now turn to a discussion of limitations that could inform future work. While some of these are also limitations of LLMs themselves, others are not, and we nonetheless provide a thorough and critical discussion of our work. We hope that progress in autonomous research will address these limitations.</p>
<h3>5.1. Workflow limitations</h3>
<p>Challenges with self-evaluation The paper-solver is being evaluated for quality by using LLMs emulated NeurIPS reviewers. This has two limitations: (1) while the reviewing agents were shown to have high alignment with real reviewers (Lu et al. (2024b)), qualitatively research reports from Agent Laboratory are less satisfying than research papers from The AI Scientist (Lu et al. (2024b)), with ours having lower quality figures, despite Agent Laboratory papers obtaining higher scores overall. (2) The research reports produced by Agent Laboratory are not meant to replace the paper writing process done by humans as it was in The AI Scientist, rather it is meant to provide a report for the human to understand what has been accomplished, so that they can scale up the experiment and write their own research report. However, we nonetheless use NeurIPS reviewer scores as the heuristic for the quality of our presented paper-solver, which aims to evaluate the reports from the perspective of a complete research paper. Additionally, contrasting with Lu et al. (2024b) demonstrate that LLMs perform less reliably for self-evaluation compared with human reviewers, with lower agreement scores ( $53.3 \%$ vs. $56.1 \%$ ). Although LLMs demonstrate reasonable consistency, this may stem from reliance on superficial patterns rather than robust evaluation criteria, resulting in discrepancies between LLM and human rankings. This limits LLMs in subjective tasks like research idea evaluation, which is the foundation of mle-solver and paper-solver.</p>
<p>Challenges with automated structure There are also some limitations that present themselves due to the structure enforced in the workflow. For example, paper-solver is encouraged to a organize the paper into a relatively fixed structure (abstract, introduction, etc), which disallows unique paper organizations and section orders. Another limitation is that mle-solver and paper-solver are limited to generating only two figures for the paper. This can be solved in future work, by allowing all of the figures generated by the mle-solver (without restriction) to be incorporated into</p>
<p>paper-solver by detecting image files and providing those paths to the solver. Agent Laboratory is also not able to manage repository-level code on its own, but rather the appropriate files are provided to it at each necessary step and files are saved based on which phase produced the file. Enabling flexible repository-level file modification and execution is a clear next step for future work.</p>
<p>Challenges with hallucination While uncommon, we also found that in some of the research papers, particularly from lower performing models, such as gpt-4o, there were hallucinations regarding experimental results that did not occur, such as the following example from a gpt-4o paper on the topic of Are image transformers more or less sensitive to noise than convolutional networks?: "Hyperparameter optimization played a crucial role in achieving these results. The learning rate was set at 0.001 , with a batch size of 32, and the number of reasoning steps $L=\left{l_{1}, l_{2}, \ldots, l_{n}\right}$ varied between 5 to 10, depending on the complexity of the query. The model was trained over 50 epochs, with early stopping criteria applied to prevent overfitting." While the issue of hallucination is more generally a problem with LLMs themselves, future work must appropriately address these challenges in order to prevent misinformation from being propagated when using automated research tools.</p>
<h1>5.2. Common failure modes</h1>
<p>In addition to the limitations outlined in Section 5.1, we also outline common failure modes observed during the runtime of Agent Laboratory. We report a list of the most common failure modes observed below:</p>
<ul>
<li>Many of the more capable models (gpt-4o, o1-mini, o1-preview) struggled with instructionfollowing during the literature review phase, and had a tendency to repeatedly use the summarize command until the maximum phase steps have been reached, leading to a termination.</li>
<li>Retrieved papers during the literature review phase had been observed to reach the maximum token limit for some models.</li>
<li>Experiments run by mle-solver sometimes obtain 0\% accuracy for all tested methods which is not corrected by the agent by the time mle-solver runs out of solving steps.</li>
<li>mle-solver has a tendency to edit line 0 more than other lines in the code, causing to the replace command to more often lead to successful code compiles.</li>
<li>Printed output from the data preparation or experimental results can lead to the LLMs reaching their token limit.</li>
<li>mle-solver often generated the python exit() command, which terminated the entire process. This had to be detected and removed manually.</li>
<li>mle-solver has been observed to run system commands on the host computer using the subprocess.run() command. While nothing problematic has been observed, safeguards should be implemented around this.</li>
<li>paper-solver often struggles to search for relevant papers using the arXiv engine. Before a search time-limit was enforced, it could take up to 100 tries for a successful search query to return any papers. A limit of 5 was place thereafter to prevent this cycle.</li>
</ul>
<h3>5.3. Ethical considerations</h3>
<p>Agent Laboratory offers potential to accelerate the field of machine learning research by automating time-intensive tasks and enabling researchers to focus on ideation and experimental design. However, its capabilities also bring ethical challenges that require careful consideration. The ability to autonomously generate research code, reports, and experiment plans may inadvertently lower the</p>
<p>barriers to producing substandard or misleading scientific outputs. This could overwhelm peer review systems and jeopardize the integrity of academic discourse. Furthermore, the automated processes may reflect or even amplify biases inherent in the underlying datasets or algorithms, leading to skewed outcomes in research findings. Transparent disclosure of AI involvement in research outputs is important in order to mitigate such risks and maintain accountability.</p>
<p>There are additional concerns about potential misuse of Agent Laboratory for unethical purposes, such as developing harmful technologies or generating content that bypasses ethical oversight. For instance, the misuse of autonomous research agents in fields like cybersecurity could lead to the automated creation of malware (Begou et al. (2023); Francia et al. (2024); Happe \&amp; Cito (2023); Xu et al. (2024)) or in environmental studies, it may generate biased analyses that downplay climate risks or overstate the benefits of certain interventions. Moreover, as the platform matures, the risk of its misuse increases if safeguards are not implemented to ensure alignment with ethical research standards (Jiao et al. (2024); Watkins (2024)). Thus, while Agent Laboratory demonstrates immense promise for accelerating scientific discovery, there is a need for robust governance mechanisms to ensure that the underlying LLMs produce content that aligns with ethical principles and societal values.</p>
<h1>6. Discussion</h1>
<p>In this paper, we introduce Agent Laboratory, an open-source LLM agent framework for accelerating the individual's ability to perform research in machine learning. Unlike fully automated research pipelines that attempt to conceive their own research directions, Agent Laboratory is designed as a co-pilot, enabling a more human-centric mode of scientific exploration. Because of this, we present results from human-centered experiments. Our initial evaluations focused on the quality of generated papers in autonomous mode, assessing human evaluations of experimental and report quality, usefulness, as well as reviewer scores based on standard academic criteria across different language models. We also assessed the effectiveness of Agent Laboratory in co-pilot mode, comparing its performance with autonomous mode, receiving positive feedback from researchers.</p>
<p>The findings of this work highlight the variability in performance across LLM backends, with the o1-preview model being rated most useful, while o1-mini demonstrated the highest experimental quality. Autonomous mode outputs, although generally well-received, revealed gaps when evaluated against human expectations for high-quality research papers, particularly in terms of clarity and soundness. We also find that automated reviewer scores do not predict human reviewer scores demonstrating the importance of human evaluations in automated research. Integrating human feedback in co-pilot mode overall produced higher-quality outputs than autonomous mode, with higher scores across most metrics. The co-pilot feature in Agent Laboratory is overall found to have high utility and usability when rated by human users, with most participants deciding to continue usage after their experience. Finally, runtime and cost analyses demonstrated the efficiency of the framework, with the gpt-4o backend offering the fastest execution and lowest costs. Finally, evaluations of the mle-solver on MLE-Bench demonstrates improved ability to solve general ML problems over previous methods.</p>
<p>Agent Laboratory builds upon an emerging trend in the use of language agents for science, where previous works have shown the potential of LLMs to generate research ideas (Baek et al. (2024); Li et al. (2024a); Si et al. (2024)), implement machine learning projects (Chan et al. (2024); Huang et al. (2024); Jing et al. (2024)), and even produce scientific papers (Lu et al. (2024b)). While many of these prior efforts leverage LLMs as tools to be applied at discrete stages, Agent Laboratory integrates these processes into a single, continuous pipeline that can scale and adapt to</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*https://papercopilot.com/statistics/neurips-statistics/neurips-2024-statistics&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
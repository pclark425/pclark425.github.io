<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-426 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-426</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-426</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-258841008</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.551.pdf" target="_blank">Detecting and Mitigating Hallucinations in Multilingual Summarisation</a></p>
                <p><strong>Paper Abstract:</strong> Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed loss weighting method drastically increases both performance and faithfulness according to both automatic and human evaluation when compared to strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset are available at https://github.com/yfqiu-nlp/mfact-summ.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e426.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e426.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT-noise-in-translate-train</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-Translation Induced Noise in Translate-Train Faithfulness Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Translation errors in the pipeline used to create multilingual faithfulness training data (translate-train) introduce noise and can create or mask hallucinations in target-language examples, misaligning the intended natural-language specification of 'faithful' samples and the actual code-produced training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>mFACT translate-train pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that: (1) scores English document-summary pairs with assembled English faithfulness metrics, (2) selects top/bottom k examples, (3) translates these into target languages via a machine translation API, and (4) fine-tunes a multilingual classifier (mBERT) on the translated silver dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section (metric construction pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data preprocessing and training scripts (translation + classifier training)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>translation-induced data corruption / noisy label propagation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper's natural-language description posits that translating top/bottom ranked English examples into target languages yields faithful/hallucinated labeled training data; in implementation, some translated samples were corrupted by MT (introducing hallucinations) and some labels were incorrect due to imperfect English metrics, causing a mismatch between the intended labels and the actual target-language training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / dataset creation (translate-train stage)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual translation quality check and error analysis (random sample of 100 Chinese positive examples) combined with downstream classifier validation</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>manual annotation of 100 translated 'positive' Chinese samples to count translation-caused hallucinations and label-ranking errors; also measured classifier validation/test performance on translated test splits (accuracy/precision/recall/F1 reported for mFACT)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Introduced a small but non-negligible amount of noisy labels: in a 100-sample manual check, 13 samples were hallucinated; 4 of those (4% of the checked set) were caused by poor translation and 9 were due to incorrect ranking by English metrics. Noisy translation reduced the ideal purity of the silver dataset and could degrade learned classifier reliability if unaddressed.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in the sanity check for Chinese: 4% translation-caused noise among 100 sampled positives; label-ranking errors (from English metric ensemble) caused a further ~9% of noise in the same sample. More generally, the paper notes that MT noise is 'mostly small' but non-zero and a limiting factor for extending to languages with poor MT.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Propagation of machine translation errors into the training data and reliance on English-only metric labels (teacher models) that are imperfect; absence of native-language annotated faithfulness data and language-specific auxiliary tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Manual sanity checks; selecting diverse English teacher metrics to reduce bias; rely on top/bottom k ranking (extreme examples) to reduce ambiguous samples; keep MT-based translate-train but acknowledge limits; propose that resources may constrain extension to other languages.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Sanity check showed only a small fraction of translations introduced hallucinations (4% in checked sample). Aggregating multiple English metrics and selecting extremes (top/bottom k) reduces ambiguous labels relative to random selection, and the resulting mFACT classifiers still achieved high test accuracy (~92–95% across languages).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / multilingual summarisation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detecting and Mitigating Hallucinations in Multilingual Summarisation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e426.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e426.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLI-vs-faithfulness-misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Misalignment Between Natural Language Inference (NLI) Descriptions and Faithfulness Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using NLI-style models or NLI supervision as described in related work does not align with the specific requirements of faithfulness detection for summarisation in practice: NLI yields substantially worse faithfulness classification performance than a metric-trained classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>faithfulness classification baselines</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Comparison of faithfulness classifiers: (a) multilingual BERT fine-tuned on XNLI (NLI task), (b) XNLI further fine-tuned on translated mFACT data, (c) mFACT classifiers trained on translate-train silver datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methodological assumption / baseline description in paper (related work and baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training/fine-tuning scripts for multilingual BERT (NLI and faithfulness datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>task-specification mismatch (different task than described)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Prior language in literature suggests NLI models can be repurposed to detect factual inconsistency; however, implemented NLI-based models performed poorly compared to classifiers trained specifically on translated faithfulness labels, indicating the natural-language characterization is overbroad and mismatched to the implemented faithfulness task.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / classifier training (baseline vs target task)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical comparison of classifier performance on translated faithfulness test sets (quantitative experiments reported in Table 1 and Appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>standard classification metrics (accuracy, precision, recall, F1) across six target languages; reported average accuracy: XNLI baseline ~52.9% (±0.7) vs mFACT ~95.3% (±1.7)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Relying on NLI baselines would substantially under-detect hallucinations (low recall and F1), producing misleading claims about model faithfulness; the paper shows that NLI-based systems are 'not on par' with dedicated faithfulness classifiers, which affects evaluation validity and model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently across six tested languages; the paper reports a large and consistent performance gap between NLI-trained and faithfulness-trained classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Conceptual conflation in natural-language descriptions between NLI (entailment detection) and summarisation faithfulness (which involves both intrinsic and extrinsic hallucinations and token/span-level phenomena).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Construct a dedicated faithfulness training dataset in the target language (translate-train) using an ensemble of English teacher metrics, and train a dedicated classifier (mFACT) rather than using NLI models.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Highly effective per experiments: mFACT classifiers achieved ~95% accuracy and strong precision/recall/F1 across languages, substantially outperforming XNLI baselines; thus dedicated training aligns code behavior with the intended faithfulness evaluation described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detecting and Mitigating Hallucinations in Multilingual Summarisation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e426.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e426.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>checkpoint-selection-misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Checkpoint Selection Mismatch: ROUGE vs Faithfulness (mFACT) Objectives</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Selecting model checkpoints by validation ROUGE (performance-oriented) as described in many summarisation experiments can misalign with faithfulness goals; the paper shows selecting by validation mFACT yields higher faithfulness though ROUGE-based selection is common and described.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>model selection / validation monitoring</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Training/validation loop where checkpoints are selected based on validation metrics (either ROUGE-1 or validation mFACT) for later evaluation and deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / model selection procedure</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training and checkpoint selection code (validation scoring and early stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>metric-objective mismatch / selection criterion misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The natural-language practice of selecting checkpoints by validation ROUGE (to optimise summarisation performance) does not ensure optimal faithfulness; implemented experiments reveal that selecting by ROUGE can give models with worse faithfulness than selecting by a dedicated faithfulness metric (mFACT), leading to different final behaviors than implied by ROUGE-only descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / model selection</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>ablation experiments comparing final model performance when selecting best checkpoint by validation ROUGE vs by validation mFACT (reported in Appendix A.10 / Table 12)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>compare downstream ROUGE and faithfulness (mFACT) on test sets for models checkpointed by each criterion; also observe trends in validation curves (Figure 5)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Selecting by ROUGE often yields higher ROUGE but lower faithfulness; selecting by mFACT increases faithfulness (and can sometimes improve faithfulness without large ROUGE loss). The paper reports that selecting by validation faithfulness 'has a higher positive contribution to model's faithfulness.'</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across multiple languages and models in the paper; the selection effect is robust enough that the authors recommend using validation faithfulness when the goal is faithful summarisation.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Using a single performance metric (ROUGE) as a proxy for all desired qualities (including faithfulness) and omission in standard experimental protocols of faithfulness-aware selection criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use validation faithfulness metrics (mFACT) as selection/early-stopping criterion, or jointly monitor ROUGE and mFACT; report both checkpoints and their trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirically beneficial: checkpointing by mFACT yields higher final faithfulness scores and better alignment with human judgments (paper notes positive contribution), though trade-offs with ROUGE can vary by language and model.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning experimentation / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detecting and Mitigating Hallucinations in Multilingual Summarisation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e426.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e426.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cross-lingual-transfer-faithfulness-gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance vs Faithfulness Trade-off in Cross-lingual Transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-lingual transfer methods (e.g., MAD-X or full-model transfer) improve ROUGE summarisation performance in target languages but systematically introduce or amplify hallucinations compared to monolingual English training, revealing a gap between natural-language expectations of transfer benefits and actual faithfulness outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>cross-lingual transfer experimental pipeline (MAD-X and full-model transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Adapter-based (MAD-X) and full-model cross-lingual transfer procedures where models are first trained on source-language data (English) and then applied to or fine-tuned for target languages, compared with monolingual fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental hypothesis / expected outcome described in introduction and experiments</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training pipelines for MAD-X adapters and full-model fine-tuning (adapter stacking, task adapter training, zero-shot/few-shot inference)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>behavioural mismatch / unintended side-effect of transfer</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>While methodological descriptions posit that cross-lingual transfer transfers task knowledge beneficially, implemented experiments reveal that transfer also increases the frequency of hallucinations (both intrinsic and extrinsic) in target-language summaries — a qualitative discrepancy between the expected 'benefit-only' description and the implemented effect.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / transfer stage (source-language pretraining -> target inference)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical evaluation: compare ROUGE and multiple faithfulness metrics (DAE, QAFactEval, ENFS%, EntFA) between monolingual fine-tuning and cross-lingual transfer (zero-shot and few-shot); also human evaluation and mFACT scoring</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>reporting of ROUGE-1/2/L and multiple faithfulness metrics on XL-Sum for six languages, plus mFACT classifier predictions and human A/B preference tests; observed distributions shown in figures (e.g., Figure 3) and tables (e.g., Table 3) indicate decreased faithfulness after transfer despite improved ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Cross-lingual transfer increased ROUGE but decreased faithfulness metrics across languages — i.e., better surface performance masks a deterioration in factual consistency. This can mislead evaluations that only report ROUGE and thus harms trustworthiness and reproducibility of claimed improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across six typologically diverse languages (Chinese, Spanish, French, Hindi, Turkish, Vietnamese) and for multiple transfer methods (MAD-X and full-model), indicating this is a general phenomenon in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Transfer of task-specific behavior from source-language data carries over spurious correlations or generation tendencies (including hallucination patterns) that are not filtered in target languages; lack of target-language faithfulness supervision during transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Several: (a) expert/anti-expert adapters (TVN, CAPE, DExpert) adapted from monolingual settings; (b) weighted-loss approach that scales each training example's loss by its faithfulness score (mFACT) to downweight hallucinated training examples; (c) checkpoint selection by mFACT.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Weighted-loss approach (WL) achieved consistent gains: improved ROUGE across most languages while achieving comparable or improved faithfulness according to mFACT and human judgments; human A/B tests preferred WL over MAD-X. Expert/anti-expert methods produced mixed and inconsistent gains, sometimes trading ROUGE for faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>multilingual NLP / transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detecting and Mitigating Hallucinations in Multilingual Summarisation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e426.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e426.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>metric-ensemble-bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias and Coverage Gap from Relying on Single English Faithfulness Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Relying on a single English faithfulness metric to label or rank training data produces biased coverage of hallucination types; assembling multiple English metrics into an ensemble (mFACT) reduces bias and yields better alignment with human judgments in target languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>mFACT metric construction (ensemble of English metrics -> multilingual classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Construction of the mFACT metric by averaging normalized scores from four diverse English faithfulness metrics (DAE, QAFactEval, ENFS%, EntFA), ranking XSum samples, translating extremes, and training a multilingual classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>metric specification and justification (methods and motivation)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>scoring, ranking, translation, and classifier training scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>single-metric bias / incomplete specification of evaluation coverage</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The natural-language description of using an English faithfulness metric 'as is' for other languages is incomplete: a single metric captures some hallucination types but misses others; implemented experiments show that classifiers trained from a single-metric ranking perform worse and correlate less with humans than the assembled mFACT.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics design and data labeling</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>ablation experiments training classifiers from single-metric rankings (DAE-T, QAFE-T, ENFS-T, EntFA-T) versus aggregated mFACT; human-correlation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>compare Pearson and Spearman correlations with human judgements across metrics and ablations; the paper reports mFACT achieving the highest human correlation (Pearson ρ=0.45, Spearman ρ=0.34) compared to single-metric variants</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Using single metrics produced classifiers that correlate less strongly with human judgements and less effective downstream weighting for faithfulness improvements; ensemble (mFACT) yields more robust detectors, improving downstream mitigation (weighted-loss) utility.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently in ablation studies and human-evaluation correlation experiments reported in Appendix A.9 and Table 11.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Different metrics capture complementary types of hallucinations (intrinsic vs extrinsic, span-level vs entity-level), so single metrics are incomplete; natural-language simplification of 'use a faithfulness metric' hides this diversity requirement.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Assemble multiple, diverse English faithfulness metrics to create labels (ensemble averaging) and train a multilingual classifier (mFACT); validate against human judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: mFACT outperformed single-metric-derived classifiers in human-alignment experiments (highest correlations) and produced better downstream improvements when used for loss weighting and model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>evaluation methodology / NLP metrics</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Detecting and Mitigating Hallucinations in Multilingual Summarisation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Translation artifacts in cross-lingual transfer learning <em>(Rating: 2)</em></li>
                <li>On faithfulness and factuality in abstractive summarization <em>(Rating: 2)</em></li>
                <li>QAFactEval: Improved QA-based factual consistency evaluation for summarization <em>(Rating: 2)</em></li>
                <li>DExperts: Decoding-time controlled text generation with experts and anti-experts <em>(Rating: 1)</em></li>
                <li>Contrastive parameter ensembling for reducing hallucination in abstractive summarization <em>(Rating: 1)</em></li>
                <li>Learning with rejection for abstractive text summarization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-426",
    "paper_id": "paper-258841008",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "MT-noise-in-translate-train",
            "name_full": "Machine-Translation Induced Noise in Translate-Train Faithfulness Dataset",
            "brief_description": "Translation errors in the pipeline used to create multilingual faithfulness training data (translate-train) introduce noise and can create or mask hallucinations in target-language examples, misaligning the intended natural-language specification of 'faithful' samples and the actual code-produced training data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "mFACT translate-train pipeline",
            "system_description": "Pipeline that: (1) scores English document-summary pairs with assembled English faithfulness metrics, (2) selects top/bottom k examples, (3) translates these into target languages via a machine translation API, and (4) fine-tunes a multilingual classifier (mBERT) on the translated silver dataset.",
            "nl_description_type": "research paper methods section (metric construction pipeline)",
            "code_implementation_type": "data preprocessing and training scripts (translation + classifier training)",
            "gap_type": "translation-induced data corruption / noisy label propagation",
            "gap_description": "The paper's natural-language description posits that translating top/bottom ranked English examples into target languages yields faithful/hallucinated labeled training data; in implementation, some translated samples were corrupted by MT (introducing hallucinations) and some labels were incorrect due to imperfect English metrics, causing a mismatch between the intended labels and the actual target-language training examples.",
            "gap_location": "data preprocessing / dataset creation (translate-train stage)",
            "detection_method": "manual translation quality check and error analysis (random sample of 100 Chinese positive examples) combined with downstream classifier validation",
            "measurement_method": "manual annotation of 100 translated 'positive' Chinese samples to count translation-caused hallucinations and label-ranking errors; also measured classifier validation/test performance on translated test splits (accuracy/precision/recall/F1 reported for mFACT)",
            "impact_on_results": "Introduced a small but non-negligible amount of noisy labels: in a 100-sample manual check, 13 samples were hallucinated; 4 of those (4% of the checked set) were caused by poor translation and 9 were due to incorrect ranking by English metrics. Noisy translation reduced the ideal purity of the silver dataset and could degrade learned classifier reliability if unaddressed.",
            "frequency_or_prevalence": "Observed in the sanity check for Chinese: 4% translation-caused noise among 100 sampled positives; label-ranking errors (from English metric ensemble) caused a further ~9% of noise in the same sample. More generally, the paper notes that MT noise is 'mostly small' but non-zero and a limiting factor for extending to languages with poor MT.",
            "root_cause": "Propagation of machine translation errors into the training data and reliance on English-only metric labels (teacher models) that are imperfect; absence of native-language annotated faithfulness data and language-specific auxiliary tools.",
            "mitigation_approach": "Manual sanity checks; selecting diverse English teacher metrics to reduce bias; rely on top/bottom k ranking (extreme examples) to reduce ambiguous samples; keep MT-based translate-train but acknowledge limits; propose that resources may constrain extension to other languages.",
            "mitigation_effectiveness": "Sanity check showed only a small fraction of translations introduced hallucinations (4% in checked sample). Aggregating multiple English metrics and selecting extremes (top/bottom k) reduces ambiguous labels relative to random selection, and the resulting mFACT classifiers still achieved high test accuracy (~92–95% across languages).",
            "domain_or_field": "natural language processing / multilingual summarisation",
            "reproducibility_impact": true,
            "uuid": "e426.0",
            "source_info": {
                "paper_title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "NLI-vs-faithfulness-misalignment",
            "name_full": "Misalignment Between Natural Language Inference (NLI) Descriptions and Faithfulness Evaluation",
            "brief_description": "Using NLI-style models or NLI supervision as described in related work does not align with the specific requirements of faithfulness detection for summarisation in practice: NLI yields substantially worse faithfulness classification performance than a metric-trained classifier.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "faithfulness classification baselines",
            "system_description": "Comparison of faithfulness classifiers: (a) multilingual BERT fine-tuned on XNLI (NLI task), (b) XNLI further fine-tuned on translated mFACT data, (c) mFACT classifiers trained on translate-train silver datasets.",
            "nl_description_type": "methodological assumption / baseline description in paper (related work and baselines)",
            "code_implementation_type": "training/fine-tuning scripts for multilingual BERT (NLI and faithfulness datasets)",
            "gap_type": "task-specification mismatch (different task than described)",
            "gap_description": "Prior language in literature suggests NLI models can be repurposed to detect factual inconsistency; however, implemented NLI-based models performed poorly compared to classifiers trained specifically on translated faithfulness labels, indicating the natural-language characterization is overbroad and mismatched to the implemented faithfulness task.",
            "gap_location": "evaluation metrics / classifier training (baseline vs target task)",
            "detection_method": "empirical comparison of classifier performance on translated faithfulness test sets (quantitative experiments reported in Table 1 and Appendix)",
            "measurement_method": "standard classification metrics (accuracy, precision, recall, F1) across six target languages; reported average accuracy: XNLI baseline ~52.9% (±0.7) vs mFACT ~95.3% (±1.7)",
            "impact_on_results": "Relying on NLI baselines would substantially under-detect hallucinations (low recall and F1), producing misleading claims about model faithfulness; the paper shows that NLI-based systems are 'not on par' with dedicated faithfulness classifiers, which affects evaluation validity and model selection.",
            "frequency_or_prevalence": "Observed consistently across six tested languages; the paper reports a large and consistent performance gap between NLI-trained and faithfulness-trained classifiers.",
            "root_cause": "Conceptual conflation in natural-language descriptions between NLI (entailment detection) and summarisation faithfulness (which involves both intrinsic and extrinsic hallucinations and token/span-level phenomena).",
            "mitigation_approach": "Construct a dedicated faithfulness training dataset in the target language (translate-train) using an ensemble of English teacher metrics, and train a dedicated classifier (mFACT) rather than using NLI models.",
            "mitigation_effectiveness": "Highly effective per experiments: mFACT classifiers achieved ~95% accuracy and strong precision/recall/F1 across languages, substantially outperforming XNLI baselines; thus dedicated training aligns code behavior with the intended faithfulness evaluation described in the paper.",
            "domain_or_field": "natural language processing / evaluation metrics",
            "reproducibility_impact": true,
            "uuid": "e426.1",
            "source_info": {
                "paper_title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "checkpoint-selection-misalignment",
            "name_full": "Model Checkpoint Selection Mismatch: ROUGE vs Faithfulness (mFACT) Objectives",
            "brief_description": "Selecting model checkpoints by validation ROUGE (performance-oriented) as described in many summarisation experiments can misalign with faithfulness goals; the paper shows selecting by validation mFACT yields higher faithfulness though ROUGE-based selection is common and described.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "model selection / validation monitoring",
            "system_description": "Training/validation loop where checkpoints are selected based on validation metrics (either ROUGE-1 or validation mFACT) for later evaluation and deployment.",
            "nl_description_type": "experimental protocol / model selection procedure",
            "code_implementation_type": "training and checkpoint selection code (validation scoring and early stopping)",
            "gap_type": "metric-objective mismatch / selection criterion misalignment",
            "gap_description": "The natural-language practice of selecting checkpoints by validation ROUGE (to optimise summarisation performance) does not ensure optimal faithfulness; implemented experiments reveal that selecting by ROUGE can give models with worse faithfulness than selecting by a dedicated faithfulness metric (mFACT), leading to different final behaviors than implied by ROUGE-only descriptions.",
            "gap_location": "training procedure / model selection",
            "detection_method": "ablation experiments comparing final model performance when selecting best checkpoint by validation ROUGE vs by validation mFACT (reported in Appendix A.10 / Table 12)",
            "measurement_method": "compare downstream ROUGE and faithfulness (mFACT) on test sets for models checkpointed by each criterion; also observe trends in validation curves (Figure 5)",
            "impact_on_results": "Selecting by ROUGE often yields higher ROUGE but lower faithfulness; selecting by mFACT increases faithfulness (and can sometimes improve faithfulness without large ROUGE loss). The paper reports that selecting by validation faithfulness 'has a higher positive contribution to model's faithfulness.'",
            "frequency_or_prevalence": "Observed across multiple languages and models in the paper; the selection effect is robust enough that the authors recommend using validation faithfulness when the goal is faithful summarisation.",
            "root_cause": "Using a single performance metric (ROUGE) as a proxy for all desired qualities (including faithfulness) and omission in standard experimental protocols of faithfulness-aware selection criteria.",
            "mitigation_approach": "Use validation faithfulness metrics (mFACT) as selection/early-stopping criterion, or jointly monitor ROUGE and mFACT; report both checkpoints and their trade-offs.",
            "mitigation_effectiveness": "Empirically beneficial: checkpointing by mFACT yields higher final faithfulness scores and better alignment with human judgments (paper notes positive contribution), though trade-offs with ROUGE can vary by language and model.",
            "domain_or_field": "machine learning experimentation / NLP evaluation",
            "reproducibility_impact": true,
            "uuid": "e426.2",
            "source_info": {
                "paper_title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "cross-lingual-transfer-faithfulness-gap",
            "name_full": "Performance vs Faithfulness Trade-off in Cross-lingual Transfer",
            "brief_description": "Cross-lingual transfer methods (e.g., MAD-X or full-model transfer) improve ROUGE summarisation performance in target languages but systematically introduce or amplify hallucinations compared to monolingual English training, revealing a gap between natural-language expectations of transfer benefits and actual faithfulness outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "cross-lingual transfer experimental pipeline (MAD-X and full-model transfer)",
            "system_description": "Adapter-based (MAD-X) and full-model cross-lingual transfer procedures where models are first trained on source-language data (English) and then applied to or fine-tuned for target languages, compared with monolingual fine-tuning.",
            "nl_description_type": "experimental hypothesis / expected outcome described in introduction and experiments",
            "code_implementation_type": "training pipelines for MAD-X adapters and full-model fine-tuning (adapter stacking, task adapter training, zero-shot/few-shot inference)",
            "gap_type": "behavioural mismatch / unintended side-effect of transfer",
            "gap_description": "While methodological descriptions posit that cross-lingual transfer transfers task knowledge beneficially, implemented experiments reveal that transfer also increases the frequency of hallucinations (both intrinsic and extrinsic) in target-language summaries — a qualitative discrepancy between the expected 'benefit-only' description and the implemented effect.",
            "gap_location": "training procedure / transfer stage (source-language pretraining -&gt; target inference)",
            "detection_method": "empirical evaluation: compare ROUGE and multiple faithfulness metrics (DAE, QAFactEval, ENFS%, EntFA) between monolingual fine-tuning and cross-lingual transfer (zero-shot and few-shot); also human evaluation and mFACT scoring",
            "measurement_method": "reporting of ROUGE-1/2/L and multiple faithfulness metrics on XL-Sum for six languages, plus mFACT classifier predictions and human A/B preference tests; observed distributions shown in figures (e.g., Figure 3) and tables (e.g., Table 3) indicate decreased faithfulness after transfer despite improved ROUGE.",
            "impact_on_results": "Cross-lingual transfer increased ROUGE but decreased faithfulness metrics across languages — i.e., better surface performance masks a deterioration in factual consistency. This can mislead evaluations that only report ROUGE and thus harms trustworthiness and reproducibility of claimed improvements.",
            "frequency_or_prevalence": "Observed across six typologically diverse languages (Chinese, Spanish, French, Hindi, Turkish, Vietnamese) and for multiple transfer methods (MAD-X and full-model), indicating this is a general phenomenon in their experiments.",
            "root_cause": "Transfer of task-specific behavior from source-language data carries over spurious correlations or generation tendencies (including hallucination patterns) that are not filtered in target languages; lack of target-language faithfulness supervision during transfer.",
            "mitigation_approach": "Several: (a) expert/anti-expert adapters (TVN, CAPE, DExpert) adapted from monolingual settings; (b) weighted-loss approach that scales each training example's loss by its faithfulness score (mFACT) to downweight hallucinated training examples; (c) checkpoint selection by mFACT.",
            "mitigation_effectiveness": "Weighted-loss approach (WL) achieved consistent gains: improved ROUGE across most languages while achieving comparable or improved faithfulness according to mFACT and human judgments; human A/B tests preferred WL over MAD-X. Expert/anti-expert methods produced mixed and inconsistent gains, sometimes trading ROUGE for faithfulness.",
            "domain_or_field": "multilingual NLP / transfer learning",
            "reproducibility_impact": true,
            "uuid": "e426.3",
            "source_info": {
                "paper_title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "metric-ensemble-bias",
            "name_full": "Bias and Coverage Gap from Relying on Single English Faithfulness Metrics",
            "brief_description": "Relying on a single English faithfulness metric to label or rank training data produces biased coverage of hallucination types; assembling multiple English metrics into an ensemble (mFACT) reduces bias and yields better alignment with human judgments in target languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "mFACT metric construction (ensemble of English metrics -&gt; multilingual classifier)",
            "system_description": "Construction of the mFACT metric by averaging normalized scores from four diverse English faithfulness metrics (DAE, QAFactEval, ENFS%, EntFA), ranking XSum samples, translating extremes, and training a multilingual classifier.",
            "nl_description_type": "metric specification and justification (methods and motivation)",
            "code_implementation_type": "scoring, ranking, translation, and classifier training scripts",
            "gap_type": "single-metric bias / incomplete specification of evaluation coverage",
            "gap_description": "The natural-language description of using an English faithfulness metric 'as is' for other languages is incomplete: a single metric captures some hallucination types but misses others; implemented experiments show that classifiers trained from a single-metric ranking perform worse and correlate less with humans than the assembled mFACT.",
            "gap_location": "evaluation metrics design and data labeling",
            "detection_method": "ablation experiments training classifiers from single-metric rankings (DAE-T, QAFE-T, ENFS-T, EntFA-T) versus aggregated mFACT; human-correlation analysis",
            "measurement_method": "compare Pearson and Spearman correlations with human judgements across metrics and ablations; the paper reports mFACT achieving the highest human correlation (Pearson ρ=0.45, Spearman ρ=0.34) compared to single-metric variants",
            "impact_on_results": "Using single metrics produced classifiers that correlate less strongly with human judgements and less effective downstream weighting for faithfulness improvements; ensemble (mFACT) yields more robust detectors, improving downstream mitigation (weighted-loss) utility.",
            "frequency_or_prevalence": "Observed consistently in ablation studies and human-evaluation correlation experiments reported in Appendix A.9 and Table 11.",
            "root_cause": "Different metrics capture complementary types of hallucinations (intrinsic vs extrinsic, span-level vs entity-level), so single metrics are incomplete; natural-language simplification of 'use a faithfulness metric' hides this diversity requirement.",
            "mitigation_approach": "Assemble multiple, diverse English faithfulness metrics to create labels (ensemble averaging) and train a multilingual classifier (mFACT); validate against human judgements.",
            "mitigation_effectiveness": "Effective: mFACT outperformed single-metric-derived classifiers in human-alignment experiments (highest correlations) and produced better downstream improvements when used for loss weighting and model selection.",
            "domain_or_field": "evaluation methodology / NLP metrics",
            "reproducibility_impact": true,
            "uuid": "e426.4",
            "source_info": {
                "paper_title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Translation artifacts in cross-lingual transfer learning",
            "rating": 2,
            "sanitized_title": "translation_artifacts_in_crosslingual_transfer_learning"
        },
        {
            "paper_title": "On faithfulness and factuality in abstractive summarization",
            "rating": 2,
            "sanitized_title": "on_faithfulness_and_factuality_in_abstractive_summarization"
        },
        {
            "paper_title": "QAFactEval: Improved QA-based factual consistency evaluation for summarization",
            "rating": 2,
            "sanitized_title": "qafacteval_improved_qabased_factual_consistency_evaluation_for_summarization"
        },
        {
            "paper_title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
            "rating": 1,
            "sanitized_title": "dexperts_decodingtime_controlled_text_generation_with_experts_and_antiexperts"
        },
        {
            "paper_title": "Contrastive parameter ensembling for reducing hallucination in abstractive summarization",
            "rating": 1,
            "sanitized_title": "contrastive_parameter_ensembling_for_reducing_hallucination_in_abstractive_summarization"
        },
        {
            "paper_title": "Learning with rejection for abstractive text summarization",
            "rating": 1,
            "sanitized_title": "learning_with_rejection_for_abstractive_text_summarization"
        }
    ],
    "cost": 0.01533675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Detecting and Mitigating Hallucinations in Multilingual Summarisation</p>
<p>Yifu Qiu yifu.qiu@ed.ac.uk 
Institute for Language, Cognition and Computation
University of Edinburgh</p>
<p>Yftah Ziser yftah.ziser@ed.ac.uk 
Institute for Language, Cognition and Computation
University of Edinburgh</p>
<p>Language Technology Lab
University of Cambridge</p>
<p>Anna Korhonen 
Institute for Language, Cognition and Computation
University of Edinburgh</p>
<p>Edoardo M Ponti eponti@ed.ac.uk 
Institute for Language, Cognition and Computation
University of Edinburgh</p>
<p>Shay B Cohen scohen@ed.ac.uk 
Detecting and Mitigating Hallucinations in Multilingual Summarisation
82D2ECA853A6E794D14B91058B4FF8FF
Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation.While automatically generated summaries may be fluent, they often lack faithfulness to the original document.This issue becomes even more pronounced in lowresource languages, where summarisation requires cross-lingual transfer.With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in crosslingual settings is hard.To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics.Through extensive experiments in multiple languages, we demonstrate that mFACT is best suited to detect hallucinations compared to alternative metrics.With mFACT, we assess a broad range of multilingual large language models, and find that they all tend to hallucinate often in languages different from English.We then propose a simple but effective method to reduce hallucinations in cross-lingual transfer, which weighs the loss of each training example by its faithfulness score.This method drastically increases both performance and faithfulness according to both automatic and human evaluation when compared to strong baselines for cross-lingual transfer such as MAD-X.Our code and dataset are available at https: //github.com/yfqiu-nlp/mfact-summ.</p>
<p>Introduction</p>
<p>Recent neural abstractive summarisation models (Lewis et al., 2020;Liu and Liu, 2021;Liu et al., 2022;Fonseca et al., 2022;Ravaut et al., 2022) have shown promise in terms of ROUGE scores (Lin and Och, 2004).However, a well-known problem with these models is hallucination (Maynez et al., 2020;Kryscinski et al., 2020;Laban et al., 2022;Cao et al., 2022a)-generating summaries that cannot be supported by ground-truth knowl-edge (e.g., news, documents, meeting notes).Anecdotally, it was shown that the percentage of generated summaries for CNN/DailyMail (Hermann et al., 2015;See et al., 2017) and XSum (Narayan et al., 2018) containing hallucinations amounts to up to 74.8% and 96.9% (Pagnoni et al., 2021), respectively.Hallucinations hinder the reliability of abstractive summarisation systems by potentially misleading users with the misinformation they produce.</p>
<p>In addition, current summarisation models, opensource or proprietary, struggle in low-resource settings (Parida and Motlicek, 2019;Hasan et al., 2021;Bai et al., 2021;Urlana et al., 2023), when the target language is under-represented (e.g., Vietnamese and Urdu).Fortunately, cross-lingual transfer methods (Pfeiffer et al., 2020b;Xue et al., 2021;Hu et al., 2020) leverage task-specific knowledge learned from a resource-rich source language to summarise documents in many resource-poor target languages, in a zero-shot fashion or only with few annotated examples.Nevertheless, it remains unclear to what extent cross-lingual summarisation suffers from the problem of hallucination, compared to monolingual systems where English is the only language.</p>
<p>The main challenge in addressing this question is that most faithfulness evaluation metrics are available only for English and do not support lowresource languages.Hence, our first contribution (Section 2) is a model-based metric (mFACT) that measures the factual consistency of multilingual conditional generation, obtained from four diverse English faithfulness metrics (Goyal and Durrett, 2021;Fabbri et al., 2022;Cao et al., 2022a) via 'translate train' knowledge transfer (Artetxe et al., 2020).As illustrated in Figure 1, we use existing faithfulness metrics to label the English documentsummary pairs as positive (i.e., faithful) or negative (i.e., hallucinated) and translate them into each target language.We then train a classifier in each We average the score of four English metrics to rank the training samples in XSum.We then translate the most faithful and hallucinated samples into each target language and train a classifier to distinguish them.</p>
<p>target language to predict the faithfulness scores for the translated document-summary pairs.We verify the reliability of mFACT on the translated test set and, most importantly, with human evaluation.These confirm the effectiveness of mFACT in capturing hallucinations in target languages.</p>
<p>Equipped with this new metric, we conduct extensive cross-lingual transfer experiments on XL-Sum (Hasan et al., 2021) for abstractive summarisation in six typologically diverse languages: Chinese, Spanish, French, Hindi, Turkish and Vietnamese.We find that state-of-the-art cross-lingual transfer methods increase summarisation performance in the target languages, but also introduce more hallucinations compared to English monolingual models in comparable experimental settings, thus further exacerbating this tendency (Section 6).</p>
<p>We also employ the mFACT metric to assess the faithfulness of some recently released multilingual large language models (LLMs), including Phoenix, BLOOMZ, and Vicuna (Chen et al., 2023;Muennighoff et al., 2022;Chiang et al., 2023;Le Scao et al., 2022).We show that LLMs that use multilingual data for pre-training or conversational finetuning fail to ensure faithfulness in summarisation in various languages, producing more hallucinations in low-resource ones.</p>
<p>To overcome this limitation and promote faithful summarisation in multiple languages, we adapt a series of existing methods for reducing hallucinations originally devised for monolingual summarisation (Section 3.2).In addition, we introduce a novel, simple but effective method (Section 3.3): we weigh the loss for each training example according to their faithful scores.We evaluate our loss-weighting method with automated metrics and human judgements.We observe significant gains in both summarisation performance and faithfulness over a series of strong baselines (Section 8).In a nutshell, our main contributions are the following: • We propose mFACT, a multilingual faithful metric developed from four English faithfulness metrics.This enables detecting hallucinated summaries in languages other than English.• To the best of our knowledge, we are the first to study hallucination in a cross-lingual transfer setting.We show that state-of-the-art methods like MAD-X (Pfeiffer et al., 2020b) can improve the performance for low-resource summarisation, but also amplify hallucinations.• We apply mFACT to study the faithfulness in summarisation of the recent multilingual Large Language Models.We observe that despite their scale, these models are still struggling to reduce hallucinations for languages other than English.• We propose a novel method to enhance faithfulness and performance in cross-lingual transfer for summarisation, which consists of weighting training samples' loss based on their faithfulness score.Both automatic and human evaluations validate the superiority of our method over existing baselines.</p>
<p>mFACT: A Multilingual Metric for Faithfulness</p>
<p>metrics into any target language, given the availability of a machine translation model.</p>
<p>Translation-based Transfer for Faithfulness Metrics</p>
<p>One straightforward way to implement a faithful metric in any target language is by implementing it from scratch following the design of monolingual English metrics.However, these often rely on data annotated with auxiliary language-specific tools.For instance, Dependency Arc Entailment (DAE; Goyal and Durrett 2021) requires an external dependency parser to label fine-grained hallucinated segments.This is impractical due to the lack of annotated data and auxiliary tools in most languages.Another strategy relies on "translate test" knowledge transfer (Artetxe et al., 2020), where test documents and their corresponding generated summaries are translated from the target language to English.Then, English metrics can measure faithfulness; however, this introduces noise from translation and is costly at inference time, which makes this unsuitable for model development.For instance, model selection is commonly based on early stopping according to validation faithful scores (Choubey et al., 2021;Aharoni et al., 2022), which necessitates translating all generated summaries at each validation step.</p>
<p>Our solution instead is to formulate faithfulness evaluation as a binary classification problem, i.e., to predict whether a given document-summary pair is faithful or hallucinated.In other terms, our proposed approach aims to distil knowledge from multiple teacher models, i.e., existing English model-based metrics, into a target-language classifier as a student model.Specifically, we use multiple English faithful metrics to assign the pseudo labels of "faithful" or "hallucinated" for English document-summary pairs, then translate them to create a faithfulness binary classification dataset in target languages.We then train the target-language classifier on the resulting silver dataset.Formally, we aim to obtain a faithfulness scoring model g(•) in target language tgt that predicts the faithfulness for a given document-summary pair (x, y).Hence g (tgt) (x (tgt) , y (tgt) ) ≜ p(z = 1 | x (tgt) , y (tgt) ) where z = 1 and z = 0 represent whether the pair is faithful or hallucinated, respectively.</p>
<p>The pipeline for creating mFACT is presented in Figure 1.We start with four diverse English faith-fulness metrics1 , and use them to score the training samples from the English XSum summarisation dataset (Narayan et al., 2018).Following Maynez et al. (2020), we select the metrics based on two categories of hallucinations generated by the model: 1) intrinsic hallucinations where the summary distorts the information present in the document; 2) extrinsic hallucinations where the model adds information that cannot be directly supported by the document.We select two model-based metrics capturing intrinsic hallucinations:</p>
<p>• DAE (Goyal and Durrett, 2021) which consists in an entailment classifier trained with annotation at a fine-grained dependency level; • QAFactEval (Fabbri et al., 2022) which focuses on generating questions whose answer is a span of the summary, and attempts to answer them based on the document alone; and two metrics for extrinsic hallucinations:</p>
<p>• ENFS% is a simple rule-based measurement presented by Cao et al. (2022a), which counts the proportion of entities which appear in a summary but not in its corresponding document.• EntFA (Cao et al., 2022a) which estimates the posterior and prior probabilities of generated entities with language models conditioned (or not conditioned, respectively) on the source document.Using these probabilities as features, a KNN classifier detects token-level hallucination.We chose XSum as the source English dataset because 1) all our selected faithfulness metrics are trained on XSum, which allows us to maximise the reliability of these metrics.2) XSum has been shown to include abundant and diverse hallucinations (Maynez et al., 2020;Pagnoni et al., 2021), which allows our metrics to capture as many types of hallucinations as possible.</p>
<p>We then normalise the scores from the abovementioned four metrics between [0, 1] and average them for each training sample.We rank the samples from the most faithful to the most hallucinated according to the resulting faithfulness scores.The k top-ranked and k bottom-ranked documentsummary pairs are then considered positive and negative examples, respectively.We translate these into a series of target languages with the Google Translation API2 and create our silver faithfulness dataset splitting its examples with a proportion of 95/2.5/2.5 as the training/validation/testing sets.</p>
<p>Finally, a multilingual BERT-based classifier is fine-tuned on our dataset.We follow the sentencepair classification setting from (Devlin et al., 2019) to concatenate the document-summary pairs as the input.A classifier receives the last-layer representation for the [CLS] special token and returns a score between 0 (hallucinated) and 1 (faithful).</p>
<p>Reducing Hallucination in Cross-lingual Transfer</p>
<p>We first provide some background on cross-lingual transfer.Then, we show how to adapt several methods promoting faithfulness in monolingual summarisation to cross-lingual transfer settings.Finally, we describe a new approach based on loss weighting.</p>
<p>Cross-lingual Transfer with MAD-X</p>
<p>We adopt the Multiple ADapters framework (MAD-X; Pfeiffer et al. 2020b), which constitutes a stateof-the-art method for cross-lingual transfer.MAD-X learns independent language and task adapters (i.e., parameter-efficient model fine-tunings), and then combines them.Specifically, to transfer the ability to summarise documents from a source language to a target language, we follow these steps: 1) We train two separate language adapters on the Wikipedia corpora for both the source and target languages.2) We stack the (frozen) source language adapter with a randomly initialised task adapter and train the latter with annotated data in the source language.3) We stack the trained task adapter with the target language adapter and then perform zero-shot inference in the target language.</p>
<p>Expert and Anti-Expert Approaches</p>
<p>The majority of strategies to reduce hallucinations in monolingual settings rely on creating experts or anti-experts that steer the model towards positive behaviour or away from negative behaviour.As a by-product of the pipeline to create our metric, mFACT (Section 2), we obtained two separate subsets of faithful and hallucinated samples in both source and target languages.These subsets can serve as training data for experts/anti-experts in multiple languages, thus making them suitable for cross-lingual transfer.We explore three methods in this family.In all in stances, we first train a base adapter with the source summarisation dataset.Then, we further tune it with the faithful (hallucinated) subset to obtain an expert (anti-expert)</p>
<p>adapter.</p>
<p>Task Vector Negation (TVN; Ilharco et al. 2022).Task vector negation mitigates hallucinated generation by subtracting the task vector of the antiexpert model from the fine-tuned model.Formally, given a fine-tuned model with parameter θ 0 and an anti-expert model θ − , the interpolated model parameters θ ⋆ are obtained as
θ ⋆ = θ 0 − λ(θ − − θ 0 ), (1)
where λ is the importance hyperparameter that controls the degree of fusion between the fine-tuned model and the anti-expert.</p>
<p>Contrastive Parameter Ensembling (CAPE; Choubey et al. 2021).To compensate for the potential loss of summarisation ability by only subtracting the anti-expert task vector from the base model, CAPE proposes to also add the expert parameters.Formally, the interpolated model parameters θ ⋆ are obtained as:
θ ⋆ = θ 0 + λ(θ + − θ − ),(2)
where λ again is the importance hyperparameter.</p>
<p>DExpert Decoding (Liu et al., 2021).Contrary to Task Vector Negation and CAPE, which directly manipulate the model parameters, DExpert uses expert and anti-expert models to modify the predicted logits at each decoding step.Given the base model f θ and a pair of expert f θ + and anti-expert f θ − models, the scores for the next token at each decoding step t are:
p(y t |x, y &lt;t ) = softmax(z t + λ(z + t − z − t )),(3)
where z t , z + t , z − t are the outputs from f θ , f θ + , f θ − at time step t, respectively.Again, an importance hyper-parameter λ controls the degree of fusion during decoding.</p>
<p>Weighted Loss Approach</p>
<p>We also introduce a simple but effective approach to reduce hallucination during cross-lingual transfer.Previous works have shown that controlling the quality of the training samples can improve the model's faithfulness (Kang and Hashimoto, 2020;Aharoni et al., 2022).However, simply filtering out hallucinated training data may sacrifice the summarisation performance (Dziri et al., 2022).</p>
<p>We thus propose a "soft" data filtering approach where we weigh the training loss according to each sample's faithfulness score.More formally, we rely Model Acc.</p>
<p>Prec.Recall F1
µ σ µ σ µ σ µ σ
XNLI 52.9 0.7 71.6 3.3 8.4 1.5 15.0 2.5 X.-mF 95.0 2.3 95.7 2.4 94.3 2.2 94.9 2.3 mF-T 64.2 6.2 98.9 0.9 28.3 12.9 42.5 16.9 mF 95.3 1.7 95.4 1.8 95.1 2.0 95.2 1.8</p>
<p>Table 1: Mean values (µ) and standard deviations (σ) of the test performance of four faithfulness classifiers over six target languages.X.-mF, mF-T, mF stand for XNLI-mFACT, mFACT-Transfer and mFACT, respectively.The detailed results for each language are given in Appendix A.5.</p>
<p>on a faithfulness metric for the source language, which outputs a score z (i) for the i th documentsummary pair's faithfulness.Then the update rule of training parameters for each batch becomes
θ * = θ − α 1 m m i=1 z (i) ∇ θ J(x (i) , y (i) ; θ) ,(4
) where θ is the vector of trainable model parameters, α is the learning rate, m is the batch size, J(•; θ) is the loss function for a single training example (x (i) , y (i) ), and ∇ θ J(•) is the gradient of the loss function wrt. the model parameters.</p>
<p>Experimental Setup</p>
<p>Evaluation Metrics.We use ROUGE-1/2/L scores (Lin and Och, 2004) to evaluate the task of abstractive summarisation.We use the four metrics mentioned in Section 2 to evaluate the faithfulness of English summaries and our mFACT metric for summaries in other languages.Dataset.We conduct our experiments on XL-Sum, which is a large-scale multilingual summarisation dataset (Hasan et al., 2021).XL-Sum provides a large collection of annotated document-summary pairs in 45 languages in addition to English.We test our approach on six target languages: Chinese, Spanish, French, Hindi, Turkish and Vietnamese.Table 7 shows the dataset statistics.</p>
<p>Faithfulness Classification Experiments</p>
<p>Classification Results</p>
<p>Firstly, we verify the reliability of mFACT by using our translated test sets in multiple languages to benchmark mFACT and several baselines for faithfulness classification.Baselines.Previous works (Maynez et al., 2020;Kryscinski et al., 2020) showed that models train for natural language inference (NLI), a related task for which more annotated data is readily available, can be used also for assessing faithfulness for English summarisation.We thus include a baseline, namely XNLI, which consists in fine-tuning multilingual BERT with the corresponding language split in the XNLI dataset (Conneau et al., 2018).As an alternative, we further fine-tune the XNLI baseline with our translated data (XNLI-mFACT), thus verifying whether combining the supervision signal from both sources boosts the performance.Finally, we incorporate an ablation study for using zeroshot multilingual transfer instead of "translate train" (Artetxe et al., 2020).In mFACT-Transfer, we train a multilingual encoder on our English faithfulness classification dataset without translating it, then deploy it directly on examples in other languages.</p>
<p>Results and Discussion.We report the classification performance in Table 1.We find that NLI classifiers do not achieve a level of performance on par with classifiers trained on our faithfulness classification dataset.This demonstrates that evaluating faithfulness is indeed distinct from NLI, which is consistent with previous findings in assessing faithfulness in English (Kryscinski et al., 2020;Maynez et al., 2020).Comparing mFACT and mFACT-Transfer, we also observe the positive effects of translation-based transfer, which achieves a much higher recall rate than zero-shot cross-lingual transfer.Hence, mFACT is more likely to identify faithful document-summary pairs as such.</p>
<p>External Evaluation by Inverse Transfer</p>
<p>Finally, we conduct an evaluation based on inverse cross-lingual transfer (i.e., from other languages to English) as a downstream task with our newly intro-duced approach (Section 3.3).This setting allows us to compare the impact of using different multilingual faithfulness metrics, among those listed in Section 5.1, to weigh the training samples in target languages.The logic behind this experiment is that if the scorer captures the model's faithfulness in target languages, the English summaries generated by the corresponding model should be more faithful according to the four English metrics from Section 2.1.</p>
<p>The results are shown in Table 2. Unsurprisingly, we observe that in general weighting the training samples in target languages with faithfulness metrics can achieve considerable improvements over the MAD-X baseline on English faithfulness scores.This suggests that these metrics are well aligned with the actual faithfulness of generated summaries.Specifically, comparing mFACT-Ours and mFACT-Transfer methods with XNLI and XNLI+mFACT, we find that our constructed dataset is much more effective in improving faithfulness than NLI signal, which again verifies our previous assumption that faithfulness classification and NLI are only vaguely related.Finally mFACT-Transfer performs worse than mFACT in ROUGE, which can be caused by the much lower recall rate of mFACT-Transfer in faithfulness classification (see Table 1).</p>
<p>Cross-lingual Transfer Introduces Additional Hallucinations</p>
<p>The second analysis of this paper aims to corroborate our observation that cross-lingual transfer can introduce additional hallucinations over monolingual fine-tuning, though it improves the task performance for summarisation in the target language.</p>
<p>Transfer Setup.We compare two data scenarios and two styles of fine-tuning.To begin, we investigate the impact of initial training on source data, followed by applying few-shot learning techniques on target data (cross-lingual transfer) instead of direct application.We attribute the difference in faithfulness scores to the additional hallucinations introduced by the training phase in the source language.Taking Chinese as an example of few-shot cross-lingual transfer, we train the summarisation model first on XL-Sum (Chinese) and then with 1K randomly sampled XSum (English) examples.Secondly, we compare fine-tuning the full model, where all parameters are updated, with parameterefficient fine-tuning, where only the adapters are updated.This allows us to study the effect of dif- ferent transfer methods on faithfulness.</p>
<p>Results and Discussion</p>
<p>In Figure 3, we observe that cross-lingual transfer improves ROUGE scores for both full-model fine-tuning and MAD-X, outperforming monolingual fine-tuning.This underscores its effectiveness in transferring task-specific knowledge from source to target languages in lowresource scenarios.However, it's important to note that leveraging source language data can also increase hallucination in both cases.</p>
<p>Hallucinations in Multilingual Large Language Models</p>
<p>We also assess the summarisation performance of recent multilingual large language models (LLMs) on XL-Sum in Table 4: Automatic evaluation for zero-shot crosslingual transfer performance from English to other languages when selecting the checkpoint with the best validation mFACT.Numbers represent the average of three runs with different random seeds.mF stands for mFACT and mF-T stands for mFACT-Transfer.bi% and tr% stand for the percentages of novel bigrams and trigrams.</p>
<p>BLOOMZ with an additional 267K and 189K instances of multilingual instructions and conversation rounds.</p>
<p>We select three languages, aside from English, which are present in the pre-training data for BLOOMZ and the conversational tuning data for Vicuna and Phoenix.We also report the percentage of examples in each of these languages that these models have been exposed to during their Lang.</p>
<p>P. (%) R-1 R-L mF.Table 5: Assessing the multilingual summarisation performance for Vicuna-7B, Phoenix-7B, and BLOOMZ-7.1Bon four languages (Lang.)using ROUGE-1/L (R-1/L) and mFACT (mF.) metrics.We also report the percentage (P.(%)) of samples in each language an LLM was exposed to during their multilingual training.</p>
<p>Phoenix</p>
<p>multilingual training.Table 5 demonstrates that current LLMs display notable faithfulness limitations in cross-lingual transfer contexts for languages beyond English, including well-resourced languages like French and Spanish.Furthermore, a noticeable trend emerges: LLM faithfulness across languages tends to correlate highly to the number of samples from target languages observed during their training.These observations align with recent findings (Lai et al., 2023;Laskar et al., 2023) which highlight the challenges in maintaining faithfulness while generating content in low-resource languages.</p>
<p>Reducing Hallucinations</p>
<p>In this section, we test different methods for crosslingual transfer of summarisation to multiple languages and for promoting faithfulness.We compare our new method of loss weighting based on mFACT with MAD-X, as well as with a series of approaches for reducing hallucinations (Section 4).We evaluate these methods with automated metrics for performance, faithfulness, and abstractiveness (i.e., the ability to rephrase the document instead of copy-pasting spans of text).We also conduct human evaluations to corroborate these results.Automatic Evaluation.We report ROUGE scores for performance, faithfulness (mFACT), and abstractiveness (novel bigrams and trigrams in the summary) for the test set of each target language in Table 4.We first observe that the expert/anti-expert methods adapted from monolingual summarisation are partly effective for improving ROUGEs and mFACT score in cross-lingual transfer over MAD-X; however, no clear winner emerges among them, as their gains are marginal or inconsistent.For example, TVN produces the most faithful summaries for Hindi and Vietnamese, CAPE for Turkish, and DExpert for French.All three models, however, display a similar trend of sacrificing ROUGE scores to improve faithfulness.Instead, as Table 4 demonstrates, our proposed weighted-loss approach (WL) improves the performance across the board while achieving a comparable mFACT score with the most faithful expert models.In particular, WL achieves the best faithfulness in Chinese and Spanish and the best ROUGE scores for all languages except Hindi.These results suggest that our weightedloss method strikes the best balance between summarisation abilities and faithfulness.</p>
<p>Abstractiveness.We also measure the levels of ab-stractiveness of different methods, which is known to be inversely correlated with faithfulness (Ladhak et al., 2022;Daheim et al., 2023).In fact, reducing hallucinations has the side effect of encouraging the model to copy-paste spans of the document (i.e., acquiring an extractive behaviour).Following Cao et al. (2022a) and See et al. (2017), we use the percentage of novel n-grams in summaries compared with the document as a measure of abstractiveness.Figure 3 illustrates the distributions of abstractiveness and faithfulness for all models in six XL-Sum datasets.Both positive and negative predictions of mFACT scatter with different levels of abstractiveness.We also observe that summaries generated by the weighted loss method generally have a higher level of abstractiveness when they are similarly faithful compared with other baselines.Table 4 shows most expert/anti-expert models sacrifice abstractiveness to improve faithfulness score.In contrast, the weighted loss approach produces more novel n-grams.These findings show that our method does not improve faithfulness by simply favouring extractive summaries.Human Evaluation.Finally, we recruited human annotators from the Prolific platform3 for a blind comparison between MAD-X and our weighted-loss model.We randomly sampled nine documents for each language and paired them with the summaries generated by the two models.We asked the human participants to evaluate the summaries via A/B testing in two aspects, Informativeness: An informative summary should cover as much information from the document as possible, while it should convey the main idea of the document.Faithfulness: A faithful summary should only contain information already present in the document 4 and should not contain information contradicting the document.Participants will first read the document, then select the better summary (or both, if they are similar) in terms of informativeness and faithfulness (see Appendix A.5).We require participants to be native speakers of the language they evaluate and have obtained at least a bachelor's degree.Each document and its paired summaries are evaluated by 3 participants.These settings allow us to achieve a fair inter-rater agreement of 0.28 in terms of Fleiss' κ (Landis and Koch, 1977).</p>
<p>The results in Figure 2 indicate that human evaluators prefer the summaries generated by our weighted loss method rather than MAD-X, demonstrating that our weighted loss approach improves faithfulness and informativeness for all six languages.</p>
<p>Finally, we study the correlation between the human preferences from Figure 2 and various faithfulness metrics presented in Section 5.1.From Table 6, it emerges that mFACT achieves the strongest correlation with human judgements (0.45 Pearson ρ and 0.34 Spearman ρ), which is statistically significant.In comparison with XNLI and XNLI-mF, we reconfirm that metrics designed for faithfulness classification, rather than natural language inference, more effectively align with human preferences.</p>
<p>Related Work</p>
<p>While faithfulness in summarisation is a highly researched topic, previous works focused mostly on the English language (Pagnoni et al., 2021;Maynez et al., 2020;Fabbri et al., 2021).To evaluate faithfulness, state-of-the-art methods fall into three categories.Firstly, validating faithfulness can be cast as a classification problem (Kryscinski et al., 2020;Goyal and Durrett, 2021;Laban et al., 2022).Secondly, faithfulness can be interpreted as answerability, and assessed with existing question answering models (Fabbri et al., 2022;Scialom et al., 2021).Finally, language models may be adopted to identify extrinsic hallucinations (Filippova, 2020;Cao et al., 2022a).To improve faithfulness in summarisation models, an approach related to ours changes the training dynamics, e.g. by filtering out hallucinated data (Cao et al., 2022b;Kang and Hashimoto, 2020;Goyal et al., 2022).The expert/anti-expert approach aims to learn experts and anti-experts that alter the behaviour of a base generative model (Liu et al., 2021;Choubey et al., 2021;Ilharco et al., 2022).Other methods include designing the specific neural architecture (Huang et al., 2020;Qiu and Cohen, 2022;Cao et al., 2018), summary ranking (Falke et al., 2019;Liu et al., 2022) and posthoc correction (Zhu et al., 2021;Dong et al., 2020;Cao et al., 2020;Zhao et al., 2020).However, there is still a limited understanding of the effectiveness of these methods in cross-lingual transfer.</p>
<p>Conclusion</p>
<p>We investigate how to measure and mitigate hallucinations of summarisation models in cross-lingual transfer scenarios.We first propose a multilingual metric, mFACT, to facilitate the evaluation of faithfulness in low-resource languages.By virtue of this new metric, we find empirical evidence that while common cross-lingual transfer methods benefit summarisation performance, they amplify hallucinations compared to monolingual counterparts.We also point out that faithfulness in summarisation for languages other than English is still challenging for multilingual large language models.Finally, with the aim of reducing these hallucinations, we adapt several monolingual methods to crosslingual transfer and propose a new method based on weighting the loss according to the mFACT score of each training example.Based on both automated metrics and human evaluation, we demonstrate that mFACT is the most reliable metric in detecting hallucinations in multiple languages.Moreover, compared to a series of state-of-the-art baselines, we find that summaries produced by loss weighting achieve higher performance and abstractiveness, competitive faithfulness, and a higher alignment with human preferences.We hope that this work will attract more attention from the community to the phenomenon of hallucination in languages different from English and facilitate future research by establishing evaluation metrics and baselines.</p>
<p>Limitations</p>
<p>We use machine translation to construct the faithfulness classification dataset for training the faithfulness metrics in target languages.The required resources may constrain the feasibility of extending mFACT to other languages.The quality of the learned metrics may also be limited by the propagation of errors during translation, especially for languages with poor translation performance.Additionally, although the weighted-loss approach is effective in a diverse sample of languages, we note that its gains in faithfulness are not consistent for all languages, as we discussed in Section 8. Finding a method that is equally effective in reducing hallucinations across all languages is still an open research question for future work.</p>
<p>Ethical Consideration</p>
<p>All human workers participating in our evaluation are informed of the intended use of the provided assessments of summary quality and comply with the terms and conditions of the experiment, as specified by Prolific.In regards to payment, workers from different regions are paid on the same high scale with a wage of £13.5 hourly.This work (and specifically, the human evaluation) has also passed an ethical review by the ethical panel in our institute.</p>
<p>A Appendix</p>
<p>A.1 Dataset Statistics</p>
<p>We show the dataset statistics for all six used subsets of XL-Sum in table 7.</p>
<p>A.2 Implementation Details.</p>
<p>mFACT Classifiers We implement mFACT with the transformers package (Wolf et al., 2020).We train the multilingual BERT model for two epochs, with a batch size of 32 and a learning rate of 5e-5.We set the max input length to 512 and apply truncation to the input article if necessary.The same hyper-parameter settings are applied to all the languages we test.Weighted Loss Summarisation Models We implement our weighted loss model for cross-lingual transfer with adapter-transformers package (Pfeiffer et al., 2020a).We use the officially released mBART-50 checkpoint as the base model for equipping language and task adapters.</p>
<p>To train the language adapters, we follow the same adapter architecture and training settings in (Pfeiffer et al., 2020b).We use the batch size of 64, and a learning rate of 1e-4.We train each adapter with 48K update steps.Task Adapters To train the task adapters for summarisation, we set the batch size to 32, the learning rate to 1e-4, label smoothing factor to 0.1.We use the polynomial scheduler for adjusting the learning rate during training, with weighted decay at 0.01 and maximum gradient norm at 0.1.The model is trained for ten epochs, and we set the first 500 update steps as the warm-up stage.We select the best checkpoint following either the best validation ROUGE or the best mFACT score, respectively.During the decoding step for zero-shot cross-lingual transfer, we follow most settings of (Hasan et al., 2021).We apply the beam search with a size of 6, and the minimum/maximum decoding steps are set to 30/84, respectively.The length penalty is applied at 0.6, and we block all repeated tri-grams.</p>
<p>A.3 Sanity Check for English Faithfulness Metrics</p>
<p>We perform a sanity check experiment and report the results in Table 9 to verify the reliability of these model-based hallucination metrics.We randomly shuffle the alignments of document-summary pairs predicted by the mBART model and the reference.We then feed these misaligned document-summary pairs into the evaluation models and test their performance.We observe that all hallucination metrics drop considerably, showing that these metrics are indeed sensitive to random summaries and reliable to some extent.</p>
<p>A.4 Translation Quality Check</p>
<p>Our first experiment is to confirm the effectiveness of mFACT in capturing hallucinations in target languages.To support our method, we conduct a quality check for translation outputs, a comparison of different metrics on our translated faithfulness classification dataset, and an external evaluation of downstream tasks.</p>
<p>Machine translation (MT)-based transfer can arguably suffer from error propagation, where MT tools introduce hallucinations into their outputs.This issue is even more serious in our setting where translating faithful samples is necessary to create the mFACT metric as training with false positives might significantly degrade its quality.To ensure the feasibility of our pipeline to develop mFACT, we first check the translation quality manually.We randomly pick 100 samples from the Chinese positive set and label their faithfulness.Through this sanity check, we found 13 hallucinated samples; however, only 4 of them are caused by poor translation, while the other 9 are due to an incorrect ranking based on the four English metrics.This shows that MT-based transfer is mostly reliable: only a small amount of noise is introduced by MT.</p>
<p>A.5 Extended Results for Faithfulness Classification</p>
<p>To gain a deeper comprehension of the averaged faithfulness classification results presented in add a reference to Table 1, we analyse the individual language-specific outcomes (Table 10).Across the six language experiments, we consistently observe a significant performance gap between the models trained on the NLI task and those trained on the faithfulness classification task.</p>
<p>The following is the guide for annotators to indicate whether a summary is informative and faithful.</p>
<p>A.6 Full-model transfer vs. MAD-X transfer</p>
<p>We conduct a comparative study on the performance of summarisation and faithfulness in two cross-lingual transfer approaches: MAD-X style and full-model transfer.</p>
<p>For both MAD-X style and full-model crosslingual transfer, we observe that cross-lingual trans-</p>
<p>A.8 Prompts Used for Multilingual LLM's Summarisation</p>
<p>We show the prompt templates used for all languages in our LLM's summarisation experiments in Figure 6.</p>
<p>A.9 Assembling Metrics for mFACT does better than Single Metric</p>
<p>We conducted an additional experiment to support our assembling design of mFACT.Rather than averaging four metrics, we individually apply single English metric -DAE, QAFE, ENFS, and EntFA -to rank the XSum dataset and train a multilingual classifier similar to mFACT-Transfer without translation, denoted as DAE-T, QAFE-T, ENFS-T, and EntFA-T.</p>
<p>To examine mFACT with other metrics originating from each single metric, we extend the human evaluation results in Table 6.We compare these four metrics with mFACT-Transfer, and again we measure the Pearson and Spearman correlations to human annotations.</p>
<p>In Table 11, we find mFACT consistently  emerges with the highest human correlation when compared to other four metrics.This observation underscores mFACT's better correlation with human evaluations.The reason could be relying on a single metric can introduce biased preference in models and a lack of diversity for captured hallucinations.In general, multiple teacher models lead to a robust, unbiased process (Wu et al., 2021;Ilichev et al., 2021)</p>
<p>A.10 Strategy for Selecting Best Model Checkpoint</p>
<p>Table 12 compares the summarisation model performance when we select the model checkpoint with the best ROUGE-1 or the best mFACT score.We find that under both strategies, the weighted loss model can achieve better ROUGE and faithfulness scores in most languages.However, similar to other works (Choubey et al., 2021;Aharoni et al., 2022), selecting the model checkpoint with the best validation faithfulness score has a higher positive contribution to model's faithfulness.</p>
<p>A.11 Distributions of Faithfulness and Abstractiveness for All Languages</p>
<p>We show the distributions for the percentage of novel 2-grams and mFACT scores for all six languages in Figure 7.</p>
<p>Figure 1 :
1
Figure1: Pipeline of mFACT for transferring English faithfulness metrics to target languages via machine translation.We average the score of four English metrics to rank the training samples in XSum.We then translate the most faithful and hallucinated samples into each target language and train a classifier to distinguish them.</p>
<p>Figure 3 :
3
Figure 3: Distributions for Novel 2-gram% and mFACT scores for all five hallucination reduction methods in cross-lingual transfer for the datasets of 6 languages in XL-Sum.</p>
<p>Figure 5 :
5
Figure 5: Validation mFACT scores curve for each model's training dynamics.Weighted loss consistently outperforms MAD-X in terms of faithfulness during the whole training period.</p>
<p>Table 3 :
3
Performance and faithfulness scores for fewshot cross-lingual transfer (CLTF) and monolingual finetuning (MFT) on abstractive summarisation.CLTF generally improves the model's performance but decreases its faithfulness.↑ and ↓ indicate higher or lower values are better, respectively.
MetricsMAD-X MFT CLTF MFT CLTF Full ModelPerform.R-1 R-2 R-L30.25 30.96 23.96 32.05 8.57 9.11 5.9 9.69 22.48 23.14 17.96 23.85Faithful.DAE (↑) QAFE (↑) ENFS% (↓) 33.29 35.07 16.82 41.45 68.17 66.69 84.33 53.24 34.44 33.87 63.52 30.98 EntFA (↑) 87.58 87.87 95.14 82.96</p>
<p>Table 5
5. We carefully select threerepresentative multilingual LLMs for investigation,• BLOOMZ-P3-7.1B (Muennighoff et al., 2022) represents the instruction-tuned model with En-glish P3 dataset, which derives from the multilin-gual BLOOM (Le Scao et al., 2022). We decidenot test BLOOMZ-xP3 trained with machine-translated instructions from English P3 becausewe consider this experiment as an assessment tothe cross-lingual transfer capabilities from themultilingual model.• Vicuna-7B (Chiang et al., 2023) harnesses 70K multilingual conversation-style interactions tofine-tune LLaMA. Vicuna originates from themonolingual LLaMA, and the inclusion of Vi-cuna aims to test the cross-lingual transfer abilityarising from multilingual conversational tuning.• Phoenix-7B (Chen et al., 2023) is the cur-rent state-of-the-art, which continues to train</p>
<p>Table 6 :
6
Correlation between several faithfulness metrics and human preferences.mF and mF-T stand for mFACT and mFACT-Transfer, respectively.We calculate both Pearson and Spearman statistics on documentsummary pairs from all six languages to ensure that the sample size is significant.
ChineseSpanishFrench Hindi TurkishInform.VietnameseChineseSpanishFrench Hindi TurkishFaithful.Vietnamese0.00.20.40.60.81.0MAD-XSimilar/Non-differentiableWLFigure 2: Human preferences for summaries in terms of Inform[ativeness] and Faithful[ness]. Annotators could choose MAD-X, weighted loss (WL, ours), or non-differentiable (if summaries are too similar).ρPearson p value ρSpearman p valueXNLI0.220.100.250.07XNLI-mF 0.25 mF-T 0.44  *  mF 0.45  <em>0.07 0.00 0.000.28  *  0.36  *  0.34  </em>0.04 0.01 0.01</p>
<p>Table 10 :
10
Classification performance on our translated faithfulness dataset for all target languages.
ModelsAcc. Prec. Recall F1SpanishXNLI XNLI-mFACT mFACT-Transfer 70.60 98.10 41.53 54.00 76.47 10.48 96.40 97.13 95.5618.44 96.34 58.36mFACT97.00 97.55 96.3796.96XNLI53.00 74.07 8.0614.55HindiXNLI-mFACT mFACT-Transfer 54.00 100 94.20 94.33 93.95 7.2694.14 13.53mFACT94.00 95.04 92.7493.88TurkishXNLI XNLI-mFACT mFACT-Transfer 59.60 100 52.40 69.23 7.26 96.60 97.53 95.56 18.5513.14 96.54 31.29mFACT97.00 97.17 96.7796.97VietnameseXNLI XNLI-mFACT mFACT-Transfer 68.00 97.83 36.29 53.40 72.73 9.68 96.00 96.72 95.16 mFACT 95.40 94.47 96.3717.08 95.93 52.94 95.41FrenchXNLI XNLI-mFACT mFACT-Transfer 65.80 98.73 31.45 52.60 67.74 8.47 96.20 96.73 95.5615.05 96.15 47.71mFACT95.60 95.20 95.9795.58ChineseXNLI XNLI-mFACT mFACT-Transfer 67.40 98.85 34.67 52.20 69.57 6.45 90.80 91.39 89.9111.80 90.65 51.34mFACT92.60 92.71 92.3492.53</p>
<p>Figure 4: Comparison of Full-model and MAD-X crosslingual transfer in ROUGE and faithfulness.The left column is the zero-shot performance, and the right column is the few-shot performance.We provide the average scores over all six languages.
20 22 18 5.222.49 22.745.10Model Full Model MAD-X 16.43 16.5132.5 32.05 30.96 30.0 27.5 25.0 10.00Model Full Model MAD-X 23.85 23.145.04.959.50 9.759.694.89.259.11100Rouge-1Rouge-2 Rouge-L9.00 100Rouge-1Rouge-2 Rouge-L40 50 60 70 80 90DAE QAFE 1-ENFS EntFA 54.17 50.87 51.31 70.91 73.65 76.27 85.16 92.03 Model Full Model MAD-X30 40 50 60 70 80 90DAE QAFE 1-ENFS EntFA 30.98 33.87 53.24 58.55 64.93 66.69 Model 87.87 Full Model 82.96 MAD-XZero-shot TransferFew-shot Transfer0.50ModelVal. mFACT-English0.42 0.44 0.46 0.48MAD-X Weighted Loss246810Epoch</p>
<p>Table 11 :
11
. Using diverse metrics in mFACT's training helps the classifier detect various hallucination types -our inverse transfer experiments (Table2) also show mFACT's promising correlations with both intrinsic and extrinsic hallucination metrics.Correlation with human preferences for mFACT and four transferred metrics developing from single metric.We again calculate both Pearson and Spearman statistics on document-summary pairs from all six languages to ensure that the sample size is significant.
LanguagePromptEnglishSummarize the given article: ${en_article}Chinese总结给定的文章: ${zh_article}FrenchRésumer l'article donné: ${fr_article}SpanishResume el artículo dado: ${es_article}Hindiिदए गए ले ख को सं क्षे प में प्रस्तु त करें : ${hi_article}
The lack of faithful metrics in languages other than English greatly limits the evaluation (and hence, the prevention) of hallucinations in cross-lingual transfer for low-resource languages. In this section, we fill this gap by introducing mFACT. This is constructed by transferring multiple English faithful
Our simple sanity check in Appendix A.3 shows these model-based hallucination metrics to be reliable.
https://cloud.google.com/translate
https://app.prolific.co
AcknowledgementsWe would like to thank Zheng Zhao and the anonymous reviewers for their helpful feedback.We are grateful for an Apple AI/ML scholarship awarded to Yifu Qiu.We appreciate the use of computing resources through the Baskerville cluster at the University of Birmingham.(Hasan et al., 2021). We report the document length (Doc.Len.), summary length (Sum.Len.) and the corresponding compression percentage (Comp%).All lengths are measured in the unit of tokens.We also include quality measurements from the XL-Sum paper according to human annotators.Prop.A reports the percentage of the summaries that convey the main idea of the document.Prop.C reports the percentage of summaries that contain some additional information not presented in the source.Weighted Loss: The accident occurred in the undersea tunnel near Calais, France.One worker was poisoned and 18 workers were injured, one of them in serious condition.0.12 0.78Table8: Examples of hallucinations (highlighted in a orange colour) generated by MAD-X, a method for zero-shot cross-lingual transfer, on top of an mBART-50 backbone.In this work, we present 1) the mFACT metric to evaluate the faithfulness of summarisation models in target languages other than English and 2) a loss weighting method to reduce hallucinations in cross-lingual transfer.We highlight the faithful pieces of information produced by our loss weighting but missed by MAD-X in a green colour.fer leads to improved ROUGE scores, but it also results in reduced faithfulness scores.Interestingly, when comparing the effects of Full-model and MAD-X transfer in Figure3, we see that Fullmodel transfer exhibit a greater improvement in ROUGE scores.However, this improvement come at the expense of introducing more hallucinations.In Figure4, we further compare the performance of Full-model and MAD-X transfer in both zero-shot and few-shot transfer scenarios.While Fullmodel transfer demonstrates an advantage in the few-shot transfer scenario compare to MAD-X, MAD-X performs better in the zero-shot scenario.Additionally, regardless of the transfer scenario, MAD-X exhibits a lower occurrence of hallucinations.A.7 Validation Faithfulness Curve for Weighted Loss MethodWe provide the training curve of MAD-X and our weighted loss method in Figure5. Upon examining the curve, it becomes evident that the model trained with the weighted loss consistently exhibits higher faithfulness compared to the MAD-X baseline throughout the entire training process.This observation serves as evidence for the effectiveness of our approach in guiding the model's optimisation towards increased faithfulness by diverting its training away from hallucinated samples.
Mirella Lapata. 2022. mface: Multilingual summarization with factual consistency evaluation. Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, and</p>
<p>Translation artifacts in cross-lingual transfer learning. Mikel Artetxe, Gorka Labaka, Eneko Agirre, 10.18653/v1/2020.emnlp-main.618Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Crosslingual abstractive summarization with limited parresources. Yu Bai, Yang Gao, Heyan Huang, 10.18653/v1/2021.acl-long.538Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. Meng Cao, Yue Dong, Jackie Cheung, 10.18653/v1/2022.acl-long.236Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022a1</p>
<p>Learning with rejection for abstractive text summarization. Meng Cao, Yue Dong, Jingyi He, Jackie Chi, Kit Cheung, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022b</p>
<p>Factual error correction for abstractive summarization models. Meng Cao, Yue Dong, Jiapeng Wu, Jackie Chi, Kit Cheung, 10.18653/v1/2020.emnlp-main.506Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Faithful to the original: Fact-aware neural abstractive summarization. Ziqiang Cao, Furu Wei, Wenjie Li, Sujian Li, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'18/IAAI'18/EAAI'18. the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'18/IAAI'18/EAAI'18AAAI Press2018</p>
<p>Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, arXiv:2304.10453Phoenix: Democratizing chatgpt across languages. 2023arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Prafulla Kumar Choubey, Alexander R Fabbri, Jesse Vig, Chien-Sheng Wu, Wenhao Liu, Nazneen Fatema, Rajani , Cape: Contrastive parameter ensembling for reducing hallucination in abstractive summarization. arXiv e-prints. 20212110</p>
<p>XNLI: Evaluating crosslingual sentence representations. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, Veselin Stoyanov, 10.18653/v1/D18-1269Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels; Belgium2018Association for Computational Linguistics</p>
<p>Elastic weight removal for faithful and abstractive dialogue generation. Nico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, Edoardo M Ponti, arXiv:2303.175742023arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Multifact correction in abstractive text summarization. Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi, Kit Cheung, Jingjing Liu, 10.18653/v1/2020.emnlp-main.749Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>FaithDial: A faithful benchmark for informationseeking dialogue. Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M Ponti, Siva Reddy, 10.1162/tacl_a_00529/114373/FaithDial-A-Faithful-Benchmark-for-InformationTransactions of the Association for Computational Linguistics. 102022</p>
<p>QAFactEval: Improved QAbased factual consistency evaluation for summarization. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, Caiming Xiong, 10.18653/v1/2022.naacl-main.187Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>SummEval: Re-evaluating Summarization Evaluation. Alexander R Fabbri, Wojciech Kryściński, Bryan Mc-Cann, Caiming Xiong, Richard Socher, Dragomir Radev, 10.1162/tacl_a_00373Transactions of the Association for Computational Linguistics. 92021</p>
<p>Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. Tobias Falke, Leonardo F R Ribeiro, 10.18653/v1/P19-1213Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych</p>
<p>Controlled hallucinations: Learning to generate faithfully from noisy data. Katja Filippova, 10.18653/v1/2020.findings-emnlp.76Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Factorizing content and budget decisions in abstractive summarization of long documents. Marcio Fonseca, Yftah Ziser, Shay B Cohen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Annotating and modeling fine-grained factuality in summarization. Tanya Goyal, Greg Durrett, 10.18653/v1/2021.naacl-main.114Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Training dynamics for text summarization models. Tanya Goyal, Jiacheng Xu, Junyi , Jessy Li, Greg Durrett, 10.18653/v1/2022.findings-acl.163Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>XLsum: Large-scale multilingual abstractive summarization for 44 languages. Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M Sohel Rahman, Rifat Shahriyar, 10.18653/v1/2021.findings-acl.413Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Teaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in neural information processing systems. 2015</p>
<p>Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, International Conference on Machine Learning. PMLR2020</p>
<p>Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward. Luyang Huang, Lingfei Wu, Lu Wang, 10.18653/v1/2020.acl-main.457Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi, arXiv:2212.04089Editing models with task arithmetic. 2022arXiv preprint</p>
<p>Multiple teacher distillation for robust and greener models. Artur Ilichev, Nikita Sorokin, Irina Piontkovskaya, Valentin Malykh, Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021). the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)Held Online. INCOMA Ltd2021</p>
<p>Improved natural language generation via loss truncation. Daniel Kang, Tatsunori B Hashimoto, 10.18653/v1/2020.acl-main.66Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Evaluating the factual consistency of abstractive text summarization. Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, 10.18653/v1/2020.emnlp-main.750Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>SummaC: Re-visiting NLIbased models for inconsistency detection in summarization. Philippe Laban, Tobias Schnabel, Paul N Bennett, Marti A Hearst, 10.1162/tacl_a_00453Transactions of the Association for Computational Linguistics. 102022</p>
<p>Faithful or extractive? on mitigating the faithfulness-abstractiveness tradeoff in abstractive summarization. Faisal Ladhak, Esin Durmus, He He, Claire Cardie, Kathleen Mckeown, 10.18653/v1/2022.acl-long.100Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Dac Viet, Lai, Trung Nghia, Amir Ngo, Ben Pouran, Hieu Veyseh, Franck Man, Trung Dernoncourt, Thien Huu Bui, Nguyen, arXiv:2304.05613Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. 2023arXiv preprint</p>
<p>The measurement of observer agreement for categorical data. Richard Landis, Gary G Koch, biometrics. 1977</p>
<p>A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Huang, 10.18653/v1/2023.findings-acl.29Findings of the Association for Computational Linguistics: ACL 2023. Toronto, Canada2023Association for Computational Linguistics</p>
<p>. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra , Sasha Luccioni, Franc ¸ois Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBart2020</p>
<p>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. Chin-Yew Lin, Franz Josef, Och , 10.3115/1218955.1219032Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)Barcelona, Spain2004</p>
<p>DExperts: Decoding-time controlled text generation with experts and anti-experts. Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, Yejin Choi, 10.18653/v1/2021.acl-long.522Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>SimCLS: A simple framework for contrastive learning of abstractive summarization. Yixin Liu, Pengfei Liu, 10.18653/v1/2021.acl-short.135Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20212Short Papers)</p>
<p>BRIO: Bringing order to abstractive summarization. Yixin Liu, Pengfei Liu, Dragomir Radev, Graham Neubig, 10.18653/v1/2022.acl-long.207Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland20221Association for Computational Linguistics</p>
<p>On faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, 10.18653/v1/2020.acl-main.173Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, arXiv:2211.01786Crosslingual generalization through multitask finetuning. 2022arXiv preprint</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, 10.18653/v1/D18-1206Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov, 10.18653/v1/2021.naacl-main.383Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Abstract text summarization: A low resource challenge. Shantipriya Parida, Petr Motlicek, 10.18653/v1/D19-1616Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>AdapterHub: A framework for adapting transformers. Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych, 10.18653/v1/2020.emnlp-demos.7Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020a</p>
<p>MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder, 10.18653/v1/2020.emnlp-main.617Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020b</p>
<p>Abstractive summarization guided by latent hierarchical document structure. Yifu Qiu, Shay B Cohen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>SummaReranker: A multi-task mixture-of-experts re-ranking framework for abstractive summarization. Shafiq Mathieu Ravaut, Nancy Joty, Chen, 10.18653/v1/2022.acl-long.309Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>QuestEval: Summarization asks for fact-based evaluation. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, Patrick Gallinari, 10.18653/v1/2021.emnlp-main.529Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Get to the point: Summarization with pointergenerator networks. Abigail See, Peter J Liu, Christopher D Manning, 10.18653/v1/P17-1099Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Pmindiasum: Multilingual and cross-lingual headline summarization for languages in india. Ashok Urlana, Pinzhen Chen, Zheng Zhao, Manish Shay B Cohen, Barry Shrivastava, ; Thomas Haddow, Lysandre Wolf, Victor Debut, Julien Sanh, Clement Chaumond, Anthony Delangue, Pierric Moi, Tim Cistac, Remi Rault, Morgan Louf, Joe Funtowicz, Sam Davison, Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Gugger, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Mariama Drame, Quentin Lhoest, Alexander Rush, the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2023. 2020Findings of EMNLP</p>
<p>One teacher is enough? pre-trained language model distillation from multiple teachers. Chuhan Wu, Fangzhao Wu, Yongfeng Huang, 10.18653/v1/2021.findings-acl.387Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, 10.18653/v1/2021.naacl-main.41Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics</p>
<p>Reducing quantity hallucinations in abstractive summarization. Zheng Zhao, Shay B Cohen, Bonnie Webber, 10.18653/v1/2020.findings-emnlp.203Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Enhancing factual consistency of abstractive summarization. Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng, Xuedong Huang, Meng Jiang, 10.18653/v1/2021.naacl-main.58Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Best Validation ROUGE-1. L Method, </p>
<p>Best Validation mFACT-English R-1 R-2 R-L mF mF-T bi% tr% R-1 R-2 R-L mF mF-T bi% tr% Chinese MAD-X 27. 14.10 19.95 38.29 33.99 28.23 44.15 29.59 14.86 20.61 39.62 35.08 21.62 34.37 CAPE 28.15 14.57 20.74 42.56 39.59 27.12 42.11 29.64 14.80 20.58 38.83 34.01 20.11 32.32</p>
<p>. Spanish, MAD-X 20.77 4.89 15.06 20.57 29.25 33.91 50.47 23.36 5.13 16.34 21.87 29.36 21.98 34.20 CAPE 21.16 4.91 15.28 22.03 31.02 29.99 45.22 23.24 5.01 16.24 21.65 29.40 19.98 31.29</p>
<p>. Hindi, MAD-X 20.08 5.44 15.10 22.75 16.42 34.96 51.35 25.51 7.78 19.07 28.41 19.32 25.57 39.02 CAPE 20.97 6.11 16.07 23.50 16.45 30.95 45.81 25.80 7.85 19.20 29.11 19.53 23.77 36.58</p>
<p>. Turkish, MAD-X 17.16 5.56 14.00 30.16 21.08 43.59 60.56 17.22 6.33 14.59 33.24 25.53 38.72 54.12 CAPE 17.66 5.72 14.33 29.88 22.50 39.32 56.45 17.12 6.23 14.55 35.04 26.69 36.47 51.47</p>
<p>. Vietnamese, MAD-X 25.85 11.85 19.45 35.09 34.10 33.99 50.73 27.23 12.57 20.32 36.64 37.75 27.40 42.67 CAPE 26.20 11.96 19.65 36.98 36.79 30.68 46.77 27.01 12.45 20.15 36.71 37.79 25.89 40.64</p>
<p>. French, MAD-X 25.82 8.31 19.10 38.21 40.79 27.38 42.21 26.02 7.97 19.02 38.71 42.66 18.88 30.29 CAPE 25.83 8.40 19.08 38.30 41.44 26.11 40.44 25.75 7.93 18.80 37.54 40.91 18.00 28.88</p>
<p>Automatic evaluation for zero-shot performance in English-to-others cross-lingual transfer direction while selecting the checkpoint with the best validation ROUGE-1 and the best validation mFACT score. We run all model results with three different random seeds. mF stands for mFACT and mF-T stands for mFACT-Transfer. tr% and bi% are the percentage of novel tri-gram and bi-gram. Table. 12respectively</p>            </div>
        </div>

    </div>
</body>
</html>
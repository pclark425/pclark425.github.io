<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1025 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1025</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1025</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-240070795</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2107.12931v2.pdf" target="_blank">Autonomous Reinforcement Learning via Subgoal Curricula</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) promises to enable autonomous acquisition of complex behaviors for diverse agents. However, the success of current reinforcement learning algorithms is predicated on an often under-emphasised requirement -- each trial needs to start from a fixed initial state distribution. Unfortunately, resetting the environment to its initial state after each trial requires substantial amount of human supervision and extensive instrumentation of the environment which defeats the goal of autonomous acquisition of complex behaviors. In this work, we propose Value-accelerated Persistent Reinforcement Learning (VaPRL), which generates a curriculum of initial states such that the agent can bootstrap on the success of easier tasks to efficiently learn harder tasks. The agent also learns to reach the initial states proposed by the curriculum, minimizing the reliance on human interventions into the learning. We observe that VaPRL reduces the interventions required by three orders of magnitude compared to episodic RL while outperforming prior state-of-the art methods for reset-free RL both in terms of sample efficiency and asymptotic performance on a variety of simulated robotics problems.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1025.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1025.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VaPRL-tabletop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value-accelerated Persistent Reinforcement Learning — table-top rearrangement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A goal-conditioned RL agent (VaPRL) trained with a value-based curriculum of start states to learn a table-top mug-rearrangement task under persistent (reset-free) training; uses SAC as base learner, goal relabeling, and a value-function driven curriculum generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VaPRL agent (goal-conditioned SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A unified goal-conditioned policy trained with Soft Actor-Critic (off-policy RL) augmented by: (1) a value-function based curriculum generator C(g) that selects start states from which the current policy has at least a threshold value for reaching a given goal, and (2) goal relabeling of replayed transitions (from demonstrations and online data). The same policy is used both to practice the task and to reach curriculum start states.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Table-top rearrangement (mug to 4 goal squares)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A simulated table-top manipulation environment where a gripper (point-mass model that can attach to object) must pick up and place a mug into one of 4 discrete goal squares. Challenges include sparse rewards (binary success within distance threshold), exploration to contact/grasp, and multi-modal goal locations. Evaluation horizon H_E = 200; training horizon H_T = 200,000 (persistent setting with rare resets). Demonstrations provided (6 per goal).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Sparse reward task; requires contacting and grasping object then placing it at one of 4 discrete goal locations; state/action dimensionality modest (gripper + object); horizon H_E = 200 steps; number of goal locations = 4; training horizon H_T = 200,000 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (sparse-reward manipulation with discrete multi-goal options and nontrivial exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation encoded via initial state distribution ρ (object / gripper starting positions) and goal distribution p_g (4 possible goal squares). Demonstrations include trajectories from and to goals; amount of training resets extremely low (few interventions).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (4 goal locations; randomized initial object positions across episodes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate measured as number of successful goal reaches out of 10 evaluation trials (binary success metric); sample efficiency measured by environment interactions (training steps) and number of extrinsic resets/interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>VaPRL required ≈1000× fewer environment resets than episodic (oracle) RL for this environment (paper reports <20 total interventions for VaPRL). VaPRL outperforms naive RL, FBRL and R3L in sample efficiency; oracle RL performed substantially worse than VaPRL in this sparse-reward table-top task (exact success fraction not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper argues and empirically demonstrates that choosing start states closer to the goal (lower complexity) and progressively moving starts toward the evaluation initial distribution (increasing variation) yields faster learning; they hypothesize that non-deterministic / broadened initial distributions can improve downstream performance and that appropriate starting-state curricula mitigate sparse-reward exploration failures.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (value-function driven curriculum of start states), goal relabeling, off-policy RL (SAC); uses demonstrations to seed replay and to measure X_rho (step-index distance).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Policy is evaluated on the episodic evaluation environment M_E (resets to initial distribution ρ and goals sampled from p_g). VaPRL achieves better evaluation performance than naive persistent methods and even outperforms oracle RL in this sparse-reward tabletop case (text reports oracle RL did substantially worse than VaPRL). Evaluations are success counts over 10 trials, measured intermittently during training.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training horizon H_T = 200,000 environment steps given; VaPRL reported up to ~30% improved sample efficiency relative to prior reset-free baselines for tasks generally; for table-top specifically, VaPRL learned faster than FBRL/R3L/naive RL within the same training budget (exact step counts to thresholds not enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A value-driven start-state curriculum substantially improves exploration and final task success in a sparse multi-goal table-top manipulation task, enabling strong evaluation performance with dramatically fewer environment resets (≈1000× fewer than episodic RL), and in this case outperforming oracle episodic RL; curriculum states moved from goal-proximal to initial-distribution-proximal as learning progressed, correlating with improved evaluation success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Reinforcement Learning via Subgoal Curricula', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1025.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1025.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VaPRL-door</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value-accelerated Persistent Reinforcement Learning — Sawyer door closing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>VaPRL applied to a simulated Sawyer robot door-closing task: the agent learns to close the door from intermediate angles via a curriculum of start states and must also implicitly learn to open the door to practice repeatedly under persistent training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VaPRL agent (goal-conditioned SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Goal-conditioned SAC policy trained with value-based curriculum generator and relabeling; the policy both practices the closing task and reaches curriculum start states, implicitly learning the reverse (opening) behavior necessary under persistent training where resets are scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (Sawyer arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sawyer door closing</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A simulated Sawyer robot arm must close a door to a target angle. Environment uses sparse rewards (success when door angle within threshold). Training horizon H_T = 200,000; evaluation horizon H_E = 400. Persistent setting makes repeated practice require the agent to also learn opening behavior (to return to initial states) unless curriculum reduces that need.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Sparse-reward continuous control requiring precise control of arm and door interactions; evaluation horizon H_E = 400; complexity arises from contact dynamics and needing to learn both closing and reversing behaviors in persistent setting.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (contact-rich but single-goal continuous control)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation primarily via initial state distribution ρ (starting door and arm configurations) and possibly small variations in goal angle; demonstrations provided include opening and closing trajectories (3 each).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium (single goal target but varying initial conditions across trials)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (binary success by threshold on door angle) measured as successes out of 10 evaluation trials; number of extrinsic resets/interventions also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>VaPRL matched oracle RL performance on this task while requiring ~500× fewer environment resets than episodic RL; overall achieved high success in evaluation (exact success fraction not numerically listed in text).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper notes that a curriculum that selects start states closer to the goal (simpler tasks) and gradually moves starts toward the initial distribution enables learning without frequent resets; in this task VaPRL matched oracle performance despite far fewer resets, indicating curricula can compensate for limited variation in resets by controlling start-state complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (value-thresholded start-state selection), goal relabeling, off-policy RL (SAC); uses demonstrations (including reverse trajectories) to seed replay and to compute step-index distances for X_rho.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated on episodic evaluation M_E (resets to initial distribution); VaPRL achieves evaluation performance comparable to oracle RL despite having orders-of-magnitude fewer resets during training, demonstrating effective generalization from the practiced curriculum to the intended initial-state evaluation distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training horizon H_T = 200,000 steps; VaPRL requires far fewer extrinsic interventions (~500× fewer) and learns as effectively as episodic (oracle) RL under the same or smaller number of effective task trials.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A value-driven curriculum enables learning of a contact-rich door-closing behavior in the persistent (minimal-reset) setting, matching episodic training performance while reducing required human/engineered interventions by ~500×; curricula allow the agent to practice from progressively harder start states rather than repeatedly returning to the full initial-state distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Reinforcement Learning via Subgoal Curricula', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1025.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1025.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VaPRL-hand</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value-accelerated Persistent Reinforcement Learning — dexterous hand manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>VaPRL applied to a high-dimensional dexterous manipulation task (16-DoF hand + 6-DoF arm) to pick up and lift a 3-pronged object to a target pose from varied starting positions, using dense rewards and a value-based start-state curriculum with demonstration relabeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VaPRL agent (goal-conditioned SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A unified goal-conditioned policy trained with SAC, augmented by value-function driven start-state curriculum, extensive goal relabeling (from many demonstrations), and large replay buffers to handle high-dimensional, contact-rich manipulation dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (dexterous hand mounted on Sawyer arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Hand manipulation (dexterous pickup and lift)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A challenging dexterous manipulation environment with a 16-DoF hand on a 6-DoF arm manipulating a 3-pronged object: must pick up object from random positions on table and lift it to a goal position above table. Complex contact dynamics, high-dimensional state/action spaces, and requirement to reconfigure object to diverse locations to match test-time conditions. Evaluation H_E = 400; training H_T = 400,000.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>High-dimensional control: 16-DoF hand + 6-DoF arm; complex contact dynamics; dense multi-term reward (hand-object distance, object-goal distance, exponential bonuses); long horizon H_E = 400; demonstrations: 30 trajectories of different subskills. Task difficulty high due to manipulation and repositioning requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High variation in initial object positions on table (randomized across episodes) and diverse goal starts for repositioning; demonstrations include many trajectories for repositioning and lifting (10+ and 20+ sets); curriculum must grow to cover a large set of start locations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success metric is I(d(o,g) ≤ 0.05) (binary success of object within 5cm of goal) measured over 10 evaluation trials; also reported: relative performance improvement factors and sample-efficiency gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>VaPRL provided a 2.5× gain in performance compared to the next best baseline on this dexterous task (exact absolute success rates not provided in text); VaPRL required ≈1000× fewer resets than episodic RL; achieved substantially higher sample efficiency and better asymptotic performance within the given training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper emphasizes that when both complexity and variation are high (high-DoF dexterous manipulation with many object start locations), a curriculum that starts from goal-proximal states and progressively expands start-state variation is crucial; VaPRL's value-based curriculum enabled bootstrapping from easier subproblems to the harder, highly-varying evaluation distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Tested: high complexity & high variation (this dexterous environment). VaPRL outperformed baselines with ~2.5× higher performance relative to the next best method and required ≈1000× fewer resets than episodic RL, but exact success-rate numbers are not reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (value-thresholded start-state selection), dense goal relabeling (demonstrations re-labeled extensively), off-policy RL (SAC), and use of large replay buffers (25M) to retain relabeled samples.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated on the episodic evaluation environment (random initial object positions); VaPRL generalized to the intended evaluation distribution and achieved substantially higher performance than other persistent RL baselines and better asymptotic performance within training budget (2.5× improvement over next best).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training horizon H_T = 400,000 steps; VaPRL achieves substantial sample-efficiency gains over prior persistent RL methods and reaches higher evaluation performance within this budget (reported as up to 30% better sample efficiency across tasks and 2.5× performance gain on this task relative to next best baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In a high-dimensional, contact-rich dexterous manipulation domain with large start-state variation, a value-driven start-state curriculum plus targeted goal relabeling dramatically improves learning: VaPRL attained much higher performance (2.5× over the next best) and drastically reduced required environment interventions (~1000× fewer resets) compared to episodic baselines and prior reset-free methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Reinforcement Learning via Subgoal Curricula', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Leave no trace: Learning to reset for safe and autonomous reinforcement learning <em>(Rating: 2)</em></li>
                <li>Reverse curriculum generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention <em>(Rating: 2)</em></li>
                <li>Intrinsic motivation and automatic curricula via asymmetric self-play <em>(Rating: 1)</em></li>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1025",
    "paper_id": "paper-240070795",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "VaPRL-tabletop",
            "name_full": "Value-accelerated Persistent Reinforcement Learning — table-top rearrangement",
            "brief_description": "A goal-conditioned RL agent (VaPRL) trained with a value-based curriculum of start states to learn a table-top mug-rearrangement task under persistent (reset-free) training; uses SAC as base learner, goal relabeling, and a value-function driven curriculum generator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "VaPRL agent (goal-conditioned SAC)",
            "agent_description": "A unified goal-conditioned policy trained with Soft Actor-Critic (off-policy RL) augmented by: (1) a value-function based curriculum generator C(g) that selects start states from which the current policy has at least a threshold value for reaching a given goal, and (2) goal relabeling of replayed transitions (from demonstrations and online data). The same policy is used both to practice the task and to reach curriculum start states.",
            "agent_type": "simulated robotic agent",
            "environment_name": "Table-top rearrangement (mug to 4 goal squares)",
            "environment_description": "A simulated table-top manipulation environment where a gripper (point-mass model that can attach to object) must pick up and place a mug into one of 4 discrete goal squares. Challenges include sparse rewards (binary success within distance threshold), exploration to contact/grasp, and multi-modal goal locations. Evaluation horizon H_E = 200; training horizon H_T = 200,000 (persistent setting with rare resets). Demonstrations provided (6 per goal).",
            "complexity_measure": "Sparse reward task; requires contacting and grasping object then placing it at one of 4 discrete goal locations; state/action dimensionality modest (gripper + object); horizon H_E = 200 steps; number of goal locations = 4; training horizon H_T = 200,000 steps.",
            "complexity_level": "medium (sparse-reward manipulation with discrete multi-goal options and nontrivial exploration)",
            "variation_measure": "Variation encoded via initial state distribution ρ (object / gripper starting positions) and goal distribution p_g (4 possible goal squares). Demonstrations include trajectories from and to goals; amount of training resets extremely low (few interventions).",
            "variation_level": "medium (4 goal locations; randomized initial object positions across episodes)",
            "performance_metric": "Success rate measured as number of successful goal reaches out of 10 evaluation trials (binary success metric); sample efficiency measured by environment interactions (training steps) and number of extrinsic resets/interventions.",
            "performance_value": "VaPRL required ≈1000× fewer environment resets than episodic (oracle) RL for this environment (paper reports &lt;20 total interventions for VaPRL). VaPRL outperforms naive RL, FBRL and R3L in sample efficiency; oracle RL performed substantially worse than VaPRL in this sparse-reward table-top task (exact success fraction not tabulated in text).",
            "complexity_variation_relationship": "The paper argues and empirically demonstrates that choosing start states closer to the goal (lower complexity) and progressively moving starts toward the evaluation initial distribution (increasing variation) yields faster learning; they hypothesize that non-deterministic / broadened initial distributions can improve downstream performance and that appropriate starting-state curricula mitigate sparse-reward exploration failures.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (value-function driven curriculum of start states), goal relabeling, off-policy RL (SAC); uses demonstrations to seed replay and to measure X_rho (step-index distance).",
            "generalization_tested": true,
            "generalization_results": "Policy is evaluated on the episodic evaluation environment M_E (resets to initial distribution ρ and goals sampled from p_g). VaPRL achieves better evaluation performance than naive persistent methods and even outperforms oracle RL in this sparse-reward tabletop case (text reports oracle RL did substantially worse than VaPRL). Evaluations are success counts over 10 trials, measured intermittently during training.",
            "sample_efficiency": "Training horizon H_T = 200,000 environment steps given; VaPRL reported up to ~30% improved sample efficiency relative to prior reset-free baselines for tasks generally; for table-top specifically, VaPRL learned faster than FBRL/R3L/naive RL within the same training budget (exact step counts to thresholds not enumerated in text).",
            "key_findings": "A value-driven start-state curriculum substantially improves exploration and final task success in a sparse multi-goal table-top manipulation task, enabling strong evaluation performance with dramatically fewer environment resets (≈1000× fewer than episodic RL), and in this case outperforming oracle episodic RL; curriculum states moved from goal-proximal to initial-distribution-proximal as learning progressed, correlating with improved evaluation success.",
            "uuid": "e1025.0",
            "source_info": {
                "paper_title": "Autonomous Reinforcement Learning via Subgoal Curricula",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "VaPRL-door",
            "name_full": "Value-accelerated Persistent Reinforcement Learning — Sawyer door closing",
            "brief_description": "VaPRL applied to a simulated Sawyer robot door-closing task: the agent learns to close the door from intermediate angles via a curriculum of start states and must also implicitly learn to open the door to practice repeatedly under persistent training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "VaPRL agent (goal-conditioned SAC)",
            "agent_description": "Goal-conditioned SAC policy trained with value-based curriculum generator and relabeling; the policy both practices the closing task and reaches curriculum start states, implicitly learning the reverse (opening) behavior necessary under persistent training where resets are scarce.",
            "agent_type": "simulated robotic agent (Sawyer arm)",
            "environment_name": "Sawyer door closing",
            "environment_description": "A simulated Sawyer robot arm must close a door to a target angle. Environment uses sparse rewards (success when door angle within threshold). Training horizon H_T = 200,000; evaluation horizon H_E = 400. Persistent setting makes repeated practice require the agent to also learn opening behavior (to return to initial states) unless curriculum reduces that need.",
            "complexity_measure": "Sparse-reward continuous control requiring precise control of arm and door interactions; evaluation horizon H_E = 400; complexity arises from contact dynamics and needing to learn both closing and reversing behaviors in persistent setting.",
            "complexity_level": "medium (contact-rich but single-goal continuous control)",
            "variation_measure": "Variation primarily via initial state distribution ρ (starting door and arm configurations) and possibly small variations in goal angle; demonstrations provided include opening and closing trajectories (3 each).",
            "variation_level": "low-to-medium (single goal target but varying initial conditions across trials)",
            "performance_metric": "Success rate (binary success by threshold on door angle) measured as successes out of 10 evaluation trials; number of extrinsic resets/interventions also reported.",
            "performance_value": "VaPRL matched oracle RL performance on this task while requiring ~500× fewer environment resets than episodic RL; overall achieved high success in evaluation (exact success fraction not numerically listed in text).",
            "complexity_variation_relationship": "Paper notes that a curriculum that selects start states closer to the goal (simpler tasks) and gradually moves starts toward the initial distribution enables learning without frequent resets; in this task VaPRL matched oracle performance despite far fewer resets, indicating curricula can compensate for limited variation in resets by controlling start-state complexity.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (value-thresholded start-state selection), goal relabeling, off-policy RL (SAC); uses demonstrations (including reverse trajectories) to seed replay and to compute step-index distances for X_rho.",
            "generalization_tested": true,
            "generalization_results": "Evaluated on episodic evaluation M_E (resets to initial distribution); VaPRL achieves evaluation performance comparable to oracle RL despite having orders-of-magnitude fewer resets during training, demonstrating effective generalization from the practiced curriculum to the intended initial-state evaluation distribution.",
            "sample_efficiency": "Training horizon H_T = 200,000 steps; VaPRL requires far fewer extrinsic interventions (~500× fewer) and learns as effectively as episodic (oracle) RL under the same or smaller number of effective task trials.",
            "key_findings": "A value-driven curriculum enables learning of a contact-rich door-closing behavior in the persistent (minimal-reset) setting, matching episodic training performance while reducing required human/engineered interventions by ~500×; curricula allow the agent to practice from progressively harder start states rather than repeatedly returning to the full initial-state distribution.",
            "uuid": "e1025.1",
            "source_info": {
                "paper_title": "Autonomous Reinforcement Learning via Subgoal Curricula",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "VaPRL-hand",
            "name_full": "Value-accelerated Persistent Reinforcement Learning — dexterous hand manipulation",
            "brief_description": "VaPRL applied to a high-dimensional dexterous manipulation task (16-DoF hand + 6-DoF arm) to pick up and lift a 3-pronged object to a target pose from varied starting positions, using dense rewards and a value-based start-state curriculum with demonstration relabeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "VaPRL agent (goal-conditioned SAC)",
            "agent_description": "A unified goal-conditioned policy trained with SAC, augmented by value-function driven start-state curriculum, extensive goal relabeling (from many demonstrations), and large replay buffers to handle high-dimensional, contact-rich manipulation dynamics.",
            "agent_type": "simulated robotic agent (dexterous hand mounted on Sawyer arm)",
            "environment_name": "Hand manipulation (dexterous pickup and lift)",
            "environment_description": "A challenging dexterous manipulation environment with a 16-DoF hand on a 6-DoF arm manipulating a 3-pronged object: must pick up object from random positions on table and lift it to a goal position above table. Complex contact dynamics, high-dimensional state/action spaces, and requirement to reconfigure object to diverse locations to match test-time conditions. Evaluation H_E = 400; training H_T = 400,000.",
            "complexity_measure": "High-dimensional control: 16-DoF hand + 6-DoF arm; complex contact dynamics; dense multi-term reward (hand-object distance, object-goal distance, exponential bonuses); long horizon H_E = 400; demonstrations: 30 trajectories of different subskills. Task difficulty high due to manipulation and repositioning requirements.",
            "complexity_level": "high",
            "variation_measure": "High variation in initial object positions on table (randomized across episodes) and diverse goal starts for repositioning; demonstrations include many trajectories for repositioning and lifting (10+ and 20+ sets); curriculum must grow to cover a large set of start locations.",
            "variation_level": "high",
            "performance_metric": "Success metric is I(d(o,g) ≤ 0.05) (binary success of object within 5cm of goal) measured over 10 evaluation trials; also reported: relative performance improvement factors and sample-efficiency gains.",
            "performance_value": "VaPRL provided a 2.5× gain in performance compared to the next best baseline on this dexterous task (exact absolute success rates not provided in text); VaPRL required ≈1000× fewer resets than episodic RL; achieved substantially higher sample efficiency and better asymptotic performance within the given training budget.",
            "complexity_variation_relationship": "The paper emphasizes that when both complexity and variation are high (high-DoF dexterous manipulation with many object start locations), a curriculum that starts from goal-proximal states and progressively expands start-state variation is crucial; VaPRL's value-based curriculum enabled bootstrapping from easier subproblems to the harder, highly-varying evaluation distribution.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Tested: high complexity & high variation (this dexterous environment). VaPRL outperformed baselines with ~2.5× higher performance relative to the next best method and required ≈1000× fewer resets than episodic RL, but exact success-rate numbers are not reported in text.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (value-thresholded start-state selection), dense goal relabeling (demonstrations re-labeled extensively), off-policy RL (SAC), and use of large replay buffers (25M) to retain relabeled samples.",
            "generalization_tested": true,
            "generalization_results": "Evaluated on the episodic evaluation environment (random initial object positions); VaPRL generalized to the intended evaluation distribution and achieved substantially higher performance than other persistent RL baselines and better asymptotic performance within training budget (2.5× improvement over next best).",
            "sample_efficiency": "Training horizon H_T = 400,000 steps; VaPRL achieves substantial sample-efficiency gains over prior persistent RL methods and reaches higher evaluation performance within this budget (reported as up to 30% better sample efficiency across tasks and 2.5× performance gain on this task relative to next best baseline).",
            "key_findings": "In a high-dimensional, contact-rich dexterous manipulation domain with large start-state variation, a value-driven start-state curriculum plus targeted goal relabeling dramatically improves learning: VaPRL attained much higher performance (2.5× over the next best) and drastically reduced required environment interventions (~1000× fewer resets) compared to episodic baselines and prior reset-free methods.",
            "uuid": "e1025.2",
            "source_info": {
                "paper_title": "Autonomous Reinforcement Learning via Subgoal Curricula",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Leave no trace: Learning to reset for safe and autonomous reinforcement learning",
            "rating": 2,
            "sanitized_title": "leave_no_trace_learning_to_reset_for_safe_and_autonomous_reinforcement_learning"
        },
        {
            "paper_title": "Reverse curriculum generation for reinforcement learning",
            "rating": 2,
            "sanitized_title": "reverse_curriculum_generation_for_reinforcement_learning"
        },
        {
            "paper_title": "Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention",
            "rating": 2,
            "sanitized_title": "resetfree_reinforcement_learning_via_multitask_learning_learning_dexterous_manipulation_behaviors_without_human_intervention"
        },
        {
            "paper_title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
            "rating": 1,
            "sanitized_title": "intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay"
        },
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 1,
            "sanitized_title": "automatic_goal_generation_for_reinforcement_learning_agents"
        }
    ],
    "cost": 0.0138,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Autonomous Reinforcement Learning via Subgoal Curricula</p>
<p>Archit Sharma architsh@stanford.edu 
Stanford University
‡ Google Brain# UCBerkeley</p>
<p>Abhishek Gupta abhishekunique@google.com 
Stanford University
‡ Google Brain# UCBerkeley</p>
<h1></h1>
<p>Stanford University
‡ Google Brain# UCBerkeley</p>
<p>Sergey Levine slevine@google.com 
Stanford University
‡ Google Brain# UCBerkeley</p>
<h1></h1>
<p>Stanford University
‡ Google Brain# UCBerkeley</p>
<p>Karol Hausman karolhausman@google.com 
Stanford University
‡ Google Brain# UCBerkeley</p>
<p>Chelsea Finn cbfinn@stanford.edu 
Stanford University
‡ Google Brain# UCBerkeley</p>
<p>Autonomous Reinforcement Learning via Subgoal Curricula</p>
<p>Reinforcement learning (RL) promises to enable autonomous acquisition of complex behaviors for diverse agents. However, the success of current reinforcement learning algorithms is predicated on an often under-emphasised requirement -each trial needs to start from a fixed initial state distribution. Unfortunately, resetting the environment to its initial state after each trial requires substantial amount of human supervision and extensive instrumentation of the environment which defeats the goal of autonomous acquisition of complex behaviors. In this work, we propose Value-accelerated Persistent Reinforcement Learning (VaPRL), which generates a curriculum of initial states such that the agent can bootstrap on the success of easier tasks to efficiently learn harder tasks. The agent also learns to reach the initial states proposed by the curriculum, minimizing the reliance on human interventions into the learning. We observe that VaPRL reduces the interventions required by three orders of magnitude compared to episodic RL while outperforming prior state-of-the art methods for reset-free RL both in terms of sample efficiency and asymptotic performance on a variety of simulated robotics problems 1 .</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) offers an appealing opportunity to enable autonomous acquisition of complex behaviors for interactive agents. Despite recent RL successes on robots [26,34,25,28,35,22,32,23,14], several challenges exist that inhibit wider adoption of reinforcement learning for robotics [48]. One of the major challenges to the autonomy of current reinforcement learning algorithms, particularly in robotics, is the assumption that each trial starts from an initial state drawn from a specific state distribution in the environment. Conventionally, reinforcement learning algorithms assume the ability to arbitrarily sample and reset to states drawn from this distribution, making such algorithms impractical for most real-world setups.</p>
<p>Many prior examples of reinforcement learning on real robots have relied on extensive instrumentation of the robotic setup and human supervision to enable environment resets to this initial state distribution. This is accomplished through a human providing the environment reset themselves throughout the training [8,12,4], scripted behaviors for the robot to reset the environment [28,39], an additional robot executing scripted behavior to reset the environment [32], or engineered mechanical contraptions [46,23]. The additional instrumentation of the environment and creating scripted behaviors are both time-intensive and often require additional resources such as sensors or even robots. The scripted reset behaviors are narrow in application, often designed for just a single task or environment, and their brittleness mandates human oversight of the learning process. Eliminating or minimizing the algorithmic reliance on the reset mechanisms can enable more autonomous learning, and in turn it will allow agents to scale to broader and harder set of tasks. Figure 1: Comparison of the persistent RL setting with the episodic RL setting. Interventions (human or otherwise orchestrated) reset the environment to the initial state distribution after every episode in episodic RL, while the state of the environment persists through the training in persistent RL. The learned policy is tested starting from the initial state distribution for both the settings.</p>
<p>To address these challenges, some recent works have developed reinforcement learning algorithms that can effectively learn with minimal resets to the initial distribution [19,6,48,43,14]. We provide a formal problem definition that encapsulates and sheds light on the general setting addressed by these prior methods, which we refer to as the persistent reinforcement learning in this work. In this problem setting, we disentangle the training and the test time settings such that the test-time objective matches that of the conventional RL setting but the train-time setting restricts access to the initial state distribution by giving a low frequency periodic reset. In this setting, the agent must persistently learn and interact with the environment with minimal human interventions, as shown in Figure 1. Conventional episodic RL algorithms often fail to solve the task entirely in this setting, as shown by Zhu et al. [48] and Figure 2. This is because these methods rely on the ability to sample the initial state distribution arbitrarily. One solution to this problem is to additionally learn a reset policy that recovers the initial state distribution [19,6] allowing the agent to repeatedly alternate between practicing the task and practicing the reverse. Unfortunately, not only can solving the task directly from the initial state distribution be hard from an exploration standpoint, but (attempting to) return to the initial state repeatedly can be sample inefficient. In this paper, we propose to instead have the agent reset itself to and attempt the task from different initial states along the path to the goal state. In particular, the agent can learn to solve the task from easier starting states that are closer to the goal and bootstrap on these to solve the task from harder states farther away from the goal. The main contribution of this work is Value-accelerated Persistent Reinforcement Learning (VaPRL), a goal-conditioned RL method that creates an adaptive curriculum of starting states for the agent to efficiently improve test-time performance while substantially reducing the reliance on extrinsic reset mechanisms. Additionally, we provide a formal description of the persistent RL problem setting to conceptualize our work and prior methods. We benchmark VaPRL on several robotic control tasks in the persistent RL setting against state-of-the-art methods, which either simulate the initial state distribution by learning a reset controller, or incrementally grow the state-space from which the given task can be solved. Our experiments indicate that using a tailored curriculum generated by VaPRL can be up to 30% more sample-efficient in acquiring task behaviors compared to these prior methods. For the most challenging dexterous manipulation problem, VaPRL provides a 2.5× gain in performance compared to the next best performing method.</p>
<p>Related Work</p>
<p>Robot learning. Prior works using reinforcement learning have relied on manually design controllers or human supervision to enable episodic environmental resets, as is required by the current algorithms. This can be through human orchestrated resets [8,13,12,4,16], which requires high frequency human intervention in robot training. In some cases, it is possible to execute a scripted behavior to reset the environment [28,32,47,39,46,1]. However, programming such behaviors is time-intensive for the practitioner, and robot training still requires human oversight as the scripted behaviors are often brittle. Some prior works have designed the environment [35,7,22] to bypass the need for having a reset mechanism. This is not generally applicable and can require extensive environment design. Some recent works leverage multi-task RL to bypass the need for extrinsic reset mechanisms [15,14]. Typically, a task-graph uses the current state to decides the next task for the agent, such that only minimal intervention is required during training. However, these task-graphs are specific to a problem and require additional engineering to appropriately decide the next task.</p>
<p>Reset-free reinforcement learning. Constraining the access to these orchestrated reset mechanisms severely impedes policy learning when using current RL algorithms [48]. Recent works have proposed several algorithms to reduce reliance on extrinsic reset mechanisms by learning a reset controller to retrieve the initial state distribution [19,6], by learning a perturbation controller [48], or by learning reset skills in adversarial games [43]. These works implicitly define a target state distribution for a reset controller: Han et al. [19], Eysenbach et al. [6] target a fixed initial state distribution; Zhu et al. [47] target a uniform distribution over the state space as a consequence of novelty-seeking behavior of the reset controller; and Xu et al. [43] target an adversarial form of initial state distribution to produce a more robust policy. In contrast, our proposed algorithm VaPRL generates a curriculum of starting states tailored to the task and agent's performance. Our experiments demonstrate that VaPRL outperforms these prior methods in both sample efficiency and absolute performance. Other recent work like [29] has considered combining model-based RL with unsupervised skill discovery to solve reset-free learning problems, but largely focus on avoiding sink states rather than attempting tasks repeatedly with a curriculum like VaPRL.</p>
<p>Curriculum generation for reinforcement learning. Curriculum generation is a crucial aspect of sample-efficient learning in VaPRL. Prior works have shown that using a curriculum can enable faster learning and improve performance [9,10,40,30,33,27]. Task-tailored curriculum can simplify the exploration as it is easier to solve the task from certain states [21,9] enabling faster progress on the downstream task. In addition to proposing a novel method for curriculum generation, we design it for the persistent RL setting without requiring the ability to reset the environment to arbitrary states as assumed by prior work.</p>
<p>Persistent vs. lifelong reinforcement learning. Prior reinforcement learning algorithms that reduce the need for oracle resets have relied on the problem setting of lifelong or continual reinforcement learning [41,24], when the objective in practice is to learn episodic behaviors. Both the persistent RL and the lifelong learning frameworks do transcend the episodic setting for training, promoting more autonomy in reinforcement learning. However, persistent reinforcement learning distinguishes between the training and evaluation objectives, where the evaluation objective matches that of the episodic reinforcement learning. While the assumptions of episodic reinforcement learning are hard to realize for real-world training, real-world deployment of policies is often episodic. This is commonly true for robotics, where the assigned tasks are expected to be repetitive but it is hard to orchestrate resets in the training environment. This makes persistent reinforcement learning a suitable framework for modelling robotic learning tasks.</p>
<p>Persistent Reinforcement Learning</p>
<p>In this section, we formalize the persistent reinforcement learning as an optimization problem. The key insight is to separate the evaluation and training objectives such that the evaluation objective measures the performance of the desired behavior while the training objective enables us to acquire those behaviors, while recognizing that frequent invocation of a reset mechanism is untenable. We first provide a general formulation, and then adapt persistent RL to the goal-conditioned setting.</p>
<p>Definition. Consider a Markov decision process (MDP) M E ≡ (S, A, p, r, ρ, γ, H E ) [37]. Here, S denotes the state space, A denotes the action space, p : S × A × S → R ≥0 denotes the transition dynamics, r : S × A → R denotes the reward function, ρ : S → R ≥0 denotes the initial state distribution, γ ∈ [0, 1] denotes the discount factor, and H E denotes the episode horizon. Our objective is to learn a policy π that maximizes J E (π) = E[ H E t=1 γ t r(s t , a t )], where s 0 ∼ ρ(·), a t ∼ π(· | s t ) and s t+1 ∼ p(· | s t , a t ), the episodic expected sum of discounted rewards.</p>
<p>However, generating samples from the initial state distribution ρ invokes a reset mechanism, which is hard to realize in the real world. We want to construct a MDP M T corresponding to our training environment which reduces invocations of the reset mechanism. To reduce such interventions, we consider a training environment M T ≡ (S, A, p,r t ,ρ, γ, H T ) with episode horizon H T H E . Näively optimizing r can substantially deteriorate the performance of episodic RL algorithms, as shown in Figure 2 where we compare the evaluation performance with H E = 200 when training in environments with H T = 200 (with resets) versus H T = 200, 000 (without resets). Therefore, it becomes beneficial to consider a surrogate reward functionr t rather than just optimizing for r naively. As a motivating example, consider a forward-backward controller which alternates between solving the task corresponding to r and recovering the initial state distribution ρ. The surrogate reward function corresponding for this approach can be written as:
r t (s t , a t ) = r(s t , a t ) t = [1, H E ], [2H E + 1, 3H E ], . . . r ρ (s t , a t ) t = [H E + 1, 2H E ], . . .(1)
Here,r alternates between the task-reward r for H E steps and r ρ (which encourages initial state distribution recovery) for H E steps 2 , also illustrated in Figure 3 (right). This surrogate reward function allows the agent to repeatedly practice the task, thus using the autonomous interaction more judiciously as compared to the näive approach. Note, thisr loosely recovers the objectives used in some prior works [19,6]. For a general time-dependent surrogate reward functionr t , we define the training objective as
J T (π) = E s0∼ρ,at∼π(·|st),st+1∼p(·|st,at) H T t=1 γ tr t (s t , a t )(2)
whereρ is the initial state distribution at training time (which does not need to match the evaluationtime initial state distribution ρ). The persistent RL optimization objective is to maximize J T (π) efficiently under the constraint that J E (arg max π J T (π)) = max π J E (π). Intuitively, the objective encourages construction of a training environment that can recover the optimal policy for the evaluation environment. The primary design choice isr t , which as shown above leads to different algorithms. Another design choice isρ, which may or may not match ρ. Importantly, we do not assumeρ is any easier to sample compared to ρ.</p>
<p>Finally, we note that the formulation discussed here is suitable only for reversible environments. Reversible environments guarantee that the agent can continue to make progress on the task and not get "stuck" (for example, if the object goes out of reach of the robot's arm). A large class of practical tasks can be considered reversible (door opening, cloth folding, and so on) or the environment can be constructed to enforce reversibility (add bounding walls so the object does not go out of reach). A formal definition for reversible environments is provided in Appendix A. In this work, we will restrict ourselves to reversible environments, and defer a full discussion of persistent RL for environments with irreversible states to future work.</p>
<p>Goal-conditioned persistent reinforcement learning. We adapt the general formulation above to a goal-conditioned [20,38] instantiation of persistent RL. Consider a goal-conditioned MDP M E ≡ (S, A, G, p, r, ρ, γ, H E ), where G ⊆ S denotes the goal space. For a goal distribution p g :
G → R ≥0 , the evaluation objective is J E (π) = E g∼pg(·) E π(·|s,g) [ H E t=1 γ t r(s, g)] for π : S × A × G → R ≥0 .
The training objective is then stated as:
J T (π) = E s0∼ρ,at∼π(·|st,G(st,pg)),st+1∼p(·|st,at) H T t=1 γ t r(s t , G(s t , p g )) ,(3)
where we assume thatr = r remains as a goal reaching objective, but where algorithms instead use a goal generator G to generate a curriculum of goals to practice throughout training 3 . The intuition is that, since H T H E , the algorithm can repeatedly practice reaching various task-goals. However, the objective is to learn a policy that can reach task-goals from p g in the test environment, i.e., starting from the initial state distribution ρ. This implies the goal generator G should expand the goal space beyond the task-goals to improve the policy π for the test environment. For example, the goal generator could alternate task-goals (g ∼ p g ) and the initial state distribution (s ∼ ρ), which again loosely recovers prior works [19,6]. This instantiation transforms the problem of finding the right reward functionr to the right curriculum of goals using G.</p>
<p>Value-Accelerated Persistent Reinforcement Learning</p>
<p>To address the goal-conditioned persistent RL problem, we now describe our proposed algorithm, VaPRL. The key idea in VaPRL is that the agent does not need to return to the initial state distribution between every attempt at the task, and can instead choose to practice from states that facilitate efficient learning. Section 4.1 discusses how to generate this curriculum of initial states. Using  Figure 3: An overview of the VaPRL algorithm (left) compared to forward-backward RL (right). For VaPRL, the value function gives us a set of states from where the agent can solve the task with some confidence (shaded in green), and the VaPRL chooses the state closest to the initial state distribution among them (purple square). In each iteration, the agent can bootstrap on the knowledge of solving the task from a future state (bold green) which simplifies the exploration from its current state (broken green line). As the performance of the agent improves, the states commanded by VaPRL move closer to the initial state distribution. This is in contrast to the forward-backward controller that alternates between the test-goals and the initial state distribution.</p>
<p>goal-conditioned RL within VaPRL allows us to use the same policy to solve the task and reach the initial states suggested by the curriculum, in contrast to prior work that learns a separate reset and task policy. Section 4.2 describes how careful goal relabeling can be leveraged to efficiently learn this unified goal-reaching policy. We also discuss how VaPRL can effectively use prior data, which often becomes crucial for efficiently solving hard sparse-reward tasks.</p>
<p>Generating a Curriculum Using the Value Function</p>
<p>Consider the problem of reaching a goal g ∼ p g in the MDP M E . Learning how to reach the goal g is easier starting from a state s ∈ S that is close to g, especially when the rewards are sparse. Knowing how to reach the goal g from a state s in turn makes it easier to reach the goal from states in the neighborhood of s, enabling us to incrementally move farther away from the goal g. Bootstrapping on the success of an easier problem to solve a harder problems motivates the use of curriculum in reinforcement learning, also illustrated in Figure 3.</p>
<p>Following the intuition above, we aim to define an increasingly-difficult curriculum such that the policy is eventually able to reach the goal g starting from the initial state distribution ρ. Our simple scheme is to sample a task goal g ∼ p g , run the policy π with a subgoal C(g) as input, and then run the policy with the task goal g as input. The main question now becomes: given a goal g, how do we select the subgoal C(g) to attempt the goal g from? We propose to set up C(g) as follows:
C(g) = arg min s X ρ (s) s.t. V π (s, g) ≥ ,(4)
where X ρ is a user-specified distance function between the state s and the initial state distribution ρ,
V π (s, g) = E[ H E t=1 γ t r(s, g) | s 1 = s]
denotes the value function of the policy π reaching the goal g from the state s, and ∈ R is some fixed threshold. Here, the value function represents the ability of the policy to reach the goal g from the state s. To see that, consider the case when discount factor γ = 1 and r(s, g) = 1 when s ≈ g and 0 otherwise, the value function V π exactly represents the probability of reaching a goal g from state s when following the policy π. The intuition carries over to γ ∈ [0, 1) too, where the environment can go into a terminal state with probability 1 − γ at every transition. For general goal-reaching reward functions, a state s with a higher value under V π (s, g) would still represent greater ability to reach the goal g for the policy π.</p>
<p>Revisiting Equation 4 with this understanding of the value function, the objective C(g) chooses the state closest to the initial state distribution for which the value function V π (s, g) crosses the threshold . This encourages the curriculum to be closer to the goal state in the early stages of the training as the policy would be less effective at reaching the goal. As the policy improves, a larger number of states satisfy the constraint and the curriculum progressively moves closer to the initial state distribution. Eventually, the curriculum converges to the initial state distribution leading to a policy π that would optimize the evaluation objective in the MDP M E . Following this intuition, we can write the goal generator G(s t , p g ) as:</p>
<p>G(s t , p g ) = g s.t. g task ∼ p g , g ← C(g task ) if switch(s t , g) = subgoal g ← g task elif switch(s t , g) = task goal (5) where the switch(s t , g cur ) is true if the g cur has been reached or g cur has been in place for H E steps. For every new goal g task ∼ p g , we first attempt to reach the curriculum subgoal C(g task ) (that is switch(s t , g) = subgoal), and then we attempt to reach goal g task (that is switch(s t , g) = task goal). This cycles repeats until the environment resets after H T steps.</p>
<p>Computing the Curriculum Generator C(g). Equation 4 involves a minimization over the state space S, which is intractable in general. While it is possible to come up with general solutions by constructing a generative model p(s) and taking a minimum over the samples generated by it, we opt for a simpler solution: we use the data collected by the policy π during the training and minimize C(g) over a randomly sampled subset of it by enumeration. If an offline dataset or demonstrations are available, we can also minimize C(g) on this data exclusively. The constrained minimization can similarly be approximated by considering the subset of the data which satisfies the constraint V π (s, g) ≥ and choosing the state from this subset which minimizes X ρ . If no state satisfies the constraint, C(g) returns the state with the maximum V π (s, g).</p>
<p>Measuring the Initial State Distribution Distance. An important component of curriculum generation is choosing the distance function X ρ (s), which should reflect the distance to the state from the initial state distribution under the environment dynamics. To factor in the dynamics, we can use the learned goal-conditioned value function as measure of shortest distance between the state and the goal [20,36]. In particular, we use X ρ (s) = −E s0∼ρ V π (s, s 0 ). While this choice is convenient as we are already estimating V π (s, g), there is an even simpler choice for X ρ (s) when offline demonstration data is available. Assuming that the trajectories in the provided dataset start from the initial state distribution ρ, we can use the timestep index of the state as the distance from the initial state distribution, that is X ρ (s) = arg t (s, D) where D denotes the offline demonstration set. The step index distance function defined here encodes the intuition that states which require more steps by the policy are farther away. The function naturally accommodates for the dynamics of the environment. Finally, since we are minimizing X ρ (s) in Equation 4, if there are multiple trajectories to the same state or suboptimal loops within a single trajectory, we use the shortest distance to that state.</p>
<p>Relabeling Goals</p>
<p>Curriculum Goals ….  Not only does our policy need to learn how to reach g ∼ p g , but it also needs to learn how to reach all the goals generated by C(g) over the course of training, causing the effective goal-space to grow substantially. However, there is a lot of shared structure in reaching goals, especially those generated by the curriculum. The knowledge of how to reach a goal g 1 also conveys meaningful information about how to reach a goal g 2 . This structure can be leveraged by using techniques from goal relabeling [2]. In particular, we relabel every trajectory collected in the training environment with N goals sampled randomly from the set of goals that may be a part of the curriculum. If we do not have any prior data, we randomly sample the replay buffer for relabeling goals. If we are given some prior data D, this reduces sampling to g ∼ D ∪ {g ∼ p g } for relabeling.</p>
<p>There is a subtle difference between hindsight experience replay (HER) and the goal relabeling strategy we employ. While HER chooses future states from within an episode as goals for relabeling, we exclusively choose states that may be used as goal states in the curriculum, which may not occur in the collected trajectory at all. Since our policy will only be tasked with reaching goals generated by the goal generator G, it is advantageous to extract signal specifically for these goals.</p>
<p>To summarize, while the goal-space has grown, goal relabeling enables us to generate data for the algorithm commensurately to improve sample-efficiency. Initialize replay buffer B, π(a | s, g), Q π (s, a, g); // If demos, add them to replay buffer and relabel B ← B ∪ D; relabel_demos(B); while not done do s ∼ρ; // sample initial state for HT steps do g ← G(s, pg) (Eq 5); a ∼ π(· | s, g), s ∼ p(· | s, a); B ← B ∪ {(s, a, s , g, r(s , g))}; for i ← 1, i ≤ N dõ g ∼ D ∪ pg; // if D = ∅, sample replay buffer B ← B ∪ {(s, a, s ,g, r(s ,g)}; update π, Q π ; s ← s ; generator G to get the current goal and collects the next transition using the current policy π. This transition is added to replay buffer R along with N relabelled transitions, as described in Sec 4.2. The policy π and the critic Q π are updated every step, using any off-policy reinforcement learning algorithm. This loop is repeated for H T steps till an extrinsic intervention resets the environment to a state s ∼ρ. Note, it isn't necessary to initialize the agent close to the goal. Additional details pertaining to the algorithm can be found in the Appendix B.</p>
<p>Experiments</p>
<p>In this section, we study the performance of VaPRL on continuous control environments for goalconditioned persistent RL and provide ablations and visualization to isolate the effect of the curriculum. In particular, we aim to answer the following questions:</p>
<ol>
<li>Does VaPRL allow efficient reinforcement learning with minimal episodic resets? 2. How does the scheme for generating a curriculum in VaPRL perform compared to other methods for persistent reinforcement learning? 3. Does VaPRL scale to high dimensional state and action spaces? 4. What does the generated curriculum look like? Is the curriculum effective?</li>
</ol>
<p>We next describe the specific choices of environments, evaluation metrics and comparisons in order to answer the questions above.</p>
<p>Environments. For our experimental evaluation, we consider three continuous control environments, shown in Figure 5. The table-top rearrangement is a simplified manipulation environment, where a gripper (modelled as a point mass which can attach to the object if it is close to it) is tasked with taking the mug to one of the 4 potential goal squares. The evaluation horizon is H E = 200 steps and the training horizon is H T = 200, 000 steps. This task involves a challenging exploration problem in navigating to objects, picking them up, and dropping them at the right location. The sawyer door closing environment involves using a sawyer robot arm to close a door to a particular target angle [45]. For this environment, we set the horizon for evaluation to be H E = 400 and H T = 200, 000 steps for training. Since environment resets are not freely available, repeatedly practicing the task implicitly requires the agent to also learn how to open the door. The hand manipulation environment, introduced in [14], involves a dexterous hand attached to a sawyer robot. This environment entails a 16 DoF hand that is mounted onto a 6 DoF arm, with the goal of manipulating a 3 pronged object as seen in Figure 5. In particular, the task involves picking up the object from random positions on a table and lifting it to a goal position above the table. This task is particularly challenging since it involves complex contact dynamics with high dimensional state and action spaces. Additionally, the robot has to learn how to reconfigure the object to diverse locations to simulate the test-time conditions where the agent is expected to pickup the object from random locations. For this environment, we set the horizon for evaluation H E = 400 and for training H T = 400, 000 steps. Environment Setup. For table-top rearrangement and sawyer door closing, we consider a sparse reward function r(s, g) = I(s, g), which is 1 when the state s is close to the goal position g, and 0 otherwise. Since the hand manipulation environment is a substantially more challenging problem, we consider a dense reward function that rewards the the hand and the object to be close to the goal position. To aid exploration in table-top rearrangement and sawyer door closing, we provide all the algorithms with a small set of trajectories (6 per goal, 3 going from initial state to the goal and the other 3 going in reverse) for each environment, though we do not assume that the trajectories take the optimal path (for example, the trajectories could come from teleoperation in practice). For the hand manipulation environment, we provide the agent with 10 trajectories demonstrating the pickup task from random positions on the table and 20 trajectories showing how to reposition the object to different locations on the table. For all environments, we report results by evaluating the number of times the policy successfully reaches the goal out of 10 trials in the evaluation environment M E (by resetting to a state from the initial state distribution ρ and sampling an appropriate goal from the goal distribution p g ), performing intermittent evaluations as the training progresses. Note, the training agent does not receive the evaluation experience and it is only used to measure the performance on the evaluation environment. Further details about problem setup, demonstrations, implementation, hyperparameters and evaluation metrics can be found in the Appendix.</p>
<p>Comparisons. We compare VaPRL to four approaches: (a) A standard off-policy RL algorithm that only trains to reach the goal distribution, such that a new goal g ∼ p g (s) is sampled every H E steps (labelled naïve RL), (b) A forward-backward controller [19,6] which alternates between g ∼ p g (s) and g ∼ ρ(s) for H E steps each, as described in Section 3 (labelled FBRL), (c) A perturbation controller [48] that alternates between optimizing a controller to maximize task reward and a controller to maximally perturb the state via task agnostic exploration (labelled R3L), and (d) RL directly on the evaluation environment, resetting to the initial state distribution after every H E steps (labelled oracle RL). This oracle is an expected upper bound on the performance of VaPRL, since it has access to episodic resets. We use soft actor-critic [17] as the base RL algorithm for all methods to ensure fair comparison, although any value-based method would be equally applicable. To emphasize, all the algorithms are provided the same set of demonstrations. Further implementation details can be found in the Appendix.</p>
<p>Persistent RL Results</p>
<p>The performance of each of the algorithms on the three evaluation domains are shown in Figure 6. We see that VaPRL outperforms naïve RL, FBRL and R3L, providing substantial improvements in terms of sample efficiency. For our most challenging domain of hand manipulation, the sample efficiency enables us a reach a much better performance within the training budget. The primary difference between the methods is that VaPRL uses a curriculum of starting states progressing from easier to harder states. In contrast, FBRL always attempts to reach the initial state distribution and R3L uses a perturbation controller to reach novel states in attempt to cover the entire state space uniformly. In the table-top rearrangement environment, the agent starts close to the goal and then gradually brings it back to the initial state distribution, trying different intermediate states in the process (discussed in Section 5.3). In the sawyer door closing environment, the agent learns to close the door from intermediate angles, incrementally improving the performance. For the hand manipulation domain, the agent focuses on picking up the object from a particular location, and then incrementally grow the locations from which it can complete pickup. In contrast, FBRL chooses attempts the pickup from random states from the initial state distribution ρ and R3L attempts to find new states to pickup the object from (even though it might not be succeeding to pickup the object from previous locations). Figure 8: Visualization of curricula generated by VaPRL on the table-top rearrangement environment. We plot the step index distance between the initial state and the curriculum goals generated by C(g) (blue) and the evaluation performance (orange) as the training progresses. The distance is normalized to be on the same scale as the success metric, such that a value of 1 corresponds to the test-goal distribution and 0 corresponds to the initial state distribution. We visualize some of the commanded goals C(g) during the training, observing that the curriculum gradually progresses from goal states to initial states with a correlated improvement in evaluation performance.</p>
<p>Compared to oracle RL with resets, VaPRL learns to solve the task while requiring 500× fewer environment resets in the door closing environment, 1000× fewer environment resets in the table-top rearrangement environment and dexterous hand manipulation. This amounts to less than 20 total interventions for VaPRL, indicating the substantial autonomy with which the algorithm can run.</p>
<p>Surpisingly, in the domains with a sparse reward function (that is, sawyer door closing and table-top reaarrangement), VaPRL matches or even outperforms the oracle RL method. In the table-top rearrangement environment, oracle RL does substantially worse than VaPRL. It has been noted in prior work that multi-goal RL problems can converge suboptimally due to issues arising in the optimization [44]. We hypothesize that an appropriate initial state distribution can ameliorate some of these issues. In particular, moving beyond deterministic initial distributions may lead to better downstream performance (also noted in [47]). For the door opening environment, VaPRL matches the performance of oracle RL. To emphasize, oracle RL is training on the evaluation environment directly, that is H T = H E with the environment resetting to a state s 0 ∼ ρ. In contrast, VaPRL also learns how to reverse the task it is solving and thus only spends half of its training samples collecting the data for the evaluation task (for example, VaPRL learns how to open the door and close it). We construct an experiment to isolate the effect of the starting state distribution on learning efficiency and downstream performance. In this experiment, the environment resets directly to the state C(g) for VaPRL, such that the policy only has to learn reaching the goals g ∼ p g (labelled VaPRL + reset). Analogously, for FBRL, the environment resets to the state s 0 ∼ ρ, which is identical to the oracle RL method (labelled oracle RL / FBRL + reset). For R3L, the environment resets to a state uniformly sampled from the state space (labelled uniform / R3L + reset). We run this ablation on the table-top rearrangement environment, where the episode horizon for all the algorithms is H E = 200. The results in Figure 7 indicate that the starting state distribution induced by the VaPRL curriculum improves the performance, translating into improved performance in the persistent RL setting.</p>
<p>Isolating the Role of the Initial State Distribution</p>
<p>Visualizations of Generated Curricula</p>
<p>To better understand the curriculum generated by VaPRL, we visualize the sequence of states chosen by Equation 4 as the training progresses on the table-top rearrangement environment, shown in Figure 8. As we can observe, initially the curriculum chooses states which are farther away from the initial state and closer to the goal distribution. As training progresses, the curriculum moves towards the initial state distribution. Correspondingly, the evaluation performance starts to improve as we move closer to the initial state distribution. Thus, VaPRL can generates an adaptive curriculum for the agent to efficiently improve the performance on the evaluation setting.</p>
<p>Conclusion</p>
<p>In this work, we propose VaPRL, an algorithm that can efficiently solve reinforcement learning problems with minimal episodic resets by generating a curriculum of starting states. VaPRL is able to reduce amount of human intervention required in the learning process by a factor of 1000 compared to episodic RL, while outperforming prior methods. In the process, we also formalize the problem setting of persistent RL to understand current algorithms and aid the development of future ones.</p>
<p>There are a number of interesting avenues for future work that VaPRL does not currently address. A natural extension is to environments with irreversible states. This setting can likely be addressed by leveraging ideas from the literature in safe reinforcement learning [11,42,3,5]. Another extension is to work with visual state spaces, allowing the algorithm to be more broadly applicable in the real world. These two extensions would be a significant step towards enabling autonomous agents in the real world that minimize the reliance on human interventions.</p>
<p>Disclosure of Funding</p>
<p>This work was supported in part by Schmidt Futures and ONR grant N00014-21-1-2685.</p>
<p>A Ergodic MDPs.</p>
<p>As alluded to in Section 3, the formulation discussed in this paper is suitable for reversible environments. For an environment to be considered reversible, we assume that the MDP M E is ergodic, as defined in [31]. The MDP is considered ergodic if for all states a, b ∈ S, ∃π, such that E s∼π(s)|s0=a [I{s = b}] &gt; 0, where I denotes the indicator function, and s is sampled from the trajectory generated by following policy π starting from the state a. Any policy which assigns a non-zero probability to all actions will ensure that all states in the environment are visited in the infinite limit for ergodic MDPs, satisfying the condition above.</p>
<p>B Implementing VaPRL</p>
<p>VaPRL uses SAC [17] as the base RL algorithm, following the implementation in [18]. Hyperparameters follow the default values: initial collect steps: 10,000, batch size sampled from replay buffer for updating policy and critic: 256, steps collected per iteration: 1, trained per iteration: 1, discount factor: 0.99, learning rate: 3e − 4 (for critics, actors and dual gradient descent used to adjust entropy temperature). The actor and critic network were parameterized as neural networks with two hidden layers each of size 256. The output of the actor network is passed through a tanh non-linearity to scale all action dimensions to [−1, 1]. Two key differences from the default hyperparameters: The size of the replay buffer was large enough to ensure that none of the collected and relabelled experience is discarded. For sawyer door closing and table-top rearrangement, the replay buffer has a size of 10M and for hand manipulation environment, the replay buffer had a size of 25M. While the weight for entropy is automatically adjusted using dual gradient descent, it was helpful to have a higher initial weight on the reward for environments with a sparse reward function. So, the initial value of temperature α = 0.1 for sawyer door closing and table-top rearrangement (equivalent to reward being scaled 10 times), while for hand manipulation environment, the initial value is the default α = 1.</p>
<p>VaPRL computes
V π (s) = E a∼π(·|s) [Q π (s, a)] ≈ 1 L L i=1 Q π (s, a i ) where a i ∼ π(· | s) for L = 5. Note, Q π (s, a) = E[ H E
t=0 γ t r(s t , a t )] needs to be estimated in addition to the critic function estimated by SAC, as the default critic adds the entropy of the policy to the reward while computing the expected sum. Q π (without the compounded entropy) is estimated identically to the default critic otherwise.</p>
<p>As shown in Algorithm 1, there are two instances of goal relabeling: for trajectories collected online and for demonstrations. For every trajectory collected online, VaPRL samples N goals and relabels these trajectories to generate N new trajectories. The goals are sampled from D∪{g ∼ p g }, that is the set of states in the demonstrations and the goal distribution. N = 4 is fixed for all environments for online relabeling. A similar scheme to relabel the demonstration set can be followed. However, if the demonstration set is small, a denser of set of relabelled trajectories by using every intermediate state in the trajectory as a goal can be more informative. For a demonstration {s 0 , s 1 , s 2 . . . s T }, first generate a relabelled trajectory with s 0 as the goal, then with s 1 and so on to create T new trajectories for every trajectory in the demonstration. VaPRL follows this scheme for table-top rearrangement and sawyer door closing as these environments only have 6 demonstrations per goal. However, for hand manipulation environment, VaPRL reverts to N = 4 randomly sampled goals to relabel each demonstration. As there are nearly 30 demonstrations for hand manipulation environment, the dense relabeling scheme would produce greater than 1M samples even before any data collection.</p>
<p>VaPRL also uses the demonstrations to compute the distance function X ρ (s). For a trajectory going from initial state to the goal {s 0 , s 1 , . . . s T }, X ρ (s 0 ) = 0, X ρ (s 1 ) = 1 and so on. Similarly, if VaPRL has demonstrations going from goal to the initial, it can either exclude them from the curriculum or label them X ρ (s) in the reverse order. For table-top rearrangement and sawyer door closing, VaPRL opts for the latter. For hand manipulation, there are no demonstrations corresponding reversing the task, so VaPRL simply excludes the trajectories corresponding to object repositioning from the curriculum. To compute the curriculum goal C(g) in Equation 4, VaPRL minimizes X ρ (s) over the set of states where V π (s, g) ≥ . If multiple states minimize X ρ (s) while satisfying the constraint, VaPRL chooses a random state amongst the states with minimal X ρ (s). If no state satisfies the constraint, VaPRL chooses the state which maximizes V π (s, g).</p>
<p>C Experimental Setup</p>
<p>First, we describe the reward functions and the success metrics corresponding to each environment. r(s, g) = I( s − g 2 ≤ 0.2), where I denotes the indicator function. The success metric is the same as the reward function. The environment has 4 possible goal locations for the mug, and goal location for the gripper is in the center.</p>
<p>sawyer door closing:
r(s, g) = I( s − g 2 ≤ 0.1),
where I again denotes the indicator function. The success metric is the same as the reward function.</p>
<p>The goal for the door and the robot arm is the closed door position. (g x , g y , g z ) 2 corresponds to the distance between the object and goal. The reward function encourages the hand to be close to the object and the object to be close to the goal (with higher weight on the object being close to the goal as the coefficient is 10). The exponential terms are bonuses which are close to 0 when the object is far from the goal and close to 10 when the object is close to the goal. The success metric for hand manipulation is given by I(d(o, g) ≤ 0.05). The goal for the agent is to bring the object to the center raised 0.2 meters above the table.</p>
<p>For sawyer door closing and table-top rearrangement, the environment terminates whenever the agent reaches the goal. Therefore, the maximum return in the environment is 1. We set the value function threshold = 0.1 for these environments. For hand manipulation, the environment terminates after H E = 400 steps regardless of whether the goal has been achieved. The minimum and maximum return are roughly −3000 and 7000. We set the threshold = −300.</p>
<p>Next, we discuss the details corresponding to demonstrations for each of the environments.</p>
<p>table-top rearrangement: The demonstrations were generated by a human with a discrete action space of {up, down, right, left, grip} which were translated with into noisy continuous actions. We collected 3 demonstrations taking the mug from the initial state to the goal, and 3 demonstrations reversing those trajectories, for a total of 24 demonstrations (as there are 4 possible goal locations). These demonstrations were sub-optimal as the discrete actions took a smaller step size than the environment allowed (to keep the discrete action space small while still being able to solve the task) and also took sub-optimal route between the initial state and the goals.</p>
<p>sawyer door closing: The demonstrations were generated using controller trained via reinforcement learning on an environment with dense rewards and episodic reset interventions. The final demonstrations generated used a noisy version of this learned policy. All methods were provided with 3 demonstrations closing the door from the initial position and 3 demonstrations opening the door from the closed position.</p>
<p>hand manipulation: The demonstrations for this environment were particularly hard to generate even when using reinforcement learning with episodic reset interventions. We first learned a policy to reposition the object to the center, next we learned a separate policy to raise the object to a height of 0.2 meters above the center from the center position. We also learned a policy to move the object to different positions on the table. We generated 10 trajectories from the first two policies, and 20 trajectories for the last policy and used this as our demonstration dataset. Note, the demonstrations provided are again sub-optimal. In fact, we were not able to provide a single continuous demonstration picking up the object from arbitrary positions on the table (forcing us to train separate policies and collect disjoint demonstrations).</p>
<p>All the baselines use the same hyperparameters and environmental setup as those for VaPRL and have access to same set of demonstrations.</p>
<p>We do not use GPUs for any of the experiments. We used an internal cluster to parallelize the run for different seeds and baselines. The table-top rearrangement environment was run for ∼ 12 hours, the sawyer door closing environment was run for 18 hours and hand manipulation environment was run for 7 days when using VaPRL and R3L, 5 days when using oracle RL, FBRL and naive RL. We prematurely stopped oracle RL, FBRL and naive RL due to limited computational budget and because the performance of these algorithms showed no signs of further progress (oracle RL had converged while FBRL and naive RL were not improving at solving the task).</p>
<p>Figure 2 :
2The performance of episodic RL algorithms substantially deteriorates when environment resets are not available.</p>
<p>Figure 4 :
4An illustration of goal relabeling in VaPRL. Every transition in a trajectory is relabeled with a randomly sampled subset of curriculum goals, yielding a large set of relabeled tuples that are added to the replay buffer. This ensures efficient data reuse.</p>
<p>Algorithm
Summary. The outline for VaPRL is given in Algorithm 1. At a high level, VaPRL takes a set of demonstrations as input and adds it to the replay buffer R. These demonstrations are relabelled to generate additional trajectories such that every intermediate state is used as a goal. Next, VaPRL starts collecting data in the training MDP M T . At every step, VaPRL samples the goal</p>
<p>Figure 5 :
5Continuous control environments for goalconditioned persistent RL. (top left) A table-top rearrangement task, where a gripper is tasked with moving the mug to four potential goal positions, (top right) a sawyer robot learns how to close the door and (bottom) a high-dimensional dexterous hand attached to a sawyer robot is tasked to pick up a three-pronged object.</p>
<p>Figure 6 :
6Performance of each method on (left) the table-top rearrangement environment, (center) the sawyer door closing environment, and (right) the hand manipulation environment. Plots show learning curves with mean and standard error over 5 random seeds. VaPRL is more sample-efficient and outperforms prior methods.</p>
<p>Figure 7 :
7Ablation isolating the effect of curriculum generated by VaPRL.</p>
<p>hand manipulation:r(s, g) = 4 · d(h, o) + 10 · d(o, g) + 10 · e −d(o,g) 2 /0.01 + 10 · e −d(o,g) 2 /0.001 where d(h, o) = (h x , h y , h z ) − (o x ,o y , o z ) 2 corresponds to the distance between hand and the object and d(o, g) = (o x , o y , o z ) −</p>
<h2>table - top rearrangement:</h2>
<p>Code and supplementary videos are available at https://sites.google.com/view/vaprl/home 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We assume that the state includes information indicating the reward function being optimized so that agent can take appropriate actions, for example, one-hot task indicators as is common in multi-task RL.3  The goal generator may use additional memory which is not explicitly represented here.
(c) Did you discuss any potential negative societal impacts of your work?[No], this work inherits the potential negative societal impacts of reinforcement learning and its applications to robotics. We do not anticipate any additional negative impacts that are unique to this work.
Learning to poke by poking: Experiential learning of intuitive physics. D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. GarnettNeurIPSLearning to poke by poking: Experiential learning of intuitive physics. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, NeurIPS, 2016.</p>
<p>M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, arXiv:1707.01495Hindsight experience replay. arXiv preprintM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience replay. arXiv preprint arXiv:1707.01495, 2017.</p>
<p>Safe model-based reinforcement learning with stability guarantees. F Berkenkamp, M Turchetta, A P Schoellig, A Krause, arXiv:1705.08551arXiv preprintF. Berkenkamp, M. Turchetta, A. P. Schoellig, and A. Krause. Safe model-based reinforcement learning with stability guarantees. arXiv preprint arXiv:1705.08551, 2017.</p>
<p>Combining modelbased and model-free updates for trajectory-centric reinforcement learning. Y Chebotar, K Hausman, M Zhang, G Sukhatme, S Schaal, S Levine, International conference on machine learning. PMLRY. Chebotar, K. Hausman, M. Zhang, G. Sukhatme, S. Schaal, and S. Levine. Combining model- based and model-free updates for trajectory-centric reinforcement learning. In International conference on machine learning, pages 703-711. PMLR, 2017.</p>
<p>A lyapunov-based approach to safe reinforcement learning. Y Chow, O Nachum, E Duenez-Guzman, M Ghavamzadeh, arXiv:1805.07708arXiv preprintY. Chow, O. Nachum, E. Duenez-Guzman, and M. Ghavamzadeh. A lyapunov-based approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708, 2018.</p>
<p>Leave no trace: Learning to reset for safe and autonomous reinforcement learning. B Eysenbach, S Gu, J Ibarz, S Levine, arXiv:1711.06782arXiv preprintB. Eysenbach, S. Gu, J. Ibarz, and S. Levine. Leave no trace: Learning to reset for safe and autonomous reinforcement learning. arXiv preprint arXiv:1711.06782, 2017.</p>
<p>Deep visual foresight for planning robot motion. C Finn, S Levine, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEEC. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017.</p>
<p>Deep spatial autoencoders for visuomotor learning. C Finn, X Y Tan, Y Duan, T Darrell, S Levine, P Abbeel, 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEEC. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel. Deep spatial autoencoders for visuomotor learning. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 512-519. IEEE, 2016.</p>
<p>Reverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, M Zhang, P Abbeel, Conference on robot learning. PMLRC. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel. Reverse curriculum generation for reinforcement learning. In Conference on robot learning, pages 482-495. PMLR, 2017.</p>
<p>Automatic goal generation for reinforcement learning agents. C Florensa, D Held, X Geng, P Abbeel, International conference on machine learning. PMLRC. Florensa, D. Held, X. Geng, and P. Abbeel. Automatic goal generation for reinforcement learning agents. In International conference on machine learning, pages 1515-1528. PMLR, 2018.</p>
<p>A comprehensive survey on safe reinforcement learning. J Garcıa, F Fernández, Journal of Machine Learning Research. 161J. Garcıa and F. Fernández. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437-1480, 2015.</p>
<p>Deep predictive policy training using reinforcement learning. A Ghadirzadeh, A Maki, D Kragic, M Björkman, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEA. Ghadirzadeh, A. Maki, D. Kragic, and M. Björkman. Deep predictive policy training using reinforcement learning. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2351-2358. IEEE, 2017.</p>
<p>Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. S Gu, E Holly, T Lillicrap, S Levine, 2017 IEEE international conference on robotics and automation (ICRA). IEEES. Gu, E. Holly, T. Lillicrap, and S. Levine. Deep reinforcement learning for robotic manipula- tion with asynchronous off-policy updates. In 2017 IEEE international conference on robotics and automation (ICRA), pages 3389-3396. IEEE, 2017.</p>
<p>Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention. A Gupta, J Yu, T Zhao, V Kumar, A Rovinsky, K Xu, T Devlin, S Levine, abs/2104.11203ArXiv. A. Gupta, J. Yu, T. Zhao, V. Kumar, A. Rovinsky, K. Xu, T. Devlin, and S. Levine. Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention. ArXiv, abs/2104.11203, 2021.</p>
<p>Learning to walk in the real world with minimal human effort. S Ha, P Xu, Z Tan, S Levine, J Tan, S. Ha, P. Xu, Z. Tan, S. Levine, and J. Tan. Learning to walk in the real world with minimal human effort, 2020.</p>
<p>Learning to walk via deep reinforcement learning. T Haarnoja, S Ha, A Zhou, J Tan, G Tucker, S Levine, arXiv:1812.11103arXiv preprintT. Haarnoja, S. Ha, A. Zhou, J. Tan, G. Tucker, and S. Levine. Learning to walk via deep reinforcement learning. arXiv preprint arXiv:1812.11103, 2018.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International Conference on Machine Learning. PMLRT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861-1870. PMLR, 2018.</p>
<p>Tensorflow agents: Efficient batched reinforcement learning in tensorflow. D Hafner, J Davidson, V Vanhoucke, arXiv:1709.02878arXiv preprintD. Hafner, J. Davidson, and V. Vanhoucke. Tensorflow agents: Efficient batched reinforcement learning in tensorflow. arXiv preprint arXiv:1709.02878, 2017.</p>
<p>Learning compound multi-step controllers under unknown dynamics. W Han, S Levine, P Abbeel, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEW. Han, S. Levine, and P. Abbeel. Learning compound multi-step controllers under unknown dynamics. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6435-6442. IEEE, 2015.</p>
<p>Learning to achieve goals. L P Kaelbling, IJCAI. CiteseerL. P. Kaelbling. Learning to achieve goals. In IJCAI, pages 1094-1099. Citeseer, 1993.</p>
<p>Approximately optimal approximate reinforcement learning. S Kakade, J Langford, Proc. 19th International Conference on Machine Learning. 19th International Conference on Machine LearningCiteseerS. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.</p>
<p>Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, arXiv:1806.10293arXiv preprintD. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakr- ishnan, V. Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018.</p>
<p>D Kalashnikov, J Varley, Y Chebotar, B Swanson, R Jonschkowski, C Finn, S Levine, K Hausman, arXiv:2104.08212Continuous multi-task robotic reinforcement learning at scale. arXiv preprintD. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.</p>
<p>K Khetarpal, M Riemer, I Rish, D Precup, arXiv:2012.13490Towards continual reinforcement learning: A review and perspectives. arXiv preprintK. Khetarpal, M. Riemer, I. Rish, and D. Precup. Towards continual reinforcement learning: A review and perspectives. arXiv preprint arXiv:2012.13490, 2020.</p>
<p>Reinforcement learning in robotics: A survey. J Kober, J A Bagnell, J Peters, The International Journal of Robotics Research. 3211J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238-1274, 2013.</p>
<p>Policy gradient reinforcement learning for fast quadrupedal locomotion. N Kohl, P Stone, IEEE International Conference on Robotics and Automation. IEEE3Proceedings. ICRA'04N. Kohl and P. Stone. Policy gradient reinforcement learning for fast quadrupedal locomotion. In IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA'04. 2004, volume 3, pages 2619-2624. IEEE, 2004.</p>
<p>Skill discovery in continuous reinforcement learning domains using skill chaining. G Konidaris, A Barto, Advances in neural information processing systems. 22G. Konidaris and A. Barto. Skill discovery in continuous reinforcement learning domains using skill chaining. Advances in neural information processing systems, 22:1015-1023, 2009.</p>
<p>End-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, P Abbeel, The Journal of Machine Learning Research. 171S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.</p>
<p>Reset-free lifelong learning with skill-space planning. K Lu, A Grover, P Abbeel, I Mordatch, arXiv:2012.03548arXiv preprintK. Lu, A. Grover, P. Abbeel, and I. Mordatch. Reset-free lifelong learning with skill-space planning. arXiv preprint arXiv:2012.03548, 2020.</p>
<p>Teacher-student curriculum learning. T Matiisen, A Oliver, T Cohen, J Schulman, IEEE transactions on neural networks and learning systems. 31T. Matiisen, A. Oliver, T. Cohen, and J. Schulman. Teacher-student curriculum learning. IEEE transactions on neural networks and learning systems, 31(9):3732-3740, 2019.</p>
<p>Safe exploration in markov decision processes. T M Moldovan, P , arXiv:1205.4810arXiv preprintT. M. Moldovan and P. Abbeel. Safe exploration in markov decision processes. arXiv preprint arXiv:1205.4810, 2012.</p>
<p>Deep dynamics models for learning dexterous manipulation. A Nagabandi, K Konolige, S Levine, V Kumar, Conference on Robot Learning. PMLRA. Nagabandi, K. Konolige, S. Levine, and V. Kumar. Deep dynamics models for learning dexterous manipulation. In Conference on Robot Learning, pages 1101-1112. PMLR, 2020.</p>
<p>Learning curriculum policies for reinforcement learning. S Narvekar, P Stone, arXiv:1812.00285arXiv preprintS. Narvekar and P. Stone. Learning curriculum policies for reinforcement learning. arXiv preprint arXiv:1812.00285, 2018.</p>
<p>Autonomous helicopter flight via reinforcement learning. A Y Ng, H J Kim, M I Jordan, S Sastry, S Ballianda, NIPS. Citeseer16A. Y. Ng, H. J. Kim, M. I. Jordan, S. Sastry, and S. Ballianda. Autonomous helicopter flight via reinforcement learning. In NIPS, volume 16. Citeseer, 2003.</p>
<p>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. L Pinto, A Gupta, 2016 IEEE international conference on robotics and automation (ICRA). IEEEL. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages 3406-3413. IEEE, 2016.</p>
<p>Temporal difference models: Model-free deep rl for model-based control. V Pong, S Gu, M Dalal, S Levine, arXiv:1802.09081arXiv preprintV. Pong, S. Gu, M. Dalal, and S. Levine. Temporal difference models: Model-free deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018.</p>
<p>Markov decision processes. Handbooks in operations research and management science. M L Puterman, 2M. L. Puterman. Markov decision processes. Handbooks in operations research and manage- ment science, 2:331-434, 1990.</p>
<p>Universal value function approximators. T Schaul, D Horgan, K Gregor, D Silver, International conference on machine learning. PMLRT. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In International conference on machine learning, pages 1312-1320. PMLR, 2015.</p>
<p>Emergent real-world robotic skills via unsupervised off-policy reinforcement learning. A Sharma, M Ahn, S Levine, V Kumar, K Hausman, S Gu, arXiv:2004.12974arXiv preprintA. Sharma, M. Ahn, S. Levine, V. Kumar, K. Hausman, and S. Gu. Emergent real-world robotic skills via unsupervised off-policy reinforcement learning. arXiv preprint arXiv:2004.12974, 2020.</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. S Sukhbaatar, Z Lin, I Kostrikov, G Synnaeve, A Szlam, R Fergus, arXiv:1703.05407arXiv preprintS. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017.</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>B Thananjeyan, A Balakrishna, S Nair, M Luo, K Srinivasan, M Hwang, J E Gonzalez, J Ibarz, C Finn, K Goldberg, arXiv:2010.15920Recovery rl: Safe reinforcement learning with learned recovery zones. arXiv preprintB. Thananjeyan, A. Balakrishna, S. Nair, M. Luo, K. Srinivasan, M. Hwang, J. E. Gonzalez, J. Ibarz, C. Finn, and K. Goldberg. Recovery rl: Safe reinforcement learning with learned recovery zones. arXiv preprint arXiv:2010.15920, 2020.</p>
<p>Continual learning of control primitives: Skill discovery via reset-games. ArXiv, abs. K Xu, S Verma, C Finn, S Levine, K. Xu, S. Verma, C. Finn, and S. Levine. Continual learning of control primitives: Skill discovery via reset-games. ArXiv, abs/2011.05286, 2020.</p>
<p>Gradient surgery for multi-task learning. T Yu, S Kumar, A Gupta, S Levine, K Hausman, C Finn, arXiv:2001.06782arXiv preprintT. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020.</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Conference on Robot Learning. PMLRT. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pages 1094-1100. PMLR, 2020.</p>
<p>Tossingbot: Learning to throw arbitrary objects with residual physics. A Zeng, S Song, J Lee, A Rodriguez, T Funkhouser, IEEE Transactions on Robotics. 364A. Zeng, S. Song, J. Lee, A. Rodriguez, and T. Funkhouser. Tossingbot: Learning to throw arbitrary objects with residual physics. IEEE Transactions on Robotics, 36(4):1307-1319, 2020.</p>
<p>Dexterous manipulation with deep reinforcement learning: Efficient, general, and low-cost. H Zhu, A Gupta, A Rajeswaran, S Levine, V Kumar, 2019 International Conference on Robotics and Automation (ICRA). IEEEH. Zhu, A. Gupta, A. Rajeswaran, S. Levine, and V. Kumar. Dexterous manipulation with deep reinforcement learning: Efficient, general, and low-cost. In 2019 International Conference on Robotics and Automation (ICRA), pages 3651-3657. IEEE, 2019.</p>
<p>The ingredients of real-world robotic reinforcement learning. H Zhu, J Yu, A Gupta, D Shah, K Hartikainen, A Singh, V Kumar, S Levine, arXiv:2004.12570arXiv preprintd) Have you read the ethics review guidelines and ensured that your paper conforms to them? [YesH. Zhu, J. Yu, A. Gupta, D. Shah, K. Hartikainen, A. Singh, V. Kumar, and S. Levine. The ingredients of real-world robotic reinforcement learning. arXiv preprint arXiv:2004.12570, 2020. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</p>
<p>(a) Did you state the full set of assumptions of all theoretical results. If you are including theoretical results.... N/A] (b) Did you include complete proofs of all theoretical results? [N/AIf you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]</p>
<p>(a) Did you include the code, data, and instructions needed to reproduce the main experimental results. If you ran experiments.... either in the supplemental material or as a URL)? [No] , we will release the code and the environments upon publicationIf you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] , we will release the code and the environments upon publication.</p>
<p>Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes. See Section 5Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] , See Section 5.</p>
<p>data, models) or curating/releasing new assets... (a) If your work uses existing assets. If you are using existing assets (e.g., code,. did you cite the creators? [Yes] See Section 5. (b) Did you mention the license of the assets? [Yes] , See Appendix. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/AIf you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 5. (b) Did you mention the license of the assets? [Yes] , See Appendix. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]</p>
<p>Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content. N/ADid you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]</p>
<p>If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots. N/AIf you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]</p>
<p>Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?. N/ADid you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]</p>
<p>Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation. N/ADid you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</p>            </div>
        </div>

    </div>
</body>
</html>
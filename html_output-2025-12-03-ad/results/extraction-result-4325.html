<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4325 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4325</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4325</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-271843140</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.04665v1.pdf" target="_blank">LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations</a></p>
                <p><strong>Paper Abstract:</strong> The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from literature text has been challenging but crucial for the logical design of new MOFs with desirable functionality. The recent advent of large language models (LLMs) provides disruptively new solution to this long-standing problem and latest researches have reported over 90% F1 in extracting correct conditions from MOFs literature. We argue in this paper that most existing synthesis extraction practices with LLMs stay with the primitive zero-shot learning, which could lead to downgraded extraction and application performance due to the lack of specialized knowledge. This work pioneers and optimizes the few-shot in-context learning paradigm for LLM extraction of material synthesis conditions. First, we propose a human-AI joint data curation process to secure high-quality ground-truth demonstrations for few-shot learning. Second, we apply a BM25 algorithm based on the retrieval-augmented generation (RAG) technique to adaptively select few-shot demonstrations for each MOF's extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs, the proposed few-shot method achieves much higher average F1 performance (0.93 vs. 0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under fully automatic evaluation that are more objective than the previous human evaluation. The proposed method is further validated through real-world material experiments: compared with the baseline zero-shot LLM, the proposed few-shot approach increases the MOFs structural inference performance (R^2) by 29.4% in average.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4325.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4325.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot RAG (BM25) + GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Retrieval-Augmented Generation with BM25 retrieval feeding GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that retrieves a small set (K) of human-AI curated demonstration examples for each input paragraph using BM25, concatenates them plus background prompts as few-shot in-context examples, and asks GPT-4 to extract structured synthesis conditions (including numerical values) from MOF literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Few-shot RAG LLM extraction (BM25 retrieval + GPT-4 few-shot in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each MOF synthesis paragraph p, a retrieval module scores candidate demonstration paragraph–extraction pairs and selects the top-K (K=4 optimal) examples using the BM25 sparse retrieval algorithm. The chosen demonstrations are concatenated with a fixed background prompt that encodes domain definitions and deterministic constraints; this prompt + examples + target paragraph are supplied to GPT-4 (GPT-4 Turbo) as an in-context few-shot prompt. The model outputs structured synthesis condition fields (metal/linker/solvent/modulator names and amounts, reaction temperature and duration). The pipeline excludes the same paragraph from the example pool during retrieval and uses fixed example ordering and prompt format to reduce variance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (GPT-4 Turbo is specified as the high-performance LLM used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / chemistry (Metal–Organic Frameworks synthesis literature)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>36,177 full-text papers available in corpus; evaluated on 123 papers (1230 ground-truth conditions) for extraction accuracy and on 5,269 papers for downstream inference</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Structured extraction of quantitative synthesis conditions (numerical reaction temperatures, durations, reagent amounts) enabling statistical correlations and predictive models (regression relationships between synthesis conditions and MOF properties)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured entity-value outputs (named entities + numeric values), formatted JSON-like lists of 10 synthesis fields; later converted to fixed-length feature vectors (length-198) for downstream regression</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to expert-curated ground-truth annotations (1230 labeled conditions from 123 MOFs), statistical evaluation (F1, accuracy), and downstream validation via machine-learning structure-property inference (10-fold cross-validation; R^2 metrics); also expert review during annotation curation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Extraction: average F1 = 0.93 and ACC = 0.90 (K=4, BM25 retrieval) on 1,230 conditions; Baseline zero-shot GPT-4: F1 = 0.81, ACC = 0.77. Downstream structure inference: average R^2 improvement of 29.4% vs zero-shot; example: XGBoost R^2 few-shot = 0.4421 vs zero-shot = 0.3559.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against zero-shot GPT-4 (same model without retrieved few-shot examples) and random example selection. Few-shot + BM25 outperformed zero-shot (F1 +14.8 percentage points) and outperformed random selection by >0.05 F1 in most settings. Also compared retrieval methods (BM25 vs SBERT vs BERT embeddings): BM25 gave best F1 = 0.93.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires high-quality demonstration pool; sensitive to example quality and prompt constraints; cost of commercial LLM API calls at scale; diminishing returns beyond K=4 in this domain; general-purpose LLMs lack domain coverage for rare/sparse synthesis patterns so many demonstrations may be needed for full coverage; possible failure modes include incorrect numeric parsing, missed entities, and hallucinated synonyms.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4325.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4325.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-AI joint data curation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human–AI joint annotation and reflection loop for demonstration curation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-round annotation pipeline that combines initial zero-shot LLM pre-extraction, human expert correction, few-shot LLM re-extraction, and final human consolidation to produce high-quality few-shot demonstrations for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Human-AI joint demonstration curation (iterative AI-human reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pipeline steps: (1) run LLM zero-shot on raw synthesis paragraphs to produce initial AI annotations; (2) human experts review and correct to produce human annotations (first reflection); (3) use these human annotations as few-shot examples to run LLM again to generate few-shot AI annotations (second reflection); (4) experts reconcile human and few-shot AI annotations into final joint annotations (final reflection). The approach leverages complementary strengths: AI consistency and humans' domain judgment. Final joint examples form the demonstration pool used by the few-shot RAG pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 for both zero-shot pre-extraction and few-shot re-extraction</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / MOFs synthesis literature annotation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Human-AI joint curation produced a ground-truth set of 123 demonstrations used for evaluation; initial human annotation pool included 147 suites from pilot/batch annotation steps (derived from 200 candidate papers)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not a law per se; enables accurate extraction of quantitative synthesis parameters (numerical temps, times, amounts) which are inputs to discovery of statistical relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Curated structured demonstration examples: context paragraph + labelled completion with 10 synthesis fields (entity names and numerical values) in consistent format for few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Expert review of LLM outputs and annotation disagreements; empirical ablation showing few-shot performance using human-only vs AI-only vs joint annotations (F1 outcomes); error analysis by MOF experts on 261 potential errors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using joint-curated demonstrations with BM25 retrieval and K=4 gave F1 = 0.93 (ACC = 0.90); human-only demonstrations peaked at F1 ~0.86 (K=2) in experiments; AI-only and raw-context examples performed worse.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to purely human annotation, purely AI annotation, and raw paragraphs as in-context examples. The joint approach outperformed all alternatives (F1: 0.93 vs human-only ~0.86 and zero-shot 0.81).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires domain experts (labor cost), iteration overhead, and careful exclusion of ambiguous paragraphs (e.g., multiple MOFs in one paragraph). Human fatigue and inconsistency are issues that the pipeline mitigates but does not eliminate.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4325.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4325.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM + regex coreference resolver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid LLM and regular-expression coreference resolution for chemical proxies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid method where LLMs extract anaphoric reference candidates from text preceding a synthesis paragraph, and regular expressions identify and replace proxy tokens (e.g., 'L', 'H2L') in synthesis conditions with the resolved full linker names.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-assisted coreference resolution + regex matching</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Locate synthesis paragraph, feed all preceding text to an LLM to extract anaphoric references and original entity mentions; apply regex to identify proxy tokens in extracted synthesis conditions; match proxies to the anaphoric references discovered by the LLM and substitute resolved names. This restores abbreviated linker notations to full identifiers to improve downstream material use.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (used to extract anaphoric references and original words)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / MOFs literature (chemical coreference resolution)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to 5,269 synthesis paragraphs; coreference analysis identified 578 proxy cases within that set</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Utility method for restoring textual referents—enables more accurate extraction of quantitative reagent identities and counts used in subsequent data-driven analysis</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Resolved textual entity strings for organic linkers (replacement of proxy tokens with full names)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Measured resolution rate against annotated references: reported resolution rates per proxy word and overall success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Resolved 79% of the 578 coreference cases; only 0.023 linkers per paragraph remained unresolved after applying the method; five most common proxy tokens had resolution rates >85% (table provided).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Hybrid LLM+regex method compared implicitly to pure regex which was insufficient because of writing variation; hybrid approach substantially improved resolution rates (no numeric baseline provided for regex-only).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Fails on some cases where proxy referent is ambiguous or defined far and unclearly; depends on LLM’s ability to extract antecedents reliably; requires feeding earlier paper text (increases input size/cost); not fully perfect (21% unresolved).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4325.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4325.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthesis-paragraph detection (BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthesis paragraph detection using a fine-tuned BERT classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A binary paragraph classifier (bert-base-uncased) trained to identify synthesis-related paragraphs in full-text papers, used to filter and reduce LLM input to the most relevant text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Synthesis paragraph detection with BERT (bert-base-uncased)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Annotate positive (synthesis) and negative (non-synthesis) paragraphs from a set of papers, train a BERT-based binary classifier with stratified 5-fold cross-validation to detect paragraphs that contain synthesis conditions. Use the classifier to trim full-text to synthesis paragraphs before invoking expensive LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>None for detection step (classical transformer BERT used: bert-base-uncased); this component reduces calls to GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / MOFs literature preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Trained with 440 papers (1,349 positive annotated synthesis paragraphs and 11,783 negative samples); applied across corpus of 36,177 downloadable papers leading to extraction of 57,081 synthesis paragraphs overall and a core subset of 5,269 single-paragraph MOF publications</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not extracting laws; supports extraction of quantitative synthesis parameters by isolating relevant paragraph contexts</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Binary classification labels indicating which paragraphs are synthesis-related</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>5-fold stratified cross-validation on annotated dataset; standard classification metrics reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy = 0.989, Precision = 0.955, Recall = 0.947, F1 = 0.951 on paragraph classification</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not explicitly compared to alternatives in paper; main point is cost/efficiency gains: reduces text passed to LLMs by ~94% (average literature vs synthesis paragraph word counts estimate)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires annotated paragraphs for training; may misclassify borderline paragraphs or paragraphs with multiple MOF syntheses; relies on labeled training set quality</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4325.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4325.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt engineering with material knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Background prompt engineering augmented with material definitions and deterministic constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An enhanced background prompt that provides definitions of each synthesis condition field and deterministic numerical/textual/structural constraints, used alongside few-shot examples to improve extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Domain-augmented background prompting (definitions + constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The background prompt includes explicit definitions for each of the 10 synthesis fields and three classes of constraints: numerical ranges (e.g., plausible reaction durations), textual formatting rules (e.g., canonical solvent naming), and structural rules (domain-specific rules about what counts as a reaction vs. crystallization). This prompt is concatenated before examples and target paragraph for GPT-4 to reduce format errors and to help post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / MOFs synthesis extraction</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used across the same experimental corpus (36k+ papers; evaluated on 123 and 5,269 subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not a law extractor itself — enforces correct extraction of quantitative parameters (numeric ranges and units) to enable downstream quantitative analysis</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured entity-value outputs with constrained formats (e.g., numeric units standardized to hours and Celsius)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation: measured extraction F1 with and without background material knowledge in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Adding material-knowledge background prompt improved F1 from ~0.91 to 0.93 when combined with few-shot examples; alone (without few-shots) it did not lead to significant gains.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to same few-shot setup without the domain-augmented background prompt; improvement small but measurable (F1 +0.02).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Background knowledge alone insufficient; effectiveness depends on quality and correctness of constraints; overly strict constraints risk excluding valid variants; prompt size contributes to input token cost.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4325.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4325.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synonym merging & feature embedding (GPT-4 + RDKit/Pymatgen/Matminer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-assisted synonym merging and downstream chemical feature extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing pipeline that uses GPT-4 to group synonymous names for metals, linkers and solvents, maps names to formulas/SMILES, and computes molecular/material features (RDKit, Pymatgen, Matminer) to produce numeric feature vectors for predictive modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-assisted synonym disambiguation and chemical feature embedding</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Steps: (1) similarity disambiguation using Levenshtein distance to group near-duplicates; (2) GPT-4 prompted (PROMPT1 + reflection prompt) to merge synonyms into canonical groups (returns JSON arrays); (3) GPT-4 (and manual curation for linkers) used to obtain chemical formulae and SMILES for canonical names; (4) RDKit used to compute molecular descriptors for solvents/linkers; Pymatgen and Matminer compute metal salt and elemental features; (5) assemble per-MOF fixed-length feature vectors (length-198) used as inputs to regression models predicting MOF microstructure properties (e.g., framework density).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 for synonym merging and SMILES/formula retrieval (with manual curation for complex linkers)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / cheminformatics (MOF precursor/solvent/linker feature engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to top-N entities filtered from extraction outputs (e.g., top-100 metal sources, top-135 linkers, top-20 solvents) drawn from extractions across the corpus and used to build datasets of ~800 MOFs for inference experiments</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Enables discovery of statistical relationships (regression) between synthesis features and microstructure properties rather than explicit symbolic laws; supports quantitative correlation and predictive modeling</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Numeric feature vectors (molecular descriptors, elemental featurizations) and canonical text-to-formula/SMILES mappings</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Downstream regression tasks (six ML models including XGBoost) evaluated by 10-fold cross-validation and R^2 metrics; also manual checking and a reflection prompt step for synonym merging accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Downstream predictive performance improved when using few-shot extracted and post-processed features: example XGBoost R^2 few-shot = 0.4421 vs zero-shot = 0.3559. Synonym merging applied to items with frequency thresholds (8/4/5) to reduce low-frequency errors; no single numeric metric for merging accuracy reported beyond human review.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared formulation where zero-shot LLM extractions were used and then post-processed; few-shot extractions fed to the same embedding pipeline produced higher predictive R^2 across models (~average +29.4% improvement in R^2 vs zero-shot), indicating better quantitative signal.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>GPT-4 sometimes cannot reliably produce SMILES for complex linkers (manual curation needed); synonym merging can introduce errors for low-frequency items; thresholding decisions influence downstream dataset composition and bias.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4325.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4325.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual MOFs Extraction Engine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual MOFs Synthesis Condition Extraction Engine and Database</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated system that automates document ingestion, paragraph detection, LLM-based extraction, post-processing, visualization and query over extracted synthesis conditions for tens of thousands of MOF publications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Integrated visual extraction engine (document pipeline + LLM extraction + DB + visualization)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pipeline modules: PDF-to-text conversion, synthesis-paragraph detection (BERT), optional LLM pre-extraction, retrieval-driven few-shot LLM extraction (BM25 + GPT-4), post-processing (synonym merging, numeric standardization), coreference resolution, storage in a database supporting advanced logical search and visualization panels for entity resolution heatmaps and paragraph similarity. The engine supports configurable example selection and re-processing of paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (for extraction, synonym merging, coreference resolution) plus BERT for paragraph detection</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / MOF literature mining and knowledgebase construction</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Engine processed >30,000 papers and extracted 57,081 synthesis paragraphs across corpus (36k+ downloadable papers); public online database shows extracted conditions for 36,177 MOFs with literature available</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Produces structured quantitative synthesis data (numeric temperatures, times, amounts) that enable discovery of statistical relationships and predictive models; the engine itself organizes quantitative empirical patterns rather than symbolic laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Relational database records and visualizations of structured extraction fields; supports logical queries and exports to structured formats</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>System-level validation: measured extraction accuracy against ground-truth (1230 conditions) and downstream predictive performance (R^2) plus internal dashboards for manual re-inspection and re-processing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>System produced 57k+ synthesized paragraphs and 36k+ MOF extractions; core extraction performance on held ground-truth: few-shot pipeline within engine achieved F1 = 0.93 and ACC = 0.90. Paragraph detection reduced LLM token cost by estimated 94%.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Scale-limited by LLM API costs and annotation pool size; needs configuration (filters, example selection) for different downstream tasks; some manual curation required for edge cases; not all extracted entities are perfectly canonicalized without human review.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from scientific text with large language models <em>(Rating: 2)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4325",
    "paper_id": "paper-271843140",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "Few-shot RAG (BM25) + GPT-4",
            "name_full": "Few-shot Retrieval-Augmented Generation with BM25 retrieval feeding GPT-4",
            "brief_description": "A pipeline that retrieves a small set (K) of human-AI curated demonstration examples for each input paragraph using BM25, concatenates them plus background prompts as few-shot in-context examples, and asks GPT-4 to extract structured synthesis conditions (including numerical values) from MOF literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Few-shot RAG LLM extraction (BM25 retrieval + GPT-4 few-shot in-context learning)",
            "method_description": "For each MOF synthesis paragraph p, a retrieval module scores candidate demonstration paragraph–extraction pairs and selects the top-K (K=4 optimal) examples using the BM25 sparse retrieval algorithm. The chosen demonstrations are concatenated with a fixed background prompt that encodes domain definitions and deterministic constraints; this prompt + examples + target paragraph are supplied to GPT-4 (GPT-4 Turbo) as an in-context few-shot prompt. The model outputs structured synthesis condition fields (metal/linker/solvent/modulator names and amounts, reaction temperature and duration). The pipeline excludes the same paragraph from the example pool during retrieval and uses fixed example ordering and prompt format to reduce variance.",
            "llm_model_used": "GPT-4 (GPT-4 Turbo is specified as the high-performance LLM used in experiments)",
            "scientific_domain": "Materials science / chemistry (Metal–Organic Frameworks synthesis literature)",
            "number_of_papers": "36,177 full-text papers available in corpus; evaluated on 123 papers (1230 ground-truth conditions) for extraction accuracy and on 5,269 papers for downstream inference",
            "type_of_quantitative_law": "Structured extraction of quantitative synthesis conditions (numerical reaction temperatures, durations, reagent amounts) enabling statistical correlations and predictive models (regression relationships between synthesis conditions and MOF properties)",
            "extraction_output_format": "Structured entity-value outputs (named entities + numeric values), formatted JSON-like lists of 10 synthesis fields; later converted to fixed-length feature vectors (length-198) for downstream regression",
            "validation_method": "Comparison to expert-curated ground-truth annotations (1230 labeled conditions from 123 MOFs), statistical evaluation (F1, accuracy), and downstream validation via machine-learning structure-property inference (10-fold cross-validation; R^2 metrics); also expert review during annotation curation",
            "performance_metrics": "Extraction: average F1 = 0.93 and ACC = 0.90 (K=4, BM25 retrieval) on 1,230 conditions; Baseline zero-shot GPT-4: F1 = 0.81, ACC = 0.77. Downstream structure inference: average R^2 improvement of 29.4% vs zero-shot; example: XGBoost R^2 few-shot = 0.4421 vs zero-shot = 0.3559.",
            "baseline_comparison": "Compared against zero-shot GPT-4 (same model without retrieved few-shot examples) and random example selection. Few-shot + BM25 outperformed zero-shot (F1 +14.8 percentage points) and outperformed random selection by &gt;0.05 F1 in most settings. Also compared retrieval methods (BM25 vs SBERT vs BERT embeddings): BM25 gave best F1 = 0.93.",
            "challenges_limitations": "Requires high-quality demonstration pool; sensitive to example quality and prompt constraints; cost of commercial LLM API calls at scale; diminishing returns beyond K=4 in this domain; general-purpose LLMs lack domain coverage for rare/sparse synthesis patterns so many demonstrations may be needed for full coverage; possible failure modes include incorrect numeric parsing, missed entities, and hallucinated synonyms.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4325.0",
            "source_info": {
                "paper_title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Human-AI joint data curation",
            "name_full": "Human–AI joint annotation and reflection loop for demonstration curation",
            "brief_description": "A multi-round annotation pipeline that combines initial zero-shot LLM pre-extraction, human expert correction, few-shot LLM re-extraction, and final human consolidation to produce high-quality few-shot demonstrations for in-context learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Human-AI joint demonstration curation (iterative AI-human reflection)",
            "method_description": "Pipeline steps: (1) run LLM zero-shot on raw synthesis paragraphs to produce initial AI annotations; (2) human experts review and correct to produce human annotations (first reflection); (3) use these human annotations as few-shot examples to run LLM again to generate few-shot AI annotations (second reflection); (4) experts reconcile human and few-shot AI annotations into final joint annotations (final reflection). The approach leverages complementary strengths: AI consistency and humans' domain judgment. Final joint examples form the demonstration pool used by the few-shot RAG pipeline.",
            "llm_model_used": "GPT-4 for both zero-shot pre-extraction and few-shot re-extraction",
            "scientific_domain": "Materials science / MOFs synthesis literature annotation",
            "number_of_papers": "Human-AI joint curation produced a ground-truth set of 123 demonstrations used for evaluation; initial human annotation pool included 147 suites from pilot/batch annotation steps (derived from 200 candidate papers)",
            "type_of_quantitative_law": "Not a law per se; enables accurate extraction of quantitative synthesis parameters (numerical temps, times, amounts) which are inputs to discovery of statistical relationships",
            "extraction_output_format": "Curated structured demonstration examples: context paragraph + labelled completion with 10 synthesis fields (entity names and numerical values) in consistent format for few-shot prompting",
            "validation_method": "Expert review of LLM outputs and annotation disagreements; empirical ablation showing few-shot performance using human-only vs AI-only vs joint annotations (F1 outcomes); error analysis by MOF experts on 261 potential errors",
            "performance_metrics": "Using joint-curated demonstrations with BM25 retrieval and K=4 gave F1 = 0.93 (ACC = 0.90); human-only demonstrations peaked at F1 ~0.86 (K=2) in experiments; AI-only and raw-context examples performed worse.",
            "baseline_comparison": "Compared to purely human annotation, purely AI annotation, and raw paragraphs as in-context examples. The joint approach outperformed all alternatives (F1: 0.93 vs human-only ~0.86 and zero-shot 0.81).",
            "challenges_limitations": "Requires domain experts (labor cost), iteration overhead, and careful exclusion of ambiguous paragraphs (e.g., multiple MOFs in one paragraph). Human fatigue and inconsistency are issues that the pipeline mitigates but does not eliminate.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4325.1",
            "source_info": {
                "paper_title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LLM + regex coreference resolver",
            "name_full": "Hybrid LLM and regular-expression coreference resolution for chemical proxies",
            "brief_description": "A hybrid method where LLMs extract anaphoric reference candidates from text preceding a synthesis paragraph, and regular expressions identify and replace proxy tokens (e.g., 'L', 'H2L') in synthesis conditions with the resolved full linker names.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LLM-assisted coreference resolution + regex matching",
            "method_description": "Locate synthesis paragraph, feed all preceding text to an LLM to extract anaphoric references and original entity mentions; apply regex to identify proxy tokens in extracted synthesis conditions; match proxies to the anaphoric references discovered by the LLM and substitute resolved names. This restores abbreviated linker notations to full identifiers to improve downstream material use.",
            "llm_model_used": "GPT-4 (used to extract anaphoric references and original words)",
            "scientific_domain": "Materials science / MOFs literature (chemical coreference resolution)",
            "number_of_papers": "Applied to 5,269 synthesis paragraphs; coreference analysis identified 578 proxy cases within that set",
            "type_of_quantitative_law": "Utility method for restoring textual referents—enables more accurate extraction of quantitative reagent identities and counts used in subsequent data-driven analysis",
            "extraction_output_format": "Resolved textual entity strings for organic linkers (replacement of proxy tokens with full names)",
            "validation_method": "Measured resolution rate against annotated references: reported resolution rates per proxy word and overall success rate",
            "performance_metrics": "Resolved 79% of the 578 coreference cases; only 0.023 linkers per paragraph remained unresolved after applying the method; five most common proxy tokens had resolution rates &gt;85% (table provided).",
            "baseline_comparison": "Hybrid LLM+regex method compared implicitly to pure regex which was insufficient because of writing variation; hybrid approach substantially improved resolution rates (no numeric baseline provided for regex-only).",
            "challenges_limitations": "Fails on some cases where proxy referent is ambiguous or defined far and unclearly; depends on LLM’s ability to extract antecedents reliably; requires feeding earlier paper text (increases input size/cost); not fully perfect (21% unresolved).",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4325.2",
            "source_info": {
                "paper_title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Synthesis-paragraph detection (BERT)",
            "name_full": "Synthesis paragraph detection using a fine-tuned BERT classifier",
            "brief_description": "A binary paragraph classifier (bert-base-uncased) trained to identify synthesis-related paragraphs in full-text papers, used to filter and reduce LLM input to the most relevant text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Synthesis paragraph detection with BERT (bert-base-uncased)",
            "method_description": "Annotate positive (synthesis) and negative (non-synthesis) paragraphs from a set of papers, train a BERT-based binary classifier with stratified 5-fold cross-validation to detect paragraphs that contain synthesis conditions. Use the classifier to trim full-text to synthesis paragraphs before invoking expensive LLM calls.",
            "llm_model_used": "None for detection step (classical transformer BERT used: bert-base-uncased); this component reduces calls to GPT-4",
            "scientific_domain": "Materials science / MOFs literature preprocessing",
            "number_of_papers": "Trained with 440 papers (1,349 positive annotated synthesis paragraphs and 11,783 negative samples); applied across corpus of 36,177 downloadable papers leading to extraction of 57,081 synthesis paragraphs overall and a core subset of 5,269 single-paragraph MOF publications",
            "type_of_quantitative_law": "Not extracting laws; supports extraction of quantitative synthesis parameters by isolating relevant paragraph contexts",
            "extraction_output_format": "Binary classification labels indicating which paragraphs are synthesis-related",
            "validation_method": "5-fold stratified cross-validation on annotated dataset; standard classification metrics reported",
            "performance_metrics": "Accuracy = 0.989, Precision = 0.955, Recall = 0.947, F1 = 0.951 on paragraph classification",
            "baseline_comparison": "Not explicitly compared to alternatives in paper; main point is cost/efficiency gains: reduces text passed to LLMs by ~94% (average literature vs synthesis paragraph word counts estimate)",
            "challenges_limitations": "Requires annotated paragraphs for training; may misclassify borderline paragraphs or paragraphs with multiple MOF syntheses; relies on labeled training set quality",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4325.3",
            "source_info": {
                "paper_title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Prompt engineering with material knowledge",
            "name_full": "Background prompt engineering augmented with material definitions and deterministic constraints",
            "brief_description": "An enhanced background prompt that provides definitions of each synthesis condition field and deterministic numerical/textual/structural constraints, used alongside few-shot examples to improve extraction accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Domain-augmented background prompting (definitions + constraints)",
            "method_description": "The background prompt includes explicit definitions for each of the 10 synthesis fields and three classes of constraints: numerical ranges (e.g., plausible reaction durations), textual formatting rules (e.g., canonical solvent naming), and structural rules (domain-specific rules about what counts as a reaction vs. crystallization). This prompt is concatenated before examples and target paragraph for GPT-4 to reduce format errors and to help post-processing.",
            "llm_model_used": "GPT-4",
            "scientific_domain": "Materials science / MOFs synthesis extraction",
            "number_of_papers": "Used across the same experimental corpus (36k+ papers; evaluated on 123 and 5,269 subsets)",
            "type_of_quantitative_law": "Not a law extractor itself — enforces correct extraction of quantitative parameters (numeric ranges and units) to enable downstream quantitative analysis",
            "extraction_output_format": "Structured entity-value outputs with constrained formats (e.g., numeric units standardized to hours and Celsius)",
            "validation_method": "Ablation: measured extraction F1 with and without background material knowledge in prompt",
            "performance_metrics": "Adding material-knowledge background prompt improved F1 from ~0.91 to 0.93 when combined with few-shot examples; alone (without few-shots) it did not lead to significant gains.",
            "baseline_comparison": "Compared to same few-shot setup without the domain-augmented background prompt; improvement small but measurable (F1 +0.02).",
            "challenges_limitations": "Background knowledge alone insufficient; effectiveness depends on quality and correctness of constraints; overly strict constraints risk excluding valid variants; prompt size contributes to input token cost.",
            "requires_human_in_loop": false,
            "fully_automated": false,
            "uuid": "e4325.4",
            "source_info": {
                "paper_title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Synonym merging & feature embedding (GPT-4 + RDKit/Pymatgen/Matminer)",
            "name_full": "LLM-assisted synonym merging and downstream chemical feature extraction pipeline",
            "brief_description": "A post-processing pipeline that uses GPT-4 to group synonymous names for metals, linkers and solvents, maps names to formulas/SMILES, and computes molecular/material features (RDKit, Pymatgen, Matminer) to produce numeric feature vectors for predictive modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LLM-assisted synonym disambiguation and chemical feature embedding",
            "method_description": "Steps: (1) similarity disambiguation using Levenshtein distance to group near-duplicates; (2) GPT-4 prompted (PROMPT1 + reflection prompt) to merge synonyms into canonical groups (returns JSON arrays); (3) GPT-4 (and manual curation for linkers) used to obtain chemical formulae and SMILES for canonical names; (4) RDKit used to compute molecular descriptors for solvents/linkers; Pymatgen and Matminer compute metal salt and elemental features; (5) assemble per-MOF fixed-length feature vectors (length-198) used as inputs to regression models predicting MOF microstructure properties (e.g., framework density).",
            "llm_model_used": "GPT-4 for synonym merging and SMILES/formula retrieval (with manual curation for complex linkers)",
            "scientific_domain": "Materials science / cheminformatics (MOF precursor/solvent/linker feature engineering)",
            "number_of_papers": "Applied to top-N entities filtered from extraction outputs (e.g., top-100 metal sources, top-135 linkers, top-20 solvents) drawn from extractions across the corpus and used to build datasets of ~800 MOFs for inference experiments",
            "type_of_quantitative_law": "Enables discovery of statistical relationships (regression) between synthesis features and microstructure properties rather than explicit symbolic laws; supports quantitative correlation and predictive modeling",
            "extraction_output_format": "Numeric feature vectors (molecular descriptors, elemental featurizations) and canonical text-to-formula/SMILES mappings",
            "validation_method": "Downstream regression tasks (six ML models including XGBoost) evaluated by 10-fold cross-validation and R^2 metrics; also manual checking and a reflection prompt step for synonym merging accuracy",
            "performance_metrics": "Downstream predictive performance improved when using few-shot extracted and post-processed features: example XGBoost R^2 few-shot = 0.4421 vs zero-shot = 0.3559. Synonym merging applied to items with frequency thresholds (8/4/5) to reduce low-frequency errors; no single numeric metric for merging accuracy reported beyond human review.",
            "baseline_comparison": "Compared formulation where zero-shot LLM extractions were used and then post-processed; few-shot extractions fed to the same embedding pipeline produced higher predictive R^2 across models (~average +29.4% improvement in R^2 vs zero-shot), indicating better quantitative signal.",
            "challenges_limitations": "GPT-4 sometimes cannot reliably produce SMILES for complex linkers (manual curation needed); synonym merging can introduce errors for low-frequency items; thresholding decisions influence downstream dataset composition and bias.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4325.5",
            "source_info": {
                "paper_title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Visual MOFs Extraction Engine",
            "name_full": "Visual MOFs Synthesis Condition Extraction Engine and Database",
            "brief_description": "An integrated system that automates document ingestion, paragraph detection, LLM-based extraction, post-processing, visualization and query over extracted synthesis conditions for tens of thousands of MOF publications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Integrated visual extraction engine (document pipeline + LLM extraction + DB + visualization)",
            "method_description": "Pipeline modules: PDF-to-text conversion, synthesis-paragraph detection (BERT), optional LLM pre-extraction, retrieval-driven few-shot LLM extraction (BM25 + GPT-4), post-processing (synonym merging, numeric standardization), coreference resolution, storage in a database supporting advanced logical search and visualization panels for entity resolution heatmaps and paragraph similarity. The engine supports configurable example selection and re-processing of paragraphs.",
            "llm_model_used": "GPT-4 (for extraction, synonym merging, coreference resolution) plus BERT for paragraph detection",
            "scientific_domain": "Materials science / MOF literature mining and knowledgebase construction",
            "number_of_papers": "Engine processed &gt;30,000 papers and extracted 57,081 synthesis paragraphs across corpus (36k+ downloadable papers); public online database shows extracted conditions for 36,177 MOFs with literature available",
            "type_of_quantitative_law": "Produces structured quantitative synthesis data (numeric temperatures, times, amounts) that enable discovery of statistical relationships and predictive models; the engine itself organizes quantitative empirical patterns rather than symbolic laws",
            "extraction_output_format": "Relational database records and visualizations of structured extraction fields; supports logical queries and exports to structured formats",
            "validation_method": "System-level validation: measured extraction accuracy against ground-truth (1230 conditions) and downstream predictive performance (R^2) plus internal dashboards for manual re-inspection and re-processing",
            "performance_metrics": "System produced 57k+ synthesized paragraphs and 36k+ MOF extractions; core extraction performance on held ground-truth: few-shot pipeline within engine achieved F1 = 0.93 and ACC = 0.90. Paragraph detection reduced LLM token cost by estimated 94%.",
            "challenges_limitations": "Scale-limited by LLM API costs and annotation pool size; needs configuration (filters, example selection) for different downstream tasks; some manual curation required for edge cases; not all extracted entities are perfectly canonicalized without human review.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4325.6",
            "source_info": {
                "paper_title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from scientific text with large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_scientific_text_with_large_language_models"
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Chatgpt chemistry assistant for text mining and the prediction of mof synthesis",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.018732,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations
6 Aug 2024</p>
<p>Lei Shi 
Beihang University</p>
<p>Zhimeng Liu 
University of Science and Technology
Beijing</p>
<p>Yi Yang 
Beihang University</p>
<p>Weize Wu 
Beihang University</p>
<p>Yuyang Zhang 
Beihang University</p>
<p>Hongbo Zhang 
Westlake University</p>
<p>Jing Lin 
University of Science and Technology
Beijing</p>
<p>Siyu Wu 
Beihang University</p>
<p>Zihan Chen 
Beihang University</p>
<p>Ruiming Li 
Beihang University</p>
<p>Nan Wang 
Beihang University</p>
<p>Zipeng Liu 
Beihang University</p>
<p>Huobin Tan 
Beihang University</p>
<p>Hongyi Gao 
University of Science and Technology
Beijing</p>
<p>Yue Zhang yue.zhang@wias.org.cn 
Westlake University</p>
<p>Ge Wang gewang@ustb.edu.cn 
University of Science and Technology
Beijing</p>
<p>Yue Zhang 
LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations
6 Aug 2024FA83F3D5B0723F0227DE0D57D0A2478AarXiv:2408.04665v1[cs.CL]
The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from literature text has been challenging but crucial for the logical design of new MOFs with desirable functionality.The recent advent of large language models (LLMs) provides disruptively new solution to this long-standing problem and latest researches have reported over 90% F1 in extracting correct conditions from MOFs literature.We argue in this paper that most existing synthesis extraction practices with LLMs stay with the primitive zero-shot learning, which could lead to downgraded extraction and application performance due to the lack of specialized knowledge.This work pioneers and optimizes the few-shot in-context learning paradigm for LLM extraction of material synthesis conditions.First, we propose a human-AI joint data curation process to secure high-quality ground-truth demonstrations for few-shot learning.Second, we apply a BM25 algorithm based on the retrieval-augmented generation (RAG) technique to adaptively select few-shot demonstrations for each MOF's extraction.Over a dataset randomly sampled from 84,898 well-defined MOFs, the proposed few-shot method achieves much higher average F1 performance (0.93 vs. 0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under fully automatic evaluation that are more objective than the previous human evaluation.The proposed method is further validated through real-world material experiments: compared with the baseline zero-shot LLM, the proposed few-shot approach increases the MOFs structural inference performance (R 2 ) by 29.4% in average.</p>
<p>Introduction</p>
<p>Metal-Organic Frameworks (MOFs), a class of high performance porous material, have been widely applied to catalysis, gas storage, and groundwater remediation [5] for its prestige in structural tunability and functional versatility [2].These advantages are deeply rooted in the flexible yet logical synthesis configuration of MOFs.Herein, precise and comprehensive knowledge of MOFs synthesis conditions becomes extremely important to fully understand its structural mechanism and discover new MOFs or sub-types, posing a fundamental challenge to the whole discipline of MOFs and reticular chemistry [23].</p>
<p>Currently, there have been 100k+ MOFs successfully synthesized in the laboratory.Their detailed synthesis conditions are often recorded by academic literature in various textual or tabular formats.Machine learning methods, in particular, text mining algorithms, are normally applied to the literature text to automatically extract synthesis conditions.However, the complexity and volatility of free text limits the accuracy of synthesis condition extraction [13], which could jeopardize the effectiveness of downstream material applications over extracted synthesis data.</p>
<p>The emergence of large language models (LLMs) to some extent resolves the problem of synthesis condition extraction from disparate forms of scientific texts, due to their well-known expertise in the whole-spectrum of text mining tasks [3].Recently, Zheng et al. [24], Dagdelen et al. [7], Polak and Morgan [16] have applied zero-shot or fine-tuned LLMs to extract synthesis conditions from experimental MOFs literature.They reported extraction performance of close to 0.9 in F1 metric, but mostly over small datasets and evaluated by subjective evaluations.It should be pointed out that the baseline zero-shot LLMs are notorious for their poor performance on sparse scenarios like MOFs synthesis, which are infrequently covered by the general-purpose LLM training data [6].Therefore, evaluating the MOFs condition extraction performance with large-scale, real-life datasets become crucial for improving both the quantity and quality of MOFs synthesis knowledgebase.In addition, guided material experiments over extracted synthesis conditions, which are rarely conducted in previous works, should also be an important norm to evaluate the effectiveness of targeted synthesis condition extraction task.</p>
<p>In this work, we set out to overcome the notable limitations when applying primitive zero-shot LLMs to the problem of MOFs synthesis condition extraction from scientific texts.The main theme of this paper is to introduce the few-shot in-context learning paradigm as the standard approach to augment general-purpose LLMs on the material synthesis condition extraction problem.As shown by our experiment results of Figure 1, in a dataset randomly sampled from 84,898 well-defined MOFs, the proposed few-shot method achieves much higher average F1 performance (0.93 vs. 0.81, +14.8%) than the native zeroshot LLMs, both using the state-of-the-art GPT-4 Turbo model * [1], as shown in Figure 1.</p>
<p>Nevertheless, deploying few-shot LLMs to solve the current problem still faces multiple nontrivial challenges.First, the superiority of few-shot LLMs depends on the data quality of their ground-truth demonstrations.In the scenario of MOFs synthesis extraction, obtaining ground-truth textual conditions scattered in scientific literature in numerous formats remains a daunting task.It would be extremely costly to apply traditional human annotation approach given that a change of material would require a totally new demonstration dataset.Second, the quantity of ground-truth demonstrations selected for each LLM extraction is also critical as high-performance LLMs are mostly commercial and charged by input size.For example, the fine-tuning technology is known to greatly improve the LLM performance [7], but will normally require hundreds of examples and a locally-stored large set of model weights.The application overheads to new synthesis extraction scenarios are quite high, thus reducing the adaptability of fine-tuning methods.In our case, minimizing the number of few-shot demonstrations would require an elaborate algorithm to select the demonstrations adaptively for each MOF's raw synthesis text.</p>
<p>In this paper, we introduce two new methods to resolve the above challenges.First, on the preparation of ground-truth demonstrations, to our surprise, human annotation and AI annotation show complementary advantages, not only in the annotation cost, but also in their output data quality.We then propose a human-AI joint data curation process, which enjoys the best of both worlds and offers the highest data quality in ground-truth demonstrations produced.Second, based on the popular retrieval-augmented generation (RAG) technique, we propose to apply the BM25 algorithm to adaptively select the best combination of few-shot demonstrations for each MOF's synthesis extraction, whose performance significantly outruns the baseline random selection method.Our experiment results also suggest the most appropriate number of demonstrations for the trade-off between performance and cost.It is shown that a small overhead of 4-shots could already achieve the optimal performance, contrasting to tens to a hundred shots in other domains.In addition, we study the utility of different kinds of knowledge on our task when incorporated by LLM: the background knowledge on retrieved synthesis conditions, their application constraints on the numerical/textual format, and the few-shot demonstrations.Notably, the few-shot examples are shown to be the most critical.To our knowledge, we are the first to apply and optimize few-shot in-context learning LLM methods for the material synthesis condition extraction problem from scientific text.</p>
<p>Moreover, we have considered the scalability issue for high-throughput synthesis extraction.The additional overhead includes the labor cost to acquire external knowledge (e.g., expert annotations on the literature text), the financial cost to request LLM APIs, and the computational cost to potentially train in-house LLMs.For example, by the latest GPT-4 pricing model (10$ per 1M tokens), a single pass over all the 100k available MOFs synthesis literature (est.10k words per literature) sums up to a non-negligible cost of 10k$, while performance tuning normally requires several passes.Three techniques adapted to large-scale material data are proposed.First, we learn an offline model to detect the most relevant synthesis paragraphs out of each literature, with an overall accuracy of 98.9%.In this way, the financial cost in using commercial LLMs is reduced by 94% (the average word count of a literature and its synthesis paragraphs are est.15,000 and 900, respectively).The fully-tuned high-throughput synthesis extraction workflow now processes over 500 millions of scientific texts from all available MOFs literature within 7 hours.Second, we conduct experiments to quantify the size of demonstration pool as material data scales.Though it is shown that larger example pools almost always contribute to the performance, the margin quickly drops as more annotations are available.In the extreme, a pool of size K for K-shot (K &gt; 1) LLMs is the most cost-effective.Third, we develop a LLM-based coreference resolution method to restore proxy words like "L" or "H2L" into its entirety.Though only a small portion of extracted synthesis condition on organic linker suffer from the use of proxy words, it mounts to a big number and affects the downstream material tasks on large-scale data.By applying our method, only 2.3% linkers remain unresolved.</p>
<p>To validate the importance of our proposal, we set up a real-world MOFs synthesis-structure inference experiment, in which the proposed few-shot LLM method is compared with the existing LLM application in material synthesis extraction scenario (e.g., zero-shot [24] [16]).On a set of 5,269 MOFs curated from the CSD database, we manage to build machine learning models to predict MOFs microstructure properties (framework density, cavity diameter, etc.) with the synthesis conditions obtained by LLM.By 6 off-the-shelf machine learning models, the inference performance (R 2 ) by the proposed few-shot method is consistently higher than the benchmark zero-shot approach, with an average improvement of 29.4%.</p>
<p>We also make available an online visual database showing the extracted synthesis conditions of all the 36,177 MOFs with literature available from the CSD database (see the supplemental material for more details).</p>
<p>Results</p>
<p>As shown in our technology pipeline of Figure 2, the MOFs literature dataset are first collected and pre-processed into compatible input format for LLMs (see Sec. 5.1 for details).The latest high-performance LLM (i.e., GPT-4) is employed to extract 10 essential conditions for the synthesis of each MOF: metal precursor name &amp; amount, organic linker name &amp; amount, solvent name &amp; amount, modulator name &amp; amount, and synthesis reaction duration &amp; temperature.The synthesis extraction result is first evaluated on their literal accuracy with respect to an expert-curated ground-truth dataset, and then tested on the real-world scenarios of material structure inference and design.On the randomly sampled 123 MOFs synthesis literature from all the 36177 MOFs, the extraction of 1230</p>
<p>AI Annotation</p>
<p>Zero-shot LLM</p>
<p>Human Annotation</p>
<p>MOFs Experts</p>
<p>RAG By BM25</p>
<p>Few-shot LLM MOFs Experts</p>
<p>Few-Shot Examples</p>
<p>Synthesis Paragraph</p>
<p>Zero-Shot AI Annotations</p>
<p>Human Annotations</p>
<p>Few-shot AI Annotations</p>
<p>Human-AI Joint Annotations</p>
<p>AI Reflection</p>
<p>Human Reflection synthesis conditions using the proposed few-shot LLM model achieves a best average F1 metric of 0.93 (ACC = 0.90), using a few-shot demonstration of only 4 examples.The full performance result is illustrated in Figure 1(a), in comparison to the baseline zero-shot approach with an average F1 of 0.81 (ACC = 0.77) in Figure 1(b).The dataset statistics is listed in Figure 4.</p>
<p>Human-AI Joint Data Curation</p>
<p>To introduce the few-shot LLM method, a prerequisite is to obtain a highquality demonstration pool on the synthesis condition extraction task, i.e., the ground-truth annotations.Traditionally, human annotations are the sole means to collect these examples for the few-shot learning.In this work, we also start with a standard annotation protocol which includes three steps: 1) pilot annotations on 20 typical literature by the leading experts to reach consensus on the rigorous format of MOFs synthesis conditions; 2) batch annotations conducted by 6 experts over 180 MOFs synthesis paragraphs randomly chosen from the entire dataset.Each paragraph is double annotated by two experts to ensure reliability; 3) finalized annotations by only keeping the MOFs synthesis conditions that are agreeing between the two experts, while removing annotated paragraphs that are inappropriate as examples (e.g., having more than one suite of MOFs synthesis conditions in the same paragraph).Eventually, we obtain a ground-truth human annotation dataset composed of 147 suites of MOFs synthesis conditions.The full detail of our annotation approach and an online software to assist the process is described in Sec.5.2.</p>
<p>Using the human annotations developed above as examples, the performance of few-shot LLM models is depicted by the solid orange+triangle lines of Figure 6.The average F1 metric rises from 0.81 (zero-shot) to the peak of 0.86 (K = 2), and does not increase any more.The random example selection shown by the dotted orange line oscillates slightly above the zero-shot performance.Both algorithms over purely human annotation perform much worse than the new annotation approach described later (an average F1 as high as 0.93, solid blue line).It is hypothesized that the key limitation lies in the low data quality of few-shot examples.For more information, we experiment with two other ways to generate annotated demonstrations.In the first trial, LLM is initially applied in a zero-shot mode to extract all synthesis conditions from the input paragraph.The 1st-round LLM output is then used as the data annotation (examples) in the 2nd-round few-shot LLM in-context learning.The green+diamond lines in Figure 6 indicate that the performance with this AI-based annotation is still constantly below the best approach.In the final trial, we input synthesis paragraph without annotation as the examples (i.e., the lowest data quality).As expected, the red+square performance charts in Figure 6 achieves the lowest F1 and ACC.As more raw paragraphs are used, i.e., increasing K, both metrics drop.The explanation might be that more information without ground-truth distracts the LLM, rather than coaches it.</p>
<p>The above results indicate that neither human annotations nor purely AIgenerated examples achieve optimal data quality for LLM few-shot learning.To delve deeper into the issue, several leading MOF experts were consulted to evaluate all errors produced by the few-shot LLM method when using human annotations as the sole examples and ground-truths.Out of 261 potential errors, 103 LLM outputs (39.5%) were identified as correct, 38 (14.6%) had certain issues but contributed to refining the corresponding ground-truth, and only 120 (45.9%) were true errors.The experts then compiled a revised set of ground-truth annotations, including the synthesis conditions for 123 individual MOFs.The remaining 23 annotated conditions were deemed inappropriate because they either involved chiral MOFs with duplicate synthesis conditions and paragraphs or contained multiple MOF synthesis processes within a single paragraph.Although our technical framework can deal with the case of having multiple MOFs in a single paragraph, we chose the paragraphs describing the synthesis of only one MOF for more precise demonstrations.</p>
<p>With this empirical experience, we propose a human-AI joint data curation process for the data quality optimization of ground-truth demonstrations in LLM-based few-shot learning paradigm.As shown in Figure 3(a), raw synthesis paragraphs are first processed by LLM in a zero-shot mode.Human experts then work on the initial AI annotation to achieve a best-effort human annotation (Figure 3(b)), which is the first round of reflection.After that, these human annotations are used as demonstrations in a LLM-based few-shot synthesis condition extraction Figure 3(c), which is the second round of reflection and generates few-shot AI annotations.Lastly, human experts combines human annotations and few-shot AI annotations into the human-AI joint annotation (Figure 3(d)), in the final round of reflection.We apply the few-shot LLM model over the final demonstrations with the highest-level of data quality.The best performance (F1=0.93 and ACC=0.9) is achieved at the point of most appropriate few-shot quantity (K = 4, as shown by the solid blue lines in Figure 6.</p>
<p>We ascribe the superiority of human-AI joint data curation to three reasons, all due to the complementary nature of human expertise and AI's capacity.First, though human are excellent in flexible usage of material knowledge, they often fail to strictly follow pre-defined annotation rules.For example, to standardize the solvent condition, it is required to leave out all modifiers of a common solvent.Human annotators sometimes extract "hot water" instead of "water", because his/her focus is on the knowledge extraction and neglects the rules.Human are poor multi-objective task executors compared with AI, who will not introduce error if only these rules are provided in either background prompt or examples.Second, human often suffer from fatigue issue when working with a large set of annotation tasks as ours.Random errors are then generated, i.e., missing or adding a few characters/words.Though redundant annotation by more than one expert can eliminate these random errors, it is at the cost of excluding many useful annotations when the redundant outputs are different.</p>
<p>Here AI is applied to alleviate this issue: an initial zero-shot LLM annotation reduces human efforts, their fatigue, and the resulting random errors in the first round of reflection; the few-shot LLM output also works as a caliber to help resolve differences between redundant human annotations in the second round of reflection.</p>
<p>Finally, on the other hand, the current general-purpose LLM alone is not the ultimate solution to our task.According to the medium performance of zero-shot LLM, it lacks specialized knowledge on MOFs synthesis conditions.Though the few-shot demonstrations mitigate this deficiency through in-context learning, the scalability issue makes it very hard to achieve a close to 100% accuracy.For example, retrieving one synthesis condition from a paragraph may require several demonstrations to cover all the lexical and syntactic pattern around the target condition.Retrieving all 10 conditions then demands tens of examples, inducing a cost magnitudes more than the current setting of K = 4. Customized few-shot algorithms will be needed to achieve such goal.Therefore, in the final round of reflection, human experts are hired to generate the bestquality ground-truth demonstrations over existing human-AI efforts.</p>
<p>Few-Shot Large Language Model with Material Knowledge</p>
<p>Few-shot in-context learning with random examples</p>
<p>In the research area of natural language processing (NLP), few-shot incontext learning (FS-ICL) [6] generally refers to one typical learning paradigm to adapt the task-agnostic language models to various downstream tasks while achieving optimized performance on each task.In more detail, FS-ICL takes a few prompted examples as input (known as shots), each composed of a context and a labeled completion, in addition to background prompts such as task description (Figure 4).In the task of MOFs synthesis extraction for instance, a context refers to a paragraph containing all the synthesis conditions of a MOF and the labeled completion refers to the ground-truth synthesis conditions annotated and curated by human experts in our work.The top-right part of Figure 4 gives an example of the labeled completion.FS-ICL is often discussed in comparison to the fine-tuning (FT) paradigm, which updates the pre-trained language models by incorporating a set of labeled examples via supervised learning.In both FS-ICL and FT, the final prediction is made by prompting a new context and asking the language model to complete it.</p>
<p>The main advantage of FS-ICL vs. FT lies in its versatility to work on , training and maintaining a small fraction of updated language model weights can still be costly for lightweight LLM usage scenarios such as synthesis extraction in this work.FT also suffers from spurious correlations due to the overfitting effect [15].Despite the superiority of FS-ICT in our scenario, the paradigm also draws concerns due to its disadvantages.First, the inclusion of few-shots in the prompt brings additional computation cost to the language model.In mainstream implementations, the number of shots, denoted as K, ranges from 10, 20 to 100 [6] [15].Yet, we will show in this work, for the task of MOFs synthesis extraction, the setting of K = 4 or even K = 1 (known as one-shot) can significantly boost the performance over the setting of K = 0 (known as zero-shot).Second, in previous studies, the format of prompts in FS-ICT (e.g., the wording and ordering of examples) can have unpredictable influence on the final performance.In some cases, FS-ICL even performs well on incorrect examples.We have investigate these issues and demonstrate that our approach can achieve very low variance by fixing the prompt format and example orders according to the algorithm.The data quality of few-shot examples in our task is also shown to be an important factor to the synthesis extraction performance.</p>
<p>Background prompt</p>
<p>Example (1)
Context
It is also previously believed that FT can achieve better performance than FS-ICT, but the latest study reveals that under the same size of shots, both paradigms obtain similar performance and exhibit large variance depending on the task specification [15].In our scenario, FS-ICT reaches an excellent performance of F1&gt;0.9, which is enough for real-life deployment.</p>
<p>Using RAG to enhance few-shot data quantity and quality At the core of FS-ICL approach, we introduce the RAG algorithm which retrieves the aforementioned K examples for each input context to augment the LLM and then generates the predicted completion (Figure 4).Quite a few RAG algorithms have been proposed in the literature, with differences on how to compute the similarity between the input context and candidate examples.Sec.3.2 describes in more detail two major classes of these algorithms: term frequency statistics and semantic similarity.We have applied three mainstream algorithms: BM25 [18], BERT [8], and Sentence-BERT (SBERT) [17], in a typical setting of K = 4.The test data is the 1230 ground-truth conditions out of 123 MOFs (synthesis paragraph).Note that in the experiment, when the RAG algorithm is applied to one synthesis paragraph, the same paragraph is excluded from the few-shot example selection for legality consideration.The average extraction performance in Figure 5(a)(b) indicates that the BM25 algorithm achieves the best F1 (0.93) and ACC (0.90) in all the compared algorithms and the result is quite stable among 5 repeated runs (95% CI is 0.003 for F1).Notably, any of the tested algorithm is significantly better than a random selection of examples (p &lt;.001 in F1 comparison), showcasing the effectiveness of RAG mechanism.On the best BM25 algorithm, we further test the impact of few shots' input order as in the LLM prompt.As shown in Figure 5(c), the differences are not significant among most ordering strategies.</p>
<p>Here we note that the F1 and ACC (overall accuracy) metrics used follow the standard definition computed from TP (true positive), FP (false positive), TN (true negative), FN (false negative), throughout this work.The LLM output on each synthesis condition of a MOF will be classified into one of TP/FP/TN/FN by comparing with the predefined ground-truth annotation, as described in the confusion matrix of Figure 1(c).Note that our definition is different from the previous research in Zheng et al. [24] where the TP/FP/TN/FN classification is evaluated by human experts case by case.We argue that the subjective human evaluation may introduce bias while the fully objective classification will ensure a consistent format in retrieved synthesis conditions, which is beneficial for the follow-up material applications (see Sec. 5.3 for more details).</p>
<p>A key parameter of the RAG algorithm lies in the number of example shots used in the LLM prompt, i.e., K.While the original GPT-4 paper claims that a 3-shot approach could achieve considerable performance [3], field experiments will be needed in most applications to determine an appropriate choice.As shown in Figure 6, the solid blue lines with circle symbol give the performance variation with different Ks, using the best BM25 as RAG algorithm.Both F1 and ACC increase the most from zero-shot to one-shot, and continue to grow until K = 4.After this peak (F 1 = 0.93, ACC = 0.90), the metrics oscillate without surpassing the best performance.Meanwhile, the few-shot method with random example selection (dotted blue lines) shows the same trend as K increases.However, its performance metrics are consistently below the BM25 algorithm, mostly having gaps more than 0.05 on F1.This result again demonstrates the effectiveness of the proposed RAG algorithm.</p>
<p>Material knowledge augmentation via prompt engineering</p>
<p>In addition to few-shot examples, another way to augment the domain knowledge of general-purpose LLM is through the fixed background prompt [9].The previous LLM adaptations on MOFs synthesis extraction by Zheng et al. [24] introduce a preliminary prompt engineering approach, which include the task description of MOFs synthesis extraction and the output format specification.In our work, based on the latest prompt engineering expertise [21], we propose to further incorporate two types of material knowledge into the background prompt: definition of each MOFs synthesis condition, and deterministic constraints on each condition's numerical/textual value or structure (if any).As shown in Figure 5(d), by integrating the new material knowledge, the F1 metric increases from 0.91 to 0.93.However, when the few-shot examples are not incorporated, the background material knowledge will not lead to significant improvement by itself.</p>
<p>The details of newly introduced MOFs synthesis definitions and constraints as background prompts are listed in Table 1.Notably, we summarize three types of constraints on synthesis conditions: numerical that the value of a condition should fall into certain range according to prior knowledge, textual that an extracted condition by text should adhere to certain format to speedup follow-up material application, and structural that certain rules related to the condition are followed in all MOFs synthesis process.</p>
<p>Optimization for High-Throughput MOFs Synthesis Extraction</p>
<p>Organic Linker (name &amp; amount)</p>
<p>The organic precursor linking metal ions or clusters ...</p>
<p>N/A</p>
<p>Solvent (name &amp; amount)</p>
<p>The liquid medium in which reactants are dissolved ... Textual: include "solution" if the solvent contains water ...</p>
<p>Modulator (name &amp; amount)</p>
<p>The substance to adjust reaction conditions (e.g., pH value) ... Structural: the elements of modulator will not become part of the backbone of MOF structure ...</p>
<p>Reaction process (duration &amp; temperature)</p>
<p>The synthesis process producing MOFs ... Numerical: The reaction duration will last several minutes to hours ... Structural: Crystallization is not a reaction process ... when the method is deployed to high-throughput scenarios involving thousands of real-world literature and millions of material texts.The challenges include but not limited to the large bill from calling commercial LLM APIs, the high cost to annotate enough examples for few-shot learning, and the pragmatic issues in material application.</p>
<p>Synthesis paragraph detection</p>
<p>To train the machine learning model for synthesis paragraph detection, we first annotate a dataset of 440 papers randomly sampled from the large dataset of Sec.5.1.Details can be accessed in Sec.5.2.Finally, this process yields 1,349 synthesis paragraphs as positive samples.To train the classifier, negative samples by non-synthesis paragraphs are obtained after removing all annotated paragraphs from a paper, leading to 11,783 negative samples.We employed the standard BERT model, specifically the pre-trained bert-base-uncased model from HuggingFace, for training.The training and validation processes utilized a 5-fold cross-validation method.Given the imbalance dataset, we used stratified 5-fold cross-validation to ensure that the ratio of positive to negative samples remained consistent in each split.The final classification performance is quite high, with an ACC of 0.989, precision of 0.955, recall of 0.947, and F1 = 0.951.</p>
<p>Paragraphs related to the synthesis process constitute only about 6% of an a b article's total length but concentrate the main synthesis condition.Extracting condition from synthesized paragraphs, rather than the entire text, can significantly reduce the overhead of LLM-based approach and increase the density of synthesis conditions in text, thereby enhancing extraction performance.</p>
<p>Sizing the few-shot example pool In this study, we have discussed both the quantity and quality of few-shot examples.Yet, it is still unknown how many ground-truth annotations, namely the example pool where few-shots are selected from, are required for highthroughput synthesis extraction over thousands of MOFs literature or more.To answer this question, we design an experiment that assumes the entire dataset to be 123 synthesis paragraphs (all with ground-truth synthesis conditions known), and the example pool size (# of annotations) to increase from zero.The example pool in each setting is randomly chosen from the entire dataset.To alleviate the uncertainty from randomness, we give 5 trials on each example pool size setting.Also, to further understand the scalability of example pool sizing, we create a new dataset with 60 synthesis paragraphs from the entire data.</p>
<p>Figure 7(a) illustrates the result on the effect of example pool size.First, the first few annotations (from 0 to 5 in the figure) contribute the most performance gain, regardless of the size of entire dataset.This is coherent with the effect observed on zero-shot learning vs. one-shot.Second, smaller datasets (i.e., 60-paragraph by the green line) require fewer # of annotations than larger datasets (i.e., 123-paragraph by the red line), while achieving the same level of performance.The green line stays above the red line, especially under smaller example pool size.Third, more annotations will almost always bring performance gains and less uncertainty, though most boosts happen at initial few annotations.On the 60-paragraph data, the performance peak (F1=0.92)appears at the pool size of 40, 66.7% of the data size; on the 123-paragraph data, the peak (F1=0.93)does not happen before the pool size of 65, thus at least larger than 52.8% of the data size.A future work would be studying the active learning mechanism, which may help to reduce the required example pool in few-shot learning of LLM. Figure 7(b) further demonstrates the effect of both example pool size and K-shots.As shown in the figure, when the labeled pool size increases from 0 to 5, the performance metrics improve rapidly, indicating that a labeled dataset is much more effective than an unlabeled one.Subsequently, the performance metrics increase slowly until the labeled pool size reaches the range of 40-55.The performance metrics then stabilize, with the F1 score fluctuating slightly around 0.91 and ACC around 0.88.</p>
<p>Coreference Resolution</p>
<p>For convenience of writing, proxy words like "L" or "H2L" are frequently used in the MOFs literature to represent specific organic linkers, which are called coreference in NLP.In all the extracted synthesis conditions from 5269 paragraphs, 578 coreference cases are identified.These proxy words could refer to substances defined far in the same article, which makes it difficult to use the extraction results in downstream material application.</p>
<p>Due to different writing styles, regular expression can not be employed as the sole method to resolve the coreference of these proxy words.We introduce a hybrid method combining LLM and regular expression for coreference resolution.The resolving of proxy word coreference is done in three steps.First, the synthesis paragraph is located in the literature and all the text before the paragraph is input to LLM.The LLM is asked to extract all anaphoric references and the original words.Second, a regular expression is designed to identify coreference proxy words from all the extracted conditions by LLM.Finally, these proxy words in the synthesis condition are matched with the detected anaphoric reference.If a match exist, the proxy word is resolved into the original words discovered by LLM in the second step.</p>
<p>Overall, in all the 578 organic linker conditions using coreference, 79% of them can be resolved by our method.Only 0.023 linkers per paragraph remain unresolved.As shown in Table 2, The five most appearing coreference words are "L", "H2L", "HL", "L1", and "H4L", with all the resolution rates over 85%.</p>
<p>Proxy</p>
<p>MOFs Structure Inference</p>
<p>To better validate the accuracy and potential of few-shot synthesis extraction method in downstream tasks, we set up a real-world MOFs synthesis-structure inference task and compared it with existing benchmark methods (zero-shot LLM).The specific task is to predict the microscopic property of MOFs: global cavity diameter, pore limiting diameter, largest cavity diameter, and framework density, using the synthesis conditions including metals, organic links, solvents, and reaction duration/temperature.We evaluate the task performance using coefficient of determination (R 2 ) of each inference model.The R 2 metric effectively quantifies a model's explanatory power regarding the actual data variation and the model accuracy.Therefore, it can be used to reflect the impact of different synthesis conditions on MOFs microstructure.The evaluation data is a subset of the CSD database [14], which encompasses 5269 MOFs.As detailed in Sec.5.1, these MOFs are carefully selected so that each MOF is described by only one scientific literature and the literature will only have one synthesis paragraph.The resulting dataset ensures the validity of evaluation by exact correspondence between a MOF's microscopic structure and its extracted synthesis conditions.</p>
<p>Using the few-shot/zero-shot LLMs and other benchmark methods, the 10 synthesis conditions under study are extracted from a unique synthesis paragraph linked to each of the 5269 MOFs.The raw textual conditions extracted are post-processed to improve data quality, such as synonym merging and standardization of temperature/time scales (Sec.5.3).On the LLM output by the few-shot method, the top 100, 135, and 20 precursor names of metals, linkers, and solvents are selected, which leads to a smaller dataset of 800 MOFs.On the LLM by zero-shot method, the distribution of conditions are less longer-tailed, so that a stricter filter is applied to obtain the same number of 800 MOFs.These precursor names are embedded into one length-198 feature vector by the methods in Sec.5.3, where serves as the input features in the material inference task.The target outcome variables are the four microstructure property of a MOF.Their calculation procedure is described in Sec.5.1.</p>
<p>We apply six machine learning models for the inference: Lasso Regression, Bayesian Ridge Regression, AdaBoost, Random Forest, Gradient Boosting Regression, and Extreme Gradient Boosting (XGBoost).The first five models do not support missing value as input, so we use mean imputation instead.</p>
<p>With each model, we compare the two LLM-based method, few-shot learning vs. zero-shot learning, in a 10-fold cross-validation.On the four microstructure properties inferred, the first three lead to negative or close to zero R 2 in every trial.This corresponds to the material knowledge that these properties are not related to synthesis conditions.The last property of framework density is mostly predictable with synthesis conditions, with all positive R 2 values larger than 0.2.This also validates the fact that MOFs density is highly correlated with the metal and organic precursors used in MOFs material synthesis, as well as the reaction duration and temperature.The R 2 metrics on MOFs density inference by the two LLM models is given in Table 3.It can be seen that the synthesis conditions obtained by the few-shot method enjoy much higher predictive power than those obtained by the zero-shot method, in all six machine learning models.The average R 2 value of zero-shot is only 77.3% of that of the proposed few-shot method.Among the six models, XGBoost achieves best performance, an R 2 of 0.4421 on the test set for the few-shot synthesis conditions, and a relatively lower R 2 of 0.3559 on the test set for the zero-shot method.We illustrate the inference result by XGBoost on the scatterplot of Figure 8(a).It shows that the actual vs. predicted distribution of the few-shot method (green dots) preserves higher affinity to the optimal prediction line (red dashed line), than the predictions by zero-shot method (blue dots).The result demonstrates that the proposed few-shot method not only extracts more accurate synthesis conditions in comparison to the baseline, but also significantly improves the performance of downstream material inference tasks.</p>
<p>To further showcase the superiority of the few-shot method, we conducted more trials using the best-performing model, XGBoost.We gradually reduce the test dataset into more densely distributed synthesis conditions, by enforcing stricter data filters and selecting only higher-ranked synthesis condition values.The XGBoost model is tuned with the best hyperparameters on each dataset following the method by Akiba et al. [4].As shown in Figure 8(b), when the test dataset increases with more conditions, the R 2 of predictive models by fewshot method rises quickly while the R 2 of zero-shot method stays stable or rises much slowly.The gap between the two methods widens as the dataset includes more unique conditions.Meanwhile, the data size by the number of MOFs used remains comparable between the two methods in every setting, as indicated by the grouped bar charts in Figure 8(b).The result indicates that considering less frequently appearing synthesis conditions will significantly improve the accuracy of material structure inference when the few-shot method is applied.In contrast, the zero-shot method showed a steady trend in predictive performance.This also reveals the superior performance of the few-shot method in downstream material inference task compared to the zero-shot method.</p>
<p>Methods</p>
<p>Synthesis paragraph detection</p>
<p>To train a machine learning model for binary classification to determine whether a paragraph is synthesized, we randomly obtained 440 papers from the database in Appendix A for annotation.Each paper was annotated by two different annotators to ensure inner annotator agreement.The 880 annotation tasks were assigned to four annotators who used our platform, shown in Figure 9, to annotate synthesis-related paragraphs.After annotation, only paragraphs annotated by both annotators were considered valid, while paragraphs annotated by only one annotator were discarded.If there was an overlap in the positions of the paragraphs annotated by the two annotators, we found that mismatched paragraphs often occurred because one annotator noted more synthesis parameters and thus marked a larger range.In such cases, the larger annotated paragraph was considered valid.This method also resolved minor annotation deviations within a few characters, allowing two slightly different synthesis paragraphs to be considered valid.This process yielded 1,349 valid annotated paragraphs.</p>
<p>To train the discrimination model, non-synthesis paragraphs were needed as negative samples.After removing all annotated paragraphs from a paper, the remaining paragraphs served as negative samples.This method resulted in 11,783 negative samples.We employed the standard BERT model, specifically the pre-trained bert-base-uncased model from HuggingFace, for training.The training and validation processes utilized a 5-fold cross-validation method (k = 5).Given the imbalance dataset, we used stratified k-fold cross-validation to ensure that the ratio of positive to negative samples remained consistent in each split.After training and cross-validation testing, the model's evaluation metrics were as follows: Accuracy = 0.989, Precision = 0.955, Recall = 0.947, and F1 Score = 0.951.The trained model achieved high overall accuracy and can be used for synthesized paragraph classification in extracted paragraphs, which will facilitate subsequent chemical named entity recognition tasks.</p>
<p>The results of applying the model to our MOFs dataset in Appendix A are as follows: According to statistics, the dataset contains a total of 36,233 DOIs, corresponding to 78,741 MOF-IDs.Among these, 21,031 DOIs can be used to extract synthesis paragraphs, and 22,461 DOIs remain one DOI corresponding to one MOF-ID.The intersection of these two sets contains 9,855 DOIs.Further filtering for DOIs that contain only one synthesis paragraph results in 5,269 DOIs.In other words, we extracted 5,269 valid synthesis paragraphs from the complete dataset.</p>
<p>Paragraphs related to the synthesis process constitute only about 2% of an article's total length but concentrate the main synthesis condition.Extracting condition from synthesized paragraphs, rather than the entire text, can significantly reduce the model's extraction overhead and increase the density of field distribution, thereby enhancing data quality.</p>
<p>Few-Shot RAG Algorithms</p>
<p>To maximize the extraction performance of the model, we provide examples of extraction by human-AI annotation as demonstrations.By using a retrieve K demonstrations, the performance of LLMs in extracting synthesis conditions on MOFs can be further improved [9,12,19].Given the set of demonstrations D = d 1 , d 2 , . . ., d n and an input paragraph p, the top K similar demonstrations are obtained as:
Top-K = sort((score(p, d i ), d i ) n i=1 )[: k] (1)
Here, the score is used to estimate the similarity between the embeddings of document d i and paragraph p.The embedding models can be categorized into traditional sparse vector encoders (e.g., TF-IDF, BM25 [18]) and semantic dense vector encoders (e.g., SBERT [8,17]) [10].In our experiments, we compared these two classes of retrieval methods and selected the one that performed best as the final approach.For the traditional sparse vector retrieval method, we use the BM25 algorithm.BM25 is a probabilistic information retrieval model that ranks documents based on the frequency of query terms within the documents.It balances term frequency (how often a term appears in a document) with inverse docu-ment frequency (how rare a term is across the entire document set), thus giving more weight to terms that are significant.The scoring function is defined as:
Score(p, d) = n i=1 IDF(p i ) • f (p i , d) • (k 1 + 1) f (p i , d) + k 1 • (1 − b + b • |d| avg dl ) (2) avg dl = 1 N N j=1 |d j |(3)
where f (p i , d) is the term frequency of p i in document d, |d| is the length of document d, avg dl is the average length of all documents in the set, IDF(q i ) is the inverse document frequency of term q i , and k 1 and b are hyperparameters.</p>
<p>In our experiments, we use the default settings with k 1 = 1.5 and b = 0.75.For the semantic information-based method, we use the embedding vector representation of the text obtained from a pre-trained language model:
Score(p, d) = f (p) • f (d) |f (p)||f (d)|(4)
where f (x) = PLM(x).In this context, PLM refers to a pre-trained language model, such as SBERT.This embedding can be derived by either averaging the token embeddings (mean pooling) or using the embedding of the [CLS] token from a pre-trained language model.Benefiting from its pre-training on large-scale corpora, the PLM encodes rich semantic information into vector representations, enabling more accurate retrieval based on the meaning of the text.Finally, we concatenate the k most relevant paragraphs-extraction pairs obtained in the previous step before the input as few-shot demonstrations.</p>
<p>Conclusion</p>
<p>This work studies the new paradigm of applying few-shot in-context learning to the popular approach of LLM literature extraction for discovering MOFs synthesis conditions.It is shown through experiments that both the quality and the quantity of few-shot demonstrations are important in the studied scenario.We introduce both a novel process of human-AI joint data curation to enhance fewshot demonstration quality and a calibrated BM-25 RAG algorithm to size the optimal few-shot quantity.Scalability issues regarding high-throughput MOFs synthesis condition extraction are resolved using many practical methods such as offline synthesis paragraph detection and LLM-based coreference resolution.Our proposal is thoroughly evaluated using large-scale real-life MOFs dataset, on both text extraction performance for synthesis condition discovery and the downstream material task on structural property inference.</p>
<p>Appendix</p>
<p>MOFs Data</p>
<p>CSD and the retrieved dataset We base our work on the MOF subset of Cambridge Structural Database (CSD) [14] retrieved in June 2022, which lists 84,898 MOFs covering the bonding motifs of all common MOFs in CSD.The entry of a MOF in the database contains its structure in CIF format, the physical properties, a DOI linking to the relevant publication, and a unique MOF ID.</p>
<p>The dataset is then pre-processed according to the goal of this work.First, the full-text describing the MOFs under study should be available.Out of all the 84,898 MOFs, 78,741 has non-empty DOIs.Since the same DOI could be linked to multiple MOFs (one paper reporting more than one MOFs), there leaves 39,579 different DOI links after deduplication and 36,177 downloadable paper full-text.For the convenience of follow-up processing, we focus on the DOIs where the associated publication reports the information of only one MOF in CSD.This leads to a subset of 22,461 MOFs, each with a unique publication file in PDF format.</p>
<p>Next, the PDF of each MOF is converted to plain text [20] and segmented into paragraphs.The high performance classification model in Sec.3.1 is applied to detect synthesis paragraphs enclosing the desired synthesis condition information.Again, for the sake of convenience and accuracy, we only consider the 5,269 MOFs/publications that contain exactly one synthesis paragraph.Another 12,606 publications do not have any synthesis paragraph, probably because these papers are not related to MOFs experiments.The other 4,586 publications have more than one synthesis paragraphs, as they are describing multiple MOFs or synthesis routes.Our pipeline could work with papers having more than one suite of synthesis conditions, but the potential MOF-synthesis mismatch may downgrade the application performance in evaluation.Therefore, throughout this work we stick to the core dataset of 5,269 MOFs/publications and their unique synthesis paragraph.</p>
<p>Microstructure Property Computation</p>
<p>For material evaluation purpose, we also calculate structural and physical properties of the 5,269 MOFs under consideration.The CIF file of each MOF is retrieved from CSD and input to the Zeo++ tool [22].In total, four structural and physical properties are calculated: global cavity diameter, pore limiting diameter, largest cavity diameter, and framework density.We set the probe radius to 1.29A to simulate helium gas molecules, and the number of Monte Carlo samples to 100,000 to ensure the accuracy of calculations.All Zeo++ parameters adhere to standard routines, guaranteeing that the computed properties accurately represent the behavior of gas molecules within the MOF structure.</p>
<p>Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions</p>
<p>High-quality annotations are the cornerstone of few-shot in-context learning; only accurate and highly coherence annotations can improve the precision of extracting.Therefore, we enlisted the help of eight experts in materials science and engineering to assist with the annotations.Additionally, we developed a batch interactive annotation platform to enhance the convenience of the annotation process.During the annotation process, we discovered that the task was challenging and had a high error rate done by human only, which led to poor model extraction performance when using erroneously annotated examples.Consequently, we implemented a comprehensive annotation process to improve quality.</p>
<p>Synthesis Paragraph Annotation</p>
<p>To annotate synthesis paragraphs for offline machine learning, 440 papers were randomly obtained from the database in Appendix A. For inner annotator agreement, each paper was annotated by two different annotators.The 880 annotation tasks were assigned to four annotators, who used our platform shown in Figure 9 to annotate synthesis-related paragraphs.After annotation, only paragraphs annotated by both annotators were considered valid, and paragraphs annotated by only one annotator were discarded.If there was an overlap in the positions of the paragraphs annotated by the two annotators, we found through checking the annotated data that the common mismatched paragraphs often occurred because one annotator noted more synthesis conditons and thus marked a larger range for the synthesis paragraph.In such cases, the paragraph should also be considered valid.Therefore, we treated the larger annotated paragraph as a valid synthesis paragraph.This method also resolved the issue of minor annotation deviations within a few characters, allowing two slightly dif-ferent synthesis paragraphs to be considered valid.This process yielded 1,349 valid annotated paragraphs.To train the discrimination model, non-synthesis paragraphs are needed as negative samples.After removing all paragraphs annotated by annotators from a paper, the remaining paragraphs serve as negative samples.This method resulted in 11,783 negative samples used for training the synthesis paragraph discrimination model.</p>
<p>Synthesis Condition Annotation</p>
<p>We randomly selected 200 papers from the paper database constructed in 5.1, with each paper only contains less than 3 MOFs IDs.The annotation process can be divided into five sections: task configuration, GPT pre-extraction annotation, pilot annotation, batch annotation, and data curation.</p>
<p>In the task configuration stage, domain experts define the key synthesis condition to be annotated and configure the annotation settings.The core standard for annotation configuration is .Due to the diversity of expressions in material papers, consistent annotation methods help increase the density of the resulting data, thereby reducing the complexity of subsequent data cleaning and enhancing the effectiveness of using ML to infer material structure or properties.The selected annotation synthesis condition must: 1) be common in synthesis paragraphs of related papers, and 2) be beneficial for predicting performance condition.We exclude Active process including active temperature and active time after pilot annotation because we recognized its low frequency.Also, Molecular formula was excluded for it helps little in performance parameter prediction.Once the annotation requirements and background knowledge are set by domain experts, the pre-extraction annotation phase can begin.</p>
<p>Before the pilot annotation, the GPT pre-extraction method can be used to preliminarily locate synthesis paragraphs and relevant condition, assisting experts in annotation.The synthesis paragraph discrimination model extracts relevant paragraphs from the papers.Using the annotation requirements and domain knowledge configured in the task configuration stage, zero-shot prompts are applied for pre-extraction to obtain initial extraction data, which is then imported into the annotation system.Although the accuracy of this process is limited, it helps locate paragraphs and reduces annotation difficulty.</p>
<p>Pilot annotation: twenty papers are randomly selected from the 200 candidate papers for pilot annotation to validate and adjust the annotation task settings and platform configuration.The annotation platform is shown in Figure 9. Two annotators independently annotate the 20 papers on the platform.The results are used to check inner annotator agreement to ensure accuracy.This process requires annotators and researchers to analyze and discuss the following: 1) identify ambiguous and unclear parts of the annotation task configuration to clarify specific annotation methods, 2) reanalyze the synthesis condition to determine if some field are too sparse and need to be removed, or if some field are dense enough to be included as synthesis condition for predicting performance, and 3) identify any unreasonable designs in the annotation platform and make necessary modifications.</p>
<p>This pilot annotation stage resulted in 20 valid papers.Annotators and researchers refined the annotation task configuration to maximize the quality of subsequent batch annotations.</p>
<p>Batch annotations: the remaining 180 papers will produce 360 annotation tasks, assigned to six annotators.Each annotator is randomly assigned 60 papers, ensuring each paper is annotated by two different annotators.GPT pre-extraction is also used to enhance annotation accuracy and efficiency.Upon completion, the annotation data undergo a simple inner annotator agreement check, using Jaccard similarity to verify the consistency between the two annotators' results.For each annotation field, a validity threshold of 0.8 overlap between the annotators is required, then the result field was the union of two fields, which is better for subsequent data cleaning than intersection.For each paper, a verified annotation item overlap rate of 80% or higher between the two annotators is considered a valid annotation paper.Non valid papers were not used in follow-up steps and can be used as supplementary data after manual review.</p>
<p>This stage resulted in 147 papers with high overlap rates, used in this experiment.53 papers are excluded for subsequent process.</p>
<p>Joint Human-AI Data Curation: to improve annotation quality, we introduce a joint human-AI data curation process.Experts finalize the data annotations step by double-check the results from both LLM and human annotations.The LLM results, using BM25 few-shot extraction of synthesis condition, are compared with the annotated results to identify inconsistencies.This step helps detect problems in batch annotations and assists in identifying erroneous annotations.Invalid papers will be excluded.We detected and excluded chiral MOFs in annotated papers, with duplicate synthesis conditions and paragraph.Also, for better sampling, we excluded all paragraph with more than one MOF synthesis process.Although our resolution framework can handle multiple MOFs in a single paragraph, we chose one-to-one paragraphs for better samples.</p>
<p>Additionally, common LLM extraction errors by the model are identified, allowing for targeted constraint writing in prompts to improve knowledge-based corrections.Constraints must be presented in the form of knowledge provision and must not contain any examples to avoid overfitting.This human-AI data curation process identified 120 annotation errors and added 5 constraints.</p>
<p>Post-processing of Synthesis Conditions</p>
<p>The raw synthesis conditions extracted by LLM-based method often suffer from data quality issue, which potentially affects the downstream material inference task.We introduce several data postprocessing methods to improve the quality of derived synthesis conditions so that the input data to the inference model can be more formatted and densely distributed.</p>
<p>Data Cleansing on Textual Conditions</p>
<p>The synthesis conditions include discrete names for Metal, Organic Linker, Solvent, and additives.These names often have different representations for the same substance (e.g., "H2O" and "Water" both represent water, "Cd(NO3)2.4H2O"and "Cd(NO3)2?4H2O"both represent cadmium nitrate tetrahydrate).Using unprocessed discrete names increases redundancy and noise in the dataset, complicating the embedding process and affecting the consistency and performance of the model.</p>
<p>Similarity Disambiguation First, we use similarity disambiguation to create an initial list of assimilated names, eliminating some ambiguities caused by inconsistent spelling, based on the Levenshtein distance (edit distance).The specific steps are as follows:</p>
<ol>
<li>
<p>Calculate the Levenshtein distance between two strings.</p>
</li>
<li>
<p>Normalize the similarity score by converting the Levenshtein distance to a score between 0 and 100 using the formula: Similarity ratio = 1 − Levenshtein distance maximum length of the two strings × 100 (5)</p>
</li>
<li>
<p>Set a threshold.We set the threshold at 90 to filter out most unrelated string pairs while ensuring that only truly similar strings are identified as the same object.</p>
</li>
</ol>
<p>Synonym Merging Using GPT-4</p>
<p>Next, we use GPT-4 for synonym merging.This method leverages the powerful capabilities of the GPT-4 model to successfully identify and group different names representing the same substance.The system also includes a reflection mechanism to ensure the accuracy of the classifications.The detailed workflow is as follows:</p>
<ol>
<li>
<p>Parse the input text to prepare the chemical substance names.</p>
</li>
<li>
<p>Use a predefined prompt (PROMPT1) to ask the GPT-4 model to classify the chemical substances and group identical substances.The model returns a JSON array, each item being a list of synonymous substances.</p>
</li>
<li>
<p>Use a reflection prompt (REFLECT PROMPT1) to re-evaluate the initial classification results, ensuring classification accuracy.This step checks if the substances within each group belong together and if two groups represent the same substance.Finally, output the final classification results.</p>
</li>
</ol>
<p>This method is suitable for synonym merging tasks in materials chemistry, such as Metal Source, Organic Linker, and Solvent.We merged data with frequencies of 8/4/5 and above for Metal Source, Organic Linker, and Solvent, respectively, instead of merging all data.This reduces potential errors from merging low-frequency data and ensures fairness in subsequent comparisons.</p>
<p>Standardization of Numeric Conditions on Time and Temperature</p>
<p>After processing the discrete names in the synthesis conditions, we continued to parse and capture numerical data for time and temperature.These data may have quality issues such as inconsistent units and the presence of special characters.To address these issues, we performed the following normalization: Extracting and Formatting Data Using GPT-4, we extracted and formatted relevant data for time and temperature.</p>
<p>Unit Standardization</p>
<p>We defined standard units for each data type.For example, time was standardized to hours, and temperature was standardized to Celsius (room temperature set at 25 • C).</p>
<p>Cleaning Special Characters.</p>
<p>Using regular expressions, we cleaned and formatted data that might contain special characters (such as spaces, commas, etc.).Through these steps, we ensured the integrity and usability of the data, laying a solid foundation for subsequent processing and analysis.</p>
<p>Data Filtering by Synthesis Condition Distributions</p>
<p>After data cleansing and standardization, the distribution of different synthesis conditions becomes more centralized.As shown in Figure 10, the entity lists of both metal source and solvent are shortened.The number of unique organic linkers remains high due to its long-tailed distribution.In the application of MOFs microstructure property inference, we will only select these MOFs synthesized by top entities in metal source, organic linker, and solvent.For example, by default we apply a filter of (100, 135, 20), which select the MOFs having top-100 metal source in the ranked list of Figure 10(a), top-135 organic linker, and top-20 solvent.Note that for LLM models in comparison, different filters may be applied to ensure the same number of MOFs in the dataset.</p>
<p>Feature Embedding for Metal, Organic Linker, and Solvent Data</p>
<p>After disambiguation and merging, we obtained high-quality precursor/solvent data.To build accurate predictive models, we need to perform corresponding feature embedding to capture the material/structural characteristics of the precursor/solvent data.The specific steps are as follows:</p>
<p>Obtaining Chemical Formulas and SMILES Using GPT-4, we obtained the chemical formulas and SMILES for the top 100 Metals and the top 20 Solvents after disambiguation and merging.For Organic Linkers, due to the complexity of their naming, GPT-4 could not accurately obtain the corresponding SMILES.Therefore, we manually collected the SMILES for the top 135 Organic Linkers after disambiguation and merging.</p>
<p>Calculating Molecular Features Based on the obtained SMILES, we used RDKit to calculate the molecular features of Metals, Organic Linkers, and Solvents, including molecular weight, LogP values, the number of hydrogen bond donors and acceptors, Labute surface area, maximum molecular distance, molecular length, width, height, and topological polar surface area (TPSA).</p>
<p>Calculating Metal Salt Features Using the Composition class from Pymatgen, we automatically inferred and assigned oxidation states for the chemical formulas of metal salts.Using the MultipleFeaturizer class from the Matminer library, we calculated a series of chemical features, including elemental properties, atomic orbitals, electron affinity, and electronegativity differences.Additionally, we included features of the metal elements contained in the MOFs, such as atomic mass, atomic radius, thermal conductivity, and detailed electronic configuration vector representa-S y n t h e s i s P a r a g r a tions.These features provide more comprehensive elemental property information for in-depth analysis of the performance and behavior of MOFs.</p>
<p>Visual MOFs Synthesis Condition Extraction Engine and Database</p>
<p>Database and Engine</p>
<p>To streamline the entire workflow and efficiently organize the extraction results from related papers, we developed the Visual MOFs Synthesis Extraction Engine and Database.Using our approach, we processed over 30,000 papers and extracted 57,081 synthesis paragraphs, on which we then performed synthesis condition extraction.To better view and analyze the vast amount of extraction results, we built a comprehensive database with 2 features: 1) Basic Statistics: The database provides basic statistics on all extraction results, including data on synthesis paragraphs and various synthesis conditions (Figure 11).2) Advanced Search Capabilities: This database is designed to support logical expression searches for specific fields, allowing users to search for synthesis conditions, paper titles, and synthesis paragraph content with precision, and enables visualization of the retrieval results.2. Automatic Paragraph Extraction: The system will automatically extract synthesis paragraphs from the uploaded papers for users to select the paragraphs to process and proceed with synthesis condition extraction.</p>
<p>Configurable Extraction:</p>
<p>The engine supports configuration for synthesis condition extraction, allowing users to adjust the sample quantity and selection method input into the large model.</p>
<p>4.</p>
<p>Organized and Visualized Data: The extracted conditions are systematically organized and visualized for data interpretation and analysis.</p>
<p>Synthesis Visualization</p>
<p>The visualization system we designed can support users in analyzing synthesis paragraphs.Initially, users upload batch PDF papers and process through the LLM.Once extraction is complete, users can utilize the filtering panel to select specific paragraphs for analysis.The overall performance panel (Fig. 12(a)) then displays four key performance metrics of the LLM resolution, with a default HeatMap (Fig. 12(b).I) providing a detailed view of entity resolution performance across all evaluation metrics.Suppose further detail on specific metrics is needed.In that case, users can access the second tab (Fig. 12(b).II), sliding down to the relevant rows to view the distribution of paragraph performance across various parameters in bar charts.To explore similarities with other paragraphs in the database, users can switch to the third tab (Fig. 12(b).III).Here, red dots indicate newly extracted paragraphs; users can look for nearby black dots representing similar paragraphs in the database to compare specific composite parameters.Should users decide to replace or re-examine certain paragraphs, they can reselect them in the filtering panel (Fig. 12(c)).This action triggers an automatic update of the corresponding performance metrics and visual charts, allowing users to repeat the analysis as needed.</p>
<p>Figure 1 :
1
Figure 1: Key indicators (F1, ACC, Precision, Recall) of the synthesis condition extraction performance on 123 MOFs with ground-truth data: (a) our 4-shot RAG algorithm; (b) zero-shot LLM as the baseline; (c) confusion matrix definition for evaluation.</p>
<p>Figure 2 :
2
Figure 2: The overall pipeline of the few-shot in-context learning method for synthesis condition extraction from MOFs literature.</p>
<p>Figure 3 :
3
Figure 3: The human-AI reflection procedure to improve few-shot examples.</p>
<p>Figure 4 :
4
Figure 4: Optimization of general-purpose LLMs for the MOFs synthesis extraction: few-shot in-context learning and RAG.many tasks (e.g., synthesis extraction of various materials) without the need to re-train the model, as shown in Figure 4.In comparison, FT requires gradientbased training for each new task to update the model weights and a considerable number of labeled examples for supervision.Though the latest technology of few-shot fine-tuning (FS-FT) has reduced the requirement of examples to the same level of FS-ICL[11] [15], training and maintaining a small fraction of updated language model weights can still be costly for lightweight LLM usage scenarios such as synthesis extraction in this work.FT also suffers from spurious correlations due to the overfitting effect[15].Despite the superiority of FS-ICT in our scenario, the paradigm also draws concerns due to its disadvantages.First, the inclusion of few-shots in the prompt brings additional computation cost to the language model.In mainstream implementations, the number of shots, denoted as K, ranges from 10, 20 to 100[6] [15].Yet, we will show in this work, for the task of MOFs synthesis extraction, the setting of K = 4 or even K = 1 (known as one-shot) can significantly boost the performance over the setting of K = 0 (known as zero-shot).Second, in previous studies, the format of prompts in FS-ICT (e.g., the wording and ordering of</p>
<p>Figure 5 :
5
Figure 5: Comparison of different RAG algorithms and their configurations.</p>
<p>Figure 6 :
6
Figure 6: The impact of example data quality on extraction performance, with varying number of shots.</p>
<p>Figure 7 :
7
Figure 7: Synthesis extraction performance with varying sizes of example pool: (a) average F1 and its 95% CI of 123-paragraph and 60-paragraph datasets; (b) 123-paragraph dataset with different K-shots.</p>
<p>Figure 8 :
8
Figure 8: Performance on MOFs structure inference task: (a) predictive power of the best XGBoost model, few-shot vs. zero-shot; (b) comparison of R 2 values and data counts across different data filters.</p>
<p>Figure 9 :
9
Figure 9: User interface of the annotation platform.</p>
<p>Figure 10 :
10
Figure 10: Frequencies of occurrence for MOFs synthesis conditions: (a) metal precursor; (b) organic linker; (c) solvent.</p>
<p>Figure 11 :
11
Figure 11: Statistics on the database containing all the extraction results.</p>
<p>Figure 12 :
12
Figure 12: Visualization interface for illustrating the synthesis extraction process and result.</p>
<p>Table 1 :
1
Material knowledge as background prompts: synthesis condition definition and numerical/textual/structural constraints.</p>
<p>Table 2 :
2
Five most frequently used proxy words and their resolution results.
OccurrenceResolutionResolutionWordsCountCountRatioL1069286.8%H2L645890.6%HL4545100%L1393794.9%H4L383386.8%</p>
<p>Table 3 :
3
Performance comparison of few-shot and zero-shot LLMs across different machine learning models on the inference MOFs framework density.
ModelZero-shot R 2Few-shot R 2Lasso0.17550.2257Bayesian Ridge0.17580.2318AdaBoost0.25700.3298Random Forest0.24980.3468Gradient Boosting0.29190.3632XGBoost0.35590.4421</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Optuna: A nextgeneration hyperparameter optimization framework. T Akiba, S Sano, T Yanase, T Ohta, M Koyama, The 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2019</p>
<p>Harvesting water from air with high-capacity, stable furan-based metal-organic frameworks. A H Alawadhi, S Chheda, G D Stroscio, Z Rong, D Kurandina, H L Nguyen, N Rampal, Z Zheng, L Gagliardi, O M Yaghi, Journal of the American Chemical Society. 14632024</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, A Dunn, S Lee, N Walker, A S Rosen, G Ceder, K A Persson, A Jain, Nature Communications. 15114182024</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Z Sui, arXiv:2301.00234A survey on in-context learning. 2022arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, H Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. H Liu, D Tam, M Muqeeth, J Mohta, T Huang, M Bansal, C A Raffel, Advances in Neural Information Processing Systems. 202235</p>
<p>J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Mof synthesis prediction enabled by automatic data mining and machine learning. Y Luo, S Bag, O Zaremba, A Cierpka, J Andreo, S Wuttke, P Friederich, M Tsotsalas, Angewandte Chemie International Edition. 6119e2022002422022</p>
<p>Development of a cambridge structural database subset: a collection of metal-organic frameworks for past, present, and future. P Z Moghadam, A Li, S B Wiggin, A Tao, A G Maloney, P A Wood, S C Ward, D Fairen-Jimenez, Chemistry of Materials. 2972017</p>
<p>Fewshot fine-tuning vs. in-context learning: A fair comparison and evaluation. M Mosbach, T Pimentel, S Ravfogel, D Klakow, Y Elazar, arXiv:2305.169382023arXiv preprint</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, Nature Communications. 15115692024</p>
<p>N Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. S Robertson, H Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Selective annotation makes language models better few-shot learners. H Su, J Kasai, C H Wu, W Shi, T Wang, J Xin, R Zhang, M Ostendorf, L Zettlemoyer, N A Smith, arXiv:2209.019752022arXiv preprint</p>
<p>. H Tian, W Liu, 2024and other contributors. pdf2htmlex.</p>
<p>J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, arXiv:2302.11382A prompt pattern catalog to enhance prompt engineering with chatgpt. 2023arXiv preprint</p>
<p>Algorithms and tools for high-throughput geometry-based analysis of crystalline porous materials. T F Willems, C H Rycroft, M Kazi, J C Meza, M Haranczyk, Microporous and Mesoporous Materials. 14912012</p>
<p>Reticular synthesis and the design of new materials. O M Yaghi, M O'keeffe, N W Ockwig, H K Chae, M Eddaoudi, J Kim, Nature. 42369412003</p>
<p>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, Journal of the American Chemical Society. 145322023</p>            </div>
        </div>

    </div>
</body>
</html>
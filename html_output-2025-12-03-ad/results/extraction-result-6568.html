<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6568 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6568</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6568</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-267412329</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.02611v2.pdf" target="_blank">PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?</a></p>
                <p><strong>Paper Abstract:</strong> Recent works show that the largest of the large language models (LLMs) can solve many simple reasoning tasks expressed in natural language, without any/much supervision. But, can they also solve challenging first-order combinatorial reasoning problems, such as graph coloring, knapsack and cryptarithmetic? To answer this question, we present PuzzleBench, a dataset of 31 such challenging problems along with a few solved instances for each problem. These problems are all first order, i.e., they can be instantiated with problem instances of varying sizes, and most of them are NP-hard, requiring several reasoning steps to reach the solution. We first observe that LLMs, even when aided by symbolic solvers, perform rather poorly on our dataset. In response, we propose a new approach, Puzzle-LM, which combines LLMs with both symbolic solvers and program interpreters, along with feedback from solved examples, to achieve huge performance gains. Our extensive experimentation and analyses offer new insights into the reasoning abilities and limitations of present-day LLMs.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6568.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6568.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Puzzle-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Puzzle-LM (Program + SMT solver + LLM feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic pipeline that prompts an LLM to generate an input-agnostic Python program which converts serialized puzzle instances to SMT (Z3) constraints, invokes a symbolic solver, and post-processes solver outputs; programs are iteratively corrected using feedback from solved examples so inference on new instances requires no LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (used as the LLM in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large decoder-only transformer accessed via OpenAI API; used to generate instance-agnostic Python programs and to process feedback during program refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Many grid-based and combinatorial puzzles (e.g., Sudoku, Sujiko, Magic-square, Binairo, Dosun Fuwari, Survo, Keisuke, Latin-square and others in PuzzleBench)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Spatial / grid-based puzzles and general first-order combinatorial constraint-satisfaction problems</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PuzzleBench (31 first-order combinatorial reasoning problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Program-aided prompt: LLM is prompted to emit an input-agnostic Python program; iterative feedback from solved examples (up to multiple rounds) is used to refine generated programs; multiple sampled runs are used to pick best program</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Translation to SMT (SMT2) + symbolic solving by Z3; programmatic pre- and post-processing in Python; uses declarative encodings so solver performs search/search-pruning</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Serialized textual formats for inputs/outputs (grid rows as space-separated integers, '0' for empty, separators for subgrids etc.); program constructs SMT2 constraints (range, Distinct, subgrid constraints, path-existence encodings where applicable) and maps solver variable assignments back to output format.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 SMT solver for solving the declarative constraints generated by the program; a Python interpreter executes the generated program (reads input.txt, writes output.txt). The LLM writes the Python program that constructs SMT queries for Z3.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro-average accuracy across problems (fraction of test instances solved correctly using provided verifiers); binary per-instance correctness</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Puzzle-LM (with GPT-4-Turbo) reported macro-average accuracy up to 93.55% after multiple runs and feedback (single-run baseline without feedback reported 60.48%; single-run with 4 rounds of feedback ≈ 80.98%; 5 runs with feedback reached 93.55%).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Puzzle-LM exploits the first-order structure by producing instance-agnostic declarative programs that are correctable via feedback; declarative SMT encodings let the solver efficiently exploit instance structure (more efficient inference than brute-force Python programs); feedback from small solved-example sets and multiple sampled runs yields large gains; programmatic approaches (Puzzle-LM, PAL) are more robust to increasing instance size than purely LLM prompting or direct LLM→SMT translations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared to few-shot prompting and Logic-LM, Puzzle-LM shows large gains (paper reports Puzzle-LM outperforming few-shot prompting by tens of percentage points and outperforming Logic-LM and PAL; specific reported numbers: Puzzle-LM outperforms few-shot prompting by 29.98 percentage points, PAL by 11.97 points and Logic-LM by 23.14 points in one reported comparison; single-run baseline 60.48% -> with 4 feedback rounds 80.98% (≈+20.5 pts); multiple runs further increase to 93.55%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Fails or underperforms on puzzles whose natural-language-to-SMT encoding is nontrivial (examples: number-link, Hamiltonian path) where LLMs struggled to encode path-existence constraints; depends on the expressivity and language of the chosen solver (some puzzles are difficult to express in the solver's logic); requires LLM to produce correct program initially or to be correctable via feedback; potential dataset contamination concerns and nontrivial compute costs for LLM calls during program synthesis/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6568.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6568.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo (few-shot / direct prompting baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct prompting of GPT-4-Turbo with natural-language rules and input instance; model is asked to output the solution without invoking external tools or generating programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned decoder-only transformer; used zero/few-shot to directly produce solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Many puzzles in PuzzleBench including grid-based ones such as Sudoku, Sujiko, Magic-square, Binairo etc.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Spatial / combinatorial reasoning / constraint satisfaction</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PuzzleBench</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot / few-shot direct answer prompting (natural-language description of rules + input instance; for reported experiments often zero-shot for fairness given structured nature of tasks; some few-shot runs described in table)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Pure LLM internal reasoning (chain-of-thought not explicitly stated as used in main comparisons); LLM must perform search/planning internally</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Direct textual serialized input format (rows as lines, 0 for empty cells, etc.) fed to the LLM; no intermediate program or symbolic encoding created by user.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro-average accuracy (fraction of test instances solved correctly by direct LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported macro-average ~48.51% for GPT-4-Turbo few-shot/direct prompting (Table 1 shows 48.51% in the paper for this baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Few-shot prompting struggled on many PuzzleBench tasks, particularly as instance size increased; the method could not reliably leverage problem structure and produced poor scaling behavior compared to programmatic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Substantially outperformed by Puzzle-LM and PAL; Puzzle-LM reported tens of percentage points higher accuracy than this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>LLM-alone approach often fails to perform the extensive search/constraint solving required for NP-hard fcore problems; syntactic/formatting errors or invalid output formats counted as incorrect; performance degrades as problem instance size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6568.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6568.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-Aided Language models (PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where the LLM is prompted to generate an input-agnostic imperative program (Python) that is executed by an interpreter; the program performs the reasoning algorithmically (often blind search or procedural methods).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (and GPT-3.5-Turbo, Mixtral 8x7B in other experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM generates Python code (imperative) implementing problem-solving logic; the code is run by a Python interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Various PuzzleBench tasks including Sudoku, Latin-square, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Spatial/grid and general combinatorial problems</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PuzzleBench</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>PAL-style prompting: LLM asked to write a Python program that reads input.txt and writes output.txt; programs use standard Python libraries only; optional feedback rounds used to fix code.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Imperative procedural algorithms / brute-force search implemented in generated Python; execution offloaded to Python interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Programmatic (Python) representation operating on parsed serialized textual inputs; often brute-force loops over candidate assignments rather than declarative SMT encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Python interpreter executes the generated program; no symbolic solver is required in vanilla PAL (though programs could call libraries).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro-average accuracy over PuzzleBench; also inference-time comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For GPT-4-Turbo PAL reported ~66.45% macro-average accuracy in Table 1 (paper reports PAL performance much lower than Puzzle-LM but better than naive prompting in many cases).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>PAL leverages LLMs' code-generation abilities (benefits from pretraining on code) but LLM-written imperative programs are often brute-force and inefficient, leading to timeouts or inability to solve larger instances within time limits; PAL benefits from feedback but is harder to correct in practice because imperative code can be brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>PAL improved with feedback and multiple runs but still lagged behind Puzzle-LM (paper reports Puzzle-LM outperforming PAL by ~11.97 percentage points in one comparison; other reported comparisons give PAL trailing Puzzle-LM by ~27.1% in some aggregate evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generated programs often brute-force and slow (higher inference times); hard to correct via feedback compared to declarative program+SMT approach; scales poorly on large instances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6568.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6568.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM (LLM + symbolic solver pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where the LLM translates the natural-language problem instance into a symbolic specification (SMT2), which is then solved by an external SMT solver (Z3); the solver model is then translated back to the desired output format by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (and smaller LLMs evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used as translator to produce SMT2 constraints; relies on solver for actual combinatorial search.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Several PuzzleBench problems; paper gives explicit Logic-LM SMT2 snippets for Sudoku, Subset-Sum, Graph Coloring, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Constraint satisfaction / combinatorial problems</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PuzzleBench</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>LLM prompted to produce SMT2 code only (Logic-LM prompt template); SMT2 is then passed to Z3; LLM converts solver output to desired textual answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Translation to SMT (SMT2) + solving with Z3 (symbolic solver)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>SMT2 constraint language (declare-const, assert, Distinct, arithmetic constraints, ite for sums, etc.); textual grid serialization for inputs</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 SMT solver is invoked to solve constraints generated by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro-average accuracy; analysis of syntactic/semantic correctness of generated SMT2</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logic-LM reported substantially worse performance than Puzzle-LM; one table cell shows a Logic-LM figure of ~60.48% in some conditions for GPT-4-Turbo (paper emphasizes Logic-LM struggles, especially with smaller LLMs, and makes more syntactic/semantic mistakes in generated constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Logic-LM can produce correct SMT encodings in some cases (paper shows correct SMT2 examples for Sudoku and Subset Sum), but tends to suffer from syntactic mistakes (especially with smaller LLMs) and semantic errors that feedback cannot always correct; direct LLM→SMT translation is brittle and less able to exploit the problem's first-order structure compared to instance-agnostic program strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Logic-LM benefits from larger LLMs but still underperforms Puzzle-LM; syntactic mistakes reduce with bigger LLMs but semantic correctness problems remain and can be hard to remediate via feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Brittleness in producing syntactically correct and semantically valid solver code; difficulty encoding certain constraints (e.g., path existence) reliably; sensitive to LLM size and prompt quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6568.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6568.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sudoku (example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sudoku (n x n generalization) as evaluated in PuzzleBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard n x n Sudoku problems serialized as rows of integers (0 for empty) and solved via LLM-based approaches (direct prompting, PAL, Logic-LM, Puzzle-LM); used as a representative grid-based spatial puzzle in scaling experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (evaluated under multiple methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned decoder-only transformer; used both to generate direct answers, Python programs (PAL/Puzzle-LM) and SMT2 encodings (Logic-LM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Spatial reasoning / constraint satisfaction (grid-based)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PuzzleBench (Sudoku subset; training examples smaller than test instances e.g., 9x9 -> larger ones)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Direct prompting; PAL (Python program); Logic-LM (SMT2); Puzzle-LM (instance-agnostic Python program -> Z3).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>SMT encodings (range constraints, Distinct for rows/columns/subgrids) when using solver-based methods; procedural brute-force when using PAL; direct LLM reasoning for few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Textual row-major representation with integers per cell; example SMT2 encodings shown in paper use declarations for each cell a11..aNN, asserts for range constraints and Distinct for rows/columns/subgrids.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 SMT solver used in Logic-LM and Puzzle-LM; Python interpreter executes generated programs that construct SMT2.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-instance binary correctness; inference time (seconds) reported for PAL vs Puzzle-LM on example instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Inference-time example: PAL average ~2.01s per Sudoku instance vs Puzzle-LM ~0.215s (Puzzle-LM much faster due to solver); overall Puzzle-LM achieved high aggregate accuracy on PuzzleBench (see Puzzle-LM entry).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>SMT-based encodings (Puzzle-LM) lead to succinct declarative constraints that let Z3 exploit structure and solve quickly; PAL programs often brute-force and slow; Puzzle-LM is robust to increasing Sudoku instance sizes compared to Logic-LM and few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Puzzle-LM much faster than PAL on Sudoku inference (2.01s vs 0.215s) and yields higher solve rates in aggregate comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>If the LLM encodes constraints incorrectly (syntactic/semantic errors) the solver can fail; some LLMs produce syntactic SMT errors requiring feedback; for very large instances brute-force PAL programs time out.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6568.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6568.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binairo (feedback case)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binairo (example of constraint-encoding errors corrected via feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binairo is a grid-based puzzle included in PuzzleBench; GPT-4-Turbo initially encoded row/column distinctness constraints incorrectly in generated SMT/Python program but corrected them after feedback on solved examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (within Puzzle-LM pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM used to emit program encoding; corrected via iterative feedback prompts containing example input/output and error messages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Binairo</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based spatial constraint satisfaction</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PuzzleBench (Binairo subset)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Puzzle-LM generation of Python program + feedback using solved examples</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Program constructs SMT constraints for parity/adjacency/distinctness rules and leverages Z3 solver; LLM refines encoding after verification errors.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Serialized grid format; Python program converts cell variables into SMT constraints (Distinct/inequality constraints) and maps back to grid output.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 SMT solver; Python interpreter executes the program that queries Z3.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-instance correctness; observed correction of encoding errors via feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as a single-number isolate; paper reports that Binairo was among problems where GPT-4-Turbo initially made encoding errors but reached 100% accuracy with feedback in Puzzle-LM for many problems.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Feedback on solved examples is effective in correcting systematic encoding mistakes (e.g., row/column distinctness) for many puzzles; demonstrates that LLM-generated declarative programs are amendable to iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>With feedback Binairo moved from incorrect encodings to correct program and solved training instances; demonstrates the utility of solved-example feedback compared to zero-feedback program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Some puzzles require more subtle encodings that may need multiple feedback rounds; single-run generation may fail and require multiple sampled runs and feedback cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6568.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6568.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hamiltonian-Path limitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hamiltonian Path (example of encoding difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hamiltonian path decision variant (graph path visiting each vertex exactly once) in PuzzleBench was reported as an example where LLMs (even GPT-4-Turbo with Puzzle-LM) struggled to encode existence-of-path constraints into SMT; feedback did not reliably fix these encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (evaluated in Puzzle-LM and other baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used to generate programmatic or SMT encodings; failed to find robust encodings for path-existence constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Hamiltonian Path</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Graph / path existence (combinatorial, NP-hard)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PuzzleBench</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Puzzle-LM and Logic-LM attempted to encode the problem; verification via solved examples and feedback applied but did not succeed reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Intended technique: encode path-existence as SMT constraints; reported difficulty in producing correct encodings (existence of path is nontrivial to express compactly in solver-friendly form).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Serialized edge list (N then edge pairs) as text; attempted SMT encodings of path variables and constraints mapping to solver variables.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 SMT solver was the intended backend, but solver encodings produced by LLMs were often incorrect or inadequate to represent path existence, leading to failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-instance correctness (binary); these problems were among the set that remained below 100% accuracy and were not corrected by feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as among 6 problems where performance remained below 100% and could not be corrected through feedback; no reliable solve rate reported for this puzzle specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Conversions that encode path-existence (e.g., Hamiltonian path, number-link) appear peculiar and challenging even for high-capacity LLMs; authors note these conversions may be hard even for average computer-science students.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Feedback and multiple runs which helped other problems did not reliably fix encoding issues for Hamiltonian path; Puzzle-LM shows weakness on such encoding-heavy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Difficulty stems from representing existential path constraints in the SMT fragment used and from LLMs' inability to systematically produce such encodings; solver-language expressivity and complexity of required encoding are core obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Program-aided Language models (PAL) <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Satlm: Satisfiability-aided language models using declarative prompting <em>(Rating: 2)</em></li>
                <li>Gao et al. (2023) Program-aided Language models (PAL) <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6568",
    "paper_id": "paper-267412329",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "Puzzle-LM",
            "name_full": "Puzzle-LM (Program + SMT solver + LLM feedback)",
            "brief_description": "A neuro-symbolic pipeline that prompts an LLM to generate an input-agnostic Python program which converts serialized puzzle instances to SMT (Z3) constraints, invokes a symbolic solver, and post-processes solver outputs; programs are iteratively corrected using feedback from solved examples so inference on new instances requires no LLM calls.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (used as the LLM in experiments)",
            "model_description": "Instruction-tuned large decoder-only transformer accessed via OpenAI API; used to generate instance-agnostic Python programs and to process feedback during program refinement.",
            "model_size": null,
            "puzzle_name": "Many grid-based and combinatorial puzzles (e.g., Sudoku, Sujiko, Magic-square, Binairo, Dosun Fuwari, Survo, Keisuke, Latin-square and others in PuzzleBench)",
            "puzzle_type": "Spatial / grid-based puzzles and general first-order combinatorial constraint-satisfaction problems",
            "dataset_name": "PuzzleBench (31 first-order combinatorial reasoning problems)",
            "prompting_method": "Program-aided prompt: LLM is prompted to emit an input-agnostic Python program; iterative feedback from solved examples (up to multiple rounds) is used to refine generated programs; multiple sampled runs are used to pick best program",
            "reasoning_technique": "Translation to SMT (SMT2) + symbolic solving by Z3; programmatic pre- and post-processing in Python; uses declarative encodings so solver performs search/search-pruning",
            "internal_representation": "Serialized textual formats for inputs/outputs (grid rows as space-separated integers, '0' for empty, separators for subgrids etc.); program constructs SMT2 constraints (range, Distinct, subgrid constraints, path-existence encodings where applicable) and maps solver variable assignments back to output format.",
            "use_of_external_tool": true,
            "external_tool_description": "Z3 SMT solver for solving the declarative constraints generated by the program; a Python interpreter executes the generated program (reads input.txt, writes output.txt). The LLM writes the Python program that constructs SMT queries for Z3.",
            "evaluation_metric": "Macro-average accuracy across problems (fraction of test instances solved correctly using provided verifiers); binary per-instance correctness",
            "performance": "Puzzle-LM (with GPT-4-Turbo) reported macro-average accuracy up to 93.55% after multiple runs and feedback (single-run baseline without feedback reported 60.48%; single-run with 4 rounds of feedback ≈ 80.98%; 5 runs with feedback reached 93.55%).",
            "analysis_findings": "Puzzle-LM exploits the first-order structure by producing instance-agnostic declarative programs that are correctable via feedback; declarative SMT encodings let the solver efficiently exploit instance structure (more efficient inference than brute-force Python programs); feedback from small solved-example sets and multiple sampled runs yields large gains; programmatic approaches (Puzzle-LM, PAL) are more robust to increasing instance size than purely LLM prompting or direct LLM→SMT translations.",
            "ablation_comparison": "Compared to few-shot prompting and Logic-LM, Puzzle-LM shows large gains (paper reports Puzzle-LM outperforming few-shot prompting by tens of percentage points and outperforming Logic-LM and PAL; specific reported numbers: Puzzle-LM outperforms few-shot prompting by 29.98 percentage points, PAL by 11.97 points and Logic-LM by 23.14 points in one reported comparison; single-run baseline 60.48% -&gt; with 4 feedback rounds 80.98% (≈+20.5 pts); multiple runs further increase to 93.55%).",
            "limitations": "Fails or underperforms on puzzles whose natural-language-to-SMT encoding is nontrivial (examples: number-link, Hamiltonian path) where LLMs struggled to encode path-existence constraints; depends on the expressivity and language of the chosen solver (some puzzles are difficult to express in the solver's logic); requires LLM to produce correct program initially or to be correctable via feedback; potential dataset contamination concerns and nontrivial compute costs for LLM calls during program synthesis/refinement.",
            "uuid": "e6568.0",
            "source_info": {
                "paper_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4-Turbo few-shot",
            "name_full": "GPT-4-Turbo (few-shot / direct prompting baseline)",
            "brief_description": "Direct prompting of GPT-4-Turbo with natural-language rules and input instance; model is asked to output the solution without invoking external tools or generating programs.",
            "citation_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo",
            "model_description": "Instruction-tuned decoder-only transformer; used zero/few-shot to directly produce solutions.",
            "model_size": null,
            "puzzle_name": "Many puzzles in PuzzleBench including grid-based ones such as Sudoku, Sujiko, Magic-square, Binairo etc.",
            "puzzle_type": "Spatial / combinatorial reasoning / constraint satisfaction",
            "dataset_name": "PuzzleBench",
            "prompting_method": "Zero-shot / few-shot direct answer prompting (natural-language description of rules + input instance; for reported experiments often zero-shot for fairness given structured nature of tasks; some few-shot runs described in table)",
            "reasoning_technique": "Pure LLM internal reasoning (chain-of-thought not explicitly stated as used in main comparisons); LLM must perform search/planning internally",
            "internal_representation": "Direct textual serialized input format (rows as lines, 0 for empty cells, etc.) fed to the LLM; no intermediate program or symbolic encoding created by user.",
            "use_of_external_tool": false,
            "external_tool_description": "",
            "evaluation_metric": "Macro-average accuracy (fraction of test instances solved correctly by direct LLM outputs)",
            "performance": "Reported macro-average ~48.51% for GPT-4-Turbo few-shot/direct prompting (Table 1 shows 48.51% in the paper for this baseline).",
            "analysis_findings": "Few-shot prompting struggled on many PuzzleBench tasks, particularly as instance size increased; the method could not reliably leverage problem structure and produced poor scaling behavior compared to programmatic approaches.",
            "ablation_comparison": "Substantially outperformed by Puzzle-LM and PAL; Puzzle-LM reported tens of percentage points higher accuracy than this baseline.",
            "limitations": "LLM-alone approach often fails to perform the extensive search/constraint solving required for NP-hard fcore problems; syntactic/formatting errors or invalid output formats counted as incorrect; performance degrades as problem instance size increases.",
            "uuid": "e6568.1",
            "source_info": {
                "paper_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PAL",
            "name_full": "Program-Aided Language models (PAL)",
            "brief_description": "An approach where the LLM is prompted to generate an input-agnostic imperative program (Python) that is executed by an interpreter; the program performs the reasoning algorithmically (often blind search or procedural methods).",
            "citation_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (and GPT-3.5-Turbo, Mixtral 8x7B in other experiments)",
            "model_description": "LLM generates Python code (imperative) implementing problem-solving logic; the code is run by a Python interpreter.",
            "model_size": null,
            "puzzle_name": "Various PuzzleBench tasks including Sudoku, Latin-square, etc.",
            "puzzle_type": "Spatial/grid and general combinatorial problems",
            "dataset_name": "PuzzleBench",
            "prompting_method": "PAL-style prompting: LLM asked to write a Python program that reads input.txt and writes output.txt; programs use standard Python libraries only; optional feedback rounds used to fix code.",
            "reasoning_technique": "Imperative procedural algorithms / brute-force search implemented in generated Python; execution offloaded to Python interpreter",
            "internal_representation": "Programmatic (Python) representation operating on parsed serialized textual inputs; often brute-force loops over candidate assignments rather than declarative SMT encodings.",
            "use_of_external_tool": true,
            "external_tool_description": "Python interpreter executes the generated program; no symbolic solver is required in vanilla PAL (though programs could call libraries).",
            "evaluation_metric": "Macro-average accuracy over PuzzleBench; also inference-time comparisons",
            "performance": "For GPT-4-Turbo PAL reported ~66.45% macro-average accuracy in Table 1 (paper reports PAL performance much lower than Puzzle-LM but better than naive prompting in many cases).",
            "analysis_findings": "PAL leverages LLMs' code-generation abilities (benefits from pretraining on code) but LLM-written imperative programs are often brute-force and inefficient, leading to timeouts or inability to solve larger instances within time limits; PAL benefits from feedback but is harder to correct in practice because imperative code can be brittle.",
            "ablation_comparison": "PAL improved with feedback and multiple runs but still lagged behind Puzzle-LM (paper reports Puzzle-LM outperforming PAL by ~11.97 percentage points in one comparison; other reported comparisons give PAL trailing Puzzle-LM by ~27.1% in some aggregate evaluations).",
            "limitations": "Generated programs often brute-force and slow (higher inference times); hard to correct via feedback compared to declarative program+SMT approach; scales poorly on large instances.",
            "uuid": "e6568.2",
            "source_info": {
                "paper_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM (LLM + symbolic solver pipeline)",
            "brief_description": "An approach where the LLM translates the natural-language problem instance into a symbolic specification (SMT2), which is then solved by an external SMT solver (Z3); the solver model is then translated back to the desired output format by the LLM.",
            "citation_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (and smaller LLMs evaluated)",
            "model_description": "LLM used as translator to produce SMT2 constraints; relies on solver for actual combinatorial search.",
            "model_size": null,
            "puzzle_name": "Several PuzzleBench problems; paper gives explicit Logic-LM SMT2 snippets for Sudoku, Subset-Sum, Graph Coloring, etc.",
            "puzzle_type": "Constraint satisfaction / combinatorial problems",
            "dataset_name": "PuzzleBench",
            "prompting_method": "LLM prompted to produce SMT2 code only (Logic-LM prompt template); SMT2 is then passed to Z3; LLM converts solver output to desired textual answer.",
            "reasoning_technique": "Translation to SMT (SMT2) + solving with Z3 (symbolic solver)",
            "internal_representation": "SMT2 constraint language (declare-const, assert, Distinct, arithmetic constraints, ite for sums, etc.); textual grid serialization for inputs",
            "use_of_external_tool": true,
            "external_tool_description": "Z3 SMT solver is invoked to solve constraints generated by the LLM.",
            "evaluation_metric": "Macro-average accuracy; analysis of syntactic/semantic correctness of generated SMT2",
            "performance": "Logic-LM reported substantially worse performance than Puzzle-LM; one table cell shows a Logic-LM figure of ~60.48% in some conditions for GPT-4-Turbo (paper emphasizes Logic-LM struggles, especially with smaller LLMs, and makes more syntactic/semantic mistakes in generated constraints).",
            "analysis_findings": "Logic-LM can produce correct SMT encodings in some cases (paper shows correct SMT2 examples for Sudoku and Subset Sum), but tends to suffer from syntactic mistakes (especially with smaller LLMs) and semantic errors that feedback cannot always correct; direct LLM→SMT translation is brittle and less able to exploit the problem's first-order structure compared to instance-agnostic program strategy.",
            "ablation_comparison": "Logic-LM benefits from larger LLMs but still underperforms Puzzle-LM; syntactic mistakes reduce with bigger LLMs but semantic correctness problems remain and can be hard to remediate via feedback.",
            "limitations": "Brittleness in producing syntactically correct and semantically valid solver code; difficulty encoding certain constraints (e.g., path existence) reliably; sensitive to LLM size and prompt quality.",
            "uuid": "e6568.3",
            "source_info": {
                "paper_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Sudoku (example)",
            "name_full": "Sudoku (n x n generalization) as evaluated in PuzzleBench",
            "brief_description": "Standard n x n Sudoku problems serialized as rows of integers (0 for empty) and solved via LLM-based approaches (direct prompting, PAL, Logic-LM, Puzzle-LM); used as a representative grid-based spatial puzzle in scaling experiments.",
            "citation_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (evaluated under multiple methods)",
            "model_description": "Instruction-tuned decoder-only transformer; used both to generate direct answers, Python programs (PAL/Puzzle-LM) and SMT2 encodings (Logic-LM).",
            "model_size": null,
            "puzzle_name": "Sudoku",
            "puzzle_type": "Spatial reasoning / constraint satisfaction (grid-based)",
            "dataset_name": "PuzzleBench (Sudoku subset; training examples smaller than test instances e.g., 9x9 -&gt; larger ones)",
            "prompting_method": "Direct prompting; PAL (Python program); Logic-LM (SMT2); Puzzle-LM (instance-agnostic Python program -&gt; Z3).",
            "reasoning_technique": "SMT encodings (range constraints, Distinct for rows/columns/subgrids) when using solver-based methods; procedural brute-force when using PAL; direct LLM reasoning for few-shot prompting.",
            "internal_representation": "Textual row-major representation with integers per cell; example SMT2 encodings shown in paper use declarations for each cell a11..aNN, asserts for range constraints and Distinct for rows/columns/subgrids.",
            "use_of_external_tool": true,
            "external_tool_description": "Z3 SMT solver used in Logic-LM and Puzzle-LM; Python interpreter executes generated programs that construct SMT2.",
            "evaluation_metric": "Per-instance binary correctness; inference time (seconds) reported for PAL vs Puzzle-LM on example instances.",
            "performance": "Inference-time example: PAL average ~2.01s per Sudoku instance vs Puzzle-LM ~0.215s (Puzzle-LM much faster due to solver); overall Puzzle-LM achieved high aggregate accuracy on PuzzleBench (see Puzzle-LM entry).",
            "analysis_findings": "SMT-based encodings (Puzzle-LM) lead to succinct declarative constraints that let Z3 exploit structure and solve quickly; PAL programs often brute-force and slow; Puzzle-LM is robust to increasing Sudoku instance sizes compared to Logic-LM and few-shot prompting.",
            "ablation_comparison": "Puzzle-LM much faster than PAL on Sudoku inference (2.01s vs 0.215s) and yields higher solve rates in aggregate comparisons.",
            "limitations": "If the LLM encodes constraints incorrectly (syntactic/semantic errors) the solver can fail; some LLMs produce syntactic SMT errors requiring feedback; for very large instances brute-force PAL programs time out.",
            "uuid": "e6568.4",
            "source_info": {
                "paper_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Binairo (feedback case)",
            "name_full": "Binairo (example of constraint-encoding errors corrected via feedback)",
            "brief_description": "Binairo is a grid-based puzzle included in PuzzleBench; GPT-4-Turbo initially encoded row/column distinctness constraints incorrectly in generated SMT/Python program but corrected them after feedback on solved examples.",
            "citation_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (within Puzzle-LM pipeline)",
            "model_description": "Instruction-tuned LLM used to emit program encoding; corrected via iterative feedback prompts containing example input/output and error messages.",
            "model_size": null,
            "puzzle_name": "Binairo",
            "puzzle_type": "Grid-based spatial constraint satisfaction",
            "dataset_name": "PuzzleBench (Binairo subset)",
            "prompting_method": "Puzzle-LM generation of Python program + feedback using solved examples",
            "reasoning_technique": "Program constructs SMT constraints for parity/adjacency/distinctness rules and leverages Z3 solver; LLM refines encoding after verification errors.",
            "internal_representation": "Serialized grid format; Python program converts cell variables into SMT constraints (Distinct/inequality constraints) and maps back to grid output.",
            "use_of_external_tool": true,
            "external_tool_description": "Z3 SMT solver; Python interpreter executes the program that queries Z3.",
            "evaluation_metric": "Per-instance correctness; observed correction of encoding errors via feedback.",
            "performance": "Not reported as a single-number isolate; paper reports that Binairo was among problems where GPT-4-Turbo initially made encoding errors but reached 100% accuracy with feedback in Puzzle-LM for many problems.",
            "analysis_findings": "Feedback on solved examples is effective in correcting systematic encoding mistakes (e.g., row/column distinctness) for many puzzles; demonstrates that LLM-generated declarative programs are amendable to iterative correction.",
            "ablation_comparison": "With feedback Binairo moved from incorrect encodings to correct program and solved training instances; demonstrates the utility of solved-example feedback compared to zero-feedback program generation.",
            "limitations": "Some puzzles require more subtle encodings that may need multiple feedback rounds; single-run generation may fail and require multiple sampled runs and feedback cycles.",
            "uuid": "e6568.5",
            "source_info": {
                "paper_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Hamiltonian-Path limitation",
            "name_full": "Hamiltonian Path (example of encoding difficulty)",
            "brief_description": "Hamiltonian path decision variant (graph path visiting each vertex exactly once) in PuzzleBench was reported as an example where LLMs (even GPT-4-Turbo with Puzzle-LM) struggled to encode existence-of-path constraints into SMT; feedback did not reliably fix these encodings.",
            "citation_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (evaluated in Puzzle-LM and other baselines)",
            "model_description": "LLM used to generate programmatic or SMT encodings; failed to find robust encodings for path-existence constraints.",
            "model_size": null,
            "puzzle_name": "Hamiltonian Path",
            "puzzle_type": "Graph / path existence (combinatorial, NP-hard)",
            "dataset_name": "PuzzleBench",
            "prompting_method": "Puzzle-LM and Logic-LM attempted to encode the problem; verification via solved examples and feedback applied but did not succeed reliably.",
            "reasoning_technique": "Intended technique: encode path-existence as SMT constraints; reported difficulty in producing correct encodings (existence of path is nontrivial to express compactly in solver-friendly form).",
            "internal_representation": "Serialized edge list (N then edge pairs) as text; attempted SMT encodings of path variables and constraints mapping to solver variables.",
            "use_of_external_tool": true,
            "external_tool_description": "Z3 SMT solver was the intended backend, but solver encodings produced by LLMs were often incorrect or inadequate to represent path existence, leading to failures.",
            "evaluation_metric": "Per-instance correctness (binary); these problems were among the set that remained below 100% accuracy and were not corrected by feedback.",
            "performance": "Reported as among 6 problems where performance remained below 100% and could not be corrected through feedback; no reliable solve rate reported for this puzzle specifically.",
            "analysis_findings": "Conversions that encode path-existence (e.g., Hamiltonian path, number-link) appear peculiar and challenging even for high-capacity LLMs; authors note these conversions may be hard even for average computer-science students.",
            "ablation_comparison": "Feedback and multiple runs which helped other problems did not reliably fix encoding issues for Hamiltonian path; Puzzle-LM shows weakness on such encoding-heavy tasks.",
            "limitations": "Difficulty stems from representing existential path constraints in the SMT fragment used and from LLMs' inability to systematically produce such encodings; solver-language expressivity and complexity of required encoding are core obstacles.",
            "uuid": "e6568.6",
            "source_info": {
                "paper_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Program-aided Language models (PAL)",
            "rating": 2,
            "sanitized_title": "programaided_language_models_pal"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Satlm: Satisfiability-aided language models using declarative prompting",
            "rating": 2,
            "sanitized_title": "satlm_satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "Gao et al. (2023) Program-aided Language models (PAL)",
            "rating": 1,
            "sanitized_title": "gao_et_al_2023_programaided_language_models_pal"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.020760749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
22 Feb 2024</p>
<p>Chinmay Mittal chinmay.mittal.iitd@gmail.com 
Krishna Kartik kartikkrishna108@gmail.com 
Mausam Parag mausam@cse.iitd.ac.in 
Tom Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared D Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Joshua Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Dario Amodei 
Sam Mccandlish 
Ilya Sutskever 
Wojciech 2021 Zaremba 
Evaluat 
Peter Clark 
Oyvind Tafjord 
Kyle 2021 Richardson 
Karl Cobbe 
Vineet Kosaraju 
Jacob Hilton 
Reiichiro Nakano 
John Schulman 
Simeng Han 
Hailey Schoelkopf 
Yilun Zhao 
Zhenting Qi 
Martin Riddell 
Luke Benson 
Lucy Sun 
Eka- Terina Zubova 
Yujie Qiao 
Matthew Burtell 
David Peng 
Jonathan Fan 
Yixin Liu 
Brian Wong 
Mal- Colm Sailor 
Ansong Ni 
Linyong Nan 
Jungo Kasai 
Tao Yu 
Rui Zhang 
Shafiq R Joty 
Alexander R Fab- Bri 
Wojciech Kryscinski 
Victoria Xi 
Lin 
Albert Q Jiang 
Alexandre Sablayrolles 
Antoine Roux 
Arthur Mensch 
Blanche Savary 
Chris Bamford 
Singh Devendra 
Diego Chaplot 
Emma Bou De Las Casas 
Florian Hanna 
Gi- Anna Bressand 
Guillaume Lengyel 
Guillaume Bour 
Lam- Ple 
Renard Lélio 
Lucile Lavaud 
Marie- Anne Saulnier 
Pierre Lachaux 
Sandeep Stock 
Sophia Subramanian 
Szymon Yang 
Teven Antoniak 
Théophile Le Scao 
Thibaut Gervet 
Thomas Lavril 
Timothée Wang 
William Lacroix 
Sayed 2024 El 
Mix </p>
<p>Indian Institute of Technology
Delhi</p>
<p>Henrique Pondé de Oliveira Pinto
Jared Kaplan, Yuri BurdaHarrison Edwards</p>
<p>Nicholas Joseph</p>
<p>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
22 Feb 20249D55E7C8AB3B9A5A0A6FD57C3C11E707arXiv:2402.02611v2[cs.AI]
Recent works show that the largest of the large language models (LLMs) can solve many simple reasoning tasks expressed in natural language, without any/much supervision.But, can they also solve challenging first-order combinatorial reasoning problems, such as graph coloring, knapsack and cryptarithmetic?To answer this question, we present PuzzleBench, a dataset of 31 such challenging problems along with a few solved instances for each problem.These problems are all first order, i.e., they can be instantiated with problem instances of varying sizes, and most of them are NP-hard, requiring several reasoning steps to reach the solution.We first observe that LLMs, even when aided by symbolic solvers, perform rather poorly on our dataset.In response, we propose a new approach, Puzzle-LM, which combines LLMs with both symbolic solvers and program interpreters, along with feedback from solved examples, to achieve huge performance gains.Our extensive experimentation and analyses offer new insights into the reasoning abilities and limitations of present-day LLMs.</p>
<p>Introduction</p>
<p>Recent works have shown that large language models (LLMs) such as GPT4 (OpenAI, 2023) can reason like humans (Wei et al., 2022a), and solve diverse natural language reasoning tasks, without the need for any fine-tuning (Wei et al., 2022c;Kojima et al., 2022;Zhou et al., 2023;Zheng et al., 2023).We note that, while impressive, most of the tasks studied are simple reasoning problems, generally requiring only a handful of reasoning steps to reach a solution.</p>
<p>We are motivated by the goal of assessing the reasoning limits of modern-day LLMs.In this paper, we study the computationally intensive, firstorder combinatorial problems posed in natural language.These problems (e.g., sudoku, knapsack, graph coloring, cryptarithmetic) have long served as important testbeds to assess the intelligence of AI systems (Russell and Norvig, 2010), and strong traditional AI methods have been developed for them.Can LLMs solve these directly?If not, can they solve these with the help of symbolic AI systems like SMT solvers?</p>
<p>To answer these questions, we release a dataset named PuzzleBench, consisting of 31 such problems along with a few solved instances of each problem (see Figure 1).We refer to such problems as puzzles / fcore (first-order combinatorial reasoning) problems.Fcore problems can be instantiated with instances of varying sizes, e.g., 9×9 and 16×16 sudoku.Most of the problems in PuzzleBench are NP-hard and solving them will require extensive planning and search over a large number of combinations.Not surprisingly, our initial experiments reveal that the best of LLMs can only solve less than a third of these problems.</p>
<p>We then turn to recent approaches that augment LLMs with tools for better reasoning.Programaided Language models (PAL) (Gao et al., 2023) uses LLMs to generate programs, offloading execution to a program interpreter.Logic-LM (Pan et al., 2023) and SAT-LM (Ye et al., 2023) use LLMs to convert questions to symbolic representations, and external symbolic solvers perform the actual reasoning.Our experiments show that, by themselves, their performances are not that strong on PuzzleBench.At the same time, both these methods demonstrate complementary strengths -PAL can handle first-order structures well, whereas Logic-LM is better at complex reasoning.</p>
<p>In response, we propose a new approach named Puzzle-LM, which combines the powers of both PAL and symbolic solvers with LLMs.In particular, the LLM generates an instance-agnostic program for an fcore problem that converts any problem instance to a symbolic (in our case, SMT) representation.This program passes this representation to a symbolic SMT solver, Z3 (De Moura and Bjørner, 2008), which returns a solution back to the program.The program then converts the symbolic solution to the desired output representation, as per the natural language instruction.Interestingly, in contrast to LLMs with symbolic solvers, once this program is generated, inference on new fcore instances can be done without any LLM calls.</p>
<p>Puzzle-LM outperforms few-shot prompting by 29.98, PAL by 11.97 and Logic-LM by 23.14 percent points on PuzzleBench, with GPT-4-Turbo as the LLM.Given the structured nature of fcore problems, we also show how solved examples of small sizes can help improve the overall reasoning.Utilizing feedback from solved examples for just four rounds yields a further 20.5 percent points gain for Puzzle-LM.We perform additional analyses to understand the impact of other hyperparameters, and also describe the errors made by Puzzle-LM.We release the dataset and code for further research.</p>
<p>Related Work</p>
<p>Neuro-Symbolic AI: Our work falls in the broad category of neuro-symbolic AI (Yu et al., 2023) which builds models leveraging the complementary strengths of neural and symbolic methods.Several prior works build neuro-symbolic models for solving combinatorial reasoning problems (Palm et al., 2018;Wang et al., 2019;Paulus et al., 2021;Nandwani et al., 2022a,b).These develop specialized problem-specific modules (that are typically not size-invariant), which are trained over large training datasets.In contrast, Puzzle-LM uses LLMs and bypasses problem-specific architectures, generalizes to problems of varying sizes and is trained with very few solved instances.</p>
<p>Reasoning with Language Models: The previous paradigm to reasoning was fine-tuning of LLMs (Clark et al., 2021;Tafjord et al., 2021;Yang et al., 2022), but as LLMs scaled, they are found to reason well, when provided with in-context examples without any fine-tuning (Brown et al., 2020;Wei et al., 2022b).Since then, many prompting approaches have been developed that leverage incontext learning.Prominent ones include Chain of Thought (CoT) prompting (Wei et al., 2022c;Kojima et al., 2022), Least-To-Most prompting (Zhou et al., 2023), Progressive-Hint prompting (Zheng et al., 2023) and Tree-of-Thoughts (ToT) prompting (Yao et al., 2023).These works generally study relatively simple reasoning tasks.</p>
<p>Tool Augmented Language Models: Augmenting LLMs with external tools has emerged as a way to solve complex reasoning problems (Schick et al., 2023;Paranjape et al., 2023).The idea is to offload a part of the task to specialized external tools, thereby reducing error rates.Program-aided Language models (Gao et al., 2023) invoke a Python interpreter over a program generated by an LLM.Logic-LM (Pan et al., 2023) and SAT-LM (Ye et al., 2023) integrate reasoning of symbolic solvers with LLMs, which convert the natural language problem into a symbolic representation.Puzzle-LM falls in this category, but combines both PAL and symbolic solvers in a specific way to solve fcore problems.</p>
<p>Answer Verification: Several existing works verify the correctness of LLM's answers via an external validation, such as via symbolic solvers (Zhang and Callison-Burch, 2023), or external databases (Peng et al., 2023).In the same spirit, Puzzle-LM utilizes solved examples in training set for verifying LLM's output, and provide feedback to LLMs, in case of errors.</p>
<p>Logical Reasoning Benchmarks: There are several natural language reasoning benchmarks in literature, such as LogiQA (Liu et al., 2020) and Re-Clor (Yu et al., 2020) for mixed reasoning, GSM8K (Cobbe et al., 2021) for arithmetic reasoning, FO-LIO (Han et al., 2022) for first-order logic, PrOn-toQA (Saparov and He, 2023) and ProofWriter (Tafjord et al., 2021) for deductive reasoning, and AR-LSAT (Zhong et al., 2021) for analytical reasoning.We propose PuzzleBench, which substantially extends the complexity of these benchmarks by investigating computationally hard, first-order combinatorial reasoning problems.</p>
<p>Problem Setup</p>
<p>A first-order combinatorial reasoning problem P has three components: a space of legal input problem instances (X ), a space of legal outputs (Y), and a set of constraints (C) that every input-output pair must satisfy.Example, for sudoku, input space is the space of grids with n × n cells (n being a perfect square), with cells partially filled with numbers from 1 to n.Output space is a fully-filled grid of the same size.And, constraints comprise row, column, and box alldiff constraints, along with input cell persistence.</p>
<p>To communicate a structured problem instance (or its output) to an NLP system, it must be serialized in text.We overload X and Y to denote the formats for these serialized input and output instances.Two instances of serialized inputs/outputs for sudoku are shown in Figure 2 (grey box).We are also provided (serialized) training data of input-output instance pairs,
D P = {(x (i) , y (i) )} N i=1 , where x (i) ∈ X , y (i) ∈ Y, such that (x (i) , y (i) ) honors all constraints in C.
Further, we verbalize all three components -input/output formats and constraints -in natural language instructions.We denote these instructions by N L(X ), N L(Y), and N L(C), respectively.Figure 2 illustrates these for our running example.</p>
<p>With this notation, we summarize our setup as follows.For an abstract fcore problem P = ⟨X , Y, C⟩, we are provided N L(X ), N L(Y), N L(C) and training data D P , and our goal is to learn a function F, which maps any (serialized) x ∈ X to its corresponding (serialized) solution y ∈ Y such that (x, y) honors all constraints in C.</p>
<p>PuzzleBench: Dataset Construction</p>
<p>First, we shortlist computationally challenging problems.For that we manually scan Wikipedia for diverse puzzles and NP-hard algorithmic problems.We also find puzzles from other publishing houses (e.g., Nikoli). 1 From this set, we select problems (1) whose rules are easy to describe in words, (2) for whom it is easy to create a serialized input/output format, (3) for whom, the training and test datasets can be created with a reasonable programming effort, and (4) whose constraints have an SMT 2.0 encoding.This manual process led to identification of 31 fcore problems (see Table 5 1 https://www.nikoli.co.jp/en/puzzles/ for a complete list), of which 24 are known to be NP-hard and others have unknown complexity.10 problems are graph-based (e.g., graph coloring), 15 are grid based (e.g., sudoku), and 5 are set-based (e.g., knapsack).</p>
<p>We then create natural language instructions for each problem.We first develop input-output formats and create instructions for them (N L(X ), N L(Y)).Second, we rewrite all constraints/rules of the problem in our own words (N L(C)).This is done so that an LLM cannot easily invoke its prior knowledge about the same problem.For the same reason, we hide the name of the puzzle.Appendices A.2 and A.3 have detailed examples of rules and formats, respectively.</p>
<p>Next, we create train/test data for each problem.These instances are generated synthetically when possible or else hand-crafted by human experts.A single problem instance can potentially have multiple correct solutions (Nandwani et al., 2021) -all solutions are provided for each training input.The instances in the test set are typically larger in size than those in training.Because of their size, test instances may have too many solutions, and computing all of them can be expensive.Instead, we provide a verification script for each problem, which outputs the correctness of a candidate solution for any test instance.We construct 15 training instances per fcore problem, and 20-50 test instances.</p>
<p>5 Puzzle-LM</p>
<p>Preliminaries and Background</p>
<p>Preliminaries: In the following, we assume that we have access to an LLM L, which can work with various prompting strategies, a program interpreter I, which can execute programs written in its programming language and a symbolic solver S, which takes as input a pair of the form (E, V ), where E is set of equations (constraints) specified in the language of S, and V is a set of (free) variables in E, and produces an assignment A to the variables in V that satisfies the set of equations in E. Given the an fcore problem P = ⟨X , Y, C⟩ described by N L(C), N L(X ), N L(Y) and D P , we would like to make effective use of L, I and S, to learn the F, which takes any input x ∈ X , and maps it to y ∈ Y, such that (x, y) honors the constraints in C. Background: We consider the following possible representations for F which cover existing work.</p>
<p>• Exclusively LLM: Many prompting strategies (Wei et al., 2022c;Zhou et al., 2023;Zheng et al., 2023) make exclusive use of L (potentially multiple times) to represent F, i.e., L is supplied with a prompt consisting of the description of P in the natural language, via N L(C), N L(X ), N L(Y), the input x, along with specific instructions on how to solve the problem and asked to output y directly.This puts the entire burden of discovering the mapping F on L, ignoring S and I. • LLM → Program: In a variation of this strategy, the LLM is prompted to instead output a program which then is interpreted by I on the input x to produce the output y.This is the approach taken by methods such as PAL (Gao et al., 2023) • LLM + Solver: This strategy makes use of both the LLM L and the symbolic solver S. Whereas the primary goal of L is to to act as interface for translating the problem description for P and the input x, to the language of the solver S, and the primary burden of solving the problem is on S, whose output is then parsed as y.This is the approach taken by works such as Logic-LM (Pan et al., 2023) and Sat-LM (Ye et al., 2023).</p>
<p>Our Approach</p>
<p>Our approach can be seen as a combination of LLM→Program and LLM+Solver strategies described above.While the primary role of the LLM is to do the interfacing between the natural language description of the problem P, the task of solving the actual problem is delegated to the solver S as in LLM+Solver strategy.But unlike them, where the LLM directly calls the solver, we now prompt it to write an input agnostic program, ψ, which can work with any given input x ∈ X .This allows us to get rid of the LLM calls at inference time, resulting in a "lifted" implementation.The program ψ internally represents the specification of the problem.It takes as argument an input x, and then converts it according to the inferred specification of the problem to a set of equations (E x , V x ) in the language of the solver S to get the solution to the original problem.The solver S then ouptuts an assignment A x in its own representation, which is then passed back to the program ψ which converts it back to the desired output format specified by Y and produces output ŷ.Broadly, our pipeline consists of the 3 components which we describe next in detail.</p>
<p>• Prompting LLMs: The LLM is prompted with N L(C), N L(X ), N L(Y) to generate an inputagnostic Python program ψ.The LLM is instructed to write ψ to read an input from a file, convert it to a symbolic representation according to the inferred specification of the problem, pass the symbolic representation to the solver and then use the solution from the solver to generate the output in the desired format and write it to another file.The LLM is also prompted with information about the solver and its the underlying language in which the symbolic representation is to be generated.Optionally we can also provide the LLM with a subset of D P .• Symbolic Solver: ψ can convert any input instance x to (E x , V x ) which it passes to the symbolic solver.The solver is agnostic to how the representation (E x , V x ) was created and tries to find an assignment A x to V x which satisfies E x which is passed back to ψ. • Generating the Final Output: ψ then uses A x to generate the predicted output ŷ.This step is need because the symbolic representation was created by ψ and it must recover the desired output representation from A x which might not be straightforward for all problem representations.</p>
<p>Verification from Solved Examples</p>
<p>We make use of D P to verify and correct (if needed) ψ.For each (x, y) ∈ D P (solved input-output pair), we run ψ on x to generate the prediction ŷ, during which the following can happen: 1) Errors during execution of ψ; 2) The solver is unable to find A x under a certain time limit; 3) ŷ ̸ = y i.e. the predicted output is incorrect; 4) ŷ = y i.e. the predicted output is correct.If for any training input one of the first three cases occur we provide feedback to the LLM through prompts to improve and generate a new program.This process is repeated till all training examples are solved correctly or till a maximum number of feedback rounds is reached.</p>
<p>The feedback is simple in nature and includes the nature of the error, the actual error from the interpreter/symbolic solver and the input instance on which the error was generated.For example, in the case where the output doesn't match the gold output we prompt the LLM with the solved example it got wrong and the expected solution.In the case where the solver is not able to solve for the constraints in the time limit, we prompt the LLM to optimize the programs.Appendix B contains details of feedback prompts.</p>
<p>Experimental Setup</p>
<p>Our experiments answer these research questions: 1.How well do LLMs perform on PuzzleBench? 2. How does Puzzle-LM compare with other toolaugmented LLMs on fcore problems?3. How useful is using feedback from solved examples and multiple runs for fcore problems?</p>
<p>Baselines: We compare our method with 3 baselines: 1) Standard LLM prompting, which leverages in-context learning to directly answer the questions;</p>
<p>2) Program-aided Language Models, which use imperative programs for reasoning and offload the solution step to a program interpreter; 3) Logic-LM, which offloads the reasoning to a symbolic solver.We use Z3 (De Moura and Bjørner, 2008)   Prompting LLMs: For each method the prompt contains the natural language description of the rules of the problem and the input-output format.each testing set instance.For Logic-LM for each test set instance we prompt the LLM to convert it into its symbolic representation which is then fed to a symbolic solver, the prompt additionally contains the description of the language of the solver.We then prompt the LLM with the solution from the solver and ask it to generate the output in the desired format.Details for Puzzle-LM can be found in Section 5.1.Prompt templates can be found in Appendix B and other experimental details can be found in Appendix C.</p>
<p>Metrics:</p>
<p>For each problem we use the verification script to check if the candidate solution generated for each test set instance is correct.We use this script to compute the accuracy, i.e. fraction of test set instances solved correctly for each problem using binary marking i.e. 0 for incorrect solution and 1 for a correct solution.It is possible that no solution is generated or the solution generated is not in the output-format specified, we consider these to be incorrect and assign a score of 0. We use the macro-average of test set accuracies of all problems in PuzzleBench as our metric.</p>
<p>Results</p>
<p>Table 1 describes the main results for Puzzle-LM and other baselines on PuzzleBench.GPT-4-Turbo is significantly better than other models across all methods.Mixtral 8x7B struggles on our benchmark indicating that smaller LLMs (even with mixture of experts) are not as effective at complex reasoning tasks.Mixtral's performance with few-shot prompting is slightly better than random guessing and with other methods (without refinement) is even worse than random guessing.PAL and Puzzle-LM tend to perform better than other baselines benefiting from the vast pre-training of LLMs on code (Brown et al., 2020;Chen et al., 2021).Logic-LM performs rather poorly with smaller LLMs indicating that they struggle to invoke symbolic solvers directly.Hereafter, we focus on just GPT-4-Turbo's performance, since it is far superior to other models.</p>
<p>Puzzle-LM vs Baselines</p>
<p>Puzzle-LM outperforms few-shot prompting and Logic-LM across all problems in PuzzleBench.</p>
<p>On average with GPT-4-Turbo the improvements are by an impressive 63.05% against few-shot prompting and by 53.64% against Logic-LM on PuzzleBench (with refinement).Few-shot prompting struggles, solving less than a third of the problems even with GPT-4-Turbo indicating the need for augmenting LLMs with tools for complicated reasoning.While Logic-LM performs better, it still struggles indicating that combining LLMs with symbolic solvers is not enough for structured reasoning problems.</p>
<p>Our results indicate that few-shot prompting and Logic-LM are unable to leverage the structure of fcore problems.As problem instance size grows, Logic-LM tends to make syntactic mistakes in the generated constraints with smaller LLMs (Table 2).With larger LLMs syntax mistakes in the constraints reduce, but they still remain semantically incorrect and could not be corrected through feedback.In contrast, Puzzle-LM exploits the first order structure of these problems by generating instance-agnostic programs during training that can then be corrected through feedback from solved examples.They can then be used independently at inference to solve any input instance, highlighting the importance of using program interpreters along with symbolic solvers.refinement).This highlights the benefits of offloading the reasoning to a symbolic solver.When LLMs are forced to perform complicated reasoning while writing programs, they tend to produce brute-force solutions which are either often incorrect or extremely slow (see Table-6 in the appendix).Puzzle-LM tends to be much more efficient in terms of inference-time given that the solver can exploit the nature of the input instance while performing the reasoning unlike brute-force programs.Interestingly, feedback from solved examples and re-runs are more effective in improving performance for Puzzle-LM as indicated by the larger gains with increasing number of feedback rounds and runs.We attribute this to the fact that the programs generated by Puzzle-LM are declarative in nature, while PAL produces imperative programs which are difficult to correct.</p>
<p>Effect of Feedback on Solved Examples</p>
<p>Figure 4 describes the effect of multiple rounds of feedback for Puzzle-LM.Feedback helps performance significantly; utilizing 4 rounds of feedback improves reasoning abilities by 25.36%.Even the largest LLMs commit errors, making it important to verify and correct their work.But feedback on its own is not enough, a single run might end-up in a wrong reasoning path, which is not corrected by feedback making it important to utilize multiple runs for effective reasoning.Utilizing 5 runs improves the performance by additional 12.32% after which the gains tend to saturate.Performance also increases with an increase in the number of solved examples (</p>
<p>Effect of Problem Instance Size</p>
<p>We compare performance of Puzzle-LM and other baselines against varying problem instance sizes (see Figure 5) for 3 problems in PuzzleBench (sudoku, sujiko and magic-square).Increasing the problem instance size increases the number of variables, accompanying constraints and reasoning steps required to reach the solution.We observe that the performance of Puzzle-LM and PAL, which are programmatic approaches, is relatively robust against increase in size of input instances.</p>
<p>In comparison, performance of Logic-LM and fewshot prompting declines sharply.Performance of PAL declines in some cases because of inefficient programs generated, which are unable to find a solution in the time limit.</p>
<p>Qualitative Analysis</p>
<p>We now discuss our insights on where Puzzle-LM excels and where the largest LLMs still struggle.</p>
<p>Qualitatively, the problems in PuzzleBench can be broadly categorized into three categories: 1) Problems that our solved with 100% accuracy without any feedback from solved examples.8 such problems exist out of the 31, including vertex-cover and latin-square.These problems have a one-toone correspondence between the natural language description of the rules and the program for generating the constraints and the LLM essentially has to perform a pure translation task.</p>
<p>2) Problems that our solved with 100% accuracy but which need feedback from solved examples.</p>
<p>There are 17 such problems.They typically do not have a one-to-one correspondence between rule descriptions and code, thus requiring some reasoning to encode the problem in the solver's language.GPT-4-Turbo initially misses constraints or encodes the problem incorrectly, but with feedback, it can spot its mistakes and corrects its programs.</p>
<p>Examples include k-clique and binairo.In binairo, for example, GPT-4-Turbo incorrectly encodes the constraints for ensuring all columns and rows are distinct.In k-clique, it interprets the problem incorrectly and adds an extra constraint to ensure that if a vertex is part of a clique, then all its neighbours are also a part of the clique.</p>
<p>3) Problems with performance below 100% which are not corrected through feedback or utilizing multiple runs.For these 6 problems, LLM finds it difficult to encode some natural language constraint into SMT.Examples include number-link and hamiltonian path, where GPT-4-Turbo is not able to figure out how to encode existence of paths as SMT constraints.In our opinion, these conversions are peculiar, and may be hard even for average computer science students.</p>
<p>Conclusion and Future Work</p>
<p>We explore the reasoning abilities of LLMs on challenging first-order combinatorial reasoning problems.We present PuzzleBench, a novel benchmark of 31 such problems and find that existing tool-augmented techniques fair poorly.In response, we present Puzzle-LM a new technique to aid LLMs with both program interpreters and symbolic solvers.Our extensive experiments show Puzzle-LM's superior performance on our dataset.We also find that solved examples provide useful feedback to improve LLM reasoning.Further analyis reveals that Puzzle-LM struggles for a certain class of problems where conversion to symbolic representation is not straightforward.In such cases simple feedback strategies do not improve reasoning; exploring methods to alleviate such problems is a promising direction for future work.We release our code and data for further research.</p>
<p>Dataset contamination is a big problem in the era of pretrained language models which have been trained on large web corpora.Therefore, its really hard to determine if a dataset has been seen as part of the training data.Determining the extent of contamination is also not easy.Since PuzzleBench does contain some standard problems from Computer Science literature, there is possibility of contamination.Usage of LLMs for reasoning on text is also limited by availability of large compute resources.</p>
<p>Our approach is also limited by the nature of the symbolic solver used.Some problems cannot be encoded in the symbolic language used by the solver, although our approach remains flexible enough to include several such solvers.Even in the case that a problem can be encoded in the symbolic language used by the solver, this encoding can potentially be complicated.Our method relies on the in-context learning ability of LLMs to achieve this.Additionally we assume that input instances and their outputs have a fixed pre-defined (serialized) representation.</p>
<p>11 Ethics Statement (Strubell et al., 2019) show that the financial and environmental costs that are associated with the training of LLMs lead to significant contribution to global warming.We don't train the model from scratch and leverage in context learning, so the energy footprint of our work is less.The large language models whose API we use for inference, especially GPT-4, consume significant energy.</p>
<p>A PuzzleBench</p>
<p>A.1 Dataset Details and Statistics</p>
<p>Our dataset namely PuzzleBench has 31 different fcore problems that have been collected from various sources.Some of these problems have been collected from Nikoli and other problems are wellknown computational problems from Computer Science literature such as hamiltonian path and minimum-dominating set.</p>
<p>A.2 Natural Language Description of Rules</p>
<p>This section describes how we create the natural language description of rules for problems in PuzzleBench.We extract rules from Wikipedia/Nikoli pages of the corresponding problems.These rules are reworded by a human expert to reduce dataset contamination.Another human expert ensures that there are no ambiguities in the reworded description of the rules.The rules are generalized, when needed (for eg.from a 9 × 9 Sudoku to a n × n Sudoku).The following sections provide few examples.Survo (Figure 6) is an example problem from PuzzleBench.The task is to fill a m × n rectangular board with numbers from 1 − m * n such that each row and column sums to an intended target.(Survo-Wikipedia).The box given below describes the rules of Survo more formally in natural language.</p>
<p>A.2.1 Example Problem: Survo</p>
<p>We are given a partially filled m × n rectangular board, intended row sums and column sums.</p>
<p>-Empty cells are to be filled with numbers -Numbers in the solved board can range from 1 to m * n -Numbers present in filled cells on the input board cannot be removed -Each number from 1 to m*n must appear exactly once on the solved board -All the empty cells should be filled such that each row and each column of the solved board must sum to the respective row sum and column sum as specified in the input Hamiltonian path is a well-known problem in graph theory in which we have to find a path in an un-directed and an un-weighted graph such that each vertex is visited exactly once by the path.We consider the decision variant of this problem which is equally hard in terms of computational complexity.The box below shows the formal rules for this problem expressed in natural language.</p>
<p>A.2.2 Example Problem: Hamiltonian Path</p>
<p>We are given an un-directed and un-weighted graph.</p>
<p>-We have to determine if the graph contains a path that visits every vertex exactly once.Dosun Fuwari (Nikoli) as shown in Figure 8 is another example problem from PuzzleBench.We are given a square board with regions (cells enclosed in bold lines) and we have to fill the board with balloons and iron balls such that one balloon and one iron ball is placed in each region.Balloons are light and float, so they must be placed in one of the cells at the top, in a cell right under a black cell (filled-in cell), or under other balloons.Iron balls are heavy and sink, so they must be placed in one of the cells at the bottom, or in a cell right over a black cell or over other iron balls.The box given below gives the more formal description of the rules of dosun fuwari in natural language.</p>
<p>A.2.3 Example Problem: Dosun Fuwari</p>
<p>We are given a partially filled n*n square board.We are also given subgrids of the input board.Cells in the input board can either be empty or filled (that is, nothing can be placed in them, they are blackened) or can be balloons or iron balls.</p>
<p>-The only thing we can do is place balloons or iron balls in some of or all of the empty cells -Each subgrid specified in the input should have exactly one balloon and iron ball in the solved board -Because balloons are buoyant, they should be positioned either in one of the cells located at the top of the board or in a cell directly below a filled cell (i.e., one of the blackened cells in the input) or below other balloons.</p>
<p>-Iron balls, being dense, will sink and should therefore be positioned either directly on one of the cells located at the bottom of the input board, or on a cell directly above a filled cell (i.e., one of the blackened cells in the input), or above another iron ball.</p>
<p>A.3 Natural Language Description of Input and Output format</p>
<p>For many problems we consider input-output instances are typically not represented in text.For each problem we describe a straightforward conversion of the input and output space to text in natural language.The following sections consider examples of a few problems from PuzzleBench.Figure 9 represents the conversion of the inputs to survo, originally represented as grid images to text.Here empty cells are denoted by 0's and the filled cells have corresponding values.For a given m × n board, each row has m + 1 space separated integers with the first m integers representing the first row of the input board and the (m+1) th integer representing the row sum.The last row contains n integers represent the column sums.The box below describes this conversion more formally in natural language.</p>
<p>A.3.1 Example Problem: Survo</p>
<p>Input Format:</p>
<p>-The input will have m + 1 lines -The first m lines will have n + 1 space-separated integers -Each of these m lines represents one row of the partially solved input board (n integers), followed by the required row sum (a single integer) -The last line of the input will have n space-separated integers each of which represents the required column sum in the solved board Sample Input: 0 6 0 0 0 30 8 1 0 0 0 17 0 9 3 0 30 27 16 10 25  Figure 10 represents conversion of the inputs to dosun fuwari, originally represented as grid images to text.Here the first few lines represent the input board followed by a string '--' which acts as a separator following which each of the lines has space-separated integers representing the subgrids of the input board.Cells are numbered in rowmajor order starting from 0, and this numbering is used to represent cells in each of the lines describing the subgrids.In the first few lines representing the input board, 0's represent the empty cells that must be filled.1's denote the blackened cell, 2s denote the balloons and 3's denote the iron balls.The box below describes these rules more formally in natural language</p>
<p>Input-Format: -The first few lines represent the input board, followed by a line containing ---, which acts as a separator, followed by several lines where each line represents one subgrid -Each of the lines representing the input board will have space-separated integers ranging from 0 to 3 -0 denotes empty cells, 1 denotes a filled cell (blackened cell), 2 denotes a cell with a balloon, 3 denotes a cell with an iron ball -After the board, there is a separator line containing ----Each of the following lines has space-separated elements representing the subgrids on the input board -Each of these lines has integers representing cells of a subgrid -Cells are numbered in row-major order starting from 0, and this numbering is used to represent cells in each of the lines describing the subgrids Sample-Input: 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 ---0 1 2 3 6 10 4 8 12 5 9 13 14 15 7 11 Output Format: -The output should contain as many lines as the size of the input board, each representing one row of the solved board -Each row should have n space separate integers (ranging from 0-3) where n is the size of the input board -Empty cells will be denoted by 0s, filled cells (blackened) by 1s, balloons by 2s and iron balls by 3s Sample-Output: 2 3 0 2 2 1 0 2 0 2 3 3 3 0 3 1</p>
<p>A.3.3 Example Problem: Hamiltonian Path</p>
<p>Figure 11 represents the conversion of inputs to hamiltonian-path, originally represented as graph image to text.The first line denotes the number of vertices present in the graph followed by which each node of the graph will be numbered from 0 -N- 1.Each of the subsequent lines represents an edge of the graph and will contain two space-separated integers (according to the numbering defined previously).The output is a single word (YES/NO) indicating if a hamiltonian path exists in the graph.The box below describes this more formally in natural language.</p>
<p>Input Format: -The first line will contain a single integer N, the number of nodes in the graph -The nodes of the graph will be numbered from 0 to N-1 -Each of the subsequent lines will represent an edge of the graph and will contain two space-separated integers (according to the numbering defined above) Sample-Input: 5 0 1 1 2 2 3 3 4</p>
<p>Output Format: -The output should contain a single line with a single word -The word should be YES if a path exists in the input graph according to constraints specified above and NO otherwise</p>
<p>B Prompt Templates</p>
<p>In this section we provide prompt templates used for our experiments on PuzzleBench, including the templates for the baselines we experimented with, Puzzle-LM as well as prompt templates for providing feedback.</p>
<p>B</p>
<p>B.4 Feedback Prompt Templates</p>
<p>These prompt templates are used to provide feedback in the case of Puzzle-LM or PAL.</p>
<p>B.4.1 Programming Errors</p>
<p>Your code is incorrect and produces the following runtime error:<RUN TIME ERROR> for the following input: <INPUT> rewrite your code and fix the mistake</p>
<p>B.4.2 Verification Error</p>
<p>Your code is incorrect, when run on the input: <INPUT> the output produced is <OUTPUT-GENERATED> which is incorrect whereas one of the correct output is <GOLD-OUTPUT>.</p>
<p>Rewrite your code and fix the mistake.</p>
<p>B.4.3 Timeout Error</p>
<p>Your code was inefficient and took more than <TIME-LIMIT> seconds to execute for the following input: <INPUT>.Rewrite the code and optimize it.</p>
<p>B.5 Logic-LM Prompt Template</p>
<p>The following box describes the prompt for Logic-LM experiments with PuzzleBench, the prompt is used to convert the input to its symbolic representation.The task is to declare variables and the corresponding constraints on them in SMT2 for the input mentioned above.</p>
<p>The variables and constraints should be such that once the variables are solved for, one can use the solution to the variables (which satisfies the constraints) to get to the output in the desired format for the above mentioned input.</p>
<p>Only Write the SMT2 code and nothing else.Write the complete set of SMT2 variables and constraints.Enclose SMT2 code in "'smt2 "'</p>
<p>C Experimental Details</p>
<p>C.1 PuzzleBench</p>
<p>All methods are evaluated zero-shot, meaning no in-context demonstrations for the task are provided to the LLM.We choose the zero-shot setting for PuzzleBench because of the structured nature of problems, making it unfair to provide demonstrations of highly related problems instances to the LLM.The LLM is only given a description of the rules of the problem and the task it has to perform.For PAL and Puzzle-LM we present results with 10 solved examples for feedback.</p>
<p>C.2 Hardware Details</p>
<p>All experiments were conducted on an Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz, 32 cores, 64-bit, with 512 KiB L1 cache, 16 MiB L2 cache, and 22 MiB L3 cache.We accessed GPT-4-Turbo and GPT-3.5-Turbo by invoking both models via the OpenAI API.Mixtral 8x7B was also accessed by using the Mistral AI API although the model weights are available publicly.We preferred the API, over running the model locally given the ease of setup because all our other experiments were with APIs.</p>
<p>D Additional Results</p>
<p>D.1 Inference Time</p>
<p>The following tables describes the average inference time for test set instances of a few illustrative problems in PuzzleBench.Puzzle-LM performs # Constraints for each cell to be between 1 and n cells_c = [ And (1 &lt;= X
[ i ][ j] , X [i ][ j ] &lt;= n ) for i in range (n) for j in range (n)]</p>
<h1>Constraints for rows and columns to be distinct   # Add constraint that at most K vertices can be selected solver .add ( Sum ([ If (v ,1 ,0)</h1>
<p>rows_c = [ Distinct ( X[ i ]) for i in range ( n) ] cols_c = [ Distinct ([ X [i ][ j ] for i in range (n) ]) for j in range (n)]
Figure 1 :
1
Figure 1: Illustrative Examples of problems in PuzzleBench.</p>
<p>Figure 2 :
2
Figure 2: PuzzleBench Example: Filling a n × n Sudoku board along with its rules, input-output format and a couple of sample input-output pairs.</p>
<p>It is possible that a single run of Puzzle-LM does not generate the correct solution for all training examples.Puzzle-LM is run multiple times for a given problem.Given the probabilistic nature of LLMs a new program is generated each time and a new feedback process continues.For the final program, we pick the best program generated during these runs, as judged by the accuracy on the training set. Figure 3 describes our entire approach diagrammatically.</p>
<p>Figure 3 :
3
Figure 3: Puzzle-LM: Solid lines indicate the main flow and dotted lines indicate feedback pathways.</p>
<p>Models:</p>
<p>For each method, we experiment with 3 LLMs: GPT-4-Turbo (gpt-4-0125-preview) (OpenAI, 2023) which is a SOTA LLM by Ope-nAI, GPT-3.5-Turbo(gpt-3.5-turbo-0125), a relatively smaller LLM by OpenAI and Mixtral 8x7B (mistral-small) (Jiang et al., 2024), an open-source mixture-of-experts model developed by Mistral AI.For experiments with few-shot prompting and Logic-LM, we set the temperature to 0 for reproducible results.For experiments with PAL and Puzzle-LM we use a temperature of 0.7 to sample several runs, each with a potentially different feedback cycle.</p>
<p>Figure 4: Dashed lines show results for individual problems in PuzzleBench, with coloured lines highlighting specific cases and grey lines others.The red bold represents the average effect across all problems with GPT-4-Turbo.</p>
<p>Figure 5 :
5
Figure 5: Effect of increasing problem instance size on baselines and Puzzle-LM for GPT-4-Turbo.</p>
<p>Figure 6 :
6
Figure 6: Conversion of an input survo problem instance to its solution.</p>
<p>Figure 7 :
7
Figure 7: A sample input graph instance and its solution to the hamiltonian-path problem.Vertices are represented by yellow circles and the hamiltonian path is represented by the red line.</p>
<p>Figure 8 :
8
Figure 8: Conversion of an input dosun fuwari problem instance to its solution.</p>
<p>Figure 9 :
9
Figure 9: Representation of input instances of survo as text.</p>
<p>Output Format: -The output should have m lines, each representing one row of the solved board -Each of these m lines should have n space-separated integers representing the cells of the solved board -Each integer should be from 1 to m * n</p>
<p>Figure 10 :
10
Figure 10: Representation of inputs instances to dosunfuwari as text.</p>
<p>Figure 11 :
11
Figure 11: Representation of input instances to hamiltonian-path as text.</p>
<h1></h1>
<p>Constraints for subgrids to be distinct subgrids_c = [ Distinct ([ X[ i ][ j] for i in range ( k<em> sqrt_n , (k +1) * sqrt_n ) for j in range ( l</em> sqrt_n , (l +1) * sqrt_n ) ]) for k in range ( sqrt_n ) for l in range ( sqrt_n )] # Constraints for the given cells to match the input given_c = [X [i ][ j ] == board [i ][ j ] for i in range (n) for j in range (n) if board [i ][ j] != 0]</p>
<p>Figure 13 :Figure 14 :
1314
Figure 13: Puzzle-LM example: correct program for keisuke generated by GPT-4-Turbo.</p>
<p>Figure 15 :
15
Figure 15: Puzzle-LM example: correct program for vertex-cover generated by GPT-4-Turbo.</p>
<p>Figure 16 :
16
Figure 16: Puzzle-LM example: snippet of incorrect program for binairo generated by GPT-4-Turbo and same snippet after correction by feedback.</p>
<p>Table 1 :
1
Main Results for PuzzleBench and Puzzle-LM.-and + indicate results before and after refinement.
Random Guessing Few-Shot PromptingPALLogic-LMPuzzle-LMModel-+-+-+Mixtral 8x7B23.6%16.1% 37.92%0.0%1.61% 10.77% 36.72%GPT-3.5-Turbo17.91%25.4%35.82% 53.4%6.87%7.41% 30.74% 56.86%GPT-4-Turbo30.5%48.51% 66.45% 37.34% 39.91% 60.48% 93.55%
It also contains two solved examples -pairs of input and one its corresponding outputs -in the prompt.No additional intermediate supervision (e.g., SMT or Python program) is given in the prompt.For few-shot prompting we directly prompt the LLM to solve test set instances (each tested separately).For PAL we prompt the LLM to write an input-agnostic Python program which reads the input from a file, reasons to solve the input and then writes the solution to another file, the program generated is run on</p>
<p>Table 2 :
2
Logic-LM's performance on PuzzleBench evaluated with feedback to fix syntactic mistakes.</p>
<p>Table 3
3
compares the effect of feedback and multiple runs on PAL and Puzzle-LM.Puzzle-LM outperforms PAL by 27.1% on PuzzleBench (with</p>
<p>Table 4 )
4
. Each solved example helps in detecting and correcting different errors.However, performance saturates at 7 solved examples and no new errors are discovered, even with additional training data.↑ 17.17 % ↑ 19.99 % ↑ 21.4 % ↑ 21.18 % (a) Effect of number of rounds of feedback for a single run ↑ 24.3 % ↑ 25.95 % ↑ 26.85 % ↑ 27.1 % (b) Effect of number of runs each having 4 rounds of feedback
Number of Rounds of Feedback01234PAL48.51%54.52%57.83% 58.98% 59.80%Puzzle-LM 60.48%71.69%77.37% 80.38% 80.98%↑ 11.97 % Number of Runs12345PAL59.80% 62.45% 63.93%65.23% 66.45&amp;Puzzle-LM 80.98% 86.75% 89.88%92.08% 93.55%↑ 21.18 %</p>
<p>Table 3 :
3
Comparative analysis between PAL andPuzzle-LM on PuzzleBench for GPT-4-Turbo
Number of Feedback Rounds01234N = 060.48% 60.48% 60.48% 60.48% 60.48%N = 160.48% 69.47% 73.99% 75.73% 75.75%N = 460.48% 71.43% 76.87% 79.52% 79.54%N = 760.48% 71.69% 77.37% 80.38% 80.98%N = 1060.48% 71.69% 77.37% 80.38% 80.98%</p>
<p>Table 4 :
4
Effect of number of solved examples (N)for feedback and number of feedback rounds for Puzzle-LM with GPT-4-Turbo on PuzzleBench for a single run.</p>
<p>Table 5
5gives the de-tails of all problems in our dataset. To create ourtraining and test sets, we synthetically generateproblem instances when possible and we handcraftproblem instances otherwise. Each problem hassome solved training instances and a separate set oftesting instances. Each problem also has a naturallanguage description of its rules, and a natural lan-guage description of the input-format which spec-ify how input problem instances and their solutionsare represented in text. The next few sections giveillustrative examples and other details.</p>
<p>Table 5 :
5
Names of problems in the PuzzleBench, number of samples in the training set, number of samples in the test set, average size of input instances in training set, average size of input instances in test set and computational complexity.The brackets in the 4th column describe how input instance sizes are measured.? in the computational complexity column indicates that results are not available for the corresponding problem.</p>
<p>.1 Few-Shot Prompt Template 2&gt; ............................ ............................
B.3 Puzzle-LM TemplateB.3.1 Base PromptWrite a Python program to solve the followingproblem:Task:<Description of the Rules of the problem>Input-Format:<Description of Textual Representation of Inputs>Sample-Input:<Sample Input from Feedback Set>Task:Output-Format:<Description of the Rules of the problems><Description of Textual Representation of Outputs>Sample-Output:Input-Format:<Output of Sample Input from Feedback Set><Description of Textual Representation of Inputs><Input Few Shot Example-1>The Python program must read the input from<Input Few Shot Example-2>input.txt and convert that particular input to the........................corresponding constraints, which it should pass........................to the Z3 solver, and then it should use the Z3<Input Few Shot Example-n>solver's output to write the solution to a filenamed output.txtOutput-Format<Description of Textual Representation of Outputs>Don't write anything apart from the Python program;use Python comments if needed.<Output of Few Shot Example-1>&lt;Output of Few Shot Example-<Output of Few Shot Example-n>Input problem instance to be solved:<Puzzle from the Test Set>B.2 PAL Prompt TemplateThe following box describes the base prompttemplate used for PAL experiments withPuzzleBench.Write a Python program to solve the followingproblem:Task:<Description of the Rules of the problem>Input-Format:<Description of Textual Representation of Inputs>Sample-Input:<Sample Input from Feedback Set>Output-Format:<Description of Textual Representation of Outputs>Sample-Output:<Output of Sample Input from Feedback Set>Don't write anything apart from the Python program;use Python comments if needed.The Python program is expected to read the inputfrom input.txt and write the output to a filenamed output.txt.The Python program must only use standard Pythonlibraries.</p>
<p>Table 6 :
6
Average inference time in seconds ofPuzzle-LM and PAL for test set instances for selected problems in PuzzleBench much better compared to PAL because PAL programs often tend to be brute force and inefficient whereas the solver can exploit the nature of the input-instance while performing the reasoning with Puzzle-LM.This section includes example programs generated by Puzzle-LM for some illustrative problems in PuzzleBench.Each program reads the input from a file, generates the corresponding constraints, calls the solver internally and then uses the solution from the solver to write the output in the desired format to a file.
ProblemPAL Puzzle-LMSudoku2.010.215Latin Square5.460.2Cryptarithmetic0.830.73Independent Set1.4380.106Minimum Dominating0.980.112SetSujiko0.7420.102Vertex Cover1.580.105</p>
<p>for v in vertices ]) &lt;= k) # For each edge , add constraint that at least one of the vertices should be selected for u , v in edges : solver .add(Or(vertices[u ], vertices[ v ]) )</p>
<h1>Check if the problem is solvable with the given constraintsif solver . check () == sat :return " YES "else :return " NO "# Function to write the output to a filedef write_output ( filename , result ):with open ( filename , 'w ') as file :file . write ( result )# Main functiondef main () :n , k , edges = read_input ( ' input . txt ')result = is_possible (n , k , edges )write_output ( ' output . txt ', result )if <strong>name</strong> == " <strong>main</strong> " :main ()</h1>
<p>AcknowledgementsMausam is supported by grants from Google, Huawei, and Jai Gupta Chair Professorship from IIT Delhi.Parag was supported by the DARPA Explainable Artificial Intelligence (XAI) Program (#N66001-17-2-4032).Mausam and Parag are also supported by IBM SUR awards.Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of the funding agencies.We also acknowledge the Microsoft Accelerate Foundation Models Research Program for GPT4 access.with open (" output .txt " , "w ") as f: if solution is None : f .write (" None ") else :f .write(" ". join ( map ( str , solution )))# Function to solve the problem with Z3 def solve_with_z3 ( array , target_sum ): solver = Solver () # Create a boolean variable for each element in the array to indicate if it is included in the sum include_vars = [ Bool (f " include_ { i} ") for i in range ( len ( array ))]# Add constraint that the sum of the selected elements should be equal to the target sum solver .add( Sum ([ If ( include_vars [ i], array [ i], 0) for i in range ( len ( array )) ]) == target_sum )# Try to find a solution if solver .check () == sat : model = solver .model () # Extract the solution : which elements to include solution = [ array[i ]for i in range ( len ( array )) if is_true( model [ include_vars [i ]]) ] return solution else :return None # Main function to read the input , solve the problem , and write the output def main () : with open ( " input .txt " , "r ") as f: lines = f .read () .strip () .split (" \n ") array = list ( map ( int , linesE.2 PALThis section includes example programs generated by PAL for some illustrative problems in PuzzleBench.Each program reads the input from a file, performs the reasoning and writes the output to another text file.F Logic-LMThis section describes example runs of Logic-LM for certain problems in PuzzleBench.""" Input Instance """ ## 0 4 3 2 ## 2 0 1 0 ## 0 0 0 0 ## 0 0 2 3 ; Initialize known values ( assert (= a12 4) ) ( assert (= a13 3) ) ( assert (= a14 2) ) ( assert (= a21 2) ) ( assert (= a23 1) ) ( assert (= a43 2) ) ( assert (= a44 3) ); Each cell must be between 1 and 4 ( assert ( and ( &gt;= a11 1) ( &lt;= a11 4) ) ) ( assert ( and ( &gt;= a22 1) ( &lt;= a22 4) ) ) ( assert ( and ( &gt;= a24 1) ( &lt;= a24 4) ) ) ( assert ( and ( &gt;= a31 1) ( &lt;= a31 4) ) ) ( assert ( and ( &gt;= a32 1) ( &lt;= a32 4) ) ) ( assert ( and ( &gt;= a33 1) ( &lt;= a33 4) ) ) ( assert ( and ( &gt;= a34 1) ( &lt;= a34 4) ) ) ( assert ( and ( &gt;= a41 1) ( &lt;= a41 4) ) ) ( assert ( and ( &gt;= a42 1) ( &lt;= a42 4) ) ); Each number must appear exactly once in each row ( assert ( distinct a11 a12 a13 a14 ) ) ( assert ( distinct a21 a22 a23 a24 ) ) ( assert ( distinct a31 a32 a33 a34 ) ) ( assert ( distinct a41 a42 a43 a44 ) ); Each number must appear exactly once in each column ( assert ( distinct a11 a21 a31 a41 ) ) ( assert ( distinct a12 a22 a32 a42 ) ) ( assert ( distinct a13 a23 a33 a43 ) ) ( assert ( distinct a14 a24 a34 a44 ) ); Each number must appear exactly once in each 2 x2 subgrid ( assert ( distinct a11 a12 a21 a22 ) ) ( assert ( distinct a13 a14 a23 a24 ) ) ( assert ( distinct a31 a32 a41 a42 ) ) ( assert ( distinct a33 a34 a43 a44 ) ); Check and get the model ( check -sat ) ( get -model )Figure 23: Logic-LM example: correct constraints for a sudoku instance generated by GPT-4-Turbo.""" Input Instance """ # 1 2 3 4 5 6 7 8 9 10 # 55( declare -const a1 Bool ) ( declare -const a2 Bool ) ( declare -const a3 Bool ) ( declare -const a4 Bool ) ( declare -const a5 Bool ) ( declare -const a6 Bool ) ( declare -const a7 Bool ) ( declare -const a8 Bool ) ( declare -const a9 Bool ) ( declare -const a10 Bool )( assert (= (+ ( ite a1 1 0) ( ite a2 2 0) ( ite a3 3 0) ( ite a4 4 0) ( ite a5 5 0) ( ite a6 6 0) ( ite a7 7 0) ( ite a8 8 0) ( ite a9 9 0) ( ite a10 10 0) ) 55) )( check -sat ) ( get -model )Figure 24: Logic-LM example: correct constraints for a subset sum instance generated by GPT-4-Turbo.; Constraint : Each colour variable must be in the range [0 , K -1] , where K =3 for this problem ( assert ( and ( &gt;= c0 0) ( &lt; c0 3) )) ( assert ( and ( &gt;= c1 0) ( &lt; c1 3) )) ( assert ( and ( &gt;= c2 0) ( &lt; c2 3) )) ( assert ( and ( &gt;= c3 0) ( &lt; c3 3) )) ( assert ( and ( &gt;= c4 0) ( &lt; c4 3) )) ( assert ( and ( &gt;= c5 0) ( &lt; c5 3) )) ( assert ( and ( &gt;= c6 0) ( &lt; c6 3) )) ( assert ( and ( &gt;= c7 0) ( &lt; c7 3) )); Constraint : Adjacent vertices must have different colours ( assert ( not; Check if there is a solution ( check -sat )Figure 25: Logic-LM example: correct constraints for graph coloring instance generated by GPT-4-Turbo.""" Input Instance """ # 0 1 3 0 # 4 3 2 1 # 0 4 1 0 # 1 0 0 0 ( declare -const a11 Int ( assert ( and ( &gt;= a21 1) ( &lt;= a21 4) ) ) ( assert ( and ( &gt;= a22 1) ( &lt;= a22 4) ) ) ( assert ( and ( &gt;= a23 1) ( &lt;= a23 4) ) ) ( assert ( and ( &gt;= a24 1) ( &lt;= a24 4) ) )( assert ( and ( &gt;= a31 1) ( &lt;= a31 4) ) ) ( assert ( and ( &gt;= a32 1) ( &lt;= a32 4) ) ) ( assert ( and ( &gt;= a33 1) ( &lt;= a33 4) ) ) ( assert ( and ( &gt;= a34 1) ( &lt;= a34 4) ) )( assert ( and ( &gt;= a41 1) ( &lt;= a41 4) ) ) ( assert ( and ( &gt;= a42 1) ( &lt;= a42 4) ) ) # ## SYNTAX ERROR ( assert ( &gt;= a43 1) ( &lt;= a43 4) )) # ## SYNTAX ERROR ( assert ( and ( &gt;= a44 1) ( &lt;= a44 4) ) )( assert ( distinct a11 a12 a13 a14 ) ) ( assert ( distinct a21 a22 a23 a24 ) ) ( assert ( distinct a31 a32 a33 a34 ) ) ( assert ( distinct a41 a42 a43 a44 ) )( assert ( distinct a11 a21 a31 a41 ) ) ( assert ( distinct a12 a22 a32 a42 ) ) ( assert ( distinct a13 a23 a33 a43 ) ) ( assert ( distinct a14 a24 a34 a44 ) )( assert ( distinct a11 a12 a21 a22 ) ) ( assert ( distinct a13 a14 a23 a24 ) ) ( assert ( distinct a31 a32 a41 a42 ) ) ( assert ( distinct a33 a34 a43 a44 ) )( assert ( distinct a11 a12 a13 a14 ) ) ( assert ( distinct a21 a22 a23 a24 ) ) ( assert ( distinct a31 a32 a33 a34 ) ) ( assert ( distinct a41 a42 a43 a44 ) )( check -sat ) ( get -model )
J-pop: Japanese puzzles as optimization problems. Huw Lloyd, Matthew Crossley, Mark Sinclair, 10.1109/TG.2021.3081817IEEE Transactions on Games. 143Martyn Amos. 2022</p>
<p>Neural models for output-space invariance in combinatorial problems. Yatin Nandwani, Vidit Jain, Mausam , Parag Singla, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022a. April 25-29, 2022OpenReview.net</p>
<p>Rishabh Ranjan, Mausam, and Parag Singla. 2022b. A solver-free framework for scalable learning in neural ILP architectures. Yatin Nandwani, Deepanshu Jindal, Mausam , Parag Singla, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022. Austria; NeurIPS; New Orleans, LA, USA2021. May 3-7, 2021. 2022. November 28 -December 9, 20229th International Conference on Learning Representations, ICLR 2021, Virtual Event</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Recurrent relational networks. Rasmus Berg Palm, Ulrich Paquet, Ole Winther, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. NeurIPS; Montréal, Canada2018. 2018. 2018. December 3-8, 2018</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>ART: automatic multistep reasoning and tool-use for large language models. Bhargavi Paranjape, Scott M Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Túlio, Ribeiro , 10.48550/ARXIV.2303.09014CoRR, abs/2303.090142023</p>
<p>Comboptnet: Fit the right np-hard problem by learning integer programming constraints. Anselm Paulus, Michal Rolínek, Vít Musil, Brandon Amos, Georg Martius, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. the 38th International Conference on Machine Learning, ICML 2021PMLR2021. 18-24 July 2021139of Proceedings of Machine Learning Research</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, 10.48550/ARXIV.2302.12813CoRR, abs/2302.128132023</p>
<p>Stuart Russell, Peter Norvig, Artificial Intelligence: A Modern Approach. Prentice Hall20103 edition</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 10.48550/ARXIV.2302.04761CoRR, abs/2302.047612023</p>
<p>Energy and policy considerations for deep learning in NLP. Emma Strubell, Ananya Ganesh, Andrew Mccallum, 10.18653/V1/P19-1355Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics2019. July 28-August 2, 20191</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/V1/2021.FINDINGS-ACL.317Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Association for Computational Linguistics2021. August 1-6, 2021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. Po-Wei Wang, Priya Donti, Bryan Wilder, Zico Kolter, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningPMLR201997</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 2022a. 2022</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 2022b. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022c35</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, 10.18653/v1/2022.emnlp-main.7Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 10.48550/ARXIV.2305.10601CoRR, abs/2305.106012023</p>
<p>Complexity and completeness of finding another solution and its application to puzzles. Yato Takayuki, Seta Takahiro, IEICE Transactions on Fundamentals of Electronics. E862003Communications and Computer Sciences</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett ; Dongran, Bo Yu, Dayou Yang, Hui Liu, Shirui Wang, Pan, 10.1016/J.NEUNET.2023.06.028Proceedings of NeurIPS. NeurIPS2023. 2023166A survey on neural-symbolic learning systems</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020</p>
<p>Language models are drummers: Drum composition with natural language pre-training. Li Zhang, Chris Callison-Burch, 10.48550/ARXIV.2301.01162CoRR, abs/2301.011622023</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, 10.48550/ARXIV.2304.09797CoRR, abs/2304.097972023</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, CoRR, abs/2104.06598AR-LSAT: investigating analytical reasoning of text. 2021</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>            </div>
        </div>

    </div>
</body>
</html>
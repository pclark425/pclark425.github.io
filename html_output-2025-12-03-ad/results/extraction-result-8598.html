<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8598 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8598</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8598</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270258104</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.02787v1.pdf" target="_blank">Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities</a></p>
                <p><strong>Paper Abstract:</strong> This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains. We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant. We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM's reasoning ability in real-world scenarios, disentangled from contextual support in practical settings? (2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa? To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning. In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia. Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential. The code and dataset are available at: https://github.com/agiresearch/ContextHub.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8598.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8598.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-1.5 series</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of pre-trained LLMs (Qwen-1.5 series) evaluated at multiple parameter scales in this study for propositional deductive and abductive reasoning, and used in fine-tuning experiments with QLora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Series of Qwen transformer language models (multiple sizes) used for zero/few-shot benchmarking and finetuning experiments in this paper; no further training-data details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, 110B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ContextHub (propositional deductive and abductive reasoning - abstract and contextualized instantiations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks of propositional deductive and abductive logic generated with DyVal tree-based templates at 4 difficulty levels and instantiated across 12 domains (11 contextual domains + abstract). Tasks require deriving truth values (True/False/N/A) from premises or abducing premises from observations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation across abstract vs contextualized instantiations; fine-tuning experiments using QLora on (1) purely abstract data, (2) sampled contextualized data (sampled-ctx), and (3) single-domain contextualized data; metric: average (weighted) F1 across truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: larger Qwen models show markedly better performance on abstract logic samples compared to instantiated variants; smaller Qwen models (0.5B, 1.8B) often perform worse on abstract data and sometimes better on contextualized instantiations. No single absolute numeric reported for all sizes; fine-tuning on sampled contextualized data improved general performance versus abstract-only fine-tuning. Qwen-14 showed an instance where fine-tuning on contextualized level-4 data performed slightly worse than abstract fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared across scales (0.5B → 110B) and across fine-tuning regimes (abstract-only vs sampled-ctx vs single-domain). Smaller Qwen models lag behind larger Qwen models on abstract tasks; fine-tuning on contextualized data yields stronger cross-type generalization than abstract-only fine-tuning for many sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles on highest difficulty (level 4) contextualized tasks even after contextualized fine-tuning; abstract-only fine-tuning fails to generalize well to contextualized text; smaller sizes fail at abstract reasoning and often rely on contextual cues.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Scaling improves abstract reasoning ability; contextualized (instantiated) data is more effective than purely abstract data for generalization under fine-tuning; however extreme complexity (level 4) remains difficult and may not be resolved by contextualized instantiation alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8598.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8598.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 series</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA-2 family of models (multiple parameter scales) evaluated on the ContextHub propositional reasoning benchmark showing scale-dependent performance differences between abstract and contextualized instantiations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation LLM family (LLaMA-2) evaluated at 7B, 13B, and 70B parameter scales in this study; used for benchmarking abstract vs contextualized propositional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ContextHub (propositional deductive and abductive reasoning - abstract and instantiated)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Propositional deductive and abductive logic questions produced by DyVal templates and contextualized across multiple domains; tasks require deduction (from premises to certain conclusions) and abduction (from observations to plausible premises).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero/few-shot benchmarking across abstract and contextualized instantiations; comparisons across model scales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: smaller LLaMA-2 (7B) performed significantly worse on abstract reasoning samples; larger model (70B) performs substantially better, consistent with scaling trends reported in the paper. Exact numeric F1 values not reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Scale ablation within LLaMA-2 (7B vs 13B vs 70B) shows improved abstract reasoning with scale; similar domain-dependent performance variance as other model families.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Smaller LLaMA-2 models struggle on abstract tasks; domain/context influences performance (math and philosophy harder); no detailed failure-mode breakdown given beyond aggregate observations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Model scale correlates with improved ability to solve abstract propositional logic problems; contextualization can reduce or shift difficulty depending on domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8598.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8598.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-1.5 series</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Yi-1.5 family of transformer LLMs evaluated at several scales on ContextHub; showed mixed performance with domain-specific weaknesses (notably math).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Series of Yi-1.5 models evaluated at 6B, 9B, and 34B parameter sizes for propositional reasoning benchmarking in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 9B, 34B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ContextHub (propositional deductive and abductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same DyVal-derived propositional deductive and abductive tasks, instantiated across domains and including abstract variable-only instances; tasks measure ability to deduce truth values or abduce premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Benchmarking across abstract vs instantiated samples; statistical tests (chi-square) to evaluate domain effects; comparison across sizes within Yi series.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Yi-series models and some Qwen large models achieve relatively higher weighted F1 scores in several categories; Yi models show consistent lower performance on Math domain tasks. Yi-34 shows notably higher weighted F1 on abstract category in some results.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared across Yi sizes (6B→34B) and against other families (Qwen, LLaMA); Yi-34 and larger Qwen models often outperform smaller ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Domain-specific poor performance (Math consistently low); abductive tasks generally harder than deductive ones across models.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Higher-capacity Yi models improve abstract reasoning but still exhibit domain sensitivity; abductive reasoning is more challenging than deduction for Yi series.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8598.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8598.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-3.5-turbo evaluated in fine-tuning experiments showing strong gains when fine-tuned on contextualized instantiations; achieved near-perfect weighted F1 on easier contextualized levels after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 variant used here for fine-tuning on ContextHub data (sampled contextualized and single-domain) with QLora hyperparameters; exact training corpus not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ContextHub (propositional deductive and abductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Propositional logic tasks (deduction and abduction) instantiated across multiple domains; evaluation metric is average (weighted) F1 across truth labels per dataset/difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning using QLora on (a) sampled contextualized data (1280 points sampled across domains), and (b) single-domain contextualized datasets (1280 points each) to study generalization; evaluation on held-out abstract and all-contextualized data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Quantitative summary reported: after fine-tuning on sampled-ctx, GPT-3.5 achieved near-perfect weighted F1 on contextualized data for difficulty levels 1 and 2 (paper text: 'near-perfect weighted F1 results for contextualized data in level 1 and 2 in GPT-3.5'). On harder levels the performance drops; fine-tuning on sampled-ctx also improved abstract performance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared fine-tuning on abstract-only vs sampled-ctx vs single-domain; GPT-3.5 fine-tuned on sampled-ctx outperformed abstract-only fine-tuning on contextualized evaluation; single-domain fine-tuning sometimes matched or exceeded sampled-ctx performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance degrades for higher difficulty levels (level 4 remains largely unsatisfactory); abstract-only fine-tuning does not generalize well to contextualized instances; some overlap between fine-tune and evaluation data (sampled-ctx) exists but is small (≈1/11).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Contextualized instantiated training data yields superior generalization to both contextualized and abstract evaluation than abstract-only data for GPT-3.5; domain diversity is not strictly required (single-domain fine-tuning can be competitive).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8598.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8598.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (referred)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o is reported in this paper's results as frequently excelling across difficulty levels on ContextHub tasks, particularly on higher-difficulty samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A high-capacity OpenAI model referenced in benchmark results as achieving strong performance on the ContextHub propositional logic tasks; no training or size details provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ContextHub (propositional deductive and abductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same DyVal-derived propositional tasks across 4 difficulty levels and multiple contextual domains.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Included in benchmarking comparisons (appears in aggregated heatmaps and analyses); compared qualitatively against other families and sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: GPT-4o 'frequently appears to excel, particularly in higher difficulty levels' according to the paper; higher weighted F1 relative to many other evaluated models. Exact numeric scores are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Observed to outperform many smaller and mid-sized models across difficulties; reported alongside Yi-series and top Qwen sizes as high-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No detailed failure cases or numeric breakdowns for GPT-4o presented in the paper; domain-specific weaknesses (e.g., Math) are noted for model families generally but not explicitly dissected for GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>High-capacity instruction-tuned models (GPT-4o here) are better at abstract and high-difficulty propositional logic tasks; however context/domain still modulates performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8598.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8598.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3-Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Advanced LLM used in this study for contextual instantiation of DyVal templates and as an automated verifier during quality control of the instantiated dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3-Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Claude-3 Opus (Anthropic) used to instantiate formal logic templates into domain-specific natural language and to perform automated quality-control checks (common-sense, sensibility, tautology checks) on generated samples; not evaluated as a benchmarked reasoning model here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ContextHub (dataset instantiation and verification)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to transform DyVal abstract propositional templates into coherent contextualized natural language instances across Wikipedia-derived domains and to validate those instances against constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompting Claude-3-Opus to perform two-step contextualization: (1) variable-based transformation (instantiate each propositional variable into a sentence in a chosen sub-category); (2) template-based transformation (combine instantiated variable sentences into a coherent problem). Also used for automated checks prior to human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not evaluated on the reasoning benchmark for model performance; reported as a tool used to generate and verify dataset samples. The authors report subsequent human review rates indicating high template adherence and fact reckoning for the instantiated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not applicable (tool for dataset construction and automated QC). Human expert review (5 annotators) used as the additional verification baseline; final correctness rates reported in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Potential dataset-generation bias as the dataset is synthetically generated by Claude-3 and then human-verified; the paper describes mitigation via human verification but does not quantify any residual bias.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Using a strong LLM (Claude-3-Opus) to create contextualized logical instances is effective when combined with human expert verification; however, synthetic generation may still require careful auditing to ensure problems require logical inference rather than common-knowledge shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dyval: Graph-informed dynamic evaluation of large language models <em>(Rating: 2)</em></li>
                <li>On the paradox of generalizable logical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>PrOntoQA: A synthetic question-answering dataset to evaluate multi-step reasoning <em>(Rating: 2)</em></li>
                <li>Large language models still can't plan (a benchmark for llms on planning and reasoning about change) <em>(Rating: 1)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8598",
    "paper_id": "paper-270258104",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Qwen-1.5",
            "name_full": "Qwen-1.5 series",
            "brief_description": "A family of pre-trained LLMs (Qwen-1.5 series) evaluated at multiple parameter scales in this study for propositional deductive and abductive reasoning, and used in fine-tuning experiments with QLora.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-1.5",
            "model_description": "Series of Qwen transformer language models (multiple sizes) used for zero/few-shot benchmarking and finetuning experiments in this paper; no further training-data details provided here.",
            "model_size": "0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, 110B",
            "reasoning_task_name": "ContextHub (propositional deductive and abductive reasoning - abstract and contextualized instantiations)",
            "reasoning_task_description": "Benchmarks of propositional deductive and abductive logic generated with DyVal tree-based templates at 4 difficulty levels and instantiated across 12 domains (11 contextual domains + abstract). Tasks require deriving truth values (True/False/N/A) from premises or abducing premises from observations.",
            "method_or_approach": "Evaluation across abstract vs contextualized instantiations; fine-tuning experiments using QLora on (1) purely abstract data, (2) sampled contextualized data (sampled-ctx), and (3) single-domain contextualized data; metric: average (weighted) F1 across truth labels.",
            "performance": "Qualitative: larger Qwen models show markedly better performance on abstract logic samples compared to instantiated variants; smaller Qwen models (0.5B, 1.8B) often perform worse on abstract data and sometimes better on contextualized instantiations. No single absolute numeric reported for all sizes; fine-tuning on sampled contextualized data improved general performance versus abstract-only fine-tuning. Qwen-14 showed an instance where fine-tuning on contextualized level-4 data performed slightly worse than abstract fine-tuning.",
            "baseline_comparison": "Compared across scales (0.5B → 110B) and across fine-tuning regimes (abstract-only vs sampled-ctx vs single-domain). Smaller Qwen models lag behind larger Qwen models on abstract tasks; fine-tuning on contextualized data yields stronger cross-type generalization than abstract-only fine-tuning for many sizes.",
            "limitations_or_failures": "Struggles on highest difficulty (level 4) contextualized tasks even after contextualized fine-tuning; abstract-only fine-tuning fails to generalize well to contextualized text; smaller sizes fail at abstract reasoning and often rely on contextual cues.",
            "insights_or_conclusions": "Scaling improves abstract reasoning ability; contextualized (instantiated) data is more effective than purely abstract data for generalization under fine-tuning; however extreme complexity (level 4) remains difficult and may not be resolved by contextualized instantiation alone.",
            "uuid": "e8598.0",
            "source_info": {
                "paper_title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA-2",
            "name_full": "LLaMA-2 series",
            "brief_description": "LLaMA-2 family of models (multiple parameter scales) evaluated on the ContextHub propositional reasoning benchmark showing scale-dependent performance differences between abstract and contextualized instantiations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_description": "Open foundation LLM family (LLaMA-2) evaluated at 7B, 13B, and 70B parameter scales in this study; used for benchmarking abstract vs contextualized propositional reasoning.",
            "model_size": "7B, 13B, 70B",
            "reasoning_task_name": "ContextHub (propositional deductive and abductive reasoning - abstract and instantiated)",
            "reasoning_task_description": "Propositional deductive and abductive logic questions produced by DyVal templates and contextualized across multiple domains; tasks require deduction (from premises to certain conclusions) and abduction (from observations to plausible premises).",
            "method_or_approach": "Zero/few-shot benchmarking across abstract and contextualized instantiations; comparisons across model scales.",
            "performance": "Qualitative: smaller LLaMA-2 (7B) performed significantly worse on abstract reasoning samples; larger model (70B) performs substantially better, consistent with scaling trends reported in the paper. Exact numeric F1 values not reported in main text.",
            "baseline_comparison": "Scale ablation within LLaMA-2 (7B vs 13B vs 70B) shows improved abstract reasoning with scale; similar domain-dependent performance variance as other model families.",
            "limitations_or_failures": "Smaller LLaMA-2 models struggle on abstract tasks; domain/context influences performance (math and philosophy harder); no detailed failure-mode breakdown given beyond aggregate observations.",
            "insights_or_conclusions": "Model scale correlates with improved ability to solve abstract propositional logic problems; contextualization can reduce or shift difficulty depending on domain.",
            "uuid": "e8598.1",
            "source_info": {
                "paper_title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Yi-1.5",
            "name_full": "Yi-1.5 series",
            "brief_description": "Yi-1.5 family of transformer LLMs evaluated at several scales on ContextHub; showed mixed performance with domain-specific weaknesses (notably math).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi-1.5",
            "model_description": "Series of Yi-1.5 models evaluated at 6B, 9B, and 34B parameter sizes for propositional reasoning benchmarking in this paper.",
            "model_size": "6B, 9B, 34B",
            "reasoning_task_name": "ContextHub (propositional deductive and abductive reasoning)",
            "reasoning_task_description": "Same DyVal-derived propositional deductive and abductive tasks, instantiated across domains and including abstract variable-only instances; tasks measure ability to deduce truth values or abduce premises.",
            "method_or_approach": "Benchmarking across abstract vs instantiated samples; statistical tests (chi-square) to evaluate domain effects; comparison across sizes within Yi series.",
            "performance": "Qualitative: Yi-series models and some Qwen large models achieve relatively higher weighted F1 scores in several categories; Yi models show consistent lower performance on Math domain tasks. Yi-34 shows notably higher weighted F1 on abstract category in some results.",
            "baseline_comparison": "Compared across Yi sizes (6B→34B) and against other families (Qwen, LLaMA); Yi-34 and larger Qwen models often outperform smaller ones.",
            "limitations_or_failures": "Domain-specific poor performance (Math consistently low); abductive tasks generally harder than deductive ones across models.",
            "insights_or_conclusions": "Higher-capacity Yi models improve abstract reasoning but still exhibit domain sensitivity; abductive reasoning is more challenging than deduction for Yi series.",
            "uuid": "e8598.2",
            "source_info": {
                "paper_title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "OpenAI GPT-3.5-turbo evaluated in fine-tuning experiments showing strong gains when fine-tuned on contextualized instantiations; achieved near-perfect weighted F1 on easier contextualized levels after fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-tuned GPT-3.5 variant used here for fine-tuning on ContextHub data (sampled contextualized and single-domain) with QLora hyperparameters; exact training corpus not specified in this paper.",
            "model_size": null,
            "reasoning_task_name": "ContextHub (propositional deductive and abductive reasoning)",
            "reasoning_task_description": "Propositional logic tasks (deduction and abduction) instantiated across multiple domains; evaluation metric is average (weighted) F1 across truth labels per dataset/difficulty.",
            "method_or_approach": "Fine-tuning using QLora on (a) sampled contextualized data (1280 points sampled across domains), and (b) single-domain contextualized datasets (1280 points each) to study generalization; evaluation on held-out abstract and all-contextualized data.",
            "performance": "Quantitative summary reported: after fine-tuning on sampled-ctx, GPT-3.5 achieved near-perfect weighted F1 on contextualized data for difficulty levels 1 and 2 (paper text: 'near-perfect weighted F1 results for contextualized data in level 1 and 2 in GPT-3.5'). On harder levels the performance drops; fine-tuning on sampled-ctx also improved abstract performance.",
            "baseline_comparison": "Compared fine-tuning on abstract-only vs sampled-ctx vs single-domain; GPT-3.5 fine-tuned on sampled-ctx outperformed abstract-only fine-tuning on contextualized evaluation; single-domain fine-tuning sometimes matched or exceeded sampled-ctx performance.",
            "limitations_or_failures": "Performance degrades for higher difficulty levels (level 4 remains largely unsatisfactory); abstract-only fine-tuning does not generalize well to contextualized instances; some overlap between fine-tune and evaluation data (sampled-ctx) exists but is small (≈1/11).",
            "insights_or_conclusions": "Contextualized instantiated training data yields superior generalization to both contextualized and abstract evaluation than abstract-only data for GPT-3.5; domain diversity is not strictly required (single-domain fine-tuning can be competitive).",
            "uuid": "e8598.3",
            "source_info": {
                "paper_title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (referred)",
            "brief_description": "GPT-4o is reported in this paper's results as frequently excelling across difficulty levels on ContextHub tasks, particularly on higher-difficulty samples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "A high-capacity OpenAI model referenced in benchmark results as achieving strong performance on the ContextHub propositional logic tasks; no training or size details provided in the paper.",
            "model_size": null,
            "reasoning_task_name": "ContextHub (propositional deductive and abductive reasoning)",
            "reasoning_task_description": "Same DyVal-derived propositional tasks across 4 difficulty levels and multiple contextual domains.",
            "method_or_approach": "Included in benchmarking comparisons (appears in aggregated heatmaps and analyses); compared qualitatively against other families and sizes.",
            "performance": "Qualitative: GPT-4o 'frequently appears to excel, particularly in higher difficulty levels' according to the paper; higher weighted F1 relative to many other evaluated models. Exact numeric scores are not provided in the text.",
            "baseline_comparison": "Observed to outperform many smaller and mid-sized models across difficulties; reported alongside Yi-series and top Qwen sizes as high-performing.",
            "limitations_or_failures": "No detailed failure cases or numeric breakdowns for GPT-4o presented in the paper; domain-specific weaknesses (e.g., Math) are noted for model families generally but not explicitly dissected for GPT-4o.",
            "insights_or_conclusions": "High-capacity instruction-tuned models (GPT-4o here) are better at abstract and high-difficulty propositional logic tasks; however context/domain still modulates performance.",
            "uuid": "e8598.4",
            "source_info": {
                "paper_title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Claude-3-Opus",
            "name_full": "Claude 3 Opus",
            "brief_description": "Advanced LLM used in this study for contextual instantiation of DyVal templates and as an automated verifier during quality control of the instantiated dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3-Opus",
            "model_description": "Claude-3 Opus (Anthropic) used to instantiate formal logic templates into domain-specific natural language and to perform automated quality-control checks (common-sense, sensibility, tautology checks) on generated samples; not evaluated as a benchmarked reasoning model here.",
            "model_size": null,
            "reasoning_task_name": "ContextHub (dataset instantiation and verification)",
            "reasoning_task_description": "Used to transform DyVal abstract propositional templates into coherent contextualized natural language instances across Wikipedia-derived domains and to validate those instances against constraints.",
            "method_or_approach": "Prompting Claude-3-Opus to perform two-step contextualization: (1) variable-based transformation (instantiate each propositional variable into a sentence in a chosen sub-category); (2) template-based transformation (combine instantiated variable sentences into a coherent problem). Also used for automated checks prior to human verification.",
            "performance": "Not evaluated on the reasoning benchmark for model performance; reported as a tool used to generate and verify dataset samples. The authors report subsequent human review rates indicating high template adherence and fact reckoning for the instantiated examples.",
            "baseline_comparison": "Not applicable (tool for dataset construction and automated QC). Human expert review (5 annotators) used as the additional verification baseline; final correctness rates reported in Table 3.",
            "limitations_or_failures": "Potential dataset-generation bias as the dataset is synthetically generated by Claude-3 and then human-verified; the paper describes mitigation via human verification but does not quantify any residual bias.",
            "insights_or_conclusions": "Using a strong LLM (Claude-3-Opus) to create contextualized logical instances is effective when combined with human expert verification; however, synthetic generation may still require careful auditing to ensure problems require logical inference rather than common-knowledge shortcuts.",
            "uuid": "e8598.5",
            "source_info": {
                "paper_title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dyval: Graph-informed dynamic evaluation of large language models",
            "rating": 2,
            "sanitized_title": "dyval_graphinformed_dynamic_evaluation_of_large_language_models"
        },
        {
            "paper_title": "On the paradox of generalizable logical reasoning in large language models",
            "rating": 2,
            "sanitized_title": "on_the_paradox_of_generalizable_logical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "PrOntoQA: A synthetic question-answering dataset to evaluate multi-step reasoning",
            "rating": 2,
            "sanitized_title": "prontoqa_a_synthetic_questionanswering_dataset_to_evaluate_multistep_reasoning"
        },
        {
            "paper_title": "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
            "rating": 1,
            "sanitized_title": "large_language_models_still_cant_plan_a_benchmark_for_llms_on_planning_and_reasoning_about_change"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 1,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.014547499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities
4 Jun 2024</p>
<p>Wenyue Hua wenyue.hua@rutgers.edu 
Kaijie Zhu kaijiezhu11@gmail.com 
Microsoft Lingyao Li 
Lizhou Fan 
Shuhang Lin 
Mingyu Jin 
Haochen Xue 
Zelong Li 
Jindong Wang 
Microsoft Yongfeng Zhang </p>
<p>Rutgers University</p>
<p>University of Michigan</p>
<p>University of Michigan</p>
<p>Rutgers University</p>
<p>Rutgers University</p>
<p>The University of Liverpool</p>
<p>Rutgers University</p>
<p>Rutgers University</p>
<p>Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities
4 Jun 2024B43D162D9C1E015DFE75D0469234AE48arXiv:2406.02787v1[cs.CL]
This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains.We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant.We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM's reasoning ability in real-world scenarios, disentangled from contextual support in practical settings?(2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa?To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning.In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia.Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential.The code and dataset are available at: https://github.com/agiresearch/ContextHub.</p>
<p>Introduction</p>
<p>Large language models (LLMs) [1,2,3] have demonstrated significant potential in reasoning capabilities across a variety of reasoning benchmarks [4,5,6,7,8,9,10,11], broadening their potential applications in fields such as psychology, education, and social sciences [12,13,14].The widespread use of LLMs accentuates the necessity of rigorously evaluating their reasoning abilities, particularly in context-rich scenarios that mirror real-world complexities.</p>
<p>While assessments on abstract logical problems [15,9] showcase LLMs' theoretical reasoning capacities, they do not entirely capture their practical utility in real-life applications where context drastically affects outcomes.Conversely, focusing exclusively on context-specific tasks may conceal the fundamental mechanisms that empower LLMs to process and reason with information.Thus, exploring the balance between contextualized and abstract reasoning is vital for responsibly advancing LLM technology and ensuring its effectiveness across various domains.</p>
<p>To this end, we introduce ContextHub -a pioneering benchmark designed to meticulously disentangle and evaluate the core reasoning capabilities of LLMs from the influences of contextual information.By leveraging a dual-assessment framework, ContextHub compares LLMs' performance on identical logical constructs within both abstract and richly contextualized settings.This approach not only highlights the differential impacts of context on reasoning but also provides a scalable and flexible methodology that can be adapted across various domains and LLM architectures.Our approach aims to address two main questions:</p>
<ol>
<li>
<p>Evaluation disentanglement: how accurate and robust is it to evaluate LLMs' reasoning abilities using abstract logic problems or various contextualized logic problems?By comparing the performance of LLMs on abstract and contextualized logical problems, we can gain a better understanding of the role of context in LLMs' reasoning abilities.</p>
</li>
<li>
<p>Fine-tuning disentanglement: how does model generalization differ when fine-tuning LLMs using abstract logic problems or contextualized logic problems?By comparing the performance of LLMs on unseen abstract and contextualized logic problems, we can gain insights into the types of data that are most effective for improving LLMs' reasoning abilities while maintaining consistent performance across different domains.</p>
</li>
</ol>
<p>We first utilize DyVal [9] to generate 4 different difficulty levels of formal logic templates.Then, we use advanced LLMs to contextualize these logical templates.Our key finds are:</p>
<p>1.The relative performance of LLMs on abstract logic and corresponding instantiated logic is dependent on model size or general model performance.Stronger models tend to perform better on abstract logic, while smaller models typically rely on contextual cues.</p>
<ol>
<li>
<p>The domain of contextualization has a statistically significant impact on model performance.This suggests that the choice of contextualization domain can affect the accuracy and reliability of LLMs for logical reasoning tasks.</p>
</li>
<li>
<p>The generalization power of abstract logic data is limited compared with that of instantiated logic data.This indicates that LLMs fine-tuned on instantiated logic data may be better equipped to handle a wider range of logical reasoning tasks, including those that involve real-world scenarios and contextual cues.</p>
</li>
</ol>
<p>To sum up, this paper makes the following contributions:</p>
<ol>
<li>
<p>Flexible instantiation evaluation framework.ContextHub enables researchers to easily create contextualized logical datasets for their own studies, allowing for more comprehensive and diverse evaluations of LLMs' reasoning abilities.</p>
</li>
<li>
<p>Extensive benchmarking and analysis.Empirical evidence demonstrates that abstract logic assessments do not fully reflect an LLMs' reasoning capabilities and different instantiations impose significant effect on model performance.</p>
</li>
<li>
<p>Investigation into data generalization potential.Extensive experiments show that abstract logic has limited generalization power while contextually instantiated show better generalization ability.</p>
</li>
</ol>
<p>Related Work</p>
<p>Efforts for evaluating LLMs reasoning abilities have been intensified significantly across numerous disciplines, including biomedical informatics [16,17,18], humanities [19,20,21], and social sciences [22,12,13,14].Numerous studies have instantiated the problem of logical reasoning based on reasoning-dependent datasets, such as deduction, induction or abduction, and studies solving the tasks with neural models [23,24,25].Despite the promising performance that LLMs have shown on certain reasoning tasks and those techniques that can help improve LLMs' reasoning abilities, it remains unclear whether LLMs have generalizable logical reasoning abilities and to what extent they are capable of logical reasoning [26].In this regard, Valmeekam et al. [27] stated that LLMs were still fell short of delivering satisfactory performance in common planning and reasoning tasks that are typically straightforward for humans to perform.This limitation was also highlighted by Wei et al. [28] that although CoT can stimulate the thought processes of human reasoners, it does not answer whether the neural network is actually reasoning.Other studies have also highlighted the limitations of modern LLMs in performing logical reasoning tasks.For example, Tang et al. [26] found that LLaMA2, relied on template matching to respond to reasoning queries but failed to generalize to novel logic rules, as demonstrated through experiments on Symbolic Trees and ProofWriter.This led them to question whether modern LLMs have truly mastered inductive, deductive, and abductive reasoning abilities akin to human intelligence.In a recent benchmark study, Saparov and He [29] presented a synthetic question-answering dataset called PrOntoQA to explore the logical reasoning ability of LLMs.Their analysis revealed that while LLMs were capable of making correct individual deduction steps, they encountered difficulties in exploring the solution space when presented with multiple valid deduction steps.</p>
<p>Based on our review of current studies, the prevalent focus on question answering and mathematical problems in current benchmarks may not sufficiently capture the essence of reasoning -the ability to logically process and deduce information beyond memorized knowledge or instantiated cues.In particular, two specific questions regarding the logical reasoning abilities of LLMs remain unclear.First, it is unclear whether LLMs genuinely comprehend logical reasoning patterns within the text and generate answers through reasoning, or if they merely follow the textual instructions and answer based on prior knowledge.Second, it is uncertain whether current reasoning benchmarks accurately assess the true reasoning abilities exhibited by LLMs in practical scenarios.To investigate these questions, we focus on propositional logic, specifically deductive and abductive logic reasoning in this study.</p>
<p>Benchmark Construction</p>
<p>In this paper, we focus on two types of propositional logical reasoning tasks: deductive logic and abductive logic.Deductive logic is a method of reasoning from one or more general statements (premises) to reach a logically certain conclusion, where abductive logic involves starting with an observation and then abduce the truth value of premises.</p>
<p>As illustrated in Figure 1, constructing instantiated logical reasoning benchmarks involves three steps:</p>
<ol>
<li>Creating Formal Logical Reasoning Question Templates.We first construct abstract deductive and abductive logic questions spanning 4 difficulty levels based on DyVal [9].DyVal utilize graph structure to dynamically construct logic questions.We call these questions formal logic template for further contextualization.</li>
</ol>
<p>2.</p>
<p>Instantiation.Each logical template T will then be instantiated in 12 different domains.In each domain, we randomly select one of its sub-categories to diversify the context.We then ask LLMs to instantiate the logical template T using the context in each sub-category.</p>
<ol>
<li>Quality Control.To ensure the correctness of the instantiated evaluation samples, we implement a two-step quality control process.Initially, the samples are assessed by an advanced LLM, Claude 3 Opus, which validates the samples against specified constraints.Subsequently, for each level of difficulty, 220 randomly selected samples undergo a further review by 5 human experts.</li>
</ol>
<p>Creating Formal Logical Reasoning Question Templates</p>
<p>We generate formal logic templates X is based on the dynamic evaluation framework : DyVal [9].For deductive and abductive logic, DyVal utilize tree structure to to generate template samples on the fly with controllable difficulty.The tree structure naturally align with the inference process of a logic reasoning question.Take deductive logic as an example, the premises are given by the leaf nodes, where the intermediate nodes represents the intermediate inference steps, the final result is shown by the root node.</p>
<p>Tree-based DyVal consists of three components:</p>
<ol>
<li>
<p>Constraint C. It aims to modulate the evaluation samples' complexity and validity.In our experiment, we define the complexity level of a logical question by its depth of the generated tree.The validity constraints ensure the correctness of the generated logic question, for example, constrain the 'NOT' operation to have only one children node;</p>
</li>
<li>
<p>Tree generation algorithm G.After defining the constraints, the generation algorithm G generates fixed complexity evaluation samples following the constraint C, during the generation process, the final answer is also be calculated;</p>
</li>
<li>
<p>Description function F. It translate each node in the graph into natural languages and finally form all nodes to a logical reasoning question.For example, in deductive logic, for a leaf node 'A' with truth value 'True', it will be translated as "A is True.", for a non-leaf node 'C' with 'OR' operation and its children 'A' and 'B', it will be translated as "(A OR B) -C", where '-' means deductive operation.</p>
</li>
</ol>
<p>In our experiments, we defined 4 levels of difficulty with depth equals to (2,3,4,5), respectively.The width are set to be 2.For level 1 difficulty, there can only be 10 different deductive questions and 6 abductive questions.For other levels, we generate 40 deductive logic templates and 40 abductive logic templates.To balance data distribution, we generate the same number of datapoints assigned with the truth values of True, False, and N/A respectively.</p>
<p>Contextual Instantiation</p>
<p>Domains of contextualization We instantiate the above formal logic templates in the below contextual domains following the categorization of Wikipedia [30]:</p>
<p>1 Culture and the arts , Geography and places , Health and fitness , Human activities , Mathematics and logic , Natural and physical science , People and self , Philosophy and thinking , Religion and belief systems , Society and social sciences , Technology and applied sciences .</p>
<p>Listing 1: Categories of Wikipedia</p>
<p>The domain of "History and events" is removed because instantiated sentences are often about known fact and the question can be answer without going through the reasoning process, thus nullify the reasoning problem.</p>
<p>Each instantiation of a domain is created based on a randomly selected sub-categories in the domain from above based on sub-categories established in Wikipedia to encourage diversity and specification.For example, "Culture and the arts" has the following sub-categories: Listing 2: Sub-categories of Culture and the arts in Wikipedia Contextualization process After obtaining the formal logic templates, for each domain, we first randomly selected one sub-category c, then we ask LLMs (in our experiment, Claude-3-Opus) to instantiate each variable in the original logic templates with the relevant context in the selected sub-category.This contextualizatin process is divided into 2 steps:
1. Variable-based Transformation: T v . For each variable V contained in the logic template X , a instantiated sentence s V is generated by T v (c, V ∈ X ).
For example, a leaf node variable V can be instantiated as "Alice studied hard for the following math test" in the sub-category of "Mathematics Education" in the category of "Mathematics and Logic".2. Template-based Transformation: T t .After generating {s V } for all {V} in the template X , a coherent natural language description will be generated by T t ({s V }, X ) by forming a instantiated version of the original formal logic template.</p>
<p>Abstract Instantiation</p>
<p>Other than the 11 contextual domains from Wikipedia, we also create an "abstract" domain where we simply substitute by heuristic rules the propositional variables in the formal logic template with arbitrary character sequences of varying lengths, ranging from 3 to 5. The purpose of creating this domain is to augment the number of datapoints expressed in an abstract form, thereby enabling a fair comparison with other contextualized domains in terms of sample size.Furthermore, by employing multiple instantiations, we can mitigate the impact of any potential outliers and obtain a more reliable and generalizable estimate of the performance of abstract data as we have only 256 formal logic templates in total.</p>
<p>Below is an example of instantiation in Table 1 of a formal logic template of difficulty level 1, where propositional variables are represented by strings such as "aaa", "aab", and "aac".</p>
<p>1 ( aaa or aab ) -aac .Given aac is False , what is the value of aab ?</p>
<p>We provide an abstract instance and a contextualized instance on the domain of "Geography and Places", where we provide the instantiations of each proposition in the template and the final combined logic reasoning task based on propositional instantiations.More examples can be found in Appendix 7.1.</p>
<p>Data Statistics</p>
<p>Consequently, for each formal logic template, there are a total of 12 domains used for contextualization, including the abstract domain.For each domain, we generate 5 distinct instantiations.Thus, for level 1 abductive logic, we have in total 360 datapoints; for level 2 deductive logic, we have in total 600 datapoints.For other levels of abductive and deductive logic, we have 2,880 datapoints.Thus the whole benchmark contains 18,240 datapoints.</p>
<p>Quality Control</p>
<p>The quality verification of our instantiated benchmarks is managed using a hybrid model involving Claude-3 Opus, and a diverse panel of 5 human annotators.These verification steps are implemented reasoning task: If an area of land has experienced significant uplift or been shaped by powerful erosional forces, then the terrain will feature tall, steep mountains.Given that the area does not have tall, steep mountains, can it be determined if powerful erosional forces have shaped the land?Table 1: Example of abstract and instantiated logic reasoning task based on the original formal logic template.</p>
<p>to maintain a high standard of quality and relevance in our benchmarks, ensuring that they not only test logical reasoning but also engage with the domain knowledge in a meaningful way.Notice that since the "abstract" instantiations are created simply by rule-based heuristics, no further verification is needed and thus quality control is only conducted in contextually instantiated reasoning questions.</p>
<p>LLMs verification Ensuring the validity of our benchmarks involves three primary checks, which are critical to establishing the reliability of the logic problems within the instantiated settings.These checks are designed to assess the problems for fundamental logical soundness and adherence to rational thinking patterns:</p>
<p>• Common Sense Checking: This verification step assesses whether the contextually instantiated logic problem relies on universally recognized knowledge, such as common sense facts that do not require logical reasoning to solve.The purpose of this check is to ensure that the questions demand genuine logical inference rather than mere recognition of widely known facts.• Sensibility Checking: Each logic problem is scrutinized for its sensibility and coherence.This process ensures that the scenarios and the associated questions are coherently constructed and present a clear, understandable challenge to models without any internal contradictions or ambiguous phrasing.• Tautology Checking: This check is crucial for identifying any statements within the logic problems that are inherently tautological.A tautology in this context would be a condition or a statement that is true in every possible interpretation, thus rendering the problem trivial or meaningless.This step helps in maintaining the intellectual rigor and challenge expected from a benchmark in logical reasoning.</p>
<p>Human verification In addition to automated checks, human verification plays a pivotal role in ensuring the quality and applicability of our instantiated benchmarks.For detailed demographics of these trained annotators, refer to Appendix 7.3, which includes their degrees, gender, age, and other relevant information.The human verification focuses on two critical aspects:</p>
<ol>
<li>Template Adherence: The first aspect of human verification assesses whether the instantiated logic accurately follows the structure and intent of the original formal logic template.This check is essential to ensure that the fundamental logical framework remains intact and that the contextual adaptations do not alter the core logical challenges intended by the original template.Annotators evaluate the fidelity of the instantiated problems to their original versions, confirming that the essential logical elements are preserved.2. Fact Reckoning: The second aspect evaluates whether the contextually instantiated template accounts for commonly known facts, which could potentially simplify the logical reasoning required to answer the question.This step is crucial to ensure that the problems require genuine logical deduction rather than mere recall or memorization.It seeks to eliminate any instances where the answer to a problem might be directly inferred from general knowledge or trivial facts, thereby maintaining the complexity and educational value of the benchmark.</li>
</ol>
<p>The dual-layer verification process ensures that our benchmarks not only test logical reasoning skills but also do so in a manner that is both challenging and fair.</p>
<p>Experimental Design</p>
<p>The experiments in this study are divided into two parts: benchmarking and fine-tuning.The benchmarking part aims to evaluate model performance across different domains and also compare the performance of contextualized logic to that of abstract logic, examining whether LLM truly understands the underlying logic structure regardless of the contexts or instantiations.The data points generated for instantiated logic can also be used for fine-tuning models, which is the focus of the fine-tuning part.This part explores various aspects of fine-tuning using synthetic data, including the use of abstract logic instantiations or contextualized logic instantiations, model scaling on the effect of generalization, and the domains of contextualized data.</p>
<p>We benchmark with three distinct series of well-trained models: Qwen-1.5 [31], LLaMA-2 [32], and Yi-1.5 [33].The selection of these models is primarily based on their varying sizes, which enables us to conduct a comprehensive analysis of the impact of model size and scaling on the performance of LLMs.The Qwen-1.5 series includes the following models: Qwen1.5-0.5b,Qwen1.5-1.8b,Qwen1.5-4b,Qwen1.5-7b,Qwen1.5-14b,Qwen1.5-32b,Qwen1.5-72b, and Qwen1.5-110b.The LLaMA-2 series includes the following models: LLaMA2-7b, LLaMA2-13b, and LLaMA2-70b.The Yi-1.5 series includes the following models: Yi1.5-6b, Yi1.5-9b, and Yi1.5-34b.</p>
<p>In the fine-tuning stage of our study, we employ four models: Qwen1.5-0.5b,Qwen1.5-7b,Qwen1.5-14b, and GPT-3.5-turbo.Our goal is to investigate the factors that impact model generalization for logic reasoning, and to this end, we explore several fine-tuning settings: (1) fine-tune the models solely on abstract data, (2) fine-tune the models on sampled contextualized data sampled from all domains, and (3) fine-tune the models on contextualized data from single-domains.Results from ( 1) and ( 2) allow us to investigate the generalization ability of abstract data and to compare it to the generalization ability of contextualized data; while results from (3) allow us to investigate the impact of domain specificity and domain diversity on generalization.</p>
<p>Evaluation Metrics for Benchmarking To assess the reasoning capabilities of LLMs, we employ the average F1 score.The calculation of the average F1 score involves determining the average of the F1 scores for data points (d) that possess the same truth values.For datapoints with identical ground truth (gt) truth value T , the F1 score is computed by first ascertaining the true positive (T T p ), false positive(F T p ), and false negative(F T n ):
T T p = {d ∈ D|f (d) = gt(d), gt(d) = T }(1)F T p = {d ∈ D|f (d) ̸ = gt(d), f (d) = T }(2)F T n = {d ∈ D|f (d) ̸ = gt(d), gt(d) = T }(3)
F1 T for for datapoints with the truth value T is then computed by:
F 1 T = 2T T p 2T T p + F T p + F T n(4)
The average F1 score for the entire dataset is calculated by determining the average of the F1 T scores for all possible truth values T .</p>
<p>Hyperparameters for Finetuning We leverage QLora [34] for finetuning on open-source models.</p>
<p>Other relevant hyperparameters are: epochs = 3, warmup proportion = 0.01, learning rage = 3e-4, weight decay = 0.01, lora rank = 64, lora dropout = 0.05, lora alpha = 16, batch size = 4, accumulate gradient steps = 8.</p>
<p>Experiment Analysis</p>
<p>In this section, we present a comprehensive analysis of the results obtained from our experiments.Specifically, we focus on two main areas: benchmark results and fine-tuning results.The benchmark results provide a general analysis of the performance trends, as well as a statistical analysis of the impact of domain on model performance.Meanwhile, the fine-tuning results offer insights into the factors that influence model generalization for logic reasoning.By examining these two areas in detail, we aim to provide a thorough understanding of the behavior of large language models in the context of logic reasoning.</p>
<p>Benchmarking</p>
<p>Overview of Model Performance The evaluation results are presented in Figure 2. The performance across models varies, as depicted in the heatmap distribution.At a granular level, GPT-4o models frequently appear to excel, particularly in higher difficulty levels.In contrast, smaller models like Qwen-0.5 and Qwen-1.8often lag, struggling notably with abstract reasoning tasks.This disparity underscores the influence of model size.When aggregating results across all models and logic levels, certain domains consistently present more challenges.Specifically, the domains of Math and Philosophy appear to be the most demanding, likely due to their intrinsic requirement for deep logical structuring and abstract reasoning.Conversely, the domain labeled People generally shows the best performance, suggesting that the models are better attuned to reasoning about human-centered contexts, which might be less abstract or feature more contextual cues.The performance difference among domains are tested to be statistically significant as discussed below.</p>
<p>Influence of Model Size A pivotal observation from our data is the interaction between model size and sample type, presented in Figure 3. Larger models demonstrate a marked proficiency in abstract logical reasoning samples compared to their performance with their corresponding instantiated samples.This trend holds regardless of the difficulty level, suggesting that as models scale, their ability to decipher and apply abstract logic patterns improves significantly.While smaller models, like Qwen-0.5, Yi-6, LLaMA-7, demonstrate either better performance on instantiated samples than abstract samples or less difference between these two types.This discovery differs from previous observations [26,29] which in general state that LLMs are better at instantiated data.Inter-domain Disparities Further analysis of specific model performance within different domains reveals notable patterns.For abstract reasoning tasks, performance is highly variable: smaller models like Qwen-0.5 and Qwen-1.8perform significantly worse, while larger configurations often excel.</p>
<p>In the domain of Math, both Yi and Qwen series models exhibit consistently lower performance, reinforcing the notion of this domain's complexity.Interestingly, there is a general trend from observation where models that generally perform well show more pronounced disparities across domains, suggesting that higher capabilities amplify domain-specific challenges or advantages.</p>
<p>Statistical Analysis on Domain-specific Performance Difference The statistical results are presented in Figures 4 and 5.Each row in either figure consists of four distinct sub-figures.The two sub-figures on the left side illustrate the performance of the respective model for abductive reasoning, while the two on the right side demonstrate the deductive performance.In each pair of two sub-figures, the barplot shows the weighted F1-score for each category across difficulty levels calculated using equation ( 4), while the heatmap displays the results of the chi-square test [35] with each cell corresponding to the p-value of the test regarding any pairwise categories.The application of chi-square test in this regard aims to determine whether there is a significant association between two distributions.As shown in each heatmap, the darker blue (p − value = 0.05 at different thresholds) implies a significant difference between the distributions of two categories, while the lightest blue (p − value &gt; 0.05) suggests no significant association.</p>
<p>Based on the barplots and heatmaps in Figures 4 and 5, there are several observations to highlight in terms of models' performance.First, GPT-4o, the series of Yi models, along with Qwen-7 and Qwen-14, exhibit a relatively higher weighted F1-score for these categories.In particular, GPT-4o and the series of Yi models demonstrate a higher weighted F1-score compared to the other models for abductive reasoning tasks.Second, most of these models perform better in deductive reasoning tasks than abductive reasoning tasks.This observed pattern is consistent across most of the models and categories under investigation in this study.Third, the models' performance varies significantly across different categories.For instance, based on the results of Yi-34 and Qwen-32, the weighted F1-score for the abstract category is much higher than that of other categories.However, it is also noted that the math category consistently displays a comparatively lower weighted F1-score across these categories.Fourth, the abstract category is more likely to display significant differences when compared to other models, as demonstrated in the cases of Qwen-0.5, Qwen-1.5, Qwen-14, Qwen-32, Qwen-110, Yi-9, Yi-34, and GPT-4o.</p>
<p>It is possible that some may question whether the observed differences in model performance are due to the varying input lengths, rather than the effect of different instantiations.To address this potential concern, we have conducted a series of experiments to investigate the correlation between input length and model performance.The results of these experiments, which can be found in Appendix 7.2, suggest that there is little to no correlation between the two.</p>
<p>Generalization by Finetuning</p>
<p>We use finetuning to study the generalization ability from data.The following research questions are addressed: (1) How does the generalization potential of abstract data compare with contextualized data?Specifically, can a model trained on abstract logic instances generalize to contextualized logical reasoning text, and vice versa?(2) What is the impact of model scaling on generalization performance, and does this impact vary depending on the type of data used for fine-tuning (abstract vs. contextualized)?(3) Can models trained on instantiated data in one domain generalize to other domains, and does the diversity of domains in the fine-tuning data impact generalization performance?</p>
<p>Abstract vs. Contextualized To compare the generalization ability of abstract and contextualized data, four models (Qwen-4, Qwen-7, Qwen-14, and GPT-3.5-turbo) are fine-tuned on both purely abstract and purely contextualized data.The benchmark consists of 256 formal logic templates, resulting in 1280 abstract data points.While there are significantly more contextualized data points (1280 * 11), a random sample of 1280 is used for fine-tuning, with each logic template having approximately five instantiations.This subset of contextualized data is referred to as sampled-ctx to distinguish it from the whole contextually instantiated datapoints.</p>
<p>The results indicate that when fine-tuning on abstract data alone, although it learns well on abstract logic samples, it struggles to generalize to contextualized data, with performance decreasing as the dataset difficulty level increases.Conversely, fine-tuning on sampled-ctx data leads to significant improvements in general performance.The performance on purely abstract data is greatly improved.Furthermore, substantial performance improvements are observed in all contextualized data, with near-perfect weighted F1 results for contextualized data in level 1 and 2 in GPT-3.5.It should be noted that while some data points in the evaluation set are present in the fine-tuning set (sampled-ctx), they constitute only a small fraction (1/11) of the contextualized data in the evaluation set.</p>
<p>However, the results of the fine-tuning experiments for the most challenging level (level 4) were found to be unsatisfactory.The performance of the Qwen-14 model on contextualized data is observed to be even slightly worse than that of the same model fine-tuned on purely abstract data.This finding suggests that it is significantly more difficult to uncover the underlying logic reasoning pattern in highly complex reasoning tasks using contextualized data, even when fine-tuning on multiple such instantiations.While contextualized-data fine-tuning may be beneficial for simpler levels, it may be more straightforward to utilize purely abstract logic data to discern the underlying logic pattern in cases of extreme complexity.</p>
<p>Model Scale Effect on Generalization</p>
<p>The impact of model scaling on generalization performance varies depending on the type of data used for fine-tuning.When fine-tuning on abstract data, larger models provide only marginal improvements in performance, suggesting that the potential of abstract data for generalization is limited.However, when fine-tuning on sampled-ctx data, the rate of improvement in evaluation performance increases in relation to model size noticeably when evaluated on both abstract and all-contextualized data, indicating the potential of contextualized data for learning underlying patterns and generalization.Single-domain vs. Multi-domain One potential explanation for the superior generalization capacity of sampled-ctx data is the diversity of domains from which it is sampled, as opposed to the relative homogeneity of abstract data.To investigate the impact of domain diversity, GPT-3.5 is fine-tuned on various single-domain instantiated datasets, each containing 1280 data points, and evaluated on the entire instantiated and abstract datasets.</p>
<p>The results show that the performance of models finetuned on different single-domain instantiated datasets is similar, and in some cases, even better than that of models fine-tuned on sampled-ctx data.This suggests that domain diversity does not play a significant role in the observed generalization capacity.</p>
<p>Conclusion</p>
<p>This paper presents a thorough investigation into the logic reasoning abilities of large language models.By utilizing the ContextHub benchmark, we were able to disentangle logic reasoning from text understanding when performing reasoning tasks.Our results demonstrate that a model's performance on a given reasoning task can be significantly influenced by the context or domain in which the variables of the task are situated.</p>
<p>Furthermore, we examined the data generalization capabilities of abstract and instantiated data.Our findings reveal an intriguing pattern: instantiated data exhibits a greater potential for logic reasoning generalization in fine-tuning, regardless of the domain or number of domains utilized.This suggests that instantiated data may be more effective for fine-tuning models to perform well on a wide range of reasoning tasks.</p>
<p>In summary, our study sheds light on the factors that impact the logic reasoning abilities of large language models and provides insights into how to improve their performance.Future work could explore the use of instantiated data for fine-tuning models on more complex reasoning tasks, as well as the development of benchmarks that better capture the nuances of real-world reasoning scenarios.reasoning task: In Bayesian statistics, if a prior probability distribution is specified or new data is collected, then the posterior probability can be calculated using Bayes' theorem to update the probability based on the new evidence.If the prior probability is not a uniform distribution, then it expresses existing beliefs or knowledge about the values of the parameters.If the prior probability expresses existing beliefs or the posterior probability is calculated, then there is sufficient information to update the probability distribution.Given that the statement "The posterior probability provides an improved estimate of the parameters" is false, can it be determined whether the prior probability is a uniform distribution or not?Continued reasoning task: The soprano sang her aria beautifully and the orchestra played flawlessly, but the tenor forgot his lines.If the orchestra played well or the tenor forgot his lines, then the performance did not go entirely smoothly.The opera house was not empty since the soprano's beautiful aria meant some people attended.The costumes were not delivered late and the sets did not malfunction, so there were no technical difficulties.If some people attended or there were technical difficulties, the show would have faced some challenges.Since the performance did not go smoothly, it implies there was a major disruption.If the show faced challenges or had a major disruption, the opening night of this opera was quite eventful.Given this, was the opening night of the opera eventful?or pswg) -&gt; fkyxi.(fkyxi or qvb) -&gt; abc.Deduce the result of abc.reasoning task: Sue did push-ups but not pull-ups yesterday.If she did push-ups or pull-ups, then she did some upper body exercises.Sue trained her core by doing planks.Since she did not do a full body workout, it means she did a partial body workout.Sue did squats yesterday, so it is not true that she did not do squats.If Sue did some upper body exercises and did not do squats, then she only trained upper body.If Sue did planks or trained her core muscles, then she had an effective core workout.Sue did lunges and step-ups, but not calf raises.If she did lunges or step-ups, then she trained her leg muscles.If Sue did wall sits or calf raises, then she did some quad and hamstring exercises.If Sue trained her leg muscles and did some quad/hamstring exercises, then she had an effective lower body workout.If Sue did not do a partial body workout, then she did a full body workout.If Sue had an effective lower body workout or did a full body workout, then she had a comprehensive workout.Sue did burpees yesterday, so it is not true that she did not do burpees.If Sue did not do burpees and had an effective core workout, then she did an intense workout.If Sue only trained upper body or did an intense workout, then she had a focused or intense workout.If Sue had a focused/intense workout or a comprehensive workout, then she had a productive bodyweight training session.Did Sue have a productive bodyweight training session yesterday?</p>
<p>Length Correlation</p>
<p>Some maybe curious whether the performance degradation and variance on instantiated data are correlated with input text length.To see whether there is indeed a correlation between model performance and text length, we employ four models of varying sizes (Qwen-0.5,Qwen-7, Qwen-32, Qwen-110) and conduct length-based performance ablation.We then analyze the performance of each model based on the length of the input text.Specifically, for each graph presented below, the x-axis represents the text length, while the y-axis represents the corresponding model performance.The y-axis value for each x-axis value x is the model performance on the part of the data whose corresponding input text length is smaller than x.</p>
<p>Based on the two images Figure8 and Figure 9, we cannot see any consistent correlation between model performance and input length after tokenization using model corresponding tokenizer.</p>
<p>Human verification</p>
<p>The benchmark dataset used in this study was synthetically generated based on the Claude-3 model, but was subsequently reviewed by a panel of five annotators, each holding a Ph.D. degree in a diverse range of fields, including computer science, informatics, civil engineering, and medicine.Each annotator was responsible for reviewing 2-3 domains, with each domain consisting of 20 data datapoints for each level of the dataset.In total, 220 datapoints were manually reviewed for each level of the dataset, comprising 10% of the total dataset.</p>
<p>The following table 3 presents the rate of correctness for template adherence and fact reckoning, respectively, as determined by the panel of annotators.The high rates of correctness for both template adherence and fact reckoning suggest that the synthetically generated dataset is of high quality and is suitable for use in evaluating the performance of LLMs on instantiated logic tasks.</p>
<p>Figure 1 :
1
Figure 1: Benchmark Construction Procedure</p>
<p>Figure 2 :
2
Figure 2: Main Benchmark Performance</p>
<p>OHYHOFRQWH[WXDOL]HGOHYHODEVWUDFW OHYHOFRQWH[WXDOL]HGOHYHODEVWUDFW OHYHOFRQWH[WXDOL]HGOHYHODEVWUDFW OHYHOFRQWH[WXDOL]HGOHYHODEVWUDFW</p>
<p>Figure 3 :
3
Figure 3: Abstract performance vs. instantiated performance</p>
<p>Figure 4 :Figure 5 :Figure 6 :
456
Figure 4: Results of weighted F1-score and Chi-square test</p>
<p>Different Categories and their Performance</p>
<p>Figure 7 :
7
Figure 7: Model performance by finetuning on different domains.</p>
<p>(wmejd or rdbk) -&gt; rqmc.(eek and rqmc) -&gt; bw.(NOT pg) -&gt; qbli.(bw or qbli) -&gt; qvb.hujcf is True.(NOT hujcf) -&gt; pil.(pil and stbf) -&gt; pswg.</p>
<p>Figure 8 :
8
Figure 8: Length-based performance collection on abductive logic.The four rows correspond to four models, and four columns correspond to four difficulty levels.</p>
<p>Figure 9 :
9
Figure 9: Length-based performance collection on deductive logic.The four rows correspond to four models, and four columns correspond to four difficulty levels.</p>
<p>Table 2 :
2
The following table presents several examples showing abductive and deductive reasoning with their respective difficulty levels and domains.The left column shows examples of abstract instantiations, while the right column shows contextually instantiated examples in specific domains.Examples of abductive and deductive reasoning.
7 Appendix7.1 Data ExamplesAbstract ExampleSpecific Domain ExampleAbductive ReasoningLevel 1 -AbstractLevel 1 -Geography and Placesaaa: vxkgraaa: The terrain has experienced significant uplift.aab: cauncaab: Powerful erosional forces have shaped the land.aac: ybyzaac: The area features tall, steep mountains.reasoning task: (vxkgrreasoning task: If an area of land has experienced significant upliftor caunc) -ybyz. Givenor been shaped by powerful erosional forces, then the terrain willybyz is False, what is thefeature tall, steep mountains. Given that the area does not have tall,value of caunc?steep mountains, can it be determined if powerful erosional forceshave shaped the land?Level 2 -AbstractLevel 2 -Mathematics and Logicaaa: ttjmxaaa: The prior probability is a uniform distributionaab: kottzaab: The prior probability expresses existing beliefs about the param-eters.aac: wqeqaac: A prior probability distribution is specified.aad: mnzeaad: New data is collected.aae: zkxaae: The posterior probability is calculated using Bayes' theorem.aaf: pofkaaf: The posterior probability provides an improved estimate of theparameters.reasoning task: (wqeq ormnze) -zkx. (NOT ttjmx)-kottz. (kottz or zkx)-pofk. Given pofk isFalse, what is the valueof ttjmx?</p>
<p>Table 2
2
The balance beam not being set up properly means Sophie cannot practice her beam routine.Similarly, if the uneven bars are not at the correct height, Sophie cannot work on her bar skills.If Sophie is unable to train on at least one apparatus, she faces a major hindrance to her practice.Torn floor mats needing replacement or insufficient floor space makes it unsafe for Sophie to practice floor exercises.A broken springboard or unstable vault makes performing vault runs risky.If it is unsafe to practice floor or vault exercises, Sophie cannot train safely or productively.Sophie's coach not being at practice means she does not have supervision.Having supervision allows Sophie to train.If Sophie did not fuel properly before practice, she will not have enough energy to train effectively.With an upcoming competition, Sophie needs to prepare new skills, requiring dedicated practice time.If Sophie's training is compromised by risky apparatus or lack of practice time, she will not be able to practice effectively.If Sophie's training session is unproductive or she faces major hindrances, then she is not making progress in her gymnastics.Lack of progress or likely negative performance impacts put Sophie's gymnastics career at risk.Given that Sophie is not considering withdrawing from competitions, what can be determined about the balance beam being set up properly?The Lee family identifies as Korean-American, but they do not speak Korean fluently.If the Lee family speaks Korean fluently or identifies as Korean-American, then they have a connection to Korean culture.John Lee was born in the United States, and his parents immigrated from South Korea.If John Lee was born in the U.S. or his parents immigrated from South Korea, then he has Korean ancestry.If John Lee has Korean ancestry and his family has a connection to Korean culture, then he is considered Korean-American.Can we conclude that John Lee is considered Korean-American based on the given information?
-continued from previous pageAbstract ExampleSpecific Domain Exampleaaw: tkaaw: Sophie does not have enough energy to train effectively.aax: vodaax: Sophie's gymnastics career is at risk.aay: dngjaaay: Sophie's gymnastics performance will likely be impacted nega-tively.aaz: ozyueaaz: Sophie may need to consider withdrawing from competitions.reasoning task: (NOT yxt) -&gt; ln. (NOT qa) -&gt; py. (ln or py) -&gt; qe. (ng or bhjb) -&gt; djay. (vwwf or lj) -&gt; qd. (NOT tfxbc) -&gt; aww. (NOT aww) -&gt; oftr. (NOT pvize) -&gt; tk. (cg or ysjeo) -&gt; uby. (uby or qd) -&gt; miz. (miz or oftr) -&gt; fzsq. (djay or tk) -&gt; vod. (qe or vod) -&gt; dngja. (fzsq or dngja) -&gt; ozyue. Given ozyue is False, what is the value of yxt?reasoning task: Deductive ReasoningLevel 1 -AbstractLevel 1 -Natural and Physical Sciencesaaa: pusvuaaa: A cold front is approaching the regionaab: hsaab: A warm air mass is stagnant over the areaaac: ivlaac: Atmospheric instability is likely to developreasoning task: pusvu isreasoning task: A cold front is approaching the region, but there is noTrue. hs is False. (pusvuwarm air mass stagnant over the area. If a cold front approaches or aor hs) -ivl. Deduce thewarm air mass is stagnant, then atmospheric instability is likely toresult of ivl.develop. Can we say that atmospheric instability will likely developin this scenario?Level 2 -AbstractLevel 2 -Society and Social Sciencesaaa: jdaaa: John Lee was born in the United Statesaab: bfkaab: John Lee's parents immigrated from South Koreaaac: wngaac: John Lee has Korean ancestryaad: vkoaad: The Lee family speaks Korean fluentlyaae: cvaaae: The Lee family identifies as Korean-Americanaaf: qymwaaaf: The Lee family has a connection to Korean cultureaag: craag: John Lee is considered Korean-AmericanContinued</p>
<p>Table 2
2-continued from previous pageAbstract ExampleSpecific Domain Exampleaau: rqmcaau: Sue did some quad and hamstring exercises yesterdayaav: bwaav: Sue had an effective lower body workout yesterdaaaw: xvdaaw: Sue did not do a full body workout yesterdayaax: pgaax: Sue did a partial body workout yesterdayaay: qbliaay: Sue did a full body workout yesterdayaaz: qvbaaz: Sue had a comprehensive workout yesterdayreasoning task: fo isFalse.msta is True.(msta or fo) -&gt; jfnrh.dyue is True. xvd is False.(NOT xvd) -&gt; pg. ssbis True. (NOT ssb) -&gt;ac. (jfnrh and ac) -&gt; dzda.sgniu is True. (dyue orsgniu) -&gt; stbf. outm isTrue. rdbk is False. ybjjis True. (outm or ybjj)-&gt; eek. wmejd is True.</p>
<p>Table 3 :
3
Accuracy of Synthetically-generated Data
data level template adherence fact reckoning1100%95.45%2100%94.54%3100%96.36%496.36%93.18%
Classics , Critical theory , Cultural anthropology , Clothing , Folklore , Food and drink culture , Language , Literature , Museology , Mythology
Level 4 -Abstract Level 4 -Health and Fitness aaa: msta aaa: Sue did push-ups yesterday aab: fo aab: Sue did not do pull-ups yesterday aac: jfnrh aac: Sue did some upper body exercises yesterday aad: ssb aad: Sue did squats yesterday aae: ac aae: Sue did not do squats yesterday aaf: dzda aaf: Sue only trained upper body yesterday aag: hujcf aag: Sue did burpees yesterday aah: pil aah: Sue did not do burpees yesterday aai: dyue aai: Sue trained her core muscles yesterday aaj: sgniu aaj: Sue did planks yesterday aak: stbf aak: Sue had an effective core workout yesterday aan: pswg aan: Sue did an intense workout yesterday aao: fkyxi aao: Sue had a focused or intense workout yesterday aap: outm aap: Sue did lunges yesterday aaq: ybjj aaq: Sue did step-ups yesterday aar: eek aar: Sue trained her leg muscles yesterday aas: wmejd aas: Sue did wall sits yesterday aat: rdbk aat: Sue did not do calf raises yesterday Continued
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda , Advances in neural information processing systems. 202033</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Wang Jue, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, Transactions on Machine Learning Research. 2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. BIG bench authors. 2023</p>
<p>Dyval: Graph-informed dynamic evaluation of large language models. Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie, International Conference on Learning Representations. 2024</p>
<p>Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, Yongfeng Zhang, arXiv:2312.14890Nphardeval: Dynamic benchmark on reasoning ability of large language models via complexity classes. 2023arXiv preprint</p>
<p>Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, Willie Neiswanger, arXiv:2404.01266Isobench: Benchmarking multimodal foundation models on isomorphic representations. 2024arXiv preprint</p>
<p>Understanding social reasoning in language models with language models. Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, Noah Goodman, Advances in Neural Information Processing Systems. 362024</p>
<p>hot" chatgpt: The promise of chatgpt in detecting and discriminating hateful, offensive, and toxic comments on social media. Lingyao Li, Lizhou Fan, Shubham Atreja, Libby Hemphill, ACM Transactions on the Web. 1822024</p>
<p>Datachat: Prototyping a conversational agent for dataset search and visualization. Lizhou Fan, Sara Lafia, Lingyao Li, Fangyuan Yang, Libby Hemphill, arXiv:2305.183582023arXiv preprint</p>
<p>Arb: Advanced reasoning benchmark for large language models. Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J Nay, Kshitij Gupta, Aran Komatsuzaki, arXiv:2307.136922023arXiv preprint</p>
<p>Can large language models reason about medical questions?. Valentin Liévin, Egeberg Christoffer, Andreas Geert Hother, Ole Motzfeldt, Winther, Patterns. 532024</p>
<p>Evaluating the chatgpt family of models for biomedical reasoning and classification. Shan Chen, Yingya Li, Sheng Lu, Hoang Van, Jwl Hugo, Guergana K Aerts, Danielle S Savova, Bitterman, Journal of the American Medical Informatics Association. 3142024</p>
<p>Health-llm: Personalized retrieval-augmented disease prediction system. Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Zhu, Meng, Wang, Du, arXiv:2402.007462024arXiv preprint</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang, arXiv:2311.172272023arXiv preprint</p>
<p>Shuhang Lin, Wenyue Hua, Lingyao Li, Che-Jui Chang, Lizhou Fan, Jianchao Ji, Hang Hua, Mingyu Jin, Jiebo Luo, Yongfeng Zhang, arXiv:2404.15532Battleagent: Multi-modal dynamic emulation on historical battles to complement historical analysis. 2024arXiv preprint</p>
<p>What if llms have different world views: Simulating alien civilizations with llm-based agents. Mingyu Jin, Beichen Wang, Zhaoqian Xue, Suiyuan Zhu, Wenyue Hua, Hua Tang, Kai Mei, Mengnan Du, Yongfeng Zhang, arXiv:2402.131842024arXiv preprint</p>
<p>Can large language models transform computational social science? Computational Linguistics. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, 202450</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Reason from fallacy: Enhancing large language models' logical reasoning through logical fallacy understanding. Yanda Li, Dixuan Wang, Jiaqing Liang, Guochao Jiang, Qianyu He, Yanghua Xiao, Deqing Yang, arXiv:2404.042932024arXiv preprint</p>
<p>Language models show human-like content effects on reasoning. Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.070512022arXiv preprint</p>
<p>On the paradox of generalizable logical reasoning in large language models. Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, Muhan Zhang, 2023</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Wikipedia categories. </p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, arXiv:2309.16609Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, arXiv:2403.046522024ai. arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Lorentz Sorana D Bolboacȃ, Adriana F Jäntschi, Radu E Sestraş, Doru C Sestraş, Pamfil, Pearson-fisher chi-square statistic revisited. Information. 20112</p>            </div>
        </div>

    </div>
</body>
</html>
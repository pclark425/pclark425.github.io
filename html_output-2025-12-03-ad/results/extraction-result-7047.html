<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7047 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7047</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7047</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-24a2d2d044ed2dda17cecb96366b8a4079f898b9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/24a2d2d044ed2dda17cecb96366b8a4079f898b9" target="_blank">Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work establishes principles for designing high-quality samples for designing high-quality samples by integrating symbolic logic theory and previous empirical insights, and constructs a synthetic corpus, comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose $\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named $\textbf{Formal Logic Deduction Diverse}$ ($\textbf{FLD}$$_{\times 2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on FLD$_{\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7047.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7047.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALT-FLDx2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Additional Logic Training on FLD×2 (ALT-FLD×2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised fine-tuning approach that trains large LLMs on a synthetic corpus (FLD×2) of multistep deductive proofs, with adversarial distractors and diverse linguistic templates, to improve strict logical reasoning and proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B / LLaMA-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-3.1 transformer-family language models (decoder-only), fine-tuned with supervised targets of intermediate logical steps and final labels; training excludes loss on inputs (facts) to avoid learning to generate unknown facts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B and 70B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (decoder-only), supervised fine-tuning for proof-step generation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>FLD×2 synthetic corpus (100k training samples used for ALT experiments; 0.1B tokens; 1 epoch), compared to ALT trained on other synthetic corpora (RuleTaker, PARARULE-Plus, FLD).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised additional training on synthetic multistep deduction samples that require generation of intermediate logical steps (proof generation); evaluation done with few-shot in-context prompting for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>31 benchmark suite (selected examples: LogicNLI, FOLIO, RobustLR, bAbI, AbductionRules, AR-LSAT, LogiQA, ReClor, BBH, GSM8k, MATH, HumanEval, MBPP, MultiPL-E)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A broad set of benchmarks covering deductive logical reasoning (first-order style), natural language inference, robustness to logical perturbations, abductive reasoning, math, and code generation; specific logic datasets include RuleTaker-style tasks, LogicNLI, FOLIO, RobustLR, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multistep deductive reasoning (proof generation), entailment classification, and related logical tasks (some abductive and NLI tasks evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / benchmark-specific accuracy (micro-average reported across many tasks); some tasks report per-benchmark accuracy with ± std.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples (LLaMA-3.1-70B): Avg. 60.0 → 64.4 (ALT-FLD×2) micro-avg; Logic suite 57.4 → 66.1; RobustLR 49.6 → 81.6 (+32.0); LogicNLI 34.9 → 50.9 (+16.0); Code (HumanEval) 46.2 → 52.4; Math (GSM8k) 80.9 → 83.3; BBH (3-shot) 60.4 → 65.4. (See paper Table 2 and Table 4a–c.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ALT-FLD×2 outperforms baseline pretrained LLaMA-3.1 models and ALT variants trained on other synthetic corpora (ALT-RT, ALT-PRP, ALT-FLD), e.g., LLaMA-3.1-70B avg: baseline 60.0 → ALT-RT 62.7 → ALT-FLD 64.2 → ALT-FLD×2 64.4; RobustLR showed the largest single-benchmark gain (+32 points).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning state-of-the-art LLMs with a principled synthetic logic corpus (FLD×2) substantially improves strict logical reasoning (proof generation and entailment) and transfers to related tasks (abductive reasoning, NLI, some math and code gains); FLD×2 (design principles DP1–DP4) yielded the largest gains among compared synthetic corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not teach new factual knowledge (so knowledge-heavy QA not improved); risk of catastrophic forgetting if knowledge-forgetting prevention is not used; modest gains on some general tasks (≤2 pts); remaining failures on knowledge-intensive or highly procedural multi-choice tasks and on certain advanced domain questions (Table 6); LLMs still struggle with very long proofs/steps and some forms of self-verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7047.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7047.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLD×2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formal Logic Deduction Diverse (FLD×2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic corpus created for ALT containing multistep deductive reasoning samples with a large vocabulary (~100k), adversarial distractors, ~50 deduction rules (axioms and representative theorems), logical proofs up to 8 steps, and many linguistic templates per formula to diversify surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Synthetic corpus built from programmatic generation (vocabulary from WordNet full, adversarial distractor generation, axioms+theorems for deduction rules, templates to vary expression), used as the fine-tuning dataset for ALT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Dataset design encodes deductive reasoning (first-order style) for supervised proof-step generation; emphasizes unknown facts, negative (illogical) examples, axioms-based multi-step proofs, and linguistic diversity (DP1–DP4).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Training data for multistep deductive reasoning / proof generation tasks and entailment/inference tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared in ablation experiments to smaller-vocab or less-diverse corpora (RuleTaker, PARARULE-Plus, original FLD); FLD×2 produced the largest downstream improvements when used for ALT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Design principles implemented in FLD×2 (DP1: unknown facts via large vocab; DP2: adversarial distractors; DP3: axioms + theorems and diverse step counts; DP4: many linguistic templates) are critical: ablating any principle degrades downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Synthetic nature risks overfitting to corpus-specific cues if used directly for evaluation; contains unknown facts which can overwrite pre-trained factual knowledge if training does not prevent forgetting; creation does not itself add real-world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7047.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7047.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recall Adam</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recall Adam optimizer (knowledge-forgetting prevention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An optimizer that regularizes fine-tuning updates to prevent divergence from pre-trained parameters, used during ALT to reduce forgetting of factual knowledge when training on synthetic samples with many unknown facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Optimizer-level regularization (Recall Adam) to preserve pre-trained knowledge while fine-tuning on logic data.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Not an external reasoning tool; an optimizer/regularizer integrated into fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Fine-tuning stabilization / knowledge retention</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream benchmark performance (aggregate); ablation effect measured by comparing ALT with and without Recall Adam.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports that ALT without Recall Adam underperformed compared to ALT with Recall Adam and even underperformed the no-ALT baseline (see Section 5 and Appendix F.8).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Using Recall Adam during ALT is critically important: models trained without it performed worse than those with it and sometimes worse than models that did not receive ALT at all, indicating catastrophic forgetting without this regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Regularizing fine-tuning to remain near pre-trained weights prevents displacement of factual knowledge by synthetic unknown facts and is necessary for net positive gains from ALT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Adds hyperparameters (Fisher coefficient, etc.) and constraints that may reduce the magnitude of permissible updates; the optimal regularization strength may vary by model size and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7047.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7047.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALT-RT / ALT-PRP / ALT-FLD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Additional Logic Training on comparison corpora (ALT-RT, ALT-PRP, ALT-FLD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation/alternative experiments where ALT is applied using other synthetic logic corpora: RT (RuleTaker), PRP (PARARULE-Plus), original FLD; used to compare the effect of corpus design on downstream reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B / LLaMA-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLaMA-3.1 models fine-tuned with ALT on alternative synthetic corpora (RuleTaker, PARARULE-Plus, FLD) to measure comparative benefits versus FLD×2.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B and 70B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer, supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>RuleTaker (RT), PARARULE-Plus (PRP), FLD (previous FLD), each with distinct feature sets (vocabulary size, distractor design, deduction rules, expression diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised fine-tuning for proof-step generation similar to ALT-FLD×2 but with each corpus' specific characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same 31-benchmark suite used in main experiments</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See ALT-FLD×2 entry; used to compare generalization from different synthetic corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multistep deductive reasoning, entailment, NLI, and related tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / micro-average across benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples (LLaMA-3.1-70B): baseline avg 60.0; ALT-PRP 60.4; ALT-RT 62.7; ALT-FLD 64.2; ALT-FLD×2 64.4 (Table 2). For 8B model similar ordering with ALT-FLD×2 best.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>FLD×2 gave the largest gains; ablating design principles or using other corpora resulted in consistently lower downstream performance, demonstrating corpus design importance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Corpus design matters: richer vocabulary, adversarial distractors, axioms+theorems, and diverse linguistic expressions (FLD×2) yield larger improvements than prior corpora (RuleTaker, PARARULE-Plus, FLD).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even best synthetic corpora cannot replace real-world knowledge; different corpora may still leave gaps in long-proof generalization and may be sensitive to superficial cues unless carefully regularized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7047.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7047.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.1 (70B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art, decoder-only transformer language model (LLaMA family) pre-trained on a very large corpus (>15 trillion tokens according to paper) and evaluated both before and after ALT to measure gains in strict logical reasoning and related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (LLaMA-3.1 family) pre-trained at large scale; used as the base model for ALT experiments and evaluated on 31 benchmarks with few-shot in-context learning and after supervised fine-tuning on synthetic logic corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale pretraining on diverse web/text/code corpora (paper notes pretraining on over 15 trillion tokens as context for the model); further fine-tuned with ALT on synthetic logic corpora (FLD×2 and alternatives).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Baseline reasoning relies on pretraining plus in-context few-shot prompting; improved reasoning via supervised fine-tuning (ALT) to generate logical steps/proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>31-benchmark suite including LogicNLI, FOLIO, RobustLR, bAbI, AbductionRules, AR-LSAT, LogiQA, ReClor, BBH, GSM8k, MATH, HumanEval, MBPP, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Comprehensive evaluation across logical deduction, NLI, robustness, abductive reasoning, math, and code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Strict logical deduction (proof generation), entailment, NLI, abductive reasoning, math problem solving, code generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (per-benchmark), micro-average across suite; BBH uses 3-shot and 0-shot reporting where noted.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Selected numbers: baseline Avg. 60.0; ALT-FLD×2 Avg. 64.4. Logic: 57.4 → 66.1 (ALT-FLD×2). RobustLR: 49.6 → 81.6. HumanEval: 32.3 → 42.6 (ALT-FLD in Table 4c); with ALT-FLD×2 HumanEval 42.6 (reported). BBH (3-shot): 60.4 → 65.4. Improvements up to ~32 points on specific logic benchmarks, up to 10 points on math/code benchmarks, and ~5 points on BBH were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Fine-tuning with FLD×2 improved across many domains compared to base LLaMA-3.1-70B and compared to ALT on other synthetic corpora; best-performing ALT variant was ALT-FLD×2.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even very large pre-trained LLMs benefit substantially from targeted synthetic logic fine-tuning: improvements in strict logical reasoning (proof generation and entailment) and transfer to related domains were observed; FLD×2 produced the largest gains among synthetic corpora tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pretrained knowledge is required for some tasks; without knowledge-preserving optimization models can forget facts when fine-tuned on synthetic unknown facts; remaining failures on knowledge-heavy questions (ARC challenge), some graduate-level physics (GPQA), and complex multi-choice reasoning indicate limits; sensitivity to superficial surface cues and premise order remain known failure modes (cited related work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7047.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7047.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.1 (8B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller variant of LLaMA-3.1 (8B parameters) used to test ALT benefits across model sizes; fine-tuned with ALT on different synthetic corpora to measure gains in logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer; smaller-capacity LLaMA-3.1 variant used for controlled ALT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained LLaMA-3.1-8B (base) then fine-tuned on synthetic logic corpora (100k samples, 1 epoch) with Recall Adam to avoid forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised ALT yielding generation of intermediate logical steps; evaluated with 5-shot in-context learning on 31 benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same 31-benchmark suite as for 70B experiments</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LLaMA-3.1-70B entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multistep deductive reasoning, NLI, math, code, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / micro-average</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from Table 2 (8B): baseline Avg. 47.9; ALT-FLD×2 Avg. 52.0; Logic: 42.8 → 52.2; Math ~39.6 → 43.2; Code 35.4 → 38.0. Gains smaller than 70B but consistent improvements were observed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ALT on FLD×2 improved 8B model performance versus baseline and versus ALT on other corpora; ablating design principles reduced gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ALT benefits are not limited to very large models — smaller LLaMA variants also improve, supporting the generality of synthetic logic fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller capacity yields smaller absolute gains compared to 70B; same limitations as above regarding knowledge-dependent tasks and complex multi-step procedural QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation <em>(Rating: 2)</em></li>
                <li>Learning deductive reasoning from synthetic corpus based on formal logic <em>(Rating: 2)</em></li>
                <li>Program synthesis with large language models <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7047",
    "paper_id": "paper-24a2d2d044ed2dda17cecb96366b8a4079f898b9",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "ALT-FLDx2",
            "name_full": "Additional Logic Training on FLD×2 (ALT-FLD×2)",
            "brief_description": "A supervised fine-tuning approach that trains large LLMs on a synthetic corpus (FLD×2) of multistep deductive proofs, with adversarial distractors and diverse linguistic templates, to improve strict logical reasoning and proof generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B / LLaMA-3.1-70B",
            "model_description": "LLaMA-3.1 transformer-family language models (decoder-only), fine-tuned with supervised targets of intermediate logical steps and final labels; training excludes loss on inputs (facts) to avoid learning to generate unknown facts.",
            "model_size": "8B and 70B",
            "architecture_type": "transformer (decoder-only), supervised fine-tuning for proof-step generation",
            "training_data": "FLD×2 synthetic corpus (100k training samples used for ALT experiments; 0.1B tokens; 1 epoch), compared to ALT trained on other synthetic corpora (RuleTaker, PARARULE-Plus, FLD).",
            "reasoning_method": "Supervised additional training on synthetic multistep deduction samples that require generation of intermediate logical steps (proof generation); evaluation done with few-shot in-context prompting for downstream tasks.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "31 benchmark suite (selected examples: LogicNLI, FOLIO, RobustLR, bAbI, AbductionRules, AR-LSAT, LogiQA, ReClor, BBH, GSM8k, MATH, HumanEval, MBPP, MultiPL-E)",
            "benchmark_description": "A broad set of benchmarks covering deductive logical reasoning (first-order style), natural language inference, robustness to logical perturbations, abductive reasoning, math, and code generation; specific logic datasets include RuleTaker-style tasks, LogicNLI, FOLIO, RobustLR, etc.",
            "task_type": "Multistep deductive reasoning (proof generation), entailment classification, and related logical tasks (some abductive and NLI tasks evaluated)",
            "performance_metric": "Accuracy / benchmark-specific accuracy (micro-average reported across many tasks); some tasks report per-benchmark accuracy with ± std.",
            "performance_value": "Examples (LLaMA-3.1-70B): Avg. 60.0 → 64.4 (ALT-FLD×2) micro-avg; Logic suite 57.4 → 66.1; RobustLR 49.6 → 81.6 (+32.0); LogicNLI 34.9 → 50.9 (+16.0); Code (HumanEval) 46.2 → 52.4; Math (GSM8k) 80.9 → 83.3; BBH (3-shot) 60.4 → 65.4. (See paper Table 2 and Table 4a–c.)",
            "comparison_with_baseline": "ALT-FLD×2 outperforms baseline pretrained LLaMA-3.1 models and ALT variants trained on other synthetic corpora (ALT-RT, ALT-PRP, ALT-FLD), e.g., LLaMA-3.1-70B avg: baseline 60.0 → ALT-RT 62.7 → ALT-FLD 64.2 → ALT-FLD×2 64.4; RobustLR showed the largest single-benchmark gain (+32 points).",
            "key_findings": "Fine-tuning state-of-the-art LLMs with a principled synthetic logic corpus (FLD×2) substantially improves strict logical reasoning (proof generation and entailment) and transfers to related tasks (abductive reasoning, NLI, some math and code gains); FLD×2 (design principles DP1–DP4) yielded the largest gains among compared synthetic corpora.",
            "limitations": "Does not teach new factual knowledge (so knowledge-heavy QA not improved); risk of catastrophic forgetting if knowledge-forgetting prevention is not used; modest gains on some general tasks (≤2 pts); remaining failures on knowledge-intensive or highly procedural multi-choice tasks and on certain advanced domain questions (Table 6); LLMs still struggle with very long proofs/steps and some forms of self-verification.",
            "uuid": "e7047.0",
            "source_info": {
                "paper_title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "FLD×2",
            "name_full": "Formal Logic Deduction Diverse (FLD×2)",
            "brief_description": "A synthetic corpus created for ALT containing multistep deductive reasoning samples with a large vocabulary (~100k), adversarial distractors, ~50 deduction rules (axioms and representative theorems), logical proofs up to 8 steps, and many linguistic templates per formula to diversify surface forms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": null,
            "training_data": "Synthetic corpus built from programmatic generation (vocabulary from WordNet full, adversarial distractor generation, axioms+theorems for deduction rules, templates to vary expression), used as the fine-tuning dataset for ALT.",
            "reasoning_method": "Dataset design encodes deductive reasoning (first-order style) for supervised proof-step generation; emphasizes unknown facts, negative (illogical) examples, axioms-based multi-step proofs, and linguistic diversity (DP1–DP4).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Training data for multistep deductive reasoning / proof generation tasks and entailment/inference tasks",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Compared in ablation experiments to smaller-vocab or less-diverse corpora (RuleTaker, PARARULE-Plus, original FLD); FLD×2 produced the largest downstream improvements when used for ALT.",
            "key_findings": "Design principles implemented in FLD×2 (DP1: unknown facts via large vocab; DP2: adversarial distractors; DP3: axioms + theorems and diverse step counts; DP4: many linguistic templates) are critical: ablating any principle degrades downstream performance.",
            "limitations": "Synthetic nature risks overfitting to corpus-specific cues if used directly for evaluation; contains unknown facts which can overwrite pre-trained factual knowledge if training does not prevent forgetting; creation does not itself add real-world knowledge.",
            "uuid": "e7047.1",
            "source_info": {
                "paper_title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Recall Adam",
            "name_full": "Recall Adam optimizer (knowledge-forgetting prevention)",
            "brief_description": "An optimizer that regularizes fine-tuning updates to prevent divergence from pre-trained parameters, used during ALT to reduce forgetting of factual knowledge when training on synthetic samples with many unknown facts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": "Optimizer-level regularization (Recall Adam) to preserve pre-trained knowledge while fine-tuning on logic data.",
            "external_tool_used": false,
            "external_tool_description": "Not an external reasoning tool; an optimizer/regularizer integrated into fine-tuning.",
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Fine-tuning stabilization / knowledge retention",
            "performance_metric": "Downstream benchmark performance (aggregate); ablation effect measured by comparing ALT with and without Recall Adam.",
            "performance_value": "Paper reports that ALT without Recall Adam underperformed compared to ALT with Recall Adam and even underperformed the no-ALT baseline (see Section 5 and Appendix F.8).",
            "comparison_with_baseline": "Using Recall Adam during ALT is critically important: models trained without it performed worse than those with it and sometimes worse than models that did not receive ALT at all, indicating catastrophic forgetting without this regularization.",
            "key_findings": "Regularizing fine-tuning to remain near pre-trained weights prevents displacement of factual knowledge by synthetic unknown facts and is necessary for net positive gains from ALT.",
            "limitations": "Adds hyperparameters (Fisher coefficient, etc.) and constraints that may reduce the magnitude of permissible updates; the optimal regularization strength may vary by model size and dataset.",
            "uuid": "e7047.2",
            "source_info": {
                "paper_title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ALT-RT / ALT-PRP / ALT-FLD",
            "name_full": "Additional Logic Training on comparison corpora (ALT-RT, ALT-PRP, ALT-FLD)",
            "brief_description": "Ablation/alternative experiments where ALT is applied using other synthetic logic corpora: RT (RuleTaker), PRP (PARARULE-Plus), original FLD; used to compare the effect of corpus design on downstream reasoning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B / LLaMA-3.1-70B",
            "model_description": "Same LLaMA-3.1 models fine-tuned with ALT on alternative synthetic corpora (RuleTaker, PARARULE-Plus, FLD) to measure comparative benefits versus FLD×2.",
            "model_size": "8B and 70B",
            "architecture_type": "transformer, supervised fine-tuning",
            "training_data": "RuleTaker (RT), PARARULE-Plus (PRP), FLD (previous FLD), each with distinct feature sets (vocabulary size, distractor design, deduction rules, expression diversity).",
            "reasoning_method": "Supervised fine-tuning for proof-step generation similar to ALT-FLD×2 but with each corpus' specific characteristics.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Same 31-benchmark suite used in main experiments",
            "benchmark_description": "See ALT-FLD×2 entry; used to compare generalization from different synthetic corpora.",
            "task_type": "Multistep deductive reasoning, entailment, NLI, and related tasks",
            "performance_metric": "Accuracy / micro-average across benchmarks",
            "performance_value": "Examples (LLaMA-3.1-70B): baseline avg 60.0; ALT-PRP 60.4; ALT-RT 62.7; ALT-FLD 64.2; ALT-FLD×2 64.4 (Table 2). For 8B model similar ordering with ALT-FLD×2 best.",
            "comparison_with_baseline": "FLD×2 gave the largest gains; ablating design principles or using other corpora resulted in consistently lower downstream performance, demonstrating corpus design importance.",
            "key_findings": "Corpus design matters: richer vocabulary, adversarial distractors, axioms+theorems, and diverse linguistic expressions (FLD×2) yield larger improvements than prior corpora (RuleTaker, PARARULE-Plus, FLD).",
            "limitations": "Even best synthetic corpora cannot replace real-world knowledge; different corpora may still leave gaps in long-proof generalization and may be sensitive to superficial cues unless carefully regularized.",
            "uuid": "e7047.3",
            "source_info": {
                "paper_title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLaMA-3.1-70B",
            "name_full": "LLaMA-3.1 (70B parameters)",
            "brief_description": "A state-of-the-art, decoder-only transformer language model (LLaMA family) pre-trained on a very large corpus (&gt;15 trillion tokens according to paper) and evaluated both before and after ALT to measure gains in strict logical reasoning and related tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-70B",
            "model_description": "Decoder-only transformer (LLaMA-3.1 family) pre-trained at large scale; used as the base model for ALT experiments and evaluated on 31 benchmarks with few-shot in-context learning and after supervised fine-tuning on synthetic logic corpora.",
            "model_size": "70B parameters",
            "architecture_type": "decoder-only transformer",
            "training_data": "Large-scale pretraining on diverse web/text/code corpora (paper notes pretraining on over 15 trillion tokens as context for the model); further fine-tuned with ALT on synthetic logic corpora (FLD×2 and alternatives).",
            "reasoning_method": "Baseline reasoning relies on pretraining plus in-context few-shot prompting; improved reasoning via supervised fine-tuning (ALT) to generate logical steps/proofs.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "31-benchmark suite including LogicNLI, FOLIO, RobustLR, bAbI, AbductionRules, AR-LSAT, LogiQA, ReClor, BBH, GSM8k, MATH, HumanEval, MBPP, etc.",
            "benchmark_description": "Comprehensive evaluation across logical deduction, NLI, robustness, abductive reasoning, math, and code tasks.",
            "task_type": "Strict logical deduction (proof generation), entailment, NLI, abductive reasoning, math problem solving, code generation",
            "performance_metric": "Accuracy (per-benchmark), micro-average across suite; BBH uses 3-shot and 0-shot reporting where noted.",
            "performance_value": "Selected numbers: baseline Avg. 60.0; ALT-FLD×2 Avg. 64.4. Logic: 57.4 → 66.1 (ALT-FLD×2). RobustLR: 49.6 → 81.6. HumanEval: 32.3 → 42.6 (ALT-FLD in Table 4c); with ALT-FLD×2 HumanEval 42.6 (reported). BBH (3-shot): 60.4 → 65.4. Improvements up to ~32 points on specific logic benchmarks, up to 10 points on math/code benchmarks, and ~5 points on BBH were reported.",
            "comparison_with_baseline": "Fine-tuning with FLD×2 improved across many domains compared to base LLaMA-3.1-70B and compared to ALT on other synthetic corpora; best-performing ALT variant was ALT-FLD×2.",
            "key_findings": "Even very large pre-trained LLMs benefit substantially from targeted synthetic logic fine-tuning: improvements in strict logical reasoning (proof generation and entailment) and transfer to related domains were observed; FLD×2 produced the largest gains among synthetic corpora tested.",
            "limitations": "Pretrained knowledge is required for some tasks; without knowledge-preserving optimization models can forget facts when fine-tuned on synthetic unknown facts; remaining failures on knowledge-heavy questions (ARC challenge), some graduate-level physics (GPQA), and complex multi-choice reasoning indicate limits; sensitivity to superficial surface cues and premise order remain known failure modes (cited related work).",
            "uuid": "e7047.4",
            "source_info": {
                "paper_title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLaMA-3.1-8B",
            "name_full": "LLaMA-3.1 (8B parameters)",
            "brief_description": "Smaller variant of LLaMA-3.1 (8B parameters) used to test ALT benefits across model sizes; fine-tuned with ALT on different synthetic corpora to measure gains in logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B",
            "model_description": "Decoder-only transformer; smaller-capacity LLaMA-3.1 variant used for controlled ALT experiments.",
            "model_size": "8B parameters",
            "architecture_type": "decoder-only transformer",
            "training_data": "Pretrained LLaMA-3.1-8B (base) then fine-tuned on synthetic logic corpora (100k samples, 1 epoch) with Recall Adam to avoid forgetting.",
            "reasoning_method": "Supervised ALT yielding generation of intermediate logical steps; evaluated with 5-shot in-context learning on 31 benchmarks.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Same 31-benchmark suite as for 70B experiments",
            "benchmark_description": "See LLaMA-3.1-70B entry.",
            "task_type": "Multistep deductive reasoning, NLI, math, code, etc.",
            "performance_metric": "Accuracy / micro-average",
            "performance_value": "Examples from Table 2 (8B): baseline Avg. 47.9; ALT-FLD×2 Avg. 52.0; Logic: 42.8 → 52.2; Math ~39.6 → 43.2; Code 35.4 → 38.0. Gains smaller than 70B but consistent improvements were observed.",
            "comparison_with_baseline": "ALT on FLD×2 improved 8B model performance versus baseline and versus ALT on other corpora; ablating design principles reduced gains.",
            "key_findings": "ALT benefits are not limited to very large models — smaller LLaMA variants also improve, supporting the generality of synthetic logic fine-tuning.",
            "limitations": "Smaller capacity yields smaller absolute gains compared to 70B; same limitations as above regarding knowledge-dependent tasks and complex multi-step procedural QA.",
            "uuid": "e7047.5",
            "source_info": {
                "paper_title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation",
            "rating": 2
        },
        {
            "paper_title": "Learning deductive reasoning from synthetic corpus based on formal logic",
            "rating": 2
        },
        {
            "paper_title": "Program synthesis with large language models",
            "rating": 1
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        }
    ],
    "cost": 0.018963,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus</h1>
<p>Terufumi Morishita ${ }^{1}$ Gaku Morio ${ }^{1 *}$ Atsuki Yamaguchi ${ }^{2 * \dagger}$ Yasuhiro Sogawa ${ }^{1}$<br>${ }^{1}$ Advanced AI Innovation Center, Hitachi ${ }^{2}$ The University of Sheffield</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose Additional Logic Training (ALT), which aims to enhance LLMs' reasoning capabilities by programgenerated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named Formal Logic Deduction Diverse $\left(\mathrm{FLD}<em 2="2" _times="\times">{\times 2}\right)$, comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on FLD $</em>$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.</p>
<h2>1 Introduction</h2>
<p>Knowledge and reasoning have long been considered essential elements for achieving artificial intelligence (McCarthy, 1959; Weizenbaum, 1966; Winograd, 1971; Colmerauer and Roussel, 1973; Shortliffe, 1976; Elkan and Greiner, 1993). Knowledge refers to facts about the world, e.g., "objects with mass generate a gravitational field" and "the Earth has mass." Reasoning involves combining multiple facts according to specific rules to obtain new knowledge. For example, the new knowledge that "the Earth generates a gravitational field" can be derived from the aforementioned two facts.</p>
<p>Recent observations suggest that LLMs can solve problems using memorized knowledge of similar samples seen during pre-training, but they cannot solve novel, unknown problems that require reasoning (Hodel and West, 2023; Dasgupta et al., 2023; Zhang et al., 2024). For instance, LLMs can solve famous arithmetic problems as is but not when the numbers or names are changed (Razeghi et al., 2022; Mirzadeh et al., 2024), and they can solve coding tests from past years before the "knowledge cutoff" but not from the present year (Mitchell, 2023). This bias towards knowledge has been observed even in state-of-the-art LLMs such as GPT-4 (Liu et al., 2023b; Wu et al., 2023; Dziri et al., 2023).</p>
<p>LLMs' poor reasoning capabilities can stem from the lack of high-quality reasoning samples in the pre-training corpus, which primarily consists of human-written texts (Betz et al., 2021; Morishita et al., 2023). Indeed, reasoning samples in human-written texts often exhibit low quality, as evidenced by fallacies and biases commonly found in online debates (Hansson, 2004; Guiaşu and Tindale, 2018; Cheng et al., 2017). This is unsurprising given that humans usually think reflexively rather than through rigid reasoning (Kahneman, 2011; Sunstein and Hastie, 2015; Paglieri, 2017). Thus, a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The performance gains to LLaMA-3.1-70B by Additional Logic Training (ALT) on the proposed synthetic corpus, FLD×2 (Formal Logic Deduction Diverse). Each benchmark set, such as "Logic" and "Math", comprises various benchmarks in that domain. Tables 2, 4 shows the details.</p>
<p>Straightforward strategy to improve LLMs' reasoning capabilities is to prepare many high-quality reasoning samples and train LLMs on them.</p>
<p>We propose one such approach, Additional Logic Training (ALT), which utilizes high-quality samples of logical reasoning, the most fundamental form of reasoning. To prepare such samples, we utilize synthetic generation (Clark et al., 2021; Betz et al., 2021; Tafjord et al., 2021; Morishita et al., 2023), where computer programs generate deductive reasoning samples in which a given hypothesis is proven or disproven by combining given facts following rigid reasoning rules. We overview ALT in Figure 2.</p>
<p>In synthetic generation, computer programs generate samples according to pre-designed patterns, so this design largely determines the quality of these samples by nature. Thus, we start by discussing what is the ideal design for synthetic logic samples, incorporating symbolic logic theory and empirical findings (Section 2). The essence of logical reasoning lies in its ability to handle unknown facts, unlike knowledge, which deals solely with established facts, such as commonsense facts; therefore, samples must cover reasoning with unknown facts. Samples must include both illogical and logical reasoning to enable LLMs to distinguish between them. The samples must cover various patterns regarding a comprehensive set of reasoning aspects, such as reasoning rules and linguistic expressions of logical statements. We summarize these discussions into design principles, which guide the design of synthetic logic samples. Finally, based on these principles, we construct a synthetic corpus named Formal Logic Deduction Diverse (FLD×2), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors (Section 3).</p>
<p>We then empirically verify that ALT can enhance LLMs' reasoning capabilities (Sections 4, 5). Using 31 benchmarks covering diverse tasks, we observed that ALT on FLD×2 substantially boosts state-of-the-art LLMs' reasoning capabilities. Even LLaMA-3.1-70B, the largest LLM pre-trained on over 15 trillion tokens, shows substantial improvements with ALT (Figure 1). Among synthetic logic corpora with different sample designs, FLD×2 yielded the largest performance gains, validating our proposed design principles. Moreover, we discovered that employing a knowledge-forgetting prevention method during ALT is critically important, as it likely prevents the LLM's knowledge of established facts from being displaced by the unknown facts included in synthetic logic corpora.</p>
<p>Finally, we analyze which task-solving capabilities ALT can enhance and why (Section 6). We observed a substantial improvement of up to 30 points on logical reasoning tasks (Table 4a). Surprisingly, we also observed improvements in abductive reasoning tasks, which go beyond the synthetic logic corpora's original deductive reasoning tasks. Case analyses indicate that these improvements result from LLMs having acquired the fundamentals of the logic reflected in the design principles. We also observed improvements of up to 10 points on math and coding tasks, indicating the generalizability of the obtained reasoning capabilities (Tables 4b, 4c). We also observed improvements of up to 6 points on natural language inference (NLI) tasks (Table 4d). Case analyses suggest that LLMs successfully integrated the commonsense knowledge they had originally acquired during pre-training with the logical reasoning capabilities newly acquired from ALT.</p>
<p>Improvements across various other tasks (Table 4e) demonstrate the broad benefits of the obtained reasoning capabilities beyond standard reasoning tasks, though the modest improvements of up to 2 points indicate the need for future research on more effective application of these capabilities.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our proposed Additional Logic Training (ALT) aims to enhance LLMs' reasoning capabilities through training on many synthetically generated logical reasoning samples. Our sample generator (left) first generates a sample of multi-step deductive reasoning and then converts it into a deduction sample written in English (right). LLMs must generate logical steps to derive a given hypothesis from provided facts. The sample generator adheres to theoretically and empirically grounded design principles discussed in Section 2. Refer to Figure D. 3 for a real sample.</p>
<p>Our contributions are summarized as follows:</p>
<ul>
<li>We propose Additional Logic Training (ALT) and empirically verify that it can enhance the reasoning capability of state-of-the-art LLMs across various sizes, from 7B to 70B.</li>
<li>We establish systematic design principles for synthetic logic samples; then, we construct a synthetic corpus named Formal Logic Deduction Diverse (FLD 2), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. We empirically verify that Formal Logic Deduction Diverse indeed leads to the largest improvements among corpora with different sample designs.</li>
<li>We demonstrate that LLMs enhanced by ALT can solve not only the original logical reasoning tasks present in synthetic logic corpora but also other tasks, such as math and coding tasks, and notably NLI tasks, which require integrating knowledge and reasoning. This finding underscores the potential for advancing truly versatile AI possessing both knowledge and reasoning capabilities. We release the corpus, code, and the trained model under a permissive license ${ }^{1}$.</li>
</ul>
<h1>2 How Should Synthetic Logic Samples Be Designed?</h1>
<p>In synthetic generation, computer programs generate samples according to pre-designed patterns, so this design largely determines the quality of the samples. While Previous studies have examined several designs (Clark et al., 2021; Betz et al., 2021; Tafjord et al., 2021; Morishita et al., 2023), these designs were not systematically discussed, so they may not be the most effective ones.
Thus, we start by discussing how to optimally design synthetic logic samples. To this end, we consider symbolic logic theory as suggested by Morishita et al. (2023) and integrate empirical findings from previous studies. First, we observe that the essence of logical reasoning, based solely on the logical relationships between facts, lies in its ability to handle unknown facts, unlike knowledge, which by definition deals solely with established facts (Section 2.1). Therefore, we argue that samples should cover reasoning with unknown facts to represent this essential aspect of logical reasoning. We also observe that logical reasoning involves various other aspects, such as illogical reasoning, reasoning rules, and linguistic expressions that represent logical statements (sections 2.2 to 2.4). The samples should cover various patterns regarding these aspects to enable LLMs to solve various reasoning problems. We summarize these discussions into the following design principles, which guide the design of synthetic logic samples.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.1 Teaching Reasoning with Unknown Facts</h1>
<p>We first explore the essence of logical reasoning that differentiates itself from knowledge. Consider the following logical step:</p>
<p>The Earth orbits the Sun. If the Earth orbits the sun, the Earth has four seasons.
The Earth has four seasons.
This step is valid because the conclusion is logically derived from the two premises. Next, consider another logical step:</p>
<p>The Earth orbits the Sun. If the Earth orbits the sun, the Earth does not have four seasons.
The Earth does not have four seasons.
The second premise and consequently, the conclusion, is factually wrong. Nevertheless, if the premise was hypothetically correct, the conclusion could be logically derived. Therefore, step (2) is also logically valid. Finally:</p>
<ol>
<li>A Foo star exists. 2. If a Foo star exists, a Bar star also exists.</li>
</ol>
<p>A Bar star exists.
"Foo star" and "Bar star" are unknowns; nonetheless, we can still determine that step (3) is logically valid. Steps (1) to (3) above can be abstracted into a deduction rule, i.e., modus ponens, using symbols:</p>
<p>$$
\underline{\mathcal{F}} \quad \underline{\mathcal{F}} \rightarrow \underline{\mathcal{G}} \text { modus ponens }
$$</p>
<p>As we have seen, the logical validity of a deduction rule depends solely on whether the conclusion is logically derived from the premises, not on the factual correctness of the contents of $\mathcal{F}$ and $\mathcal{G}$. Therefore, the contents of $\mathcal{F}$ and $\mathcal{G}$ can be arbitrary.
Now, we consider what kind of samples would be needed to teach the deduction rule (4) to LLMs. We assume a task to generate the conclusion given the premises as prompt inputs. If the learner were human, they would be able to infer the underlying deduction rule (4) by observing samples such as (1) to (2). As a result, they would become able to solve the unknown problem (3).</p>
<p>However, from a purely inductive perspective, samples (1) to (2) cannot simply be generalized to the deduction rule (4). This is because the samples (1) to (2) themselves do not contain the information that the contents of $\mathcal{F}$ and $\mathcal{G}$ are arbitrary. In fact, one could generalize samples (1) to (2) to other rules; for example, the conclusion $\mathcal{G}$ can be derived if $\mathcal{F}$ and $\mathcal{F} \rightarrow \mathcal{G}$ are given as premises and $\mathcal{F}$ and $\mathcal{G}$ include 'Earth' as their contents. Innumerable such deduction rules can be inductively inferred from the given samples. In other words, induction has arbitrariness (Hume, 1748; Goodman, 1954; Quine, 1969).</p>
<p>Humans prefer simpler rules (Bertrand; Wittgenstein, 1922), so they boldly induce up to the deduction rule (4). However, it is unclear how purely inductive learners such as LLMs, which extract only what can be inferred from samples without prior preferences, induce up to (4). For example, if only specific contents such as "Alice is kind" and "Bob is smart" are assigned to $\mathcal{F}$ and $\mathcal{G}$ in training samples, an LLM could develop into a machine that generates the conclusion $\mathcal{G}$ only when the input contains the specific contents. In order for LLMs to accurately induce that $\mathcal{F}$ and $\mathcal{G}$ are indeed arbitrary:
Design Principle 1 (Reasoning with Unknown Facts). Prepare many samples assigning arbitrary contents to $\mathcal{F}$ and $\mathcal{G}$. They will make LLMs accurately induce $\mathcal{F}$ and $\mathcal{G}$ are indeed arbitrary, ultimately enabling them to reason with unknown facts.</p>
<h3>2.2 Teaching Illogical Reasoning</h3>
<p>Suppose we have LLMs trained on a large number of samples as follows:</p>
<p>$$
\underline{\mathcal{F} \wedge \mathcal{G}} \quad \frac{(\mathcal{F} \wedge \mathcal{G}) \rightarrow \mathcal{H}}{\mathcal{H}}
$$</p>
<p>where $\wedge$ denotes logical conjunction, and arbitrary contents are assigned to $\mathcal{F}, \mathcal{G}, \mathcal{H}$. Suppose that we give this LLM a problem such as:</p>
<p>$$
\underline{\mathcal{F}} \quad \frac{(\mathcal{F} \wedge \mathcal{G}) \rightarrow \mathcal{H}}{? ?}
$$</p>
<p>Since the premises are insufficient for logically deducting the conclusion, outputting nothing is the correct answer.</p>
<p>Unfortunately, an LLM could output $\mathcal{H}$, which was indeed often observed in our preliminary experiments. This is because while the LLMs can induce from sample (5) that it can generate the conclusion $\mathcal{H}$ when the two premises of (5) are given, the LLMs cannot induce from the sample that it is not allowed to generate the conclusion $\mathcal{H}$ when the premises of (6) are given, as such information is not included in the sample (5) itself. Therefore,</p>
<p>Design Principle 2 (Illogical Reasoning). Include negative samples such as (6). These samples will make LLMs induce that conclusions cannot be derived from insufficient premises.</p>
<h1>2.3 Teaching Diverse Reasoning Rules</h1>
<p>Deduction rules other than (4) exist:</p>
<p>$$
\begin{aligned}
&amp; \frac{(\mathcal{F} \wedge \mathcal{G})}{\mathcal{F}} \xrightarrow[\mathcal{G}]{\frac{(\mathcal{F} \wedge \mathcal{G})}{\mathcal{G}} \wedge \text { elimination }} \quad \frac{(\mathcal{F} \rightarrow \mathcal{G}) \wedge(\mathcal{G} \rightarrow \mathcal{H})}{\mathcal{F} \rightarrow \mathcal{H}} \text { syllogism } \
&amp; \frac{\mathcal{F} \rightarrow \mathcal{G}}{\neg \mathcal{G} \rightarrow \neg \mathcal{F}} \text { contraposition } \quad \frac{\neg(\mathcal{F} \vee \mathcal{G})}{\neg \mathcal{F} \wedge \neg \mathcal{G}} \xrightarrow[\neg \mathcal{F} \vee \neg \mathcal{G}]{\neg(\mathcal{F} \wedge \mathcal{G})} \text { De Morgan's laws }
\end{aligned}
$$</p>
<p>where $\vee$ denotes logical disjunction and $\neg$ negation. Since there are infinitely many possible logical formulas that can appear as premises and conclusions, there are infinitely many deduction rules. Providing LLMs with these infinite deduction rules is obviously intractable.</p>
<p>Instead of directly providing these infinite deduction rules, we can take another approach. Consider multi-step deductive reasoning (Figure 2 left), where multiple deduction rules derive a conclusion. Notice that the syllogism in (7) can be expressed by multi-step deductive reasoning using more "atomic" deduction rules. Indeed, there exists a set of atomic deduction rules called the axioms that satisfies the following:
Theorem 2.1 (Completeness of First-Order Predicate Logic Gödel (1930)). Any valid deduction rule can be expressed by multistep deductive reasoning constructed from the axioms.</p>
<p>In contrast to the axioms, the 'compound' deduction rules, such as syllogism, contraposition, and De Morgan's laws, are called theorems. According to the completeness Theorem 2.1, if we can handle the axioms, we can effectively handle other deduction rules as well. Indeed, Morishita et al. (2023) empirically verified that a language model trained on the axioms generalizes to handle other deduction rules more effectively than those trained on non-axiom deduction rules. Therefore,</p>
<p>Design Principle 3 (Diverse Reasoning Rules). Samples should express multi-step deduction constructed from the axioms. They will effectively teach LLMs diverse deduction rules (Morishita et al., 2023)</p>
<p>In multi-step deductive reasoning, the number of logical steps $s$ from premises to a conclusion can vary largely depending on the problem. Therefore:
Design Principle 3' (Diverse Reasoning Rules). Samples should include diverse numbers of logical steps $s$.</p>
<p>Ideally, this would be sufficient, but empirical evidence has shown that LLMs struggle with constructing multi-step deductive reasoning with large steps $s$ (Gontier et al., 2020; Morishita et al., 2023). Consequently, LLMs would not excel at handling theorems that require a large number of steps $s$ when expressed by the axioms. Therefore, as an additional countermeasure:</p>
<p>Design Principle 3" (Diverse Reasoning Rules). Samples should also include representative theorems, such as syllogism, contraposition, and De Morgan's laws.</p>
<h3>2.4 Teaching Diverse Linguistic Expressions that Represent Logical Statements</h3>
<p>There are various linguistic structures for expressing the logical relationship $\mathcal{F} \rightarrow \mathcal{G}$, such as "If $\mathcal{F}$ then $\mathcal{G}$ ", " $\mathcal{F}$ leads to $\mathcal{G}$ ", and " $\mathcal{F}$ results in $\mathcal{G}$ ". If we only include specific expressions in the corpora, LLMs may only learn to react to these specific expressions, which has been observed in previous experiments (Zhang et al., 2022; Yuan et al., 2023). To prevent this,</p>
<p>Design Principle 4 (Diverse Linguistic Expressions). Samples should include diverse linguistic expressions that represent logical statements.</p>
<p>In this chapter, we have established the principles to guide the design of synthetic logic samples. Next, we construct a synthetic logic corpus based on these principles.</p>
<p>Table 1: Synthetic logic corpora compared in this study, with their features categorized according to our proposed design principles (DP). Note that the last row of the ablation corpora lists variations of FLD_{×2}, each of which differs from the original regarding one of the design principles.</p>
<table>
<thead>
<tr>
<th></th>
<th>DP1</th>
<th>DP2</th>
<th>DP3</th>
<th>DP4</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>vocabulary size</td>
<td>distractors</td>
<td>deduction rules</td>
<td>logical steps</td>
</tr>
<tr>
<td>RuleTaker <em>Clark et al. (2021)</em></td>
<td>$\leq 100$</td>
<td>random</td>
<td>2</td>
<td>1–5</td>
</tr>
<tr>
<td>(RT)</td>
<td>(hand-selected)</td>
<td>formula</td>
<td>(implication)</td>
<td></td>
</tr>
<tr>
<td>PARARULE-Plus <em>Bao et al. (2022)</em></td>
<td>$\leq 100$</td>
<td>random</td>
<td>2</td>
<td>1–5</td>
</tr>
<tr>
<td>(PRP)</td>
<td>(hand-selected)</td>
<td>formula</td>
<td>(implication)</td>
<td></td>
</tr>
<tr>
<td>FLD <em>Morishita et al. (2023)</em></td>
<td>$\simeq 15$k</td>
<td>random</td>
<td>13</td>
<td>1–8</td>
</tr>
<tr>
<td></td>
<td>(WordNet, subset)</td>
<td>formula</td>
<td>(axioms)</td>
<td></td>
</tr>
<tr>
<td>FLD_{×2}</td>
<td>$\simeq 100$k</td>
<td>adversarial</td>
<td>$\simeq 50$</td>
<td>1–8</td>
</tr>
<tr>
<td></td>
<td>(WordNet, full)</td>
<td>formula</td>
<td>(axioms and theorems)</td>
<td></td>
</tr>
<tr>
<td>FLD_{×2}</td>
<td>100</td>
<td>not used</td>
<td>2 (implication)</td>
<td>1</td>
</tr>
<tr>
<td>ablation corpora $\rightarrow$</td>
<td>$\rightarrow w/o$ DP1</td>
<td>$\rightarrow w/o$ DP2</td>
<td>$\rightarrow w/o$ DP3_rules</td>
<td>$\rightarrow w/o$ DP3_steps</td>
</tr>
</tbody>
</table>
<p>3 Creating a Synthetic Corpus based on Design Principles</p>
<p>To prepare diverse samples reflecting the design principles 1 to 4 (DP1-4), we built a novel sample generator by extending the previous one by Morishita et al. (2023) and then generated the synthetic logic corpus named FLD_{×2} (Formal Logic Deduction Diverse). Figure 2 shows a schematic of our generator and a deduction sample. Table 1 compares FLD_{×2} with existing corpora. Figure D.3 provides an actual deduction sample included in FLD_{×2}.</p>
<p>More specifically, our generator generates deduction samples through the following steps. First, the generator randomly generates a sample of multi-step deductive reasoning written in logical formulas, as shown on the left side of Figure 2, where a conclusion is derived from premises using multiple deduction rules (See Appendix D.3 for more details of this generation procedure). At this time, the generator also generates 'distractor' logical formulas, which express negative premises of DP2. Next, the generator converts each logical formula into English expressions. To achieve this, the generator first randomly selects a template from pre-defined options, such as “If $\mathcal{F}$, then $\mathcal{G}$,” “$\mathcal{F}$ leads to $\mathcal{G}$,” or “$\mathcal{F}$ results in $\mathcal{G}$,” for the logical formula “$\mathcal{F} \rightarrow \mathcal{G}$.” It then assigns English content randomly constructed from a vocabulary, such as “(that) a Foo star exists” and “(that) a Bar star exists,” to each symbol, such as $\mathcal{F}$ and $\mathcal{G}$. Finally, it converts the multi-step deduction into a deduction sample (right side of Figure 2) by using the premises as ‘facts’, the conclusion as ‘hypothesis’, and the intermediate logical steps as ‘logical steps’. The deduction sample requires LLMs to generate logical steps that derive a given hypothesis based on the given facts.</p>
<p>Table 1 outlines the comparison of FLD_{×2} with other existing corpora (Clark et al., 2021; Bao et al., 2022; Morishita et al., 2023) in terms of DP1-4, which is detailed as follows:</p>
<ul>
<li>DP1: We assign $\mathcal{F}$ and $\mathcal{G}$ content randomly constructed from a vocabulary. While the existing corpora used small-sized vocabulary of up to 15k, we use a large vocabulary of around 100k words built from WordNet (Miller, 1995). This will teach LLMs that $\mathcal{F}$ and $\mathcal{G}$ are truly arbitrary, ultimately enabling them to reason with unknown facts.</li>
<li>DP2: The existing corpora used randomly generated logical formulas as distractors. In contrast, we implement adversarial distractors. For example, for a premise $\mathcal{F} \wedge \mathcal{G}$, we use $\mathcal{F}$ with missing information (see Equations (5), (6)), and for a premise $\mathcal{F} \rightarrow \mathcal{H}$, we use $\mathcal{F} \wedge \mathcal{G} \rightarrow \mathcal{H}$ with missing information as distractors. These distractors teach LLMs precisely when a conclusion can and cannot be derived. As with previous corpora, we include a variable number of distractors in each sample, randomly chosen from a range of 0 to 20.</li>
<li>DP3-3": While the existing corpora used a small number of deduction rules of up to 13 (refer to Figure B.4 of Morishita et al. (2023)), we include diverse deduction rules, encompassing the axioms and representative theorems, such as modus ponens, syllogisms, and contraposition, totaling about 50 rules. We include samples with up to $s=8$ logical steps, following (Morishita et al., 2023).</li>
<li>DP4: We manually craft several more English templates per logical formulas than those used in FLD. Since the templates have a nested structure, they yield combinatorially more diverse English expressions. While counting the exact number of the resulting expressions is intractable, we observed at least dozens of expressions per logical formula, including minor variations. See Appendix D.4 for details.</li>
</ul>
<p>4 Experimental Setup</p>
<p>We briefly explain the experimental settings. Refer to Appendix E for the details.
Synthetic Logic Corpora: We examine the proposed FLD_{×2} and previous corpora (Table 1).
LLMs: We used the state-of-the-art LLM, LLaMA-3.1 (8B and 70B) (AI@Meta, 2024).
Training Settings: We trained the LLMs by a method similar to supervised fine-tuning; as illustrated in Figure 2, we used the facts and hypothesis as inputs and logical steps and additional answer label (see Appendix D.1) as outputs. We excluded loss computation for the inputs to prevent LLMs from learning to generate unknown facts. We trained the LLMs for 1 epoch on 100k samples ( 0.1B tokens) from the training split of each corpus, with a batch size of 256, resulting in 390 steps, with a linear warmup for 200 steps. We used the learning rate of 2e-05 for the 8B model and 3e-06 for the 70B model. We used Huggingface <em>Wolf et al. (2020)</em> for implementation.
Prevention of Knowledge Forgetting by Recall Adam Optimizer: Synthetic logic corpora include many samples with unknown facts, so training on them should cause LLMs to forget their knowledge of existing facts. To prevent this, we employed the Recall Adam optimizer <em>Chen et al. (2020)</em>, which regularizes parameter updates to avoid deviating too far from the pre-training parameters. Recall Adam stands out for LLM training for several reasons (see Appendix E.0.1 for details). We used our re-implemented version . The hyperparameters were: $\beta_{1}=0.9, \beta_{2}=0.999, \epsilon=$ $10^{-6}$, fisher coefficient $=4000$ for the 8 B model and 2000 for the 70 B model.</p>
<p>Benchmarks: We evaluated the trained LLMs on 31 benchmarks shown in Table E. 7 using 5-shot in-context learning, except for BBH and AbuductionRules, which used 3-shot in-context learning. These benchmarks cover a wide range of tasks and are prominent in LLM evaluation. Note that we excluded the synthetic logic corpora used for training, as training on them often leads to overfitting to their superficial and statistical cues <em>Zhang et al. (2022); Yuan et al. (2023)</em>, failing to measure truly generalizable reasoning capabilities. We used lm-evaluation-harness <em>Gao et al. (2023)</em> and bigcode-evaluation-harness <em>Ben Allal et al. (2022)</em> for the implementation.</p>
<h1>5 Can Additional Logic Training Enhance LLMs’ Capabilities?</h1>
<p>Table 2 show the performance of LLMs before and after ALT. Most LLMs trained with ALT outperformed their counterparts without ALT. Notably, ALT yielded substantial gains of up to 10 points even for LLaMA-3.1-70B, the largest LLM pre-trained on over 15 trillion tokens. These results verify that ALT can enhance the capabilities of state-of-the-art LLMs.
Among the LLMs trained with ALT, the one trained on FLD $<em 2="2" x="x">{x 2}$ (i.e., $\oplus \mathbf{A L T}-\mathrm{FLD}</em>$ corpora, each of which lacks one of the design principles. As seen, ablating any design principle almost always led to performance degradation. These results demonstrate that the proposed design principles are critical to obtaining the maximum possible gain from ALT, and each principle is indispensable.
Table F. 8 shows that the LLMs trained with ALT without preventing knowledge forgetting by Recall Adam optimizer underperformed compared to their counterparts trained with knowledge forgetting prevention and even the LLM without ALT. This behavior presumably occurred because the unknown facts included in synthetic logic corpora displaced the LLM’s knowledge of existing facts. Therefore, knowledge-forgetting prevention is critically important for the success of ALT.}$ ) achieved the highest generalization performance across the benchmarks. Table 3 shows the performance of the LLMs trained on ablated FLD $_{x 2</p>
<h2>6 What Capabilities Can Additional Logic Training Enhance and Why?</h2>
<p>We analyze the results on each benchmark or each case and discuss whether and why the LLM's capabilities to solve the tasks can or cannot be enhanced by ALT.</p>
<h3>6.1 Logical Reasoning Tasks</h3>
<p>Table 4a shows that ALT substantially boosted LLaMA-3.1-70B’s performance by up to 30 points on various benchmarks dealing with logical reasoning tasks. Surprisingly, we also observed improvements on abductive reasoning tasks, which go beyond the original deductive reasoning tasks</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: 5-shot performance of LLMs before and after ALT. $\oplus$ ALT- $x$ denotes the LLM trained with ALT on the synthetic logic corpus $x$ from Table 1. The color shows the rank in each column (darker is better). Each benchmark set, such as "Logic" and "Math", comprises various benchmarks in that domain (see Table E.7). "Avg." represents the micro-average of all the benchmarks.
(a) LLaMA-3.1-8B.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
<th style="text-align: center;">Logic</th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">NLI</th>
<th style="text-align: center;">Others</th>
<th style="text-align: center;">BBH (3-shot)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BBH (0-shot)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pro</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-3.1-8B</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-PRP</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">35.3</td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-RT</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">35.7</td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-FLD</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">36.2</td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-FLD $\times 2$</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">36.4</td>
</tr>
</tbody>
</table>
<p>(b) LLaMA-3.1-70B.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
<th style="text-align: center;">Logic</th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">NLI</th>
<th style="text-align: center;">Others</th>
<th style="text-align: center;">BBH (3-shot)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BBH (0-shot)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pro</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-3.1-70B</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">50.7</td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-PRP</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-RT</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">52.4</td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-FLD</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">54.4</td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-FLD $\times 2$</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">54.4</td>
</tr>
</tbody>
</table>
<p>Table 3: LLaMA-3.1-8B trained on the ablation corpora.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
<th style="text-align: center;">Logic</th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">NLI</th>
<th style="text-align: center;">Others</th>
<th style="text-align: center;">BBH (3-shot)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BBH (0-shot)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pro</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\oplus$ ALT-FLD $\times 2$</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: center;">w/o DP1</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">36.1</td>
</tr>
<tr>
<td style="text-align: center;">w/o DP2</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">35.7</td>
</tr>
<tr>
<td style="text-align: center;">w/o DP3.rules</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">36.2</td>
</tr>
<tr>
<td style="text-align: center;">w/o DP3.steps</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">36.3</td>
</tr>
<tr>
<td style="text-align: center;">w/o DP4</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">36.3</td>
</tr>
</tbody>
</table>
<p>in synthetic logic corpora. Abductive reasoning involves guessing the missing premises that caused the observed conclusion rather than deriving a conclusion from the premises. For example, from the observed conclusion, "the window glass at home was broken and the room was ransacked," we guess the premise "a burglar broke in." The improvements would be due to the fact that, while the surface form of abductive reasoning problems differs from that of deductive reasoning, they share the fundamentals of logic reflected in the design principles.</p>
<p>Next, we conduct case analyses to see whether the LLM enhanced by ALT acquired the abilities intended by the proposed design principles (DP1-4). Table 5 shows problems where LLaMA-3.170 B 's errors have been corrected by ALT. The first problem is very simple, so it is surprising that LLaMA-3.1-70B failed to solve it, indicating the inherent difficulty of learning logical reasoning solely from pre-training. In contrast, $\oplus \mathrm{ALT}-\mathrm{FLD}<em 2="2" _times="\times">{\times 2}$, which was additionally trained on $\mathrm{FLD}</em>}$, solved the problem correctly. The premises of the problem are randomly constructed to express unknown facts. Therefore, the result suggests that $\oplus \mathrm{ALT}-\mathrm{FLD<em 2="2" _times="\times">{\times 2}$ acquired genuine logical reasoning ability, which can handle unknown facts (DP1).
In the second problem, $\oplus \mathrm{ALT}-\mathrm{FLD}</em>$ correctly answered "neutral", indicating that it successfully learned that conclusions cannot be derived from insufficient facts (DP2).
The third problem comes from the FOLIO benchmark. To solve this problem, LLMs must use syllogism at the first step as follows: "All eels are fish, and no fish are plants. Therefore, all ells are not plants." $\oplus \mathrm{ALT}-\mathrm{FLD}_{\times 2}$ answered this problem correctly, suggesting that it successfully learned diverse deduction rules (DP3).</p>
<p>FOLIO problems are created based on Wikipedia topics, describing them in more natural and realistic linguistic expressions than in other benchmarks. As seen in the fourth problem, $\oplus \mathrm{ALT}-\mathrm{FLD}_{\times 2}$ understands such expressions, suggesting the effect of diverse expressions from DP4 and/or that LLMs can integrate their original linguistic ability with the newly acquired logical reasoning ability.</p>
<p>Table 4: Benchmark-wise 5-shot performance of LLaMA-3.1-70B before and after ALT on FLD_{x:2}. Refer to Table F.9 for LLaMA-3.1-8B results. Table E.7 details each benchmark.</p>
<table>
<thead>
<tr>
<th>bAbiD FOLIO LogicNLI RobustLR AR-LSAT LogiQA ReClor AbductionR ART</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA-3.1-70B</td>
<td>83.8_{±1.2}</td>
<td>58.9_{±1.6}</td>
<td>34.9_{±1.1}</td>
<td>49.6_{±0.9}</td>
<td>21.5_{±1.0}</td>
<td>64.3_{±1.2}</td>
<td>33.7_{±0.7}</td>
<td>84.0_{±0.7}</td>
</tr>
<tr>
<td>@ALT-FLD_{x:2}</td>
<td>83.5_{±0.5}</td>
<td>66.7_{±0.6}</td>
<td>50.9_{±0.5}</td>
<td>81.6_{±0.3}</td>
<td>25.0_{±0.4}</td>
<td>69.4_{±0.5}</td>
<td>36.3_{±0.3}</td>
<td>95.7_{±0.2}</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>(b) Math.</p>
<table>
<thead>
<tr>
<th>GSM8k</th>
<th>MATH MathQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoT CoT (0-shot)</td>
<td>-</td>
</tr>
<tr>
<td>LLaMA-3.1-70B</td>
<td>80.9_{±1.1}</td>
</tr>
<tr>
<td>@ALT-FLD_{x:2}</td>
<td>83.3_{±0.4}</td>
</tr>
<tr>
<td>80.9_{±0.4}</td>
<td>75.2_{±1.2}</td>
</tr>
<tr>
<td>80.4_{±0.4}</td>
<td>73.0_{±0.5}</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>(c) Code.</p>
<table>
<thead>
<tr>
<th>HumanEval MBPP MBPP+ MultiPL-E (cpp) MultiPL-E (go)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA-3.1-70B</td>
<td>32.3</td>
<td>43.4</td>
<td>48.7</td>
<td>29.8</td>
<td>76.6</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>@ALT-FLD_{x:2}</td>
<td>42.6</td>
<td>49.5</td>
<td>52.5</td>
<td>38.7</td>
<td>78.6</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>(d) Natural language inference (NLI).</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: Problems where LLaMA-3.1-70B initially answered incorrectly and then correctly after training with ALT on FLD_{×2}. Red highlights the premises related to the hypothesis.</p>
<table>
<thead>
<tr>
<th>benchmark</th>
<th>premises</th>
<th>hypothesis</th>
<th>answer (LLaMA-3.1-70B/gold)</th>
<th>required ability</th>
</tr>
</thead>
<tbody>
<tr>
<td>LogicNLI</td>
<td>Mice are afraid of wolves. Cats are afraid of sheep. Jessica is a cat. Wolves are afraid of cats. Winona is a wolf. Sheep are afraid of cats.</td>
<td>Jessica is afraid of sheep.</td>
<td>neutral/ entailment</td>
<td>DP1</td>
</tr>
<tr>
<td></td>
<td>Rhett is not modest. Vivian is confused. Rhett is lazy. If someone is modest or not confused, then he is not eager.</td>
<td>Rhett is confused.</td>
<td>entailment/ neutral</td>
<td>DP2</td>
</tr>
<tr>
<td>FOLIO</td>
<td>All eels are fish. No fish are plants. Everything displayed in the collection is either a plant or an animal. All animals displayed in the collection are multicellular. A sea eel is displayed in the collection. The sea eel is an eel or an animal or not a plant.</td>
<td>The sea eel is multicellular or is bacteria.</td>
<td>neutral/ entailment</td>
<td>DP3</td>
</tr>
<tr>
<td></td>
<td>Common utilities include water, electricity, gas, heating, sewer, trash, and recycling. Many apartment rents cover the cost of water and electricity. Susan lives in an apartment where the rent covers all utilities. The rent of the apartment where Ava lives does not cover any utility expenses. Noah lives in an apartment where the rent does not cover heating.</td>
<td>Noah and Ava both need to pay the heating bill.</td>
<td>neutral/ entailment</td>
<td>DP4</td>
</tr>
<tr>
<td>SNLI</td>
<td>An Indian woman is dancing with her partner.</td>
<td>A woman is moving.</td>
<td>neutral/ entailment</td>
<td>reasoning with</td>
</tr>
<tr>
<td></td>
<td>This church choir sings to the masses as they sing joyous songs from the book at a church.</td>
<td>A choir is singing at a baseball game.</td>
<td>entailment/ contradiction</td>
<td>commonsense knowledge</td>
</tr>
</tbody>
</table>
<p>Table 6: Problems that LLaMA-3.1-70B trained with ALT on FLD_{×2} still cannot solve.</p>
<table>
<thead>
<tr>
<th>benchmark</th>
<th>question</th>
<th>answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>ARC (challenge)</td>
<td>The end result in the process of photosynthesis is the production of sugar and oxygen. Which step signals the beginning of photosynthesis?</td>
<td>Chlorophyll in the leaf captures light energy.</td>
</tr>
<tr>
<td>GPQA</td>
<td>A spin-half particle is in a linear superposition $0.8</td>
<td>\uparrow\rangle+0.6</td>
</tr>
<tr>
<td>ARC (challenge)</td>
<td>Beavers build their homes in ponds and streams. Which characteristic is least critical to building homes in an aquatic environment?</td>
<td>(A) waterproof fur (B) webbed hind feet (C) arge, sharp teeth (D) flat, wide tail</td>
</tr>
</tbody>
</table>
<h1>6.4 Other Tasks</h1>
<p>Improvements across various other tasks (Table 4 e ) demonstrate the broad benefits of the obtained reasoning capabilities beyond standard reasoning tasks; though the improvements were modest at up to 2 percentage points, which may be due to the following reasons. First, these benchmarks include problems that purely test knowledge, such as the first one in Table 6 Since ALT does not aim to provide new knowledge, the ability to solve such problems does not improve by nature. Next, some problems may require knowledge that is too advanced for LLMs, so potential improvements by the enhanced reasoning capabilities may be bottlenecked. For example, the second problem does involve reasoning but requires sufficient quantum mechanics knowledge as a prerequisite. However, these knowledge-related issues should be solved by improving the quantity and quality of pre-training.</p>
<p>Finally, LLMs may not be able to fully utilize the potential of enhanced reasoning capabilities for problems that require complex procedures. To solve the third problem, LLMs first must attempt reasoning related to each choice as follows: "To build homes in an aquatic environment, one needs to maintain body heat and insulation despite being frequently submerged in cold water. Therefore, the waterproof fur of (A) is essential", and "To build ..., one must gather and process natural materials like wood. Large, sharp teeth of (C) are critical as they allow beavers to cut down trees and shape branches." Next, while reasoning traces on (A) to (D) all seem reasonable, LLMs must choose the single best answer, considering the subtle nuance of the question context, as follows: "Since the question emphasizes the aquatic environment, the least related reasoning trace should be (C)." This complex procedure contrasts with logical reasoning and NLI problems, where LLMs can directly obtain an answer from a single reasoning trace. Previous studies also observed that such procedure on multiple-choice QA problems are challenging for LLMs (Robinson and Wingate, 2023; Zheng et al., 2024; Wang et al., 2024a). Since ALT alone does not teach LLMs such task-specific procedures, additional training on these procedures should be necessary to solve these problems.</p>
<h2>7 Conclusion</h2>
<p>Towards versatile artificial intelligence with reasoning capabilities, we proposed Additional Logic Training on synthetic logic samples. We established systematic design principles well-grounded on symbolic logic theory and previous empirical findings. We constructed a corpus named Formal Logic Deduction Diverse $\left(\mathrm{FLD}<em 2="2" _="×">{× 2}\right)$ based on the design principles. We empirically showed that ALT on FLD $</em>$ substantially enhances the capabilities of state-of-the-art LLMs.</p>
<h1>Acknowledgement</h1>
<p>Computational resources of AI Bridging Cloud Infrastructure (ABCI) provided by the National Institute of Advanced Industrial Science and Technology (AIST) were used. We thank Dr. Masaaki Shimizu at Hitachi for the convenience of additional computational resources. We thank Dr. Naoaki Okazaki, a professor at the Tokyo Institute of Technology, for the keen comments.</p>
<h2>References</h2>
<p>AI@Meta. 2024. Llama 3 model card.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operationbased formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, and Mitsuhiro Okada. 2023. Evaluating large language models with NeuBAROCO: Syllogistic reasoning ability and human-like biases. In Proceedings of the 4th Natural Logic Meets Machine Learning Workshop, pages 1-11, Nancy, France. Association for Computational Linguistics.</p>
<p>Yoichi Aoki, Keito Kudo, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi, and Kentaro Inui. 2024. First heuristic then rational: Dynamic use of heuristics in language model reasoning.</p>
<p>Amanda Askell. 2020. Gpt-3: Towards renaissance models. Daily Nous Blog: Philosophers On GPT-3.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.</p>
<p>Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, and Jiamou Liu. 2022. Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation. In Proceedings of the 16th International Workshop on NeuralSymbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning \&amp; Reasoning (IJCLR 2022), pages 202-217, Cumberland Lodge, Windsor Great Park, United Kingdom.</p>
<p>Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024. Cosmopedia.</p>
<p>Loubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von Werra. 2022. A framework for the evaluation of code generation models. https://github. com/bigcode-project/bigcode-evaluation-harness.</p>
<p>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In Text Analysis Conference.</p>
<p>Leonardo Bertolazzi, Albert Gatt, and Raffaella Bernardi. 2024. A systematic analysis of large language models as soft reasoners: The case of syllogistic inferences.</p>
<p>Russell Bertrand. A history of western philosophy.
Gregor Betz, Christian Voigt, and Kyle Richardson. 2021. Critical thinking for language models. In Proceedings of the 14th International Conference on Computational Semantics (IWCS), pages 63-75, Groningen, The Netherlands (online). Association for Computational Linguistics.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. 2019. Abductive commonsense reasoning. arXiv preprint arXiv:1908.05739.</p>
<p>Neeladri Bhuiya, Viktor Schlegel, and Stefan Winkler. 2024. Seemingly plausible distractors in multi-hop reasoning: Are large language models attentive readers?</p>
<p>Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural language deductions. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6266-6278, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference.</p>
<p>Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2023. Multipl-e: A scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):36753691 .</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.</p>
<p>Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. 2020. Recall and learn: Fine-tuning deep pretrained language models with less forgetting. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7870-7881, Online. Association for Computational Linguistics.</p>
<p>Xinyun Chen, Ryan Andrew Chi, Xuezhi Wang, and Denny Zhou. 2024. Premise order matters in reasoning with large language models. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 6596-6620. PMLR.
J. Cheng, M. Bernstein, C. Danescu-Niculescu-Mizil, and J. Leskovec. 2017. Anyone can become a troll: Causes of trolling behavior in online discussions. CSCW: Proceedings of the Conference on Computer-Supported Cooperative Work. Conference on Computer-Supported Cooperative Work, 2017.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 3882-3890.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.
A. Colmerauer and P Roussel. 1973. The birth of prolog. The ALP Newsletter.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. pages 177-190.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358-7370, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Hannah R. Sheahan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill. 2023. Language models show human-like content effects on reasoning tasks.</p>
<p>John Dougrez-Lewis, Mahmud Elahi Akhter, Yulan He, and Maria Liakata. 2024. Assessing the reasoning abilities of chatgpt in the context of claim verification.</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality.</p>
<p>Tiwalayo Eisape, Michael Tessler, Ishita Dasgupta, Fei Sha, Sjoerd Steenkiste, and Tal Linzen. 2024. A systematic comparison of syllogistic reasoning in humans and language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8425-8444, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Charles Elkan and Russell Greiner. 1993. Building large knowledge-based systems: Representation and inference in the cyc project: Db lenat and rv guha.</p>
<p>Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1-9.</p>
<p>Kurt Gödel. 1930. Uber die vollständigkeit des logikkalküls. Ph.D. thesis, Ph. D. dissertation, University of Vienna.</p>
<p>Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. 2020. Measuring systematic generalization in neural proof generation with transformers. Advances in Neural Information Processing Systems, 33:22231-22242.</p>
<p>Nelson Goodman. 1954. Fact, fiction, and forecast. london: University of london.
Radu Cornel Guiaşu and Christopher W Tindale. 2018. Logical fallacies and invasion biology. Biology \&amp; philosophy, 33(5-6):34.</p>
<p>Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018. The argument reasoning comprehension task: Identification and reconstruction of implicit warrants. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1930-1940, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Pengrui Han, Peiyang Song, Haofei Yu, and Jiaxuan You. 2024. In-context learning may not elicit trustworthy reasoning: A-not-b errors in pretrained language models.</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. 2022. Folio: Natural language reasoning with first-order logic. arXiv e-prints, pages arXiv-2209.</p>
<p>Sven Ove Hansson. 2004. Fallacies of risk. Journal of Risk Research, 7(3):353-360.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. NeurIPS.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023. Large language models are reasoning teachers.
Damian Hodel and Jevin West. 2023. Response: Emergent analogical reasoning in large language models.</p>
<p>Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, and Changshui Zhang. 2024. A closer look at the self-verification abilities of large language models in logical reasoning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 900-925, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Peng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, and Shujian Huang. 2024. Large language models are limited in out-of-context knowledge reasoning.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations.</p>
<p>David Hume. 1748. An enquiry concerning human understanding (section iv). Recuperado de http://www. clorenzano. com. ar.</p>
<p>Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, and Dan Roth. 2024a. A peek into token bias: Large language models are not yet genuine reasoners.</p>
<p>Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin Cao, Liangcai Gao, and Zhi Tang. 2024b. Logicpro: Improving complex logical reasoning via program-guided learning.</p>
<p>Daniel Kahneman. 2011. Thinking, fast and slow. Macmillan.
Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. 2023. Measuring faithfulness in chain-of-thought reasoning.</p>
<p>Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023. Symbolic chain-of-thought distillation: Small models can also "think" step-by-step. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2665-2679, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan. 2022. Explanations from large language models make small reasoners better.</p>
<p>Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang. 2023a. Logiqa 2.0—an improved dataset for logical reasoning in natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:2947-2962.</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023b. Evaluating the logical reasoning ability of chatgpt and gpt-4.</p>
<p>Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang. 2023c. LogiCoT: Logical chain-of-thought instruction tuning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2908-2921, Singapore. Association for Computational Linguistics.</p>
<p>Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 3622-3628. International Joint Conferences on Artificial Intelligence Organization. Main track.</p>
<p>Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023d. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, and Jieyu Zhao. 2024. Self-contradictory reasoning evaluation and detection.</p>
<p>Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. 2024. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms.</p>
<p>YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2024. At which training stage does code data help LLMs reasoning? In The Twelfth International Conference on Learning Representations.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2023. Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1773-1781, Toronto, Canada. Association for Computational Linguistics.</p>
<p>John W. McCarthy. 1959. Programs with common sense. In Proc. Tedding Conf. on the Mechanization of Thought Processes, pages 75-91.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.</p>
<p>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41.</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models.</p>
<p>Melanie Mitchell. 2023. Can large language models reason? blog, pages https://aiguide.substack.com/p/can-large-language-models-reason.</p>
<p>Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023. Orca 2: Teaching small language models how to reason.</p>
<p>Philipp Mondorf and Barbara Plank. 2024. Liar, liar, logical mire: A benchmark for suppositional reasoning in large language models.</p>
<p>Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. 2023. Learning deductive reasoning from synthetic corpus based on formal logic. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 25254-25274. PMLR.</p>
<p>Terufumi Morishita, Atsuki Yamaguchi, Gaku Morio, Hikaru Tomonari, Osamu Imaichi, and Yasuhiro Sogawa. 2024. JFLD: A Japanese benchmark for deductive reasoning based on formal logic. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 9526-9535, Torino, Italia. ELRA and ICCL.</p>
<p>Aliakbar Nafar, K. Brent Venable, and Parisa Kordjamshidi. 2024. Teaching probabilistic logical reasoning to transformers. In Findings of the Association for Computational Linguistics: EACL 2024, pages 1615-1632, St. Julian's, Malta. Association for Computational Linguistics.</p>
<p>Kentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, and Mitsuhiro Okada. 2024. Exploring reasoning biases in large language models through syllogism: Insights from the NeuBAROCO dataset. In Findings of the Association for Computational Linguistics ACL 2024, pages 16063-16077, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.</p>
<p>Fabio Paglieri. 2017. A plea for ecological argument technologies. Philosophy \&amp; Technology, 30(2):209-238.</p>
<p>Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, and Chitta Baral. 2024. LogicBench: Towards systematic evaluation of logical reasoning ability of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13679-13707, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, and Chitta Baral. 2024. Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models.</p>
<p>Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, Jian-Guang Lou, and Weizhu Chen. 2022. Reasoning like program executors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 761-779, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Willard Van Orman Quine. 1969. Epistemology naturalized. ontological relativity and other essays. New York: Columbia UP.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840-854.</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. 2023. Gpqa: A graduate-level google-proof q\&amp;a benchmark. arXiv preprint arXiv:2311.12022.</p>
<p>Joshua Robinson and David Wingate. 2023. Leveraging large language models for multiple choice question answering. In The Eleventh International Conference on Learning Representations.</p>
<p>Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching soft rules to pre-trained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1460-1476, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. 2020. PRover: Proof generation for interpretable reasoning over rules. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 122-136, Online. Association for Computational Linguistics.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106.</p>
<p>Soumya Sanyal, Zeyi Liao, and Xiang Ren. 2022a. Robustlr: Evaluating robustness to logical perturbation in deductive reasoning. arXiv preprint arXiv:2205.12598.</p>
<p>Soumya Sanyal, Harman Singh, and Xiang Ren. 2022b. Fairr: Faithful and robust deductive reasoning over natural language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1075-1093.
eh Shortliffe. 1976. Computer based medical consultations: Mycin. Elsevier.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023. Distilling reasoning capabilities into smaller language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7059-7073, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Damien Sileo. 2024. Scaling synthetic logical reasoning datasets with context-sensitive declarative grammars.</p>
<p>Zayne Rea Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2024. MuSR: Testing the limits of chain-of-thought with multistep soft reasoning. In The Twelfth International Conference on Learning Representations.</p>
<p>Cass R Sunstein and Reid Hastie. 2015. Wiser: getting beyond groupthink to make groups smarter. Harvard Business Review Press, Boston.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937.</p>
<p>Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. 2021. Diagnosing the first-order logical reasoning ability through logicnli. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3738-3747.</p>
<p>Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. 2024. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476-482.</p>
<p>Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting.</p>
<p>Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, and Yutaka Matsuo. 2024. Which programming language and what features at pre-training stage affect downstream logical inference performance?</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30.</p>
<p>Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen tse Huang, Pinjia He, Wenxiang Jiao, and Michael R. Lyu. 2024. Logicasker: Evaluating and improving the logical reasoning ability of large language models.</p>
<p>Haochun Wang, Sendong Zhao, Zewen Qiang, Bing Qin, and Ting Liu. 2024a. Beyond the answers: Reviewing the rationality of multiple choice question answering for the evaluation of large language models.</p>
<p>Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-consistent chain-of-thought distillation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5546-5558, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. 2024b. Can LLMs reason with rules? logic scaffolding for stress-testing and improving LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7523-7543, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024c. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark (published at neurips 2024 track datasets and benchmarks).</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Joseph Weizenbaum. 1966. Eliza-a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9(1):36-45.</p>
<p>Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94-106, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of NAACL-HLT, pages 1112-1122.</p>
<p>T Winograd. 1971. Procedures as a representation for data in a computer program for understanding natural language, mit ai technical report 235 .</p>
<p>Ludwig Wittgenstein. 1922. Tractatus Logico Philosophicus: Logical-Philosophical Treatise. Really Simple Media.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45.</p>
<p>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.</p>
<p>Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, and Johan Bos. 2019. Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning. arXiv preprint arXiv:1904.12166.</p>
<p>Nathan Young, Qiming Bao, Joshua Bensemann, and Michael J Witbrock. 2022. Abductionrules: Training transformers to explain unexpected inputs. In Findings of the Association for Computational Linguistics: ACL 2022, pages 218-227.</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations (ICLR).</p>
<p>Zhangdie Yuan, Songbo Hu, Ivan Vulić, Anna Korhonen, and Zaiqiao Meng. 2023. Can pretrained language models (yet) reason deductively? In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1439-1454.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800.</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. 2022. On the paradox of learning to reason from data.</p>
<p>Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue. 2024. A careful examination of large language model performance on grade school arithmetic.</p>
<p>Jun Zhao, Jingqi Tong, Yurong Mou, Ming Zhang, Qi Zhang, and Xuanjing Huang. 2024a. Exploring the compositional deficiency of large language models in mathematical reasoning.</p>
<p>Wenting Zhao, Justin Chiu, Jena Hwang, Faeze Brahman, Jack Hessel, Sanjiban Choudhury, Yejin Choi, Xiang Li, and Alane Suhr. 2024b. UNcommonsense reasoning: Abductive reasoning about uncommon situations. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8487-8505, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations.</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2021. Ar-lsat: Investigating analytical reasoning of text. arXiv preprint arXiv:2104.06598.</p>
<p>Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy, Kilian Q Weinberger, and Yuhuai Wu. 2024a. Don't trust: Verify - grounding LLM quantitative reasoning with autoformalization. In The Twelfth International Conference on Learning Representations.</p>
<p>Yue Zhou, Yada Zhu, Diego Antognini, Yoon Kim, and Yang Zhang. 2024b. Paraphrase and solve: Exploring and exploiting the impact of surface form on mathematical reasoning in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 2793-2804, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. 2024. Dyval: Dynamic evaluation of large language models for reasoning tasks. In The Twelfth International Conference on Learning Representations.</p>
<h1>A Related Work</h1>
<h2>A. 1 Investigation of Reasoning Capabilities of LLMs</h2>
<p>Many studies examine LLMs' reasoning capabilities (Askell, 2020; Rae et al., 2021; Razeghi et al., 2022; Liu et al., 2023b; Turpin et al., 2023; Lanham et al., 2023; Wu et al., 2023; Hodel and West, 2023; Dziri et al., 2023; Dasgupta et al., 2023). Patel et al. (2024) observed LLMs' performance significantly declines as reasoning steps increase in multi-step logical reasoning tasks. DougrezLewis et al. (2024) revealed ChatGPT struggles with abductive reasoning when verifying claims by decomposing their evidence into atomic reasoning steps. Wang et al. (2024b) found that GPT-series models showed significant gaps compared to humans in dealing with inference rules. Parmar et al. (2024) introduced LogicBench and showed that existing LLMs struggle with instances involving complex reasoning and negations. Wan et al. (2024) introduced LogicAsker, which assesses whether LLMs can employ a set of atomic reasoning skills grounded in propositional and predicate logic and found significant gaps in LLMs' learning of logical rules. Bhuiya et al. (2024) proposed a challenging multi-hop reasoning benchmark with seemingly plausible but incorrect multi-hop reasoning chains and found that state-of-the-art LLMs' capabilities to perform multi-hop reasoning is affected by such chains. Mondorf and Plank (2024) introduced TruthQuest, which assesses LLMs' capabilities to conduct suppositional reasoning, i.e., reasoning where each statement can be false, and found that LLMs exhibit significant difficulties solving these tasks. Sprague et al. (2024) introduced a complex multi-step reasoning benchmark, MuSR, and characterized the gaps that remain for techniques like chain-of-thought to perform robust reasoning.</p>
<p>Biases and Errors Ando et al. (2023); Ozeki et al. (2024); Bertolazzi et al. (2024); Eisape et al. (2024) found that LLMs exhibit human-like reasoning biases in syllogistic arguments. Jiang et al. (2024a) found that LLMs exibit "token-biases" in solving logical reasoning problems. Aoki et al. (2024) revealed that LMs rely heavily on heuristics, such as lexical overlap, in the earlier stages of reasoning. Zhao et al. (2024a) constructed a MATHTRAP with carefully designed logical traps into the problem descriptions of MATH and GSM8k and found that while LLMs possess the knowledge required to solve these traps, they do not spontaneously use such knowledge them to handle the problems. Han et al. (2024) found that LLMs exhibit A-Not-B errors similar to human infants, failing to suppress the previously established response pattern during ICL. Liu et al. (2024) found that LLMs often contradict themselves in reasoning tasks involving contextual information understanding or commonsense. Zhou et al. (2024b) found that subtle alterations in the surface form can significantly impact the answer distribution, suggesting that LLMs solve reasoning problems using surface cues. Chen et al. (2024) found that the reasoning performance of LLMs is affected by the order of the premises. Hong et al. (2024); Huang et al. (2024) found that LLMs struggle to identify fallacious reasoning steps accurately, suggesting challenges in self-verification methods.</p>
<p>Reasoning in Unknown Situation Zhao et al. (2024b) found that LLMs struggle with reasoning in uncommon situations. Zhu et al. (2024) introduced a framework to dynamically generate reasoning samples, and LLMs perform worse in those samples. Hu et al. (2024) found that while LLMs can conduct reasoning when relevant knowledge is given in context, they are not proficient at reasoning with knowledge embedded in the training data.</p>
<h2>A. 2 Synthetic Logic Corpus for Training LLMs</h2>
<p>RuleTaker Clark et al. (2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. They showed that Transformer Vaswani et al. (2017) LMs can solve these problems in the sense that they can predict the final answer (i.e., "proved", "disproved", or "unknown") of each deductive proof given the fact set. Later studies Saha et al. (2020); Dalvi et al. (2021); Tafjord et al. (2021); Sanyal et al. (2022b) showed that generative LMs can generate even the intermediate proofs as well as the final answer. Later studies (Saha et al., 2020; Dalvi et al., 2021; Tafjord et al., 2021; Sanyal et al., 2022b) showed that T5 can generate even the intermediate logical steps as well as the final answer.
PARARULE-Plus (Bao et al., 2022) is the enhanced version of PARARULE (Clark et al., 2021), a variation of RuleTaker, that includes more samples and more logical steps. RoBERTa (Liu et al., 2019) trained on PARARULE-Plus outperformed the models trained on RuleTaker.</p>
<p>Artificial Argument Corpus (Betz et al., 2021) includes single-step deductive reasoning samples constructed from hand-selected deduction rules useful for critical thinking. They showed that the GPT-2 (Radford et al., 2019) trained on this corpus can generalize to solve NLI tasks. However, at the same time, they found that the LM does not generalize well to solve more challenging reasoning tasks such as ARC (Habernal et al., 2018) and LogiQA (Liu et al., 2020).</p>
<p>FLD by Morishita et al. $(2023,2024)$ is the first synthetic logic corpus based on formal logic theory. It includes multistep deductive reasoning samples constructed from the axioms of first-order predicate logic, which can express any deduction rule due to the completeness theorem. Due to this nature, T5 trained on FLD generalizes most effectively to other synthetic logic corpora, compared to models trained on other corpora.
Gontier et al. (2020) investigated the deductive reasoning capabilities of LMs on a corpus composed of a specific type of multistep inference, kinship relationships on synthetic kinship graphs. They found that LMs can solve this task when there are relatively few proof steps, but it is difficult for them to generalize to solve proof steps longer than those shown in training data. Bostrom et al. (2021) studied how to create realistic natural language expressions that represent deduction rules. To this end, they scraped sentences from Wikipedia using a template-based method and paraphrased them. They showed that training on this corpus helps solve real-world deductive reasoning problems such as EntailmentBank (Dalvi et al., 2021). Pi et al. (2022) used synthetic data from program executors, most notably SQL programs. They verified that this data can enhance numerical reasoning, logical reasoning, and multi-hop reasoning abilities. Trinh et al. (2024) generated 100 million geometry problems and verified that the capability of artificial intelligence can be enhanced to to pass the bronze medal threshold of the International Mathematics Olympiad. Saeed et al. (2021); Nafar et al. (2024) created soft reasoning rules involving with probabilistic logic, instead of hard-logic examined by the aforementioned studies. Sileo (2024) introduced a simpler and more general declarative framework for synthetic generation, and verified its effectiveness. Zhou et al. (2024a) synthetically generated a large dataset of mathematics, and gained over 12 points on GSM8k.</p>
<p>While these studies partly examined the effect of synthetic logic corpora, whether this approach is promising remains an open question. It has been unexplored whether the capabilities obtained from synthetic logic corpora generalizes to solve various tasks beyond the original tasks in these corpora. Additionally, the effect of these corpora has only been examined for small LMs trained on small pre-training corpora such as T5 and RoBERTa; it has been highly questionable whether they can still benefit state-of-the-art LLMs trained on a huge pre-training corpus. Furthermore, even if their benefits were verified, it remains unclear which design of synthetic logic samples yields the largest benefits due to the lack of systematic discussions on sample designs and empirical verification of these designs. We aimed to answer these questions in this paper and demonstrate the potential of synthetic logic corpora.</p>
<h1>A. 3 Distilling Reasoning Traces from Very Large LLMs</h1>
<p>Recent approaches (Ho et al., 2023; Magister et al., 2023; Li et al., 2022, 2023; Shridhar et al., 2023; Wang et al., 2023; Mitra et al., 2023; Liu et al., 2023c; Ben Allal et al., 2024; Lu et al., 2024) utilize very large LLMs, such as GPT-4, to prepare synthetic reasoning datasets to train smaller LLMs. A typical procedure is as follows: (i) prepare existing reasoning problems, (ii) prompt large LLMs to generate reasoning traces to solve these problems using techniques such as chain-of-thought prompting (Wei et al., 2022), and (iii) train smaller LLMs on these reasoning traces.</p>
<p>The distillation approach and the synthetic logic corpora approach examined in this paper have specific advantages and disadvantages, as follows.</p>
<p>The advantage of the distillation approach is its immediate practical effect, as it directly teaches LLMs solutions to various existing problems. The disadvantages could be that (i) it is non-trivial for specific solutions to specific problems to generalize to other problems, (ii) the number of training samples is limited to existing problems in nature, (iii) the correctness and faithfulness of the reasoning traces are not guaranteed; indeed, some studies (Turpin et al., 2023; Lanham et al., 2023) suggest that large LLMs do not always faithfully follow the "reasoning traces" they themselves generate, and (iv) it cannot enhance the very large LLMs themselves by nature.</p>
<p>The advantages of synthetic logic corpus approaches are that (i) since they teach the fundamentals of reasoning, such as deductive reasoning, they have the potential to generalize to various problems,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/hitachi-nlp/rec-adam&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
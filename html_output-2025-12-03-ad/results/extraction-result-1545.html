<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1545 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1545</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1545</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-fb9b0a6e88ca6e3cef9fc6ba060b27c5303da258</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fb9b0a6e88ca6e3cef9fc6ba060b27c5303da258" target="_blank">Teacher–Student Curriculum Learning</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Neural Networks and Learning Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Teacher–Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task, and the Teacher automatically chooses subtasks from a given set for the Student to train on.</p>
                <p><strong>Paper Abstract:</strong> We propose Teacher–Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task, and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e., where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student’s performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with long short-term memory (LSTM) and navigation in Minecraft. Our automatically ordered curriculum of submazes enabled to solve a Minecraft maze that could not be solved at all when training directly on that maze, and the learning was an order of magnitude faster than a uniform sampling of those submazes.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1545",
    "paper_id": "paper-fb9b0a6e88ca6e3cef9fc6ba060b27c5303da258",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0038315,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Teacher-Student Curriculum Learning</h1>
<p>Tambet Matiisen<em>, $\dagger$<br>University of Tartu<br>Avital Oliver</em>, $\ddagger$<br>OpenAI<br>Taco Cohen*<br>University of Amsterdam<br>John Schulman<br>OpenAI</p>
<h4>Abstract</h4>
<p>We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.</p>
<h2>1 Introduction</h2>
<p>Deep reinforcement learning algorithms have been used to solve difficult tasks in video games (Mnih et al., 2015), locomotion (Schulman et al., 2015; Lillicrap et al., 2015) and robotics (Levine et al., 2015). But tasks with sparse rewards like "Robot, fetch me a beer" remain challenging to solve with direct application of these algorithms. One reason is that the number of samples needed to solve a task with random exploration increases exponentially with the number of steps to get a reward (Langford, 2011). One approach to overcome this problem is to use curriculum learning (Bengio et al., 2009; Zaremba and Sutskever, 2014; Graves et al., 2016; Wu and Tian, 2017), where tasks are ordered by increasing difficulty and training only proceeds to harder tasks once easier ones are mastered. Curriculum learning helps when after mastering a simpler task the policy for a harder task is discoverable through random exploration.</p>
<p>To use curriculum learning, the researcher must:</p>
<ul>
<li>Be able to order subtasks by difficulty.</li>
<li>Decide on a "mastery" threshold. This can be based on achieving certain score (Zaremba and Sutskever, 2014; Wu and Tian, 2017), which requires prior knowledge of acceptable performance of each task. Alternatively this can be based on a plateau of performance, which can be hard to detect given the noise in the learning curve.</li>
<li>Continuously mix in easier tasks while learning harder ones to avoid forgetting. Designing these mixtures effectively is challenging (Zaremba and Sutskever, 2014).</li>
</ul>
<p>In this paper, we describe a new approach called Teacher-Student Curriculum Learning (TSCL). The Student is the model being trained. The Teacher monitors the Student's training progress and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>determines the tasks on which the Student should train at each training step, in order to maximize the Student’s progression through the curriculum. The Student can be any machine learning model. The Teacher is itself learning about the Student as it’s giving tasks, all as part of a single training session.</p>
<p>We describe several Teacher algorithms based on the notion of learning progress (Oudeyer and Kaplan, 2007). The main idea is that the Student should practice more the tasks on which it is making fastest progress i.e. the learning curve slope is highest. To counter forgetting, the Student should also practice tasks where the performance is getting worse i.e. the learning curve slope is negative.</p>
<p>The main contributions of the paper are:</p>
<ul>
<li>We formalize TSCL, a Teacher-Student framework for curriculum learning as partially observable Markov decision process (POMDP).</li>
<li>We propose a family of algorithms based on the notion of learning progress. The algorithms also address the problem of forgetting previous tasks.</li>
<li>We evaluate the algorithms on two supervised and reinforcement learning tasks: addition of decimal numbers with LSTM and navigation in Minecraft.</li>
</ul>
<h2>2 Teacher-Student Setup</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The Teacher-Student setup</p>
<p>Figure 1 illustrates the Teacher-Student interaction. At each timestep, the Teacher chooses tasks for the Student to practice on. The Student trains on those tasks and returns back a score. The Teacher’s goal is for the Student to succeed on a final task with as few training steps as possible. Usually the task is parameterized by a categorical value representing one of N subtasks, but one can imagine also multi-dimensional or continuous task parameterization. The score can be episode total reward in reinforcement learning or validation set accuracy in supervised learning.</p>
<p>We formalize the Teacher’s goal of helping the Student to learn a final task as solving a partially observable Markov decision process (POMDP). We present two POMDP formulations: (1) Simple, best suited for reinforcement learning; and (2) Batch, best suited for supervised learning.</p>
<h3>2.1 Simple POMDP Formulation</h3>
<p>The simple POMDP formulation exposes the score of the Student on a single task and is well-suited for reinforcement learning problems.</p>
<ul>
<li>The state s_{t} represents the entire state of the Student (i.e. neural network parameters and optimizer state) and is not observable to the Teacher.</li>
<li>The action a_{t} corresponds to the parameters of the task chosen by Teacher. In following we only consider a discrete task parameterization. Taking an action means training Student on that task for certain number of iterations.</li>
<li>The observation o_{t} is the score x_{t}^{(i)} of the task i = a_{t} the Student trained on at timestep t, i.e. the episode total reward. While in theory the Teacher could also observe other aspects of the Student state like network weights, for simplicity we choose to expose only the score.</li>
<li>Reward r_{t} is the change in score for the task the Student trained on at timestep t: r_{t} = x_{t}^{(i)} - x_{t_{i}^{(i)}}^{t, i}, where t_{i}^{(i)} is the previous timestep when the same task was trained on.</li>
</ul>
<h3>2.2 Batch POMDP Formulation</h3>
<p>In supervised learning a training batch can include multiple tasks. Therefore action, observation, and reward apply to the whole training set and scores can be measured on a held-out validation set. This motivates the batch formulation of the POMDP:</p>
<ul>
<li>The state $s_{t}$ represents training state of the Student.</li>
<li>The action $a_{t}$ represents a probability distribution over $N$ tasks. Each training batch is sampled according to the distribution: $a_{t}=\left(p_{t}^{(1)}, \ldots, p_{t}^{(N)}\right)$, where $p_{t}^{(i)}$ is the probability of task $i$ at timestep $t$.</li>
<li>The observation $o_{t}$ is the scores of all tasks after the training step: $o_{t}=\left(x_{t}^{(1)}, \ldots, x_{t}^{(N)}\right)$ In the simplest case the scores could be accuracies of the tasks in the training set. But in the case of minibatch training the model evolves during training and therefore additional evaluation pass is needed anyway to produce consistent results. Therefore we use a separate validation set that contains uniform mix of all tasks for this evaluation pass.</li>
<li>The reward $r_{t}$ is the sum of changes in evaluation scores from the previous timestep: $r_{t}=\sum_{i=1}^{N} x_{t}^{(i)}-x_{t-1}^{(i)}$.</li>
</ul>
<p>This setup could also be used with reinforcement learning by performing training in batches of episodes. But because scoring one sample (one episode) in reinforcement learning is usually much more computationally expensive than in supervised learning, it makes sense to use simple POMDP formulation and make decision about the next task after each training step.</p>
<h1>2.3 Optimization Criteria</h1>
<p>For either of the POMDP formulations, maximizing the Teacher episode total reward is equivalent to maximizing the score of all tasks at the end of the episode: $\sum_{t=1}^{T} r_{t}=\sum_{i=1}^{N} x_{T_{i}}^{(i)}$, where $T_{i}$ is the last training step where task $i$ was being trained on ${ }^{4}$.
While an obvious choice for optimization criteria would have been the performance in the final task, initially the Student might not have any success in the final task and this does not provide any meaningful feedback signal to the Teacher. Therefore we choose to maximize the sum of performances in all tasks. The assumption here is that in curriculum learning the final task includes the elements of all previous tasks, therefore good performance in the intermediate tasks usually leads to good performance in the final task.</p>
<h2>3 Algorithms</h2>
<p>POMDPs are typically solved using reinforcement learning algorithms. But those require many training episodes, while we aim to train the Student in one Teacher episode. Therefore, we resort to simpler heuristics. The basic intuition is that the Student should practice those tasks more for which it is making most progress (Oudeyer and Kaplan, 2007), while also practicing tasks that are at risk of being forgotten.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Idealistic curriculum learning. Left: Scores of different tasks improve over time, the next task starts improving once the previous task has been mastered. Right: Probability of sampling a task depends on the slope of the learning curve.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Figure 2 is a demonstration of the ideal training progress in a curriculum learning setting:</p>
<ol>
<li>At first, the Teacher has no knowledge so it samples from all tasks uniformly.</li>
<li>When the Student starts making progress on task 1, the Teacher allocates more probability mass to this task.</li>
<li>When the Student masters task 1, its learning curve flattens and the Teacher samples the task less often. At this point Student also starts making progress on task 2, so the Teacher samples more from task 2.</li>
<li>This continues until the Student masters all tasks. As all task learning curves flatten in the end, the Teacher returns to uniform sampling of the tasks.</li>
</ol>
<p>The picture above is idealistic, since in practice some unlearning often occurs, i.e. when most of the probability mass is allocated to the task 2, performance on task 1 might get worse. To counter this the Student should also practice all learned tasks, especially those where unlearning occurs. For this reason we sample tasks according to the absolute value of the slope of the learning curve instead. If the change in scores is negative, this must mean that unlearning occurred and this task should be practiced more.</p>
<p>This description alone does not prescribe an algorithm. We need to propose a method of estimating learning progress from noisy task scores, and a way to balance exploration and exploitation. We take inspiration from algorithms for the non-stationary multi-armed bandit problem (Sutton and Barto, 1998) and adapt them to TSCL. For brevity we only give intuition for the simple formulation algorithms here, the formal descriptions can be found in appendices A and B.</p>
<h1>3.1 Online algorithm</h1>
<p>The Online algorithm is inspired by the basic non-stationary bandit algorithm (Sutton and Barto, 1998). It uses exponentially weighted moving average to track the expected return $Q$ from different tasks:</p>
<p>$$
Q_{t+1}\left(a_{t}\right)=\alpha r_{t}+(1-\alpha) Q_{t}\left(a_{t}\right)
$$</p>
<p>where $\alpha$ is learning rate. The next task can be chosen by $\epsilon$-greedy exploration: sample a random task with probability $\epsilon$, or $\operatorname{argmax} Q_{t}(a)$ otherwise.</p>
<p>Alternatively the next task can be chosen using Boltzmann distribution:</p>
<p>$$
p(a)=\frac{e^{Q_{t}(a) / \tau}}{\sum_{i=1}^{N} e^{Q_{t}(i) / \tau}}
$$</p>
<p>where $\tau$ is the temperature of Boltzmann distribution. For details, see Algorithm 1 in Appendix A.</p>
<h3>3.2 Naive algorithm</h3>
<p>To estimate the learning progress more reliably one should practice the task several times. The Naive algorithm trains each task $K$ times, observes the resulting scores and estimates the slope of the learning curve using linear regression. The regression coefficient is used as the reward in the above non-stationary bandit algorithm. For details, see Algorithm 2 in Appendix A.</p>
<h3>3.3 Window algorithm</h3>
<p>Repeating the task a fixed number of times is expensive, when clearly no progress is made. The Window algorithm keeps FIFO buffer of last $K$ scores, and timesteps when these scores were recorded. Linear regression is performed to estimate the slope of the learning curve for each task, with the timesteps as the input variables. The regression coefficient is used as the reward in the above non-stationary bandit algorithm. For details, see Algorithm 3 in Appendix A.</p>
<h3>3.4 Sampling algorithm</h3>
<p>The previous algorithms require tuning of hyperparameters to balance exploration. To get rid of exploration hyperparameters, we take inspiration from Thompson sampling. The Sampling algorithm</p>
<p>keeps a buffer of last $K$ rewards for each task. To choose the next task, a recent reward is sampled from each task’s $K$-last-rewards buffer. Then whichever task yielded the highest sampled reward is chosen. This makes exploration a natural part of the algorithm: tasks that have recently had high rewards are sampled more often. For details, see Algorithm 4 in Appendix A.</p>
<h1>4 Experiments</h1>
<h3>4.1 Decimal Number Addition</h3>
<p>Addition of decimal numbers with LSTM is a well known task that requires a curriculum to learn in reasonable time (Zaremba and Sutskever, 2014). It is implemented as sequence-to-sequence model (Sutskever et al., 2014), where the input to the network is two decimal-coded numbers separated by a 'plus' sign, and output of the network is the sum of those numbers, also in decimal coding. The curriculum is based on the number of digits in the input numbers - it is easier to learn addition of short numbers and then move on to longer numbers.</p>
<p>Number addition is a supervised learning problem and therefore can be trained more efficiently by including several curriculum tasks in the mini-batch. Therefore we adopt batch training scheme as outlined in 2.2. The score we use is the accuracy of each task calculated on validation set. The results shown below are means and standard deviations of 3 runs with different random seeds. Full experiment details can be found in appendix C.</p>
<h3>4.1.1 Addition with 1-dimensional Curriculum</h3>
<p>We started with a similar setup to (Zaremba and Sutskever, 2014), where the curriculum task determines the maximum number of digits in both added numbers. The results are shown on Figure 3. Our algorithms outperformed uniform sampling and the best manual curriculum ("combined") for 9-digit addition from (Zaremba and Sutskever, 2014). An example of the task distribution during training session is given on figure 4.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results for 9-digit 1D addition, lower is better. Variants using the absolute value of the expected reward surpass the best manual curriculum ("combined").
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Progression of the task distribution over time for 9-digit 1D addition (Sampling). The algorithm progresses from simpler tasks to more complicated. Harder tasks take longer to learn and the algorithm keeps training on easier tasks to counter unlearning.</p>
<h3>4.1.2 Addition with 2-dimensional Curriculum</h3>
<p>We also experimented with a curriculum where the ordering of tasks is not obvious. We used the same decimal addition task, but in this case the length of each number is chosen separately, making the task-space 2-dimensional. Each training batch is modelled as a probability distribution over the length of both numbers $P\left(l_{1}, l_{2}\right)$. We also tried making this distribution independent such that $P\left(l_{1}, l_{2}\right)=P\left(l_{1}\right) P\left(l_{2}\right)$, but that did not work as well.</p>
<p>There is no equivalent experiment in (Zaremba and Sutskever, 2014), so we created a manual curriculum inspired by their best 1D curriculum. In particular we increase difficulty by increasing the maximum length of both two numbers, which reduces the problem to a 1D curriculum. Figure 5 shows the results for 9-digit 2D addition. Figure 6 illustrates the different approaches taken by manual and automated curriculum.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Results for 9-digit 2D addition, lower is better. The task seems easier, manual curriculum is hard to beat and uniform sampling is competitive.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Accuracy progress for 4-digit 2D addition. Top: TSCL. Bottom: the best manual curriculum. Our algorithm takes distinctively different approach by training on shorter numbers first. 9-digit videos can be found https://youtu.be/y_QIcQ6spWk and https://youtu.be/fB2kx-esjgw.</p>
<h1>4.1.3 Observations</h1>
<ul>
<li>Using absolute value of $Q$ boosts the performance of almost all the algorithms, which means it is efficient in countering forgetting.</li>
<li>There is no universal best algorithm. For 1D the Window algorithm and for 2D the Naive algorithm performed the best. Sampling is competitive in both and has least hyperparameters.</li>
<li>Whether $\epsilon$-greedy or Boltzmann exploration works better depends on the algorithm.</li>
<li>Uniform sampling is surprisingly efficient, especially in 2D case.</li>
<li>The 2D task is solved faster and the manual curriculum is hard to beat in 2D.</li>
</ul>
<h3>4.2 Minecraft</h3>
<p>Minecraft is a popular 3D video game where players can explore, craft tools and build arbitrary structures, making it a potentially rich environment for AI research. We used the Malmo platform (Johnson et al., 2016) with OpenAI Gym wrapper ${ }^{5}$ to interact with Minecraft in our reinforcement learning experiments. In particular we used ClassroomDecorator from Malmo to generate random mazes for the agent to solve. The mazes contain sequences of rooms separated by the following obstacles:</p>
<ul>
<li>Wall - the agent has to locate a doorway in the wall.</li>
<li>Lava - the agent has to cross a bridge over lava.</li>
</ul>
<p>We only implemented the Window algorithm for the Minecraft task, because other algorithms rely on score change, which is not straightforward to calculate for parallel training scheme. As baseline we use uniform sampling, training only on the last task, and a manually tuned curriculum. Full experimental details can be found in appendix D.</p>
<h3>4.2.1 5-step Curriculum</h3>
<p>We created a simple curriculum with 5 steps:</p>
<ol>
<li>A single room with a target.</li>
<li>Two rooms separated by lava.</li>
<li>Two rooms separated by wall.</li>
<li>Three rooms separated by lava and wall, in random order.</li>
<li>Four rooms separated by lava and walls, in random order.
<img alt="img-6.jpeg" src="img-6.jpeg" /></li>
</ol>
<p>Figure 7: 5-step curriculum.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Refer to Figure 7 for the room layout. The starting position of the agent and the location of the target were randomized for each episode. Manual curriculum trained first task for 200 000 steps, second, third and fourth task for 400 000 steps, and fifth task for 600 000 steps.</p>
<p>Figure 8 shows learning curves for Minecraft 5-step curriculum. The mean curve and standard deviation are based on 3 runs with different random seeds.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Minecraft 5-step curriculum results, Y-axis shows mean episode reward per 10 000 timesteps for the current task. Left: training performance, notice the manual curriculum task switches after 200 000, 600 000, 1 000 000 and 1 400 000 steps. For automatic curriculum the training score has no clear interpretation. Right: evaluation training on the last task. When training only on the last task the agent did not make any progress at all. When training on a uniform mix of the tasks the progress was slow. Manual curriculum allowed the agent to learn the last task to an acceptable level. TSCL is comparable to the manual curriculum in performance.</p>
<p>Video of the trained agent can be found here: https://youtu.be/cadaOd_aDIc. The learned policy is robust to the number of rooms, given that obstacles are of the same type. The code is available at https://github.com/tambetm/TSCL.</p>
<h1>5 Related Work</h1>
<p>Work by (Bengio et al., 2009) sparked general interest in curriculum learning. More recent results include learning to execute short programs (Zaremba and Sutskever, 2014), finding shortest paths in graphs (Graves et al., 2016) and learning to play first-person shooter (Wu and Tian, 2017). All those works rely on manually designed curricula and do not attempt to produce it automatically.</p>
<p>The idea of using learning progress as the reward could be traced back to (Schmidhuber, 1991). It has been successfully applied in the context of developmental robotics to learn object manipulation (Oudeyer et al., 2007; Baranes and Oudeyer, 2013) and also in actual classroom settings to teach primary school students (Clement et al., 2015). Using learning progress as the reward can be linked to the concept of intrinsic motivation (Oudeyer and Kaplan, 2007; Schmidhuber, 2010).</p>
<p>Several algorithms for adversarial bandits were analyzed in (Auer et al., 2002). While many of those algorithms have formal worst-case guarantees, in our experiments they did not perform well. The problem is that they come with no assumptions. In curriculum learning we can assume that rewards change smoothly over time.</p>
<p>More recently (Sukhbaatar et al., 2017) proposed a method to generate incremental goals and therefore curricula automatically. The setup consists of two agents, Alice and Bob, where Alice is generating trajectories and Bob is trying to either repeat or reverse them. Similar work by (Held et al., 2017) uses generative adversarial network to generate goal states for an agent. Compared to TSCL, they are</p>
<p>able to generate new subtasks on the go, but this mainly aids in exploration and is not guaranteed to help in learning the final task. sharma2017 apply similar setup as ours to multi-task learning. In their work they practice more tasks that are underperforming compared to preset baseline, as opposed to our approach of using learning progress. jain2017 estimate transfer between subtasks and target task, and create curriculum based on that.
The most similar work to ours was done concurrently in graves2017. While the problem statement is strikingly similar, our approaches differ. They apply the automatic curriculum learning only to supervised sequence learning tasks, while we consider also reinforcement learning tasks. They use the EXP3.S algorithm for adversarial bandits, while we propose alternative algorithms inspired by non-stationary bandits. They consider other learning progress metrics based on complexity gain while we focus only on prediction gain (which performed overall best in their experiments). Moreover, their work only uses uniform sampling of tasks as a baseline, whereas ours compares the best known manual curriculum for the given tasks. In summary they arrive to very similar conclusions to ours.
Decimal addition has also been explored in Kalchbrenner2015, Reed2015 Kaiser and Sutskever, 2015), sometimes improving results over original work in Zaremba and Sutskever, 2014). Our goal was not to improve the addition results, but to evaluate different curriculum approaches, therefore there is no direct comparison.
Minecraft is a relatively recent addition to reinforcement learning environments. Work by Oh et al., 2016) evaluates memory-based architectures for Minecraft. They use cognition-inspired tasks in visual grid-world. Our tasks differ in that they do not need explicit memory, and the movement is continuous, not grid-world. Another work by Tessler et al., 2016) uses tasks similar to ours but they take different approach: they learn a Deep Skill Module for each subtask, freeze weights of those modules and train hierarchical deep reinforcement learning network to pick either single actions or subtask policies. In contrast our approach uses simple policy network and relies on the TSCL to learn (and not forget) the subtasks.
While exploration bonuses Bellemare2016, Houthooft2016, Stadie et al., 2015) solve the same problem of sparse rewards, they apply to Student algorithms, while we were considering different Teacher approaches. For this reason we leave the comparison with exploration bonuses to future work.</p>
<h1>6 Conclusion</h1>
<p>We presented a framework for automatic curriculum learning that can be used for supervised and reinforcement learning tasks. We proposed a family of algorithms within that framework based on the concept of learning progress. While many of the algorithms performed equally well, it was crucial to rely on the absolute value of the slope of the learning curve when choosing the tasks. This guarantees the re-training on tasks which the network is starting to forget. In our LSTM decimal addition experiments, the Sampling algorithm outperformed the best manually designed curriculum as well as the uniform sampling. On the challenging 5-task Minecraft navigation problem, our Window algorithm matched the performance of a carefully designed manual curriculum, and significantly outperformed uniform sampling. For problems where curriculum learning is necessary, TSCL can avoid the tedium of ordering the difficulty of subtasks and hand-designing the curriculum.</p>
<h2>7 Future Work</h2>
<p>In this work we only considered discrete task parameterizations. In the future it would be interesting to apply the idea to continuous task parameterizations. Another promising idea to explore is the usage of automatic curriculum learning in contexts where the subtasks have not been pre-defined. For example, subtasks can be sampled from a generative model, or taken from different initial states in the same environment.</p>
<h2>8 Acknowledgements</h2>
<p>We thank Microsoft for their excellent Malmö environment for Minecraft, Josh Tobin and Pieter Abbeel for suggestions and comments, Vicky Cheung, Jonas Schneider, Ben Mann and Art Chaidarun</p>
<p>for always being helpful with OpenAI infrastructure. Also Raul Vicente, Ardi Tampuu and Ilya Kuzovkin from University of Tartu for comments and discussion.</p>
<h1>References</h1>
<p>Peter Auer, Nicolò, Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The non-stochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.
Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz. Reinforcement learning through asynchronous advantage actor-critic on a gpu. 2016.
Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471-1479, 2016.
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09, 2009. doi: $10.1145 / 1553374.1553380$.
François Chollet et al. Keras. https://github.com/fchollet/keras, 2015.
Benjamin Clement, Didier Roy, Pierre-Yves Oudeyer, and Manuel Lopes. Multi-armed bandits for intelligent tutoring systems. Journal of Educational Data Mining (JEDM), 7(2), 2015.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwińska, and et al. Sergio Gómez Colmenarejo. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):71-76, 2016.
Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks, 2017. http://arxiv.org/abs/1704.03003.
David Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366, 2017.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109-1117, 2016.
Vikas Jain and Theja Tulabandhula. Faster reinforcement learning using active simulators. arXiv preprint arXiv:1703.07853, 2017.
Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. In International joint conference on artificial intelligence (IJCAI), page 4246, 2016.
Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.</p>
<p>Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. arXiv preprint arXiv:1507.01526, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
John Langford. Efficient exploration in reinforcement learning. In Encyclopedia of Machine Learning, pages 309-311. Springer, 2011.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies, 2015. http://arxiv.org/abs/1504.00702.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, and et al. Alex Graves. Human-level control through deep reinforcement learning. Nature, 518(7540):529-33, 2015.
Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of memory, active perception, and action in minecraft. arXiv preprint arXiv:1605.09128, 2016.
Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in Neurorobotics, 1(November: 6), 2007.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286, 2007.</p>
<p>Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.</p>
<p>Jürgen Schmidhuber. Curious model-building control systems. In Neural Networks, 1991. 1991 IEEE International Joint Conference on, pages 1458-1463. IEEE, 1991.
Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE Transactions on Autonomous Mental Development, 2(3):230-47, 2010.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pages 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Sahil Sharma and Balaraman Ravindran. Online multi-task learning using biased sampling. arXiv preprint arXiv:1702.06053, 2017.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.
Sainbayar Sukhbaatar, Ilya Kostrikov, Arthur Szlam, and Rob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112, 2014.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks / a Publication of the IEEE Neural Networks Council, 9(5):1054-1054, 1998.
Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. A deep hierarchical approach to lifelong learning in minecraft. arXiv preprint arXiv:1604.07255, 2016.
Yuxin Wu and Yuandong Tian. Training agent for first-person shooter game with actor-critic curriculum learning. In Submitted to Int'l Conference on Learning Representations, 2017.
Wojciech Zaremba and Ilya Sutskever. Learning to execute., 2014. http://arxiv.org/abs/1410.4615.</p>
<h1>Appendices</h1>
<h2>A Simple versions of the algorithms</h2>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Online</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="p">(</span><span class="nx">a</span><span class="p">)=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">tasks</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Choose</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="o">|</span><span class="nx">Q</span><span class="err">\</span><span class="nx">right</span><span class="o">|</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="nx">greedy</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">Boltzmann</span><span class="w"> </span><span class="nx">policy</span>
<span class="w">        </span><span class="nx">Train</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)}</span><span class="o">-</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Update</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="err">\</span><span class="nx">alpha</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="nx">alpha</span><span class="p">)</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="nx">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nx">Naive</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="p">(</span><span class="nx">a</span><span class="p">)=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">tasks</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Choose</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="o">|</span><span class="nx">Q</span><span class="err">\</span><span class="nx">right</span><span class="o">|</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="nx">greedy</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">Boltzmann</span><span class="w"> </span><span class="nx">policy</span>
<span class="w">        </span><span class="nx">Reset</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="p">=</span><span class="err">\</span><span class="nx">emptyset</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">k</span><span class="p">}=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">K</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="nx">Train</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Store</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">list</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="nx">Apply</span><span class="w"> </span><span class="nx">linear</span><span class="w"> </span><span class="nx">regression</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">extract</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">coefficient</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Update</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="err">\</span><span class="nx">alpha</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="nx">alpha</span><span class="p">)</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="nx">Algorithm</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="nx">Window</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">FIFO</span><span class="w"> </span><span class="nx">buffers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">E</span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">length</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">tasks</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="p">(</span><span class="nx">a</span><span class="p">)=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">tasks</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Choose</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="o">|</span><span class="nx">Q</span><span class="err">\</span><span class="nx">right</span><span class="o">|</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="nx">greedy</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">Boltzmann</span><span class="w"> </span><span class="nx">policy</span>
<span class="w">        </span><span class="nx">Train</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Store</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">timestep</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">E</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Use</span><span class="w"> </span><span class="nx">linear</span><span class="w"> </span><span class="nx">regression</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">predict</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">E</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">coef</span><span class="p">.</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Update</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="err">\</span><span class="nx">alpha</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="nx">alpha</span><span class="p">)</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="nx">Sampling</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">FIFO</span><span class="w"> </span><span class="nx">buffers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">length</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">tasks</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">tilde</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="p">(</span><span class="k">if</span><span class="w"> </span><span class="err">$</span><span class="o">|</span><span class="nx">D</span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="o">|=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">tilde</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">a</span><span class="p">}=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">        </span><span class="nx">Choose</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">argmax</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="o">|</span><span class="err">\</span><span class="nx">tilde</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="o">|</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Train</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)}</span><span class="o">-</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Store</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<h1>B Batch versions of the algorithms</h1>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="nx">Online</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="p">(</span><span class="nx">a</span><span class="p">)=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">tasks</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Create</span><span class="w"> </span><span class="nx">prob</span><span class="p">.</span><span class="w"> </span><span class="nx">dist</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}=</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">p_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">p_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">N</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">.</span><span class="o">|</span><span class="nx">Q</span><span class="o">|</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="nx">greedy</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">Boltzmann</span><span class="w"> </span><span class="nx">policy</span>
<span class="w">        </span><span class="nx">Train</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">prob</span><span class="p">.</span><span class="w"> </span><span class="nx">dist</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">scores</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}=</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">N</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Calculate</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="nx">changes</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}=</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}</span><span class="o">-</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Update</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">Q</span><span class="p">}=</span><span class="err">\</span><span class="nx">alpha</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="nx">alpha</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">Q</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<p>Algorithm 6 Naive algorithm
Initialize STUDENT learning algorithm
Initialize expected return $Q(a)=0$ for all $N$ tasks
for $\mathrm{t}=1, \ldots, \mathrm{~T}$ do
Create prob. dist. $\mathbf{a}<em t="t">{\mathbf{t}}=\left(p</em>\right)$ based on $|Q|$ using $\epsilon$-greedy or Boltzmann policy
Reset $D(a)=\emptyset$ for all tasks
for $\mathrm{k}=1, \ldots, \mathrm{~K}$ do
Train STUDENT using prob. dist. $\mathbf{a}}^{(1)}, \ldots, p_{t}^{(N)<em _mathbf_t="\mathbf{t">{\mathbf{t}}$ and observe scores $\mathbf{o}</em>\right)$
Store score $o_{t}^{(a)}$ in list $D(a)$ for each task $a$
end for
Apply linear regression to each $D(a)$ and extract the coefficients as vector $\mathbf{r}}}=\left(x_{t}^{(1)}, \ldots, x_{t}^{(N)<em _mathbf_t="\mathbf{t">{\mathbf{t}}$
Update expected return $\mathbf{Q}=\alpha \mathbf{r}</em>$
end for}}+(1-\alpha) \mathbf{Q</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="nx">Window</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">FIFO</span><span class="w"> </span><span class="nx">buffers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">length</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">tasks</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="p">(</span><span class="nx">a</span><span class="p">)=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">tasks</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Create</span><span class="w"> </span><span class="nx">prob</span><span class="p">.</span><span class="w"> </span><span class="nx">dist</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}=</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">p_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">p_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">N</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">.</span><span class="o">|</span><span class="nx">Q</span><span class="o">|</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="nx">greedy</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">Boltzmann</span><span class="w"> </span><span class="nx">policy</span>
<span class="w">        </span><span class="nx">Train</span><span class="w"> </span><span class="nx">STUDENT</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">prob</span><span class="p">.</span><span class="w"> </span><span class="nx">dist</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">scores</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}=</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">x_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">N</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Store</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">a</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">tasks</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Apply</span><span class="w"> </span><span class="nx">linear</span><span class="w"> </span><span class="nx">regression</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">extract</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">coefficients</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">vector</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Update</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">Q</span><span class="p">}=</span><span class="err">\</span><span class="nx">alpha</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">t</span><span class="p">}}</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="nx">alpha</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">Q</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<p>Algorithm 8 Sampling algorithm
Initialize STUDENT learning algorithm
Initialize FIFO buffers $D(a)$ with length $K$ for all $N$ tasks
for $\mathrm{t}=1, \ldots, \mathrm{~T}$ do
Sample reward $\tilde{r}<em a="a">{a}$ from $D(a)$ for each task (if $|D(a)|=0$ then $\tilde{r}</em>=1$ )
Create one-hot prob. dist. $\tilde{\mathbf{a}}<em t="t">{\mathbf{t}}=\left(p</em>}^{(1)}, \ldots, p_{t}^{(N)}\right)$ based on $\operatorname{argmax<em a="a">{a}\left|\tilde{r}</em>\right|$
Mix in uniform dist. : $\mathbf{a}<em _mathbf_t="\mathbf{t">{\mathbf{t}}=(1-\epsilon) \tilde{\mathbf{a}}</em>+\epsilon / N$
Train STUDENT using prob. dist. $\mathbf{a}}<em _mathbf_t="\mathbf{t">{\mathbf{t}}$ and observe scores $\mathbf{o}</em>\right)$
Calculate score changes $\mathbf{r}}}=\left(x_{t}^{(1)}, \ldots, x_{t}^{(N)<em _mathbf_t="\mathbf{t">{\mathbf{t}}=\mathbf{o}</em>}}-\mathbf{o<em t="t">{\mathbf{t}-\mathbf{1}}$
Store reward $r</em>$ in $D(a)$ for each task $a$
end for}^{(a)</p>
<h1>C Decimal Number Addition Training Details</h1>
<p>Our reimplementation of decimal addition is based on Keras (Chollet et al., 2015). The encoder and decoder are both LSTMs with 128 units. In contrast to the original implementation, the hidden state is not passed from encoder to decoder, instead the last output of the encoder is provided to all inputs of the decoder. One curriculum training step consists of training on 40,960 samples. Validation set consists of 4,096 samples and 4,096 is also the batch size. Adam optimizer (Kingma and Ba, 2014) is used for training with default learning rate of 0.001 . Both input and output are padded to a fixed size.
In the experiments we used the number of steps until $99 \%$ validation set accuracy is reached as a comparison metric. The exploration coefficient $\epsilon$ was fixed to 0.1 , the temperature $\tau$ was fixed to 0.0004 , the learning rate $\alpha$ was 0.1 , and the window size $K$ was 10 in all experiments.</p>
<h2>D Minecraft Training Details</h2>
<p>The Minecraft task consisted of navigating through randomly generated mazes. The maze ends with a target block and the agent gets 1,000 points by touching it. Each move costs -0.1 and dying in lava or getting a timeout yields $-1,000$ points. Timeout is 30 seconds ( 1,500 steps) in the first task and 45 seconds ( 2,250 steps) in the subsequent tasks.
For learning we used the proximal policy optimization (PPO) algorithm (Schulman et al., 2017) implemented using Keras (Chollet et al., 2015) and optimized for real-time environments. The policy network used four convolutional layers and one LSTM layer. Input to the network was $40 \times 30$ color image and outputs were two Gaussian actions: move forward/backward and turn left/right. In addition the policy network had state value output, which was used as the baseline. Figure 10 shows the network architecture.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Network architecture used for Minecraft.</p>
<p>For training we used a setup with 10 parallel Minecraft instances. The agent code was separated into runners, that interact with the environment, and a trainer, that performs batch training on GPU, similar to Babaeizadeh et al. (2016). Runners regularly update their snapshot of the current policy weights, but they only perform prediction (forward pass), never training. After a fixed number of steps they use FIFO buffers to send collected states, actions and rewards to the trainer. Trainer collects those experiences from all runners, assembles them into batches and performs training. FIFO buffers shield the runners and the trainer from occasional hiccups. This also means that the trainer is not completely on-policy, but this problem is handled by the importance sampling in PPO.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Training scheme used for Minecraft.</p>
<p>During training we also used frame skipping, i.e. processed only every 5th frame. This sped up the learning considerably and the resulting policy also worked without frame skip. Also, we used auxiliary loss for predicting the depth as suggested in (Mirowski et al., 2016). Surprisingly this resulted only in minor improvements.
For automatic curriculum learning we only implemented the Window algorithm for the Minecraft task, because other algorithms rely on score change, which is not straightforward to calculate for parallel training scheme. Window size was defined in timesteps and fixed to 10,000 in the experiments, exploration rate was set to 0.1 .
The idea of the first task in the curriculum was to make the agent associate the target with a reward. In practice this task proved to be too simple - the agent could achieve almost the same reward by doing backwards circles in the room. For this reason we added penalty for moving backwards to the policy loss function. This fixed the problem in most cases, but we occasionally still had to discard some unsuccessful runs. Results only reflect the successful runs.
We also had some preliminary success combining continuous (Gaussian) actions with binary (Bernoulli) actions for "jump" and "use" controls, as shown on figure 10. This allowed the agent to learn to cope also with rooms that involve doors, switches or jumping obstacles, see https://youtu.be/e1oKiPlAv74.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/tambetm/gym-minecraft&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
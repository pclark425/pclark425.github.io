<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7600 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7600</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7600</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-9a31b2ce43fe198ab1fd046ca4ec70fded154aee</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9a31b2ce43fe198ab1fd046ca4ec70fded154aee" target="_blank">What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Automated Software Engineering</p>
                <p><strong>Paper TL;DR:</strong> It is shown that a carefully-designed demonstration can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90% on code summarization, bug fixing, and program synthesis, respectively.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7600.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7600.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>task-vs-instance-demo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-level versus Instance-level In-context Demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison between task-level demonstrations (same example set for all test queries) and instance-level demonstrations (examples retrieved per test query); instance-level retrieval substantially improves performance and yields more stable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained language models accessed via OpenAI APIs; experiments used API inference (no fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization, Bug fixing, Program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code->text summarization (CSN/TLC), Code+instruction->fixed code (B2F_small/B2F_medium), Text->code synthesis (CoNaLa).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context prompt containing natural language instruction plus concatenated demonstration examples (input/output pairs) followed by a test input.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Two modes compared: task-level (one fixed demonstration applied to all test samples) vs instance-level (retrieve nearest demonstration examples for each test sample, e.g., BM-25); demonstrations contained reconstructed examples with explicit [input] and [output] delimiters; experiments typically used 4-shot prompts for RQ1/RQ2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, ROUGE-L, METEOR (summarization); Exact Match (EM), BLEU-4 (bug fixing); CodeBLEU, EM, SM, DM (synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (Codex, instance-level BM-25): Code summarization (CSN) BLEU-4 = 22.35; Bug fixing (B2F_small) EM = 30.45; Program synthesis (CoNaLa) EM = 18.53.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Example (Codex, task-level best KmeansRND): CSN BLEU-4 = 20.71; B2F_small EM (task-level KmeansRND) EM ≈ 8.60; CoNaLa (task-level random) EM ≈ 16.00.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Instance-level vs best task-level: large absolute improvements reported (e.g., B2F_medium and B2F_small exact-match improvements reported as at least +141.97% and +193.64% relative over best task-level demonstrations); for CSN BLEU-4 instance-level (22.35) vs task-level KmeansRND (20.71) = +1.64 absolute (+~7.9% relative).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Temperature=0; prompts used explicit templates (see Table II); typically 4-shot for selection/order experiments; retrieval methods include BM-25 and dense encoders (SBERT, UniXcoder, CoCoSoDa).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7600.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7600.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>diversity-selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diversity-aware Task-level Demonstration Selection (KmeansRND)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using clustering to select diverse task-level demonstration examples (KmeansRND) improves average accuracy and reduces variance across different example groups versus random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessed LLMs used in few-shot in-context setups.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization (CSN, TLC); Bug fixing; Program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate comments for code snippets; predict fixed code given buggy code and guidance; synthesize code from natural language requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Task-level few-shot prompt where demonstration examples are chosen once for all test samples; comparison of random selection vs KmeansRND (clustering then random pick per cluster).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / selection strategy</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>KmeansRND: cluster training set, sample from clusters to increase diversity; demonstrations used 4 examples in many experiments; ordering varied separately.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, ROUGE-L, METEOR (summarization); EM/BLEU-4 (bug fixing); CodeBLEU/EM (synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (Codex, CSN): Random task-level BLEU-4 = 19.64; KmeansRND BLEU-4 = 20.71.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random task-level BLEU-4 = 19.64 (Codex, CSN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>KmeansRND vs Random (CSN): +1.07 absolute BLEU-4, reported as +5.45% relative improvement; ROUGE-L +7.25% and METEOR +6.80% average improvements reported; also reduced coefficient of variation (more stable predictions across groups).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>4-shot demonstrations for selection/order experiments; evaluated across three random orders to compute CV.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7600.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7600.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>bm25-vs-dense</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM-25 Retrieval versus Dense Retrieval Methods for Instance-level Demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic sparse retrieval (BM-25) as a method to select instance-level in-context examples performs as well as or better than several dense retrieval encoders for code tasks, while being simpler.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs used as few-shot predictors after retrieval of nearest training examples using various retrieval algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Program synthesis (CoNaLa) and other code tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate code from natural language intent; retrieval selects similar (input,output) examples to place in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instance-level retrieval-based few-shot prompt: retrieve N examples similar to test input using BM-25 or dense encoders (SBERT, UniXcoder, CoCoSoDa) and place them in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / retrieval method</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>BM-25 used to retrieve closest examples by sparse lexical match; prompts contain 4 examples for RQ1/RQ2; ordering experiments also applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM), CodeBLEU (CB), Syntax Match (SM), Dataflow Match (DM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Program synthesis (Codex): BM-25 EM = 18.53; SBERT EM = 16.13; UniXcoder EM = 16.00; CoCoSoDa EM = 16.30 (Table V/VI aggregated).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Dense retrieval (SBERT/UniXcoder/CoCoSoDa) EM ≈ 16.00-16.30 (CoNaLa with 4-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>BM-25 outperformed SBERT, UniXcoder, CoCoSoDa on program synthesis EM by reported relative margins of +14.88%, +15.81%, and +13.68% respectively (absolute EM improvements ~2.4 points over ~16.0 baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>4-shot prompts, temperature=0, Codex input length limit used to truncate examples if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7600.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7600.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ordering-similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstration Ordering by Similarity (ascending to test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ordering demonstrations by ascending similarity to the test sample (placing most similar examples closest to the test prompt end) generally improves performance relative to random or reverse orders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompted LLMs; ordering of concatenated in-context demonstrations varied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization, Bug fixing, Program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks as above; ordering experiments test sensitivity to sequence of examples within the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt with N retrieved examples; three ordering strategies compared: Random, Similarity (ascending similarity, i.e., examples with higher similarity placed nearer the test sample), Reverse Similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / example ordering</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Similarity measured between demonstration example inputs and the test input; Similarity ordering places more similar examples at the end (closer to test); experiments used Random, Similarity, Reverse Similarity across task-level and instance-level selection (e.g., BM-25).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, ROUGE-L, METEOR, EM, CodeBLEU components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples (Codex, Table VI): CSN BLEU-4: Random=20.46, Similarity=21.04, Reverse=19.78. Bug fixing (B2F_small) EM: Random=9.52, Similarity=9.93, Reverse=9.02. BM-25 (instance-level) EM: Random=30.45, Similarity=30.95, Reverse=29.80.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random ordering values as above (e.g., CSN BLEU-4 Random=20.46).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Similarity vs Reverse Similarity: similarity consistently outperformed reverse; e.g., CSN BLEU-4 Similarity vs Reverse = +1.26 BLEU-4 absolute (+~6.4% relative). BM-25 Similarity vs Random EM +0.50 absolute (30.95 vs 30.45). Overall, Similarity achieved best performance in 62.96% (17/27) of evaluated metrics/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>4-shot demonstrations for ordering experiments; temperature=0; evaluated over multiple random orders to compute CV.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7600.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7600.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>num-examples-truncation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number of Demonstration Examples and Prompt Truncation Effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Increasing the number of in-context examples initially improves performance, but beyond a threshold causes truncation of example content (due to model input limits) and can dramatically degrade performance; code tasks are particularly susceptible because code snippets are long.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Experiments varied few-shot count from 1 to 64 (and up to 128 for RQ4) and monitored truncation rates under different model token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization (CSN), Bug fixing (B2F_small), Program synthesis (CoNaLa)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how many demonstration examples to include in prompt for best ICL performance given token limits and code length.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt with N examples (N varied from 1..64 in main experiments; up to 128 in RQ4), using BM-25 selection and Similarity ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot count</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>When increasing N, the per-example truncation threshold = input length limit / (N+1) (paper used model-specific token limits: Codex 8001, GPT-3.5 4096, ChatGPT 4097 tokens); examples might be cut off if prompt exceeds length limit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, EM, CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Observed behavior: performance increases at small N, plateaus or peaks (varies by task) around N=16..32, then degrades when truncation becomes frequent. Example: for B2F_small, peak at N=32; when N increased to 64, truncation occurred on >80% examples and 44.32% characters of those examples were discarded, producing dramatic performance drop.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>4-shot performance cited as a pragmatic baseline: reaches ~96.48% (EM), 97.80% (BLEU-4), and 94.80% (CodeBLEU) of best observed performance for the three tasks respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Relative: using 4 examples achieves near-peak results (within ~2-5% of best) while avoiding truncation/cost; extreme increases to 64+ can cause severe absolute drops due to truncation (example: large drop in B2F_small when >80% demonstrations truncated).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>BM-25 selection + Similarity ordering used for these curves; temperature=0; model input limits considered in truncation calculations (Codex 8001 tokens; GPT-3.5 4096; ChatGPT 4097).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7600.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7600.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>zero-vs-few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot versus Few-shot Demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot ICL (instruction only, no demonstration examples) yields very poor performance on code intelligence tasks compared to few-shot demonstrations; adding properly chosen demonstrations dramatically improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003) and ChatGPT (gpt-3.5-turbo); Codex also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI LLMs accessed via API, evaluated under zero-shot (instruction only) and few-shot (with demonstrations) settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization, Bug fixing, Program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above; zero-shot uses only the template instruction without example input-output pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot: instruction + test input; Few-shot baseline: two randomly selected demonstration examples with random order (as used in prior works). Carefully-designed few-shot: BM-25 selection + Similarity ordering + 4 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot prompts contain only the high-level instruction template; few-shot contained demonstrations formatted with explicit [input] {#code} [output] {#comment} style (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, EM, CodeBLEU (CB)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples (Table VIII): GPT-3.5 zero-shot program synthesis EM = 0.20; GPT-3.5 baseline (2-shot random) EM = 10.00; GPT-3.5 carefully-designed (4-shot BM-25+Similarity) EM = 15.20. ChatGPT zero-shot program synthesis EM = 3.40; ChatGPT carefully-designed EM = 14.20.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline (widely-used) few-shot demonstration: typically 2 randomly selected examples in random order (prior work setting) — example Codex baseline EM on CoNaLa = 14.20 (Table VIII).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Zero-shot to carefully-designed few-shot: very large absolute and relative improvements (e.g., GPT-3.5 CoNaLa EM from 0.20 to 15.20 = +15.0 absolute). Carefully-designed few-shot vs baseline few-shot: Codex CoNaLa EM 21.40 vs baseline 14.20 = +7.2 absolute (+50.81% relative); reported minimum cross-task improvements claimed in paper: at least +9.90% BLEU-4 (summarization), +175.96% EM (bug fixing), +50.81% EM (synthesis) when using carefully-designed demonstrations versus baseline methods.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot uses same instruction templates; few-shot used temperature=0. Baseline demonstration = randomly select 2 examples; carefully-designed = BM-25 retrieval + Similarity ordering + 4 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7600.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7600.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>careful-demo-vs-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Carefully-designed Demonstration (BM-25 + Similarity + 4-shot) versus Baseline Demonstration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A demonstration construction recipe combining BM-25 instance-level retrieval, similarity-based ascending ordering, and 4-shot prompts yields substantial improvements over widely-used baseline demonstration construction (random 2-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002), GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-based LLMs evaluated with deterministic decoding (temperature=0) and explicit demonstration templates (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code summarization (CSN), Bug fixing (B2F), Program synthesis (CoNaLa)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks as above (code->comment, bug fix with guidance, NL->code).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt with 4 instance-level examples retrieved by BM-25, arranged by ascending similarity to the test sample (most similar close to test), using explicit [input]/[output] delimiters and a one-line template instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / crafted demonstration</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Carefully-designed demonstration: BM-25 retrieval, Similarity ordering, 4 examples, temperature=0, explicit templates per task (Table II). Baseline demonstration (prior works): randomly select 2 examples, random order.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4 (summarization), Exact Match (EM) for bug fixing and synthesis, CodeBLEU for synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Codex examples from Table VIII: Code summarization (CSN) BLEU-4: Baseline=17.37 -> Carefully-designed=22.73. Bug fixing (B2F, EM): Baseline=9.70 -> Carefully-designed=32.25. Program synthesis (CoNaLa, EM): Baseline=14.20 -> Carefully-designed=21.40.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>As above (random-2-shot baseline per Table VIII).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Codex: Summarization BLEU-4 +5.36 absolute (+30.9% relative); Bug fixing EM +22.55 absolute (+232.5% relative); Program synthesis EM +7.20 absolute (+50.8% relative). The paper reports that across tasks the carefully-designed demonstration improved BLEU-4, EM, and EM by at least +9.90%, +175.96%, and +50.81% respectively compared to widely-used baseline construction methods.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Temperature=0, frequency_penalty=0, presence_penalty=0; Codex input length limit 8001 tokens (examples truncated when necessary using limit/(N+1) policy); demonstration size = 4 in these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What makes good in-context examples for gpt-3? <em>(Rating: 2)</em></li>
                <li>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity <em>(Rating: 2)</em></li>
                <li>Retrieval-based prompt selection for code-related few-shot learning <em>(Rating: 2)</em></li>
                <li>Automated program repair in the era of large pre-trained language models <em>(Rating: 1)</em></li>
                <li>Diverse demonstrations improve in-context compositional generalization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7600",
    "paper_id": "paper-9a31b2ce43fe198ab1fd046ca4ec70fded154aee",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "task-vs-instance-demo",
            "name_full": "Task-level versus Instance-level In-context Demonstrations",
            "brief_description": "Comparison between task-level demonstrations (same example set for all test queries) and instance-level demonstrations (examples retrieved per test query); instance-level retrieval substantially improves performance and yields more stable outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT",
            "model_description": "Large pre-trained language models accessed via OpenAI APIs; experiments used API inference (no fine-tuning).",
            "model_size": null,
            "task_name": "Code summarization, Bug fixing, Program synthesis",
            "task_description": "Code-&gt;text summarization (CSN/TLC), Code+instruction-&gt;fixed code (B2F_small/B2F_medium), Text-&gt;code synthesis (CoNaLa).",
            "problem_format": "Few-shot in-context prompt containing natural language instruction plus concatenated demonstration examples (input/output pairs) followed by a test input.",
            "format_category": "prompt style / input modality",
            "format_details": "Two modes compared: task-level (one fixed demonstration applied to all test samples) vs instance-level (retrieve nearest demonstration examples for each test sample, e.g., BM-25); demonstrations contained reconstructed examples with explicit [input] and [output] delimiters; experiments typically used 4-shot prompts for RQ1/RQ2.",
            "performance_metric": "BLEU-4, ROUGE-L, METEOR (summarization); Exact Match (EM), BLEU-4 (bug fixing); CodeBLEU, EM, SM, DM (synthesis).",
            "performance_value": "Example (Codex, instance-level BM-25): Code summarization (CSN) BLEU-4 = 22.35; Bug fixing (B2F_small) EM = 30.45; Program synthesis (CoNaLa) EM = 18.53.",
            "baseline_performance": "Example (Codex, task-level best KmeansRND): CSN BLEU-4 = 20.71; B2F_small EM (task-level KmeansRND) EM ≈ 8.60; CoNaLa (task-level random) EM ≈ 16.00.",
            "performance_change": "Instance-level vs best task-level: large absolute improvements reported (e.g., B2F_medium and B2F_small exact-match improvements reported as at least +141.97% and +193.64% relative over best task-level demonstrations); for CSN BLEU-4 instance-level (22.35) vs task-level KmeansRND (20.71) = +1.64 absolute (+~7.9% relative).",
            "experimental_setting": "Temperature=0; prompts used explicit templates (see Table II); typically 4-shot for selection/order experiments; retrieval methods include BM-25 and dense encoders (SBERT, UniXcoder, CoCoSoDa).",
            "statistical_significance": null,
            "uuid": "e7600.0",
            "source_info": {
                "paper_title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "diversity-selection",
            "name_full": "Diversity-aware Task-level Demonstration Selection (KmeansRND)",
            "brief_description": "Using clustering to select diverse task-level demonstration examples (KmeansRND) improves average accuracy and reduces variance across different example groups versus random selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT",
            "model_description": "API-accessed LLMs used in few-shot in-context setups.",
            "model_size": null,
            "task_name": "Code summarization (CSN, TLC); Bug fixing; Program synthesis",
            "task_description": "Generate comments for code snippets; predict fixed code given buggy code and guidance; synthesize code from natural language requirements.",
            "problem_format": "Task-level few-shot prompt where demonstration examples are chosen once for all test samples; comparison of random selection vs KmeansRND (clustering then random pick per cluster).",
            "format_category": "prompt style / selection strategy",
            "format_details": "KmeansRND: cluster training set, sample from clusters to increase diversity; demonstrations used 4 examples in many experiments; ordering varied separately.",
            "performance_metric": "BLEU-4, ROUGE-L, METEOR (summarization); EM/BLEU-4 (bug fixing); CodeBLEU/EM (synthesis).",
            "performance_value": "Example (Codex, CSN): Random task-level BLEU-4 = 19.64; KmeansRND BLEU-4 = 20.71.",
            "baseline_performance": "Random task-level BLEU-4 = 19.64 (Codex, CSN).",
            "performance_change": "KmeansRND vs Random (CSN): +1.07 absolute BLEU-4, reported as +5.45% relative improvement; ROUGE-L +7.25% and METEOR +6.80% average improvements reported; also reduced coefficient of variation (more stable predictions across groups).",
            "experimental_setting": "4-shot demonstrations for selection/order experiments; evaluated across three random orders to compute CV.",
            "statistical_significance": null,
            "uuid": "e7600.1",
            "source_info": {
                "paper_title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "bm25-vs-dense",
            "name_full": "BM-25 Retrieval versus Dense Retrieval Methods for Instance-level Demonstrations",
            "brief_description": "Classic sparse retrieval (BM-25) as a method to select instance-level in-context examples performs as well as or better than several dense retrieval encoders for code tasks, while being simpler.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT",
            "model_description": "LLMs used as few-shot predictors after retrieval of nearest training examples using various retrieval algorithms.",
            "model_size": null,
            "task_name": "Program synthesis (CoNaLa) and other code tasks",
            "task_description": "Generate code from natural language intent; retrieval selects similar (input,output) examples to place in prompt.",
            "problem_format": "Instance-level retrieval-based few-shot prompt: retrieve N examples similar to test input using BM-25 or dense encoders (SBERT, UniXcoder, CoCoSoDa) and place them in prompt.",
            "format_category": "prompt style / retrieval method",
            "format_details": "BM-25 used to retrieve closest examples by sparse lexical match; prompts contain 4 examples for RQ1/RQ2; ordering experiments also applied.",
            "performance_metric": "Exact Match (EM), CodeBLEU (CB), Syntax Match (SM), Dataflow Match (DM).",
            "performance_value": "Program synthesis (Codex): BM-25 EM = 18.53; SBERT EM = 16.13; UniXcoder EM = 16.00; CoCoSoDa EM = 16.30 (Table V/VI aggregated).",
            "baseline_performance": "Dense retrieval (SBERT/UniXcoder/CoCoSoDa) EM ≈ 16.00-16.30 (CoNaLa with 4-shot).",
            "performance_change": "BM-25 outperformed SBERT, UniXcoder, CoCoSoDa on program synthesis EM by reported relative margins of +14.88%, +15.81%, and +13.68% respectively (absolute EM improvements ~2.4 points over ~16.0 baseline).",
            "experimental_setting": "4-shot prompts, temperature=0, Codex input length limit used to truncate examples if needed.",
            "statistical_significance": null,
            "uuid": "e7600.2",
            "source_info": {
                "paper_title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ordering-similarity",
            "name_full": "Demonstration Ordering by Similarity (ascending to test)",
            "brief_description": "Ordering demonstrations by ascending similarity to the test sample (placing most similar examples closest to the test prompt end) generally improves performance relative to random or reverse orders.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT",
            "model_description": "Prompted LLMs; ordering of concatenated in-context demonstrations varied.",
            "model_size": null,
            "task_name": "Code summarization, Bug fixing, Program synthesis",
            "task_description": "Tasks as above; ordering experiments test sensitivity to sequence of examples within the prompt.",
            "problem_format": "Few-shot prompt with N retrieved examples; three ordering strategies compared: Random, Similarity (ascending similarity, i.e., examples with higher similarity placed nearer the test sample), Reverse Similarity.",
            "format_category": "prompt style / example ordering",
            "format_details": "Similarity measured between demonstration example inputs and the test input; Similarity ordering places more similar examples at the end (closer to test); experiments used Random, Similarity, Reverse Similarity across task-level and instance-level selection (e.g., BM-25).",
            "performance_metric": "BLEU-4, ROUGE-L, METEOR, EM, CodeBLEU components.",
            "performance_value": "Examples (Codex, Table VI): CSN BLEU-4: Random=20.46, Similarity=21.04, Reverse=19.78. Bug fixing (B2F_small) EM: Random=9.52, Similarity=9.93, Reverse=9.02. BM-25 (instance-level) EM: Random=30.45, Similarity=30.95, Reverse=29.80.",
            "baseline_performance": "Random ordering values as above (e.g., CSN BLEU-4 Random=20.46).",
            "performance_change": "Similarity vs Reverse Similarity: similarity consistently outperformed reverse; e.g., CSN BLEU-4 Similarity vs Reverse = +1.26 BLEU-4 absolute (+~6.4% relative). BM-25 Similarity vs Random EM +0.50 absolute (30.95 vs 30.45). Overall, Similarity achieved best performance in 62.96% (17/27) of evaluated metrics/tasks.",
            "experimental_setting": "4-shot demonstrations for ordering experiments; temperature=0; evaluated over multiple random orders to compute CV.",
            "statistical_significance": null,
            "uuid": "e7600.3",
            "source_info": {
                "paper_title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "num-examples-truncation",
            "name_full": "Number of Demonstration Examples and Prompt Truncation Effects",
            "brief_description": "Increasing the number of in-context examples initially improves performance, but beyond a threshold causes truncation of example content (due to model input limits) and can dramatically degrade performance; code tasks are particularly susceptible because code snippets are long.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002); validated on GPT-3.5 and ChatGPT",
            "model_description": "Experiments varied few-shot count from 1 to 64 (and up to 128 for RQ4) and monitored truncation rates under different model token limits.",
            "model_size": null,
            "task_name": "Code summarization (CSN), Bug fixing (B2F_small), Program synthesis (CoNaLa)",
            "task_description": "Evaluate how many demonstration examples to include in prompt for best ICL performance given token limits and code length.",
            "problem_format": "Few-shot prompt with N examples (N varied from 1..64 in main experiments; up to 128 in RQ4), using BM-25 selection and Similarity ordering.",
            "format_category": "prompt style / few-shot count",
            "format_details": "When increasing N, the per-example truncation threshold = input length limit / (N+1) (paper used model-specific token limits: Codex 8001, GPT-3.5 4096, ChatGPT 4097 tokens); examples might be cut off if prompt exceeds length limit.",
            "performance_metric": "BLEU-4, EM, CodeBLEU",
            "performance_value": "Observed behavior: performance increases at small N, plateaus or peaks (varies by task) around N=16..32, then degrades when truncation becomes frequent. Example: for B2F_small, peak at N=32; when N increased to 64, truncation occurred on &gt;80% examples and 44.32% characters of those examples were discarded, producing dramatic performance drop.",
            "baseline_performance": "4-shot performance cited as a pragmatic baseline: reaches ~96.48% (EM), 97.80% (BLEU-4), and 94.80% (CodeBLEU) of best observed performance for the three tasks respectively.",
            "performance_change": "Relative: using 4 examples achieves near-peak results (within ~2-5% of best) while avoiding truncation/cost; extreme increases to 64+ can cause severe absolute drops due to truncation (example: large drop in B2F_small when &gt;80% demonstrations truncated).",
            "experimental_setting": "BM-25 selection + Similarity ordering used for these curves; temperature=0; model input limits considered in truncation calculations (Codex 8001 tokens; GPT-3.5 4096; ChatGPT 4097).",
            "statistical_significance": null,
            "uuid": "e7600.4",
            "source_info": {
                "paper_title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "zero-vs-few-shot",
            "name_full": "Zero-shot versus Few-shot Demonstrations",
            "brief_description": "Zero-shot ICL (instruction only, no demonstration examples) yields very poor performance on code intelligence tasks compared to few-shot demonstrations; adding properly chosen demonstrations dramatically improves results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003) and ChatGPT (gpt-3.5-turbo); Codex also evaluated",
            "model_description": "Closed-source OpenAI LLMs accessed via API, evaluated under zero-shot (instruction only) and few-shot (with demonstrations) settings.",
            "model_size": null,
            "task_name": "Code summarization, Bug fixing, Program synthesis",
            "task_description": "As above; zero-shot uses only the template instruction without example input-output pairs.",
            "problem_format": "Zero-shot: instruction + test input; Few-shot baseline: two randomly selected demonstration examples with random order (as used in prior works). Carefully-designed few-shot: BM-25 selection + Similarity ordering + 4 examples.",
            "format_category": "prompt style / few-shot vs zero-shot",
            "format_details": "Zero-shot prompts contain only the high-level instruction template; few-shot contained demonstrations formatted with explicit [input] {#code} [output] {#comment} style (Table II).",
            "performance_metric": "BLEU-4, EM, CodeBLEU (CB)",
            "performance_value": "Examples (Table VIII): GPT-3.5 zero-shot program synthesis EM = 0.20; GPT-3.5 baseline (2-shot random) EM = 10.00; GPT-3.5 carefully-designed (4-shot BM-25+Similarity) EM = 15.20. ChatGPT zero-shot program synthesis EM = 3.40; ChatGPT carefully-designed EM = 14.20.",
            "baseline_performance": "Baseline (widely-used) few-shot demonstration: typically 2 randomly selected examples in random order (prior work setting) — example Codex baseline EM on CoNaLa = 14.20 (Table VIII).",
            "performance_change": "Zero-shot to carefully-designed few-shot: very large absolute and relative improvements (e.g., GPT-3.5 CoNaLa EM from 0.20 to 15.20 = +15.0 absolute). Carefully-designed few-shot vs baseline few-shot: Codex CoNaLa EM 21.40 vs baseline 14.20 = +7.2 absolute (+50.81% relative); reported minimum cross-task improvements claimed in paper: at least +9.90% BLEU-4 (summarization), +175.96% EM (bug fixing), +50.81% EM (synthesis) when using carefully-designed demonstrations versus baseline methods.",
            "experimental_setting": "Zero-shot uses same instruction templates; few-shot used temperature=0. Baseline demonstration = randomly select 2 examples; carefully-designed = BM-25 retrieval + Similarity ordering + 4 examples.",
            "statistical_significance": null,
            "uuid": "e7600.5",
            "source_info": {
                "paper_title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "careful-demo-vs-baseline",
            "name_full": "Carefully-designed Demonstration (BM-25 + Similarity + 4-shot) versus Baseline Demonstration",
            "brief_description": "A demonstration construction recipe combining BM-25 instance-level retrieval, similarity-based ascending ordering, and 4-shot prompts yields substantial improvements over widely-used baseline demonstration construction (random 2-shot).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002), GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo)",
            "model_description": "API-based LLMs evaluated with deterministic decoding (temperature=0) and explicit demonstration templates (Table II).",
            "model_size": null,
            "task_name": "Code summarization (CSN), Bug fixing (B2F), Program synthesis (CoNaLa)",
            "task_description": "Tasks as above (code-&gt;comment, bug fix with guidance, NL-&gt;code).",
            "problem_format": "Few-shot prompt with 4 instance-level examples retrieved by BM-25, arranged by ascending similarity to the test sample (most similar close to test), using explicit [input]/[output] delimiters and a one-line template instruction.",
            "format_category": "prompt style / crafted demonstration",
            "format_details": "Carefully-designed demonstration: BM-25 retrieval, Similarity ordering, 4 examples, temperature=0, explicit templates per task (Table II). Baseline demonstration (prior works): randomly select 2 examples, random order.",
            "performance_metric": "BLEU-4 (summarization), Exact Match (EM) for bug fixing and synthesis, CodeBLEU for synthesis.",
            "performance_value": "Codex examples from Table VIII: Code summarization (CSN) BLEU-4: Baseline=17.37 -&gt; Carefully-designed=22.73. Bug fixing (B2F, EM): Baseline=9.70 -&gt; Carefully-designed=32.25. Program synthesis (CoNaLa, EM): Baseline=14.20 -&gt; Carefully-designed=21.40.",
            "baseline_performance": "As above (random-2-shot baseline per Table VIII).",
            "performance_change": "Codex: Summarization BLEU-4 +5.36 absolute (+30.9% relative); Bug fixing EM +22.55 absolute (+232.5% relative); Program synthesis EM +7.20 absolute (+50.8% relative). The paper reports that across tasks the carefully-designed demonstration improved BLEU-4, EM, and EM by at least +9.90%, +175.96%, and +50.81% respectively compared to widely-used baseline construction methods.",
            "experimental_setting": "Temperature=0, frequency_penalty=0, presence_penalty=0; Codex input length limit 8001 tokens (examples truncated when necessary using limit/(N+1) policy); demonstration size = 4 in these comparisons.",
            "statistical_significance": null,
            "uuid": "e7600.6",
            "source_info": {
                "paper_title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What makes good in-context examples for gpt-3?",
            "rating": 2
        },
        {
            "paper_title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-based prompt selection for code-related few-shot learning",
            "rating": 2
        },
        {
            "paper_title": "Automated program repair in the era of large pre-trained language models",
            "rating": 1
        },
        {
            "paper_title": "Diverse demonstrations improve in-context compositional generalization",
            "rating": 1
        }
    ],
    "cost": 0.01843375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What Makes Good In-context Demonstrations for Code Intelligence Tasks with LLMs?</h1>
<p>Shuzheng Gao^{1†}, Xin-Cheng Wen^{1}, Cuiyun Gao^{1∗}, Wenxuan Wang^{2}, Hongyu Zhang^{3}, Michael R. Lyu^{2}
^{1} School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China
^{2} Department of Computer Science and Engineering, The Chinese University of Hong Kong, China
^{3} School of Big Data and Software Engineering, Chongqing University, China
szgao98@gmail.com, xiamenwxc@foxmail.com, gaocuiyun@hit.edu.cn, hyzhang@cqu.edu.cn, {wxwang,lyu}@cse.cuhk.edu.hk</p>
<h6>Abstract</h6>
<p>Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least $\mathbf{9 . 9 0} \mathbf{\%}, \mathbf{1 7 5 . 9 6} \%$, and $\mathbf{5 0 . 8 1} \%$ on code summarization, bug fixing, and program synthesis, respectively.</p>
<h2>I. INTRODUCTION</h2>
<p>Recently, there has been an increasing focus on code intelligence research, aiming at reducing the burden on software developers and enhancing programming productivity [1], [2]. With the large-scale open-source code corpora and the progress of deep learning techniques, various neural source code models have been developed and have achieved state-of-the-art performance on a variety of code intelligence tasks including code summarization [3], bug fixing [4], and program synthesis [5].</p>
<p>In recent years, the advent of pre-training techniques has significantly advanced progress in this area. For instance, CodeBERT [6], a BERT-based model pre-trained on</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: An example of in-context learning on code summarization task.
both natural and programming language data, has demonstrated promising performance in various code intelligence tasks [4], [7]. Other subsequent pre-trained code models such as PLBART [8] and CodeT5 [9] further achieve much improvement over CodeBERT. However, the size and training data of the above models are limited, which may hinder the models from achieving their potential [10]. In these years, we have witnessed explosive growth in the size of pretrained models. Various billion-level large language models are proposed such as GPT-3 [11] and PALM-E [12]. For instance, the size of the pre-trained model PALM-E [12] (562B) in 2023 is over two thousand times larger than the largest model BERT [13] (223M) in 2018.</p>
<p>As the size of language models and training data continues to increase, large language models (LLMs) demonstrate various emergent abilities. One such ability is in-context learning (ICL) [11], [14], which allows models to learn from just a few examples within a specific context. As shown in Fig. 1, ICL utilizes a demonstration including task instructions and a few examples to describe the task, which are then concatenated with a query question to form an input for the language model to make predictions. The most significant difference between ICL and traditional tuning methods such as finetuning [6] is that it is training-free and does not need parameter updates. The training paradigm enables ICL to be directly used upon any LLMs and significantly reduces the training costs of adapting models to new tasks [11]. Recent studies show that ICL has achieved impressive results in various</p>
<p>fields, including logic reasoning [15], dialogue system [16], and program repair [17]-[19], and can even outperform the supervised fine-tuning methods trained on large task-specific data.</p>
<p>Although ICL has been proven useful in code intelligence tasks, the performance of ICL is known to strongly rely on the quality of demonstrations [20], [21]. Existing studies [17], [19] mainly construct demonstrations by randomly selecting and arranging the demonstration examples. To the best of our knowledge, there is a lack of an in-depth investigation of ICL for code intelligence tasks. Considering the impressive performance of ICL, it is worthy to understand the impact of demonstration design and investigate the challenges of applying ICL for code intelligence tasks. In this work, we systematically analyze how different demonstration construction methods influence the performance of ICL on code intelligence tasks, aiming at answering the following question: What makes good in-context demonstrations for code intelligence tasks with LLMs? By analyzing the design space of incontext demonstrations, our study mainly focuses on three aspects of in-context demonstrations, including the selection, order, and number of demonstration examples. We conduct an experimental study on three popular code intelligence tasks including code summarization, bug fixing, and program synthesis. Specifically, we mainly investigate the following four research questions (RQs):</p>
<p>1) What kind of selection methods are helpful for ICL in code intelligence tasks?
2) How should demonstration examples be arranged for ICL in code intelligence tasks?
3) How does the number of demonstration examples in a prompt impact the performance of ICL in code intelligence tasks?
4) How is the generalizability of our findings?</p>
<p>To answer the first RQ, we compare a wide range of demonstration selection methods such as random selection, similarity-based selection, and diversity-based selection. We also experiment with different retrieval methods in the similarity-based selection to find which retrieval method is more helpful for ICL in code intelligence tasks. To answer the second RQ, we compare random ordering with two other ordering methods including similarity and reverse similarity, towards exploring the impact of different ordering methods. To answer RQ3, we change the number of demonstration examples in the prompt and investigate whether the performance of ICL also grows with the increase in the number of demonstration examples. To answer the last RQ, we experiment on different LLMs and validate the findings we achieve in the above RQs.</p>
<p>Key Findings. Based on the extensive experiments, our study reveals several key findings:</p>
<p>1) Both similarity and diversity in demonstration selection are important factors for ICL in code intelligence tasks. They not only enhance the overall performance but also lead to more stable predictions.
2) The order of demonstration examples has a large impact on the performance of ICL. In most cases, placing similar samples at the end of a prompt achieves better results.
3) Increasing the number of demonstration examples can be beneficial for ICL, provided that the examples are not cut off due to the input length limitation of LLMs. Careful attention should be paid to this issue, as the length of code is generally longer than natural language.
We also show that a carefully-designed demonstration based on the achieved findings can lead to substantial improvements over the widely-used demonstration construction methods [17], [19], [22], e.g., improving BLEU-4, EM, and EM by at least $9.90 \%, 175.96 \%$, and $50.81 \%$ on code summarization, bug fixing and program synthesis, respectively.</p>
<p>Contributions. In summary, the main contributions of this work are as follows:</p>
<p>1) To the best of our knowledge, this paper represents the first systematic study on how to construct effective demonstrations for code intelligence tasks.
2) Our comprehensive exploration of demonstration design highlights a range of findings for improving ICL's performance in code intelligence tasks.
3) We discuss the implications of our findings for researchers and developers and future work for code intelligence tasks in the era of large language models.</p>
<h2>II. BACKGROUND</h2>
<h2>A. Large Language Models</h2>
<p>LLMs have become a ubiquitous part of Natural Language Processing (NLP) due to their exceptional performance [11], [23]. These models typically follow the Transformer [24] architecture and are trained on large-scale corpora using selfsupervised objectives such as masked language modeling [13]. The size of LLMs has increased significantly in the past few years. For example, the parameters of recent LLMs like GPT3 [11] and PALM-E [12] are over one hundred billion. Apart from the LLMs for general purposes, there are also LLMs with billion-level parameters trained on code corpora, such as AlphaCode [25], and Codex [2]. The OpenAI's Codex is a large pre-trained code model that is capable of powering Copilot. AlphaCode [25] is a 41-billion-large model trained for generating code in programming competitions like Codeforces. Recently, LLMs like ChatGPT [26] and GPT-4 [23] have also shown impressive performance in many code intelligence tasks.</p>
<p>Apart from proposing new LLMs, how to effectively leverage them has also become an important research topic. A prevalent method is to fine-tune the model and update its parameters on downstream datasets [13]. Recently, promptbased fine-tuning has been proposed, which aims to convert the training objective of downstream tasks into a similar form as the pre-training stage [27], [28]. Considering the cost of tuning the whole model, various Parameter Efficient Tuning methods have been proposed, such as Adapter [29], Lora [30], and prefix tuning [31]. These methods keep most of the parameters in the model frozen and only tune a small portion of them.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Illustration of design space of in-context demonstrations.</p>
<h3><em>B. In-context Learning</em></h3>
<p>Tuning a large pre-trained model can be expensive and impractical for researchers, especially when limited fine-tuned data is available for certain tasks. ICL offers a new alternative that uses language models to perform downstream tasks without requiring parameter updates [11], [14]. It leverages a demonstration in the prompt to help the model learn the input-output mapping of the task. This new paradigm has achieved impressive results in various tasks such as logic reasoning and program repair [15], [17], [19].</p>
<p>Specifically, as shown in Fig. 1, ICL employs <em>N</em> demonstration examples {(x1, y1), (x2, y2), ..., (xN, yN)} and further reconstructs them into reconstructed examples {(x'1, y'1), (x'2, y'2), ..., (x'N, y'N)} by natural language instructions and prompt template, where x<sup>i</sup>, y<sup>i</sup>, x'<sup>i</sup>, y'<sup>i</sup> are the input, output, reconstructed input, and reconstructed output, respectively. Typically, the value of <em>N</em> is relatively small, i.e., fewer than 50 samples, which is significantly smaller than the size of the training set in previous fine-tuned methods [6], [9]. This setting is referred to as <em>few-shot in-context learning</em>. Specially, when the value of <em>N</em> is zero, it is called the <em>zero-shot in-context learning setting</em>. Then, ICL concatenates the reconstructed demonstration examples d<sup>1</sup> to d<sup>N</sup> literally into demonstration D = x'1 ∥ y'1 ∥ x'2 ∥ y'2 ∥ ... ∥ x'N ∥ y'N, and further adds the test sample at the end to construct the input prompt P = D ∥ x'text, where ∥ denotes the literal concatenation operation. This prompt is finally fed into the language model for predicting the label ytext for test samples.</p>
<p>Previous studies in NLP have shown that the performance of ICL is strongly dependent on the quality of the demonstration. For example, Liu et al. [20] show that selecting demonstration examples with higher similarity or increasing the number of demonstration examples can improve ICL's performance. The results in [21] show that the order of demonstration examples also has a large impact on the results. Following previous studies, we summarize three key factors to consider when designing a demonstration for ICL: the selection, ordering, and number of demonstration examples, as shown in Fig. 2.</p>
<p>We would like to further clarify that there are two types of demonstration in ICL: <strong>task-level</strong> demonstration and <strong>instance-level</strong> demonstration [32], [33]. The task-level demonstration uses the same demonstration examples for all test samples and does not take the difference of each test sample into consideration, while the instance-level demonstration selects different demonstration examples for different test samples. Although instance-level demonstrations generally perform better than task-level demonstrations, it requires a labeled training set in advance for retrieval. The task-level demonstration is more flexible as it can be used in scenarios where very few data are labeled, or no labeled data are available by selecting few representative data for human labeling [33]. In this paper, we investigate both the task-level and instance-level demonstration construction methods for code intelligence tasks.</p>
<h1>III. EXPERIMENTAL EVALUATION</h1>
<h2><em>A. Research Questions</em></h2>
<p>We design experiments to investigate the impact of the selection, ordering, and number of demonstrations on ICL for code intelligence tasks. Our research aims to answer the following questions:</p>
<ul>
<li><strong>RQ1:</strong> What kind of selection methods are helpful for ICL in code intelligence tasks?</li>
<li><strong>RQ2:</strong> How should demonstration examples be arranged for ICL in code intelligence tasks?</li>
<li><strong>RQ3:</strong> How does the number of demonstration examples in a prompt impact the performance of ICL in code intelligence tasks?</li>
<li><strong>RQ4:</strong> How is the generalizability of our findings?</li>
</ul>
<p>In RQ1, we aim at verifying whether selecting similar and diverse demonstration examples is helpful. Besides, we also compare different retrieval methods to analyze the impact of different similarity measurement methods for ICL. RQ2 aims at investigating the influence of ordering methods by comparing random ordering with similarity-based ordering. In RQ3, we want to explore whether increasing the number of examples could bring better performance for ICL. In RQ4, we evaluate whether the findings achieved in RQ1-RQ3 are also applicable to different LLMs for verifying the generalizability of the findings.</p>
<h2><em>B. Evaluation tasks</em></h2>
<p>We conduct experiments on three popular code intelligence tasks: code summarization, bug fixing, and program synthesis.</p>
<p><em>1) Code Summarization:</em> Code summarization, also known as code comment generation, aims to generate useful comments automatically for a given code snippet [7]. Recent work mainly formulates it as a sequence-to-sequence neural machine translation (NMT) task and involves pre-trained techniques to achieve better performance [9], [34].</p>
<p>TABLE I: Statistics of the benchmark datasets.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Datasets</th>
<th>Train</th>
<th>Dev</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Code Summarization</td>
<td>CSN-Java</td>
<td>164,923</td>
<td>5,183</td>
<td>10,955</td>
</tr>
<tr>
<td></td>
<td>TLC</td>
<td>69,708</td>
<td>8,714</td>
<td>6,489</td>
</tr>
<tr>
<td>Bug Fixing</td>
<td>B2F_{small}</td>
<td>46,628</td>
<td>5,828</td>
<td>5,831</td>
</tr>
<tr>
<td></td>
<td>B2F_{medium}</td>
<td>53,324</td>
<td>6,542</td>
<td>6,538</td>
</tr>
<tr>
<td>Program Synthesis</td>
<td>CoNaLa</td>
<td>2,389</td>
<td>-</td>
<td>500</td>
</tr>
</tbody>
</table>
<p>Datasets. To evaluate the performance of code summarization, we use two widely-used datasets: CodeSearchNet (CSN) [35] and TLCodeSum (TLC) [7]. CSN is a largescale source code dataset mined from open-source GitHub repositories. It contains code summarization data in six programming languages, i.e., Java, Go, JavaScript, PHP, Python, and Ruby. The dataset is split into training, validation, and test sets in the proportion of 8:1:1. In this study, considering our time and resource limitation, we use the Java portion of the filtered CSN dataset in CodeBERT [6], which contains 181,061 samples across the training, validation, and test sets for evaluation. TLC has 87,136 code-comment pairs crawled from 9,732 open-source Java projects on GitHub with at least 20 stars. The code snippets are all at the method level and the comments of corresponding Java methods are considered as code summaries. The portion of training, validation, and test set is also 8:1:1. As reported in previous work, there are duplicated data in the training and test set. Therefore, we follow previous work [36] and remove the duplicated data, and finally get a test set with 6,489 samples.</p>
<p>Metrics. We use three widely-adopted metrics for code summarization evaluation: BLEU-4 [37], ROUGE-L [38] and METEOR [39] for evaluation. These metrics evaluate the similarity between generated summaries and ground-truth summaries and are widely used in code summarization [3], [36], [40].</p>
<p>2) Bug Fixing: Bug fixing is the task of automatically fixing bugs in the given code snippet. It helps software developers find and fix software errors [4], [41].</p>
<p>Datasets. The dataset for bug fixing is B2F which is collected by Tufano et al. [4] from bug-fixing commits in GitHub. We use the multi-model version proposed in MODIT [42] for experiments as it contains both the code changes and the fix instruction. The model is given both the buggy code and natural language fix guidance to predict the fixed code. We follow their original setting to split the dataset into two parts B2F_{medium} and B2F_{small} based on the length of code tokens (the code length of B2F_{medium} is between 50 and 100 tokens and that of B2F_{small} is below 50 tokens).</p>
<p>Metrics. We follow previous work [43] and use Exact Match (EM) and BLEU-4 for both datasets.</p>
<p>3) Program Synthesis: Program synthesis is the task of generating source code based on the given natural language description. It provides practical assistance to developers and enhances their productivity [2].</p>
<p>Datasets. For program synthesis, we use the CoNaLa [44] dataset for evaluation. This dataset consists of 2,889 (intent, code) pairs mined from Stack Overflow in Python. We directly TABLE II: Prompt template for each task. Here text in the form of {#xxx} will be filled in actual inputs from the dataset.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Template</th>
</tr>
</thead>
<tbody>
<tr>
<td>Code Summarization</td>
<td>Generate comment (summarization) for this code</td>
</tr>
<tr>
<td></td>
<td>[input] {#code} [output] {#comment}</td>
</tr>
<tr>
<td>Bug Fixing</td>
<td>Fix the bug according to the guidance [input]</td>
</tr>
<tr>
<td></td>
<td>{#buggy code} <x> {#instruction} [output] {#fixed code}</td>
</tr>
<tr>
<td>Program Synthesis</td>
<td>Generate code based on the requirement</td>
</tr>
<tr>
<td></td>
<td>[input] {#requirement}[output] {#code}</td>
</tr>
</tbody>
</table>
<p>use the original partition of the dataset, which includes 2,389 samples for training and 500 samples for testing.</p>
<p>Metrics. We follow previous work [43] and evaluate the performance of program synthesis with four metrics including Exact Match (EM), CodeBLEU (CB), Syntax Match (SM), and Dataflow Match (DM). EM measures whether the code generated by the model is identical to the goal code. CB [45] is a modified version of BLEU designed specifically for code, which leverages syntax and semantic information such as Abstract Syntax Tree (AST) and data flow to measure the similarity of two code snippets. SM and DM are two components that calculate the matching subtrees and data flow edges' proportion, respectively.</p>
<h2>C. Implementation</h2>
<p>We utilize the OpenAI Codex (code-davinci-002) API [2] in our paper for all experiments in the first three RQs. In RQ4, we further use the API of GPT-3.5 (text-davinci-003) [11] and ChatGPT (gpt-3.5-turbo) [26] for experiments. As for the hyperparameters of the APIs, following the previous work [46], [47], we set the temperature to 0 to get the deterministic output. The frequency_penalty and presence_penalty are also set to 0. The input length limitation of Codex, GPT-3.5, and ChatGPT is 8,001, 4,096, and 4,097 tokens, respectively. Hence we cut off the input code of each demonstration example to $\frac{8001}{N+1}$, $\frac{4096}{N+1}$, and $\frac{4097}{N+1}$ tokens, respectively, where $N$ represents the number of demonstration examples. Empirically, it took approximately 6 hours to evaluate 1,000 examples for Codex. To avoid excessive time costs, we randomly sample a small test set (2,000 samples) for each dataset with over 2,000 test samples. We use four examples in the demonstration in RQ1 and RQ2, and further discuss the impact of the number of demonstration examples in RQ3. The templates used in this study are shown in Table II. We also show some examples in our GitHub repository ${ }^{1}$. We conduct all the experiments on a server with 2 Nvidia RTX 3090 GPUs. The GPUs are used in the dense retrieval process.</p>
<h2>IV. EXPERIMENTAL RESULTS</h2>
<h2>A. RQ1: Demonstration Selection</h2>
<p>1) Experimental design: We first explore the impact of demonstration selection methods on ICL for code-related tasks. To provide a comprehensive study, we adopt different</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>TABLE III: Experimental results of different demonstration selection methods on Code Summarization. “Avg” and “CV” denote the average results and Coefficient of Variation over three different orders, respectively.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Code Summarization</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CSN</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>TLC</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>BLEU-4</td>
<td></td>
<td>ROUGE-L</td>
<td></td>
<td>METEOR</td>
<td></td>
<td>BLEU-4</td>
<td></td>
<td>ROUGE-L</td>
<td></td>
<td>METEOR</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
<td></td>
</tr>
<tr>
<td>Task-level Demonstration</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>19.64</td>
<td>1.44</td>
<td>35.46</td>
<td>1.88</td>
<td>15.30</td>
<td>1.54</td>
<td>17.29</td>
<td>0.71</td>
<td>34.28</td>
<td>0.61</td>
<td>12.48</td>
<td>0.67</td>
<td></td>
</tr>
<tr>
<td>KmeansRND</td>
<td>20.71</td>
<td>0.82</td>
<td>38.03</td>
<td>0.44</td>
<td>16.34</td>
<td>0.83</td>
<td>17.91</td>
<td>1.19</td>
<td>35.69</td>
<td>1.60</td>
<td>13.48</td>
<td>0.91</td>
<td></td>
</tr>
<tr>
<td>Instance-level Demonstration</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BM-25</td>
<td>22.35</td>
<td>0.46</td>
<td>38.31</td>
<td>0.56</td>
<td>17.01</td>
<td>0.78</td>
<td>36.96</td>
<td>0.84</td>
<td>51.42</td>
<td>0.79</td>
<td>24.22</td>
<td>0.99</td>
<td></td>
</tr>
<tr>
<td>SBERT</td>
<td>22.27</td>
<td>0.23</td>
<td>38.39</td>
<td>0.42</td>
<td>16.91</td>
<td>0.22</td>
<td>36.42</td>
<td>0.61</td>
<td>50.47</td>
<td>0.40</td>
<td>23.86</td>
<td>0.68</td>
<td></td>
</tr>
<tr>
<td>UniXcoder</td>
<td>22.11</td>
<td>0.61</td>
<td>38.23</td>
<td>0.53</td>
<td>16.81</td>
<td>0.23</td>
<td>36.77</td>
<td>0.52</td>
<td>51.11</td>
<td>0.29</td>
<td>24.08</td>
<td>0.79</td>
<td></td>
</tr>
<tr>
<td>CoCoSoDa</td>
<td>21.92</td>
<td>0.46</td>
<td>37.85</td>
<td>0.22</td>
<td>16.78</td>
<td>0.24</td>
<td>36.91</td>
<td>0.69</td>
<td>50.69</td>
<td>0.53</td>
<td>24.08</td>
<td>0.39</td>
<td></td>
</tr>
<tr>
<td>Oracle (BM-25)</td>
<td>27.69</td>
<td>0.43</td>
<td>46.17</td>
<td>0.14</td>
<td>20.26</td>
<td>0.22</td>
<td>43.16</td>
<td>0.15</td>
<td>59.17</td>
<td>0.09</td>
<td>28.09</td>
<td>0.16</td>
<td></td>
</tr>
</tbody>
</table>
<p>TABLE IV: Experimental results of different demonstration selection methods on Bug Fixing.</p>
<p>| Approach | Bug Fixing | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>TABLE V: Experimental results of different demonstration selection methods on Program Synthesis.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Program Synthesis</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CB</td>
<td></td>
<td>SM</td>
<td></td>
<td>DM</td>
<td></td>
<td>EM</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
</tr>
<tr>
<td></td>
<td>Task-level Demonstration</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>28.36</td>
<td>1.30</td>
<td>44.37</td>
<td>0.83</td>
<td>39.70</td>
<td>1.33</td>
<td>16.00</td>
<td>1.60</td>
</tr>
<tr>
<td>KmeansRND</td>
<td>28.03</td>
<td>1.47</td>
<td>44.41</td>
<td>0.54</td>
<td>37.31</td>
<td>1.54</td>
<td>17.03</td>
<td>1.06</td>
</tr>
<tr>
<td></td>
<td>Instance-level Demonstration</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BM-25</td>
<td>30.37</td>
<td>0.91</td>
<td>46.22</td>
<td>0.84</td>
<td>40.75</td>
<td>1.06</td>
<td>18.53</td>
<td>0.50</td>
</tr>
<tr>
<td>SBERT</td>
<td>29.08</td>
<td>0.70</td>
<td>44.91</td>
<td>0.31</td>
<td>39.81</td>
<td>3.01</td>
<td>16.13</td>
<td>2.54</td>
</tr>
<tr>
<td>UniXcoder</td>
<td>28.96</td>
<td>0.50</td>
<td>43.93</td>
<td>0.67</td>
<td>37.96</td>
<td>1.12</td>
<td>16.00</td>
<td>3.53</td>
</tr>
<tr>
<td>CoCoSoDa</td>
<td>29.42</td>
<td>0.82</td>
<td>44.62</td>
<td>0.70</td>
<td>40.91</td>
<td>1.12</td>
<td>16.30</td>
<td>0.86</td>
</tr>
</tbody>
</table>
<p>three random orders and CV which measures their sensitivity to different orders. In Fig. 3, we show the distribution of results with different groups of examples for Random and KmeansRND.</p>
<p>Diversity of examples is beneficial for task-level demonstration. As can be seen in Table III-V and Fig. 3, by comparing the results on Random and KmeansRND, we can find that in most cases improving the diversity of task-level demonstrations can not only improve the average performance of ICL but also reduce the fluctuation brought by different groups of examples. For example, as shown in Table III, comparing the results of code summarization on CSN, the average improvements of KmeansRND over Random are 5.45%, 7.25%, and 6.80% with respect to BLEU-4, ROUGE-L, and METEOR, respectively. Besides, we can also find that the performance of different in-context demonstration examples of Random varies a lot, and improving the diversity of selected examples can reduce this variation in general. For example, as shown in Fig. 3 (a), the gap between the best and worst BLEU-4 score of Random is about 2.5 while that of KmeansRND is only about 0.6. This indicates that improving the diversity of selected demonstration examples is beneficial for building task-level demonstration.</p>
<p>Finding 1: Diversity of examples is helpful for the demonstration selection of ICL. It can help improve overall performance and lead to a more stable prediction regarding different groups of examples.</p>
<p>BM-25 is a simple and effective method for instancelevel demonstration. By comparing the results of different instance-level demonstration methods, we can find that the simple BM-25 method can achieve comparable or even better performance than other dense retrieval methods on demonstration selection in ICL. For example, the average EM of BM-25 on Program Synthesis is 18.53, which outperforms three strong dense retrieval methods SBERT, UniXcoder, and CoCoSoDa by 14.88%, 15.81%, and 13.68%, respectively. This result indicates that BM-25 serves as an effective baseline approach and could be taken into account in future studies of demonstration selection for code intelligence tasks.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Comparison of the performance distribution of Random and KmeansRND regarding different groups of examples on three tasks.</p>
<p>Finding 2: The retrieval methods for demonstration selection can impact the performance of ICL, among which BM-25 is a simple and effective method.</p>
<p>Instance-level demonstration outperforms task-level demonstration greatly. As shown in Table III-V, we can find that instance-level demonstration can achieve much better performance in all tasks. Specifically, the instance-level selection methods improve the best task-level demonstration's exact match results by at least 141.97% and 193.64% on B2F_{medium} and B2F_{small}, respectively. These results indicate that selecting similar demonstration examples specifically for each test sample can benefit ICL in code intelligence tasks a lot.</p>
<p>The task-level demonstration is more sensitive to the order than the instance-level demonstration. By comparing the CV of task-level demonstration and instance-level demonstration, we can find that the performance of instance-level demonstration is generally more stable than task-level demonstration regarding different example orders. Specifically, as shown in Table IV, the CV of BLEU-4 of task-level demonstration KmeansRND to the order is 0.17 and 1.36 on two bug fixing datasets, which is much larger than that of instance-level demonstration methods (e.g., 0.09 and 0.13 for BM-25, respectively). This indicates that selecting examples by similarity is more robust to the changes in the demonstration order and we should carefully arrange the order of</p>
<p>TABLE VI: Experimental results of different demonstration ordering methods.</p>
<p>| Approach | | Code Summarization (CSN) | | | Bug Fix (B2F $_{\text {small }}$ ) | | Program Synthesis (CoNaLa) | | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | | BLEU-4 | ROUGE-L | METEOR | BLEU-4 | EM | CB | SM | DM | EM | | Random | 20.46 | 36.71 | 16.17 | 72.40 | 9.52 | 27.72 | 44.46 | 37.53 | 15.53 | | Similarity | 21.04 | 37.86 | 16.26 | 72.02 | 9.93 | 28.47 | 44.87 | 37.79 | 16.00 | | Reverse Similarity | 19.78 | 33.71 | 15.64 | 71.44 | 9.02 | 27.62 | 44.48 | 37.96 | 15.20 | | KmeansRND | Random | 20.67 | 37.64 | 15.97 | 72.29 | 8.60 | 26.64 | 42.97 | 37.24 | 16.87 | | Similarity | 20.69 | 37.62 | 16.05 | 72.90 | 10.15 | 27.20 | 42.97 | 36.93 | 16.40 | | | Reverse Similarity | 20.55 | 37.43 | 16.20 | 72.05 | 9.78 | 27.09 | 43.74 | 37.19 | 16.60 | | BM-25 | Random | 22.35 | 38.31 | 17.01 | 77.54 | 30.45 | 30.37 | 46.22 | 40.75 | 18.53 | | Similarity | 22.23 | 38.12 | 17.01 | 77.76 | 30.95 | 30.83 | 46.41 | 41.33 | 17.60 | | Reverse Similarity | 22.13 | 38.26 | 16.91 | 77.60 | 29.80 | 30.01 | 45.72 | 39.60 | 18.20 | | | | | | | | | | | |</p>
<p>demonstration examples when using task-level demonstration</p>
<p>Finding 3: Compared with task-level demonstration, instance-level demonstrations can achieve much better performance and are generally more robust to the changes in the demonstration order.</p>
<p>Apart from the above, we can also observe in Table III that the best demonstration selection method BM-25 still has a large gap with the Oracle. This indicates that these retrieval methods may fail to select semantic similar examples and there exists a large space for further improvement concerning the demonstration selection method for code intelligence tasks.</p>
<h2>B. RQ2: Demonstration Order</h2>
<p>1) Experimental setup: In RQ1, we have found that the order of demonstration examples impacts the performance of ICL on code intelligence tasks, especially on task-level demonstration. Therefore, in this section, we explore how to better arrange the demonstration examples in ICL. Inspired by the finding that the task-level demonstration is more sensitive to the example order than the instance-level demonstration, we suppose that the order of similarities between each demonstration example and test sample plays an important role in ICL.</p>
<p>To verify this, in this RQ, we compare random order with two basic ordering methods, i.e., Similarity and Reverse Similarity. In the Similarity method, we compare the similarity of each example with test sample and the example with a higher similarity will be placed closer to the test sample. On the contrary, for the Reverse Similarity method, the demonstration examples will be placed in descending order according to their similarity to the test sample. We experiment with three demonstration selection methods here. As illustrated in RQ1, since the order arrangement is important for task-level demonstration, we use both the Random and KmeansRND for experiments. As for instance-level demonstration, we conduct experiments on BM-25, since it shows the best performance among all the instance-level demonstration selection methods. 2) Analysis: From the results in Table VI, we can find that placing the demonstration examples in ascending order based on their similarity to the test sample performs generally better than the reverse. Specifically, Similarity consistently outperforms Reverse Similarity on code summarization and bug fixing by at least $0.45 \%$ and $0.21 \%$ with respect to BLEU4 and EM, respectively. By further comparing all the results together, we can observe that similarity achieves the best performance in most cases. Specifically, it achieves the best performance in $62.96 \%$ (17/27) metrics and tasks. However, we can also observe that there are some cases in which both Similarity and Reverse Similarity perform worse than the average results of using random order, indicating that more complex demonstration ordering methods can be explored by the future work.</p>
<p>Finding 4: The different orders of demonstration examples can impact the performance of ICL. Arranging the demonstration examples based on their similarity to the test sample in ascending order can achieve relatively better results in most cases.</p>
<h2>C. RQ3: The Number of Demonstration Examples</h2>
<p>1) Experimental setup: In this section, we investigate whether the increase in the number of examples will improve the performance of ICL on code intelligence tasks. We vary the number of demonstration examples from 1 to 64 . We use BM-25 and Similarity as demonstration selection and demonstration ordering methods, respectively, based on the above findings. 2) Analysis: As shown in Fig. 4, we can find that the performance of ICL on all the tasks increases with the number of demonstration examples at first. However, when the number of examples is above 16 , the results on different tasks show different trends. For example, for bug fixing, the performance achieves the peak when the number of demonstration examples is 32 and suffers from a significant drop when further increasing the number to 64 . As for program synthesis, the performance keeps increasing and tends to be stable when the number exceeds 32 . We believe that the different trends are caused by the truncation problem [59], [60]. As illustrated in Section III-C, when increasing the number of examples, the length of the whole demonstration will increase and the examples might be cut off to avoid exceeding the length limitation of LLMs. Specifically, for the $\mathrm{B} 2 \mathrm{~F}_{\text {small }}$ dataset, all the examples are complete without cutting off when the number of examples is below 32. However, when the number becomes $32,2.33 \%$ demonstration examples are cut off. When</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Experimental results of ICL with different number of demonstration examples.</p>
<p>TABLE VII: Experiments of generalization of findings on GPT3.5 and ChatGPT.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th></th>
<th>CB</th>
<th></th>
<th>EM</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Selection</td>
<td>Order</td>
<td>Avg</td>
<td>CV</td>
<td>Avg</td>
<td>CV</td>
</tr>
<tr>
<td></td>
<td></td>
<td>GPT-3.5</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>Random</td>
<td>26.60</td>
<td>3.01</td>
<td>12.32</td>
<td>4.73</td>
</tr>
<tr>
<td>KmeansRND</td>
<td>Random</td>
<td>28.26</td>
<td>1.93</td>
<td>13.60</td>
<td>1.65</td>
</tr>
<tr>
<td>UniXcoder</td>
<td>Random</td>
<td>30.06</td>
<td>0.53</td>
<td>13.73</td>
<td>1.13</td>
</tr>
<tr>
<td>BM-25</td>
<td>Random</td>
<td>30.81</td>
<td>1.05</td>
<td>14.40</td>
<td>1.81</td>
</tr>
<tr>
<td>BM-25</td>
<td>Similarity</td>
<td>30.69</td>
<td>0.00</td>
<td>15.20</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>Random</td>
<td>28.17</td>
<td>1.98</td>
<td>11.88</td>
<td>4.24</td>
</tr>
<tr>
<td>KmeansRND</td>
<td>Random</td>
<td>28.25</td>
<td>2.31</td>
<td>12.92</td>
<td>1.78</td>
</tr>
<tr>
<td>UniXcoder</td>
<td>Random</td>
<td>29.33</td>
<td>1.85</td>
<td>14.32</td>
<td>2.87</td>
</tr>
<tr>
<td>BM-25</td>
<td>Random</td>
<td>28.95</td>
<td>5.75</td>
<td>13.47</td>
<td>1.82</td>
</tr>
<tr>
<td>BM-25</td>
<td>Similarity</td>
<td>30.03</td>
<td>0.00</td>
<td>14.20</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<p>further increasing the number to 64, the truncation problem happens on over 80% examples and 44.32% characters in those examples are discarded, resulting in a dramatic performance degradation. Since the length of samples in CSN and B2F_{small} datasets is much larger than that of the CoNaLa dataset, i.e., 557, 492, 101 characters per sample for CSN, B2F_{small}, and CoNaLa, respectively, the truncation problem does not appear on program synthesis even though the number grows to 64. Therefore, balancing the number of examples and the ensuing truncation problem is important for ICL.</p>
<p>Since the code is generally much longer than natural language [35], the truncation problem is easier to appear in code intelligence tasks. Besides, more examples will also lead to a larger cost of using external API and the inference time [61]. A smaller number of examples may be more appropriate for code intelligence tasks. From the results (Fig. 4), we can also find that the performance with four demonstration examples is good enough, achieving 96.48%, 97.80%, and 94.80% of the best performance on the three tasks with respect to EM, BLEU-4, and CodeBLEU, respectively. Therefore, considering the above trade-off, using four examples in the demonstration is a good choice for code intelligence tasks.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Comparison of the performance distribution of Random and KmeansRND regarding different groups of examples on GPT-3.5 and ChatGPT.</p>
<p><strong>Finding 5:</strong> More demonstration examples in the prompt will not always lead to better performance considering the truncation problem. To save costs, it is suggested that four examples are used in the demonstration.</p>
<h3><em>D. RQ4: The Generalization of Findings</em></h3>
<p><em>1) Experimental setup:</em> In this section, we evaluate the generalization of our findings on different LLMs. Apart from Codex, we experiment on two other LLMs including GPT-3.5 [11] and ChatGPT [26]. To validate the finding 1-4, we experiment with the following combinations of demonstration selection and ordering methods: Random+Random, KmeansRND+Random, UniXcoder+Random, BM-25+Random, and BM-25+Similarity. As for the finding 5 in RQ3, we use BM-25+Similarity as the selection and ordering method and vary the number of demonstration examples from 1 to 128 to validate whether the truncation will lead to performance degradation. Due to the cost limit, we choose the program synthesis task for evaluation.</p>
<p>We also measure how much improvement could our findings bring by comparing the performance of ICL with a carefully</p>
<p>TABLE VIII: Comparison of different demonstration construction methods on three LLMs.</p>
<p>| Approach | | Code Summarization (CSN) | | | Bug Fix (B2F $_{\text {email }}$ ) | | Program Synthesis (CoNaLa) | | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | | BLEU-4 | ROUGE-L | METEOR | BLEU-4 | EM | CB | SM | DM | EM | | Codex | Baseline demonstration | 17.37 | 32.04 | 13.43 | 69.07 | 9.70 | 27.54 | 44.56 | 37.07 | 14.20 | | | Carefully-designed demonstration | 22.73 | 39.52 | 17.35 | 77.54 | 32.25 | 32.07 | 48.03 | 42.88 | 21.40 | | GPT-3.5 | Zero-shot | 6.34 | 15.05 | 14.08 | 2.81 | 0.15 | 0.06 | 0.26 | 0.00 | 0.20 | | | Baseline demonstration | 14.55 | 21.53 | 13.81 | 62.87 | 9.15 | 26.36 | 36.94 | 41.67 | 10.00 | | | Carefully-designed demonstration | 15.99 | 26.78 | 16.70 | 71.70 | 25.25 | 30.69 | 43.95 | 44.78 | 15.20 | | ChatGPT | Zero-shot | 3.63 | 11.40 | 13.16 | 2.32 | 0.05 | 25.70 | 37.64 | 54.44 | 3.40 | | | Baseline demonstration | 10.76 | 20.02 | 14.83 | 41.57 | 4.60 | 27.62 | 41.83 | 46.85 | 9.40 | | | Carefully-designed demonstration | 11.90 | 23.31 | 16.93 | 53.92 | 18.15 | 30.03 | 45.04 | 44.26 | 14.20 | |</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Experimental results of different number of demonstration examples on GPT-3.5 and ChatGPT. designed demonstration, ICL with the widely-used demonstration construction method [17], [19], [22], and zero-shot ICL. In the carefully-designed demonstration, we use BM25 and Similarity as demonstration selection and ordering methods and employ four demonstration examples; while for the widely-used baseline demonstration construction method, we use the settings in previous work [17], [19], [22] and randomly select two demonstration examples from the training set with random order. As for zero-shot ICL, as illustrated in section II-B, no demonstration example is used and the model predicts only based on the instruction. 2) Analysis: We present the average results and CV of GPT3.5 and ChatGPT in Table VII. In Fig. 5 and Fig. 6, we present the performance distribution of different groups of examples and the impact of the number of examples on these two LLMs, respectively. The comparison of different demonstrations is shown in Table VIII. Due to the space limitation, we only present the performance on EM and CB and the results on other metrics can be found in our replication package. From these results, we can observe that our findings can also be applied to GPT-3.5 and ChatGPT.</p>
<p>As shown in Table VII and Fig. 6, we can observe that KmeansRND+Random not only outperforms Random+Random on the average results, but also has a more stable prediction distribution regarding different groups of examples. Taking GPT-3.5 as an example, KmeansRND+Random improves Random+Random by $6.24 \%$ and $10.39 \%$ with respect to CB and EM, respectively. This indicates that diversity is also beneficial for the demonstration construction of these two models (finding 1). Similarly, by comparing BM25+Random and UniXcoder+Random, we can also find that</p>
<p>BM-25 can achieve similar performance and even outperforms UniXcoder on GPT-3.5 by $2.50 \%$ and $4.88 \%$ with respect to CB and EM, respectively. This shows that BM-25 is also a simple and effective demonstration selection method in these two models (finding 2). Besides, on GPT-3.5 and ChatGPT, instance-level demonstrations also consistently outperform task-level demonstrations and achieve lower CV to different orders in general. It indicates that selecting demonstration examples by similarity is also beneficial for these two LLMs (finding 3). As for the impact of example order, we can also find that BM-25+Similarity consistently improves BM25+Random on all metrics and LLMs, e.g., improving the average EM by $5.56 \%$ and $5.42 \%$ on GPT-3.5 and ChatGPT, respectively (finding 4). As for the impact of numbers, we can observe similar trends on GPT-3.5 and ChatGPT in Fig. 6, the EM first increases with the number of demonstration examples. As the number further increases to $128,25.05 \%$ examples suffer from the truncation problem, resulting in a sudden degradation (finding 5).</p>
<p>Table VIII shows the comparison of different demonstrations. We can also observe that the performance of zero-shot ICL is very poor on all tasks, which indicates the importance of using demonstration examples to guide the LLM to understand the task. Besides, by comparing the performance of the carefully-designed demonstration with the baseline demonstration, we can find that ChatGPT with a carefully-designed demonstration outperforms the baseline demonstration by at least $10.59 \%, 294.57 \%$, and $51.06 \%$ on code summarization, bug fixing, and program synthesis with respect to BLEU-4, EM, and EM, respectively. The results indicate the importance of constructing a good demonstration, and the generalizability of the findings.</p>
<h2>V. DisCussion</h2>
<h2>A. Implications of Findings</h2>
<p>In this section, we discuss the implications of our work for researchers and developers.</p>
<p>Researchers: Our research demonstrates that the performance of few-shot in-context learning is highly dependent on the design of demonstrations. With well-constructed demonstrations, ICL can achieve much better performance. Our experimental results also show potential research directions in the era of LLM and ICL for the code intelligence community. Specifically:</p>
<ul>
<li>As shown in the results of RQ1, current state-of-the-art code retrieval models still have a large gap with the Oracle, indicating that these models fail to select examples with the highest semantic similarities. Therefore, effective code representation models for zero-shot code-to-code search are worth studying. Besides, designing example selection strategies based on the prior knowledge of each task or the properties of source code are also interesting directions that are worth exploring.</li>
<li>Placing similar examples in the back of all examples leads to relatively better performance than random and reverse placings. However, such improvement is not consistent. Therefore, how to automatically design a better ordering method for code intelligence tasks needs to be further investigated.</li>
<li>Different from natural language text, the length of a code snippet is often much longer. This limits the number of examples in the prompt and could bring large computation and time costs for LLMs. Therefore, incorporating program slicing and reduction techniques into ICL to reduce the costs is worth investigating.
Developers: In-context learning is a paradigm that allows for learning from a few examples in the prompt without requiring parameter updates. This new approach has also fascinated the language-model-as-a-service community. Our findings indicate that the selection, order, and number of demonstration examples have significant impacts on the performance of ICL for code intelligence tasks. Based on our findings, we conclude the following insights and takeaways for developers to use LLM in their work:</li>
<li>Including demonstration examples in the prompt, which help the model understand the task and guide the output format.</li>
<li>Using a retrieval method to select demonstration examples when a labeled training set is available. For the retrieval methods, consider using BM-25 as it is a simple yet effective method.</li>
<li>Improving the diversity of task-level demonstration examples with clustering to obtain more accurate and stable predictions.</li>
<li>When arranging the order of demonstration examples, placing similar samples at the end of the list is a good choice in most cases.</li>
<li>Using as many demonstration examples as possible, but be mindful of the maximum length limitation to avoid truncation issues. To save costs, it is also suggested that four examples are used in the demonstration.</li>
</ul>
<h2>B. Threats to Validity</h2>
<p>We identify three main threats to validity of our study:</p>
<p>1) Potential data leakage. In this paper, we conduct experiments by using the API of OpenAI Codex, GPT3.5, and ChatGPT. However, since they are closed-source models, their parameters and training sets are not publicly available, which raises concerns about potential data leakage. Specifically, there is a possibility that the model has
already been trained on the test set and merely memorizes the results instead of predicting them. However, we can observe from our experiments that the model's performance in a zero-shot setting is catastrophic, indicating a low probability of direct memorization of the dataset. Moreover, all experiments in our paper were conducted using these models and we use the relative performance improvement to measure the effectiveness of different demonstration construction strategies. Therefore, the findings of our paper remain convincing.
2) The selection of tasks. In this study, we investigate constructions of the demonstration on representative three tasks including code summarization, bug fixing, and program synthesis. These tasks cover different types such as Code $\rightarrow$ Text, Code+Text $\rightarrow$ Code, and Text $\rightarrow$ Code. Hence, we believe the finding of our paper can generalize to a wide arrange of code intelligent tasks. In the future, we plan to conduct experiments on other types of tasks such as Code $\rightarrow$ Class tasks (e.g., vulnerability detection) and Code $\rightarrow$ Code tasks (e.g., code translation).
3) The selection of models. In this paper, we select three LLMs for experiments. Nonetheless, there are other LLMs available, such as CodeGen [62] and CodeGeeX [63]. In the future, we plan to conduct experiments on a broader range of LLMs to verify the generalizability of our findings.
4) The selection of languages. For each task, we select one popular dataset for evaluation. The datasets of three tasks only contain two programming languages, i.e., Java and Python. In the future, we will validate the effectiveness of demonstration construction methods in other languages.</p>
<h2>VI. Related work</h2>
<h2>A. Pre-trained Models of Code</h2>
<p>Recently, with the development of pre-trained techniques, the pre-trained models of code have been widely used and achieved state-of-the-art performance on various software engineering tasks. One such model is CodeBERT [6], which is an encoder-only pre-trained model on six programming languages with two self-supervised tasks. Another model, CodeT5 [9] is an encoder-decoder pre-trained model following the same architecture as T5. CodeGPT [64] is a decoder-only model that pre-trains on programming languages dataset and has the same architecture as GPT-2. PLBART [8] uses denoising sequenceto-sequence pretraining for both program understanding and generation purposes. UniXCoder [48] involves multi-modal contrastive learning and cross-modal generation objective to learn the representation of code fragments.</p>
<p>Apart from these smaller pre-trained models in academic circles, many pre-trained code models with much larger sizes have been proposed in the industry in recent years. Codex [2] is a large code pre-trained model proposed by OpenAI that supports the service of Copilot. In addition to Codex, the models recently released by OpenAI, such as ChatGPT [26] and GPT-4 [23], are also pre-trained on source code data</p>
<p>and demonstrate impressive programming abilities. AlphaCode [25] is trained for generating code for programming competitions like Codeforces, using 715G data and 41B parameters. CodeGen [62] is a large pre-trained model for multi-turn program synthesis with more than 16B parameters, while CodeGeeX [63] is a recently proposed open-source multilingual code generation model with 13B parameters.</p>
<h2>B. In-context Learning</h2>
<p>Large language models have revolutionized natural language processing (NLP) in recent years. Based on large pre-training data and model sizes, LLMs show impressive emergent abilities that have not been observed in small models [10]. Brown et al. [11] first show that GPT-3 has the ability to learn from a few examples in the context without parameter update. Liu et al. [20] first explore selecting the closest neighbors as the in-context examples. Recently, Levy et al. [65] propose to improve the diversity of in-context examples and achieve better performance on NLP compositional generalization tasks. Lu et al. [21] find that the order of in-context examples has a large impact on the performance and propose two methods LocalE and GlobalE based on the entropy. Recently, a series of work [15], [66] focus on the complex reasoning tasks and propose chain-of-thought prompt by guiding the model to output its reasoning path.</p>
<p>In addition to NLP, there has been increasing interest in applying in-context learning to code intelligence tasks [17], [19], [22], [47], [67], [68]. For example, Xia et al. [17] evaluate the effectiveness of LLMs on program repair. Nashid et al. [47] propose to use the BM-25 to retrieve similar examples and construct the demonstrations for assert generation and program repair. However, these works mainly focus on the evaluation of LLMs on one or two tasks and do not discuss the construction of in-context demonstrations in-depth. In contrast, our work aims at conducting a systematic study of designing better demonstrations for ICL in code intelligence tasks.</p>
<h2>VII. CONCLUSION AND FUTURE WORK</h2>
<p>In this paper, we experimentally investigate the impact of different demonstration selection methods, different demonstration ordering methods, and the number of demonstration examples on the performance of in-context learning for code intelligence tasks. Our research demonstrates that a carefully-designed demonstration for ICL outperforms simpler demonstrations a lot. We summarize our findings and provide suggestions to help researchers and developers construct better demonstrations for code intelligence tasks. In the future, we will explore more aspects of source code on the performance of in-context learning such as the quality of the code and the naturalness of the code. Additionally, we will also further verify our findings on other large language models. Our source code and full experimental results are available at https://github.com/shuzhenggao/ICL4code.</p>
<h2>REFERENCES</h2>
<p>[1] Z. Zeng, H. Tan, H. Zhang, J. Li, Y. Zhang, and L. Zhang, "An extensive study on pre-trained models for program understanding and generation," in ISSTA '22: 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Event, South Korea, July 18 - 22, 2022. ACM, 2022, pp. 39-51.
[2] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," CoRR, vol. abs/2107.03374, 2021.
[3] W. U. Ahmad, S. Chakraborty, B. Ray, and K. Chang, "A transformerbased approach for source code summarization," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. Association for Computational Linguistics, 2020, pp. 49985007.
[4] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk, "On learning meaningful code changes via neural machine translation," in Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019. IEEE / ACM, 2019, pp. 25-36.
[5] P. Yin and G. Neubig, "A syntactic neural model for general-purpose code generation," in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers. Association for Computational Linguistics, 2017, pp. 440-450.
[6] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, "Codebert: A pre-trained model for programming and natural languages," in Findings of the Association for Computational Linguistics: EMNLP 2020, ser. Findings of ACL, vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. $1536-1547$.
[7] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, "Summarizing source code with transferred API knowledge," in Proceedings of the TwentySeventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, 2018, pp. 22692275.
[8] W. U. Ahmad, S. Chakraborty, B. Ray, and K. Chang, "Unified pretraining for program understanding and generation," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2021, Online, June 6-11, 2021. Association for Computational Linguistics, 2021, pp. 2655-2668.
[9] Y. Wang, W. Wang, S. R. Jory, and S. C. H. Hoi, "Codet5: Identifieraware unified pre-trained encoder-decoder models for code understanding and generation," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021. Association for Computational Linguistics, 2021, pp. 8696-8708.
[10] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., "Emergent abilities of large language models," Trans. Mach. Learn. Res., vol. 2022, 2022.
[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[12] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., "Palm-e: An embodied multimodal language model," CoRR, vol. abs/2303.03378, 2023.
[13] J. Devlin, M. Chang, K. Lee, and K. Toutanova, "BERT: pre-training of deep bidirectional transformers for language understanding," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 2019, pp. 4171-4186.
[14] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, "A survey for in-context learning," arXiv preprint arXiv:2301.00234, 2022.
[15] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou, "Chain-of-thought prompting elicits reasoning in large language models," in NeurIPS, 2022.
[16] Y. Hu, C. Lee, T. Xie, T. Yu, N. A. Smith, and M. Ostendorf, "Incontext learning for few-shot dialogue state tracking," in Findings of the Association for Computational Linguistics: EMNLP 2022, Abu</p>
<p>Dhabi, United Arab Emirates, December 7-11, 2022. Association for Computational Linguistics, 2022, pp. 2627-2643.
[17] C. S. Xia, Y. Wei, and L. Zhang, "Automated program repair in the era of large pre-trained language models," in 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2023, pp. 1482-1494.
[18] Y. Peng, S. Gao, C. Gao, Y. Huo, and M. R. Lyu, "Domain knowledge matters: Improving prompts with fix templates for repairing python type errors," CoRR, vol. abs/2306.01394, 2023.
[19] J. A. Penner, H. Babii, and R. Robbes, "Can openai's codex fix bugs?: An evaluation on quickbugs," in 3rd IEEE/ACM International Workshop on Automated Program Repair, APR@ICSE 2022, Pittsburgh, PA, USA, May 19, 2022. IEEE, 2022, pp. 69-75.
[20] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen, "What makes good in-context examples for gpt-3?" in Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022. Association for Computational Linguistics, 2022, pp. 100-114.
[21] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp, "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. Association for Computational Linguistics, 2022, pp. 8086-8098.
[22] J. Y. Khan and G. Uddin, "Automatic code documentation generation using GPT-3," in 37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October 10-14, 2022. ACM, 2022, pp. 174:1-174:6.
[23] OpenAI, "GPT-4 technical report," CoRR, vol. abs/2303.08774, 2023.
[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp. 5998-6008.
[25] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al., Science, vol. 378, no. 6624, pp. 1092-1097, 2022.
[26] ChatGPT, "Chatgpt," https://chat.openai.com/, 2022.
[27] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing," ACM Comput. Surv., vol. 55, no. 9, pp. 195:1-195:35, 2023.
[28] C. Wang, Y. Yang, C. Gao, Y. Peng, H. Zhang, and M. R. Lyu, "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence," in Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022. ACM, 2022, pp. 382-394.
[29] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Altartyan, and S. Gelly, "Parameter-efficient transfer learning for NLP," in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, vol. 97. PMLR, 2019, pp. 2790-2799.
[30] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
[31] X. L. Li and P. Liang, "Prefix-tuning: Optimizing continuous prompts for generation," in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021. Association for Computational Linguistics, 2021, pp. 4582-4597.
[32] O. Rubin, J. Herzig, and J. Berant, "Learning to retrieve prompts for incontext learning," in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022. Association for Computational Linguistics, 2022, pp. 2655-2671.
[33] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf, L. Zettlemoyer, N. A. Smith, and T. Yu, "Selective annotation makes language models better few-shot learners," 2023.
[34] S. Gao, H. Zhang, C. Gao, and C. Wang, "Keeping pace with everincreasing data: Towards continual learning of code intelligence models," in 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2023, pp. 30-42.
[35] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt, "Codesearchnet challenge: Evaluating the state of semantic code search," CoRR, vol. abs/1909.09436, 2019.
[36] E. Shi, Y. Wang, L. Du, J. Chen, S. Han, H. Zhang, D. Zhang, and H. Sun, "On the evaluation of neural code summarization," in 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 2022, pp. $1597-1608$.
[37] K. Papineni, S. Roukos, T. Ward, and W. Zhu, "Bleu: a method for automatic evaluation of machine translation," in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 2002, pp. 311-318.
[38] C.-Y. Lin, "ROUGE: A package for automatic evaluation of summaries," in Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74-81.
[39] S. Banerjee and A. Lavie, "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments," in Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005. Association for Computational Linguistics, 2005, pp. 65-72.
[40] S. Gao, C. Gao, Y. He, J. Zeng, L. Nie, X. Xia, and M. R. Lyu, "Code structure-guided transformer for source code summarization," ACM Trans. Softw. Eng. Methodol., vol. 32, no. 1, pp. 23:1-23:32, 2023.
[41] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and D. Poshyvanyk, "An empirical study on learning bug-fixing patches in the wild via neural machine translation," ACM Trans. Softw. Eng. Methodol., vol. 28, no. 4, pp. 19:1-19:29, 2019.
[42] S. Chakraborty and B. Ray, "On multi-modal learning of editing source code," in 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia, November 1519, 2021. IEEE, 2021, pp. 443-455.
[43] S. Chakraborty, T. Ahmed, Y. Ding, P. T. Devanbu, and B. Ray, "Natgen: generative pre-training by "naturalizing" source code," in Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022. ACM, 2022, pp. $18-30$.
[44] P. Yin, B. Deng, E. Chen, B. Vasilescu, and G. Neubig, "Learning to mine aligned code and natural language pairs from stack overflow," in Proceedings of the 15th International Conference on Mining Software Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018. ACM, 2018, pp. 476-486.
[45] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan, M. Zhou, A. Blanco, and S. Ma, "Codebleu: a method for automatic evaluation of code synthesis," CoRR, vol. abs/2009.10297, 2020.
[46] Z. Cheng, T. Xie, P. Shi, C. Li, R. Nadkarni, Y. Hu, C. Xiong, D. Radev, M. Ostendorf, L. Zettlemoyer, N. A. Smith, and T. Yu, "Binding language models in symbolic languages," 2023.
[47] N. Nashid, M. Sintaha, and A. Mesbah, "Retrieval-based prompt selection for code-related few-shot learning," in 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), 2023, pp. $2450-2462$.
[48] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, and J. Yin, "Unixcoder: Unified cross-modal pre-training for code representation," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. Association for Computational Linguistics, 2022, pp. $7212-7225$.
[49] D. Arthur and S. Vassilvitskii, "k-means++: the advantages of careful seeding," in Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, New Orleans, Louisiana, USA, January 7-9, 2007. SIAM, 2007, pp. 1027-1035.
[50] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, "Retrieval-based neural source code summarization," in ICSE '20: 42nd International Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020. ACM, 2020, pp. 1385-1397.
[51] B. Wei, Y. Li, G. Li, X. Xia, and Z. Jia, "Retrieve and refine: Exemplarbased neural comment generation," in 35th IEEE/ACM International</p>
<p>Conference on Automated Software Engineering, ASE 2020. IEEE, 2020, pp. 349-360.
[52] N. Reimers and I. Gurevych, "Sentence-bert: Sentence embeddings using siamese bert-networks," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-UCNLP 2019, Hong Kong, China, November 3-7, 2019. Association for Computational Linguistics, 2019, pp. 3980-3990.
[53] Y. Luan, J. Eisenstein, K. Toutanova, and M. Collins, "Sparse, dense, and attentional representations for text retrieval," Trans. Assoc. Comput. Linguistics, vol. 9, pp. 329-345, 2021.
[54] Sentence-transformers, "st-codesearch-distilroberta," https://huggingface.co/flax-sentence-embeddings/ st-codesearch-distilroberta-base, 2021.
[55] E. Shi, Y. Wang, W. Gu, L. Du, H. Zhang, S. Han, D. Zhang, and H. Sun, "Cocosoda: Effective contrastive learning for code search," pp. 2198-2210, 2023.
[56] Gensim, "Gensim package," https://github.com/RaRe-Technologies/ gensim, 2010.
[57] C. E. Brown, Coefficient of Variation. Springer Berlin Heidelberg, 1998, pp. 155-157.
[58] M. Wei, N. S. Harzevili, Y. Huang, J. Wang, and S. Wang, "CLEAR: contrastive learning for API recommendation," in 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 2022, pp. 376-387.
[59] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov, "Transformer-xl: Attentive language models beyond a fixed-length context," in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers. Association for Computational Linguistics, 2019, pp. 2978-2988.
[60] A. Bulatov, Y. Kuratov, and M. Burtsev, "Recurrent memory transformer," in Advances in Neural Information Processing Systems, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.
[61] OpenAI-pricing, "Openai-pricing," https://openai.com/pricing, 2022.
[62] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, "Codegen: An open large language model for code with multi-turn program synthesis," arXiv preprint arXiv:2203.13474, 2022.
[63] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li et al., "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x," CoRR, vol. abs/2303.17568, 2023.
[64] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., "Codexglue: A machine learning benchmark dataset for code understanding and generation," in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, J. Vanschoren and S. Yeung, Eds., 2021.
[65] I. Levy, B. Bogin, and J. Berant, "Diverse demonstrations improve incontext compositional generalization," in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 2023, pp. 1401-1422.
[66] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, "Large language models are zero-shot reasoners," in NeurIPS, 2022.
[67] T. Ahmed and P. T. Devanbu, "Few-shot training llms for project-specific code-summarization," in 37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October 10-14, 2022. ACM, 2022, pp. 177:1-177:5.
[68] G. Poesia, A. Polozov, V. Le, A. Tiwari, G. Soares, C. Meek, and S. Gulwani, "Synchromesh: Reliable code generation from pre-trained language models," in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/gszsectan/ICL/tree/master/prompts&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
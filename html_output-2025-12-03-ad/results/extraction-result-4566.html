<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4566 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4566</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4566</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-c4692e5d11cde0f10cbd5a534a5870eb299e8156</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c4692e5d11cde0f10cbd5a534a5870eb299e8156" target="_blank">Jointly Measuring Diversity and Quality in Text Generation Models</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution by introducing a metric that approximates this distance using n-gram based measures.</p>
                <p><strong>Paper Abstract:</strong> Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglecting their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generatorʼs density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4566.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4566.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU (Bilingual Evaluation Understudy) score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram overlap quality metric originally developed for machine translation that measures similarity between generated sentences and a reference set by precision of n-grams, often combined across n via geometric mean and with a brevity penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bleu: a method for automatic evaluation of machine translation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes n-gram precision of generated sentences against a reference corpus (here: test set) averaged across generated sentences; higher BLEU indicates greater n-gram overlap with reference data (interpreted as quality). Typically reported BLEU2..BLEU5 for different n values and averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality (n-gram similarity to reference); does not measure diversity or coverage explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / text generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluative metric for generated textual outputs (not specific to scientific theories)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used across datasets (COCO, EMNLP2017, IMDB). Tables report BLEU2..BLEU5 per model; e.g., on COCO BLEU4: Real 0.622, MLE 0.507, SeqGAN 0.578, MaliGAN 0.536, RankGAN 0.569; used as a quality baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric (no human raters).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared against other metrics and used as a baseline in experimental comparisons; authors note BLEU's limitations (ignores diversity and coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Measures only n-gram overlap (quality) and can be gamed by producing a small set of high-overlap sentences; ignores diversity/coverage and mode collapse; sensitive to reference set choice.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied on COCO Captions, EMNLP2017 WMT News, IMDB Movie Reviews, and synthetic Oracle dataset (as baseline quality metric).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4566.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric that applies BLEU where each generated sentence is scored against the other generated sentences as references to measure diversity/variety in the generated set; lower Self-BLEU indicates higher diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Texygen: A benchmarking platform for text generation models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Self-BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each generated sentence, compute BLEU with the remainder of the generated set as references; average these scores. High Self-BLEU means generated sentences are similar to each other (low diversity); low Self-BLEU implies high diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Diversity/variety of generated outputs (intra-set similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / text generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>diversity measurement for generated text outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used for diversity evaluation across datasets. Example (COCO Self-BLEU4): Real 0.489, MLE 0.425, SeqGAN 0.700, MaliGAN 0.451, RankGAN 0.583 — demonstrating variation across models.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared with other diversity and joint metrics; used in quality-diversity plots (temperature sweep) to show trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not measure quality — a model could have low Self-BLEU (diverse) but low-quality outputs; influenced by sample size and tokenization; only intra-set comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO, EMNLP2017, IMDB, Oracle (as diversity metric).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4566.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle-NLL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle Negative Log-Likelihood (Oracle-NLL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric introduced in SeqGAN evaluation that measures the negative log-likelihood of samples produced by the trained generative model under a known synthetic oracle distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Seqgan: Sequence generative adversarial nets with policy gradient.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Oracle-NLL</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Assume a synthetic oracle probabilistic generator P (e.g., randomly initialized LSTM) that produced a dataset; compute the negative log-likelihood of generated samples under P. Lower Oracle-NLL means generated samples are more probable under the oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality measured as likelihood under a known synthetic oracle distribution; does not directly measure diversity/coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / synthetic-data evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>likelihood-based quality metric for generated sequences</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used on the Oracle synthetic dataset; Table 4: Oracle-NLL values - MLE 167.014, SeqGAN 163.179 (best), MaliGAN 168.054, RankGAN 166.774. Authors note Oracle-NLL ignores variety.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (requires a known oracle distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as part of oracle-mode evaluations and compared to symmetric distribution distances (Bhattacharyya); compared ordering with other measures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires access to a true oracle distribution (synthetic setting); evaluates only one-sided KL (likelihood) and can be insensitive to mode coverage; not applicable with real-world unknown data-generating distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Synthetic Oracle dataset (randomly-initialized LSTM generator, described in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4566.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative Log-Likelihood (test-data likelihood)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The negative log-likelihood of test data under the model; commonly used to assess how well a probabilistic model assigns high probability to real samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>NLL (test negative log-likelihood)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute average negative log probability assigned by the generative model to held-out test (real) samples; lower NLL indicates higher model likelihood on real data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>How well the model assigns probability mass to held-out real-data samples (empirical adequacy / likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / probabilistic sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>likelihood-based evaluation for generative models</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported across datasets; authors use NLL as a baseline. Paper notes that low NLL does not guarantee quality of free-running generated samples and can be unfair when comparing methods optimized on likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (likelihood computation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared NLL to other metrics (MS-Jaccard, FBD) and reported Pearson correlations; used as validation signal in training termination for MLE baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Doesn't evaluate free-running generation (exposure bias); conditions on true prefixes (train-test discrepancy); may severely penalize mode-missing models and favors mean-seeking MLE models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO, EMNLP2017, IMDB, Oracle (where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4566.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Entropy (estimated via Monte Carlo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entropy of the generative model's distribution used as a proxy for diversity: higher entropy corresponds to higher diversity of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Entropy (Monte Carlo estimated)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Estimate the Shannon entropy of the generative model (e.g., by sampling) to quantify the overall uncertainty/diversity of the model's conditional distributions; lower entropy indicates less diverse generations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Diversity (distributional entropy); measures spread of model probability mass.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>distributional diversity metric</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used for Oracle dataset as diversity measure in temperature-sweep plots. Authors note entropy increases with temperature and reflects diversity changes.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (sampling-based estimation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared trends with other diversity metrics (Self-BLEU) and quality-diversity sweeps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Direct computation infeasible for complex models; Monte Carlo estimation noisy and dependent on sample size; entropy alone says nothing about quality of high-probability samples.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used on Oracle (synthetic) dataset in paper's analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4566.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MS-Jaccard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MS-Jaccard (Multi-Set Jaccard)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram frequency-based similarity metric introduced in this paper that compares normalized n-gram multisets of generated and real samples using a Jaccard-like intersection-over-union formula and aggregates across n via geometric mean to jointly assess quality and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MS-Jaccard (MS-Jaccard-N)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Treat n-grams from generated and real sentence sets as multisets with normalized counts C_n(g,S). For each n, compute score_n = sum_g min(C_n(g,S1),C_n(g,S2)) / sum_g max(...). Then aggregate scores across n=1..N via geometric mean to produce MS-Jaccard-N. Higher values indicate greater similarity between generated and real n-gram distributions; designed to penalize both low quality and low diversity/mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Joint quality and diversity: measures how well generated n-gram frequency distribution matches real data distribution (coverage + fidelity).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / text generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>distributional similarity metric for generated text</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Introduced and applied across datasets. Example (COCO, MS-Jaccard4): Real 0.430, MLE 0.322, SeqGAN 0.164, MaliGAN 0.345, RankGAN 0.224. Authors report MS-Jaccard ordering is broadly consistent with full quality-diversity temperature-sweep dominance and correlates with NLL.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric (n-gram frequency comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by (1) comparing its model ordering to the dominance observed in full temperature-sweep quality-diversity plots (Caccia et al. style) and (2) computing Pearson correlations with other metrics (MS-Jaccard and FBD found highly correlated and correlated with NLL).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>n-gram-based — may miss semantic equivalence that isn't captured by n-grams; sensitive to tokenization and n choice; normalization choices affect sensitivity to sample sizes; may be gamed by matching n-gram frequencies without producing coherent global structure.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO Captions, EMNLP2017 WMT News, IMDB Movie Reviews (MS-Jaccard2..5 applied).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4566.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FBD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fréchet BERT Distance (FBD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feature-based distance metric introduced in this paper that adapts Fréchet Inception Distance (FID) idea to text by using pooled BERT features for sentences and computing Fréchet (Wasserstein-2) distance between Gaussians fitted to real and generated feature vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Fréchet BERT Distance (FBD)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Extract fixed-size pooled features from BERT for each sentence; model real and generated feature distributions as multivariate Gaussians (mean m and covariance C each); compute Fréchet distance: sqrt(||m1 - m2||^2 + Tr(C1 + C2 - 2 (C1 C2)^{1/2})). Lower FBD indicates generated features closely match real features (joint measure of quality and diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Joint quality and diversity in feature (semantic) space: feature means (fidelity) and covariances (diversity/coverage) are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / feature-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-space distributional distance for generated text</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Introduced and applied across datasets. Reported FBD values (lower is better); e.g., COCO FBD: Real 0.460, MLE 1.971, SeqGAN 4.590, MaliGAN 1.474 (best), RankGAN 3.574. Authors report FBD ordering is largely consistent with quality-diversity sweep and correlates with NLL and MS-Jaccard.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric using pretrained BERT features and Gaussian approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by (1) comparing model orderings to temperature-sweep dominance and (2) Pearson correlations with other metrics (FBD highly correlated with 1-MS-Jaccard and NLL).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Assumes a Gaussian model in BERT feature space (may not hold); depends on choice of BERT pooled features and their representational biases; covariance square-root numerical issues; sensitive to number of samples; may reflect BERT's inductive biases rather than human judgments directly.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO, EMNLP2017, IMDB (FBD computed on pooled BERT features for these corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4566.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bhattacharyya</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bhattacharyya symmetric distance (Monte Carlo estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symmetric distance between two probability distributions proposed here for oracle-mode evaluation, estimated via Monte Carlo using samples from both distributions and involving square-root ratios of densities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bhattacharyya (Monte Carlo estimated symmetric distance)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For distributions P (oracle) and Q (model), estimate B(P,Q) = -1/2( ln(1/N sum_i sqrt[q(x_i)/p(x_i)]) + ln(1/M sum_j sqrt[p(x_j)/q(x_j)]) ) where {x_i} ~ P and {x_j} ~ Q. Lower values mean closer distributions. Requires ability to compute densities q(x)/p(x) for sampled x (oracle-mode).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Symmetric distributional closeness between model and oracle — intended to capture both coverage and fidelity in probabilistic (oracle) setting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>text generation evaluation (synthetic/oracle setting)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>distributional divergence/distance for probabilistic models</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied on Oracle synthetic dataset (where oracle density available). Table 4 Bhattacharyya: MLE 7.105 (best), SeqGAN 10.076, MaliGAN 8.503, RankGAN 12.127 — consistent with temperature-sweep dominance ordering for oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (Monte Carlo estimation using known densities in oracle setting).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared ordering to temperature-sweep analysis and Oracle-NLL; shown consistent with dominance ordering in synthetic experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires computable densities for both P and Q (oracle-mode only); Monte Carlo estimation variance; sensitivity to support mismatch where densities near zero cause instability; computationally expensive for high-dimensional discrete sequence spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Oracle synthetic dataset (random LSTM generator with known density).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4566.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FID / Inception Score (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fréchet Inception Distance (FID) and Inception Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Image-generation evaluation metrics (Inception Score and FID) that inspired analogous feature-based metrics for text (e.g., FBD); FID measures Fréchet distance between image features from Inception network; Inception Score measures class-distribution sharpness and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gans trained by a two time-scale update rule converge to a local nash equilibrium.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Fréchet Inception Distance (FID) / Inception Score (analogy)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>FID: fit Gaussians to Inception features of real and generated images and compute Fréchet distance between them; Inception Score: evaluate classifier confidence on generated images and entropy across classes to reward both confidence and diversity. Paper references these as inspirations for feature-based text metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Joint quality and diversity in feature space (for images); used analogously to motivate text feature metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer vision (original); referenced for NLP metric design</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-space distributional distance and classifier-based score (image generation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not applied to text in this paper directly; cited as inspiration for FBD.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (feature-based).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>N/A in this paper (cited prior work where FID/Inception Score introduced and validated for images).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Original FID assumptions (Gaussian features) and classifier bias; motivates caution when applying same ideas to text using BERT features.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4566.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quality-Diversity Spectrum (Temperature sweep)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quality-diversity spectrum via temperature sweep (Softmax temperature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework (from Caccia et al.) to evaluate models across the trade-off between quality and diversity by sweeping the sampling temperature T of model conditional distributions, producing a curve of quality vs diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language gans falling short.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Quality-Diversity Spectrum (temperature sweep)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Modify generator conditional distributions G(x_t|x_{1:t-1}) by applying a softmax temperature: Softmax(o_t / T). Vary T across a wide range to obtain different operating points balancing entropy (diversity) and sample quality; plot a quality metric versus a diversity metric to obtain a spectrum and compare model dominance across the full trade-off rather than a single operating point.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality (e.g., BLEU or Oracle-NLL) vs Diversity (e.g., Self-BLEU or Entropy); evaluates dominance across entire trade-off curve rather than single metric.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework for trade-off analysis of generative models</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper uses temperature sweep (temperatures from 1.5^{-3} to 1.5^{4}) as ground-truth analysis; shows MS-Jaccard and FBD are predictive of behavior across temperature spectrum and that MLE dominates GANs in the quality-diversity space in many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated experimental framework (computes automated quality and diversity metrics across temperatures).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as a robust baseline framework to validate single-point metrics: authors check whether MS-Jaccard and FBD ordering matches dominance observed in temperature-sweep plots and compute Pearson correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires sweeping an extra hyperparameter (computational cost); results depend on choice of quality/diversity metrics used to plot the spectrum; may not capture human judgments directly.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied across COCO, EMNLP2017, IMDB and Oracle in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4566.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4566.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Likelihood-based evaluation (MLE / Likelihood on test data)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Likelihood / Negative Log-Likelihood based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using model likelihood (or negative log-likelihood on test data) as an evaluation criterion for generative models; often associated with Maximum Likelihood Estimation (MLE) baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Likelihood / NLL on test data</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate the trained probabilistic model by computing the average negative log-likelihood assigned to held-out real test sequences (decomposed via chain rule). Lower NLL implies model assigns more probability mass to the held-out data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Empirical adequacy (how well the model explains held-out data in likelihood terms); used as both training objective (MLE) and evaluation signal.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language generation / probabilistic modeling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>likelihood-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as both training termination signal and evaluation; authors note MLE often dominates GAN variants when considering combined quality-diversity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (likelihood computation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared with MS-Jaccard and FBD and temperature-sweep analyses; Pearson correlation measured.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Doesn't measure free-running generation (exposure bias), can be unfair when comparing models not optimized for likelihood, and can severely penalize mode-missing models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COCO, EMNLP2017, IMDB, Oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bleu: a method for automatic evaluation of machine translation. <em>(Rating: 2)</em></li>
                <li>Seqgan: Sequence generative adversarial nets with policy gradient. <em>(Rating: 2)</em></li>
                <li>Texygen: A benchmarking platform for text generation models. <em>(Rating: 2)</em></li>
                <li>Gans trained by a two time-scale update rule converge to a local nash equilibrium. <em>(Rating: 2)</em></li>
                <li>Language gans falling short. <em>(Rating: 2)</em></li>
                <li>Fréchet Inception Distance (FID) (original reference) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4566",
    "paper_id": "paper-c4692e5d11cde0f10cbd5a534a5870eb299e8156",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "BLEU",
            "name_full": "BLEU (Bilingual Evaluation Understudy) score",
            "brief_description": "An n-gram overlap quality metric originally developed for machine translation that measures similarity between generated sentences and a reference set by precision of n-grams, often combined across n via geometric mean and with a brevity penalty.",
            "citation_title": "Bleu: a method for automatic evaluation of machine translation.",
            "mention_or_use": "use",
            "evaluation_method_name": "BLEU",
            "evaluation_method_description": "Computes n-gram precision of generated sentences against a reference corpus (here: test set) averaged across generated sentences; higher BLEU indicates greater n-gram overlap with reference data (interpreted as quality). Typically reported BLEU2..BLEU5 for different n values and averaged.",
            "evaluation_criteria": "Quality (n-gram similarity to reference); does not measure diversity or coverage explicitly.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation / text generation",
            "theory_type": "evaluative metric for generated textual outputs (not specific to scientific theories)",
            "human_comparison": false,
            "evaluation_results": "Used across datasets (COCO, EMNLP2017, IMDB). Tables report BLEU2..BLEU5 per model; e.g., on COCO BLEU4: Real 0.622, MLE 0.507, SeqGAN 0.578, MaliGAN 0.536, RankGAN 0.569; used as a quality baseline.",
            "automated_vs_human_evaluation": "Automated metric (no human raters).",
            "validation_method": "Compared against other metrics and used as a baseline in experimental comparisons; authors note BLEU's limitations (ignores diversity and coverage).",
            "limitations_challenges": "Measures only n-gram overlap (quality) and can be gamed by producing a small set of high-overlap sentences; ignores diversity/coverage and mode collapse; sensitive to reference set choice.",
            "benchmark_dataset": "Applied on COCO Captions, EMNLP2017 WMT News, IMDB Movie Reviews, and synthetic Oracle dataset (as baseline quality metric).",
            "uuid": "e4566.0",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Self-BLEU",
            "name_full": "Self-BLEU",
            "brief_description": "A metric that applies BLEU where each generated sentence is scored against the other generated sentences as references to measure diversity/variety in the generated set; lower Self-BLEU indicates higher diversity.",
            "citation_title": "Texygen: A benchmarking platform for text generation models.",
            "mention_or_use": "use",
            "evaluation_method_name": "Self-BLEU",
            "evaluation_method_description": "For each generated sentence, compute BLEU with the remainder of the generated set as references; average these scores. High Self-BLEU means generated sentences are similar to each other (low diversity); low Self-BLEU implies high diversity.",
            "evaluation_criteria": "Diversity/variety of generated outputs (intra-set similarity).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation / text generation",
            "theory_type": "diversity measurement for generated text outputs",
            "human_comparison": false,
            "evaluation_results": "Used for diversity evaluation across datasets. Example (COCO Self-BLEU4): Real 0.489, MLE 0.425, SeqGAN 0.700, MaliGAN 0.451, RankGAN 0.583 — demonstrating variation across models.",
            "automated_vs_human_evaluation": "Automated metric.",
            "validation_method": "Compared with other diversity and joint metrics; used in quality-diversity plots (temperature sweep) to show trade-offs.",
            "limitations_challenges": "Does not measure quality — a model could have low Self-BLEU (diverse) but low-quality outputs; influenced by sample size and tokenization; only intra-set comparison.",
            "benchmark_dataset": "COCO, EMNLP2017, IMDB, Oracle (as diversity metric).",
            "uuid": "e4566.1",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Oracle-NLL",
            "name_full": "Oracle Negative Log-Likelihood (Oracle-NLL)",
            "brief_description": "A metric introduced in SeqGAN evaluation that measures the negative log-likelihood of samples produced by the trained generative model under a known synthetic oracle distribution.",
            "citation_title": "Seqgan: Sequence generative adversarial nets with policy gradient.",
            "mention_or_use": "use",
            "evaluation_method_name": "Oracle-NLL",
            "evaluation_method_description": "Assume a synthetic oracle probabilistic generator P (e.g., randomly initialized LSTM) that produced a dataset; compute the negative log-likelihood of generated samples under P. Lower Oracle-NLL means generated samples are more probable under the oracle.",
            "evaluation_criteria": "Quality measured as likelihood under a known synthetic oracle distribution; does not directly measure diversity/coverage.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation / synthetic-data evaluation",
            "theory_type": "likelihood-based quality metric for generated sequences",
            "human_comparison": false,
            "evaluation_results": "Used on the Oracle synthetic dataset; Table 4: Oracle-NLL values - MLE 167.014, SeqGAN 163.179 (best), MaliGAN 168.054, RankGAN 166.774. Authors note Oracle-NLL ignores variety.",
            "automated_vs_human_evaluation": "Automated (requires a known oracle distribution).",
            "validation_method": "Used as part of oracle-mode evaluations and compared to symmetric distribution distances (Bhattacharyya); compared ordering with other measures.",
            "limitations_challenges": "Requires access to a true oracle distribution (synthetic setting); evaluates only one-sided KL (likelihood) and can be insensitive to mode coverage; not applicable with real-world unknown data-generating distributions.",
            "benchmark_dataset": "Synthetic Oracle dataset (randomly-initialized LSTM generator, described in paper).",
            "uuid": "e4566.2",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "NLL",
            "name_full": "Negative Log-Likelihood (test-data likelihood)",
            "brief_description": "The negative log-likelihood of test data under the model; commonly used to assess how well a probabilistic model assigns high probability to real samples.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "NLL (test negative log-likelihood)",
            "evaluation_method_description": "Compute average negative log probability assigned by the generative model to held-out test (real) samples; lower NLL indicates higher model likelihood on real data.",
            "evaluation_criteria": "How well the model assigns probability mass to held-out real-data samples (empirical adequacy / likelihood).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation / probabilistic sequence models",
            "theory_type": "likelihood-based evaluation for generative models",
            "human_comparison": false,
            "evaluation_results": "Reported across datasets; authors use NLL as a baseline. Paper notes that low NLL does not guarantee quality of free-running generated samples and can be unfair when comparing methods optimized on likelihood.",
            "automated_vs_human_evaluation": "Automated (likelihood computation).",
            "validation_method": "Compared NLL to other metrics (MS-Jaccard, FBD) and reported Pearson correlations; used as validation signal in training termination for MLE baseline.",
            "limitations_challenges": "Doesn't evaluate free-running generation (exposure bias); conditions on true prefixes (train-test discrepancy); may severely penalize mode-missing models and favors mean-seeking MLE models.",
            "benchmark_dataset": "COCO, EMNLP2017, IMDB, Oracle (where applicable).",
            "uuid": "e4566.3",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Entropy",
            "name_full": "Model Entropy (estimated via Monte Carlo)",
            "brief_description": "Entropy of the generative model's distribution used as a proxy for diversity: higher entropy corresponds to higher diversity of outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Entropy (Monte Carlo estimated)",
            "evaluation_method_description": "Estimate the Shannon entropy of the generative model (e.g., by sampling) to quantify the overall uncertainty/diversity of the model's conditional distributions; lower entropy indicates less diverse generations.",
            "evaluation_criteria": "Diversity (distributional entropy); measures spread of model probability mass.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation",
            "theory_type": "distributional diversity metric",
            "human_comparison": false,
            "evaluation_results": "Used for Oracle dataset as diversity measure in temperature-sweep plots. Authors note entropy increases with temperature and reflects diversity changes.",
            "automated_vs_human_evaluation": "Automated (sampling-based estimation).",
            "validation_method": "Compared trends with other diversity metrics (Self-BLEU) and quality-diversity sweeps.",
            "limitations_challenges": "Direct computation infeasible for complex models; Monte Carlo estimation noisy and dependent on sample size; entropy alone says nothing about quality of high-probability samples.",
            "benchmark_dataset": "Used on Oracle (synthetic) dataset in paper's analyses.",
            "uuid": "e4566.4",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "MS-Jaccard",
            "name_full": "MS-Jaccard (Multi-Set Jaccard)",
            "brief_description": "An n-gram frequency-based similarity metric introduced in this paper that compares normalized n-gram multisets of generated and real samples using a Jaccard-like intersection-over-union formula and aggregates across n via geometric mean to jointly assess quality and diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "MS-Jaccard (MS-Jaccard-N)",
            "evaluation_method_description": "Treat n-grams from generated and real sentence sets as multisets with normalized counts C_n(g,S). For each n, compute score_n = sum_g min(C_n(g,S1),C_n(g,S2)) / sum_g max(...). Then aggregate scores across n=1..N via geometric mean to produce MS-Jaccard-N. Higher values indicate greater similarity between generated and real n-gram distributions; designed to penalize both low quality and low diversity/mode collapse.",
            "evaluation_criteria": "Joint quality and diversity: measures how well generated n-gram frequency distribution matches real data distribution (coverage + fidelity).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation / text generation evaluation",
            "theory_type": "distributional similarity metric for generated text",
            "human_comparison": false,
            "evaluation_results": "Introduced and applied across datasets. Example (COCO, MS-Jaccard4): Real 0.430, MLE 0.322, SeqGAN 0.164, MaliGAN 0.345, RankGAN 0.224. Authors report MS-Jaccard ordering is broadly consistent with full quality-diversity temperature-sweep dominance and correlates with NLL.",
            "automated_vs_human_evaluation": "Automated metric (n-gram frequency comparison).",
            "validation_method": "Validated by (1) comparing its model ordering to the dominance observed in full temperature-sweep quality-diversity plots (Caccia et al. style) and (2) computing Pearson correlations with other metrics (MS-Jaccard and FBD found highly correlated and correlated with NLL).",
            "limitations_challenges": "n-gram-based — may miss semantic equivalence that isn't captured by n-grams; sensitive to tokenization and n choice; normalization choices affect sensitivity to sample sizes; may be gamed by matching n-gram frequencies without producing coherent global structure.",
            "benchmark_dataset": "COCO Captions, EMNLP2017 WMT News, IMDB Movie Reviews (MS-Jaccard2..5 applied).",
            "uuid": "e4566.5",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "FBD",
            "name_full": "Fréchet BERT Distance (FBD)",
            "brief_description": "A feature-based distance metric introduced in this paper that adapts Fréchet Inception Distance (FID) idea to text by using pooled BERT features for sentences and computing Fréchet (Wasserstein-2) distance between Gaussians fitted to real and generated feature vectors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Fréchet BERT Distance (FBD)",
            "evaluation_method_description": "Extract fixed-size pooled features from BERT for each sentence; model real and generated feature distributions as multivariate Gaussians (mean m and covariance C each); compute Fréchet distance: sqrt(||m1 - m2||^2 + Tr(C1 + C2 - 2 (C1 C2)^{1/2})). Lower FBD indicates generated features closely match real features (joint measure of quality and diversity).",
            "evaluation_criteria": "Joint quality and diversity in feature (semantic) space: feature means (fidelity) and covariances (diversity/coverage) are compared.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation / feature-based evaluation",
            "theory_type": "feature-space distributional distance for generated text",
            "human_comparison": false,
            "evaluation_results": "Introduced and applied across datasets. Reported FBD values (lower is better); e.g., COCO FBD: Real 0.460, MLE 1.971, SeqGAN 4.590, MaliGAN 1.474 (best), RankGAN 3.574. Authors report FBD ordering is largely consistent with quality-diversity sweep and correlates with NLL and MS-Jaccard.",
            "automated_vs_human_evaluation": "Automated metric using pretrained BERT features and Gaussian approximation.",
            "validation_method": "Validated by (1) comparing model orderings to temperature-sweep dominance and (2) Pearson correlations with other metrics (FBD highly correlated with 1-MS-Jaccard and NLL).",
            "limitations_challenges": "Assumes a Gaussian model in BERT feature space (may not hold); depends on choice of BERT pooled features and their representational biases; covariance square-root numerical issues; sensitive to number of samples; may reflect BERT's inductive biases rather than human judgments directly.",
            "benchmark_dataset": "COCO, EMNLP2017, IMDB (FBD computed on pooled BERT features for these corpora).",
            "uuid": "e4566.6",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Bhattacharyya",
            "name_full": "Bhattacharyya symmetric distance (Monte Carlo estimation)",
            "brief_description": "A symmetric distance between two probability distributions proposed here for oracle-mode evaluation, estimated via Monte Carlo using samples from both distributions and involving square-root ratios of densities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Bhattacharyya (Monte Carlo estimated symmetric distance)",
            "evaluation_method_description": "For distributions P (oracle) and Q (model), estimate B(P,Q) = -1/2( ln(1/N sum_i sqrt[q(x_i)/p(x_i)]) + ln(1/M sum_j sqrt[p(x_j)/q(x_j)]) ) where {x_i} ~ P and {x_j} ~ Q. Lower values mean closer distributions. Requires ability to compute densities q(x)/p(x) for sampled x (oracle-mode).",
            "evaluation_criteria": "Symmetric distributional closeness between model and oracle — intended to capture both coverage and fidelity in probabilistic (oracle) setting.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "text generation evaluation (synthetic/oracle setting)",
            "theory_type": "distributional divergence/distance for probabilistic models",
            "human_comparison": false,
            "evaluation_results": "Applied on Oracle synthetic dataset (where oracle density available). Table 4 Bhattacharyya: MLE 7.105 (best), SeqGAN 10.076, MaliGAN 8.503, RankGAN 12.127 — consistent with temperature-sweep dominance ordering for oracle.",
            "automated_vs_human_evaluation": "Automated (Monte Carlo estimation using known densities in oracle setting).",
            "validation_method": "Compared ordering to temperature-sweep analysis and Oracle-NLL; shown consistent with dominance ordering in synthetic experiments.",
            "limitations_challenges": "Requires computable densities for both P and Q (oracle-mode only); Monte Carlo estimation variance; sensitivity to support mismatch where densities near zero cause instability; computationally expensive for high-dimensional discrete sequence spaces.",
            "benchmark_dataset": "Oracle synthetic dataset (random LSTM generator with known density).",
            "uuid": "e4566.7",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "FID / Inception Score (mention)",
            "name_full": "Fréchet Inception Distance (FID) and Inception Score",
            "brief_description": "Image-generation evaluation metrics (Inception Score and FID) that inspired analogous feature-based metrics for text (e.g., FBD); FID measures Fréchet distance between image features from Inception network; Inception Score measures class-distribution sharpness and diversity.",
            "citation_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
            "mention_or_use": "mention",
            "evaluation_method_name": "Fréchet Inception Distance (FID) / Inception Score (analogy)",
            "evaluation_method_description": "FID: fit Gaussians to Inception features of real and generated images and compute Fréchet distance between them; Inception Score: evaluate classifier confidence on generated images and entropy across classes to reward both confidence and diversity. Paper references these as inspirations for feature-based text metrics.",
            "evaluation_criteria": "Joint quality and diversity in feature space (for images); used analogously to motivate text feature metrics.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer vision (original); referenced for NLP metric design",
            "theory_type": "feature-space distributional distance and classifier-based score (image generation)",
            "human_comparison": false,
            "evaluation_results": "Not applied to text in this paper directly; cited as inspiration for FBD.",
            "automated_vs_human_evaluation": "Automated (feature-based).",
            "validation_method": "N/A in this paper (cited prior work where FID/Inception Score introduced and validated for images).",
            "limitations_challenges": "Original FID assumptions (Gaussian features) and classifier bias; motivates caution when applying same ideas to text using BERT features.",
            "benchmark_dataset": "",
            "uuid": "e4566.8",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Quality-Diversity Spectrum (Temperature sweep)",
            "name_full": "Quality-diversity spectrum via temperature sweep (Softmax temperature)",
            "brief_description": "A framework (from Caccia et al.) to evaluate models across the trade-off between quality and diversity by sweeping the sampling temperature T of model conditional distributions, producing a curve of quality vs diversity.",
            "citation_title": "Language gans falling short.",
            "mention_or_use": "use",
            "evaluation_method_name": "Quality-Diversity Spectrum (temperature sweep)",
            "evaluation_method_description": "Modify generator conditional distributions G(x_t|x_{1:t-1}) by applying a softmax temperature: Softmax(o_t / T). Vary T across a wide range to obtain different operating points balancing entropy (diversity) and sample quality; plot a quality metric versus a diversity metric to obtain a spectrum and compare model dominance across the full trade-off rather than a single operating point.",
            "evaluation_criteria": "Quality (e.g., BLEU or Oracle-NLL) vs Diversity (e.g., Self-BLEU or Entropy); evaluates dominance across entire trade-off curve rather than single metric.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation / evaluation methodology",
            "theory_type": "evaluation framework for trade-off analysis of generative models",
            "human_comparison": false,
            "evaluation_results": "Paper uses temperature sweep (temperatures from 1.5^{-3} to 1.5^{4}) as ground-truth analysis; shows MS-Jaccard and FBD are predictive of behavior across temperature spectrum and that MLE dominates GANs in the quality-diversity space in many experiments.",
            "automated_vs_human_evaluation": "Automated experimental framework (computes automated quality and diversity metrics across temperatures).",
            "validation_method": "Used as a robust baseline framework to validate single-point metrics: authors check whether MS-Jaccard and FBD ordering matches dominance observed in temperature-sweep plots and compute Pearson correlations.",
            "limitations_challenges": "Requires sweeping an extra hyperparameter (computational cost); results depend on choice of quality/diversity metrics used to plot the spectrum; may not capture human judgments directly.",
            "benchmark_dataset": "Applied across COCO, EMNLP2017, IMDB and Oracle in the paper's experiments.",
            "uuid": "e4566.9",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Likelihood-based evaluation (MLE / Likelihood on test data)",
            "name_full": "Likelihood / Negative Log-Likelihood based evaluation",
            "brief_description": "Using model likelihood (or negative log-likelihood on test data) as an evaluation criterion for generative models; often associated with Maximum Likelihood Estimation (MLE) baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Likelihood / NLL on test data",
            "evaluation_method_description": "Evaluate the trained probabilistic model by computing the average negative log-likelihood assigned to held-out real test sequences (decomposed via chain rule). Lower NLL implies model assigns more probability mass to the held-out data.",
            "evaluation_criteria": "Empirical adequacy (how well the model explains held-out data in likelihood terms); used as both training objective (MLE) and evaluation signal.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "natural language generation / probabilistic modeling",
            "theory_type": "likelihood-based evaluation",
            "human_comparison": false,
            "evaluation_results": "Used as both training termination signal and evaluation; authors note MLE often dominates GAN variants when considering combined quality-diversity metrics.",
            "automated_vs_human_evaluation": "Automated (likelihood computation).",
            "validation_method": "Compared with MS-Jaccard and FBD and temperature-sweep analyses; Pearson correlation measured.",
            "limitations_challenges": "Doesn't measure free-running generation (exposure bias), can be unfair when comparing models not optimized for likelihood, and can severely penalize mode-missing models.",
            "benchmark_dataset": "COCO, EMNLP2017, IMDB, Oracle.",
            "uuid": "e4566.10",
            "source_info": {
                "paper_title": "Jointly Measuring Diversity and Quality in Text Generation Models",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bleu: a method for automatic evaluation of machine translation.",
            "rating": 2
        },
        {
            "paper_title": "Seqgan: Sequence generative adversarial nets with policy gradient.",
            "rating": 2
        },
        {
            "paper_title": "Texygen: A benchmarking platform for text generation models.",
            "rating": 2
        },
        {
            "paper_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
            "rating": 2
        },
        {
            "paper_title": "Language gans falling short.",
            "rating": 2
        },
        {
            "paper_title": "Fréchet Inception Distance (FID) (original reference)",
            "rating": 1
        }
    ],
    "cost": 0.01816725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Jointly Measuring Diversity and Quality in Text Generation Models</h1>
<p>Ehsan Montahaei*<br>Sharif University of<br>Technology / Tehran, Iran<br>ehsan.montahaei@gmail.com</p>
<p>Danial Alihosseini*<br>Sharif University of<br>Technology / Tehran, Iran<br>dalihosseini@ce.sharif.edu</p>
<p>Mahdieh Soleymani Baghshah<br>Sharif University of<br>Technology / Tehran, Iran<br>soleymani@sharif.edu</p>
<h4>Abstract</h4>
<p>Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generators density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.</p>
<h2>1 Introduction</h2>
<p>Generative models and especially Generative Adversarial Networks (GANs) have been received much attention in the last few years. However, the evaluation of generated samples by these models is challenging. Although some studies have recently focused on introducing measures like Inception</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Score and Fréchet Inception Distance (FID) to compare results of different GAN models for image generation, there is not a study to propose proper metrics for evaluation of text generation models. In the last few years, many GAN-based text generation models (Yu et al., 2017; Lin et al., 2017; Che et al., 2017; Guo et al., 2018; Zhang et al., 2017) have been proposed. However, measuring the performance of these models in the corresponding papers is not comprehensive. GANs suffer from the mode collapse problem (Metz et al., 2016) and the GAN-based text generation models may just produce a highly limited set of sentences and therefore just considering the quality of these generated sentences for comparison is not comprehensive.</p>
<p>On the other hand, there are measures like SelfBLEU (Zhang et al., 2017) for evaluating the diversity of generated sentences, but they can not consider the quality of samples at all. Besides, designing an experiment of evaluating diversity by humans is not straightforward and thus it's necessary to have a jointly quality-diversity measuring metric.</p>
<p>In this paper, we intend to propose metrics sensitive to both quality and diversity simultaneously, assigning low scores not only to models generating low-quality samples but also to the ones with low-diversity samples (including the mode collapsed models). To this end, we first propose the MS-Jaccard as an n-gram based measure that considers the quality and diversity of generated samples simultaneously. It attempts to find the similarity of the set of generated samples by a model and the set of real (or test) samples. Then, a featurebased measure is proposed to compare the real data distribution and the generative model distribution in the feature space. Indeed, by borrowing the idea of FID (Heusel et al., 2017) that is a popular feature-based evaluation metric in im-</p>
<p>age generation tasks and advent of a recent highly deep model named BERT (Devlin et al., 2018) as a reference feature extractor for natural language texts, a metric is proposed for evaluation of natural language generation. Finally, appropriate divergences between the oracle distribution and the (learned) model distribution is introduced for when the probabilistic oracle is considered as synthetic data distribution (and thus the target distribution is available for evaluation).</p>
<h2>2 Text Generation Models</h2>
<p>The neural models on text generation first used LSTMs and trained them by the Maximum Likelihood Estimation (MLE) via teacher forcing (Hochreiter and Schmidhuber, 1997). These models suffer from the exposure bias problem which is due to the train-test discrepancy. Although some solutions such as scheduled sampling were introduced to overcome the exposure bias problem, it has been shown that they are incompatible with the language nature (Bengio et al., 2015; Huszar, 2015). By introducing GANs (Goodfellow et al., 2014) as successful image generation models, it has gained much attention to propose GAN-based text generation models. However, the discrete nature of text needs the generator with discrete outputs that makes passing the gradient from the discriminator to the generator difficult. SeqGAN (Yu et al., 2017) alleviates this difficulty by a gradient policy approach using a REINFORCE-like method to train the generator as a stochastic policy. This method has some difficulties such as reward sparsity and high variance for large action spaces. Subsequent methods try to pass more informative signal from the discriminator to the generator. RankGAN(Lin et al., 2017) trains the discriminator as a ranker which assigns a higher score to the more realistic sequences (in comparison with other sentences in the current batch). LeakGAN (Guo et al., 2018) takes advantage of the feudal networks and considers the discriminator as a manager and the generator as a worker while the feature layer of the discriminator is fed to the generator as leaked information. MaliGAN (Che et al., 2017) attempts to redefine the generator's objective. It minimizes KL divergence between the generator and the real distribution which is obtained by the discriminator in the optimality assumption of the discriminator. This new objective leads to an importance sampling procedure.</p>
<p>TextGAN (Zhang et al., 2017) also applies a new objective for the generator. It tries to push the generator focus from the last layer of the discriminator to its last feature layer. Real data and generator samples will each have some distribution in the feature layer of the discriminator. The generator's objective is to make them closer by Maximum Mean Discrepancy (MMD) metric.</p>
<h2>3 Metrics</h2>
<p>In this section, we first indicate the main difficulties of the existing measures for evaluation of text generation models. Then, we introduce metrics that evaluate the capability of the models in generating both right sentences and various ones. The proposed metrics (that are all symmetric) jointly specify to what extent probable sentences in real data are likely in the generative model and also the probable sentences in the model are likely in the real data.</p>
<h3>3.1 Shortcomings of the existing metrics</h3>
<p>In this section, shortcomings of the metrics that either evaluate the quality or the diversity of generated samples are presented. Moreover, a recent attempt to simultaneously considering these metrics is introduced.</p>
<h3>3.1.1 Quality metrics</h3>
<p>BLEU: It is the most widely used metric for text generation. Originally BLEU (Papineni et al., 2002) is a metric to evaluate the quality of machine-translated text. In unconditional text generation, all sentences in the test set are considered as the reference set and generated sentences are evaluated by computing their average BLEU score on this reference set. In conditional text generation tasks like machine translation which include a limited reference set (for each condition), computing the similarity of the generated text and the reference set may be sensible. However, the reference set for the unconditional text generation task is whole available sentences and measures like BLEU just consider the validity of generated sentences without measuring what proportion of the reference sentences can be covered by the text generation model. On the other hand, GAN-based text generation models may generate a highly limited set of sentences and sacrifice the diversity (due to the mode collapse problem). Therefore, evaluating these models using BLEU score just</p>
<p>shows the validity of their outputs without considering their coverage.</p>
<p>Oracle-NLL: It was introduced by SeqGAN (Yu et al., 2017) and is based on assuming a synthetic oracle distribution. It considers a random distribution as the real distribution (or the oracle) and the training dataset is prepared by sampling from this distribution. The score is defined to be the Negative Log Likelihood (NLL) of the generated samples from the trained model in the oracle distribution. In this measure, the coverage is again neglected and a model that generates only one high quality sentence can reach high performance.</p>
<h3>3.1.2 Diversity metric</h3>
<p>As mentioned above, BLUE and Oracle-NLL just consider the quality of the generated samples and ignore their diversity. Below, we introduce two metrics measuring the diversity. However, these metrics evaluate only diversity and don't consider the quality of samples at all.</p>
<p>Self-BLEU: In (Zhu et al., 2018), Self-BLEU was introduced to evaluate just variety of sentences. It measures BLEU score for each generated sentence by considering other generated sentences as reference. By averaging these BLEU scores (obtained for generated sentences), a metric that is called Self-BLEU is achieved where its lower values shows more diversity.</p>
<p>Entropy: On the other side, we can use the entropy of probabilistic generative model to measure the diversity where the lower values show lower diversity. As the direct calculation of the entropy is not feasible, a Monte-Carlo estimation of it can be used.</p>
<h3>3.1.3 Quality and diversity</h3>
<p>Recently (Caccia et al., 2018) mentioned the flaws of only evaluating the quality and found that MLE outperforms the GAN variants for text generation since it dominates GANs in the quality-diversity space. (Caccia et al., 2018) uses the qualitydiversity spectrum obtained by changing the temperature parameter that controls entropy of the models' conditional distributions. However, it does not provide a measure to assess both the quality and the diversity without needing to inspect the whole quality-diversity spectrum.</p>
<p>Likelihood: Although the likelihood of a generative model on real (test) data evaluates the ability of the model in generating the test samples, it doesn't measure the quality of the whole set of
generated texts by the model. In fact, a model with a low NLL value on test data (or equivalently a model in which the likelihood of the test data is high) may also assign high probability to many other sentences that are not valid or qualified. Specifically for sequence models, the likelihood doesn't assess the free-running mode of models. To be more detailed, most of the probabilistic sequence models, decompose the joint distribution to conditional distributions using the chain rule. These conditional distributions are the probability of each token conditioned on the prior tokens. Thus, in the likelihood evaluation, each of token's probability is conditioned on a prefix that is a real sequence itself and the likelihood is not assessed on the previously generated tokens of the model during evaluation (it is similar to the exposure bias problem of MLE for sequence generation).</p>
<p>Moreover, measuring a model by its likelihood score has another problem. When a model misses one mode of a multi-modal distribution, its score decreases severely; so it is an unfair metric for comparing MLE method with other methods because MLE method uses likelihood as its objective and has mean seeking behavior (Goodfellow, 2017).</p>
<h3>3.2 Proposed metrics</h3>
<p>In this section, we propose metrics that simultaneously considers the quality and the diversity of the generated samples. To this end, we compare the real distribution of texts with the obtained distribution by the text generation model.</p>
<h3>3.2.1 MS-Jaccard</h3>
<p>We first propose a metric that finds the similarity of the generative model and the real distribution by comparing text samples generated by them. To this end, n-grams of generated samples and those of real samples are considered as two multi-sets (that also preserve repetition of n-grams) and the similarity of the resulted multi-sets is computed. In simple words, the MS-Jaccard focuses on the similarity of the n-grams frequencies in the two sets and inspired by the well-known Jaccard Index which determines the similarity of two sets as the ratio of the cardinality of their intersection to that of their union.</p>
<p>To define it formally, let $S_{1}$ and $S_{2}$ be two sets of sentences, $G_{n}$ be the set of n-grams in $S_{1} \cup S_{2}$, and $C_{n}(g, S)$ be the normalized counts of the n gram $g$ in the set $S$. The similarity between n -</p>
<p>grams of two sets $S_{1}$ and $S_{2}$ is defined as:</p>
<p>$$
\operatorname{score}<em G__n="G_{n" _in="\in" g="g">{n}=\frac{\sum</em>
$$}} \min \left{C_{n}\left(g, S_{1}\right), C_{n}\left(g, S_{2}\right)\right}}{\sum_{g \in G_{n}} \max \left{C_{n}\left(g, S_{1}\right), C_{n}\left(g, S_{2}\right)\right}</p>
<p>The geometric mean of the $\left{\operatorname{score}<em n="1">{n}\right}</em>(g, S)$ will denotes the average frequency per sentence for n -gram $g$ in the set $S$. If the generated sentences won't have diversity or quality, the n-gram distribution of generated texts will be different from that of the real texts and causing to decrease the MS-Jaccard score consequently. As it is obvious, the MS-Jaccard is a similarity measure and so its higher value will be better.}^{N}$ will be the MS-Jaccard score called MS-Jaccard- $N$ where the $N$ is the maximum length of $n$-grams. It is worth noting that the frequencies of the n-grams in each set is normalized with respect to the total number of sentences in the set (to avoid diminishing the score when the size of only one of these sets grows). Thus, the $C_{n</p>
<h3>3.2.2 Fréchet BERT Distance (FBD)</h3>
<p>One popular metric for evaluation of image generation models is FID introduced in (Heusel et al., 2017). Each of real and generated images in a feature space (found by Inception network) is modeled by a Gaussian distribution, and the FID is defined as the Fréchet distance between these two Gaussian distributions. We want to introduce a similar measure for the text generation task. To this end, we utilize BERT (Devlin et al., 2018) that provides a proper feature space for texts. We use Fréchet distance in BERT's feature space as a metric that considers quality and variety of generated sentences, and name it Fréchet BERT Distance (FBD). There is a set of pooled features (for classification task) in the BERT network that has a constant size for different input sequence lengths; we used these features for FBD. The Fréchet distance is also known as Wasserstein-2 divergence, and this distance between two Gaussian distribution is as follows:</p>
<p>$$
\sqrt{\left|m_{1}-m_{2}\right|<em 1="1">{2}^{2}+\operatorname{Tr}\left(C</em>
$$}+C_{2}-2\left(C_{1} C_{2}\right)^{1 / 2}\right)</p>
<p>where $m_{i}$ and $C_{i}$ show the mean vector and the covariance matrix of these Gaussians respectively. It should be noted as the FBD is a distance measure, its lower values will be better.</p>
<h3>3.2.3 Oracle Based Evaluation</h3>
<p>In Oracle-NLL evaluation introduced in (Yu et al., 2017), the measured distance is Kullback-Leibler (KL) divergence of the generative model and the oracle which ignores the variety of generated sentences. On the other hand, the inverse KL (that is relevant to the likelihood of real data in the text generation model) can not guarantee the quality of generated samples by the model. We propose measuring the distance of the probabilistic oracle distribution $P$ (that generates real data) and the probabilistic generative model $Q$ by a symmetric distance as an evaluation metric. A wide range of distances can be utilized for this purpose. One symmetric distance is Bhattacharyya that can be estimated by the Monte-Carlo as below:</p>
<p>$$
\begin{aligned}
&amp; B(P, Q)= \
&amp; \quad \frac{-1}{2}\left(\ln \frac{1}{N} \sum_{i=0}^{N} \sqrt{\frac{q\left(x_{i}\right)}{p\left(x_{i}\right)}}+\ln \frac{1}{M} \sum_{j=0}^{M} \sqrt{\frac{p\left(x_{j}\right)}{q\left(x_{j}\right)}}\right)
\end{aligned}
$$</p>
<p>where $\left{x_{i}\right}$ and $\left{x_{j}\right}$ are sets of samples from $P$ and $Q$ distributions respectively. Similar to the FBD, Bhattacharyya is also a distance measure and thus its lower values are better.</p>
<h2>4 Evaluation</h2>
<p>In this section, we first conduct some experiments to evaluate text generation models using the existing and the proposed measures. Then, we discuss about the appropriateness of the proposed metrics.</p>
<h3>4.1 Datasets</h3>
<p>We evaluate the models on COCO image captions (Lin et al., 2014), EMNLP2017 WMT News (Bojar et al., 2017), and IMDB (Maas et al., 2011) as the popular datasets for text generation. In addition to these datasets, similar to (Yu et al., 2017; Lin et al., 2017; Guo et al., 2018), we also consider a synthetic oracle produced by a probabilistic text generator that is a random initialized LSTM as a synthetic dataset. The description of the datasets is as follows:</p>
<ul>
<li>COCO Captions (Lin et al., 2014): It is a collection of image captions containing around 600,000 captions. Sentences having between 5 and 25 words are selected (resulting in 524,225 sentences) where 5,328 is the vocab size of the resulted dataset. Among the resulted dataset, 40,000 samples are used for</li>
</ul>
<p>training, 20,000 samples for validation, and 20,000 for test.</p>
<ul>
<li>EMNLP2017 WMT News (Bojar et al., 2017): It is a collection of news texts for the machine translations task ${ }^{1}$. Among a version of this dataset for English corpus containing 500,000 sentences, sentences having more than 3 words with less than 150 frequency (these words are replaced with UNK) were dropped and sentences that have between 20 and 40 words selected. The vocab size of the resulted dataset is 6,148 . Among this dataset, 40,000 samples are used for training, 20,000 samples for validation, and 20,000 for test.</li>
<li>IMDB Movie Reviews (Maas et al., 2011): It is a collection of IMDB movie reviews for the sentiment analysis task, containing 25,000 labeled and 50,000 unlabeled ones. We have selected the first two sentences of each review and replace words with less that 50 times frequency with UNK and keep sentences from length 5 to 40 with less than 5 UNKs. The final dataset is subsampled to have 20,000 sentences for training data, 10,000 for validation, and 10,000 for test data leading to vocab size of 5,810 .</li>
<li>Oracle synthetic dataset (Yu et al., 2017): A randomly initialized LSTM generator as a real distribution used in oracle training mode; the network implementation is borrowed from the SeqGAN released code ${ }^{2}$. This network's hidden size is 32 and its embedding size is 3,200. Moreover, the vocab size is 5,000 and the length of sequences is 20. The dataset of 100,000 samples are generated according to the above model. Among this dataset, 50,000 samples are used for training, 25,000 for validation, and 25,000 for test.</li>
</ul>
<h3>4.2 Experimental Setup</h3>
<h3>4.2.1 Text Generation Models</h3>
<p>As the recent methods for text generation, we evaluate SeqGAN (Yu et al., 2017), RankGAN (Lin et al., 2017), and MaliGAN (Che et al., 2017). We also consider vanilla Maximum Likelihood Estimation (MLE) language model using LSTM as the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>baseline method. We used the implementation of the above methods in the Texygen platform (Zhu et al., 2018) and train them in this framework ${ }^{3}$. The models were trained on the similar dataset existing in their released code but collected from the original sites reported in corresponding reference papers.</p>
<p>In order to have a fair comparison, all settings of the models (e.g., same hidden) were kept the same as the Texygen framework. Since setting a fixed number of epochs for terminating training of different methods does not seem such reasonable and resulting in unfair scores, we targeted multiple training termination criteria. In the realworld datasets training, the training termination of the GANs were based on obtaining the best BLEU4 on validation data in addition to setting a max number of iterations for all the models. Besides, the training termination of MLE is based the NLL on the validation data while also setting a max number of iterations as above. In the oracle training mode, the termination were done based on both Oracle-NLL on the validation set and again on a max number of iterations for all models.</p>
<h3>4.2.2 Metrics</h3>
<p>Among the existing measures, BLEU2 upto BLEU5 (evaluating only quality), Self-BLUE2 upto Self-BLEU5 (evaluating only diversity), and NLL that shows the negative log likelihood of the model on test data are utilized for real datasets. Moreover, due to the low performance of the Python NLTK (Bird et al., 2009) BLEU library ${ }^{4}$ when needing to evaluate multiple sentences with a fixed reference set, we have re-implemented it to achieve parallel computation and high performance ${ }^{5}$.</p>
<p>Among the proposed measures, MS-Jaccard2 upto MS-Jaccard5 and FBD are assayed on realworld datasets. For synthetic oracle, NLL and Oracle-NLL as the existing measures and the proposed measure for comparing distributions, i.e. Bhattacharyya, are evaluated. It should be noted that, in order to make the metric's directions the same (i.e. their lower values show better performance), the $1-$ MS-Jaccard, $1-$ BLEU and $-1 \times$ Entropy is used in some plots.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table 1: Performance of models (using different measures) on COCO Captions dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{3 8 . 4 1 6}$</td>
<td style="text-align: center;">1.971</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 8}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">55.610</td>
<td style="text-align: center;">4.590</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 8 0}$</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.545</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">39.916</td>
<td style="text-align: center;">$\mathbf{1 . 4 7 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 1}$</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.288</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">48.816</td>
<td style="text-align: center;">3.574</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 2}$</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.402</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of models (using different measures) on EMNLP2017 WMT News dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.133</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 4 3 . 2 4 6}$</td>
<td style="text-align: center;">$\mathbf{4 . 8 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 6 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 7 1}$</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 9 5}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">195.867</td>
<td style="text-align: center;">5.955</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.324</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">163.931</td>
<td style="text-align: center;">5.690</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 4 1}$</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.155</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">177.346</td>
<td style="text-align: center;">5.104</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.224</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of models (using different measures) on IMDB Movie Reviews dataset. MSJ, BL, and SBL denote MS-Jaccard, BLEU, and Self-BLEU respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">FBD</th>
<th style="text-align: center;">MSJ2</th>
<th style="text-align: center;">MSJ3</th>
<th style="text-align: center;">MSJ4</th>
<th style="text-align: center;">MSJ5</th>
<th style="text-align: center;">BL2</th>
<th style="text-align: center;">BL3</th>
<th style="text-align: center;">BL4</th>
<th style="text-align: center;">BL5</th>
<th style="text-align: center;">SBL2</th>
<th style="text-align: center;">SBL3</th>
<th style="text-align: center;">SBL4</th>
<th style="text-align: center;">SBL5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Real Data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.241</td>
</tr>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 2 5 . 2 2 3}$</td>
<td style="text-align: center;">$\mathbf{3 . 5 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 5}$</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 7 9}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">150.213</td>
<td style="text-align: center;">4.587</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 4}$</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.345</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">141.558</td>
<td style="text-align: center;">4.482</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 3}$</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.290</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">151.828</td>
<td style="text-align: center;">3.958</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.331</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of models (using different measures) on Oracle dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">NLL</th>
<th style="text-align: center;">Oracle-NLL</th>
<th style="text-align: center;">Bhattacharyya</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">$\mathbf{1 4 1 . 9 4 8}$</td>
<td style="text-align: center;">167.014</td>
<td style="text-align: center;">$\mathbf{7 . 1 0 5}$</td>
</tr>
<tr>
<td style="text-align: center;">SeqGAN</td>
<td style="text-align: center;">155.353</td>
<td style="text-align: center;">$\mathbf{1 6 3 . 1 7 9}$</td>
<td style="text-align: center;">10.076</td>
</tr>
<tr>
<td style="text-align: center;">MaliGAN</td>
<td style="text-align: center;">146.260</td>
<td style="text-align: center;">168.054</td>
<td style="text-align: center;">8.503</td>
</tr>
<tr>
<td style="text-align: center;">RankGAN</td>
<td style="text-align: center;">160.424</td>
<td style="text-align: center;">166.774</td>
<td style="text-align: center;">12.127</td>
</tr>
</tbody>
</table>
<h3>4.3 Results</h3>
<p>Results of different methods on COCO Captions, EMNLP2017 WMT News, and IMDB datasets as real-world datasets are shown in Tables 1, 2, and 3 , respectively. To provide a target, we have also shown metrics for training data themselves and called the method as Real (indeed training data is considered as the generated data by Real and the measures are computed on them). These tables show that MLE has the best performance according to the proposed measures considering both quality and diversity of samples. In fact, GANbased methods can not generally achieve good performance according to the proposed measures. This result is consistent with the reported results in (Caccia et al., 2018) that compares GANs and MLE for text generation.</p>
<p>Table 4 shows results of different methods on synthetic oracle dataset and MLE again shows the best results according to the proposed metric (that approximates the distance of the real distribution and the generative model distribution).</p>
<p>As mentioned in Section 3.1.3 about (Caccia et al., 2018), the whole spectrum of qualitydiversity is considered for evaluation of Natural Language Generation (NLG) methods. In fact, in (Caccia et al., 2018), the temperature sweep is utilized to robustly evaluate text generation methods. More precisely, the generators conditional distribution $G\left(x_{t} \mid x_{1: t-1}\right)$ is defined as $\operatorname{Softmax}\left(o_{t} / T\right)$ where $o_{t}$ denotes the logit at time $t$. Decreasing $T$ below 1.0 will decrease the entropy of conditional probability and thus reduce the probability of generating low quality samples. On the other hand, increasing this temperature above 1.0 will upraise the entropy of the conditional distribution and thus improve the diversity of the generated samples (Caccia et al., 2018).</p>
<p>We intend to show that the proposed metrics are correlated with the analysis of the whole space of quality-diversity obtained by changing the temperature. In fact, using the proposed metrics we can</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Diversity vs. quality measure of various models with temperatures from $1.5^{-3}$ to $1.5^{4}$ on different datasets. Each point in the plot corresponds to the performance of a model in a special temperature (A seconddegree polynomial has been fitted to the points). Lower values in both axes show better ones.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: NLL, 1-MS-Jaccard4, and FBD scores of all the models without applying temperature (i.e. $T=1$ ) on different datasets. Lower values show better performance.
usually predict the behavior of the model in whole spectrum without needing to provide this qualitydiversity space.</p>
<p>Fig. 1 shows the diversity against quality
measures with different values of temperature. Figs. 1a, 1b, and 1c consider Self-BLEU4 as diversity and BLEU4 as quality measure for each of the methods on real-world COCO, EMNLP2017,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The performance of all models (without applying temperature, i.e. $T=1$ ) on the Oracle dataset using different measures. Lower values show better performance.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Pearson correlation of all metrics when aggregating results on the real world text datasets and all temperatures.
and IMDB datasets. The metrics are also evaluated on the train data itself which is called Real in the mentioned figures. Moreover, for Oracle dataset, since we have the probabilistic distribution of data, we can compute the likelihood of the generated samples by the model in the real distribution (i.e. Oracle) to find the quality of the generated samples. Therefore, the Oracle-NLL is used as quality measure of the methods on the synthetic dataset in Fig. 1d and Entropy is used as a diversity measure in this figure.</p>
<p>On the other hand, Figs. 2 and 3 present the performance of different methods (with $T=1$ ) on non-synthetic and synthetic datasets respectively.</p>
<p>It is worth noting that NLL, Entropy, and Bhattacharyya of Real could not be computed, since we do not have a model for real data and just considering training data as its samples. According to Fig. 2b, the ordering of the methods obtained by MSJaccard4 on these datasets is almost always consistent with the ordering of the methods according to their dominance in Figs. 1a to 1c. For example, in Fig. 1b that shows results on EMNLP2017 dataset, the best method which dominates others is MLE, the second best is MaliGAN, the third one is RankGAN, and SeqGAN is the last one that under-performs all other methods. Consistently, the proposed MS-Jaccard4 measure shown in Fig. 2b provides the same ordering. Moreover, the ordering of the methods according to FBD metric in Fig. 2c on different datasets is almost always consistent with their ordering obtained by analyzing the whole spectrum in Figs. 1a to 1c. For the oracle dataset 3, the proposed Bhattacharyya distance of the distributions introduced in Section 3.2.3 is consistent with the ordering obtained in Fig. 1d.</p>
<p>Finally, we display the Pearson correlation of different metrics on real datasets in Fig. 4. According to this figure, the proposed metrics for real-world datasets, i.e. 1-MS-Jaccard and FBD, are highly correlated. Besides, among the measures, these are the most correlated ones to NLL.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we first discussed shortcomings of the existing measures for evaluating text generation models. Then, we proposed some measures to more effectively specify the capability of models in generating both qualified and diverse texts. The MS-Jaccard as an n-gram based metric was firstly introduced that is capable of measuring both the quality and coverage of methods in text generation. Then, a feature-based metric FBD which is based on the BERT model was introduced. Moreover, for oracle training mode in which the generators density can also be calculated, we proposed to use (estimation of) divergences like Bhattacharyya defined on probability distributions as a metric to compute the distance of the generative model and the oracle. Finally, the performance of different text generation models were evaluated, the obtained results were analyzed and showed that the proposed metrics have high correlations and are almost consistent with the dominance ordering of models in quality-diversity spectrum.</p>
<h2>References</h2>
<p>Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1171-1179.</p>
<p>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O'Reilly.</p>
<p>Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 78, 2017, pages 169-214. Association for Computational Linguistics.</p>
<p>Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. CoRR, abs/1811.02549.</p>
<p>Tong Che, Yanran Li, Ruixiang Zhang, R. Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. 2017. Maximum-likelihood augmented discrete generative adversarial networks. CoRR, abs/1702.07983.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Ian J. Goodfellow. 2017. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160.</p>
<p>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative adversarial networks. CoRR, abs/1406.2661.</p>
<p>Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press.</p>
<p>Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626-6637.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, $9(8): 1735-1780$.</p>
<p>Ferenc Huszar. 2015. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? CoRR, abs/1511.05101.</p>
<p>Kevin Lin, Dianqi Li, Xiaodong He, Ming-Ting Sun, and Zhengyou Zhang. 2017. Adversarial ranking for language generation. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 49 December 2017, Long Beach, CA, USA, pages 3158-3168.</p>
<p>Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: common objects in context. In Computer Vision ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pages 740-755. Springer.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 142-150. The Association for Computer Linguistics.</p>
<p>Luke Metz, Ben Poole, David Pfau, and Jascha SohlDickstein. 2016. Unrolled generative adversarial networks. CoRR, abs/1611.02163.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA., pages 311-318. ACL.</p>
<p>Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., pages 2852-2858. AAAI Press.</p>
<p>Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. 2017. Adversarial feature matching for text generation. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 4006-4015. PMLR.</p>
<p>Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation models. SIGIR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ http://statmt.org/wmt17/translation-task.html
${ }^{2}$ https://github.com/LantaoYu/SeqGAN/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://github.com/geek-ai/Texygen
${ }^{4}$ https://www.nltk.org/ modules/nltk/
${ }^{5}$ https://github.com/Danial-Alh/FastBLEU&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-469 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-469</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-469</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-273798465</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.00640v1.pdf" target="_blank">Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations</a></p>
                <p><strong>Paper Abstract:</strong> Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e469.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e469.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-eval-stat-framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statistical framework for language model evaluations (standard errors, clustering, variance reduction, and power analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive analytic framework treating language-model evaluations as experiments drawn from an underlying super-population of questions, providing formulas for standard errors (including clustered SE), variance decomposition, resampling, next-token scoring, paired comparisons, and sample-size/power calculations to quantify and reduce measurement noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / language model evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Designing and analyzing benchmark evaluations of LLMs (scoring, comparing models, planning eval size and resampling strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling-of-questions from a super-population (finite n), conditional (per-question) randomness from model sampling (stochastic outputs), intra-cluster dependence when questions are grouped (clustered sampling), model differences in conditional means, sampling temperature, sampling method (sampling vs greedy / next-token probability), score type (Bernoulli vs fractional metrics), resampling count K (number of answers per question), and estimator/sampling scheme choices (e.g., using SE_Bernoulli when scores are fractional).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard error of the mean (SE_C.L.T.), SE_Bernoulli, variance and variance decomposition (Var(x) and E[σ^2_i]), clustered standard error (SE_clustered), covariance and Pearson correlation between model question scores, z-scores and 95% confidence intervals, Minimum Detectable Effect (MDE) and power/sample-size formula (n), intra-cluster correlation implied in clustered SE.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Analytic and worked-example quantitative results: clustered SE can be up to ~3× larger than naive SE in real-world examples (Table 4); resampling K reduces conditional variance with concrete ratios (K=2 reduces total variance by ≈1/3 relative to K=1; K=4 → 1/2 reduction; K=6 → 5/9 reduction); next-token probability scoring can reduce estimator variance by up to 2/3 relative to grading a single sampled answer; paired differences reduce variance (example: with score correlation 0.5, variance reduced by 1/3); power/sample-size example: to detect δ=0.03 with ω^2=1/9, α=0.05, β=0.2 requires n≈969 independent questions; increasing per-question sampling from K=1 to K=10 reduced MDE from 13.2% to 7.5% in a worked example.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Confidence intervals and standard errors (SE_C.L.T., SE_clustered), paired-difference standard errors and correlations, power analysis / Minimum Detectable Effect, and cluster-adjusted sample-size formulas (clustered ω^2 and σ^2 estimates).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Methodological conclusions rather than replication outcomes: demonstrations that failing to account for clustering or conditional variance leads to anti-conservative CIs (too narrow) and misleading comparisons; concrete numeric illustration that cluster adjustment materially changes uncertainty (e.g., 3× larger SE in one eval), and that appropriate paired analyses and K resampling meaningfully improve detectability of differences (see variability_results for numeric examples).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Ignoring clustered question dependence (intra-cluster correlations) causes inconsistent/anti-conservative standard errors; using SE_Bernoulli for fractional metrics mis-specifies SE; small n reduces power and increases MDE; conditional stochasticity of model sampling inflates observed variance; changing temperature for variance reduction can introduce bias and increase variance of conditional means; pooled SE across KN answers is invalid when multiple answers per question are dependent; failure to report SEs and pairwise stats impedes proper comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Compute CLT standard errors (SE_C.L.T.) and report them with means; use clustered standard errors when questions are grouped; resample answers K>1 and compute question-level mean scores; analyze next-token probabilities (expected score) when available to eliminate conditional sampling variance; perform paired-differences analysis when same questions evaluated by multiple models; compute and report score correlations; run power analysis / sample-size calculation (Equation 9/10) to choose n and K; avoid altering sampling temperature to reduce variance (unless studying that temperature); when subsampling for variance estimation, sample at cluster level.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantified in examples: resampling (K) reduces conditional variance with diminishing returns (K=2 → 1/3 reduction, K=4 → 1/2, K=6 → 5/9; upper limit in example 2/3); next-token-probability scoring can match the upper limit (≈2/3 reduction) compared to single-sample grading; paired analysis with correlation 0.5 reduced estimator variance by ≈1/3 in worked example; clustered SE adjustments increased SE by up to ~3× in one real dataset example; increasing K from 1 to 10 reduced MDE from 13.2% to 7.5% in a worked example.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-model evals contain multiple sources of variability (question sampling, conditional sampling, clustering) that must be explicitly modeled; using CLT SEs, clustered SEs, resampling (K), next-token probabilities, and paired differences substantially reduces uncertainty and improves reproducibility, while inappropriate practices (e.g., changing temperature to suppress sampling variance or mis-specifying SE formulas) can introduce bias or understate uncertainty. Specific numeric examples: clustered SE up to 3× larger than naive; resampling/next-token-probability can reduce variance up to ~2/3; detecting small differences (≈3%) may require ~1,000 independent questions under realistic assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quantifying Variance in Evaluation Benchmarks <em>(Rating: 2)</em></li>
                <li>Inspect: An open-source framework for large language model evaluations <em>(Rating: 2)</em></li>
                <li>The Llama 3 Herd of Models <em>(Rating: 2)</em></li>
                <li>With Little Power Comes Great Responsibility <em>(Rating: 2)</em></li>
                <li>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference <em>(Rating: 1)</em></li>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-469",
    "paper_id": "paper-273798465",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "LLM-eval-stat-framework",
            "name_full": "Statistical framework for language model evaluations (standard errors, clustering, variance reduction, and power analysis)",
            "brief_description": "A comprehensive analytic framework treating language-model evaluations as experiments drawn from an underlying super-population of questions, providing formulas for standard errors (including clustered SE), variance decomposition, resampling, next-token scoring, paired comparisons, and sample-size/power calculations to quantify and reduce measurement noise.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural language processing / language model evaluation",
            "experimental_task": "Designing and analyzing benchmark evaluations of LLMs (scoring, comparing models, planning eval size and resampling strategies)",
            "variability_sources": "Sampling-of-questions from a super-population (finite n), conditional (per-question) randomness from model sampling (stochastic outputs), intra-cluster dependence when questions are grouped (clustered sampling), model differences in conditional means, sampling temperature, sampling method (sampling vs greedy / next-token probability), score type (Bernoulli vs fractional metrics), resampling count K (number of answers per question), and estimator/sampling scheme choices (e.g., using SE_Bernoulli when scores are fractional).",
            "variability_measured": true,
            "variability_metrics": "Standard error of the mean (SE_C.L.T.), SE_Bernoulli, variance and variance decomposition (Var(x) and E[σ^2_i]), clustered standard error (SE_clustered), covariance and Pearson correlation between model question scores, z-scores and 95% confidence intervals, Minimum Detectable Effect (MDE) and power/sample-size formula (n), intra-cluster correlation implied in clustered SE.",
            "variability_results": "Analytic and worked-example quantitative results: clustered SE can be up to ~3× larger than naive SE in real-world examples (Table 4); resampling K reduces conditional variance with concrete ratios (K=2 reduces total variance by ≈1/3 relative to K=1; K=4 → 1/2 reduction; K=6 → 5/9 reduction); next-token probability scoring can reduce estimator variance by up to 2/3 relative to grading a single sampled answer; paired differences reduce variance (example: with score correlation 0.5, variance reduced by 1/3); power/sample-size example: to detect δ=0.03 with ω^2=1/9, α=0.05, β=0.2 requires n≈969 independent questions; increasing per-question sampling from K=1 to K=10 reduced MDE from 13.2% to 7.5% in a worked example.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Confidence intervals and standard errors (SE_C.L.T., SE_clustered), paired-difference standard errors and correlations, power analysis / Minimum Detectable Effect, and cluster-adjusted sample-size formulas (clustered ω^2 and σ^2 estimates).",
            "reproducibility_results": "Methodological conclusions rather than replication outcomes: demonstrations that failing to account for clustering or conditional variance leads to anti-conservative CIs (too narrow) and misleading comparisons; concrete numeric illustration that cluster adjustment materially changes uncertainty (e.g., 3× larger SE in one eval), and that appropriate paired analyses and K resampling meaningfully improve detectability of differences (see variability_results for numeric examples).",
            "reproducibility_challenges": "Ignoring clustered question dependence (intra-cluster correlations) causes inconsistent/anti-conservative standard errors; using SE_Bernoulli for fractional metrics mis-specifies SE; small n reduces power and increases MDE; conditional stochasticity of model sampling inflates observed variance; changing temperature for variance reduction can introduce bias and increase variance of conditional means; pooled SE across KN answers is invalid when multiple answers per question are dependent; failure to report SEs and pairwise stats impedes proper comparison.",
            "mitigation_methods": "Compute CLT standard errors (SE_C.L.T.) and report them with means; use clustered standard errors when questions are grouped; resample answers K&gt;1 and compute question-level mean scores; analyze next-token probabilities (expected score) when available to eliminate conditional sampling variance; perform paired-differences analysis when same questions evaluated by multiple models; compute and report score correlations; run power analysis / sample-size calculation (Equation 9/10) to choose n and K; avoid altering sampling temperature to reduce variance (unless studying that temperature); when subsampling for variance estimation, sample at cluster level.",
            "mitigation_effectiveness": "Quantified in examples: resampling (K) reduces conditional variance with diminishing returns (K=2 → 1/3 reduction, K=4 → 1/2, K=6 → 5/9; upper limit in example 2/3); next-token-probability scoring can match the upper limit (≈2/3 reduction) compared to single-sample grading; paired analysis with correlation 0.5 reduced estimator variance by ≈1/3 in worked example; clustered SE adjustments increased SE by up to ~3× in one real dataset example; increasing K from 1 to 10 reduced MDE from 13.2% to 7.5% in a worked example.",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "Language-model evals contain multiple sources of variability (question sampling, conditional sampling, clustering) that must be explicitly modeled; using CLT SEs, clustered SEs, resampling (K), next-token probabilities, and paired differences substantially reduces uncertainty and improves reproducibility, while inappropriate practices (e.g., changing temperature to suppress sampling variance or mis-specifying SE formulas) can introduce bias or understate uncertainty. Specific numeric examples: clustered SE up to 3× larger than naive; resampling/next-token-probability can reduce variance up to ~2/3; detecting small differences (≈3%) may require ~1,000 independent questions under realistic assumptions.",
            "uuid": "e469.0",
            "source_info": {
                "paper_title": "Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quantifying Variance in Evaluation Benchmarks",
            "rating": 2,
            "sanitized_title": "quantifying_variance_in_evaluation_benchmarks"
        },
        {
            "paper_title": "Inspect: An open-source framework for large language model evaluations",
            "rating": 2,
            "sanitized_title": "inspect_an_opensource_framework_for_large_language_model_evaluations"
        },
        {
            "paper_title": "The Llama 3 Herd of Models",
            "rating": 2,
            "sanitized_title": "the_llama_3_herd_of_models"
        },
        {
            "paper_title": "With Little Power Comes Great Responsibility",
            "rating": 2,
            "sanitized_title": "with_little_power_comes_great_responsibility"
        },
        {
            "paper_title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
            "rating": 1,
            "sanitized_title": "chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference"
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        }
    ],
    "cost": 0.00797125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations
November 4, 2024</p>
<p>Evan Miller evanmiller@anthropic.com 
Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations
November 4, 20243AF058BCCEA27B7589DEB59229A23455arXiv:2411.00640v1[stat.AP]
Evaluations are critical for understanding the capabilities of large language models (LLMs).Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning.This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations.Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment.We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.</p>
<p>Introduction</p>
<p>Language models are measured in the literature by evaluations, or evals.Evals are commonly run and reported with a "highest number is best" mentality; industry practice is to highlight a state-of-the-art (SOTA) result in bold, but not necessarily to test that result for any kind of statistical significance.[15] Chatbot Arena [4] has popularized the use of confidence intervals in its Elo scores, but error bars remain noticeably absent from traditional question-and-answer evals.One recent and notable exception is the technical report on the Llama 3 model family [7], which includes simple confidence intervals on a number of evals.</p>
<p>In this article, we seek to introduce rigorous statistical thinking into the world of language model evaluations, so that researchers may quantify the precision with which they are able to answer questions and test hypotheses using evals.After developing a comprehensive analytic framework, we make specific recommendations for the computation of confidence intervals and the reporting of eval results.Using this framework, we show that the confidence intervals recently reported in [7] are likely too narrow in some cases and too wide in other cases.</p>
<p>A short hypothetical example will motivate the overall discussion.Imagine that two competing models, code-named "Galleon" and "Dreadnought", are being considered for deployment in a particular application (say, with a bent toward coding and mathematical reasoning tasks).As part of the decision-making process, three popular language model evaluations are performed on the two models: MATH, a mathematical reasoning eval [9]; HumanEval, a Python coding eval [3]; and MGSM, a multilingual eval covering grade-school math [18].The fictional results from this hypothetical comparison are presented in Table 1 On its face, the data table presents conflicting results: Galleon appears to outperform Dreadnought on MATH (65.5% vs 63.0%), but Dreadnought has performed better on HumanEval and MGSM by slightly wider margins.Is it safe to conclude from the results that Dreadnought is generally better suited for coding and mathematical tasks, given its margin of victory on two of three evals?Or should something in the data potentially give us pause?</p>
<p>"Evaluating the evaluations" is a complex undertaking fraught with both qualitative and quantitative considerations.[8] Remaining agnostic about the relative and qualitative merits of various evals, this article develops a framework for answering quantitative questions about specific eval results.With the aim of informing holistic decision-making, we offer a series of recommendations for running and reporting evals in a way that enables researchers to test well-formed hypotheses about competing models, competing hyperparameters, and competing prompts.Our specific recommendations to researchers include:</p>
<ol>
<li>Computing standard errors of the mean using the Central Limit Theorem 2. When questions are drawn in related groups, computing clustered standard errors 3. Reducing variance by resampling answers and by analyzing next-token probabilities 4. When two models are being compared, conducting statistical inference on the questionlevel paired differences, rather than the population-level summary statistics 5. Using power analysis to determine whether an eval (or a random subsample) is capable of testing a hypothesis of interest Drawing on statistical theory and the experimental design literature, we demonstrate that a small number of conceptual assumptions unlocks a rich theoretical landscape for researchers studying language model evaluations, and show practitioners how to conduct statistical inference on often-noisy eval data.For an overview of experiment design, we refer the reader to [11].</li>
</ol>
<p>Analysis framework</p>
<p>Suppose that the questions in an eval do not represent all possible questions, but instead were drawn at random from a (hypothetical, infinite, unseen) super-population of questions.This simple supposition lets us jump "through the looking glass" of the specific questions that appear in an eval in order to study the underlying skill that the eval is attempting to measure.We modify this assumption in Section 2.2 to study questions that may have been drawn together.</p>
<p>Independent questions</p>
<p>More formally, suppose an eval consists of n independently drawn questions.We write the score on question i as s i , decomposing the score into a mean component x i and a zero-mean random
s i = x i + ϵ i
We refer to x i as the conditional mean and the variance of the random component ϵ i as the conditional variance, that is, the mean and variance conditional on the selection of question i in the eval.Denote this latter quantity σ 2 i = Var(ϵ i ).We also write an unconditional version of these numbers (that is, unconditional on the selection of i) as s = x + ϵ Let µ represent the unobserved mean value of the super-population with µ
= E[s] = E[x].
We wish to conduct inference on (that is, the "true" mean eval score) given only the observed question scores s 1 , . . ., s n .Let s = 1 n i s i represent the average of the observed scores.It follows from the Law of Large Numbers that µ may be estimated using μ = s, and from the Central Limit Theorem (C.L.T.) that the standard error of μ can be estimated as
SE C.L.T. = Var(s)/n = 1 n − 1 i (s i − s) 2 /n(1)
In the special case where s i ∈ {0, 1} (a Bernoulli variable), Equation 1becomes
SE Bernoulli = s(1 − s)/n(2)
We note that it is common practice to compute the standard error of the mean by bootstrapping; see, for instance, the OpenAI evals[16] frameworks.But the Central Limit Theorem is applicable to any evals having scores with finite variance and a large number of questions, and so we regard bootstrapping as unnecessary unless a complicated sampling scheme or estimator is being used.We also note that [7] incorrectly estimates all of its standard errors using SE Bernoulli , even when s i takes fractional values, such as with an F1 score.In these cases, SE Bernoulli will tend to be conservative (too wide) compared to SE C.L.T. .The Inspect framework [12] correctly computes SE C.L.T. with its built-in stderr() metric.</p>
<p>We suggest reporting the standard error of the mean alongside (beneath) the mean when reporting eval scores.A common practice in other sciences is to report the standard error in parentheses; we suggest emulating this practice.See Table 2 for an example.</p>
<p>A 95% confidence interval may be computed from such a table as
CI 95% = s ± 1.96 × SE C.L.T.(3)</p>
<p>Clustered questions</p>
<p>We next consider eval questions that are drawn in groups, or clusters.For instance, DROP [6], QuAC [5], RACE [13], and SQuAD [17] are reading-comprehension evals having multiple related questions about independently selected passages of text, and multilingual evals such as MGSM [18] consist of the same question translated into many languages.Because the inclusion of questions is non-independent, a key assumption of the Central Limit Theorem (or a bootstrap) is violated, and so a naive application of Equation 1 will yield inconsistent standard errors.Here we show how to use clustered standard errors [1], a technique developed in the social sciences, to account for the dependence and correlation structure present in question clusters.Let s i,c denote the ith question score within cluster c, and assume that draws of clusters are independent.Continue to denote the mean observed score as s.The cluster-adjusted standard error of the mean score can be computed as
SE clustered =   SE 2 C.L.T. + 1 n 2 c i j̸ =i (s i,c − s)(s j,c − s)   1/2(4)
The clustered standard error acts as a kind of "sliding scale" between cases where scores within a cluster are perfectly correlated (in which case each cluster acts as a single independent observation) and perfectly uncorrelated (in which case the clustered standard error is equivalent to the unclustered case).The intra-cluster correlations (or lack thereof) are captured by the triple summation (over clusters and cross-terms within clusters); for a derivation of Equation 4, see Appendix A.</p>
<p>A suggested format for reporting cluster-adjusted standard errors is presented in Table 3, and some real-world numbers are reported in Table 4.We note that the cluster adjustment in our real-world example is far from trivial (up to 3X).Failure to adjust standard errors for clustered sampling may lead an unsuspecting analyst to suppose that the measurement of the overall eval score is much more precise than it actually is.We therefore advise that the confidence intervals for reading-comprehension evals reported in [7] are likely anti-conservative (too narrow).</p>
<p>Variance reduction</p>
<p>The standard error of μ quantifies the uncertainty associated with our estimate of the overall eval score.Reducing this quantity (which is the square root of the estimate variance) improves the precision of the estimate.</p>
<p>The variance associated with μ may be decomposed into two components: the variance of the conditional mean, that is, the variance associated with choosing questions from the superpopulation, and the mean conditional variance, which is the mean variance of scores associated with the questions that were chosen.This decomposition is additive, and follows from the law of total variance.Mathematically,
Var(μ) = Var(s)/n = (Var(x) + E[σ 2 i ])/n
This equation has a few implications: the simplest way to reduce the variance of μ is to increase n, the number of sampled questions.The variance of the conditional mean, Var(x), is a property of the super-population and therefore immutable; but we have a couple of strategies available for reducing the overall variance via the expected conditional variance, E[σ 2  i ].</p>
<p>Resampling</p>
<p>The first strategy for reducing the expected conditional variance is to resample the model a number of times, and to compute the standard error using the question-level mean scores from the resamples.Suppose each question is sampled (answered) K times, and the score s i is the mean of these K answer scores.Since the conditional variances are equal for all K answer scores, the overall conditional variance becomes
Var(s i ) = σ 2 i /K
This relation should clarify the issue of how many times to resample a given question.Once E[σ 2 i ]/K ≪ Var(x), increasing K further will have little effect on the standard error of μ.We work through an example to show how to compute a value for K. Suppose scores are binary (0 or 1) and question difficulty is uniformly distributed, x ∼ U [0, 1].Then ϵ i = 1 − x i with probability x i and ϵ i = −x i otherwise.A bit of integration reveals that Var(x) = 1/12 and E[σ 2  i ] = 1/6.The required relation reduces to K ≫ 2, or equivalently 2/K ≪ 1. Writing the variance of this estimator with arbitrary K in terms of the estimator with K = 1, we have Var(μ|K &gt; 1) = Var(μ|K = 1) × (1 + 2/K)/3</p>
<p>Going from K = 1 (no resampling of answers) to K = 2, the total variance is reduced by 1/3.Increasing to K = 4, we have a variance reduction of 1/2, and setting K = 6, we reduce variance by 5/9.The upper limit on variance reduction via resampling in this example is 2/3.</p>
<p>Note that computing a pooled standard error across all KN answers will be inconsistent, as multiple answers to the same question would violate the assumption of independent draws.Refer to Section 2.2 for a discussion of questions drawn in related groups.</p>
<p>Next-token probabilities</p>
<p>The second strategy for reducing the expected conditional variance E[σ 2  i ] is to eliminate the term altogether.For language model evals that do not utilize chain-of-thought reasoning, the conditional variance can be removed by analyzing next-token probabilities, rather than evaluating the model's sampled (or resampled) output.</p>
<p>Consider for instance a multiple-choice eval, and a prompt that induces a model to produce its answer in its first token.If a correct answer is worth 1 and an incorrect answer is worth 0, and the probability of the correct token is denoted p i , then s i = x i = p i and ϵ i = 0.This yields Var(μ) = Var(p)/n.Using the uniform-difficulty example from the previous section, next-token probabilities will reduce the variance of the estimator by 2/3 (the upper limit achievable via resampling) compared to grading a single sample from each question.</p>
<p>Don't touch the thermostat!</p>
<p>It may be tempting to reduce the "sampling temperature" [10] of the model in order to reduce (or eliminate) the conditional variance.However, we advise against this practice, unless the purpose is to study the model at the new temperature.Besides altering the model's behavior, adjusting the sampling temperature may simply shift the conditional variance (which can be mitigated using the two techniques above) into the variance of the conditional means (which cannot), or else reduce conditional variance by injecting bias into the estimator.Two short examples will illustrate these points.</p>
<p>Consider a single-token true/false eval where the conditional score means at T = 1 are uniformly distributed, x T =1 ∼ U [0, 1].As in Section 3.1, Var(x T =1 ) = 1/12.But at T = 0, x T =0 = 1{x T =1 &gt; 0.5} and the uniform distribution is "rounded" into a Bernoulli distribution with p = 1/2.So Var(x T =0 ) = 1/4.In this case, reducing the sampling temperature, and thereby eliminating the conditional variance, has inadvertently tripled the minimum variance in the score data from 1/12 to 1/4.</p>
<p>In the above case, E[x T =1 ] = E[x T =0 ], but this does not always hold.Consider a similar (single-token, true/false) case where x T =1 ∼ U [1/3, 1] and (as a consequence) x T =0 is rounded to a Bernoulli distribution with p = 1/4.Then E[x T =1 ] = 2/3 &lt; E[x T =0 ] = 3/4 and Var(x T =1 ) = 1/27 ≪ Var(x T =0 ) = 3/16; that is, not only has the temperature change shifted the expected score, but the variance of the conditional means has increased approximately five-fold.</p>
<p>We therefore recommend a two-pronged variance-reduction strategy.When next-token probabilities are available, and the language model eval can be conducted using next-token probabilities (i.e.without token generation), compute the expected score for each question, and compute the standard error of expected scores across questions.When next-token probabilities are not available, or the answer requires a chain of thought or other complex interaction, choose a K such that E[σ 2 i ]/K ≪ Var(x) and compute the standard error across question-level mean scores.In neither case should the sampling temperature be adjusted for the sake of reducing variance in the scores.</p>
<p>Comparing models</p>
<p>Thus far we have only analyzed the standard error of eval scores considered in isolation.But a particular model's score on a given eval usually does not have any inherent meaning; it primarily makes sense in relation to the scores of other models.In this section we provide formulas for comparing the scores of two models so that an analyst might determine if a model is outperforming another model in a statistically significant way, or if the difference between two models is indistinguishable from noise.</p>
<p>Unpaired analysis</p>
<p>We introduce model subscripts A and B for the remainder of the article.A naive comparison between eval scores can be made by computing the difference between mean eval scores μA−B = μA − μB and an associated standard error
SE A−B = SE 2 A + SE 2 B
This two quantities can be used to compute the usual 95% confidence interval and z-score
CI A−B,95% = μA−B ± 1.96 × SE A−B (5)z A−B = μA−B /SE A−B (6)
If two models independently report their eval scores and standard errors, it is possible for an analyst to test their difference for statistical significance -even if the two model reports used non-identical random subsets of eval questions.</p>
<p>Paired analysis</p>
<p>The naive comparison above misses an opportunity to reduce the standard error when two models evaluate the same set of questions.Let s A−B,i = s A,i − s B,i represent the difference between scores on question i, and let sA−B = sA − sB represent the observed average difference.Then we can estimate the standard error of the estimated difference as
SE A−B,paired = Var(s A−B )/n = 1 n − 1 i (s A−B,i − sA−B ) 2 /n(7)
This revised standard error can be plugged into Equations 5 and 6 to compute a confidence interval and z-score.</p>
<p>We can compute the reduction in variance achieved with this paired differences test over the unpaired test.First, write out an expression for the variance in the unpaired case   5: Suggested presentation of pairwise differences, standard errors, confidence intervals, and correlation values as a supplement to main results.In the fictional data above, the difference between the two models on MATH is statistically significant (the confidence interval is positive), but the differences on HumanEval and MGSM are not statistically significant at the 5% level.</p>
<p>We can thus reduce the variance with paired differences as long as the conditional means of the model scores are correlated; that is to say, if the two models have some amount of agreement on which questions are "easy" and which questions are "hard".</p>
<p>A short calculation will demonstrate the degree of variance reduction that might be expected.Suppose an eval uses next-token probabilities to form continuous scores with zero conditional variance, and that these scores are uniformly distributed for two models over the [0, 1] interval.Suppose that the model scores have a correlation coefficient of 0.5.Then Var(s A ) = Var(s B ) = 1/12 and Cov(x A , x B ) = 0.5 Var(s A )Var(s B ) = 1/24.In this case, using paired differences will reduce the variance of the estimator by 1/3 in relative terms (that is, from 1/6 to 1/9 in absolute terms).</p>
<p>Because eval question scores are likely to be positively correlated, even across unrelated models, paired differences represent a "free" reduction in estimator variance when comparing two models.We therefore recommend using the paired version of the standard error estimate wherever practicable.We encourage authors of technical reports to include pairwise differences, pairwise standard errors, and score correlations whenever two or more models are being evaluated.Pairwise standard errors may be computed either directly on the differences, or using the single-sample standard errors, the Pearson product-moment correlation, and the relation
SE A−B,paired = SE 2 A + SE 2 B − 2 SE A SE B Corr(s A , s B )
A clustered version of the standard error, appropriate for DROP, QuAC, RACE, SQuAD, MGSM, and other evals where questions are drawn in related groups, is directly computable from the differences as
SE A−B,paired,clustered = 1 n   c i j (s A−B,i,c − sA−B )(s A−B,j,c − sA−B )   1/2(8)
where s A−B,i,c denotes the score difference on the ith question within cluster c.A suggested table format for reporting pairwise results is provided in Table 5.A 95% confidence interval on model differences may be computed from the base estimate and standard error, as in Equation 3.</p>
<p>We now possess the analytic tools needed to rigorously answer the questions posed in the Introduction.Using pairwise analysis on all three evals, and ensuring that standard errors were appropriately clustered on MSGM, the numbers in Table 5 would lead us to conclude that the Galleon indeed outperformed Dreadnought on MATH in a statistically significant way -but that the differences on HumanEval and MGSM are indistinguishable from statistical noise.In other words, while a superficial reading of the eval data might have originally tempted us to conclude that Dreadnought was the overall better-performing model, a closer examination of the data would tend to lead the careful analyst to the opposite conclusion.</p>
<p>Power analysis</p>
<p>Power refers to the ability of an experiment to make a measurement of interest in the presence of statistical noise.[14] In the context of language model evals, we may wish to know whether a model represents an improvement of some magnitude over another model.[2] Due to the variance implied by sampling questions from the super-population (plus the conditional variance after the questions are chosen), power must always be defined in terms of probability.In this section we present a sample-size formula needed to conduct power analysis for language model evals, and apply it in a worked example to answer the empirical question posed in Section 1.</p>
<p>The sample-size formula -describing the relationship between the hypothesized difference between two models and the number of questions included in an experiment -ought to prove useful in several ways.Consumers of existing evals may use the formula to determine the number of questions to subsample from a large eval, or to determine an appropriate value of K defined in Section 3.1.If the number of questions in the eval is fixed, consumers can calculate the Minimum Detectable Effect and decide whether the eval is worth running.The authors of new evals may use the formula to decide how many questions should be commissioned.</p>
<p>The inputs into the sample-size formula include:</p>
<p>• Significance level α, which represents the Type I error rate under the null hypothesis • Power level 1−β, where β represents the Type II error rate under the alternative hypothesis • Minimum Detectable Effect δ, which represents the mean score difference between two models under the alternative hypothesis</p>
<p>To simplify notation, let
ω 2 = Var(x A ) + V ar(x B ) − 2Cov(x A , x B ) σ 2 A = E[σ 2 A,i ] σ 2 B = E[σ 2
B,i ] Let z p represent the (1 − p)th percentile of a standard normal distribution.We assume a paired analysis described in Section 4.2, and that answers will be sampled K A times from model A and K B times from model B (in the simplest case, K A = K B = 1).Then the number of independent questions n required to achieve a Type I error rate α and Type II error rate β with a given Minimum Detectable Effect δ is
n = (z α/2 + z β ) 2 (ω 2 + σ 2 A /K A + σ 2 B /K B )/δ 2(9)
The quantities ω 2 , σ 2 A , and σ 2 B may be estimated from previous eval data.A short derivation of the above formula is presented in Appendix B.</p>
<p>As a simple example, suppose σ 2 A = σ 2 B = 0 and ω 2 = 1/9, following the conditions described in Section 4.2.Suppose we wish to detect an absolute difference of δ = 0.03 at least 80% of the time (β = 0.20) with a false-positive rate of 5% (α = 0.05).Then the eval will need to contain at least n = (z 0.025 + z 0.20 ) 2 (1/9)/(0.03) 2 ≈ 969 independent questions.Although these parameters are fictional, they are reasonable, and suggest that new evals should contain at least 1,000 questions in order to have good signaling ability.</p>
<p>If the number of questions is fixed, and the practitioner wishes to know the Minimum Detectable Effect associated with n, Equation 9 is easily inverted as
δ = (z α/2 + z B ) (ω 2 + σ 2 A /K A + σ 2 B /K B )/n(10)
The above equation may be used, for instance, to predict the effect of increasing the perquestion sample counts K A and K B on the Minimum Detectable Effect in a nondeterministic eval.Suppose that σ 2 A = σ 2 B = 1/6 and ω 2 = 1/9, following the conditions of Section 3.1 with an additional assumption that Corr(x A , x B ) = 0.5.Suppose n = 198, α = 0.05, and β = 0.20.It follows from Equation 10 that increasing K A = K B from 1 to 10 reduces the Minimum Detectable Effect from 13.2% to 7.5%.</p>
<p>Cluster-adjusted versions of Equations 9 and 10 are included in Appendix C.</p>
<p>Conclusion</p>
<p>This article has presented a statistical treatment of language model evaluations, drawing heavily from existing literature in experiment design.</p>
<p>For single-model analysis, we presented analytic formulas for naive and clustered standard errors, and for two-model analysis, we presented formulas for unpaired, paired, and pairedand-clustered standard errors.We recommended several techniques for reducing the variance of estimates, including resampling answers, analyzing next-token probabilities, and computing question-level differences between models, and advised against adjusting the sampling temperature for the sake of variance reduction.We suggest that practitioners include standard errors of their eval scores in their technical reports, and also include pairwise differences, pairwise standard errors, and score correlations when multiple models are being compared.We presented a sample-size formula so that model evaluators can determine in advance the size of difference that may be reliably detected between two models on a given eval using a given resampling strategy.</p>
<p>Experiment design represents a large and venerable literature.We hope that with proper statistical tools, such as those presented in this article, machine learning practitioners will think of their model evaluations as informative experiments rather than a series of contests to produce the largest number.We encourage researchers to continue exploring statistical techniques found in other experimental fields in order to further enrich our shared understanding of language models and their capabilities.These clustered versions of ω 2 , σ 2 A , and σ 2 B can be plugged into Equations 9 and 10 without further modification.</p>
<p>In practice, in both the clustered and non-clustered cases, the mean conditional variance and variance of conditional means will need to be estimated from previous data having K ≫ 1.For the sake of completeness, we briefly walk through this estimation process.</p>
<p>Let Note that we divide by K − 1 instead of K in order to obtain a consistent variance estimator with small K.</p>
<p>If a subsample of questions is being used for variance estimation, we recommend sampling at the cluster level (i.e.drawing clusters in their entirety) in order to capture the intra-cluster variance structure.</p>
<p>Var(μ A−B,unpaired ) = (Var(s A ) + Var(s B ))/n and the paired case Var(μ A−B,paired ) = (Var(s A ) + Var(s B ) − 2 Cov(s A , s B ))/n Combining, and recognizing that the cross-model residuals are uncorrelated, we have Var(μ A−B,paired ) = Var(μ A−B,unpaired ) − 2 Cov(x A , x B )/n</p>
<p>ϵ B,i,c ϵ B,j,c</p>
<p>s M,i,c,k represent the kth score on the ith question within the cth cluster on model M and estimate xM,i,c = 1 K K k=1 s M,i,c,k .This estimate is sufficient to estimate ω2 clustered .The clustered mean conditional variance on model M may then be estimated as σ2 M,clustered = 1 n(K − 1) k c i j (s M,i,c,k − xM,i,c )(s M,j,c,k − xM,j,c )</p>
<p>Table 1 :
1
. Hypothetical data from two fictional models across three (non-fictional) evals
Eval \ Model"Galleon""Dreadnought" DifferenceMATH65.5%63.0%+2.5%HumanEval83.6%87.7%−3.1%MGSM75.3%78.0%−2.7%</p>
<p>Table 3 :
3
We suggest including the cluster count alongside the question count when reporting cluster-adjusted standard errors (fictional models and numbers).</p>
<h1>Questions # Clusters"Galleon""Dreadnought"DROP9,62258887.1 (0.8)83.1 (0.9)RACE-H3,4981,04591.5% (0.5%)82.9% (0.7%)MGSM2,50025075.3% (1.6%)78.0% (1.5%)SE clusteredSE C.L.T.RatioDROP(1.34)(0.44)3.05RACE-H(0.51%)(0.46%)1.10MGSM(1.62%)(0.86%)1.88</h1>
<p>Table 4 :
4
Clustered and naive standard errors computed on two popular evals using Anthropic models (non-fictional numbers).Analyzing the same data, clustered standard errors can be over 3X larger than naive standard errors.</p>
<p>A Clustered standard errorsWe approach the problem with linear regression.Let s i,c denote the ith of n c question scores within cluster c, decomposed into a mean and random component asLet δ i,c = x i,c − µ represent the deviation of the conditional (question-level) mean from the true mean (that is, the hypothetical mean across all questions and clusters).Then the regression can be specified aswhere ϵ i,c is a random component and δ i,c acts as a question-level fixed effect that is not separately estimated.We continue to estimate μ = s and denote the regression residual u i,c = s i,c − s.The traditional clustered standard error formula iswhere X represents the full matrix of covariates, X c represents the covariates within cluster c, and Ω c represents the residual covariance matrix within cluster c.In our application, X = 1 n (a vector of n 1s), X c = 1 nc (a vector of n c 1s), andRecognizing that the first term is equal to the unclustered variance, we can writeThe two-sample version can be developed by analyzing the question-level score differences rather than the scores.B Sample-size formula derivationFollowing[14], we set up the power analysis with a hypothetical measurement sA−B that will trigger a Type I error with probability α and a Type II error rate with probability β.The z-scores on such a measurement under the null hypothesis and the alternative hypothesis are/n Combining to eliminate sA−B , we have an expression for the MDE in terms of the other variablesOr inverting the equation, we have a sample-size formula for the number of questions n required to produce a desired MDEC Cluster-adjusted sample-size formulaIn order to account for clustered questions, the sample-size formula requires cluster-adjusted versions of ω 2 , σ 2 A , and σ 2 B .In this section we develop formulas for estimating these quantities from previous eval data.Start with the clustered score variance estimatorwe can decompose s into x and ϵ,which, dropping cross-terms which are zero in expectation, will reduce toWe can denote the three terms on the right-hand side as
When should you adjust standard errors for clustering. Alberto Abadie, 10.1093/qje/qjac038The Quarterly Journal of Economics. 0033-55331381Oct. 2022</p>
<p>With Little Power Comes Great Responsibility. Dallas Card, arXiv:2010.06595[cs.CL].url2020</p>
<p>Evaluating Large Language Models Trained on Code. Mark Chen, arXiv:2107.03374[cs.LG].url2021</p>
<p>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. Wei-Lin Chiang, arXiv:2403.04132[cs.AI]2024</p>
<p>Question Answering in Context. Eunsol Choi, arXiv:1808.07036[cs.CL].url2018</p>
<p>DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. Dheeru Dua, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>The Llama 3 Herd of Models. Abhimanyu Dubey, arXiv:2407.21783[cs.AI].url2024</p>
<p>Challenges in evaluating AI systems. Deep Ganguli, Oct. 4, 2023</p>
<p>Measuring Mathematical Problem Solving With the MATH Dataset. Dan Hendrycks, arXiv:2103.03874[cs.LG].url2021</p>
<p>Distilling the Knowledge in a Neural Network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.025312015stat.ML</p>
<p>Guido W Imbens, Donald B Rubin, Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press2015</p>
<p>Inspect: An open-source framework for large language model evaluations. </p>
<p>RACE: Large-scale ReAding Comprehension Dataset From Examinations. Guokun Lai, 10.18653/v1/D17-1082Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Martha Palmer, Rebecca Hwa, Sebastian Riedel, Copenhagen, the 2017 Conference on Empirical Methods in Natural Language ProcessingDenmarkAssociation for Computational LinguisticsSept. 2017</p>
<p>So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design. Sally John A List, Mathis Sadoff, Wagner, 10.3386/w15701National Bureau of Economic Research. Jan. 2010Working Paper 15701</p>
<p>Quantifying Variance in Evaluation Benchmarks. Lovish Madaan, arXiv:2406.10229[cs.LG].url2024</p>
<p>Know What You Don't Know: Unanswerable Questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.03822[cs.CL].url2018</p>
<p>Language Models are Multilingual Chain-of-Thought Reasoners. Freda Shi, arXiv:2210.03057[cs.CL].url2022</p>            </div>
        </div>

    </div>
</body>
</html>
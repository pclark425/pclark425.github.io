<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8597 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8597</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8597</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-268987468</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.04237v1.pdf" target="_blank">Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their sophisticated reasoning to navigate complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two cornerstones of human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8597.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8597.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GroundCocoa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GroundCocoa (compositional & conditional flight-booking grounding benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lexically diverse benchmark introduced in this paper that evaluates conditional and compositional reasoning by asking models to match complex natural-language user flight requirements to one correct flight option among five choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GroundCocoa</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A grounding multiple-choice benchmark that requires conditional (if-then) and compositional reasoning over a Product-of-Sums logical form constructed from flight-slot primitives (price, ticket class, layovers, times, carbon emission, etc.). Each question has 1 positive and 4 negative options; samples vary controllably in conditional/compositional complexity and include atypical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Data-generation pipeline (POS expressions via SymPy), rule-based primitive generation, LLM paraphrasing for naturalization; evaluation with standard and Chain-of-Thought (CoT) prompting (CoT-full and CoT-partial).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across evaluated models accuracy ranged from slightly above random to ~67% on five-option multiple-choice; best model (GPT-4 Turbo) reached about 67% overall accuracy; atypical queries reduced GPT-4 Turbo accuracy by up to ~6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared models include open-source and closed-source LLMs (LLAMA 2-chat, Mixtral 8x7B, Mistral 7B, Gemini Pro, GPT-4 Turbo); CoT prompting gave mixed improvements (CoT-partial sometimes outperformed CoT-full).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Benchmark highlights failures in conditional reasoning even at lower complexity; high entropy in option-primitive satisfaction correlates with wrong predictions; CoT-full can harm performance due to long context confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GroundCocoa exposes that modern LLMs (including GPT-4 Turbo) struggle with grounding conditional/compositional constraints in realistic tasks; partial CoT (fewer exemplars/explanations) can help, and entropy over primitive satisfaction is a useful explanatory metric.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8597.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art closed-source large language model used as the strongest baseline in this paper's experiments; applied to GroundCocoa with different prompting strategies including chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source, high-performing LLM from OpenAI; used for paraphrasing during dataset creation and as an evaluator on GroundCocoa with various prompting (zero-shot, CoT-full, CoT-partial).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GroundCocoa</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GroundCocoa entry: conditional & compositional grounding for flight-booking multiple-choice matching.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with direct prompting and with in-context Chain-of-Thought (CoT) exemplars; two CoT strategies tested: CoT-full (5 explanations for 5 options) and CoT-partial (2 options/explanations). Also used as paraphraser during data creation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Best-performing model on GroundCocoa: approximately 67% accuracy on five-choice questions (did not exceed 67% despite advanced prompting). Atypical queries caused up to ~6% accuracy drop.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed other tested models by a substantial margin; CoT-partial produced better results than CoT-full for GPT-4 Turbo in experiments; CoT led to only modest gains in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant failures on conditional reasoning cases; performance declines with increased compositional/conditional complexity and with higher entropy in option primitives; CoT-full sometimes degraded performance due to long-context confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Although strongest, GPT-4 Turbo still shows major shortcomings on conditional/compositional grounding; careful CoT design (partial exemplars) and attention to context length are important; pretraining bias reduces robustness to atypical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8597.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercially-developed multimodal / large language model evaluated on GroundCocoa and compared against other proprietary and open models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closely referenced closed-source model from the Gemini family, evaluated on GroundCocoa with direct prompting and CoT variants as described for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GroundCocoa</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GroundCocoa entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with zero-shot and chain-of-thought prompting (CoT-full/CoT-partial where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Substantially lower than GPT-4 Turbo; overall accuracy not specified numerically in text but part of a wide performance spread ranging from near-random to ~67%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared in-paper to GPT-4 Turbo (which outperforms it by a large margin) and to smaller open models (which often perform worse).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles with conditional reasoning and compositional complexity similar to other models; bucked by atypical queries and high-entropy answer choices.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Performance gap between Gemini Pro and GPT-4 Turbo underscores sensitivity of grounding conditional/compositional tasks to model capabilities; CoT offers limited and inconsistent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8597.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral 8x7B - Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruct-tuned mixture-of-experts style model (reported as Mixtral 8x7B in paper) evaluated on GroundCocoa as an open/accessible baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral 8x7B - Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open/academic-style instruct-tuned model (Mixtral) reported with an 8x7B configuration in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GroundCocoa</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GroundCocoa entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with direct prompting and (where applicable) CoT variants; context length constraints noted for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Lower than GPT-4 Turbo; performance lies within the lower end of the paper's reported spectrum (overall range described as slightly above random to ~67%). Exact numeric accuracy not reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms GPT-4 Turbo; compared across a set that includes LLaMA 2-chat and Mistral 7B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails on complex conditional compositions and atypical user constraints; shows decline with increasing dependency graph size and entropy in correct options.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Mixtral's results exemplify that smaller / open models struggle more on conditional/compositional grounding; scaling and/or improved prompting yields only limited gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8597.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter instruct-tuned model included among the open models evaluated on GroundCocoa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruct-tuned 7B parameter model evaluated on GroundCocoa in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GroundCocoa</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GroundCocoa entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with zero-shot and CoT prompting variants where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as substantially worse than GPT-4 Turbo and generally near the lower end of overall model performance (text reports a wide spread but exact accuracy per-model not listed in body).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared unfavorably to larger/closed models like GPT-4 Turbo; performance degrades as compositional/conditional complexity increases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Poor handling of conditional dependencies, difficulty when dependency graph LCC or max degree increases; sensitive to high entropy answer choices and atypical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Demonstrates that moderate-size instruct-tuned models struggle on realistic conditional/compositional grounding tasks; points to limits of scale and prompting alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8597.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA 2-chat 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2-chat (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter chat-oriented LLaMA 2 model evaluated on GroundCocoa where context-length limitations prevented some CoT settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2-chat 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation chat model (70B variant) included in experiments; CoT-full could not be run due to 4096-token context length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GroundCocoa</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See GroundCocoa entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with direct prompting and CoT-partial where possible; CoT-full was not run due to context length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Part of the lower-to-middle range of evaluated models; specific numeric results not reproduced in main text but described as notably underperforming GPT-4 Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-4 Turbo (which substantially outperformed it) and to smaller open-source models (performance order varied).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Constrained by context length in CoT-full experiments; exhibits difficulty with conditional reasoning and compositional breadth (LCC/degree) increases.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Context length and exemplar design materially affect CoT utility; large-parameter open models still lag behind state-of-the-art closed models on conditional grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8597.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RuleTaker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RuleTaker (transformer-based natural language logic benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed benchmark that evaluates deductive logical reasoning by asking models to label implications entailed by a set of natural-language premises (linear deductive chains).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language deductive reasoning benchmark focusing on linear deductive chains and assignment of binary labels to candidate implications given premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes that these benchmarks focus on deductive reasoning with linear chains and differ from GroundCocoa's branching conditional reasoning and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Used in related work as contrast: RuleTaker focuses on deductive chains, whereas GroundCocoa emphasizes conditional branching and compositional grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8597.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProofWriter (deductive reasoning dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset that augments rule-based NLI-style evaluation with intermediate conclusions and proof generation to evaluate deductive reasoning depth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Proofwriter: Generating implications, proofs, and abductive statements over natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluates deductive reasoning through entailment and proof generation from natural-language facts and rules, emphasizing linear deductive chains.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Mentioned as related work focusing on linear deductive reasoning, unlike GroundCocoa's multiple branching conditional paths.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Provides a contrast point: GroundCocoa extends evaluation toward higher reasoning width and grounding realism absent in ProofWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8597.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicNLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicNLI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic benchmark covering several fundamental logical forms (conjunction, disjunction, negation, implication, quantifiers, etc.) to test logical entailment in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diagnosing the first-order logical reasoning ability through LogicNLI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicNLI</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Diagnostic NLI-style benchmark covering seven fundamental logical constructs including paradox cases; focuses on first-order logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Mentioned as primarily deductive and limited in branching condition complexity compared to GroundCocoa.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GroundCocoa complements LogicNLI by emphasizing conditional compositionality and grounding to a realistic schema.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8597.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A first-order logic reasoning dataset with larger vocabulary due to hybrid annotation approach, used to evaluate natural-language logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Folio: Natural language reasoning with first-order logic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>First-order-logic-focused dataset with broad vocabulary and hybrid annotations evaluating logical reasoning in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Referenced as deductive, with simpler reasoning width compared to GroundCocoa's conditional branching.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Cited to situate GroundCocoa relative to first-order logic benchmarks; GroundCocoa emphasizes grounding and conditional multiplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8597.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8597.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mentioned first-order logic benchmark designed with a linear ontology to avoid pretraining shortcuts and evaluate first-order reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>First-order logic benchmark using a fictional/linear ontology to reduce pretraining shortcut exploitation; focuses on deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Mentioned as linear-deduction focused and thus different in structure from GroundCocoa's branching conditional problems.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Serves as related-work contrast: GroundCocoa adds branching conditionality and grounding challenges not captured in ProntoQA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Proofwriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Diagnosing the first-order logical reasoning ability through LogicNLI <em>(Rating: 2)</em></li>
                <li>Folio: Natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8597",
    "paper_id": "paper-268987468",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GroundCocoa",
            "name_full": "GroundCocoa (compositional & conditional flight-booking grounding benchmark)",
            "brief_description": "A lexically diverse benchmark introduced in this paper that evaluates conditional and compositional reasoning by asking models to match complex natural-language user flight requirements to one correct flight option among five choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "GroundCocoa",
            "reasoning_task_description": "A grounding multiple-choice benchmark that requires conditional (if-then) and compositional reasoning over a Product-of-Sums logical form constructed from flight-slot primitives (price, ticket class, layovers, times, carbon emission, etc.). Each question has 1 positive and 4 negative options; samples vary controllably in conditional/compositional complexity and include atypical constraints.",
            "method_or_approach": "Data-generation pipeline (POS expressions via SymPy), rule-based primitive generation, LLM paraphrasing for naturalization; evaluation with standard and Chain-of-Thought (CoT) prompting (CoT-full and CoT-partial).",
            "performance": "Across evaluated models accuracy ranged from slightly above random to ~67% on five-option multiple-choice; best model (GPT-4 Turbo) reached about 67% overall accuracy; atypical queries reduced GPT-4 Turbo accuracy by up to ~6%.",
            "baseline_comparison": "Compared models include open-source and closed-source LLMs (LLAMA 2-chat, Mixtral 8x7B, Mistral 7B, Gemini Pro, GPT-4 Turbo); CoT prompting gave mixed improvements (CoT-partial sometimes outperformed CoT-full).",
            "limitations_or_failures": "Benchmark highlights failures in conditional reasoning even at lower complexity; high entropy in option-primitive satisfaction correlates with wrong predictions; CoT-full can harm performance due to long context confusion.",
            "insights_or_conclusions": "GroundCocoa exposes that modern LLMs (including GPT-4 Turbo) struggle with grounding conditional/compositional constraints in realistic tasks; partial CoT (fewer exemplars/explanations) can help, and entropy over primitive satisfaction is a useful explanatory metric.",
            "uuid": "e8597.0",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4 Turbo",
            "name_full": "GPT-4 Turbo (OpenAI)",
            "brief_description": "A state-of-the-art closed-source large language model used as the strongest baseline in this paper's experiments; applied to GroundCocoa with different prompting strategies including chain-of-thought.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_description": "Closed-source, high-performing LLM from OpenAI; used for paraphrasing during dataset creation and as an evaluator on GroundCocoa with various prompting (zero-shot, CoT-full, CoT-partial).",
            "model_size": null,
            "reasoning_task_name": "GroundCocoa",
            "reasoning_task_description": "See GroundCocoa entry: conditional & compositional grounding for flight-booking multiple-choice matching.",
            "method_or_approach": "Evaluated with direct prompting and with in-context Chain-of-Thought (CoT) exemplars; two CoT strategies tested: CoT-full (5 explanations for 5 options) and CoT-partial (2 options/explanations). Also used as paraphraser during data creation.",
            "performance": "Best-performing model on GroundCocoa: approximately 67% accuracy on five-choice questions (did not exceed 67% despite advanced prompting). Atypical queries caused up to ~6% accuracy drop.",
            "baseline_comparison": "Outperformed other tested models by a substantial margin; CoT-partial produced better results than CoT-full for GPT-4 Turbo in experiments; CoT led to only modest gains in some cases.",
            "limitations_or_failures": "Significant failures on conditional reasoning cases; performance declines with increased compositional/conditional complexity and with higher entropy in option primitives; CoT-full sometimes degraded performance due to long-context confusion.",
            "insights_or_conclusions": "Although strongest, GPT-4 Turbo still shows major shortcomings on conditional/compositional grounding; careful CoT design (partial exemplars) and attention to context length are important; pretraining bias reduces robustness to atypical constraints.",
            "uuid": "e8597.1",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Gemini Pro",
            "name_full": "Gemini Pro",
            "brief_description": "A commercially-developed multimodal / large language model evaluated on GroundCocoa and compared against other proprietary and open models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini Pro",
            "model_description": "Closely referenced closed-source model from the Gemini family, evaluated on GroundCocoa with direct prompting and CoT variants as described for other models.",
            "model_size": null,
            "reasoning_task_name": "GroundCocoa",
            "reasoning_task_description": "See GroundCocoa entry.",
            "method_or_approach": "Evaluated with zero-shot and chain-of-thought prompting (CoT-full/CoT-partial where applicable).",
            "performance": "Substantially lower than GPT-4 Turbo; overall accuracy not specified numerically in text but part of a wide performance spread ranging from near-random to ~67%.",
            "baseline_comparison": "Compared in-paper to GPT-4 Turbo (which outperforms it by a large margin) and to smaller open models (which often perform worse).",
            "limitations_or_failures": "Struggles with conditional reasoning and compositional complexity similar to other models; bucked by atypical queries and high-entropy answer choices.",
            "insights_or_conclusions": "Performance gap between Gemini Pro and GPT-4 Turbo underscores sensitivity of grounding conditional/compositional tasks to model capabilities; CoT offers limited and inconsistent improvements.",
            "uuid": "e8597.2",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Mixtral 8x7B",
            "name_full": "Mixtral 8x7B - Instruct",
            "brief_description": "An instruct-tuned mixture-of-experts style model (reported as Mixtral 8x7B in paper) evaluated on GroundCocoa as an open/accessible baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral 8x7B - Instruct",
            "model_description": "Open/academic-style instruct-tuned model (Mixtral) reported with an 8x7B configuration in the paper's experiments.",
            "model_size": "8x7B",
            "reasoning_task_name": "GroundCocoa",
            "reasoning_task_description": "See GroundCocoa entry.",
            "method_or_approach": "Evaluated with direct prompting and (where applicable) CoT variants; context length constraints noted for some models.",
            "performance": "Lower than GPT-4 Turbo; performance lies within the lower end of the paper's reported spectrum (overall range described as slightly above random to ~67%). Exact numeric accuracy not reported in main text.",
            "baseline_comparison": "Underperforms GPT-4 Turbo; compared across a set that includes LLaMA 2-chat and Mistral 7B.",
            "limitations_or_failures": "Fails on complex conditional compositions and atypical user constraints; shows decline with increasing dependency graph size and entropy in correct options.",
            "insights_or_conclusions": "Mixtral's results exemplify that smaller / open models struggle more on conditional/compositional grounding; scaling and/or improved prompting yields only limited gains.",
            "uuid": "e8597.3",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Mistral 7B",
            "name_full": "Mistral 7B Instruct",
            "brief_description": "A 7B-parameter instruct-tuned model included among the open models evaluated on GroundCocoa.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral 7B Instruct",
            "model_description": "Instruct-tuned 7B parameter model evaluated on GroundCocoa in the paper.",
            "model_size": "7B",
            "reasoning_task_name": "GroundCocoa",
            "reasoning_task_description": "See GroundCocoa entry.",
            "method_or_approach": "Evaluated with zero-shot and CoT prompting variants where applicable.",
            "performance": "Reported as substantially worse than GPT-4 Turbo and generally near the lower end of overall model performance (text reports a wide spread but exact accuracy per-model not listed in body).",
            "baseline_comparison": "Compared unfavorably to larger/closed models like GPT-4 Turbo; performance degrades as compositional/conditional complexity increases.",
            "limitations_or_failures": "Poor handling of conditional dependencies, difficulty when dependency graph LCC or max degree increases; sensitive to high entropy answer choices and atypical constraints.",
            "insights_or_conclusions": "Demonstrates that moderate-size instruct-tuned models struggle on realistic conditional/compositional grounding tasks; points to limits of scale and prompting alone.",
            "uuid": "e8597.4",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaMA 2-chat 70B",
            "name_full": "LLaMA 2-chat (70B)",
            "brief_description": "A 70B-parameter chat-oriented LLaMA 2 model evaluated on GroundCocoa where context-length limitations prevented some CoT settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA 2-chat 70B",
            "model_description": "Open foundation chat model (70B variant) included in experiments; CoT-full could not be run due to 4096-token context length constraints.",
            "model_size": "70B",
            "reasoning_task_name": "GroundCocoa",
            "reasoning_task_description": "See GroundCocoa entry.",
            "method_or_approach": "Evaluated with direct prompting and CoT-partial where possible; CoT-full was not run due to context length limits.",
            "performance": "Part of the lower-to-middle range of evaluated models; specific numeric results not reproduced in main text but described as notably underperforming GPT-4 Turbo.",
            "baseline_comparison": "Compared to GPT-4 Turbo (which substantially outperformed it) and to smaller open-source models (performance order varied).",
            "limitations_or_failures": "Constrained by context length in CoT-full experiments; exhibits difficulty with conditional reasoning and compositional breadth (LCC/degree) increases.",
            "insights_or_conclusions": "Context length and exemplar design materially affect CoT utility; large-parameter open models still lag behind state-of-the-art closed models on conditional grounding.",
            "uuid": "e8597.5",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RuleTaker",
            "name_full": "RuleTaker (transformer-based natural language logic benchmark)",
            "brief_description": "A previously proposed benchmark that evaluates deductive logical reasoning by asking models to label implications entailed by a set of natural-language premises (linear deductive chains).",
            "citation_title": "Transformers as soft reasoners over language",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "RuleTaker",
            "reasoning_task_description": "Natural-language deductive reasoning benchmark focusing on linear deductive chains and assignment of binary labels to candidate implications given premises.",
            "method_or_approach": null,
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Paper notes that these benchmarks focus on deductive reasoning with linear chains and differ from GroundCocoa's branching conditional reasoning and grounding.",
            "insights_or_conclusions": "Used in related work as contrast: RuleTaker focuses on deductive chains, whereas GroundCocoa emphasizes conditional branching and compositional grounding.",
            "uuid": "e8597.6",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ProofWriter",
            "name_full": "ProofWriter (deductive reasoning dataset)",
            "brief_description": "A dataset that augments rule-based NLI-style evaluation with intermediate conclusions and proof generation to evaluate deductive reasoning depth.",
            "citation_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "ProofWriter",
            "reasoning_task_description": "Evaluates deductive reasoning through entailment and proof generation from natural-language facts and rules, emphasizing linear deductive chains.",
            "method_or_approach": null,
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Mentioned as related work focusing on linear deductive reasoning, unlike GroundCocoa's multiple branching conditional paths.",
            "insights_or_conclusions": "Provides a contrast point: GroundCocoa extends evaluation toward higher reasoning width and grounding realism absent in ProofWriter.",
            "uuid": "e8597.7",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LogicNLI",
            "name_full": "LogicNLI",
            "brief_description": "A diagnostic benchmark covering several fundamental logical forms (conjunction, disjunction, negation, implication, quantifiers, etc.) to test logical entailment in natural language.",
            "citation_title": "Diagnosing the first-order logical reasoning ability through LogicNLI",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "LogicNLI",
            "reasoning_task_description": "Diagnostic NLI-style benchmark covering seven fundamental logical constructs including paradox cases; focuses on first-order logical reasoning.",
            "method_or_approach": null,
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Mentioned as primarily deductive and limited in branching condition complexity compared to GroundCocoa.",
            "insights_or_conclusions": "GroundCocoa complements LogicNLI by emphasizing conditional compositionality and grounding to a realistic schema.",
            "uuid": "e8597.8",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "FOLIO",
            "name_full": "FOLIO",
            "brief_description": "A first-order logic reasoning dataset with larger vocabulary due to hybrid annotation approach, used to evaluate natural-language logical reasoning.",
            "citation_title": "Folio: Natural language reasoning with first-order logic",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "FOLIO",
            "reasoning_task_description": "First-order-logic-focused dataset with broad vocabulary and hybrid annotations evaluating logical reasoning in natural language.",
            "method_or_approach": null,
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Referenced as deductive, with simpler reasoning width compared to GroundCocoa's conditional branching.",
            "insights_or_conclusions": "Cited to situate GroundCocoa relative to first-order logic benchmarks; GroundCocoa emphasizes grounding and conditional multiplicity.",
            "uuid": "e8597.9",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ProntoQA",
            "name_full": "ProntoQA",
            "brief_description": "A mentioned first-order logic benchmark designed with a linear ontology to avoid pretraining shortcuts and evaluate first-order reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "ProntoQA",
            "reasoning_task_description": "First-order logic benchmark using a fictional/linear ontology to reduce pretraining shortcut exploitation; focuses on deductive reasoning.",
            "method_or_approach": null,
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Mentioned as linear-deduction focused and thus different in structure from GroundCocoa's branching conditional problems.",
            "insights_or_conclusions": "Serves as related-work contrast: GroundCocoa adds branching conditionality and grounding challenges not captured in ProntoQA.",
            "uuid": "e8597.10",
            "source_info": {
                "paper_title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Diagnosing the first-order logical reasoning ability through LogicNLI",
            "rating": 2,
            "sanitized_title": "diagnosing_the_firstorder_logical_reasoning_ability_through_logicnli"
        },
        {
            "paper_title": "Folio: Natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.015543499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Cleared for Takeoff? Compositional &amp; Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents
5 Apr 2024</p>
<p>Harsh Kohli 
Department of Computer Science and Engineering
The Ohio State University
43201ColumbusOHUSA</p>
<p>Huan Sun 
Department of Computer Science and Engineering
The Ohio State University
43201ColumbusOHUSA</p>
<p>Cleared for Takeoff? Compositional &amp; Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents
5 Apr 20247CB8361FA791A42D0E180B189E527B4AarXiv:2404.04237v1[cs.CL]
The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks.This has enabled many downstream applications, such as LLM agents, to rely on their sophisticated reasoning to navigate complex task requirements.However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances -underscoring the need for better and more diverse evaluation setups to measure their true capabilities.To this end, we choose to study compositional and conditional reasoning, two cornerstones of human cognition, and introduce GroundCocoa -a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking.Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format.Results indicate a significant disparity in performance among current state-of-theart LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.</p>
<p>Introduction</p>
<p>Advanced reasoning in LLMs has significantly influenced and effected the rise to prominence of language agents adept at handling a diverse range of tasks (Shen et al., 2024;Wu et al., 2023;Schick et al., 2024;Significant Gravitas;Osika, 2023).Among them, web agents (Hong et al., 2023;Furuta et al., 2023;Deng et al., 2024;Zheng et al., 2024) have demonstrated potential in automating web-based tasks such as flight booking.While such success through widespread application of LLMs is indeed promising, it necessitates ongoing exploration of their core reasoning proficiencies.Conditional and compositional reasoning play a crucial role in our ability to understand and interact with complex systems through elaborate decision-making (Oaksford &amp; Chater, 2010;Simon &amp; Newell, 1971).Conditional reasoning involves the comprehension and application of logical rules that are typically structured in "if-then" formats.It is critical to personal decision-making in everyday life through an evaluation of potential scenarios and anticipation of consequences.Compositional reasoning, on the other hand, is the ability to combine solutions to simpler sub-problems, and integrate them in a structured manner to solve a more complex task.This cognitive process entails understanding the interplay between different sub-problems.Our paper focuses on assessing how well current LLMs encapsulate these essential cognitive functions, which are integral to both human intelligence and advanced artificial intelligence systems.To that end, we introduce GroundCocoa1 -a benchmark for evaluating compositional &amp; conditional reasoning in a grounding task.</p>
<p>GroundCocoa is made up of questions framed as user needs, set within a real-world inspired flight reservation scenario.User requirements might be many or could be highly convoluted -leading to higher compositional and conditional complexity respectively.We leverage a controllable method to create samples of varying complexity (examples are provided in Appendix B).Our data generation process, described in greater detail in Section 2, consists of a 5-stage pipeline including online scraping, constraint generation, and symbolic logic to impose conditionality.In order to test for robustness, we allow requirements to freely condition on one another and impose no restrictions on their nature.Additionally, we isolate a subset of more atypical queries that contain unconventional user needs (e.g., "I want at least 2 layovers") and evaluate their impact on model performance.Samples in GroundCocoa may also require reasoning about time (e.g. when constraints are imposed on arrival or departure times) and arithmetic (e.g. when constraints are imposed on ticket price), thus, integrating logical, temporal, mathematical, and compositional reasoning.</p>
<p>The statistics of our dataset are shown in Table 1.Our key findings are as follows:</p>
<ol>
<li>
<p>Accuracy among contemporary LLMs varies greatly, ranging from a little better than random guess to about 67% on a five-option multiple-choice question task.Within this spectrum, GPT-4 Turbo (OpenAI, 2023) stands out, demonstrating a superior capacity of the GPT line of models to adapt and excel in novel reasoning tasks.However, conditional reasoning poses a significant challenge to all evaluated models, even on samples of relatively lower complexity.</p>
</li>
<li>
<p>Chain of Thought (COT) prompting (Wei et al., 2022) leads to mixed results, with only a modest improvement in model performance in some cases.Prior research has noted that although COT helps decompose problems into steps, LLMs increasingly struggle as the complexity of the individual steps escalates (Hendrycks et al., 2021b;Madaan &amp; Yazdanbakhsh, 2022;Nogueira et al., 2021;Qian et al., 2023).These assertions hold true in our observations.</p>
</li>
<li>
<p>Including unconventional user requirements leads to a drop in accuracy of as much as 6% in GPT-4 Turbo, indicating a pretraining bias towards more typical needs.</p>
</li>
</ol>
<p>Approach</p>
<p>Figure 2 illustrates our proposed approach.In the process of generating a natural language user requirement for flight booking, we are faced with the following considerations:</p>
<p>Conditionality of Constraints:</p>
<p>We wish to challenge contemporary models in their ability to reason through scenarios characterized by conditional complexity.This is done through mutual dependence of flight features which we refer to as slots.In the example illustrated in Figure 2, there is an inter-dependence between the values for price and ticket class.The interplay between constraints corresponding to the different slots is represented in logical form through a Product-of-Sums (POS) expression.A POS expression consists of multiple OR operations (sums) which are later combined through AND operations (products).The inclusion of OR operations between slots introduces conditional complexity to our user requirement, necessitating consideration of potential slot values in if-then scenarios.On the other hand, a greater number of AND conditions implies a higher number of variables that a model has to simultaneously reason over resulting in increased compositional complexity.</p>
<p>Satisfiability of POS Expression:</p>
<p>While generating the logical form for a user requirement, we must ensure satisfiability of the generated POS expression.For this, we use SymPy (Meurer et al., 2017) -an open-source Python symbolic mathematics library which generates an optimal POS expression given a minterm table.This is further described in Section 2.2.</p>
<p>Fuzziness in Slot Values:</p>
<p>Corresponding to each occurrence of a slot in the POS expression there has to be a unique constraint.For the example in Figure 2, the two constraints on the price slot are {&lt;12400, &lt;500}.These constraints are randomly imposed through specialized rule-based systems corresponding to each slot.However, these might cause the final user criteria to become impossible to satisfy even if the corresponding POS expression in satisfiable.Thus, for a generated user requirement we check our flight data to ensure that there exists at least one route that satisfies the criteria and at least 4 that do not.This way we ensure there is at least one positive and 4 negative options for a generated requirement.</p>
<p>Statistics of GroundCocoa are shown in Table 1.We also include a separate validation set which may be used for tuning hyperparameters.Our 5-stage data creation pipeline is detailed in the subsequent sections.</p>
<p>Flight Data Collection</p>
<p>We use a list of the top 50 busiest airports by passenger traffic derived from Wikipedia2 .Source and destination airports are chosen randomly from this list and input to Google Flights3 with the departure date set for April 17, 2024.A small number of flights are sampled from search results for each source-destination pair.The sampled flights are chosen from each of economy, business, and first class and, for each flight option, all the relevant details such as the number of layovers, price, departure and arrival times etc. are saved.A sample flight schema with all the elements is provided in Appendix A. The entire data collection process is handled through web scraping using Selenium Webdriver4 .</p>
<p>Product-of-Sums Generation</p>
<p>To generate a POS expression, we first randomly select a small number of flight features or slots.The complete set of slots for any given flight is as follows: We vary the number of slots between 2 and 6 in order to generate samples of differing complexity.We then randomly generate 2-3 "minterms" -the list of all input combinations of slots that generate a "1".A higher number of minterms results in a greater conditional complexity and leads to more convoluted user requirements.The slot symbols and generated minterms are input to SymPy which uses a redundant-group eliminating algorithm to output the smallest POS expression consistent with the minterm table.</p>
<p>Primitive Generation</p>
<p>Corresponding to each slot, we have developed a rule-based system that randomly imposes constraints on its values.These constraints are converted to natural language through templates.Since a POS expression may contain a negation, we generate two primitives at each turn -one for the constraint and one for its negation.A sample primitive for total travel time is shown in Table 2.</p>
<p>TravelTime Travel Time should be more than 22 hours and 30 minutes.</p>
<p>TravelTime Travel Time should not be more than 22 hours and 30 minutes.</p>
<p>Table 2: Sample primitive generated from templates for total travel time.</p>
<p>At this stage, we also isolate samples that include any one of the following three primitives -(1) carbon emissions must be above the average for that route, (2) price of the flight must be above a minimum threshold, and (3) number of layovers on the route should be greater than a minimum.While this list is not exhaustive, such samples (henceforth referred to as "atypical" queries) are able to successfully encapsulate contrarian needs that are unlikely to manifest often during pretraining.</p>
<p>LLM Paraphrasing and Human Validation</p>
<p>We carry out LLM paraphrasing in two distinct steps described below.The exact prompts and an example of intermediate results are provided in Appendix D. We manually verify each query to ensure it is consistent with the primitives and make changes wherever necessary.</p>
<ol>
<li>Individual primitives are substituted into each sum term and combined using templated rules.We then use GPT-4 Turbo to paraphrase each of the sum terms.2. Next, we combine the individual sum terms into a product (logical AND).This is done by merging the individual paraphrases of sum terms, separated by periods.</li>
</ol>
<p>The resulting flight requirement is again paraphrased with GPT-4 Turbo.</p>
<p>Option Matching</p>
<p>We match the generated user requirements with the flight data collected in Section 2.1.Each route between the source and destination represents a potential choice in our multiple-choice dataset.Choices are divided into subsets containing one positive (matching the user requirement) and four negative (not matching the user requirement) options.This is done to ensure that each multiple-choice question has only a single correct answer for ease of evaluation.Many such subsets may be created from a single user requirement and, consequently, our dataset consists of queries repeated multiple times with differing choices.Details on the number of unique queries and overall samples is provided in Table 1.</p>
<p>Results &amp; Analysis</p>
<p>To measure performance on GroundCocoa, we test several models of different sizes ranging from open-source to closed-source, including LLAMA 2-chat (Touvron et al., 2023), Mixtral 8x7B -Instruct (Jiang et al., 2024) / Mistral 7B Instruct(Jiang et al., 2023), Gemini Pro (Team et al., 2023), and GPT-4 Turbo.We also test selected larger models with an in-context example using chain-of-thought (CoT) (Wei et al., 2022) reasoning.Since our task involves grounding user requirements to each answer choice, the CoT explanations are provided for each flight option given the user requirement.Thus, our standard CoT (CoT-full) consists of 5 distinct explanations.On GPT-4, we empirically observe that the large resulting context length can often prove detrimental to model performance with the models often confusing between the requirements and options of the test case and the exemplar.As alluded to previously, GroundCocoa presents a substantial challenge for each of the evaluated models, even with CoT prompting.The CoT-partial strategy with only 2 options and explanations leads to better results than CoT-full in 2 out of 3 cases where we have experimented with both, and best results are obtained using GPT-4 Turbo with CoT-partial.</p>
<p>It is noteworthy, though, that there exists a marked difference in performance between GPT-4 Turbo and the remaining models.Such a degree of variation represents a significant departure from the usual performance patterns observed in popular benchmarks such as MMLU (Hendrycks et al., 2021a), HellaSwag (Zellers et al., 2019), ARC Reasoning Challenge (Clark et al., 2018), WinoGrande (Sakaguchi et al., 2021), and GSM-8K (Cobbe et al., 2021) among others, where results are much more comparable.</p>
<p>Beyond assessing the overall model performance, we also investigate the consequences of varying the complexity of user criteria and presenting relatively unconventional user needs.</p>
<p>Impact of Increasing Complexity</p>
<p>In our analysis, we observe the performance of GPT-4 Turbo, the most effective model from among those tested on GroundCocoa across different levels of conditional and compositional complexity.In their recent work on assessing the limitations of transformer on compositional tasks, Dziri et al. ( 2023) use computational graphs as approximations of the underlying reasoning processes in such models.They define the terms reasoning depth, the length of the deepest layer in the computational graph from the source nodes, and reasoning width, the mode of number of nodes in each layer -indicating the extent of multi-hop reasoning and compositional parallelism required to solve a given problem.</p>
<p>Considering the characteristics of GroundCocoa we focus on reasoning width -the number of variables a model has to simultaneously reason over for a given problem.Intuitively, this may be represented by the number of slots used during the generation of a particular sample as described in Section 2.2.However, keeping the number of rows in the minterm table constant while increasing the slots may often lead to lower conditional complexity as the number of slots is increased.In order to effectively gauge the compositional and conditional complexity of a sample in our dataset, we define a dependency graph derived from the POS expression corresponding to that sample.Vertices represent slots and a dependency (edge) is created when a particular slot co-occurs with another slot within a sum term in the POS.A sample POS expression and its corresponding dependency graph are shown in Figure 3.The graph has 3 connected components with the largest connected component (LCC) of size 4.The maximum degree is 2 which corresponds to the two connections for nodes LayoverTime and TicketClass.Given a fixed-schema for the flight options, the number of sum terms in the POS expression as well as the LCC in the dependency graph are indicative of the reasoning width and, in turn, the compositional complexity of the user criteria.The LCC is the length of the largest chain of slots -the possible values of which are dependent on one another through OR conditions (represented by edges in the dependency graph).This metric effectively reflects the breadth of parallel computation or reasoning width required to accurately infer the given user criteria.Since increased branching in the dependency graph suggests a greater conditional complexity in user criteria, we also analyze model performance with increasing maximum degree of the dependency graph.This gives us the extent of conditioning on a single slot value.In Figure 4 we observe the decline in model performance with increased complexity as indicated by these factors.</p>
<p>Quantifying Confusion in Answer Choices through Entropy</p>
<p>Numerous recent studies have explored how deep learning models, specifically transformerbased architectures, achieve success by exploiting shortcuts (Geirhos et al., 2020;Liu et al., 2022;Tang et al., 2023;Du et al., 2023) and relying on spurious correlations present in the training data (Zhang et al., 2023;Saparov &amp; He, 2023;Saparov et al., 2023).Most recently, Dziri et al. (2023) utilized relative information gain of individual output elements in partially correct answers to explain surface pattern understanding in LLMs.In the same vein, we employ entropy as a metric to measure the confusion that might be caused due to conditions in the user query for a given flight option.We do this in an attempt to demystify how language models may succeed at some and fail at other queries with similar levels of complexity.In order to illustrate this, we take an example user requirement, and two hypothetical and simplified flight options as shown in Figure 5. Additionally, we show the reasoning path that must be navigated in each case for a successful outcome.We observe how option B in our example leads to a more convoluted reasoning path, whereas the model is able to bypass considerable conditional overhead in the case of Option A. For the purpose of quantifying this more generally, we observe the compositional primitives (values attached to individual slots in the POS expression) in each sample and attach a binary value indicating if the primitive is satisfied.For the example in Figure 5, we show the primitives and the corresponding values of both options in Figure 6.We also show the probability of a primitive being satisfied(p sat ) and being unsatisfied(p s at ) by the flight option under consideration, as well as the final entropy.</p>
<p>In our analysis, we take the entropy values of the correct answer choice for each sample.</p>
<p>Figure 7 shows the densities of entropy values for the correct and wrong predictions of GPT-4 Turbo on GroundCocoa.While correct predictions exceed wrong predictions at lower entropy values, an abrupt surge in wrong predictions is observed at higher entropy levels.Thus, entropy gives us yet another measure of conditional complexity from the perspective of the answer choices rather than just the query, and helps explain why a model might exhibit inconsistent results across user queries of similar complexity.</p>
<p>Prepreint.Under review.</p>
<p>Robustness to Unconventional User Needs</p>
<p>Several contemporary studies have sought to examine the robustness of language models by studying their resilience to out-of-distribution data (Koh et al., 2021;Wang et al., 2023) or through adversarial attacks and input perturbations (Gardner et al., 2020;Goel et al., 2021;Subhash et al., 2023;Sanyal et al., 2022;Yuan et al., 2023).In our work, we challenge models through atypical user requirements in order to assess bias from pretraining and robustness to unorthodox and nontraditional queries.We segregate queries into "Regular" and "Atypical" groups as described in Section 2.3.In Table 3, we contrast model performance on samples that describe such unconventional user needs versus those that do not.While most models in our testing show a decay in performance, the impact is more noticeable on better performing models such as GPT-4 Turbo.The in-context example used for all queries when testing with CoT includes two such primitives (ticket price &gt; 1800, carbon emission above average).We observe that the decline in performance is less pronounced with CoT.</p>
<p>Related Work</p>
<p>Reasoning Challenges in NLP.Our work extends the existing line of research on evaluating natural language processing (NLP) systems on different facets of reasoning -most notably commonsense question-answering (Talmor et al., 2019;Huang et al., 2019), physical reasoning (Bisk et al., 2020), social interaction (Sap et al., 2019), mathematical reasoning (Cobbe et al., 2021;Amini et al., 2019;Miao et al., 2020;Hendrycks et al., 2021b), story completion (Zellers et al., 2019), temporal reasoning (Zhou et al., 2019;Tan et al., 2023) abductive reasoning (Bhagavatula et al., 2020) and pronoun resolution (Sakaguchi et al., 2021) among others.Different from these benchmarks, GroundCocoa introduces a unique and substantial challenge for LLMs in the form of conditional and compositional reasoning.</p>
<p>Benchmarks on Propositional Logic.GroundCocoa also aligns with the considerable body of work on evaluating logical reasoning in language models.The RuleTaker (Clark et al., 2021) and ProofWriter (Tafjord et al., 2021) datasets proposed a modern approach to evaluating logical reasoning through a task involving assignment of binary labels to candidate implications following a set of premises expressed in natural language.The datasets emulate a linear deductive chain of reasoning of varying depths given a set of facts and rules, with ProofWriter augmenting this task through intermediate conclusions and proof generation.LogicNLI (Tian et al., 2021) provides a more comprehensive diagnostic benchmark involving reasoning through all seven fundamental logics (conjunction, disjunction, negation, implication, equation, universal and existential quantifiers).It contains an additional "paradox" label implying a situation where both the hypothesis as well as its negative proposition can be simultaneously entailed to the premise through different reasoning paths.This facilitates a non-linear reasoning, but is still limited to two contradictory reasoning paths.The FOLIO (Han et al., 2022) dataset boasts a higher vocabulary size due to a hybrid annotation approach but again consists of linear reasoning chains.Along similar lines, ProntoQA (Saparov &amp; He, 2022) proposes a first-order logic benchmark using a linear ontology which might be fictional.This is done to prevent LLMs from predicting correct outcomes through spurious correlations in their pretraining corpus.</p>
<p>The benchmarks described here are primarily focused on the evaluation of deductive reasoning.In contrast, GroundCocoa offers a more realistic grounding task with an emphasis on if-then reasoning which leads to many candidate reasoning paths for each answer choice.While deductive reasoning may involve a broader range of logical structures, conditional reasoning is a subset which deals specifically with the relationships and implications of conditional statements.Our dataset consists of a large vocabulary size and context length per sample, leading to greater linguistic diversity, and a higher reasoning width than other benchmarks in logical reasoning.Questions are designed to test for robustness against rare and unconventional user requirements and bring to the fore model bias from pretraining data.Also, unlike most other benchmarks, we do not attempt to evaluate logical reasoning in isolation -our task might require abilities such as temporal or mathematical reasoning.</p>
<p>Compositional Generalization.Samples in GroundCocoa consist of novel combination of primitives expressed as user requirements in a flight-booking task.Such reasoning falls under the umbrella of compositional generalization -an area that has garnered increasing interest in the scientific community recently.Hosseini et al. (2022) highlight the relative generalization gap with in-context learning between in-distribution and out-of-distribution samples in various semantic parsing tasks.Dziri et al. (2023) demonstrates how transformerbased LLMs may solve compositional tasks by reducing them to linearized subgraph matching.By establishing a computational graph for each problem, the authors are able to define computational complexity by metrics such as the reasoning depth and width which correspond to levels in multi-hop reasoning and average parallelism respectively.Unsurprisingly, increased task complexity leads to a rapid decay in model performance under various settings.</p>
<p>Our findings largely concur with previous literature on compositional reasoning.However, results on GroundCocoa reveal that even the most advanced LLMs struggle at relatively low levels of compositional complexity when juxtaposed with conditional reasoning and grounding.While Dziri et al. (2023) demonstrated their results using problems such as multi-digit multiplication, dynamic programming, and Einstein's puzzle -we release a new dataset that is anchored on a practical, real world use-case of parsing complex user criteria and grounding to a fixed schema representing a flight option.GroundCocoa contains a high semantic coverage and we posit that it would be of interest to the NLP community as a hard evaluation set to benchmark compositional generalization in LLMs.</p>
<p>Dialogue-State Tracking.Finally, while our task is reminiscent of a single turn in a dialogue state tracking system, it goes one step further to test a language model's grounding ability to match a flight schema with the user query.Most schema-guided dialogue datasets (Rastogi et al., 2020;Lee et al., 2022) consist of fixed slot values and the filtering of available options is handled through external systems (e.g.api's).Slot values in GroundCocoa are fuzzy due to conditional constraints on the primitives.In Figure 5, TicketPrice may take on different values based on TicketClass.GroundCocoa consists of examples with varying levels of compositional complexity due to long and complex user requirements.This differentiates it from the majority of schema-guided dialogue datasets where the primary objective is goal and slot identification, and tagging of slot values.These tasks, while challenging in their own respect, do not engage a models' compositional reasoning ability to the same extent.</p>
<p>Conclusion</p>
<p>Modern LLMs have demonstrated remarkable advancements in many tasks including those that are inherently compositional and necessitate conditional reasoning such as mathematical problem solving, and code generation and interpretation.However, discerning genuine reasoning from mere rote learning and shallow understanding continues to be a focal point of study.While LLMs have become exceedingly adept at answering questions of seemingly greater complexity, we show that they can struggle on the same skills when presented with an unfamiliar task setting.While problem size does have an impact, even the less complex samples in our dataset are challenging to the best language models today.</p>
<p>Beyond introducing a new benchmark dataset, we conduct a thorough analysis of the effects of increasing complexity, including advanced prompting techniques, and robustness to atypical queries.Our results uncover a substantial disparity in the performance of competing language models, a distinction that is not as pronounced in most other evaluation benchmarks and highlights their respective abilities in tackling novel challenges.Our data generation process is largely automatic, with human validation at the last step.In addition to the dataset and the evaluation script, we release code for the data generation which can be easily extended to generate more examples, and increase diversity (through different slots) as well as complexity.With minor modifications, the task can be further complicated by incorporating queries with multiple answers and questions that require other forms of logical reasoning such as aggregation (e.g., "Give me the cheapest flight matching my criteria?"),existential quantification (e.g., "Is there a first class seat under $5000?")etc., which we leave for future work.</p>
<p>A Sample Flight Schema</p>
<p>C Prompts used in Model Evaluation</p>
<p>Figure 1 :
1
Figure 1: Gemini-Pro and GPT-4 Turbo responses for a flight requirement with single option and a simplified schema.</p>
<p>Figure 2 :
2
Figure 2: Stepwise depiction of GroundCocoa query generation using 2 slots and 2 minterms.</p>
<p>Figure 3 :
3
Figure 3: An example of POS expression and its corresponding dependency graph.</p>
<p>Figure 4 :
4
Figure 4: Effect of increasing complexity in evaluation samples.</p>
<p>Figure 5 :
5
Figure 5: Sample user requirement and two hypothetical flight options.</p>
<p>Figure 6 :
6
Figure 6: Satisfaction of individual primitives and corresponding entropy.</p>
<p>Figure 7 :
7
Figure 7: Effect of increasing entropy in answer choice of evaluation samples.Entropy due to user criteria for each option can then be computed using the formula in Equation 1. Higher uncertainty leads to greater entropy in Option B as opposed to Option A, indicating a greater conditional overhead.</p>
<p>Figure 8 :
8
Figure 8: Schema for British Airways flight between Mexico City and Paris on 04/24/2024</p>
<p>Figure 15 :
15
Figure 15: Evaluation prompt without CoT.</p>
<p>Figure 16 :
16
Figure 16: Evaluation prompt using CoT-partial.</p>
<p airline_="airline," arrival="arrival" average="average" carbon="carbon" class_="class," date_="date," departure="departure" difference_="difference," emission="emission" layover="layover" layovers_="layovers," locations_="locations," number="number" of="of" price_="price," ticket="ticket" time_="time," times="times" total="total" travel="travel">Table 1 :
1
Key Statistics of Ground Cocoa.
Statistic(slot, minterm) configurationsTotal(2,2)(3,2)(4,2)(4, 3)(5,2)(6,2)Test Samples151110837107234513714849Test Unique Queries124136117129121101728Val. Samples1717852352Val. Unique Queries1111116Avg. Query Length65.04 88.33 103.88 119.14 124.56 148.8795.95Avg. Context Length------1252.27Vocab Size------4200</p>
<p>Table 3 :
3
Accuracy Results (in percentage) on GroundCocoa.
Regular Atypical Total
To address this, we try a different prompting strategy (CoT-partial) with only two flight choices (1 positive and 1 negative) for the in-context example.Due to limitations on context length (4096 tokens) we are unable to run LLAMA 2-chat 70B on CoT-full.The exact prompts are given in Appendix C. Results from our experiments are shown in Table3.</p>
<p>https://osu-nlp-group.github.io/GroundCocoa/
https://en.wikipedia.org/wiki/List_of_busiest_airports_by_passenger_traffic
https://www.google.com/travel/flights
https://www.selenium.dev/documentation/webdriver/
D End-to-End Generation Process
MathQA: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen Downey, Yejin Tau Yih, Choi, International Conference on Learning Representations. 2020</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems. 202436</p>
<p>Shortcut learning of large language models in natural language understanding. Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, Xia Hu, 10.1145/3596490Commun. ACM. 0001-0782671dec 2023</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Multimodal web navigation with instruction-finetuned foundation models. Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, Izzeddin Gur, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Evaluating models' local decision boundaries via contrast sets. Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Shortcut learning in deep neural networks. Robert Geirhos, J rn-Henrik, Claudio Jacobsen, Richard Michaelis, Wieland Zemel, Matthias Brendel, Felix A Bethge, Wichmann, Nature Machine Intelligence. 2112020</p>
<p>Robustness gym: Unifying the nlp evaluation landscape. Karan Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal, Christopher R, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies: Demonstrations2021</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, arXiv:2209.00840Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. Folio: Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021bNeurIPS</p>
<p>Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents. 2023</p>
<p>On the compositional generalization gap of in-context learning. Arian Hosseini, Ankit Vani, Dzmitry Bahdanau, Alessandro Sordoni, Aaron Courville, 10.18653/v1/2022.blackboxnlp-1.22Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. Jasmijn Bastings, Yonatan Belinkov, Yanai Elazar, Dieuwke Hupkes, Naomi Saphra, Sarah Wiegreffe, the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/D19-1243Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Wilds: A benchmark of in-the-wild distribution shifts. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, International Conference on Machine Learning. PMLR2021</p>
<p>Sgd-x: A benchmark for robust generalization in schema-guided dialogue systems. Harrison Lee, Raghav Gupta, Abhinav Rastogi, Yuan Cao, Bin Zhang, Yonghui Wu, The Eleventh International Conference on Learning Representations. Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang, 2022. 202236Proceedings of the AAAI Conference on Artificial Intelligence</p>
<p>Text and patterns: For effective chain of thought, it takes two to tango. Aman Madaan, Amir Yazdanbakhsh, arXiv:2209.076862022arXiv preprint</p>
<p>Sympy: symbolic computing in python. Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondej ertk, B Sergey, Matthew Kirpichev, Amit Rocklin, Sergiu Kumar, Jason K Ivanov, Sartaj Moore, Thilina Singh, Sean Rathnayake, Brian E Vig, Richard P Granger, Francesco Muller, Harsh Bonazzi, Shivam Gupta, Fredrik Vats, Fabian Johansson, Matthew J Pedregosa, Andy R Curry, tpn Terrel, Ashutosh Rouka, Isuru Saboo, Sumith Fernando, Robert Kulal, Anthony Cimrman, Scopatz, 10.7717/peerj-cs.103PeerJ Computer Science. 2376-59923e103January 2017</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin, arXiv:2102.130192021arXiv preprint</p>
<p>Cognition and conditionals: Probability and logic in human thought. Mike Oaksford, Nick Chater, 2010124383943</p>
<p>ArXiv, abs/2303.08774Gpt-4 technical report. 2023OpenAI</p>
<p>. Anton Osika, April 2023gpt-engineer</p>
<p>Limitations of language models in arithmetic and symbolic induction. Jing Qian, Hong Wang, Zekun Li, Shiyang Li, Xifeng Yan, 10.18653/v1/2023.acl-long.516012023</p>
<p>Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.1145/3474381Commun. ACM. 0001-0782649aug 2021</p>
<p>RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners. Soumya Sanyal, Zeyi Liao, Xiang Ren, 10.18653/v1/2022.emnlp-main.653Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Social IQa: Commonsense reasoning about social interactions. Maarten Sap, Derek Hannah Rashkin, Ronan Chen, Yejin Le Bras, Choi, 10.18653/v1/D19-1454Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Testing the general deductive reasoning capacity of large language models using OOD examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He, 10.48550/arXiv.2305.152692023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202436</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 202436</p>
<p>. Significant Gravitas, Autogpt, </p>
<p>Human problem solving: The state of the theory in 1970. Herbert A Simon, Allen Newell, American Psychologist. 261971</p>
<p>Why do universal adversarial attacks work on large language models?: Geometry might be the answer. Varshini Subhash, Anna Bialas, Weiwei Pan, Finale Doshi-Velez, The Second Workshop on New Frontiers in Adversarial Machine Learning. 2023</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Towards benchmarking and improving the temporal reasoning capability of large language models. Qingyu Tan, Hwee Tou Ng, Lidong Bing, 10.18653/v1/2023.acl-long.828Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Large language models can be lazy learners: Analyze shortcuts in in-context learning. Ruixiang Tang, Dehan Kong, Longtao Huang, Hui Xue, 10.18653/v1/2023.findings-acl.284012023</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Diagnosing the first-order logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, 10.18653/v1/2021.emnlp-main.303Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>On the robustness of chatgpt: An adversarial and out-of-distribution perspective. Jindong Wang, Wenxin Hu Xixu, Hao Hou, Runkai Chen, Yidong Zheng, Linyi Wang, Wei Yang, Haojun Ye, Xiubo Huang, Geng, ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.046712023arXiv preprint</p>
<p>Can pretrained language models (yet) reason deductively?. Zhangdie Yuan, Songbo Hu, Ivan Vuli, Anna Korhonen, Zaiqiao Meng, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>On the paradox of learning to reason from data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI). the 32nd International Joint Conference on Artificial Intelligence (IJCAI)aug 2023</p>
<p>Gpt-4v (ision) is a generalist web agent, if grounded. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su, arXiv:2401.016142024arXiv preprint</p>
<p>going on a vacation" takes longer than "going for a walk": A study of temporal commonsense understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
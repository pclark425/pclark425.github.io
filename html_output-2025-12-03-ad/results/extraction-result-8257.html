<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8257 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8257</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8257</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-258823296</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.11499v2.pdf" target="_blank">RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought</a></p>
                <p><strong>Paper Abstract:</strong> Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected factual inconsistency into fine-grained feedback to guide LLMs in revising solutions. Experimental results demonstrate improvements of RCoT over standard CoT, Self-Consistency and Self-Refine across seven arithmetic datasets. Moreover, we find that manually written fine-grained feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on GSM8K), encouraging the community to further explore the fine-grained feedback generation methods.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8257.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8257.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (closed-source OpenAI conversational LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source conversational large language model used as a backbone in experiments; evaluated with Chain-of-Thought prompting and with the paper's proposed RCOT procedure, and compared against multi-sample and refinement baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source conversational large language model (OpenAI). Treated as a high-capability base LLM in zero-shot and few-shot CoT settings with temperature set to 0 for reproducibility in many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT)', 'Reversing Chain-of-Thought (RCOT)', 'Self-Consistency (SC, majority voting over diverse sampled CoT traces)', 'Self-Refine (iterative self-feedback refinement)', 'Double-Check (coarse prompt encouraging re-check)', 'Active-Prompting (few-shot selection of uncertain examples)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Chain-of-Thought (CoT): prompting with "Let's think step by step" or few-shot CoT demonstrations to elicit intermediate reasoning steps. RCOT: the paper's proposed method where the model first reconstructs the original problem from its generated solution, decomposes both original and reconstructed problems into condition lists, performs fine-grained comparisons (condition-by-condition and question comparison) to detect hallucinations/overlookings/misinterpretations, then formulates fine-grained feedback to the model to revise the solution. Self-Consistency (SC): sampling many diverse CoT solutions and applying majority voting on final answers. Self-Refine: iterative self-feedback and revision (used as baseline). Double-Check: a coarse prompt instructing the model to re-check answers without specifying detected inconsistencies. Active-Prompting: selecting few-shot examples by uncertainty; used in few-shot comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Explicit comparisons and ablations: (1) compared standard single-trace CoT vs RCOT (single-trace with reconstruction+feedback); (2) compared diverse-solution majority-vote SC (30 sampled CoT traces) vs RCOT (1 or 3 trials) to examine trade-offs in accuracy and token/cost; (3) combined RCOT with SC to test complementary effects; (4) ablations on feedback granularity (fine-grained vs coarse-grained: 'w/o reasons' and 'w/o judgment+reasons'); (5) ablations on fine-grained comparison components (remove question comparison or condition comparison) and coarse-grained comparison instead of fine-grained.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Seven arithmetic reasoning datasets: GSM8K (grade-school math), AQuA (diverse/complex math multiple-choice), SVAMP (simple arithmetic), AddSub, ASDiv, Date (date arithmetic/multi-hop), SingleEq (single-equation problems); evaluation reported in zero-shot and few-shot CoT settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Highlights (zero-shot, ChatGPT): Standard CoT → GSM8K 79.0±0.95%, AQuA 51.3±0.68%, AddSub 85.2±1.26%, Date 66.7±1.49%, SingleEq 90.3±0.68%, ASDiv 84.3±0.47%, SVAMP 76.7±4.1%. +RCOT (zero-shot) → GSM8K 82.0±0.35% (+3.1), AQuA 55.5±0.88% (+4.1), AddSub 87.1±1.17% (+1.8), Date 71.7±1.39% (+5.0), SingleEq 91.4±0.88% (+1.1), ASDiv 86.0±0.37% (+1.7), SVAMP 79.6±4.1% (+2.8). Few-shot highlights: ChatGPT +RCOT (few-shot) GSM8K 84.6±0.65% (improving over few-shot baselines; Active-Prompting ~81.8%), AQuA +RCOT 57.1±0.38%. Comparison with Self-Consistency (SC): SC (30 trials) GSM8K 81.6% vs RCoT (1 trial) 82.0% and RCoT (3 trials) 83.2%; SC average token cost ~615 tokens per problem vs RCoT 31 tokens (1 trial) and 53.3 tokens (3 trials). Combining RCOT+SC produced further gains (paper reports combined highest accuracy up to 84.5% averaged across the seven datasets in one reported setting). Manual human-written fine-grained feedback to ChatGPT reached 94.6% on GSM8K (demonstrating a gap between automatic RCOT feedback and human feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Fine-grained feedback (explicit listing of hallucinated/overlooked conditions and misinterpreted question) is critical: coarse-grained prompts ("double-check") or removing 'reasons' substantially degrade performance (Table 2). Fine-grained comparison (decomposing to condition lists and comparing step-by-step) is necessary; replacing with coarse-grained reconstruction-comparison performs much worse (even below standard CoT). RCOT is particularly effective at detecting overlooked conditions and question misinterpretations (better detection rates for these error types than for hallucinations per a manual 100-problem analysis). RCOT tends to give larger gains on harder/complex datasets (AQuA, Date). Combining diverse-solution methods (SC) with RCOT's rectification produces complementary improvements. Human-written feedback is substantially better than RCOT-generated feedback (large accuracy gap on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The paper claims that using a reconstruction-then-fine-grained-comparison pipeline (RCOT) to generate fine-grained feedback and then revising solutions improves factual consistency and final accuracy over standard single-trace CoT and simple double-check baselines; RCOT competes with or outperforms Self-Consistency (diverse-solution majority voting) at significantly lower token/cost, and combining RCOT with Self-Consistency yields further gains. The paper also concludes that fine-grained feedback is essential (coarse feedback harms performance) and that there remains a substantial gap between automatic feedback and human-written feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8257.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8257.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-13B-Chat (LLaMA 13B fine-tuned as a chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 13-billion-parameter Transformer-based LLM (LLaMA family) fine-tuned for chat; used as a comparative backbone in the paper's experiments to assess RCOT and baseline methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 13B-parameter LLaMA family model adapted for chat; used with temperature 0 and evaluated in the same pipelines (CoT, RCOT, baselines) to test method generality across model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT)', 'Reversing Chain-of-Thought (RCOT)', 'Self-Consistency (SC)', 'Self-Refine', 'Double-Check', 'Active-Prompting (few-shot selection)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same approaches applied as for ChatGPT: CoT prompting (zero-shot "Let's think step by step" and few-shot CoT), RCOT reconstruction + fine-grained comparison + feedback, and baseline multi-trial/self-refinement/diverse-sampling approaches used for comparison. Experimental settings kept similar (temperature, number of sampled trials for SC etc.) to measure relative improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons: baseline CoT vs RCOT in zero-shot and few-shot; ablations of RCOT's components (question vs condition comparison); comparison to Double-Check and Active-Prompting; limited SC experiments (SC sampled extensively vs RCoT using fewer attempts) to compare accuracy vs cost tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same seven arithmetic datasets (GSM8K, AQuA, SVAMP, AddSub, ASDiv, Date, SingleEq); evaluation reported on sampled subsets (three random subsets of 256 inputs each) averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Highlights (zero-shot, LLaMA-13B-Chat): Standard CoT → GSM8K 36.9±0.82%, AQuA 27.2±0.06%, AddSub 66.7±0.55%, Date 52.4±1.56%, SingleEq 62.6±2.65%, ASDiv 52.2±3.73%, SVAMP 38.6±1.1%. +RCOT (zero-shot) → GSM8K 39.8±0.83% (+2.9), AQuA 31.9±0.06% (+4.7), AddSub 67.4±0.55% (+0.7), Date 55.3±2.06% (+2.9), SingleEq 63.5±2.15% (+0.9), ASDiv 53.0±3.74% (+0.8), SVAMP 41.1±0.8% (+2.5). Few-shot results show smaller but consistent gains. The paper notes that ChatGPT (stronger model) benefits more from RCOT than LLaMA-13B-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>RCOT improves performance across a range of difficulties also for a smaller open-source model but gains are smaller than for ChatGPT, suggesting RCOT's effectiveness scales with base model capability. Ablations on fine-grained comparison components and feedback granularity similarly impact LLaMA as with ChatGPT: removing question/condition comparisons or using coarse-grained feedback reduces or negates benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>RCOT's reconstruction + fine-grained comparison and targeted feedback improves factual consistency and accuracy for both a strong closed model (ChatGPT) and an open-source mid-size model (LLaMA-13B-Chat), though absolute gains are larger for the stronger model; fine-grained detection and feedback are necessary for improvements, and RCoT is complementary to multi-sample/diverse reasoning methods like Self-Consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Active prompting with chain-of-thought for large language models <em>(Rating: 1)</em></li>
                <li>Refiner: Reasoning feedback on intermediate representations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8257",
    "paper_id": "paper-258823296",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (closed-source OpenAI conversational LLM)",
            "brief_description": "A closed-source conversational large language model used as a backbone in experiments; evaluated with Chain-of-Thought prompting and with the paper's proposed RCOT procedure, and compared against multi-sample and refinement baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Closed-source conversational large language model (OpenAI). Treated as a high-capability base LLM in zero-shot and few-shot CoT settings with temperature set to 0 for reproducibility in many experiments.",
            "reasoning_methods": [
                "Chain-of-Thought (CoT)",
                "Reversing Chain-of-Thought (RCOT)",
                "Self-Consistency (SC, majority voting over diverse sampled CoT traces)",
                "Self-Refine (iterative self-feedback refinement)",
                "Double-Check (coarse prompt encouraging re-check)",
                "Active-Prompting (few-shot selection of uncertain examples)"
            ],
            "reasoning_methods_description": "Chain-of-Thought (CoT): prompting with \"Let's think step by step\" or few-shot CoT demonstrations to elicit intermediate reasoning steps. RCOT: the paper's proposed method where the model first reconstructs the original problem from its generated solution, decomposes both original and reconstructed problems into condition lists, performs fine-grained comparisons (condition-by-condition and question comparison) to detect hallucinations/overlookings/misinterpretations, then formulates fine-grained feedback to the model to revise the solution. Self-Consistency (SC): sampling many diverse CoT solutions and applying majority voting on final answers. Self-Refine: iterative self-feedback and revision (used as baseline). Double-Check: a coarse prompt instructing the model to re-check answers without specifying detected inconsistencies. Active-Prompting: selecting few-shot examples by uncertainty; used in few-shot comparisons.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Explicit comparisons and ablations: (1) compared standard single-trace CoT vs RCOT (single-trace with reconstruction+feedback); (2) compared diverse-solution majority-vote SC (30 sampled CoT traces) vs RCOT (1 or 3 trials) to examine trade-offs in accuracy and token/cost; (3) combined RCOT with SC to test complementary effects; (4) ablations on feedback granularity (fine-grained vs coarse-grained: 'w/o reasons' and 'w/o judgment+reasons'); (5) ablations on fine-grained comparison components (remove question comparison or condition comparison) and coarse-grained comparison instead of fine-grained.",
            "task_or_benchmark": "Seven arithmetic reasoning datasets: GSM8K (grade-school math), AQuA (diverse/complex math multiple-choice), SVAMP (simple arithmetic), AddSub, ASDiv, Date (date arithmetic/multi-hop), SingleEq (single-equation problems); evaluation reported in zero-shot and few-shot CoT settings.",
            "performance_results": "Highlights (zero-shot, ChatGPT): Standard CoT → GSM8K 79.0±0.95%, AQuA 51.3±0.68%, AddSub 85.2±1.26%, Date 66.7±1.49%, SingleEq 90.3±0.68%, ASDiv 84.3±0.47%, SVAMP 76.7±4.1%. +RCOT (zero-shot) → GSM8K 82.0±0.35% (+3.1), AQuA 55.5±0.88% (+4.1), AddSub 87.1±1.17% (+1.8), Date 71.7±1.39% (+5.0), SingleEq 91.4±0.88% (+1.1), ASDiv 86.0±0.37% (+1.7), SVAMP 79.6±4.1% (+2.8). Few-shot highlights: ChatGPT +RCOT (few-shot) GSM8K 84.6±0.65% (improving over few-shot baselines; Active-Prompting ~81.8%), AQuA +RCOT 57.1±0.38%. Comparison with Self-Consistency (SC): SC (30 trials) GSM8K 81.6% vs RCoT (1 trial) 82.0% and RCoT (3 trials) 83.2%; SC average token cost ~615 tokens per problem vs RCoT 31 tokens (1 trial) and 53.3 tokens (3 trials). Combining RCOT+SC produced further gains (paper reports combined highest accuracy up to 84.5% averaged across the seven datasets in one reported setting). Manual human-written fine-grained feedback to ChatGPT reached 94.6% on GSM8K (demonstrating a gap between automatic RCOT feedback and human feedback).",
            "qualitative_findings": "Fine-grained feedback (explicit listing of hallucinated/overlooked conditions and misinterpreted question) is critical: coarse-grained prompts (\"double-check\") or removing 'reasons' substantially degrade performance (Table 2). Fine-grained comparison (decomposing to condition lists and comparing step-by-step) is necessary; replacing with coarse-grained reconstruction-comparison performs much worse (even below standard CoT). RCOT is particularly effective at detecting overlooked conditions and question misinterpretations (better detection rates for these error types than for hallucinations per a manual 100-problem analysis). RCOT tends to give larger gains on harder/complex datasets (AQuA, Date). Combining diverse-solution methods (SC) with RCOT's rectification produces complementary improvements. Human-written feedback is substantially better than RCOT-generated feedback (large accuracy gap on GSM8K).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "The paper claims that using a reconstruction-then-fine-grained-comparison pipeline (RCOT) to generate fine-grained feedback and then revising solutions improves factual consistency and final accuracy over standard single-trace CoT and simple double-check baselines; RCOT competes with or outperforms Self-Consistency (diverse-solution majority voting) at significantly lower token/cost, and combining RCOT with Self-Consistency yields further gains. The paper also concludes that fine-grained feedback is essential (coarse feedback harms performance) and that there remains a substantial gap between automatic feedback and human-written feedback.",
            "uuid": "e8257.0",
            "source_info": {
                "paper_title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA-13B-Chat",
            "name_full": "LLaMA-13B-Chat (LLaMA 13B fine-tuned as a chat model)",
            "brief_description": "An open-source 13-billion-parameter Transformer-based LLM (LLaMA family) fine-tuned for chat; used as a comparative backbone in the paper's experiments to assess RCOT and baseline methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B-Chat",
            "model_description": "Open-source 13B-parameter LLaMA family model adapted for chat; used with temperature 0 and evaluated in the same pipelines (CoT, RCOT, baselines) to test method generality across model capabilities.",
            "reasoning_methods": [
                "Chain-of-Thought (CoT)",
                "Reversing Chain-of-Thought (RCOT)",
                "Self-Consistency (SC)",
                "Self-Refine",
                "Double-Check",
                "Active-Prompting (few-shot selection)"
            ],
            "reasoning_methods_description": "Same approaches applied as for ChatGPT: CoT prompting (zero-shot \"Let's think step by step\" and few-shot CoT), RCOT reconstruction + fine-grained comparison + feedback, and baseline multi-trial/self-refinement/diverse-sampling approaches used for comparison. Experimental settings kept similar (temperature, number of sampled trials for SC etc.) to measure relative improvements.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparisons: baseline CoT vs RCOT in zero-shot and few-shot; ablations of RCOT's components (question vs condition comparison); comparison to Double-Check and Active-Prompting; limited SC experiments (SC sampled extensively vs RCoT using fewer attempts) to compare accuracy vs cost tradeoffs.",
            "task_or_benchmark": "Same seven arithmetic datasets (GSM8K, AQuA, SVAMP, AddSub, ASDiv, Date, SingleEq); evaluation reported on sampled subsets (three random subsets of 256 inputs each) averaged.",
            "performance_results": "Highlights (zero-shot, LLaMA-13B-Chat): Standard CoT → GSM8K 36.9±0.82%, AQuA 27.2±0.06%, AddSub 66.7±0.55%, Date 52.4±1.56%, SingleEq 62.6±2.65%, ASDiv 52.2±3.73%, SVAMP 38.6±1.1%. +RCOT (zero-shot) → GSM8K 39.8±0.83% (+2.9), AQuA 31.9±0.06% (+4.7), AddSub 67.4±0.55% (+0.7), Date 55.3±2.06% (+2.9), SingleEq 63.5±2.15% (+0.9), ASDiv 53.0±3.74% (+0.8), SVAMP 41.1±0.8% (+2.5). Few-shot results show smaller but consistent gains. The paper notes that ChatGPT (stronger model) benefits more from RCOT than LLaMA-13B-Chat.",
            "qualitative_findings": "RCOT improves performance across a range of difficulties also for a smaller open-source model but gains are smaller than for ChatGPT, suggesting RCOT's effectiveness scales with base model capability. Ablations on fine-grained comparison components and feedback granularity similarly impact LLaMA as with ChatGPT: removing question/condition comparisons or using coarse-grained feedback reduces or negates benefits.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "RCOT's reconstruction + fine-grained comparison and targeted feedback improves factual consistency and accuracy for both a strong closed model (ChatGPT) and an open-source mid-size model (LLaMA-13B-Chat), though absolute gains are larger for the stronger model; fine-grained detection and feedback are necessary for improvements, and RCoT is complementary to multi-sample/diverse reasoning methods like Self-Consistency.",
            "uuid": "e8257.1",
            "source_info": {
                "paper_title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Active prompting with chain-of-thought for large language models",
            "rating": 1,
            "sanitized_title": "active_prompting_with_chainofthought_for_large_language_models"
        },
        {
            "paper_title": "Refiner: Reasoning feedback on intermediate representations",
            "rating": 1,
            "sanitized_title": "refiner_reasoning_feedback_on_intermediate_representations"
        }
    ],
    "cost": 0.014523749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RCOT: DETECTING AND RECTIFYING FACTUAL INCON-SISTENCY IN REASONING BY REVERSING CHAIN-OF-THOUGHT
2 Oct 2023</p>
<p>Tianci Xue xuetianci@smail.nju.edu.cn 
Department of Software
Nanjing University</p>
<p>Ziqi Wang ziqiw9@illinois.edu 
Department of Computer Science
University of Illinois Urbana-Champaign</p>
<p>Zhenhailong Wang wangz3@illinois.edu 
Department of Computer Science
University of Illinois Urbana-Champaign</p>
<p>Chi Han chihan3@illinois.edu 
Department of Computer Science
University of Illinois Urbana-Champaign</p>
<p>Pengfei Yu pengfei4@illinois.edu 
Department of Computer Science
University of Illinois Urbana-Champaign</p>
<p>Heng Ji hengji@illinois.edu 
Department of Computer Science
University of Illinois Urbana-Champaign</p>
<p>RCOT: DETECTING AND RECTIFYING FACTUAL INCON-SISTENCY IN REASONING BY REVERSING CHAIN-OF-THOUGHT
2 Oct 2023EC872148BACC0D135591B891F0B2D1EFarXiv:2305.11499v2[cs.CL]The current date is 10/16/1924. To find the date one year ago from today, you would subtract one year from the current year, which would be 1923. The correct answer is 10/16/1923. ✗
Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting.However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems.Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency.In this work, we propose RCOT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs' generated solutions.To detect factual inconsistency, RCOT first asks LLMs to reconstruct the problem based on generated solutions.Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions.To rectify the solution, RCoT formulates detected factual inconsistency into fine-grained feedback to guide LLMs in revising solutions.Experimental results demonstrate improvements of RCoT over standard CoT, Self-Consistency and Self-Refine across seven arithmetic datasets.Moreover, we find that manually written fine-grained feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on GSM8K), encouraging the community to further explore the fine-grained feedback generation methods.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) (Brown et al., 2020a;Zhang et al., 2022a;Narang &amp; Chowdhery, 2022;Touvron et al., 2023) have showcased strong reasoning capabilities using chain-of-thought (CoT) (Wei et al., 2023;Chowdhery et al., 2022;Fung et al., 2022), where LLMs are prompted to generate intermediate steps before the final answer.Despite the impressive performance of CoT prompting across various reasoning tasks (Dua et al., 2019;Miao et al., 2020;Cobbe et al., 2021b;Yu et al., 2020;Bhagavatula et al., 2019;Talmor et al., 2019), LLMs still struggle to maintain factual consistency in reasoning.Specifically, each reasoning problem usually consists of several conditions and a question, and LLMs exhibit tendencies to hallucinate, overlook conditions and misinterpret questions (Golovneva et al., 2022).</p>
<p>While previous research has proposed various methods to enhance Chain-of-Thought performance (Zhang et al., 2022b;Fu et al., 2022;Diao et al., 2023;Shum et al., 2023;Zhou et al., 2023;Wang et al., 2023;Gao et al., 2023;Chen et al., 2022;Weng et al., 2023;Paul et al., 2023;Shinn et al., 2023), there remains a noticeable absence of explicit studies addressing the issue of factual inconsistency.The most relevant work is probably Self-Verification (Weng et al., 2023), which verifies answers by swapping conditions and answers.However, it can only tell whether answers are correct and fail to give fine-grained feedback on factual inconsistency to guide LLMs in revising solutions.Figure 1 shows an instance of factual inconsistent solutions generated by ChatGPT, where Problem: The cave is 1218 feet deep and they are already at 849 feet.If they are travelling at speed of 17 How much farther until they reach the end of the cave?</p>
<p>ChatGPT Chain-of-Thought:</p>
<p>First, we need to find out how much deeper the cave is from their current position ......So they still need to descend 369 feet to reach the end of the cave.Next, we need to figure out how long it will take them to descend that distance at a speed of 17 feet per unit of time ...... Therefore, it will take them 22 units of time to reach the end of the cave.✗ Figure 2: An example showing that LLMs may misinterpret questions.Green: The correct solution of the problem.Red: The superfluous answer given by LLMs.Misinterpreting: LLMs have correctly answered the question in the problem, but it continues answering "how long", resulting in a wrong answer.This is a typical misinterpretation of the problem.</p>
<p>Problem: A wooden bridge can carry no more than 5000 pounds.A delivery truck filled with identical boxes, each weighing 15 pounds, will pass over the bridge.The combined weight of the driver and the empty truck is 3755 pounds.What is the maximum number of boxes which can be loaded onto the truck while not exceeding the bridge's weight limit?</p>
<p>ChatGPT Chain-of-Thought:</p>
<p>First, we need to subtract the weight of the driver and the empty truck from the maximum weight the bridge can carry ...... Let's assume each box weighs 16 pounds (15 pounds for the contents and 1 pound for the box).......This means we can load a maximum of 77 boxes onto the truck without exceeding the bridge's weight limit.✗ Figure 3: An example showing that LLMs hallucinate conditions.Red: Hallucinated conditions.Hallucinating: The problem mentions that each box weighs 15 pounds.However, LLMs assume each box weighs 16 pounds, which contradicts real conditions.</p>
<p>We evaluate RCOT on seven arithmetic reasoning datasets, including GSM8k (Cobbe et al., 2021a), AQuA (Ling et al., 2017), SVAMP (Patel et al., 2021), AddSub (Hosseini et al., 2014), ASDiv (Miao et al., 2021), Date (Srivastava et al., 2022) and SingelEq (Koncel-Kedziorski et al., 2016).Experimental results demonstrate the effectiveness of RCOT, outperforming competitive baselines in both zero-shot and few-shot settings.In-depth analysis and human evaluation suggest that fine-grained feedback on factual inconsistency is crucial for LLMs to revise solutions for arithmetic problems.For example, ChatGPT could achieve 94.6% accuracy on GSM8k with manually written fine-grained Preprint feedback.Moreover, we conduct comprehensive ablation studies examining the impact of individual modules.Our findings encourage the community to further explore detecting and rectifying factual inconsistency to enhance LLMs' reasoning ability.</p>
<p>Our contributions are summarized as follows:</p>
<p>• We propose a novel prompting method, Reversing Chain-of-Thought (RCOT) to effectively detect and rectify the factual inconsistency of LLMs in arithmetic reasoning, focusing on overlooked, hallucinated conditions and misinterpreted questions.RCoT demonstrates improvement over competitive baseline models across seven arithmetic reasoning tasks.• Prompting with fine-grained feedback on factual inconsistency shows encouraging results on improving LLMs' reasoning abilities.Though automatically generated feedback by RCoT shows consistent improvement compared with standard CoT, we find that human-written ground-truth feedback can further improve the LLMs' reasoning ability (e.g., ChatGPT reaches 94.6% accuracy on GSM8k).The gap between RCoT's feedback and humanwritten feedback encourages the community to further explore the automatic generation of fine-grained feedback.• RCoT offers more interpretability to the reasoning errors with fine-grained feedback on factual inconsistency, allowing humans to inspect solutions easier.</p>
<p>RELATED WORK</p>
<p>Language Model for Reasoning Reasoning ability is a critical skill to solve complex problems, such as arithmetic reasoning (Koncel-Kedziorski et al., 2016;Roy &amp; Roth, 2016;Miao et al., 2020;Cobbe et al., 2021b;Dua et al., 2019), logical reasoning (Yu et al., 2020), commonsense reasoning (Bhagavatula et al., 2019;Talmor et al., 2019;Zellers et al., 2018;Ye &amp; Durrett, 2022) , and tabular reasoning (Zhu et al., 2021).Recently, Large Language Models (e.g., GPT3 (Brown et al., 2020b), ChatGPT, PaLM (Narang &amp; Chowdhery, 2022) and LLaMA (Touvron et al., 2023)) have demonstrated promising reasoning capability with Chain-of-Thought methods.However, large language models exhibit tendencies to generate intermediate steps that are factually inconsistent, rendering them incapable of solving complex problems requiring multi-step reasoning.In this work, we focus on the detection and rectification of factually inconsistent errors in the intermediate reasoning steps, including question misinterpretation, condition hallucination and condition overlooking.</p>
<p>Prompt Engineering Some prompting methods can elicit useful knowledge in large language models to better solve complex tasks, two representative examples of which are In-context Learning (Brown et al., 2020b) and Chain-of-Thought (Wei et al., 2023).In-Context Learning encourages the language models to learn from a few input-output examples as prompts (Liu et al., 2022;Rubin et al., 2022;Min et al., 2022).Chain-of-Thought prompting improves reasoning performance by prompting LLMs to think of intermediate steps.Inspired by the promising performance of CoT, many methods have explored how to further improve standard CoT.Least-to-most (Zhou et al., 2023) prompting proposes to decompose a complex problem into a series of subproblems and solve them sequentially.Self-Consistency prompting (Wang et al., 2023) improves performance through majority voting on multiple solutions.Similarly, Complex CoT (Fu et al., 2022) emphasizes the importance of prompt complexity and selects the most complex examples as prompts.Auto-CoT (Zhang et al., 2022b) is proposed to reduce the workload of manual labeling.Active Prompting (Diao et al., 2023) selects the most uncertain questions as demonstration examples to further improve performance.However, these methods fail to address the factual inconsistency problem.Probably the most relevant work are Self-Verification (Wang et al., 2023), REFINER (Paul et al., 2023), and Reflexion (Shinn et al., 2023).These approaches focus on correcting LLMs outputs.However, Self-Verification can only generate binary feedback and fail to get fine-grained feedback, REFINER needs externally trained models, and Reflexion requires environmental feedback, which cannot be easily obtained in arithmetic reasoning.</p>
<p>Compared to these methods, RCoT entirely relies on the LLM itself to generate fine-grained feedback on factual consistency.</p>
<p>Reverse Engineering.RCOT is inspired by the concept of Reverse Engineering, which has various applications in machine learning research.(Fredrikson et al., 2014) proposes a reverse method for linear models to evaluate models' privacy safety.(Fredrikson et al., 2015) introduces a model</p>
<p>Overlooked conditions prompt</p>
<p>You have ignored some real conditions: 1: she received 18 new potted plants from her favorite plant nursery.</p>
<p>Here are detailed reasons:</p>
<p>1: There is no information about whether or not Mary received any new potted plants from a nursery.Therefore, the real condition of Mary receiving 18 new potted plants cannot be logically derived or deduced from your thinking.</p>
<p>Hallucinated conditions prompt</p>
<p>Misinterpreted question prompt</p>
<p>Large Language Model Original Condition:</p>
<p>1: Mary has 40 window ledges in her apartment.</p>
<p>2: She has 2 potted plants on each ledge.</p>
<p>3: She has decided to give away 1 potted plant from each ledge.</p>
<p>Reconstructed Condition List:</p>
<p>Can this candidate condition be deduced from the condition list?</p>
<p>NO 1: Mary received 18 new potted plants from her favorite plant nursery.</p>
<p>2: She already has 2 potted plants on each of the 40 window ledges of her large country home.</p>
<p>3: She will give 1 potted plant from each ledge to friends and family tomorrow.</p>
<p>Original Condition List:</p>
<p>Can this candidate condition be deduced from the condition list?</p>
<p>Reconstructed Condition  (3) Comparison: Compare both lists of sub-conditions and questions to verify whether there are hallucinations, overlookings and misinterpretations.(4)Revision: Gathering all factual inconsistencies into fine-grained feedback to instruct LLMs to revise solutions.</p>
<p>inversion method for shallow neural networks, which can reconstruct the face information.(Geva et al., 2022) unveils the internal prediction construction process of Transformer-based language models by reverse engineering the operations of the feed-forward network (FFN) layers.Inverting model hyperparameters is another application of reverse engineering techniques.(Bhagavatula et al., 2019) reverses network parameters by repeatedly requesting the predicted label from the target model.(Tramèr et al., 2016) develops an avatar method to estimate training data and model architectures, while (Oh et al., 2019) trains a set of white-box models to estimate model hyperparameters.(Hua et al., 2018) estimates both the structure and the weights of a CNN model on a hardware accelerator from information leaks of memory access patterns.Different from their goal of opening up the black-box of deep learning models, our work focuses on automatically detecting and rectifying factual inconsistencies that appeared in the solutions generated by LLMs.</p>
<p>REVERSING CHAIN-OF-THOUGHT (RCOT)</p>
<p>We propose RCoT for detecting and rectifying factual inconsistency (i.e., condition hallucinations, overlookings, and question misinterpretation) in CoT to enhance LLMs' reasoning ability.Specifically, given a complex reasoning problem Q and original solution c generated by the LLM, we first ask LLMs to detect factual inconsistency: (i) Problem Reconstruction: Reconstruct the problem Q based on the generated solution c. (ii) Fine-grained Comparison: Conduct a fine-grained comparison between the original problem Q and the reconstructed problem Q to detect condition hallucinations, overlookings, and question misinterpretation.Then we rectify LLMs using detected factual inconsistency: (iii) Fine-grained Feedback and Revision: The fine-grained comparison reveals the factual inconsistency in original solutions.The detected factual inconsistencies are formulated into fine-grained feedback to guide LLMs in revising their solution accordingly.The overall schematic illustrations of our proposed approach are illustrated in Figure 4, and an example of RCoT is shown in Appendix B.3.</p>
<p>PROBLEM RECONSTRUCTION</p>
<p>Intuitively, if the generated step-by-step solution of an arithmetic problem is logically and factually correct and complete, it is more likely for a human to infer what is the original problem.Similarly, we ask the LLM to reconstruct the problem to get Q based on its own solution c, in order to verify whether it truly understands the problem.We manually write instructions and in-context examples as the reconstruction prompt.We find that the factual inconsistencies such as condition hallucinations (e.g., the LLM uses conditions that are not mentioned in the problem Q), condition overlookings (e.g., the LLM overlooks some important conditions in the problem Q), and question misinterpretations (e.g., the LLM misunderstand the question of Q) can be effectively exposed by comparing the reconstructed problem Q with the original problem Q ( § 3.2), as shown in Figures 11, 8, and 17 in Appendix B.1, respectively.The prompt template can be found in Figure 23.</p>
<p>FINE-GRAINED COMPARISON</p>
<p>To detect condition hallucinations and overlookings, as well as question misinterpretations in the solution c from the reconstructed problem Q, a naive approach is to ask the LLM to directly compare Q with Q.However, such comparisons usually fail to produce high-quality detection results (Figure 5), which is unsurprising because Q and Q contain rich information, and the coarse-grained comparison will inevitably ignore some vital information, causing a sub-optimal result.Therefore, we use finegrained step-by-step comparisons to improve the detection quality.All prompt templates are shown in Figure 23.The process is as follows:</p>
<p>Problem Decomposition.Q and Q are unstructured texts, which are hard to be compared in an organized manner.To overcome this issue, we ask the LLM to decompose the problem into a list of conditions
L Q = [L 1 Q , • • • , L m Q ], L Q = [L 1 Q, • • • , L n Q].
The structured condition list will then be used in fine-grained comparison.</p>
<p>Condition Comparison</p>
<p>To find the differences between Q and Q, we first check whether their condition lists L Q and L Q are the same.Specifically, the LLM is required to answer whether each
L i Q can be inferred from L Q. If L i Q cannot be inferred from L Q, then L i Q is either (1)
overlooked in the solution or (2) hallucinated by the LLM as a different condition.Similarly, we ask the LLM to tell whether
L j Q can be inferred from L Q for every j. If L j Q cannot be inferred from L Q , then L j Q is hallucinated.
Apparently, we need to conduct comparisons for nm times in total.</p>
<p>Question Comparison</p>
<p>The LLM sometimes will also misinterpret the question (Figure 2).Therefore, we also ask LLM to compare the questions being asked in Q and Q.If LLMs find the two questions are different, then LLMs misinterpret the question in their solutions.This comparison only needs to be done once since there is one question per problem in most cases.</p>
<p>After these comparisons, we detect hallucinated conditions, overlooked conditions, and misinterpreted of questions.We then use them to formulate our fine-grained feedback to guide the LLM in revising its solution.</p>
<p>FINE-GRAINED FEEDBACK AND REVISION</p>
<p>We assume the original solution is correct if we do not detect any factual inconsistency through fine-grained comparison.On the contrary, we formulate fine-grained feedback to guide the LLM in revising its solution if any factual inconsistency is detected.Specifically, the fine-grained feedback will first state that the solution is incorrect, then list the detected factual inconsistency, and finally ask Preprint the LLM to revise its solution.Figure 23 shows the template we use to formulate the feedback.We take the answer of the revised solution as the final output for evaluation.</p>
<p>EXPERIMENT</p>
<p>Our extensive experiments aim to show that (1) RCoT benefits arithmetic reasoning by automatically detecting and rectifying condition hallucination and overlooking, and question misinterpretation;</p>
<p>(2) fine-grained feedback of factual consistency is critical for LLMs to self-revise the solution.(3) fine-grained comparison is essential for constructing high-quality fine-grained feedback.</p>
<p>EXPERIMENT SETTING</p>
<p>We used closed-source ChatGPT and open-source LLaMA-13B-Chat (Touvron et al., 2023) as the backbone LLMs for solution generation and set the temperature to 0 to improve reproducibility.We evaluate RCoT on seven arithmetic datasets with different difficulties, including GSM8k (Cobbe et al., 2021a), AQuA (Ling et al., 2017), SVAMP (Patel et al., 2021), AddSub (Hosseini et al., 2014), ASDiv (Miao et al., 2021), Date (Srivastava et al., 2022) and SingelEq (Koncel-Kedziorski et al., 2016).Due to the high time cost of API calls, we do not use the whole test set but randomly sample test sub-sets.To reduce the randomness caused by test set sampling and make our results more convincing, we sample three test sub-sets that each contains 256 inputs.We report the average accuracy with deviation on the three test sub-sets.For the dataset that has less than 256 test inputs, we still evaluate three times since ChatGPT's outputs may change and report the average accuracy with deviation.A detailed description of each dataset is shown in Appendix B.5.</p>
<p>We consider both zero-shot and few-shot settings.For the zero-shot setting, we add the prompt "Let's think step by step" to encourage LLMs to think intermediate steps but without any demonstration examples (Kojima et al., 2023).For the few-shot setting, we use four-shot CoT prompts that consist of problems, solutions, and final answers.</p>
<p>We compare our method with five baselines: (1) Chain-of-thought (CoT) (Wei et al., 2023) (2) Active-Prompting (Diao et al., 2023), a method that selects the most uncertainty problems as demonstration examples.(3) Double-Check asks LLMs to check their answers but does not point out whether the answer is correct.In our experiment, we use the prompt "You should double-check your answers".(4) Self-Consistency (SC) (Wang et al., 2023) through majority voting on multiple solutions to improve the performance.(5) Self-Refine (Madaan et al., 2023) uses iterative feedback and refinement to revise the answer.We use Tiktoken from Openai to calculate the cost of average tokens.1</p>
<p>RCOT BENEFITS ARITHMETIC REASONING</p>
<p>Table 1 shows the results of RCoT on seven arithmetic datasets.Our method consistently outperforms the standard CoT and the double-check methods in the zero-shot setting.Moreover, LLMs benefit more from our method on more challenging tasks that require complex reasoning.For example, the AQuA dataset contains diverse problems, and the Date dataset requires multi-hop reasoning and common sense date knowledge.Both ChatGPT and LLaMA achieve lower accuracy scores on AQuA and Date (51.3% and 66.7% for ChatGPT and 27.2% and 52.4% for LLaMA) among all seven tasks.Meanwhile, we observe that our method helps LLMs improve by apparent margins on AQuA and Date (4.1%, 5.0% and 4.7%, 2.9% for ChatGPT and LLaMA), the highest gains in all seven tasks.Our method also remains effective for easier tasks.For example, RCoT enhances the performance of the SVAMP dataset, which contains problems that usually only need one-step calculation, by 2.8% and 2.5%.Moreover, we also observe greater improvements from our method on ChatGPT than LLaMA, potentially due to the stronger abilities of ChatGPT to detect and correct errors.</p>
<p>We can observe similar results in the few-shot setting to the zero-shot setting.Although selecting the most uncertain problems for LLMs as demonstration examples is helpful for reasoning (Diao et al., 2023), RCoT still improves the accuracy.It is worth noting that the performance of Double-Check method in the few-shot CoT setting decreases immensely.On the AQuA and GSM8K datasets, its performance drops by 27.0% and 4.0%, suggesting that few-shot examples may increase the risk of revising correct solutions to the incorrect ones.LLaMA exhibits a lower degree of susceptibility compared to ChatGPT.</p>
<p>We also compared RCoT with other stronger baselines (i.e., Self-Consistency, SC for short, and Self-Refine).Specifically, We conducted 30 trials per problem for SC and 3 trials per problem for RCoT in the zero-shot setting (set temperature to 0.7 Wang et al. ( 2023)), which uses similar costs.Due to the extremely high cost, we do not experiment with the few-shot setting and leave it as our future work.We set max attempt to 5 for Self-Refine.Table 4 has shown the results.RCoT could achieve comparable performance to SC at nearly one-third of the cost (e.g., AddSub, SingleEq, SVAMP) and even outperforms SC on the GSM8K dataset.However, the performance significantly drops on AQuA and Date datasets.That is because there are multiple-choice tasks, making it exceedingly simple for the model to approximate the answer by employing multiple guesses with incorrect logical steps.Combining RCoT with SC, our method can further improve the performance and surpass all baselines, reaching a high accuracy of 84.5% across seven arithmetic datasets.Our experiments demonstrate the same conclusion as Madaan et al. (2023) that Self-Refine is not good at arithmetic reasoning.It's worth noting that Self-Refine achieves the highest accuracy on SingleEq and AddSub datasets.Nevertheless, the improvement does not come from refinement but the usage of code in the Self-Refine implementation, reducing a large number of calculation errors.The real improvements brought by refinement are actually 0.8 and 0.4 in the AddSub and SingleEq datasets, respectively.Another phenomenon is that self-refine does not bring more token cost even if we give it more refinement budget.This is because self-refine tends to state that the solution is correct after the second refinement.</p>
<p>Preprint   The success of our method comes from fine-grained feedback that points out detailed factual inconsistency (condition hallucination and overlooking, and question misinterpretation).In this section, we show that coarse-grained feedback will lead to worse performance to prove the necessity of finegrained feedback.We replace our fine-grained feedback with two kinds of coarse-grained feedback:</p>
<p>(1) w/o reasons: we do not tell LLMs the detected factual inconsistency by RCoT and only give a high-level judgment.Therefore, if RCoT detects no factual inconsistency, we take the original solution as the final output for evaluation.Otherwise, we use the prompt "Your answer is wrong.You should double-check your answer" to guide LLMs in revising solutions.</p>
<p>(2) w/o judgment+reasons (i.e., Double-Check): We further remove the high-level judgment from the prompts.Therefore, we always use "You should double-check your answer" to guide LLMs in revising solutions regardless of the detection results of RCoT.Table 2 shows the results on SVAMP(easy), GSM8K(medium), and AQuA (hard) datasets.We can see consistent performance drops when we remove detected factual inconsistency and only keep a high-level judgment, showing the effectiveness of fine-grained feedback.Moreover, we can observe that further removing judgment will make the performance even worse than standard CoT.This is not surprising because LLMs may mistakenly revise the correct solution to the incorrect one.Appendix B.6 shows an example of RCoT and Double-Check, where we can see that RCoT could help the LLM correct the solution, but Double-Check couldn't.</p>
<p>To further show the power of fine-grained feedback, we conduct a human evaluation.Specifically, we go through the generated solution and write fine-grained feedback on factual inconsistency by ourselves.Amazingly, the LLM could reach 94.6% accuracy on the GSM8K dataset and could only reach 86.3% if we remove explanations of factual inconsistencies from our feedback (i.e., the same setting as "w/o reasons" in Table 2).Appendix B.2 shows examples of manually written and RCoT-generated feedback.This result shows the same observation of Table 2 and reveals the strong Preprint power of fine-grained feedback.Since RCoT still has a gap (12.6% accuracy gap) compared with humans, we encourage the community to further explore fine-grained feedback generation.</p>
<p>FINE-GRAINED COMPARISON LEADS TO FINE-GRAINED FEEDBACK</p>
<p>To get fine-grained feedback, conditions and questions are compared in a fine-grained manner in RCoT.A simpler way is to ask LLMs to generate fine-grained feedback directly by comparing the original problems and reconstructed problems.Appendix A has illustrated that LLMs can reconstruct questions well when the reasoning maintains factual consistency but not vice versa.Table 4 shows that coarse-grained comparison will cause a significant accuracy drop (even worse than standard CoT), which suggests that it fails to generate high-quality feedback (Figure 5).Therefore, problem decomposition and fine-grained comparison are essential.We also show that both condition comparison and question comparison are important.Removing either of them will lead to worse performance.This is because LLMs may hallucinate/overlook conditions (Figure 3, 1) as well as misinterpret questions (Figure 2).</p>
<p>Original Problem: Rebecca wants to split a collection of eggs into groups of 3. Rebecca has 99 bananas 9 eggs and 27 marbles.How many groups will be created?Reconstructed Problem: Rebecca has 9 eggs 99 bananas and 27 marbles.If we can group items into sets of 3 how many groups can we make in total?</p>
<p>Coarse-Grained Comparison:</p>
<p>Yes, the reconstructed problem does not add new conditions or lack any conditions that were present in original problem.Original problem and reconstructed problem essentially ask the same question, with the only difference being the way the information is presented.Both problems ask how many groups of 3 items can be made from a collection of 9 eggs, 99 bananas, and 27 marbles.Therefore, reconstructed problem does not introduce any new conditions or leave out any conditions from the original problem.✗</p>
<p>Fine-Grained Comparison:</p>
<p>No, the candidate condition "Group items into sets of 3" cannot be deduced from the given condition list as it is not applicable to all items in the condition list.The condition only applies to the eggs that Rebecca wants to split into groups of 3. It does not apply to the bananas or the marbles.Additionally, the candidate condition implies that all items must be grouped into sets of 3, but there is no indication in the context that Rebecca wants to group all items into sets of 3.She only wants to split the eggs into groups of 3. ✔  To explore the effectiveness of RCoT, we construct further quantitative analysis on 100 problems the ChatGPT answered incorrectly.We manually divided these problems into four categories: condition hallucination, condition overlooking, question misinterpretation and other errors.The statistical results are shown in Table 5.We find that RCoT is better at detecting overlooking and misinterpretation errors than hallucination errors.</p>
<p>CONCLUSION</p>
<p>In this paper, we propose RCoT, a method that enables LLMs to detect and rectify factual inconsistency automatically to improve LLMs' reasoning abilities.RCoT detects factual inconsistency through fine-grained comparison between the reconstructed problems and original questions, and then asks LLMs to rectify inconsistencies through fine-grained feedback.Experimental results on seven arithmetic reasoning datasets demonstrate the effectiveness of RCoT.Our experiments also show Preprint encouraging results of LLMs' reasoning abilities with the help of manually written fine-grained feedback, encouraging the community to further explore fine-grained feedback generation.RCoT could, in principle, be applied to other tasks requiring CoT solutions.We discuss the limitations and future work in Appendix C.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>The supplementary material includes the code for all experiments and their corresponding running scripts.The dataset (GSM8K, AQuA, AddSub, SingleEq, Date, ASDiv and SVAMP) can be easily accessible on the HuggingFace website or from their official repositories.We explain all the experimental details (temperature, dataset size and so on ) in Section 4.1.</p>
<p>A THE QUALITY OF RECONSTRUCTED PROBLEM</p>
<p>We measured the Rouge1, Rouge2, RougeL, RougeSum, and sentence embedding similarity (using sentence-transformers/all-mpnet-base-v2) between original problems and reconstructed problems.We can observe from table ?? that higher CoT accuracies correspond to higher similarities between original problems and reconstructed problems.This is expected since correctly solved problems' reconstruction should ideally be itself, and incorrectly solved problems' reconstruction should differ from itself.Let's first calculate how much money Tobias saved up in three months:He gets a $5 allowance per month, so in three months he has saved up: $5/month * 3 months = $15 n addition to his allowance, Tobias also earns money by mowing lawns andshoveling driveways.If he mows 4 lawns, he earns: 4lawns*$15/lawn = $50 Let's say he shovels x driveways.Then, he earns:</p>
<p>x driveways*$7/driveway = $7x In total, he has saved up: $15 (allowance) + $60 (lawn mowing) + $7x (shoveling) = $75 + $7x He spent $95 on shoes, so he has: $75 + $7x-$95 = $0 Simplifying this equation: $7x -$20 = $0 $7x = $20 x= 2.85 Therefore, Tobias shoveled about 2.85 driveways.However since he can't shovel afraction of a driveway, we can round this up to 3 driveways.✗</p>
<p>Original Problem: Tobias is buying a new pair of shoes that costs $95.He has been saving up his money each month for the past three months.He gets a $5 allowance a month.He also mows lawns and shovels driveways.He charges $15 to mow a lawn and $7 to shovel.After buying the shoes, he has $15 in change.If he mows 4 lawns, how many driveways did he shovel?</p>
<p>Reconstructed Problem: How many driveways did Tobias have to shovel to save enough money to buy $95 shoes, if he gets a $5 allowance per month for three months, mows 4 lawns for $15 each, and earns $7 for every driveway he shovels?Figure 8: A example for overlooking.</p>
<p>Preprint</p>
<p>Original Solution:</p>
<p>First, we need to find out how many bottle caps and wrappers Danny had before he found the ones at the park.To do this, we can add the number he found at the park to the number he has now: 71 + 28 = 99 bottle caps 24 + 16 = 40 wrappers Next, we need to find the difference between the number of bottle caps and wrappers: 99 -40 = 59 Therefore, Danny has 59 more bottle caps than wrappers in his collection now.✗</p>
<p>Original Problem: Danny collects bottle caps and wrappers.He found 71 bottle caps and 24 wrappers at the park.Now he has 28 bottle caps and 16 wrappers in his collection.How many more bottle caps than wrappers does danny have now?</p>
<p>Reconstructed Problem: Danny found 28 bottle caps and 16 candy bar wrappers at the park.He now has 71 bottle caps and 24 candy bar wrappers in his collection.How many more bottle caps than candy bar wrappers does Danny have in his collection now? Figure 13: A example for hallucination</p>
<p>Original Solution:</p>
<p>One week ago from today would be August 25th, 2021.Therefore, the answer is not (A) or (C) since they both represent the current date.It is also not (B), (D), or (F) since they are not one week ago from today.Martha has been collecting shells since she turned 5 years old, which means she has been collecting shells for 5 years.In one year, Martha collects 12 shells (one per month).So in 5 years, Martha collects 5 x 12 = 60 shells.By her 10th birthday, Martha will have collected 60 + 12 = 72 shells.✗</p>
<p>Original Problem: Martha has been collecting shells since she turned 5 years old, every month she collects one shell.By her 10th birthday, how many shells will Martha have collected?Figure 16: A example for hallucinating</p>
<p>Original Solution:</p>
<p>First, we need to find the total number of houses in all 81 blocks.To do this, we can multiply the number of houses in each block (12) by the number of blocks (81): 12 x 81 = 972 So there are 972 houses in total.</p>
<p>Next, we need to find out how many pieces of junk mail the mailman needs to give to each house.We know that he has to give 4 pieces of junk mail to each house, so we can multiply this by the total number of houses: 4 x 972 = 3,888 Therefore, the mailman needs to give 3,888 pieces of junk mail in each block.✗</p>
<p>Original Problem: A mailman has to give 4 pieces of junk mail to each house in each of the 81 blocks.If there are 12 houses in each block How many pieces of junk mail should he give in each block?</p>
<p>Reconstructed Problem: There are 81 blocks in a neighborhood, and each block has 12</p>
<p>houses.The mailman needs to deliver 4 pieces of junk mail to each house.How many pieces of junk mail does the mailman need to deliver in total?</p>
<p>RCoT</p>
<p>Fine-grained Feedback:</p>
<p>Here are the mistakes and reasons in your answer to the problem.</p>
<p>You use some wrong candidate conditions: 1: She wants to have 140 more games than she currently has.They all can not be deduced from the true condition list.</p>
<p>Here are detailed reasons: 1: The true problem only provides information about how many games Kelly currently has and she needs to reach a certain number of games.It does not provide any information that Kelly wants to have 140 more games than she currently has.Therefore, the candidate condition of "She wants to have 140 more games than she currently has" cannot be deduced from your thinking.</p>
<p>The real problem has the conditions: 1: Kelly has 22 nintendo games.2: She wants to have 140 games left.</p>
<p>You should consider all real conditions in the problem.</p>
<p>Revised Answer:</p>
<p>Thank you for pointing out the mistakes in my previous answer.Here's a corrected response: To calculate how many Nintendo games Kelly needs to buy so that she will have 140 games left, we need to subtract 140 from the total number of games she wants to have.The total number of games she wants to have is the current number of games she has plus the number of games she wants to buy.So, we can set up an equation: 22 + x = 140 where x is the number of games Kelly needs to buy.Solving for x, we get: x = 140 -22 = 118 Therefore, Kelly needs to buy 118 Nintendo games so that she will have 140 games left.✔</p>
<p>Reasons By Humans</p>
<p>Fine-grained Feedback:</p>
<p>Your answer is wrong.The problem mentions that she will have 140 games left, but it doesn't mean that she desires 140 more games.You misunderstand it.</p>
<p>Revised Answer:</p>
<p>I apologize for the mistake in my previous response.</p>
<p>Reasons By Humans</p>
<p>Fine-grained Feedback:</p>
<p>The books and movies in crazy silly school are not relevant to calculate how many more movies than books you have read.You only need to consider the number of books and movies you have read/watched.</p>
<p>Revised Solution:</p>
<p>You are correct.I apologize for my mistake.</p>
<p>Preprint</p>
<p>Instruction: From a mathematical point of view, are these problems asking the same thing in the end?Please illustrate your reason and answer "yes" or "no".</p>
<p>Step3: Fine-grained Feedback and Revision Instruction:</p>
<p>Here are the mistakes and reasons in your answer to the problem.</p>
<p>You have ignored some real conditions: 1: She put them into stacks with 8 in each stack.</p>
<p>You should consider all real conditions in the problem.</p>
<p>Here are detailed reasons:</p>
<p>1: There is no information about whether or not She put them into stacks with 8 in each stack.Therefore, the real condition of "She put them into stacks with 8 in each stack" cannot be logically derived or deduced from your thinking.</p>
<p>You use some wrong candidate conditions: 1: She wants to stack them in piles of 5.They all can not be deduced from the true condition list.</p>
<p>You should consider all real conditions in the problem.</p>
<p>Here are detailed reasons: 1: The condition list only provides information about the number of extra nickels Lana has and she wants to put 8 nickels in each stack.There is no mention of her preference or intention to stack them in piles of 5.</p>
<p>Revised Solution:</p>
<p>I apologize for my mistake in the previous answer.Here is the corrected answer: To calculate how many stacks Lana could make with 72 extra nickels, we need to divide the total number of nickels by the number of nickels in each stack.Since there are 8 nickels in each stack, Lana could make 72/8 = 9 stacks with the 72 extra nickels.Therefore, the final answer is 9. ✔ Original Problem: At the trip to the county-level scavenger hunt competition 90 people were required to split into groups for the competition to begin.To break people up into smaller groups with different leaders 9-person groups were formed.If 3/5 of the number of groups each had members bring back 2 seashells each how many seashells did they bring?GSM8K</p>
<p>Original Solution: Since there were 90 people, and 9 people were in each group, there were 90/9 = 10 groups.3/5 of those groups brought back seashells, which is 3/5 * 10 = 6 groups.Each group brought back 2 seashells, so the total number of seashells brought back is 6 groups * 2 seashells per group = 12 seashells.The final answer is 12.</p>
<p>Reconstructed Problem: There were 90 people on a beach trip, and they split up into groups of 9.</p>
<p>3/5 of the groups brought back seashells, and each group brought back 2 seashells.How many seashells were brought back in total?</p>
<p>Double-Check</p>
<p>Coarse Feedback: You should double-check your answer.</p>
<p>Revised Solution:</p>
<p>I apologize for the mistake in my previous answer.Here's the correct solution:</p>
<p>To form groups of 9 people, divide the total number of people (90) by 9: 90/9 = 10 groups If 3/5 of the groups brought back 2 seashells each, that means:</p>
<p>(3/5) x 10 groups = 6 groups So, 6 groups brought back 2 seashells each: 6 groups x 2 seashells/group = 12 seashells Therefore, the correct answer is 12 seashells.</p>
<p>RCoT</p>
<p>Fine-grained feedback: You used some wrong candidate conditions: 1: 3/5 of the groups brought back seashells, each group brought back 2 seashells.They all can not be deduced from the true conditions.You should consider all real conditions in the problem.</p>
<p>Revised Solution:</p>
<p>Thank you for pointing out my mistake.Here is the corrected answer: Given that 90 people were required to split into groups for the competition to begin and 9-person groups were formed, we can find the total number of groups as: 90/9 = 10.So, there were 10 groups formed.Now, we know that 3/5 of the groups had members bring back 2 seashells each.So, the number of groups that brought back seashells is: 3/5 x 10 = 6 Therefore, 6 groups brought back seashells.</p>
<p>Since each member brought back 2 seashells, the total number of seashells brought back is: 6 groups x 9 members per group x 2 seashells per member = 108 seashells Therefore, they brought back a total of 108 seashells.Original Problem: At the trip to the county-level scavenger hunt competition 90 people were required to split into groups for the competition to begin.To break people up into smaller groups with different leaders 9-person groups were formed.If 3/5 of the number of groups each had members bring back 2 seashells each how many seashells did they bring?GSM8K</p>
<p>Original Solution: Since there were 90 people, and 9 people were in each group, there were 90/9 = 10 groups.3/5 of those groups brought back seashells, which is 3/5 * 10 = 6 groups.Each group brought back 2 seashells, so the total number of seashells brought back is 6 groups * 2 seashells per group = 12 seashells.The final answer is 12.</p>
<p>Reconstructed Problem: There were 90 people on a beach trip, and they split up into groups of 9.</p>
<p>3/5 of the groups brought back seashells, and each group brought back 2 seashells.How many seashells were brought back in total?</p>
<p>Double-Check</p>
<p>Coarse Feedback: You should double-check your answer.</p>
<p>Revised Solution:</p>
<p>I apologize for the mistake in my previous answer.Here's the correct solution:</p>
<p>To form groups of 9 people, divide the total number of people (90) by 9: 90/9 = 10 groups If 3/5 of the groups brought back 2 seashells each, that means:</p>
<p>(3/5) x 10 groups = 6 groups So, 6 groups brought back 2 seashells each: 6 groups x 2 seashells/group = 12 seashells Therefore, the correct answer is 12 seashells.</p>
<p>RCoT</p>
<p>Fine-grained feedback: You used some wrong candidate conditions: 1: 3/5 of the groups brought back seashells, each group brought back 2 seashells.They all can not be deduced from the true conditions.You should consider all real conditions in the problem.</p>
<p>Revised Solution:</p>
<p>Thank you for pointing out my mistake.Here is the corrected answer: Given that 90 people were required to split into groups for the competition to begin and 9-person groups were formed, we can find the total number of groups as: 90/9 = 10.So, there were 10 groups formed.Now, we know that 3/5 of the groups had members bring back 2 seashells each.So, the number of groups that brought back seashells is: 3/5 x 10 = 6 Therefore, 6 groups brought back seashells.</p>
<p>Since each member brought back 2 seashells, the total number of seashells brought back is: 6 groups x 9 members per group x 2 seashells per member = 108 seashells Therefore, they brought back a total of 108 seashells.</p>
<p>C LIMITATIONS AND FUTURE WORK</p>
<p>RCoT can not detect all possible reasoning errors.For example, it is hard for RCoT to detect computational errors.However, RCoT could be combined with other prompting techniques such as Preprint Program-of-Thought (Chen et al., 2022), a method to reduce computational errors through disentangling reasoning and computations.Besides, there is still a significant gap between revising the solutions with RCoT-generated feedback and human feedback, which encourages further exploration in the generation of fine-grained feedback with higher quality.RCoT requires multiple conversations with LLMs (e.g., ChatGPT in our paper) and may thus slow down the inference speed due to the low bandwidth of API calls.Nevertheless, a locally deployed model may alleviate such a problem.</p>
<p>Preprint</p>
<p>In the future, we will explore other applications of RCoT, such as logical reasoning and symbolic reasoning.</p>
<p>Figure 1 :
1
Figure 1: A example of factual inconsistency (condition overlooking).</p>
<p>Few</p>
<p>is an avid gardener.Yesterday, she received 18 new potted plants from her favorite plant nursery.She already has 2 potted plants on each of the 40 window ledges of her large … How many potted plants will Mary remain with?A: Let's think step by step.First,we need to find out how many potted plants Mary currently has.She has 2 potted plants on each of the 40 window ledges,so she has a total of 2 x 40 = 80 potted plants.Next,we need to subtract the number of potted plants she will give … will remain with 80 -40 = 40 potted plants after giving away 1 potted plant from each of the 40 window ledges. Instruction: Give the concrete prompt (problem)…</p>
<p>A:</p>
<p>Thank you for pointing out my mistakes.You are correct that I overlooked the fact that Mary received 18 new potted plants from the nursery.To correct my answer:First,we need to add the 18 new potted plants to the 80 potted plants Mary already has, giving us a total of 98 potted … so she will give away a total of 40 potted plants.Therefore, Mary will remain with 98 -40 = 58 potted plants after giving away 1 potted plant from each of the 40 window ledges.buys 15 oranges.When he gets home … A: 1.Will buys 15 oranges…… The answer is 3 A: First, we need to convert…… The answer is 72 Q: Ashley has an internet connection speed</p>
<p>Figure 4 :
4
Figure 4: The framework of RCoT.(1) Reconstruction: Ask LLMs to reconstruct the problem according to the original solution with instruction and demonstration examples.(2) Decomposition: Decomposing the original problem and reconstructed problem into fine-grained condition lists.(3)Comparison: Compare both lists of sub-conditions and questions to verify whether there are hallucinations, overlookings and misinterpretations.(4)Revision: Gathering all factual inconsistencies into fine-grained feedback to instruct LLMs to revise solutions.</p>
<p>Figure 5 :
5
Figure 5: This is an example showing the failure of the coarse-grained comparison.Red: The inconsistent condition between the original problem and the reconstructed problem.4.5 ADDITIONAL QUANTITATIVE ANALYSIS OF RCOT</p>
<p>First</p>
<p>Figure 6: A example for overlooking</p>
<p>Input = [instruction, original solution] Output = [revised solution]</p>
<p>Figure 20 :
20
Figure 20: This is an example showing the whole process of RCoT.Red: The overlooked condition in the original problem.Blue: The hallucinated condition in the original problem.Green: The correct solution.</p>
<p>Figure 21 :
21
Figure 21: This is an example that Double-Check fails to correct the solution, whereas RCoT succeeded in correcting the solution.Green: The correct solution of the problem.Red: The wrong intermediate step of the original solution.Brown: The factual inconsistencies found by RCoT.</p>
<p>Figure 22 :
22
Figure 22: This is an example that Double-Check fails to correct the solution, whereas RCoT succeeded in correcting the solution.Green: The correct solution of the problem.Red: The wrong intermediate step of the original solution.Brown: The factual inconsistencies found by RCoT.</p>
<p>Figure 23 :
23
Figure 23: All prompts used in RCoT</p>
<p>Original Conditions: Reconstructed Conditions:
OutputRevisionInstruction: Here are the mistakes and reasons inyour answer to the problem…DecompositionInstruction: Please list the conditions of the …Original Problem:Reconstructed Problem:Mary is an avid gardener.Mary has 40 window ledgesYesterday, she received 18in her apartment, and shenew potted plants from herhas 2 potted plants on eachfavorite plant nursery.ledge. {Overlookedpotted plants on each of theCondition} She has decided40 window ledges …to give away 1 potted plantfrom ...Large Language ModelComparisonCondition 1Condition 1Condition 2 she received 18Condition 2new pottedplants from herfavorite plant nursery. … Condition n… Condition mQuestionQuestion</p>
<p>Table 1 :
1
(Diao et al., 2023)d standard deviation on seven arithmetic reasoning datasets.Bold denotes the best result.Green: The performance improvement compared with Standard CoT and Active-Prompting in Zero-shot and Few-shot settings, respectively.<em>denotesthe LLM that uses Manual-CoT.denotes that Active-Prompting(Diao et al., 2023)does not support the dataset in their source codes.
ModelMethodGSM8KAQuAAddSubArithmetic DateSingleEqASDivSVAMPUL2-20B  </em>Standard4.423.618.214.420.216.912.5LaMDA-137B  *Standard14.320.651.926.858.746.637.5Text-davinci-002  *  Standard46.924.881.352.186.671.368.9Zero-shot CoTStandard79.0 ±0.9551.3 ±0.685.2 ±1.266.7 ±1.490.3 ±0.684.3 ±0.476.7 ±4.1ChatGPT+Double-Check79.3 ±2.142.7 ±0.685.6 ±1.260.5 ±6.588.8 ±0.882.8 ±1.477.6 ±2.0+RCoT82.0 ±0.355.5 ±0.887.1 ±1.171.7 ±1.391.4 ±0.886.0 ±0.379.6 ±4.1(+3.1 ±0.6 ) +(4.1 ±0.2 ) +(1.8 ±0.1 ) +(5.0 ±0.4 ) +(1.1 ±0.2 ) +(1.7 ±0.3 ) +(2.8 ±0.2 )Standard36.9 ±0.827.2 ±0.066.7 ±0.552.4 ±1.562.6 ±2.652.2 ±3.738.6 ±1.1LLaMA-13B-Chat+Double-Check35.6 ±1.124.8 ±0.062.0 ±0.727.0 ±0.962.1 ±3.253.2 ±3.641.1 ±0.2+RCoT39.8 ±0.831.9 ±0.067.4 ±0.555.3 ±2.063.5 ±2.153.0 ±3.741.1 ±0.8(+2.9 ±0.4 ) +(4.7 ±0.0 ) +(0.7 ±0.3 ) +(2.9 ±1.0 ) +(0.9 ±0.5 ) +(0.8 ±0.0 ) +(2.5 ±0.4 )Few-shot CoTActive-Prompting81.8 ±0.653.3 ±0.687.2 ±1.2-91.7 ±0.487.9 ±0.882.5 ±0.6ChatGPT+Double-Check77.8 ±0.726.3 ±0.586.0 ±1.6-91.5 ±0.285.7 ±2.482.2 ±0.8+RCoT84.6 ±0.657.1 ±0.388.2 ±1.5-93.0 ±0.889.3 ±0.584.9 ±1.3(+2.7 ±0.1 ) +(3.7 ±0.9 ) +(1.0 ±0.4 )-+(1.2 ±0.4 ) +(1.4 ±0.5 ) +(2.3 ±1.0 )Active-Prompting37.9 ±0.629.1 ±0.068.4 ±0.7-67.9 ±2.253.3 ±0.649.4 ±0.4LLaMA-13B-Chat+Double-Check36.2 ±0.123.2 ±0.061.9 ±2.1-64.9 ±1.350.3 ±3.547.4 ±0.8+RCoT40.1 ±0.430.7 ±0.068.8 ±0.9-68.1 ±2.353.6 ±0.451.2 ±0.3(+2.1 ±0.3 ) +(1.6 ±0.0 ) +(0.4 ±0.3 )-+(0.2 ±0.1 ) +(0.3 ±0.2 ) +(1.8 ±0.2 )</p>
<p>Table 2 :
2
The performance of RCoT using finegrained feedback and coarse-grained feedback.w/o reasons : remove explanations of specific mistakes from the original fine-grained feedback.The prompt becomes "Your answer is wrong.
You should double-check your answer.". w/o judgment+reasons: further remove the high-level judgment. The prompt becomes "You should double-check your answer." Red: The per-formance drops compared with RCoT method.MethodGSM8KAQUASVAMPStandard CoT79.051.376.7RCoT(ours)82.055.579.6-w/o reasons80.0 (-2.0) 52.3 (-3.2) 78.9 (-0.7)-w/o judgment+reasons 79.3 (-2.7) 42.7 (-12.8) 77.6 (-2.0)</p>
<p>Table 3 :
3
The performance without question comparison and condition comparison, as well as the performance with coarse-grained comparison.coarse-grained: We directly ask LLMs to com-
pare the original problem with the reconstructed problem. Results show that (1) fine-grained com-parison is important to get fine-grained feedback, and (2) both question comparison and condi-tion comparison are important in the fine-grained comparison. Red: The performance drops com-pared with the RCoT method.MethodGSM8KAQUASVAMPStandard CoT79.051.376.7RCoT82.055.579.6-w/o question comparison 80.9 (-1.1) 54.6 (-0.9) 79.2 (-0.4)-w/o condition comparison 80.1 (-1.9) 53.5 (-2.0) 78.1 (-1.5)RCoT (Corase-grained)74.2 (-7.8) 49.6 (-5.9) 76.1 (-3.5)</p>
<p>Table 4 :
4
(Madaan et al., 2023)even arithmetic reasoning datasets among Self-Consistency(Wang et al., 2023), RCoT and Self-Refine(Madaan et al., 2023).Bold denotes the best result.
MethodGSM8K AQuA AddSub Date SingleEq ASDiv SVAMP Avg Acc Avg TokensSC (30 trials per problem)81.670.888.680.092.990.280.483.55615.0RCoT (1 trial per problem)82.056.387.271.992.486.379.779.41831.0RCoT (3 trials per problem)83.272.889.878.993.891.881.284.55453.3attempt 079.145.290.651.397.683.575.274.7190.2attempt 180.749.291.452.798.084.376.876.13108.4Self-Refineattempt 2 attempt 380.7 80.749.2 49.291.4 91.452.7 52.798.0 98.084.3 84.376.8 76.876.1 76.13324.9 3359.6attempt 480.749.291.452.798.084.376.876.13367.7attempt 580.749.291.452.798.084.376.876.13367.74.3 FINE-GRAINED FEEDBACK IS CRITICAL FOR SOLUTION REVISION</p>
<p>Table 5 :
5
Found/Not Found: RCoT can or cannot find the reasons for errors.Other errors: such as computation error, logical error and so on.
TypeFound Not Found totalOverlooking516Hallucinating161531Misinterpreting538Other errors05555</p>
<p>Table 6 :
6
The metrics between original problem and reconstructed problem
DatesetRouge1 Rouge2 RougeL RougeSum Similarity Standard CoT AccGSM8K 71.4622 49.3915 58.899158.897493.5779.0AQuA54.2383 33.5828 43.577143.772184.4451.3AddSub78.0939 57.5594 66.397366.409994.0585.2Date46.9414 28.7005 39.226839.393479.1266.7SingleEq 72.8212 53.5232 64.856264.795694.0190.3ASDiv68.1849 46.5488 59.790759.791192.1684.3SVAMP75.0074 55.3162 65.153965.197393.7176.7B EXAMPLESB.1 EXAMPLES FOR DIFFERENT KINDS OF FACTUAL INCONSISTENCIES FOUND BY RCOTOriginal Solution:</p>
<p>First, we need to figure out what day was yesterday.To do that, we subtract one day from 11/23/1933, which gives us 11/22/1933.Next, we need to figure out what day is today.To do that, we subtract one more day from 11/22/1933, which gives us 11/21/1933.Therefore, the answer is (E) 08/29/1933.✗
PreprintOriginal Solution:09/01/2020 (E) 09/07/2021 (F) 09/01/209809/01/2021 (B) 12/06/2021 (C) 09/02/2021 (D)Reconstructed Problem: What is the date that was one week ago from today's date, which is September 1st, 2021? Choose the correct answer from the following options: (A) 09/01/2021 (B) 12/06/2021 (C) 09/02/2021 (D) 09/01/2020 (E) 09/07/2021 (F) 09/01/2098Figure 14: A example for hallucinationOriginal Solution:Original Problem: The day before yesterday was 11/23/1933. What is the date today in MM/DD/YYYY? Answer Choices: (A) 01/04/1934 (B) 11/04/1933 (C) 11/25/1967 (D) 12/09/1933 (E) 08/29/1933 (F) 11/25/1933Reconstructed Problem: If today is 11/23/1933, what is the date today? Choose the cor-rect answer from the following options: (A) 01/04/1934 (B) 11/04/1933 (C) 11/25/1967 (D) 12/09/1933 (E) 08/29/1933 (F) 11/25/1933Figure 15: A example for hallucinating
The correct answer is (E) 09/07/2021.✗Original Problem: It was Sept.1st, 2021 a week ago.What is the date one week ago from today in MM/DD/YYYY?Answer Choices: (A)</p>
<p>EXAMPLES THAT COMPARE RCOT FEEDBACK WITH HUMAN FEEDBACKOriginal Problem: Kelly has 22 nintendo games.How many does she need to buy so that she will have 140 games left?Original Solution: To calculate how many Nintendo games Kelly needs to buy so that she will have 140 games left, we need to subtract the number of games she currently has from the desired total number of games.The desired total number of games is 140 + 22 = 162 games.Therefore, Kelly needs to buy 162 -22 = 140 Nintendo games.Therefore, the final answer is 140.Reconstructed Problem: Kelly currently has 22 Nintendo games.She wants to have 140 more games than she currently has.How many Nintendo games does Kelly need to buy to reach her goal?
PreprintB.2Figure 17: A example for misinterpretation</p>
<p>To calculate how many Nintendo games Kelly needs to buy so that she will have 140 games left, we need to subtract 140 from the number of games she currently has.So, if Kelly currently has 22 Nintendo games and wants to have 140 games left, we can calculate the number of games she needs to buy as follows: 140 (desired number of games left) -22 (current number of games) = 118 Therefore, Kelly needs to buy 118 Nintendo games to have 140 games left.✔Figure18:Thisexampleshows that feedback generated by RCoT sometimes has the same quality as the human-written feedback.PreprintOriginal Problem: There are 11 different books and 17 different movies in the ' crazy silly school ' series.If you read 7 of the books and watched 21 of the movies How many more movies than books have you read?Original Solution: First, we need to find out how many books and movies are left unread/watched ...... Let's assume that the person has actually watched 17 movies, but some of them were repeats.So, we can say that they have watched 17 unique movies and some of them multiple times.Therefore, the person has watched 10 more movies than books they have read.Reconstructed Problem: A person has 11 books and 21 movies.They have read/watched 7 books and 17 movies.How many more movies than books have they watched?Here are the mistakes and reasons in your answer to the problem.The candidate condition states that the person has 11 books and 21 movies, but the condition list only tells us that there are 11 books and 17 movies in the series, and that the person has read 7 of the books and watched 21 of the movies.We cannot deduce that the person has all 11 books just because there are 11 books in the series.Therefore, the candidate condition cannot be deduced from the condition list.The real problem has the conditions: 1: There are 11 different books and 17 different movies in the 'crazy silly school' series.2: You read 7 of the books and watched 21 of the movies.You should consider all real conditions in the problem.
RCoTFine-grained Feedback:You use some wrong candidate conditions: 1: The person has 11 books and 21 movies. They all can not be deduced from the true condition list. Here are detailed reasons: 1: Revised Solution:
I apologize for the mistakes in my previous answer.Here is the corrected answer: The true problem has the condition: There are 11 different books and 17 different movies in the 'crazy silly school' series.To find out how many more movies than books you have read ...... Let's assume that you have actually watched 17 movies, but some of them were repeats.So, we can say that you have watched 17 unique movies and some of them multiple times.Therefore, you have watched 10 more movies than books you have read.✗</p>
<p>To find out how many more movies than books you have read, we simply need to subtract the number of books you have read from the number of movies you have watched: Number of movies watched -Number of books read = 21 -7 = 14 Therefore, you have watched 14 more movies than books you have read.</p>
<p>✔ Figure 19: This example shows RCoT sometimes cannot perform as well as humans.</p>
<p>https://github.com/openai/tiktoken
Original Solution:First, we need to find out how far John traveled during the first 3 hours at a speed of 60 mph.We can use the formula distance = speed x time, so: distance = 60 mph x 3 hours = 180 miles Next, we need to find out how far John traveled during the next 2 hours in standstill traffic.Since he didn't move, the distance is 0 miles.Then, we need to find out how far John traveled during the next half-hour at a speed of 30 mph.Using the same formula, we get: distance = 30 mph x 0.5 hours = 15 miles Finally, we need to find out how far John traveled during the last 1.5 hours at a speed of 80 mph.Again, using the same formula, we get: distance = 80 mph x 1.5 hours = 120 miles To find out how far John is from home at the end of those 4 hours, we need to add up all the distances he traveled: 180 miles + 0 miles + 15 miles + 120 miles = 315 miles Therefore, John is 315 miles away from home at the end of those 4 hours.✗Original Problem: John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home.He tries to get home in 4 hours but spends the first 2 hours in standstill traffic.He spends the next half-hour driving at a speed of 30mph, before being able to drive the remaining time of the 4 hours going at 80 mph.How far is he from home at the end of those 4 hours?Reconstructed Problem: John drove for 4 hours.During the first 3 hours, he drove at a speed of 60 mph.During the next 2 hours, he was stuck in standstill traffic.During the next half-hour, he drove at a speed of 30 mph.During the last 1.5 hours, he drove at a speed of 80 mph.How far is John from home at the end of those 4 hours?Figure9: A example for overlookingOriginal Solution:First, we need to find out how many books and movies are left unread/watched.Books left unread: 11 -7 = 4 Movies left unwatched: 17 -21 = -4 Uh oh, we have a problem.We can't have negative movies left unwatched, so we need to adjust our calculations.Let's assume that the person has actually watched 17 movies, but some of them were repeats.So, we can say that they have watched 17 unique movies and some of them multiple times.Now, let's recalculate: Books left unread: 11 -7 = 4 Movies left unwatched: 17 -17 = 0 So, the person has read 7 books and watched 17 unique movies.To find out how many more movies than books they have watched, we can subtract: 17 -7 = 10 Therefore, the person has watched 10 more movies than books they have read.✗Original Solution:First, we know that the year is 1969, so we don't need to worry about that.Next, we know that it is currently 4/19, but we need to figure out what day it will be tomorrow.To do this, we add 1 to the day, which gives us 4/20.ThereforeOriginal Solution:First, we need to calculate the total number of petals on the flowers that Rose picked: The 3 flowers with 5 petals each have a total of 3 x 5 = 15 petals.The 4 flowers with 6 petals each have a total of 4 x 6 = 24 petals.The 5 flowers with 4 petals each have a total of 5 x 4 = 20 petals.The 6 flowers with 7 petals each have a total of 6 x 7 = 42 petals.Therefore, the total number of petals on all the flowers that Rose picked is 15 + 24+ 20 + 42 = 101 petals.Next, we need to subtract one petal from each type of flower that she dropped.Sowe need to subtract: 1 petal from the 3 flowers with 5 petals each, leaving 3 x 4 = 12 petals.1 petal from the 4 flowers with 6 petals each, leaving 4 x 5 = 20 petals. 1 petal from the 5 flowers with 4 petals each, leaving 5 x 3 = 15 petals. 1 petal from the 6 flowers with 7 petals each, leaving 6 x 6 = 36 petals.Therefore, the total number of petals on the flowers that Rose puts in the vase is 12+ 20 + 15 + 36 = 83 petals.✗Original Problem: Rose is out picking flowers for a vase she wants to fill.She starts off by picking 3 flowers with 5 petals each.She then picks 4 flowers with 6 petals each.She then adds another 5 flowers with 4 petals each.Lastly she picks 6 flowers with 7 petals each.As she's carrying these flowers over to fill the vase, she drops 1 of each and the wind blows them away.She puts the remnaining flowers in the vase.How many petals in total are on the flowers in the vase?Reconstructed Problem: Rose went flower-picking and collected different types of flowers with varying numbers of petals.She picks 3 flowers with 5 petals each, 4 flowers with 6 petals each, 5 flowers with 4 petals each and 6 flowers with 7 petals each.She later dropped one petal from each type of flower and put the remaining flowers in a vase.What is the total number of petals on the flowers that she put in the vase?Original Solution: To calculate how many stacks Lana could make with 72 extra nickels, we need to divide the total number of nickels by the number of nickels in each stack.Since there are 5 nickels in each stack, 72 extra nickels is equal to 72/5 = 14.4 stacks.However, Lana cannot have a fraction of a stack, so we need to round down to the nearest whole number.Therefore, Lana could make 14 stacks with the 72 extra nickels.Therefore, the final answer is 14. ✗Instruction: Give the concrete prompt (problem) that can generate this answer.The problem should contain all basic and necessary information and correspond to the answer.The problem can only ask for one result.Reconstructed Problem: Lana has 72 extra nickels and wants to stack them in piles of 5. How many stacks of nickels can she make?Step2: Fine-grained Comparison Overlooked conditions: Instruction: Given a candidate condition: "She put them into stacks with 8 in each stack."Here is a condition list: "1: Lana has 72 extra nickels.2: She wants to stack them in piles of 5." From a mathematical point of view, can this candidate condition be deduced from the condition list?Please illustrate your reason and answer "yes" or "no".hallucinated conditions: Instruction: Given a candidate condition: "She wants to stack them in piles of 5."Here is a condition list: "1: Lana had 72 extra nickels.2: She put them into stacks with 8 in each stack."From a mathematical point of view, can this candidate condition be deduced from the condition list?Please illustrate your reason and answer "yes" or "no".
Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Wen-Tau Yih, Yejin Choi, arXiv:1908.05739Abductive commonsense reasoning. 2019arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 2020a33</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, 2020bScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. Language models are few-shot learners</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2022</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021a</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021barXiv preprint</p>
<p>Active prompting with chain-of-thought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, 2023</p>
<p>Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 2019</p>
<p>Model inversion attacks that exploit confidence information and basic countermeasures. Matt Fredrikson, Somesh Jha, Thomas Ristenpart, Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. the 22nd ACM SIGSAC conference on computer and communications security2015</p>
<p>Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, Thomas Ristenpart, 23rd {USENIX} Security Symposium. 201414</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.007202022arXiv preprint</p>
<p>Normsage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly. Preprint Yi, R Fung, Tuhin Chakraborty, Owen Rambow, Smaranda Muresan, Heng Ji, arxiv2022</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 2023</p>
<p>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg, 2022</p>
<p>Roscoe: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, 2022</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, 10.3115/v1/D14-1058Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsOctober 2014</p>
<p>Reverse engineering convolutional neural networks through side-channel information leaks. Weizhe Hua, Zhiru Zhang, Edward Suh, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation Conference2018</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/v1/N16-1136Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsJune 2016</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational LinguisticsDeeLIO 2022. May 2022Proceedings of Deep Learning Inside Out</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, 2023</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, 2021</p>
<p>MetaICL: Learning to learn in context. Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi, doi: 10.18653Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>URL. </p>
<p>Pathways language model (palm): Scaling to 540 billion parameters for breakthrough performance. Sharan Preprint, Aakanksha Narang, Chowdhery, 2022</p>
<p>Towards reverse-engineering black-box neural networks. Joon Seong, Bernt Oh, Mario Schiele, Fritz, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. 2019</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computational LinguisticsJune 2021</p>
<p>URL. </p>
<p>Refiner: Reasoning feedback on intermediate representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, 2023</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, arXiv:1608.014132016arXiv preprint</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, 2022</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, 2023</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, 2023</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard De Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Preprint Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, D Kaustubh, Kevin Dhole, Kevin Gimpel, Kory Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mcdonell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros Colón, Metz ; Maheen, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Maru, Jose Ramírez, Marie Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Mátyás Hagen, Medina Schubert, Melody Orduna Baitemirova, Melvin Arnaud, Michael A Mcelrath, Michael Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michał Strube, Michele Swędrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T Varma, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Niveditha, S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi ; Ryan Teehan, Rylan Yang, Sahib Singh, Saif M Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, R Samuel, Samuel S Bowman, Sanghyun Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Shyam Toshniwal, Upadhyay, Shyamolima, Siamak Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Reddy, Priscilla Sneha, Soo-Hwan Makini, Spencer Lee, Sriharsha Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Ermon, Stephanie Biderman, Stephen Lin, Steven T Prasad, Stuart M Piantadosi, Summer Shieber, Svetlana Misherghi, Swaroop Kiritchenko, Mishra ; Xinran, Xinyi Zhao, Xudong Wu, Yadollah Shen, Yair Yaghoobzadeh, Yangqiu Lakretz, Yasaman Song, Yejin Bahri, Yichi Choi, Yiding Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Ziyi Wang, Wu, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve. Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Omer Levy, Owain Evans, Pablo Antonio Moreno Casares; Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto; Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton ChangTe-Lin Wu2022Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu. Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong,. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, 2023</p>
<p>Stealing machine learning models via prediction {APIs}. Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, Thomas Ristenpart, 25th USENIX Security Symposium (USENIX Security Preprint 16). 2016</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao, 2023</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. Xi Ye, Greg Durrett ; Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, arXiv:2002.04326Reclor: A reading comprehension dataset requiring logical reasoning. 2022. 2020arXiv preprint</p>
<p>SWAG: A large-scale adversarial dataset for grounded commonsense inference. Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, 10.18653/v1/D18-1009Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober-November 2018</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, abs/2205.010682022aArXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, 2022b</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. 2023</p>
<p>Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, Tat-Seng Chua, arXiv:2105.076242021arXiv preprint</p>
<p>Preprint Original Solution: First, we need to find the date that is one month before the deadline of. June 1, 2021. June 1, 2021 -1 month = May 1, 2021To do this, we can subtract one month from the deadline date</p>
<p>the date that is one month before the deadline is May 1, 2021. Next, we need to write this date in the format MM/DD/YYYY. May 1, 2021 = 05/01/2021</p>
<p>Original Problem: The deadline is Jun 1, 2021, which is 2 days away from now. What is the date a month. 05/09/2021 (E) 07/06/2021 (F) 08/03/2021MM/DD/YYYY? Answer Choices. </p>            </div>
        </div>

    </div>
</body>
</html>
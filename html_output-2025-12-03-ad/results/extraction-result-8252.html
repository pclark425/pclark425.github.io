<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8252 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8252</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8252</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-279250863</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07106v2.pdf" target="_blank">Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8252.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8252.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToTh</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theorem-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent, graph-based LLM reasoning framework that instantiates three specialized agents (abductive, deductive, inductive), converts each agent's reasoning trace into a Formal Reasoning Graph (FRG), scores graphs using NLI-calibrated Bayesian belief propagation, and selects the most coherent graph to produce the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B; DeepSeek-7B; Phi-3.5 Mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three publicly available LLMs used in experiments: Mistral-7B (decoder model, efficient scaling, reasoning-focused pretraining), DeepSeek-7B (instruction-tuned, RL/ alignment emphasis for multi-turn reasoning), Phi-3.5 Mini (lightweight, educational/step-by-step curriculum). All evaluated in released form without further fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['abductive reasoning (agent a1)', 'deductive reasoning (agent a2)', 'inductive reasoning (agent a3)', 'formal reasoning graphs (FRG)', 'NLI-calibrated Bayesian belief propagation for internal consistency']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Three independent solver agents are prompted to follow distinct reasoning 'styles' (abduction: infer hypotheses given observations; deduction: derive conclusions from premises; induction: generalize rules from examples). Each agent outputs a trace transformed into a directed Formal Reasoning Graph (nodes = steps, edges = inferred inferential relations). A pretrained NLI model labels edge relations (entailment/neutral/contradiction) mapped to trust scores (0.95/0.60/0.10). Beliefs start at 0.5 and are propagated using Bayesian updates across parents (single-parent closed-form, multi-parent averaged updates). Graphs are scored by Score = mean node belief − normalized entropy; the highest-scoring graph's terminal node yields the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>ToTh explicitly instantiates diverse reasoning via three specialized agents (abduction, deduction, induction) and compares the selected multi-agent FRG output to single-stream and sampling-based baselines (CoT-Greedy, Self-Consistency with n=20 samples, CoT-Decoding) across tasks and models. No internal ablation (e.g., removing or varying number of agents) is reported; the paper notes this as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>WebOfLies (symbolic logical truth assignment; BIG-Bench-Hard subset) and MultiArith (multi-step arithmetic word problems). These datasets probe symbolic logical entanglement and compositional numerical reasoning respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Across Mistral-7B and DeepSeek-7B, ToTh consistently outperforms CoT-Greedy, Self-Consistency, and CoT-Decoding on both WebOfLies and MultiArith. Examples: On WebOfLies, ToTh improves over CoT-Greedy by 29% (Mistral-7B) and 14% (DeepSeek-7B). On Phi-3.5 Mini, CoT-Decoding achieved 99% on WebOfLies while ToTh achieved 96% (Phi-3.5 Mini). For Mistral-7B stratified by difficulty: symbolic (5 statements) ToTh = 43%, CoT-Greedy = 19%, Self-Consistency = 38%, CoT-Decoding = 46%; numerical: ToTh at d0/l3 = 59%, d0/l4 = 45%, and at d2/l3 = 21% (CoT-Decoding = 24%). Overall table summary: ToTh achieved top accuracy in 5 of 6 difficulty settings for Mistral-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The multi-agent diverse reasoning design yields more interpretable, logically-grounded chains (FRGs) and improved robustness to reasoning complexity. NLI-guided belief propagation provides internal consistency scoring that rewards agreement across steps and penalizes contradictions. Self-Consistency (majority vote over stochastic traces) underperforms on logic-heavy symbolic tasks, suggesting sampling-based majority voting fails to capture structured dependencies. ToTh shows consistent cross-model performance even at smaller scales; however, the approach is sensitive to propagation noise and the fixed three-way decomposition may be suboptimal for some inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Diverse, specialized reasoning agents (abduction, deduction, induction) composed and vetted via graph-structured verification provide more robust and interpretable reasoning than single-stream CoT prompting or sampling-based self-consistency; ToTh consistently outperforms or matches state-of-the-art baselines across symbolic and numerical benchmarks and across multiple LLMs, demonstrating that reasoning-method diversity (structured multi-paradigm inference) improves performance and consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8252.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8252.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate, linear reasoning steps from LLMs to improve multi-step problem solving by forcing decomposition into an explicit chain of inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B; DeepSeek-7B; Phi-3.5 Mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same three LLMs evaluated under CoT prompting (no fine-tuning); CoT used with greedy decoding baseline (CoT-Greedy) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought linear decomposition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompts the model to generate step-by-step intermediate reasoning before answering; implemented in experiments as CoT-Greedy (single deterministic chain) and compared as a baseline to ToTh.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single method, single linear trace)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>CoT-Greedy (single deterministic CoT trace) is used as a baseline against ToTh's multi-agent FRG approach; no internal CoT ablation reported beyond comparing greedy CoT to self-consistency and CoT-Decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>WebOfLies and MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT-Greedy performs substantially worse than ToTh on both tasks in multiple settings. Example: On WebOfLies (Mistral-7B) CoT-Greedy = 19% (vs ToTh 43% in hardest setting); overall CoT-Greedy is reported with lower accuracy across models and difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Linear single-stream CoT can produce fluent but logically unsound chains and lacks mechanisms for formal consistency checking; tends to underperform when structured dependencies and logical entanglement are required.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Single-stream CoT is less reliable on complex symbolic and compositional numerical problems compared to a diversified, graph-verified multi-agent approach like ToTh; CoT lacks explicit internal verification mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8252.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8252.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over sampled CoT traces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robustness technique that samples multiple (n=20 in this paper) reasoning traces from the model and selects the most frequent final answer under the assumption that consistent answers across samples are more likely correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B; DeepSeek-7B; Phi-3.5 Mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied with n=20 sampled completions per input; decoding temperature 0.7; used without fine-tuning as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['sampling-based multiple-chain aggregation (majority voting over CoT samples)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generates n stochastic completions with chain-of-thought prompting and picks the most frequent final answer (majority vote). Implemented in experiments with n=20 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (multiple samples of the same CoT-style reasoning, i.e., repeated similar method)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared to ToTh's three distinct agents; Self-Consistency uses multiple sampled chains of the same reasoning style (CoT) and aggregates answers by majority. Reported as a baseline; the paper notes that Self-Consistency under-performs in logic-heavy tasks and provides numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>WebOfLies and MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Self-Consistency under-performs across settings: e.g., yields only 14% on WebOfLies (DeepSeek-7B) and 21% on MultiArith (Mistral-7B) in reported examples. In difficulty-stratified Mistral-7B results, Self-Consistency scored 38% on the hardest symbolic setting versus ToTh's 43% and CoT-Decoding's 46%.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Majority vote over stochastic CoT traces fails to capture structured logical dependencies and can produce low accuracy on symbolic tasks; suggests sampling homogenous reasoning traces does not substitute for structurally diverse inference or explicit internal verification.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Sampling-based aggregation of similar reasoning traces (Self-Consistency) is insufficient for structure-sensitive reasoning tasks; diverse, structured reasoning with internal consistency checking (ToTh) outperforms majority-vote sampling in logic-heavy benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8252.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8252.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Decoding without explicit prompting (CoT-Decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding technique that attempts to elicit latent chain-of-thought reasoning behavior by using diverse decoding paths rather than explicit CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought reasoning without prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B; DeepSeek-7B; Phi-3.5 Mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a baseline using diverse decoding strategies (no explicit CoT prompt); reported performance varies by model, sometimes outperforming ToTh on specific model-task combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['latent CoT elicitation via diverse decoding paths']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Stimulates internal latent reasoning by varying decoding trajectories rather than providing step-by-step prompts; intended to produce diverse reasoning behavior without explicit intermediate-step prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (diverse decoding aims to elicit varied latent traces, but not structured multi-paradigm agents)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared against ToTh and other baselines: CoT-Decoding achieved near-perfect WebOfLies scores on Phi-3.5 Mini (99%) and sometimes outperformed ToTh on specific instances, but ToTh was more consistent across different models. No internal ablation isolating decoding diversity vs. method diversity is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>WebOfLies and MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT-Decoding achieved 99% on WebOfLies with Phi-3.5 Mini; for Mistral-7B difficulty-stratified results CoT-Decoding = 46% on hardest symbolic setting versus ToTh = 43%. On numerical d2/l3 CoT-Decoding = 24% vs ToTh = 21% (Mistral-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Diverse decoding paths can elicit strong latent reasoning behavior for some models (notably Phi-3.5 Mini), but performance is less consistent across models compared to ToTh; lacks explicit graph-based verification and differentiation between reasoning paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Diverse decoding can sometimes match or exceed structured multi-agent methods on specific models/tasks, but structured multi-paradigm reasoning with explicit verification (ToTh) gives more consistent performance across model families and benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought reasoning without prompting. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>GoT: Effective graph-of-thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Challenging BIGbench tasks and whether chain-of-thought can solve them. <em>(Rating: 1)</em></li>
                <li>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8252",
    "paper_id": "paper-279250863",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "ToTh",
            "name_full": "Theorem-of-Thought",
            "brief_description": "A multi-agent, graph-based LLM reasoning framework that instantiates three specialized agents (abductive, deductive, inductive), converts each agent's reasoning trace into a Formal Reasoning Graph (FRG), scores graphs using NLI-calibrated Bayesian belief propagation, and selects the most coherent graph to produce the final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B; DeepSeek-7B; Phi-3.5 Mini",
            "model_description": "Three publicly available LLMs used in experiments: Mistral-7B (decoder model, efficient scaling, reasoning-focused pretraining), DeepSeek-7B (instruction-tuned, RL/ alignment emphasis for multi-turn reasoning), Phi-3.5 Mini (lightweight, educational/step-by-step curriculum). All evaluated in released form without further fine-tuning.",
            "reasoning_methods": [
                "abductive reasoning (agent a1)",
                "deductive reasoning (agent a2)",
                "inductive reasoning (agent a3)",
                "formal reasoning graphs (FRG)",
                "NLI-calibrated Bayesian belief propagation for internal consistency"
            ],
            "reasoning_methods_description": "Three independent solver agents are prompted to follow distinct reasoning 'styles' (abduction: infer hypotheses given observations; deduction: derive conclusions from premises; induction: generalize rules from examples). Each agent outputs a trace transformed into a directed Formal Reasoning Graph (nodes = steps, edges = inferred inferential relations). A pretrained NLI model labels edge relations (entailment/neutral/contradiction) mapped to trust scores (0.95/0.60/0.10). Beliefs start at 0.5 and are propagated using Bayesian updates across parents (single-parent closed-form, multi-parent averaged updates). Graphs are scored by Score = mean node belief − normalized entropy; the highest-scoring graph's terminal node yields the answer.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "ToTh explicitly instantiates diverse reasoning via three specialized agents (abduction, deduction, induction) and compares the selected multi-agent FRG output to single-stream and sampling-based baselines (CoT-Greedy, Self-Consistency with n=20 samples, CoT-Decoding) across tasks and models. No internal ablation (e.g., removing or varying number of agents) is reported; the paper notes this as future work.",
            "task_or_benchmark": "WebOfLies (symbolic logical truth assignment; BIG-Bench-Hard subset) and MultiArith (multi-step arithmetic word problems). These datasets probe symbolic logical entanglement and compositional numerical reasoning respectively.",
            "performance_results": "Across Mistral-7B and DeepSeek-7B, ToTh consistently outperforms CoT-Greedy, Self-Consistency, and CoT-Decoding on both WebOfLies and MultiArith. Examples: On WebOfLies, ToTh improves over CoT-Greedy by 29% (Mistral-7B) and 14% (DeepSeek-7B). On Phi-3.5 Mini, CoT-Decoding achieved 99% on WebOfLies while ToTh achieved 96% (Phi-3.5 Mini). For Mistral-7B stratified by difficulty: symbolic (5 statements) ToTh = 43%, CoT-Greedy = 19%, Self-Consistency = 38%, CoT-Decoding = 46%; numerical: ToTh at d0/l3 = 59%, d0/l4 = 45%, and at d2/l3 = 21% (CoT-Decoding = 24%). Overall table summary: ToTh achieved top accuracy in 5 of 6 difficulty settings for Mistral-7B.",
            "qualitative_findings": "The multi-agent diverse reasoning design yields more interpretable, logically-grounded chains (FRGs) and improved robustness to reasoning complexity. NLI-guided belief propagation provides internal consistency scoring that rewards agreement across steps and penalizes contradictions. Self-Consistency (majority vote over stochastic traces) underperforms on logic-heavy symbolic tasks, suggesting sampling-based majority voting fails to capture structured dependencies. ToTh shows consistent cross-model performance even at smaller scales; however, the approach is sensitive to propagation noise and the fixed three-way decomposition may be suboptimal for some inputs.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Diverse, specialized reasoning agents (abduction, deduction, induction) composed and vetted via graph-structured verification provide more robust and interpretable reasoning than single-stream CoT prompting or sampling-based self-consistency; ToTh consistently outperforms or matches state-of-the-art baselines across symbolic and numerical benchmarks and across multiple LLMs, demonstrating that reasoning-method diversity (structured multi-paradigm inference) improves performance and consistency.",
            "uuid": "e8252.0",
            "source_info": {
                "paper_title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "A prompting technique that elicits intermediate, linear reasoning steps from LLMs to improve multi-step problem solving by forcing decomposition into an explicit chain of inference.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "Mistral-7B; DeepSeek-7B; Phi-3.5 Mini",
            "model_description": "Same three LLMs evaluated under CoT prompting (no fine-tuning); CoT used with greedy decoding baseline (CoT-Greedy) in experiments.",
            "reasoning_methods": [
                "chain-of-thought linear decomposition"
            ],
            "reasoning_methods_description": "CoT prompts the model to generate step-by-step intermediate reasoning before answering; implemented in experiments as CoT-Greedy (single deterministic chain) and compared as a baseline to ToTh.",
            "reasoning_diversity": "similar (single method, single linear trace)",
            "reasoning_diversity_experimental_setup": "CoT-Greedy (single deterministic CoT trace) is used as a baseline against ToTh's multi-agent FRG approach; no internal CoT ablation reported beyond comparing greedy CoT to self-consistency and CoT-Decoding.",
            "task_or_benchmark": "WebOfLies and MultiArith",
            "performance_results": "CoT-Greedy performs substantially worse than ToTh on both tasks in multiple settings. Example: On WebOfLies (Mistral-7B) CoT-Greedy = 19% (vs ToTh 43% in hardest setting); overall CoT-Greedy is reported with lower accuracy across models and difficulty levels.",
            "qualitative_findings": "Linear single-stream CoT can produce fluent but logically unsound chains and lacks mechanisms for formal consistency checking; tends to underperform when structured dependencies and logical entanglement are required.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Single-stream CoT is less reliable on complex symbolic and compositional numerical problems compared to a diversified, graph-verified multi-agent approach like ToTh; CoT lacks explicit internal verification mechanisms.",
            "uuid": "e8252.1",
            "source_info": {
                "paper_title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority-vote over sampled CoT traces)",
            "brief_description": "A robustness technique that samples multiple (n=20 in this paper) reasoning traces from the model and selects the most frequent final answer under the assumption that consistent answers across samples are more likely correct.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "Mistral-7B; DeepSeek-7B; Phi-3.5 Mini",
            "model_description": "Applied with n=20 sampled completions per input; decoding temperature 0.7; used without fine-tuning as a baseline.",
            "reasoning_methods": [
                "sampling-based multiple-chain aggregation (majority voting over CoT samples)"
            ],
            "reasoning_methods_description": "Generates n stochastic completions with chain-of-thought prompting and picks the most frequent final answer (majority vote). Implemented in experiments with n=20 samples.",
            "reasoning_diversity": "similar (multiple samples of the same CoT-style reasoning, i.e., repeated similar method)",
            "reasoning_diversity_experimental_setup": "Compared to ToTh's three distinct agents; Self-Consistency uses multiple sampled chains of the same reasoning style (CoT) and aggregates answers by majority. Reported as a baseline; the paper notes that Self-Consistency under-performs in logic-heavy tasks and provides numeric results.",
            "task_or_benchmark": "WebOfLies and MultiArith",
            "performance_results": "Self-Consistency under-performs across settings: e.g., yields only 14% on WebOfLies (DeepSeek-7B) and 21% on MultiArith (Mistral-7B) in reported examples. In difficulty-stratified Mistral-7B results, Self-Consistency scored 38% on the hardest symbolic setting versus ToTh's 43% and CoT-Decoding's 46%.",
            "qualitative_findings": "Majority vote over stochastic CoT traces fails to capture structured logical dependencies and can produce low accuracy on symbolic tasks; suggests sampling homogenous reasoning traces does not substitute for structurally diverse inference or explicit internal verification.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Sampling-based aggregation of similar reasoning traces (Self-Consistency) is insufficient for structure-sensitive reasoning tasks; diverse, structured reasoning with internal consistency checking (ToTh) outperforms majority-vote sampling in logic-heavy benchmarks.",
            "uuid": "e8252.2",
            "source_info": {
                "paper_title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "CoT-Decoding",
            "name_full": "Chain-of-Thought Decoding without explicit prompting (CoT-Decoding)",
            "brief_description": "A decoding technique that attempts to elicit latent chain-of-thought reasoning behavior by using diverse decoding paths rather than explicit CoT prompts.",
            "citation_title": "Chain-of-thought reasoning without prompting.",
            "mention_or_use": "use",
            "model_name": "Mistral-7B; DeepSeek-7B; Phi-3.5 Mini",
            "model_description": "Applied as a baseline using diverse decoding strategies (no explicit CoT prompt); reported performance varies by model, sometimes outperforming ToTh on specific model-task combinations.",
            "reasoning_methods": [
                "latent CoT elicitation via diverse decoding paths"
            ],
            "reasoning_methods_description": "Stimulates internal latent reasoning by varying decoding trajectories rather than providing step-by-step prompts; intended to produce diverse reasoning behavior without explicit intermediate-step prompting.",
            "reasoning_diversity": "both (diverse decoding aims to elicit varied latent traces, but not structured multi-paradigm agents)",
            "reasoning_diversity_experimental_setup": "Compared against ToTh and other baselines: CoT-Decoding achieved near-perfect WebOfLies scores on Phi-3.5 Mini (99%) and sometimes outperformed ToTh on specific instances, but ToTh was more consistent across different models. No internal ablation isolating decoding diversity vs. method diversity is reported.",
            "task_or_benchmark": "WebOfLies and MultiArith",
            "performance_results": "CoT-Decoding achieved 99% on WebOfLies with Phi-3.5 Mini; for Mistral-7B difficulty-stratified results CoT-Decoding = 46% on hardest symbolic setting versus ToTh = 43%. On numerical d2/l3 CoT-Decoding = 24% vs ToTh = 21% (Mistral-7B).",
            "qualitative_findings": "Diverse decoding paths can elicit strong latent reasoning behavior for some models (notably Phi-3.5 Mini), but performance is less consistent across models compared to ToTh; lacks explicit graph-based verification and differentiation between reasoning paradigms.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Diverse decoding can sometimes match or exceed structured multi-agent methods on specific models/tasks, but structured multi-paradigm reasoning with explicit verification (ToTh) gives more consistent performance across model families and benchmarks.",
            "uuid": "e8252.3",
            "source_info": {
                "paper_title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought reasoning without prompting.",
            "rating": 2,
            "sanitized_title": "chainofthought_reasoning_without_prompting"
        },
        {
            "paper_title": "Tree of thoughts: deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "GoT: Effective graph-of-thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "got_effective_graphofthought_reasoning_in_language_models"
        },
        {
            "paper_title": "Challenging BIGbench tasks and whether chain-of-thought can solve them.",
            "rating": 1,
            "sanitized_title": "challenging_bigbench_tasks_and_whether_chainofthought_can_solve_them"
        },
        {
            "paper_title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.",
            "rating": 1,
            "sanitized_title": "probabilistic_reasoning_in_intelligent_systems_networks_of_plausible_inference"
        }
    ],
    "cost": 0.011786499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models
31 Jul 2025</p>
<p>Samir Abdaljalil sabdaljalil@tamu.edu 
Texas A&amp;M University
College StationTXUSA</p>
<p>Hasan Kurban hkurban@hbku.edu.qa 
Hamad Bin Khalifa University
DohaQatar</p>
<p>Khalid Qaraqe 
Hamad Bin Khalifa University
DohaQatar</p>
<p>Erchin Serpedin 
Texas A&amp;M University
College StationTXUSA</p>
<p>Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models
31 Jul 2025A0EF6C64C4BEB7AB6B5A58F00A5F825EarXiv:2506.07106v2[cs.CL]
Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret.Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs.However, they lack mechanisms for enforcing logical structure and assessing internal coherence.We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive.Each agent produces a reasoning trace, which is structured into a formal reasoning graph.To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step.The most coherent graph is selected to derive the final answer.Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains.Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning.The implementation is available at https:// github.com/KurbanIntelligenceLab/theorem-of-thought.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved impressive performance across a wide range of natural language understanding and generation tasks (Wang et al., 2024), enabled by advances in in-context learning (Sia et al., 2024), instruction tuning (Zhang et al., 2024), and chain-of-thought (CoT) prompting (Wei et al., 2022).These methods have extended LLMs' capabilities to handle complex forms of reasoning, including mathematical, logical, and commonsense inference.</p>
<p>Despite these advances, LLM reasoning remains shallow and unreliable.Existing approaches often rely on single-shot or samplingbased decoding along linear reasoning paths, making them susceptible to hallucinations (Abdaljalil et al., 2025), logical inconsistencies (Uceda Sosa et al., 2024), and weak generalization (Liu et al., 2025).Methods such as CoT and Self-Consistency (Wei et al., 2022;Wang et al., 2023) encourage intermediate steps and majority voting across sampled outputs, but lack mechanisms to verify internal coherence and model the logical structure of reasoning.As a result, outputs may appear fluent and plausible while remaining logically unsound.</p>
<p>This brittleness contrasts sharply with human reasoning, which is inherently multifaceted.Drawing on insights from cognitive science (Okoli, 2022), we observe that human inference typically blends three complementary modes-abduction, deduction, and induction-that support explanation, derivation, and generalization.However, LLMs typically conflate these distinct processes into a single, undifferentiated flow, limiting both interpretability and reliability.</p>
<p>To address this gap, we propose Theoremof-Thought (ToTh), a framework that mod-els diverse reasoning strategies through structured, verifiable interactions.ToTh employs three specialized agents, each emulating a distinct cognitive mode:</p>
<p>• Abduction: inferring plausible explanations for observed facts;</p>
<p>• Deduction: deriving valid conclusions from given premises;</p>
<p>• Induction: generalizing from patterns or examples.</p>
<p>Each agent independently generates a reasoning trace, which is transformed into a Formal Reasoning Graph (FRG)-a directed graph where nodes represent intermediate conclusions and edges capture logical dependencies.We evaluate the internal consistency of each FRG using Bayesian belief propagation, with edge confidence scores calibrated via a Natural Language Inference (NLI) model.A composite score balancing average belief and logical entropy is used to select the most coherent graph, from which the final answer is extracted.</p>
<p>Contributions.</p>
<p>The key results of this work are:</p>
<p>• We introduce ToTh, a structured reasoning framework that integrates abductive, deductive, and inductive inference into a modular LLM-based pipeline.</p>
<p>• We develop a belief propagation mechanism over reasoning graphs, leveraging NLI to assess and score logical coherence through Bayesian updates.</p>
<p>• We demonstrate that ToTh consistently outperforms state-of-the-art reasoning methods (e.g., CoT, Self-Consistency, CoT-Decoding) across multiple LLMs.</p>
<p>• Our evaluation on symbolic (WebOfLies) and numerical (MultiArith) benchmarks highlights ToTh's robustness on tasks requiring multi-step inference-settings where direct prompting often fails (Allen-Zhu and Li, 2025).</p>
<p>The remainder of the paper is organized as follows: Section 2 reviews related work.Section 3 presents the ToTh framework.Section 4 describes the experimental setup, and Section 5 analyzes the results obtained.Section 6 concludes with implications for structured reasoning in LLMs and future research directions.</p>
<p>Related Work</p>
<p>Prompt-based Reasoning in LLMs.A growing body of work explores prompting strategies to enhance the reasoning capabilities of LLMs.CoT prompting (Wei et al., 2022) encourages models to decompose problems into intermediate steps, guiding reasoning along a linear path.Building on this, Auto-CoT (Zhang et al., 2023) automates prompt generation by sampling diverse questions and producing corresponding reasoning traces, reducing manual effort.Beyond prompt generation, several works focus on optimizing prompt selection strategies.ActivePrompt (Diao et al., 2024) identifies high-uncertainty instances for annotation, improving data efficiency and reasoning robustness through active learning.More recent approaches introduce explicit structure into the reasoning process.Tree-of-Thought (ToT) (Yao et al., 2023) enables multi-path exploration with internal evaluation, while Graphof-Thought (GoT) (Yao et al., 2024) structures reasoning as a graph to better model dependencies between steps.</p>
<p>Instruction Tuning for Reasoning.Instruction tuning and knowledge distillation offer alternative approaches to eliciting reasoning in LLMs without relying on explicit prompts (Lobo et al., 2025;Ranaldi and Freitas, 2024;Lai and Nissim, 2024).While effective, these methods typically require computationally intensive fine-tuning on large-scale datasets annotated with reasoning traces and CoT examples, which are often costly and domain-specific.Recent work has explored more indirect supervision strategies.For instance, Liu et al. (2024) introduce proxy tuning, which leverages auxiliary models to contrast a base LLM with its adapted variant.Although this approach reduces the need for direct supervision, it still assumes access to CoT-like outputs and prealigned reasoning benchmarks.</p>
<p>Methodology</p>
<p>ToTh is a graph-based reasoning framework designed to enhance the accuracy, interpretability, and generalization capabilities of LLMs on complex tasks.It decomposes reasoning into three modular agents, each simulating a classical inference paradigm-abduction, deduction, and induction.Each agent produces a structured reasoning trace, which is composed into a FRG.Final answers are derived via NLI-calibrated Bayesian belief propagation and composite graph scoring.The full pipeline is depicted in Fig. 1.</p>
<p>ToTh differs from prior reasoning paradigms along three axes: architecture, supervision, and verification.Prompt-based methods (e.g., CoT, ToT, GoT) elicit reasoning via linear or loosely structured traces, yet lack mechanisms for enforcing logical consistency.Instruction-tuned models embed reasoning behavior through finetuning on annotated traces, often requiring large datasets and remaining opaque at inference time.While both families reflect growing interest in structured multi-step reasoning, they typically operate within monolithic or implicit architectures and do not support formal consistency checking.In contrast, ToTh instantiates distinct cognitive agents, integrates their outputs into an interpretable graph, and explicitly verifies reasoning coherence through NLIguided Bayesian inference-enabling modular, transparent, and verifiable reasoning beyond the scope of existing methods.</p>
<p>Multi-Paradigm</p>
<p>Reasoning</p>
<p>Agents.Given a natural language question q, ToTh deploys three independent solver agents, each aligned with a distinct classical mode of inference: abductive, deductive, and inductive reasoning.These paradigms are formally defined as follows.</p>
<p>The abductive reasoning agent a 1 infers the most plausible hypothesis H given a set of observations O and background knowledge K, formalized as:
a 1 : arg max H P (H | O, K).
The deductive reasoning agent a 2 derives a conclusion C that logically follows from a set of premises {P 1 , P 2 , . . ., P n }, represented as:
a 2 : {P 1 , P 2 , . . . , P n } ⊢ C.
The inductive reasoning agent a 3 generalizes a rule R from observed examples {x 1 , x 2 , . . ., x n }, expressed as:
a 3 : {x 1 , x 2 , . . . , x n } ⇒ R.
Each agent a i ∈ {a 1 , a 2 , a 3 } independently produces a reasoning trace
r (i) = r (i) 1 , r (i) 2 , . . . , r (i) s i ,
where r (i) j denotes the j-th step in the agent's reasoning process.</p>
<p>Formal Reasoning Graph Construction.Each reasoning trace r (i) is transformed into a directed graph G (i) = (V (i) , E (i) ), where V (i) denotes the set of nodes representing individual reasoning steps, and E (i) represents directed edges encoding inferential relationships between those steps.Edges
(v u → v v ) ∈ E (i)
are inferred using a pretrained NLI model, which assesses the semantic relationship between reasoning steps.Each edge is annotated with a trust score θ uv ∈ [0, 1] based on the predicted label:
θ uv =      0.95 if entailment 0.60 if neutral 0.10 if contradiction
These scores quantify the strength of logical entailment between intermediate steps, providing a calibrated basis for probabilistic reasoning in the subsequent belief propagation stage.</p>
<p>Bayesian Confidence Propagation.To model belief flow across the graph, belief values are propagated using a Bayesian update rule, adapted from classical formulations of belief propagation in probabilistic graphical models (Pearl, 1988).</p>
<p>Each node v ∈ V is initialized with a prior confidence P (v) = 0.5, reflecting maximum uncertainty.For a node v c with a single parent v p and associated trust score θ pc , the updated belief is computed using a Bayesian update rule:
P (v c ) = P (v p ) • θ pc P (v p ) • θ pc + (1 − P (v p )) • (1 − θ pc )
.</p>
<p>In the case of multiple parents {v p 1 , . . ., v pm }, the belief for v c is computed as the average of individual updates from each parent:
P (v c ) = 1 m m j=1 f P (v p j ), θ p j c f (p, θ) = p • θ p • θ + (1 − p)(1 − θ) .
This recursive formulation propagates confidence through the graph, amplifying agreement across consistent reasoning paths while attenuating belief when upstream uncertainty or contradiction is detected.</p>
<p>Graph Scoring.Each reasoning graph G (i)  is evaluated based on a trade-off between average node confidence and logical uncertainty.We prioritize graphs that are both confident (high belief) and low in uncertainty (low entropy).The mean confidence is computed as
µ (i) = 1 |V (i) | v∈V (i) P (v),
and the normalized binary entropy is given by
H (i) = − 1 |V (i) | v∈V (i) h(P (v)) h(p) = p log p + (1 − p) log(1 − p) .
The final score combines both terms:
Score(G (i) ) = µ (i) − H (i) .
The reasoning graph with the highest score is selected as the final candidate:
G * = arg max i Score(G (i) ).
Answer Extraction.The final answer is extracted from the terminal node of the selected graph G * , corresponding to the last step in the associated reasoning trace.</p>
<p>Theoretical Complexity.Let k = 3 denote the number of reasoning agents, and s the number of reasoning steps generated per agent.The ToTh framework involves three main stages of computation: trust estimation, belief propagation, and graph scoring.During trust estimation, each agent produces a sequence of reasoning steps, and an NLI model is applied to each adjacent pair to evaluate the strength of logical connection.Since each trace contains at most s − 1 such pairs, the total number of NLI evaluations across all agents is O(k • s).</p>
<p>In the belief propagation stage, each node in the constructed reasoning graphs is visited exactly once in topological order, and its posterior confidence is updated based on incoming trust scores using a Bayesian update rule, resulting in O(k • s) total updates.Finally, graph scoring involves computing the average confidence and entropy over all nodes in each graph, which also requires O(k • s) time.Therefore, the end-toend complexity of the ToTh pipeline is O(k • s), linear in both the number of agents and the number of reasoning steps per agent.</p>
<p>This makes ToTh substantially more efficient than sampling-based methods such as Self-Consistency or CoT-Decoding, which require O(n) decoding passes, where n is the number of sampled reasoning chains.In contrast, ToTh executes a single, structured reasoning pass per agent, followed by lightweight verification and scoring, offering a more scalable and interpretable alternative to stochastic decoding.</p>
<p>Experiments</p>
<p>Data.ToTh was evaluated on two representative reasoning benchmarks.MultiArith (Roy et al., 2015) targets compositional numerical inference through multi-step arithmetic word problems.WebOfLies (Suzgun et al., 2023), part of the BIG-Bench-Hard suite, involves determining truth values among logically entangled symbolic statements.These datasets are known to challenge LLMs under direct prompting (Allen-Zhu and Li, 2025), making them suitable for testing structured reasoning capa-bilities.</p>
<p>Models.Three publicly available LLMs were selected to provide diversity in scale, alignment, and architecture: (1) Mistral-7B (Jiang et al., 2023) 1 , a general-purpose decoder model with efficient scaling; (2) DeepSeek-7B (DeepSeek-AI et al., 2025)2 , an instruction-tuned model optimized for multi-turn reasoning and alignment; and (3) Phi-3.5 Mini (Abdin et al., 2024)3 , a lightweight model designed for educational, low-cost reasoning tasks.This selection spans compact inference-efficient models to instruction-aligned reasoning-focused systems.</p>
<p>Baselines.ToTh was compared with three strong baselines: CoT (Wei et al., 2022), Self-Consistency (Wang et al., 2023), and CoT-Decoding (Wang and Zhou, 2024).CoT prompts the model to generate intermediate reasoning steps before answering.Self-Consistency improves robustness by sampling n = 20 completions and selecting the most frequent answer.CoT-Decoding eliminates explicit prompting by using diverse decoding paths to stimulate latent reasoning behaviors.</p>
<p>Experimental Setup.All models were evaluated in their released form without fine-tuning.Decoding was performed with temperature 0.7 and a maximum output length of 526 tokens.RoBERTa-MNLI4 was used for scoring reasoning coherence, consistent with prior work on NLI-based output validation (Farquhar et al., 2024).Inputs were uniformly formatted as "Q:</p>
<p>[question] \n A:" across all methods for consistency with baselines (Wang and Zhou, 2024).</p>
<p>To direct reasoning behavior, the following instruction was prepended to each input, with the appropriate {style} keyword for each agent:</p>
<p>Use the {style} reasoning style to answer the following question.Follow these instructions carefully: All experiments used a single decoding pass per input.Random seeds were fixed, and decoding settings were held constant for reproducibility.</p>
<p>Results</p>
<p>Main Experimental Results</p>
<p>Results are reported as answer accuracy (%) and summarized in Figure 2.</p>
<p>Performance Across Models.ToTh consistently outperforms all baseline methods on both tasks when evaluated with Mistral-7B and DeepSeek-7B, demonstrating clear gains in reasoning accuracy.On Phi-3.5 Mini, although CoT-Decoding marginally surpasses ToTh on certain instances, ToTh maintains consistently strong performance across both symbolic and numerical tasks.For example, on the WebOfLies dataset, ToTh improves over CoT-Greedy by 29% and 14% on Mistral-7B and DeepSeek-7B, respectively, and remains within 3% of the top-performing method on Phi-3.5 Mini.These results highlight ToTh's robustness and generalization across models of varying scale and alignment.</p>
<p>Comparison with CoT-Decoding.While CoT-Decoding performs strongly on Phi-3.5mini,achieving near-perfect scores on We-bOfLies (99%), ToTh achieves comparable or slightly lower performance (96%) while maintaining higher consistency across models.For example, on the MultiArith dataset, ToTh surpasses CoT-Decoding by 4-5 points on both Mistral-7B and DeepSeek-7B, indicating superior generalization in numerical reasoning.</p>
<p>Self-Consistency</p>
<p>Under-performance. Surprisingly, Self-Consistency under-performs across all settings, particularly on symbolic tasks.For instance, it yields only 14% and 21% on WebOfLies and MultiArith with DeepSeek-7B and Mistral-7B, respectively.This suggests that majority-vote over stochastic generations fails to capture structured dependencies, especially in logic-heavy tasks.</p>
<p>Model Sensitivity.As expected, performance scales with model capability.Phi-3.5miniachieves the highest absolute scores across all methods, reflecting its stronger alignment and training.However, ToTh's margin over baselines remains meaningful even at lower model scales, suggesting that the architecture contributes to reasoning robustness beyond just model size.While DeepSeek-7B is trained with reasoning capabilities in mind, its broader training objectives, including code generation and open-ended question answering, may diffuse its specialization in structured reasoning tasks.In contrast, Phi-3.5-minibenefits from a targeted curriculum focused on educational and step-by-step problem-solving, which likely accounts for its superior performance on both symbolic and mathematical benchmarks.Interestingly, Mistral-7B consistently outperforms DeepSeek-7B despite being similar in size.This may be attributed to Mistral's cleaner, reasoning-focused pretraining data and architecture-level optimizations, which enhance its ability to follow multi-step instructions and maintain logical coherence across token spans.</p>
<p>Robustness Under Reasoning Complexity</p>
<p>To evaluate the robustness of ToTh under increasing reasoning complexity, experiments were conducted using the Mistral-7B model on both symbolic and numerical tasks.Table 1 presents accuracy results stratified by problem difficulty: the number of interdependent statements (3-5) for WebOfLies, and operation depth/length combinations for MultiArith.</p>
<p>ToTh maintains strong performance across all difficulty levels, outperforming or closely matching leading baselines.In symbolic reasoning, ToTh achieves 43% accuracy on the most challenging setting (5 statements), significantly exceeding CoT-Greedy (19%) and Self-Consistency (38%), and closely approaching CoT-Decoding (46%).This trend persists across simpler instances, where ToTh attains the highest scores at 3 and 4 statements.</p>
<p>For numerical reasoning, ToTh delivers the strongest results at lower complexity levels-achieving state-of-the-art performance at d 0 /l 3 (59%) and d 0 /l 4 (45%)-and remains competitive even at higher complexity (d 2 /l 3 ), with accuracy comparable to CoT-Decoding (21% vs. 24%).These findings highlight ToTh's capacity to generalize across task difficulty and suggest that its structured, multi-agent reasoning design offers a scalable advantage under increased inference load.</p>
<p>Conclusion and Future Work</p>
<p>This work presents Theorem-of-Thought (ToTh), a graph-based reasoning framework that integrates abductive, deductive, and inductive inference through a modular multiagent design.Each agent generates structured reasoning traces, which are composed into formal graphs and verified using NLIcalibrated Bayesian confidence propagation.This approach supports both accurate prediction and interpretable, logically grounded reasoning.Empirical evaluations on symbolic and numerical benchmarks demonstrate that ToTh consistently outperforms strong prompting and decoding baselines, particularly in scenarios requiring structured logical inference.ToTh introduces a new paradigm in reasoning with language models by treating inference as a verifiable, compositional process, rather than a monolithic generation task.Future research will explore dynamic agent routing based on input characteristics, inter-agent collaboration protocols, and adaptive trust estimation via fine-tuned and ensemble-based NLI models.Extending the framework to scientific hypothesis validation, law and policy reasoning, and multimodal domains such as visual question answering represents a promising direction for advancing general-purpose, verifiable reasoning in large language models.</p>
<p>Limitations</p>
<p>Fixed Reasoning Types.ToTh presumes a uniform decomposition into abductive, deductive, and inductive reasoning across all inputs.While this modularity improves interpretability, it imposes a fixed cognitive scaffold that may not align with tasks requiring hybrid or atypical inference patterns.For example, creative tasks or ambiguous prompts may benefit from dynamically blending reasoning types or emphasizing one over others.This rigidity can limit ToTh's adaptability and lead to suboptimal trace composition in such cases.Future work may explore data-driven and context-sensitive agent routing, allowing the framework to selectively instantiate and suppress reasoning paradigms based on input semantics.</p>
<p>Propagation Sensitivity.The Bayesian confidence propagation mechanism is sensitive to noise in low-confidence nodes, which may attenuate otherwise valid reasoning chains or distort belief estimates in deeper regions of the graph.This can occur in longer traces where errors in early reasoning steps propagate disproportionately, reducing the reliability of final predictions.Moreover, current propagation is uniform and unregularized, lacking robustness mechanisms against adversarial and inconsistent intermediate steps.</p>
<p>Incorporating calibrated uncertainty modeling, edge dropout, and confidence smoothing-potentially informed by fine-grained entailment distributions-could enhance stability and mitigate the amplification of localized inconsistencies.</p>
<p>Figure 1 :
1
Figure 1: Overview of the Theorem-of-Thought (ToTh) reasoning pipeline.A question is independently processed by three agents, each using a distinct reasoning style: abductive (Type 1), deductive (Type 2), and inductive (Type 3).Each agent produces a structured reasoning graph, which is scored via Bayesian confidence propagation.Abduction infers the best hypothesis H given observations O and knowledge K (i.e., arg max H P (H | O, K)); deduction derives a conclusion C from premises {P 1 , . . ., P n } (i.e., {P i } ⊢ C); induction generalizes from examples {x 1 , . . ., x n } to a rule R (i.e., {x i } ⇒ R).The highest-scoring graph contributes its final node as the answer.✓ and ✗ indicate whether a given agent's output was selected.</p>
<p>Figure 2 :
2
Figure 2: Accuracy (%) comparison across reasoning pipelines on two benchmark tasks (WebOfLies and MultiArith) using three open-source language models: Mistral-7B-v0.3,DeepSeek-7B, and Phi-3.5-mini.Each group of bars corresponds to a different reasoning method: CoT-Greedy (blue), Self-Consistency (red), CoT-Decoding (yellow), and our proposed Theorem-of-Thought (green).</p>
<p>Table 1 :
1
Accuracy (%) of Mistral-7B on sym-
WebOfLiesMultiArith3 45 d0/l3 d0/l4 d2/l3CoT-G41 32 19572614SelfC48 47 3821617CoT-Dec 54 48 46554124ToTh70 56 43594521bolic (WebOfLies) and mathematical (MultiArith)reasoning tasks across increasing levels of difficulty.Columns 3-5 correspond to symbolic reasoning with3, 4, and 5 interdependent statements, respectively.Columns d0/l3, d0/l4, and d2/l3 represent arithmeticreasoning problems categorized by depth and length: ddenotes operation depth and l indicates sequence length.ToTh achieves the highest accuracy in 5 out of 6 settingsand remains competitive even on the most complex in-stances, demonstrating consistent performance acrosssymbolic and numerical domains. Bold: best perfor-mance; Underlined: second-best.
https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.3
https://huggingface.co/deepseek-ai/ deepseek-llm-7b-chat
https://huggingface.co/microsoft/Phi-3. 5-mini-instruct
https://huggingface.co/FacebookAI/ roberta-large-mnli</p>
<p>Sindex: Semantic inconsistency index for hallucination detection in llms. Samir Abdaljalil, Parichit Hasan Kurban, Sharma, arXiv:2503.05980Erchin Serpedin, and Rachad Atat. 2025Preprint</p>
<p>Jyoti Marah Abdin, Hany Aneja, Ahmed Awadalla, Ammar Awadallah, Nguyen Ahmad Awan, Amit Bach, Arash Bahree, Jianmin Bakhtiari, Harkirat Bao, Alon Behl, Misha Benhaim, Johan Bilenko, Sébastien Bjorck, Martin Bubeck, Qin Cai, Vishrav Cai, Dong Chaudhary, Chen, arXiv:2404.14219others. 2024. Phi-3 technical report: A highly capable language model locally on your phone. 110Preprint</p>
<p>Physics of language models: Part 3.2, knowledge manipulation. Zeyuan Allen, -Zhu , Yuanzhi Li, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2025ICLR 2025 Poster</p>
<p>Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, arXiv:2501.12948Ziyi Gao, and 181 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint</p>
<p>Active prompting with chain-of-thought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, Tong Zhang, 10.18653/v1/2024.acl-long.73Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Detecting hallucinations in large language models using semantic entropy. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, Yarin Gal, 10.1038/s41586-024-07421-0Nature. 63080172024. 2024The Author(s)</p>
<p>Devendra Singh Chaplot, Diego de las Casas. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>mCoT: Multilingual instruction tuning for reasoning consistency in language models. Huiyuan Lai, Malvina Nissim, 10.18653/v1/2024.acl-long.649Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241Long Papers)</p>
<p>Tuning language models by proxy. Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, Noah A Smith, Proceedings of the Conference on Language Modeling (COLM). the Conference on Language Modeling (COLM)2024</p>
<p>Zero-to-strong generalization: Eliciting strong capabilities of large language models iteratively without gold labels. Chaoqun Liu, Qin Chao, Wenxuan Zhang, Xiaobao Wu, Boyang Li, Anh Tuan Luu, Lidong Bing, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational LinguisticsAbu Dhabi, UAEAssociation for Computational Linguistics2025</p>
<p>On the impact of fine-tuning on chain-of-thought reasoning. Elita Lobo, Chirag Agarwal, Himabindu Lakkaraju, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language TechnologiesAlbuquerque, New MexicoAssociation for Computational Linguistics20251</p>
<p>Inductive, abductive and deductive theorizing. Chitu Okoli, 10.2139/ssrn.3774317SSRN Electronic Journal. 2022</p>
<p>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Judea Pearl, 1988Morgan Kaufmann Publishers IncSan Francisco, CA, USA</p>
<p>Selfrefine instruction-tuning for aligning reasoning in language models. Leonardo Ranaldi, Andre Freitas, 10.18653/v1/2024.emnlp-main.139Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Reasoning about quantities in natural language. Subhro Roy, Tim Vieira, Dan Roth, 10.1162/tacl_a_00118Transactions of the Association for Computational Linguistics. 32015</p>
<p>Where does in-context learning happen in large language models?. Suzanna Sia, David Mueller, Kevin Duh, Proceedings of the 2024 Conference on Neural Information Processing Systems (NeurIPS). the 2024 Conference on Neural Information Processing Systems (NeurIPS)NeurIPS2024. 2024Poster</p>
<p>Challenging BIGbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, Jason Wei, 10.18653/v1/2023.findings-acl.824Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Rosario Uceda Sosa, Karthikeyan Natesan Ramamurthy, Maria Chang, Moninder Singh, Reasoning about concepts with llms: Inconsistencies abound. Conference on Language Models (COLM). 2024. 10 Jul 2024. 26 Aug 2024Published. Last Modified</p>
<p>Rethinking the bounds of LLM reasoning: Are multi-agent discussions the key?. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song, 10.18653/v1/2024.acl-long.331Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Chain-ofthought reasoning without prompting. Xuezhi Wang, Denny Zhou, Proceedings of the 2024 Conference on Neural Information Processing Systems (NeurIPS). the 2024 Conference on Neural Information Processing Systems (NeurIPS)2024NeurIPS 2024 Poster</p>
<p>Chainof-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>Tree of thoughts: deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>GoT: Effective graph-of-thought reasoning in language models. Yao Yao, Zuchao Li, Hai Zhao, 10.18653/v1/2024.findings-naacl.183Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024</p>
<p>Instruction tuning for large language models: A survey. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang, arXiv:2308.107922024Preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, Proceedings of International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)2023Poster</p>            </div>
        </div>

    </div>
</body>
</html>
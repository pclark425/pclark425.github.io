<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1791 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1791</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1791</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-c14a5239ab4e8dd2964169450e727a6b089671a2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c14a5239ab4e8dd2964169450e727a6b089671a2" target="_blank">DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> The techniques to train a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated are presented.</p>
                <p><strong>Paper Abstract:</strong> Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in diverse kinds of hardware and simulator setups, and in our case, with the Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for researchers to achieve such results with commonly-available, affordable robot hands and cameras. Videos of the resulting policy and supplementary information, including experiments and demos, can be found on the website.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1791.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1791.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeXtreme</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeXtreme: Vision-guided dexterous in-hand manipulation system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end system (policy + vision-based pose estimator + deployment pipeline) that learns multi-fingered in-hand cube reorientation in GPU simulation and transfers the skill to a real Allegro Hand using heavy domain randomisation and curriculum via Vectorised ADR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Allegro Hand running the DeXtreme policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 4-finger Allegro robotic hand (wrist locked) controlled via 16 joint PD targets; used to perform agile contact-rich in-hand reorientation of a 6.5 cm cuboid using a learned LSTM policy and a vision-based pose estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (dexterous in-hand manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym (GPU-based Isaac Sim) with task environment</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>GPU-based physics simulator (Isaac Gym / Isaac Sim) used to simulate rigid-body dynamics, contact-rich interactions, and to render synthetic visual data via Omniverse/Replicator for training vision models; supports massive parallelism for thousands of agents per GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity physics for rigid bodies and contacts (GPU-accelerated), plus photorealistic visual rendering with heavy visual randomisation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>contact dynamics (collision, restitution), friction (surface friction scaling), mass and inertia scaling, joint stiffness and damping, actuator effort scaling, external forces, action/observation delays and latencies, correlated and uncorrelated sensor noise, randomized gravity sampling (global timestep), photorealistic rendering (lighting, materials, camera intrinsics/extrinsics), motion blur and camera noise.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>could not randomize collision morphologies at runtime (mass/scale randomization constrained by API), no tactile sensing modeled, limited per-environment gravity randomisation (sampled globally every 720 steps), some simulator-specific contact modeling differences vs MuJoCo (different contact model), no explicit modeling of hardware wear-and-tear (cable breakage, motor degradation), and limited modelling of the joint distribution of real-world parameter correlations (ADR treats dims independently).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical lab setup with Allegro Hand fixed at wrist, three Intel D415 RGB cameras extrinsically calibrated to the palm, cube with colored/lettered stickers, ROS-based inference and control pipeline, policy and pose estimator run on an NVIDIA RTX 3090 (policy + vision), control and visualization on separate machines, open lab lighting (not caged).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>In-hand reorientation of a 6.5 cm cuboid (continuous sequential reorientation to randomly sampled SO(3) targets) without dropping the object for as many consecutive targets as possible.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>On-policy deep reinforcement learning (PPO) with an LSTM-based actor and asymmetric critic; vision pose estimator trained in simulation with supervised learning (keypoint detection + PnP).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Number of consecutive successful target reorientations on the physical robot (consecutive successes before drop or stuck >80s); also average and median consecutive successes over trials, and pose estimator rotation/translation error on held-out sim test set.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Manual DR: ~35 consecutive successes (average reported in simulation training curves); policies evaluated in sim with goal threshold 0.1 rad during training and tested with 0.4 rad (simulation rollouts achieved much higher consecutive success counts than real).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Best ADR (vision) model: average 27.8 ± 19.0 consecutive successes (median 14.0) across trials; Non-ADR (manual DR) models: average ~11.9–14.8 consecutive successes depending on day, with medians ~9.5–14.5. Pose estimator (sim test set): rotation error 5.3° ± 0.11°, translation errors X:1.9±0.1 mm, Y:4.1±0.2 mm, Z:6.9±0.4 mm (evaluated in simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Extensive domain randomisation via ADR/VADR: object mass, object scale, object friction, hand mass/scale, joint stiffness, joint damping, actuation effort, restitution, external random forces, action delays and latencies (stochastic and fixed per-episode), action/observation correlated and uncorrelated gaussian noise, observation pose delay probability and refresh frequency, random pose injections, Random Network Adversary (RNA) blending, gravity perturbations; visual randomisations for pose estimator: albedo, lighting, reflection roughness, metallicity, camera noise/scanlines/film grain, random backgrounds, camera pose, intrinsics perturbation, and data augmentations (CutMix, blur, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Residual differences in contact dynamics between simulator and real world; imperfect pose estimation due to occlusion and lighting; camera refresh rate and lower real observation frame-rate than simulated control rate; latency/jitter in ROS inference/control; inability to perfectly model joint correlations between parameters (ADR independence); API limits preventing some per-environment morphology changes; hardware degradation/wear (malfunctioning thumb, ribbon cable burning); unmodeled microscale surface properties and real-world friction variability.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Massive parallel GPU simulation (Isaac Gym) enabling large sample sizes; automated curriculum via Vectorised ADR (VADR) to expand randomisation ranges adaptively; heavy physics and visual randomisation; training of a robust LSTM policy with asymmetric critic and action smoothing (EMA) for stability; a pose estimator trained on 5M synthetic images rendered with Omniverse/Replicator and augmented, combined with multi-camera PnP triangulation and filtering; injection of realistic delays/latencies and structured noise (RNA) during training; using extrinsic camera-to-palm calibration so policy inputs are palm-relative.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Contact dynamics and variability (friction, restitution, joint stiffness/damping) are critical to model and randomise; realistic timing/latency and observation jitter must be modeled; heavy visual randomisation is needed for robust vision-based pose estimation; no strict numeric fidelity threshold given, but adequate randomisation ranges and contact modeling are required to achieve functional transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning dexterous in-hand manipulation entirely in GPU simulation and transferring to a real, affordable Allegro Hand is feasible when (1) large-scale parallel simulation and heavy domain randomisation (including structured noise via RNA) are used, (2) an adaptive curriculum (VADR) expands randomisation ranges to the policy's limits, and (3) a robust vision pose estimator trained with massive synthetic data and augmentations supplies usable state; ADR-trained policies outperform manually randomised policies on real hardware, although some sim-to-real gaps remain (pose estimation errors, hardware wear, residual physics mismatch).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1791.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1791.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VADR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vectorised Automatic Domain Randomisation (VADR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vectorised implementation of Automatic Domain Randomisation (ADR) that adaptively expands or contracts per-parameter randomisation bounds across many parallel simulated environments to produce a curriculum of increasing environment difficulty and robustness for policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Allegro Hand / DeXtreme policy (technique applied to training)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Technique applied during training of the RL policy controlling the Allegro Hand to improve robustness and sim-to-real transfer by automatically discovering maximal randomisation ranges the policy can tolerate.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real training methodology</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym (vectorised environments)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Vectorised GPU simulation where thousands of parallel environments are executed per GPU; VADR fixes a fraction of environments to boundary values and uses their performance to adapt bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>methodological (applies across fidelity levels) — supports high-parallel, high-fidelity physics simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Not a physics model itself; used to randomise and explore fidelity aspects such as friction, mass, joint stiffness/damping, restitution, delays, noise, gravity, visual params.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not model joint distributions between randomisation dimensions (treats each parameter independent), so correlations present in the real world may be unmodeled.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Used to train policies that are later deployed on the real Allegro Hand with three RGB cameras in an open laboratory environment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Enables transfer of the learned in-hand reorientation policy by adaptively exposing it to progressively harder/randomised simulated conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Adaptive domain randomisation integrated into PPO training (vectorised across many parallel environments), with a fraction of envs used for boundary evaluation and queue-based expansion/tightening rules.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Average consecutive successes at environment boundaries used to decide expansion/tightening (training metric); final metric for transfer is real-world consecutive successes of policies trained with VADR.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>VADR policies reached high simulated performance quickly (training curves show npd increasing over time); manual DR reached ~35 consecutive successes in simulation; VADR enabled policies that in sim achieved high robustness (exact sim numbers for ADR policies not singularly enumerated beyond training curves).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Policies trained with ADR/VADR outperformed non-ADR policies in the real world: best ADR vision model average 27.8 ± 19.0 consecutive successes vs non-ADR averages ~11.9–14.8.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Adaptive per-dimension uniform ranges with per-dimension step sizes Δ^n; evaluated at boundaries with queues of past consecutive-success counts; dimensions include physics (mass, friction, stiffness, damping), action/observation noise/delays, RNA alpha, gravity, and visual parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>VADR does not capture joint distributions between parameters, so it may miss correlated real-world effects; order of randomisation expansion can bias results; ADR may expand some dims at expense of others, causing uneven robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Massively parallel simulation to evaluate many environments; reserving a fraction of envs to probe boundary performance; per-dimension step tuning and queue-based thresholds to stabilize curriculum; use of VADR per-GPU to reduce synchronisation overhead and explore multiple independent parameter sets.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Technique requires that interpretable simulator parameters are exposed for randomisation (friction, mass, damping, latencies) and that per-dimension step sizes be tuned; joint-distribution modeling would be needed to capture correlated real-world variability fully.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vectorised ADR (VADR) provides an effective, scalable curriculum for expanding randomisation ranges during training and yields policies that transfer better to the real robot than manually tuned domain randomisation; limitations include lack of joint-distribution modeling and potential uneven exploration of parameter Pareto-limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1791.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1791.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vision Pose Estimator (Replicator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic-data-trained keypoint-based pose estimator using Omniverse Isaac Sim / Replicator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mask-RCNN-inspired network trained on 5M synthetic images rendered with Omniverse/Replicator (plus on-the-fly augmentations) to detect cube bounding box, segmentation, and 8 corner keypoints, followed by PnP triangulation across three cameras to produce the cube pose for the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>DeXtreme perception module feeding the Allegro Hand policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Multi-camera RGB pose estimation pipeline producing palm-relative cube pose at runtime (15 Hz) using neural keypoint detection, PnP per-camera, filtering of inconsistent cameras, triangulation, and registration to cube model.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics perception for manipulation / sim-to-real vision</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Omniverse Isaac Sim with Replicator</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Photorealistic synthetic image generator that simulates camera optics, materials, lighting, and visual noise (scanlines, film grain), used to produce millions of images with configurable visual randomisation for supervised pose estimator training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>photorealistic visual rendering with heavy randomisation; supervised vision training-level fidelity (appearance-level), not modeling tactile/contact sensing.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Lighting variability, material properties (albedo, roughness, metallic), camera intrinsics and poses, motion blur (via augmentation), camera noise, occlusions (hand visibility probability), backgrounds, and various image corruptions; data augmentations (CutMix, blur, brightness/contrast, cropping).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Physical sensor readout dynamics beyond frame drops not fully modeled; pose estimator evaluated precisely only in simulation (real-world labelled pose ground truth not available); some real-world occlusion cases and interpenetrations when replaying real states in sim were not perfectly matched; no depth used (RGB only).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Three Intel D415 RGB cameras capturing the real scene at inference; pose estimation runs at 15 Hz on a GPU, PnP triangulation across cameras and registration yields pose in palm frame; occasional heavy occlusion and pose jumps in real scenes were observed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Transfer of visual pose estimation from synthetic images to robust real-world cube pose estimates sufficient for closed-loop control of in-hand manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised training of a Mask-RCNN-like network on 5M synthetic images (cross-entropy for segmentation/keypoints, smooth L1 for bounding box), plus on-the-fly augmentations and real-to-sim mining (replaying real configurations in sim to render more variations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Pose estimation error on synthetic test set (rotation and translation) and downstream metric: policy consecutive successes when fed the estimator output on real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Pose estimator test on simulated testset: rotation error 5.3° ± 0.11°, translation errors: X 1.9 ± 0.1 mm, Y 4.1 ± 0.2 mm, Z 6.9 ± 0.4 mm (90% confidence intervals).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Indirectly successful: policies using this estimator achieved up to 112 consecutive real-world successes in a single rollout (best vision-based policy) and average up to ~27.8 consecutive successes; however, some drop in performance vs simulation indicates residual perception sim-to-real gap.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Visual randomisation parameters: albedo desaturation/add/brightness, diffuse tint, reflection roughness, metallic/specular levels, camera noise, scanlines, film grain, random splotches, random backgrounds, camera pose sampling, focal length multiplier, hand visibility probability; augmentations: CutMix, blur, rotation, brightness/contrast, cropping.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Heavy occlusions (policy can cage the cube causing keypoint occlusion), lighting-induced color shifts (stickers changing appearance), pose estimator occasional jumps, lower real camera refresh rates and synchronization issues, and inaccuracies when replaying real states in sim (interpenetrations) limiting perfect calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Massive synthetic dataset generation (5M images) with extreme visual randomisation and data augmentation, real-to-sim mining by replaying real trajectories to render problematic configurations, multi-camera PnP filtering and triangulation, and keeping pose outputs in palm-relative frame to decouple from global placement.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Extensive visual randomisation and targeted sampling of hard occluded poses are necessary to produce robustness; maintaining accurate camera intrinsics and extrinsic calibration to palm frame is important; using multiple calibrated cameras and geometric PnP + triangulation improves real-world reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A pose estimator trained entirely on synthetic images with heavy visual randomisation and on-the-fly augmentations can produce sufficiently accurate pose estimates (few-degree rotation error, millimeter translation error in sim) to enable closed-loop vision-based dexterous manipulation on real hardware; however, occlusions and occasional pose jumps remain important failure modes that affect final transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Solving rubik's cube with a robot hand <em>(Rating: 2)</em></li>
                <li>Isaac gym: High performance gpu-based physics simulation for robot learning <em>(Rating: 2)</em></li>
                <li>Noise and the reality gap: The use of simulation in evolutionary robotics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1791",
    "paper_id": "paper-c14a5239ab4e8dd2964169450e727a6b089671a2",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "DeXtreme",
            "name_full": "DeXtreme: Vision-guided dexterous in-hand manipulation system",
            "brief_description": "An end-to-end system (policy + vision-based pose estimator + deployment pipeline) that learns multi-fingered in-hand cube reorientation in GPU simulation and transfers the skill to a real Allegro Hand using heavy domain randomisation and curriculum via Vectorised ADR.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Allegro Hand running the DeXtreme policy",
            "agent_system_description": "A 4-finger Allegro robotic hand (wrist locked) controlled via 16 joint PD targets; used to perform agile contact-rich in-hand reorientation of a 6.5 cm cuboid using a learned LSTM policy and a vision-based pose estimator.",
            "domain": "general robotics manipulation (dexterous in-hand manipulation)",
            "virtual_environment_name": "Isaac Gym (GPU-based Isaac Sim) with task environment",
            "virtual_environment_description": "GPU-based physics simulator (Isaac Gym / Isaac Sim) used to simulate rigid-body dynamics, contact-rich interactions, and to render synthetic visual data via Omniverse/Replicator for training vision models; supports massive parallelism for thousands of agents per GPU.",
            "simulation_fidelity_level": "high-fidelity physics for rigid bodies and contacts (GPU-accelerated), plus photorealistic visual rendering with heavy visual randomisation",
            "fidelity_aspects_modeled": "contact dynamics (collision, restitution), friction (surface friction scaling), mass and inertia scaling, joint stiffness and damping, actuator effort scaling, external forces, action/observation delays and latencies, correlated and uncorrelated sensor noise, randomized gravity sampling (global timestep), photorealistic rendering (lighting, materials, camera intrinsics/extrinsics), motion blur and camera noise.",
            "fidelity_aspects_simplified": "could not randomize collision morphologies at runtime (mass/scale randomization constrained by API), no tactile sensing modeled, limited per-environment gravity randomisation (sampled globally every 720 steps), some simulator-specific contact modeling differences vs MuJoCo (different contact model), no explicit modeling of hardware wear-and-tear (cable breakage, motor degradation), and limited modelling of the joint distribution of real-world parameter correlations (ADR treats dims independently).",
            "real_environment_description": "Physical lab setup with Allegro Hand fixed at wrist, three Intel D415 RGB cameras extrinsically calibrated to the palm, cube with colored/lettered stickers, ROS-based inference and control pipeline, policy and pose estimator run on an NVIDIA RTX 3090 (policy + vision), control and visualization on separate machines, open lab lighting (not caged).",
            "task_or_skill_transferred": "In-hand reorientation of a 6.5 cm cuboid (continuous sequential reorientation to randomly sampled SO(3) targets) without dropping the object for as many consecutive targets as possible.",
            "training_method": "On-policy deep reinforcement learning (PPO) with an LSTM-based actor and asymmetric critic; vision pose estimator trained in simulation with supervised learning (keypoint detection + PnP).",
            "transfer_success_metric": "Number of consecutive successful target reorientations on the physical robot (consecutive successes before drop or stuck &gt;80s); also average and median consecutive successes over trials, and pose estimator rotation/translation error on held-out sim test set.",
            "transfer_performance_sim": "Manual DR: ~35 consecutive successes (average reported in simulation training curves); policies evaluated in sim with goal threshold 0.1 rad during training and tested with 0.4 rad (simulation rollouts achieved much higher consecutive success counts than real).",
            "transfer_performance_real": "Best ADR (vision) model: average 27.8 ± 19.0 consecutive successes (median 14.0) across trials; Non-ADR (manual DR) models: average ~11.9–14.8 consecutive successes depending on day, with medians ~9.5–14.5. Pose estimator (sim test set): rotation error 5.3° ± 0.11°, translation errors X:1.9±0.1 mm, Y:4.1±0.2 mm, Z:6.9±0.4 mm (evaluated in simulation).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Extensive domain randomisation via ADR/VADR: object mass, object scale, object friction, hand mass/scale, joint stiffness, joint damping, actuation effort, restitution, external random forces, action delays and latencies (stochastic and fixed per-episode), action/observation correlated and uncorrelated gaussian noise, observation pose delay probability and refresh frequency, random pose injections, Random Network Adversary (RNA) blending, gravity perturbations; visual randomisations for pose estimator: albedo, lighting, reflection roughness, metallicity, camera noise/scanlines/film grain, random backgrounds, camera pose, intrinsics perturbation, and data augmentations (CutMix, blur, etc.).",
            "sim_to_real_gap_factors": "Residual differences in contact dynamics between simulator and real world; imperfect pose estimation due to occlusion and lighting; camera refresh rate and lower real observation frame-rate than simulated control rate; latency/jitter in ROS inference/control; inability to perfectly model joint correlations between parameters (ADR independence); API limits preventing some per-environment morphology changes; hardware degradation/wear (malfunctioning thumb, ribbon cable burning); unmodeled microscale surface properties and real-world friction variability.",
            "transfer_enabling_conditions": "Massive parallel GPU simulation (Isaac Gym) enabling large sample sizes; automated curriculum via Vectorised ADR (VADR) to expand randomisation ranges adaptively; heavy physics and visual randomisation; training of a robust LSTM policy with asymmetric critic and action smoothing (EMA) for stability; a pose estimator trained on 5M synthetic images rendered with Omniverse/Replicator and augmented, combined with multi-camera PnP triangulation and filtering; injection of realistic delays/latencies and structured noise (RNA) during training; using extrinsic camera-to-palm calibration so policy inputs are palm-relative.",
            "fidelity_requirements_identified": "Contact dynamics and variability (friction, restitution, joint stiffness/damping) are critical to model and randomise; realistic timing/latency and observation jitter must be modeled; heavy visual randomisation is needed for robust vision-based pose estimation; no strict numeric fidelity threshold given, but adequate randomisation ranges and contact modeling are required to achieve functional transfer.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Learning dexterous in-hand manipulation entirely in GPU simulation and transferring to a real, affordable Allegro Hand is feasible when (1) large-scale parallel simulation and heavy domain randomisation (including structured noise via RNA) are used, (2) an adaptive curriculum (VADR) expands randomisation ranges to the policy's limits, and (3) a robust vision pose estimator trained with massive synthetic data and augmentations supplies usable state; ADR-trained policies outperform manually randomised policies on real hardware, although some sim-to-real gaps remain (pose estimation errors, hardware wear, residual physics mismatch).",
            "uuid": "e1791.0",
            "source_info": {
                "paper_title": "DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "VADR",
            "name_full": "Vectorised Automatic Domain Randomisation (VADR)",
            "brief_description": "A vectorised implementation of Automatic Domain Randomisation (ADR) that adaptively expands or contracts per-parameter randomisation bounds across many parallel simulated environments to produce a curriculum of increasing environment difficulty and robustness for policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Allegro Hand / DeXtreme policy (technique applied to training)",
            "agent_system_description": "Technique applied during training of the RL policy controlling the Allegro Hand to improve robustness and sim-to-real transfer by automatically discovering maximal randomisation ranges the policy can tolerate.",
            "domain": "general robotics manipulation / sim-to-real training methodology",
            "virtual_environment_name": "Isaac Gym (vectorised environments)",
            "virtual_environment_description": "Vectorised GPU simulation where thousands of parallel environments are executed per GPU; VADR fixes a fraction of environments to boundary values and uses their performance to adapt bounds.",
            "simulation_fidelity_level": "methodological (applies across fidelity levels) — supports high-parallel, high-fidelity physics simulation",
            "fidelity_aspects_modeled": "Not a physics model itself; used to randomise and explore fidelity aspects such as friction, mass, joint stiffness/damping, restitution, delays, noise, gravity, visual params.",
            "fidelity_aspects_simplified": "Does not model joint distributions between randomisation dimensions (treats each parameter independent), so correlations present in the real world may be unmodeled.",
            "real_environment_description": "Used to train policies that are later deployed on the real Allegro Hand with three RGB cameras in an open laboratory environment.",
            "task_or_skill_transferred": "Enables transfer of the learned in-hand reorientation policy by adaptively exposing it to progressively harder/randomised simulated conditions.",
            "training_method": "Adaptive domain randomisation integrated into PPO training (vectorised across many parallel environments), with a fraction of envs used for boundary evaluation and queue-based expansion/tightening rules.",
            "transfer_success_metric": "Average consecutive successes at environment boundaries used to decide expansion/tightening (training metric); final metric for transfer is real-world consecutive successes of policies trained with VADR.",
            "transfer_performance_sim": "VADR policies reached high simulated performance quickly (training curves show npd increasing over time); manual DR reached ~35 consecutive successes in simulation; VADR enabled policies that in sim achieved high robustness (exact sim numbers for ADR policies not singularly enumerated beyond training curves).",
            "transfer_performance_real": "Policies trained with ADR/VADR outperformed non-ADR policies in the real world: best ADR vision model average 27.8 ± 19.0 consecutive successes vs non-ADR averages ~11.9–14.8.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Adaptive per-dimension uniform ranges with per-dimension step sizes Δ^n; evaluated at boundaries with queues of past consecutive-success counts; dimensions include physics (mass, friction, stiffness, damping), action/observation noise/delays, RNA alpha, gravity, and visual parameters.",
            "sim_to_real_gap_factors": "VADR does not capture joint distributions between parameters, so it may miss correlated real-world effects; order of randomisation expansion can bias results; ADR may expand some dims at expense of others, causing uneven robustness.",
            "transfer_enabling_conditions": "Massively parallel simulation to evaluate many environments; reserving a fraction of envs to probe boundary performance; per-dimension step tuning and queue-based thresholds to stabilize curriculum; use of VADR per-GPU to reduce synchronisation overhead and explore multiple independent parameter sets.",
            "fidelity_requirements_identified": "Technique requires that interpretable simulator parameters are exposed for randomisation (friction, mass, damping, latencies) and that per-dimension step sizes be tuned; joint-distribution modeling would be needed to capture correlated real-world variability fully.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Vectorised ADR (VADR) provides an effective, scalable curriculum for expanding randomisation ranges during training and yields policies that transfer better to the real robot than manually tuned domain randomisation; limitations include lack of joint-distribution modeling and potential uneven exploration of parameter Pareto-limits.",
            "uuid": "e1791.1",
            "source_info": {
                "paper_title": "DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Vision Pose Estimator (Replicator)",
            "name_full": "Synthetic-data-trained keypoint-based pose estimator using Omniverse Isaac Sim / Replicator",
            "brief_description": "A Mask-RCNN-inspired network trained on 5M synthetic images rendered with Omniverse/Replicator (plus on-the-fly augmentations) to detect cube bounding box, segmentation, and 8 corner keypoints, followed by PnP triangulation across three cameras to produce the cube pose for the policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "DeXtreme perception module feeding the Allegro Hand policy",
            "agent_system_description": "Multi-camera RGB pose estimation pipeline producing palm-relative cube pose at runtime (15 Hz) using neural keypoint detection, PnP per-camera, filtering of inconsistent cameras, triangulation, and registration to cube model.",
            "domain": "robotics perception for manipulation / sim-to-real vision",
            "virtual_environment_name": "Omniverse Isaac Sim with Replicator",
            "virtual_environment_description": "Photorealistic synthetic image generator that simulates camera optics, materials, lighting, and visual noise (scanlines, film grain), used to produce millions of images with configurable visual randomisation for supervised pose estimator training.",
            "simulation_fidelity_level": "photorealistic visual rendering with heavy randomisation; supervised vision training-level fidelity (appearance-level), not modeling tactile/contact sensing.",
            "fidelity_aspects_modeled": "Lighting variability, material properties (albedo, roughness, metallic), camera intrinsics and poses, motion blur (via augmentation), camera noise, occlusions (hand visibility probability), backgrounds, and various image corruptions; data augmentations (CutMix, blur, brightness/contrast, cropping).",
            "fidelity_aspects_simplified": "Physical sensor readout dynamics beyond frame drops not fully modeled; pose estimator evaluated precisely only in simulation (real-world labelled pose ground truth not available); some real-world occlusion cases and interpenetrations when replaying real states in sim were not perfectly matched; no depth used (RGB only).",
            "real_environment_description": "Three Intel D415 RGB cameras capturing the real scene at inference; pose estimation runs at 15 Hz on a GPU, PnP triangulation across cameras and registration yields pose in palm frame; occasional heavy occlusion and pose jumps in real scenes were observed.",
            "task_or_skill_transferred": "Transfer of visual pose estimation from synthetic images to robust real-world cube pose estimates sufficient for closed-loop control of in-hand manipulation.",
            "training_method": "Supervised training of a Mask-RCNN-like network on 5M synthetic images (cross-entropy for segmentation/keypoints, smooth L1 for bounding box), plus on-the-fly augmentations and real-to-sim mining (replaying real configurations in sim to render more variations).",
            "transfer_success_metric": "Pose estimation error on synthetic test set (rotation and translation) and downstream metric: policy consecutive successes when fed the estimator output on real robot.",
            "transfer_performance_sim": "Pose estimator test on simulated testset: rotation error 5.3° ± 0.11°, translation errors: X 1.9 ± 0.1 mm, Y 4.1 ± 0.2 mm, Z 6.9 ± 0.4 mm (90% confidence intervals).",
            "transfer_performance_real": "Indirectly successful: policies using this estimator achieved up to 112 consecutive real-world successes in a single rollout (best vision-based policy) and average up to ~27.8 consecutive successes; however, some drop in performance vs simulation indicates residual perception sim-to-real gap.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Visual randomisation parameters: albedo desaturation/add/brightness, diffuse tint, reflection roughness, metallic/specular levels, camera noise, scanlines, film grain, random splotches, random backgrounds, camera pose sampling, focal length multiplier, hand visibility probability; augmentations: CutMix, blur, rotation, brightness/contrast, cropping.",
            "sim_to_real_gap_factors": "Heavy occlusions (policy can cage the cube causing keypoint occlusion), lighting-induced color shifts (stickers changing appearance), pose estimator occasional jumps, lower real camera refresh rates and synchronization issues, and inaccuracies when replaying real states in sim (interpenetrations) limiting perfect calibration.",
            "transfer_enabling_conditions": "Massive synthetic dataset generation (5M images) with extreme visual randomisation and data augmentation, real-to-sim mining by replaying real trajectories to render problematic configurations, multi-camera PnP filtering and triangulation, and keeping pose outputs in palm-relative frame to decouple from global placement.",
            "fidelity_requirements_identified": "Extensive visual randomisation and targeted sampling of hard occluded poses are necessary to produce robustness; maintaining accurate camera intrinsics and extrinsic calibration to palm frame is important; using multiple calibrated cameras and geometric PnP + triangulation improves real-world reliability.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "A pose estimator trained entirely on synthetic images with heavy visual randomisation and on-the-fly augmentations can produce sufficiently accurate pose estimates (few-degree rotation error, millimeter translation error in sim) to enable closed-loop vision-based dexterous manipulation on real hardware; however, occlusions and occasional pose jumps remain important failure modes that affect final transfer performance.",
            "uuid": "e1791.2",
            "source_info": {
                "paper_title": "DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2
        },
        {
            "paper_title": "Solving rubik's cube with a robot hand",
            "rating": 2
        },
        {
            "paper_title": "Isaac gym: High performance gpu-based physics simulation for robot learning",
            "rating": 2
        },
        {
            "paper_title": "Noise and the reality gap: The use of simulation in evolutionary robotics",
            "rating": 1
        }
    ],
    "cost": 0.015876,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality</h1>
<p>Ankur Handa<em> Arthur Allshire</em> Viktor Makoviychuk<em> Aleksei Petrenko</em><br>Ritvik Singh<em> Jingzhou Liu</em> Denys Makoviichuk<br>Karl Van Wyk<br>Alexander Zhurkevich Balakumar Sundaralingam Yashraj Narang<br>Jean-Francois Lafleche Dieter Fox Gavriel State</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The DeXtreme system using an Allegro Hand in action in the real world.</p>
<h2>Abstract</h2>
<p>Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in diverse kinds of hardware and simulator setups, and in our case, with the Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for researchers to achieve such results with commonly-available, affordable robot hands and cameras. Videos of the resulting policy and supplementary information, including experiments and demos, can be found at https://dextreme.org/.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>All authors are with NVIDIA (except for Denys Makoviichuk, who is with Snap). In addition to NVIDIA, Arthur Allshire, Jingzhou Liu, and Ritvik Singh are also with the University of Toronto, and Aleksei Petrenko is with the University of Southern California.</p>
<p>Author contributions listed at the end of the paper.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 2
2 Method ..... 3
2.1 Task ..... 3
2.2 Hardware ..... 4
2.3 Policy Learning with RL ..... 4
2.4 Reward Formulation ..... 6
2.5 Simulation ..... 6
2.6 Domain Randomisation ..... 7
2.6.1 Physics Randomisations ..... 8
2.6.2 Non-physics Randomisations ..... 10
2.6.3 Measuring ADR Performance in Training and in the Real World ..... 11
2.7 Pose Estimation ..... 11
3 Results ..... 13
3.1 Training in Simulation ..... 13
3.2 Real-World Policy Performance ..... 14
3.3 Quirks, Problems, and Surprises ..... 16
4 Related work ..... 17
5 Limitations ..... 18
6 Acknowledgements ..... 19
7 Contributions ..... 20
A Appendix ..... 25
A. 1 Compute Budget Comparisons ..... 25
A. 2 Hardware Comparisons ..... 25
A. 3 PPO Hyperparameters ..... 25
A. 4 Isaac Gym Simulation Parameters ..... 26
A. 5 Progressive Improvements in Consecutive Successes in the Real World ..... 26
A. 6 Default KUKA configuration ..... 26
A. 7 Software Tools Used in the Work ..... 26</p>
<h2>1 Introduction</h2>
<p>Multi-fingered robotic hands offer an exciting platform to develop and enable human-level dexterity. Not only do they provide kinematic redundancy for stable grasps, but they also enable the repertoire of skills needed to interact with a wide range of day-to-day objects. However, controlling such high-DoF end-effectors has remained challenging. Even in research, most robotic systems today use parallel-jaw grippers.</p>
<p>In 2018, OpenAI et al. [1] showed for the first time that multi-fingered hands with a purely end-to-end deep-RL based approach could endow robots with unprecedented capabilities for challenging contactrich in-hand manipulation. However, due to the complexity of their training architecture, and the sui generis nature of their work on sim-to-real transfer, reproducing and building upon their success has proven to be a challenge for the community. Recent advancements in in-hand manipulation with RL have made progress with multiple objects and an anthropomorphic hand [2], but those results have only been in simulation.</p>
<p>While the NLP and computer vision communities have reproduced and extended the successes of large-scale models like GPT-3 [3] and DALL-E [4, 5] respectively, similar efforts have remained elusive in robotics due to hardware and infrastructure challenges. Using large-scale data from simulations may provide avenues to unlock a similar step function in robotics capabilities.</p>
<p>This paper builds on top of the prior work in [1]. We use a comparatively affordable Allegro Hand with a locked wrist and four fingers, using only position encoders on servo motors; the Shadow Hand used in OpenAI's experiments costs an order of magnitude more than the Allegro Hand. We also develop a simple vision system that requires no specialised tracking or infrastructure on the hand; the system works on three off-the-shelf RGB cameras compared to OpenAI's expensive marker-based setup, making our system easily accessible for everyone. Furthermore, we use the GPU-based Isaac Gym physics simulator [6] as opposed to the CPU-based MuJoCo [7], which allows us to reduce the amount of computational resources used and the complexity of the training infrastructure. Our best models required only 8 NVIDIA A40 GPUs to train, as opposed to OpenAI's use of a CPU cluster composed of 400 servers with 32 CPU-cores each, as well as 32 NVIDIA V100 GPUs [8] (compute requirements for block reorientation). Our more affordable hand, in combination with the simple vision system architecture and accessible compute, dramatically simplifies the process of developing and deploying agile and dexterous manipulation. We summarise our contributions below:</p>
<ul>
<li>We demonstrate a system for learning-based dexterous in-hand manipulation that uses lowcost hardware (one order of magnitude less expensive than [1]), uses a purely vision-based pipeline, sets more diverse pose targets, uses orders-of-magnitude cheaper compute, and offers further insights into this problem with detailed ablations.</li>
<li>We develop a highly robust pose estimator trained entirely in simulation which works through heavy occlusions and in a variety of robotic settings e.g. https://www.youtube. com/watch?v=-MTsm0Uh_5o.</li>
<li>While not directly comparable to [1] due to different hardware, our purely vision-based state estimation results not only outperform their best vision-based results, but also fare comparably to their marker-based results.</li>
<li>We will also release both our vision and RL pipelines for reproducibility. We seek to provide a much broader segment of the research community with access to a novel state-of-the-art in-hand manipulation system in hopes of catalyzing further studies and advances.</li>
</ul>
<h1>2 Method</h1>
<h3>2.1 Task</h3>
<p>We propose a method for performing object reorientation on an anthropomorphic hand. Initially the object to be manipulated is placed on the palm of the hand and a random target orientation is sampled in $S O(3)^{2}$. The policy then orchestrates the motion of the fingers so as to bring the object to its desired target orientation. Similar to OpenAI et al. [1], if the object orientation is within a specified threshold of 0.4 radians of the target orientation, we sample a new target orientation. The fingers continue from the current configuration and aim to move the object to its new target orientation. The success criterion is the number of consecutive target orientations achieved without dropping the object or having the object stuck in the same configuration for more than 80 seconds. Importantly, each consecutive success becomes increasingly harder to achieve as the fingers have to keep the object in the hand without dropping, hence testing the policy's ability to model the dynamics on the go.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The hardware setup used in this work, unlike [1], is not housed in a cage, and our system is robust enough to perform the task in an open laboratory environment. The background in the image is alpha-blended for visibility.</p>
<p>For a quick and high level understanding of this work, we encourage readers to watch the video https://www.youtube.com/watch?v=TAUiaYAVkfI. Figure 3 provides a quick overview of different components involved in the system.</p>
<h1>2.2 Hardware</h1>
<p>Our hardware setup (see Fig 2) consists of an Allegro Hand rigidly mounted at the wrist. We use 3 Intel D415 cameras for object tracking with RGB frames i.e. no depth images were used. The cameras are extrinsically calibrated relative to the palm link of the hand. Our object tracking is done entirely using a vision-based system, and in contrast to [1], we do not use any marker-based system to track the cube or fingertip states.
The object we learn to manipulate is a 6.5 cm cuboid with coloured and lettered stickers on the sides. These stickers allow the vision system to distinguish between different faces (see Sec. 2.7). The pose of the cube is represented with respect to the palm of the robot hand. The camera-camera extrinsics and camera-robot calibration allow us to transform the cube pose from the canonical reference frame of a camera to the palm. Since the cube is represented locally in the palm reference frame, the policy performance is not dependent on the physical location of the setup, enabling us to move the setup freely whenever desired.</p>
<h3>2.3 Policy Learning with RL</h3>
<p>RL Formulation: The task of manipulating the cube to the desired orientation is modelled as a sequential decision making problem where the agent interacts with the environment in order to maximise the sum of discounted rewards. In our case, we formulate it as a discrete-time, partially observable Markov Decision Process (POMDP). We use Proximal Policy Optimisation (PPO) [9] to learn a parametric stochastic policy $\pi_{\theta}$ (actor), mapping from observations $o \in \mathcal{O}$ to actions $a \in \mathcal{A}$. PPO additionally learns a function $V_{\phi}^{\pi}(s, o)$ (critic) to approximate the on-policy value function. Following Pinto et al. [10], the critic does not take in the same observations as the actor, but receives additional observations including states $s \in \mathcal{S}$ in the POMDP. The actor and critic observations are detailed in Table 1.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Policy training.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Vision data generation and training pipeline.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(c) Functioning in the real world.</p>
<p>Figure 3: High level overview of the training and inference systems.</p>
<table>
<thead>
<tr>
<th>Input</th>
<th>Dimensionality</th>
<th>Actor</th>
<th>Critic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Object position with noise</td>
<td>3D</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Object orientation with noise</td>
<td>4D (quaternion)</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Target position</td>
<td>3D</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Target orientation</td>
<td>4D (quaternion)</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Relative target orientation</td>
<td>4D (quaternion)</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Last actions</td>
<td>16D</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Hand joints angles</td>
<td>16D</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Stochastic delays</td>
<td>4D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Fingertip positions</td>
<td>12D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Fingertip rotations</td>
<td>16D (quaternions)</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Fingertip velocities</td>
<td>24D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Fingertip forces and torques</td>
<td>24D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Hand joints velocities</td>
<td>16D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Hand joints generalised forces</td>
<td>16D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>object scale, mass, friction</td>
<td>3D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Object linear velocity</td>
<td>3D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Object angular velocity</td>
<td>3D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Object position</td>
<td>3D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Object rotation</td>
<td>4D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Random forces on object</td>
<td>3D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Domain randomisation params</td>
<td>78D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Gravity vector</td>
<td>3D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Rotation distances</td>
<td>2D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Hand scale</td>
<td>1D</td>
<td>$\times$</td>
<td>$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Observations of the policy and value networks. The input vector is 50D in size for policy and 265D for the value function.</p>
<p>We use a high-performance PPO implementation from rl-games [11] with the following hyperparameters: discount factor $\gamma=0.998^{3}$, clipping $\epsilon=0.2$. While in some experiments the learning rate was updated adaptively based on a fixed KL threshold 0.016, our best result was obtained using linear scheduling of the learning rate for the policy (start value $l r=1 e-4$ ) and a fixed learning rate for the value function $(l r=5 e-5)$. Our best policy $\pi_{\theta}: \mathcal{O} \times \mathcal{H} \rightarrow \mathcal{A}$ was a Long Short-Term Memory (LSTM) network [12] taking in environment observations $o$ and previous hidden state $h \in \mathcal{H}$. We use an LSTM backpropagation through time (BPTT) truncation length of 16. The LSTM has 1024 hidden units with layer normalization and is followed by 2 multilayer perceptron (MLP) layers with sizes 512 and ELU activation [13]. The action space $\mathcal{A}$ of our policy is the PD controller target for each of the 16 joints on the robot hand. The value function LSTM layer has 2048 hidden units with layer normalization, followed by 2 MLP layers with 1024 and 512 units respectively with ELU activation. The output of the policy is low-pass filtered with an exponential moving average (EMA) smoothing factor. During training this factor is annealed from 0.2 to 0.15 . Our best results in the real world were obtained with an EMA of 0.1 , which provided a balance between agility and stability of the motion, preventing the hardware from breaking or motor cables from burning.</p>
<h1>2.4 Reward Formulation</h1>
<p>The reward formulation is inspired by the Shadow hand environment in Isaac Gym[6], and described and justified in Table 2.</p>
<h3>2.5 Simulation</h3>
<p>Our aim in this paper is to learn dexterous manipulation behaviours. Current on-policy learning algorithms can struggle to accomplish this on real robots due to the number of samples required. Hence, we learn our behaviours entirely in simulation. We use the GPU-based Isaac Gym physics</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reward</th>
<th style="text-align: center;">Formula</th>
<th style="text-align: center;">Weight</th>
<th style="text-align: center;">Justification</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Rotation Close to Goal</td>
<td style="text-align: center;">$\left\lfloor\left(\beta+0.1\right)\right.$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">Shaped reward to bring cube close to goal</td>
</tr>
<tr>
<td style="text-align: center;">Position Close to Fixed Target</td>
<td style="text-align: center;">$\left|\mathrm{p}<em _goal="{goal" _text="\text">{\text {object }}-p</em>\right|$}</td>
<td style="text-align: center;">-10.0</td>
<td style="text-align: center;">Encourage the cube to stay in the hand</td>
</tr>
<tr>
<td style="text-align: center;">Action Penalty</td>
<td style="text-align: center;">$|\mathrm{a}|^{2}$</td>
<td style="text-align: center;">-0.001</td>
<td style="text-align: center;">Prevent actions that are too large</td>
</tr>
<tr>
<td style="text-align: center;">Action Delta Penalty</td>
<td style="text-align: center;">$\left|\operatorname{targ}<em _gess="{gess" _text="\text">{\text {pass }}-\operatorname{targ}</em>$}}\right|^{2</td>
<td style="text-align: center;">-0.25</td>
<td style="text-align: center;">Prevent rapid changes in joint target</td>
</tr>
<tr>
<td style="text-align: center;">Joint Velocity Penalty</td>
<td style="text-align: center;">$\left|v_{\text {joints }}\right|^{2}$</td>
<td style="text-align: center;">-0.003</td>
<td style="text-align: center;">Stop fingers from moving too quickly</td>
</tr>
<tr>
<td style="text-align: center;">Reset Reward</td>
<td style="text-align: center;">Condition</td>
<td style="text-align: center;">Value</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Reach Goal Bonus</td>
<td style="text-align: center;">$\mathrm{d}&lt;0.1$</td>
<td style="text-align: center;">250.0</td>
<td style="text-align: center;">Large reward for getting the cube to the target</td>
</tr>
</tbody>
</table>
<p>Table 2: Reward terms are computed, multiplied by their weight, and summed to produce the reward at each timestep. d represents the rotational distance from the object's current to the target orientation. $\mathrm{p}<em _goal="{goal" _text="\text">{\text {object }}$ and $\mathrm{p}</em>}}$ are the position of the object and goal respectively. $a$ is the current action. $\operatorname{targ<em _gess="{gess" _text="\text">{\text {curs }}$ and $\operatorname{targ}</em>$ is the current joint velocity vector.}}$ are the current and previous joint position targets. $\mathrm{v}_{\text {joints }</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Parameter</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Distribution</th>
<th style="text-align: center;">Initial Range</th>
<th style="text-align: center;">ADR-Discovered Range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Hand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mass</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.4,1.5]$</td>
<td style="text-align: center;">$[0.4,1.5]$</td>
</tr>
<tr>
<td style="text-align: center;">Scale</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.95,1.05]$</td>
<td style="text-align: center;">$[0.95,1.05]$</td>
</tr>
<tr>
<td style="text-align: center;">Friction</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.8,1.2]$</td>
<td style="text-align: center;">$[0.54,1.58]$</td>
</tr>
<tr>
<td style="text-align: center;">Armature</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.8,1.02]$</td>
<td style="text-align: center;">$[0.31,1.24]$</td>
</tr>
<tr>
<td style="text-align: center;">Effort</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.9,1.1]$</td>
<td style="text-align: center;">$[0.9,2.49]$</td>
</tr>
<tr>
<td style="text-align: center;">Joint Stiffness</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">loguniform</td>
<td style="text-align: center;">$[0.3,3.0]$</td>
<td style="text-align: center;">$[0.3,3.52]$</td>
</tr>
<tr>
<td style="text-align: center;">Joint Damping</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">loguniform</td>
<td style="text-align: center;">$[0.75,1.5]$</td>
<td style="text-align: center;">$[0.43,1.6]$</td>
</tr>
<tr>
<td style="text-align: center;">Restitution</td>
<td style="text-align: center;">Additive</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.0,0.4]$</td>
<td style="text-align: center;">$[0.0,0.4]$</td>
</tr>
<tr>
<td style="text-align: center;">Object</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mass</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.4,1.6]$</td>
<td style="text-align: center;">$[0.4,1.6]$</td>
</tr>
<tr>
<td style="text-align: center;">Friction</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.3,0.9]$</td>
<td style="text-align: center;">$[0.01,1.60]$</td>
</tr>
<tr>
<td style="text-align: center;">Scale</td>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.95,1.05]$</td>
<td style="text-align: center;">$[0.95,1.05]$</td>
</tr>
<tr>
<td style="text-align: center;">External Forces</td>
<td style="text-align: center;">Additive</td>
<td style="text-align: center;">Refer to [1]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Restitution</td>
<td style="text-align: center;">Additive</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.0,0.4]$</td>
<td style="text-align: center;">$[0.0,0.4]$</td>
</tr>
<tr>
<td style="text-align: center;">Observation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Obj. Pose Delay Prob.</td>
<td style="text-align: center;">Set Value</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.0,0.05]$</td>
<td style="text-align: center;">$[0.0,0.47]$</td>
</tr>
<tr>
<td style="text-align: center;">Obj. Pose Freq.</td>
<td style="text-align: center;">Set Value</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[1.0,1.0]$</td>
<td style="text-align: center;">$[1.0,6.0]$</td>
</tr>
<tr>
<td style="text-align: center;">Obs Corr. Noise</td>
<td style="text-align: center;">Additive</td>
<td style="text-align: center;">gaussian</td>
<td style="text-align: center;">$[0.0,0.04]$</td>
<td style="text-align: center;">$[0.0,0.12]$</td>
</tr>
<tr>
<td style="text-align: center;">Obs Uncorr. Noise</td>
<td style="text-align: center;">Additive</td>
<td style="text-align: center;">gaussian</td>
<td style="text-align: center;">$[0.0,0.04]$</td>
<td style="text-align: center;">$[0.0,0.14]$</td>
</tr>
<tr>
<td style="text-align: center;">Random Pose Injection</td>
<td style="text-align: center;">Set Value</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.3,0.3]$</td>
<td style="text-align: center;">$[0.3,0.3]$</td>
</tr>
<tr>
<td style="text-align: center;">Action</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Action Delay Prob.</td>
<td style="text-align: center;">Set Value</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.0,0.05]$</td>
<td style="text-align: center;">$[0.0,0.31]$</td>
</tr>
<tr>
<td style="text-align: center;">Action Latency</td>
<td style="text-align: center;">Set Value</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.0,0.0]$</td>
<td style="text-align: center;">$[0.0,1.5]$</td>
</tr>
<tr>
<td style="text-align: center;">Action Corr. Noise</td>
<td style="text-align: center;">Additive</td>
<td style="text-align: center;">gaussian</td>
<td style="text-align: center;">$[0.0,0.04]$</td>
<td style="text-align: center;">$[0.0,0.32]$</td>
</tr>
<tr>
<td style="text-align: center;">Action Uncorr. Noise</td>
<td style="text-align: center;">Additive</td>
<td style="text-align: center;">gaussian</td>
<td style="text-align: center;">$[0.0,0.04]$</td>
<td style="text-align: center;">$[0.0,0.48]$</td>
</tr>
<tr>
<td style="text-align: center;">RNA $\alpha$</td>
<td style="text-align: center;">Set Value</td>
<td style="text-align: center;">uniform</td>
<td style="text-align: center;">$[0.0,0.0]$</td>
<td style="text-align: center;">$[0.0,0.16]$</td>
</tr>
<tr>
<td style="text-align: center;">Environment</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gravity (each coord.)</td>
<td style="text-align: center;">Additive</td>
<td style="text-align: center;">normal</td>
<td style="text-align: center;">$[0,0.5]$</td>
<td style="text-align: center;">$[0,0.5]$</td>
</tr>
</tbody>
</table>
<p>Table 3: Domain randomisation parameter ranges for policy learning
simulator [6], which models contacts differently than MuJoCo's soft-contact model [7] used in [1]. Isaac Gym gives the advantage of being able to simulate thousands of robots in parallel on a single GPU, mitigating the need for large amounts of CPU resources.</p>
<h1>2.6 Domain Randomisation</h1>
<p>It is widely known that there is a "sim-to-real" gap between physics simulators and real life [14]. Compounding this is the fact that the robot as a system can change from day to day (e.g., due to wear-and-tear) and even from timestep to timestep (e.g., stochastic noise). To help overcome this, we introduce various kinds of randomisations [15] into the simulated environment as listed in Table 3.</p>
<p>Vectorised Automatic Domain Randomisation: In our best policies, we set the parameters of the domain randomisations via a vectorised implementation of Automatic Domain Randomisation (ADR, introduced in [8]). ADR automatically adjusts the range of domain randomisations to keep them as wide as possible while keeping the policy performance above a certain threshold. This allows us to train policies with less randomisation earlier in training (enabling behaviour exploration) while producing final policies that are robust to the largest range of environment conditions possible at</p>
<p>the end of training, with the aim of improving sim-to-real transfer by learning policies which are robust and adaptive to a range of environment randomisation parameters. Using the parallelisation provided by Isaac Gym, we implement a vectorised variant of the algorithm, which we call Vectorised Automatic Domain Randomisation (VADR, see Algorithm 1).</p>
<p>The range of randomisations for each environment parameter in ADR is modelled as a uniform distribution $d^{n} \sim U\left(p^{2 n}, p^{2 n+1}\right)$, where $p^{2 n}$ and $p^{2 n+1}$ are the current lower and upper randomisation boundaries, and $n \in 0, \ldots D-1$ is the parameter index for each of the $D$ ADR dimensions. Each dimension starts with initial values from system calibration or best-guesses for the randomisation bounds, $p_{\text {init }}^{2 n}$ and $p_{\text {init }}^{2 n+1}$ for the lower and upper bounds of parameter $n$, respectively. Optional minimum and maximum bounds on the randomisations may also be specified, $p_{\text {min }}^{n}$ and $p_{\text {max }}^{n}$. Unlike in [8], we choose the size of step $\Delta^{n}$ separately for each parameter. This trades off more tuning work for more stable training and the mitigation of the need for custom, secondary distributions on top of certain randomisation dimensions, as were used in that work.</p>
<p>Evaluation proceeds as follows: environments sample a value for each randomisation dimension uniformly between the upper and lower bounds. A fraction ( $40 \%$ ) of the vectorised environments are dedicated to evaluation. In these environments, one of the ADR randomisation dimensions is fixed to the current lower or upper boundary (the rest of the dimensions are sampled from the aforementioned uniform distribution set by ADR). The episode proceeds to roll out; the number of consecutive successes is recorded at the end of the episode. This figure is added to a queue for the boundary of maximum length $N=256$. Then, if mean consecutive sucesses on the boundary is above a certain threshold, $t_{H}=20$, the range is widened, and if the performance is below a lower threshold $t_{L}=5$, then the range is tightened on that bound. This is depicted in Figure 4. If on a particular step the value of a bound changes, the queue is cleared (as the previous performance data then becomes invalid). In this way, ADR will discover the maximum ranges over which a policy can perform, including for example discovering the limits of parameters impacting physics stability. Our vectorised ADR (VADR) full algorithm is described below in Algorithm 1 and Algorithm 2.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 4: Parameter range adjustments, $p^{1, \mathrm{lo}}$ and $p^{1, \mathrm{hi}}$, with ADR based on the performance of policy at the boundaries $Q^{1, \text { lo }}$ and $Q^{1, \text { hi }}$ with respect to the thresholds $t_{l}$ and $t_{h}$. If the mean performance at the boundary $Q^{1, \text { lo }}$ is less than threshold $t_{l}$, the range is tightened (a) and if it is above a threshold $t_{h}$, the range is expanded (c). Similarly, if the mean performance at the boundary $Q^{1, \text { hi }}$ is above a threshold $t_{h}$, the range is expanded (b) and if it is lower than $t_{l}$, the range is tightened (d).</p>
<p>When training on multiple GPUs (8 for our best policies), we ran VADR separately on each one. This was done for two reasons: firstly, to avoid additional synchronisation overhead of buffers. Secondly, to partially mitigate the disadvantage of ADR noted below in Section 3.3 caused by the failure to model the joint distribution; having multiple independent parameter sets to some extent will allow multiple sets of extreme parameters. All randomisations are set by ADR. ${ }^{4}$ In the following, we describe the physics and non-physics randomisations in more detail.</p>
<h1>2.6.1 Physics Randomisations</h1>
<p>We apply physics randomisations to account for both changing real-world dynamics and the inevitable gaps between physics in simulation and reality. These include basic properties such as mass, friction</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>Function ADRUpdate (p, success_counts, ADR_mode)
/* Function to decide whether to update each ADR boundary&#39;s value
    based on the success level of just-completed episodes.
for \(n=0, \ldots, D-1\) do
    i_lo \(=2 n\);
    i_hi \(=2 n+1\);
    \(Q^{i \_l o} \leftarrow\) success_counts [ADR_modes==i_lo];
    \(Q^{i \_h i} \leftarrow\) success_counts [ADR_modes==i_hi];
    if length \(\left(Q^{i \_l o}\right)==N\) then
        if mean \(\left(Q^{i \_l o}\right)&gt;t_{h}\) then
            \(p^{i \_l 0} \leftarrow p^{i \_l 0}-\Delta^{n}\)
            else if mean \(\left(Q^{i \_l o}\right)&lt;t_{l}\) then
                \(p^{i \_l 0} \leftarrow p^{i \_l 0}+\Delta^{n}\)
            if \(p^{i \_l s}\) changed then
                \(p^{i \_h i} \leftarrow \operatorname{clip}\left(p^{i \_l 0}, p_{\text {min }}^{n}, p_{\max }^{n}\right) ;\)
                \(\operatorname{Clear}\left(Q^{i \_l 0}\right)\)
    if length \(\left(Q^{i \_h i}\right)==N\) then
        if mean \(\left(Q^{i \_h i}\right)&gt;t_{h}\) then
            \(p^{i \_h i} \leftarrow p^{i \_h i}+\Delta^{n}\)
            else if mean \(\left(Q^{i \_h i}\right)&lt;t_{l}\) then
                \(p^{i \_h i} \leftarrow p^{i \_h i}-\Delta^{n}\)
            if \(p^{i \_h i}\) changed then
                \(p^{i \_h i} \leftarrow \operatorname{clip}\left(p^{i \_h i}, p_{\text {min }}^{n}, p_{\max }^{n}\right) ;\)
                \(\operatorname{Clear}\left(Q^{i \_h i}\right)\)
end
    return \(p\)
Function ResetADRVals(p, ADR_modes, ADR_vals)
    /* Function to resample ADR values for the next episode just after an
        episode is done. <span class="gs">*/</span>
<span class="gs">/*</span> NB. all operations here are vectorised, so eg. ADR_modes is a <span class="gs">*/</span>
<span class="gs">        tensor with \(60 \%\) of values -1 .</span>
<span class="gs">    ADR_modes \(=\left\{\begin{array}{ll}-1, &amp; \text { with } p=60 \% ; \\ \sim \text { Categorical }(0,2 D-1), &amp; \text { with } p=40 \% ;\end{array}\right.\)</span>
<span class="gs">    /*</span> Resample ADR values for each dimension for the next episode. */
    for \(n=0, \ldots, D-1\) do
        is_eval \(=\) ADR_modes \(==2 n \mid\) ADR_modes \(==2 n+1\)
            ADR_vals \(\{n\}=\left\{\begin{array}{ll}p^{\text {ADR }_{-} \text {modes }}, &amp; \text { where is_eval } \\ \sim U\left(p^{2 n}, p^{2 n+1}\right), &amp; \text { otherwise }\end{array}\right.\);
    end
    return ADR_modes, ADR_vals
Algorithm 2: Training with VADR.
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Initialise</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">ADR_modes</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">Z</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">N_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">en</span><span class="w"> </span><span class="err">}</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="n">s</span><span class="err">}}\</span><span class="p">)</span><span class="w"> </span><span class="n">storing</span><span class="w"> </span><span class="n">whether</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">normal</span>
<span class="w">    </span><span class="n">environment</span><span class="w"> </span><span class="p">(</span><span class="n">entry</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">=-</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">performing</span><span class="w"> </span><span class="n">sampling</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">boundaries</span>
<span class="w">    </span><span class="p">(</span><span class="n">entry</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">D</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="n">Initialise</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">ADR_vals</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="n">R</span><span class="o">^</span><span class="err">{</span><span class="n">N_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">en</span><span class="w"> </span><span class="err">}</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="n">s</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">times</span><span class="w"> </span><span class="n">D</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">storing</span><span class="w"> </span><span class="n">ADR</span><span class="w"> </span><span class="n">randomisation</span><span class="w"> </span><span class="k">values</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">parameter</span>
<span class="w">    </span><span class="ow">in</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="p">(</span><span class="n">matrix</span><span class="w"> </span><span class="n">storing</span><span class="w"> </span><span class="k">values</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">phi</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="nc">text</span><span class="p">).</span>
<span class="n">Initialise</span><span class="w"> </span><span class="n">ADR</span><span class="w"> </span><span class="n">boundaries</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">p</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="mi">2</span><span class="w"> </span><span class="n">D</span><span class="err">}\</span><span class="p">)</span>
<span class="k">while</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">do</span>
<span class="w">    </span><span class="n">dones</span><span class="p">,</span><span class="w"> </span><span class="n">successes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_step</span><span class="p">(</span><span class="n">ADR_vals</span><span class="p">);</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">ADRUpdate</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">successes</span><span class="o">[</span><span class="n">dones</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">ADR_mode</span><span class="o">[</span><span class="n">dones</span><span class="o">]</span><span class="p">);</span>
<span class="w">    </span><span class="n">ADR_modes</span><span class="o">[</span><span class="n">dones</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">ADR_vals</span><span class="o">[</span><span class="n">dones</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ResetADRVals</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">ADR_modes</span><span class="o">[</span><span class="n">dones</span><span class="o">]</span><span class="p">,</span>
<span class="w">    </span><span class="n">ADR_vals</span><span class="o">[</span><span class="n">dones</span><span class="o">]</span><span class="p">);</span>
<span class="k">end</span>
</code></pre></div>

<p>and restitution of the hand and object. We also randomly scale the hand and object to avoid overreliance on exact morphology. On the hand, joint stiffness, damping, and limits are randomised. Furthermore, we add random forces to the cube in a similar fashion to [1].
Joint Stiffness, Joint Damping, and Effort are scaled using the value sampled directly from the ADR-given uniform distribution.
Mass and Scale are randomised within a fixed range due to API limitations currently. However, we did not observe this as a significant limitation for our experiments, and our policies nevertheless achieved rollouts with high consecutive successes in the real world.
Gravity cannot be randomised per-environment in Isaac Gym currently, but a new gravity value is sampled every 720 concurrent simulation steps for all environments.</p>
<h1>2.6.2 Non-physics Randomisations</h1>
<p>In addition to normal physics randomisations, Table 3 lists action and observation randomisations, which we found to be critical to achieving good real-world performance. To make our policies more robust to the changing inference frequency and jitter resulting from our ROS-based inference system, we add stochastic delays to cube pose and action delivery time as well as fixed-for-an-episode action latency. To the actions and observations, we add correlated and uncorrelated additive Gaussian noise. To account for unmodelled dynamics, we use a Random Network Adversary (RNA, see below).</p>
<h2>Observation \&amp; Action Noise</h2>
<p>We apply Gaussian noise to the observations and actions with the noise function</p>
<p>$$
f_{\delta, \epsilon}(x)=x+\delta+\epsilon
$$</p>
<p>Where $\delta$ and $\epsilon$ are sampled from Gaussian distributions parameterised by the $A D R$ values $p^{i}, p^{j}$, $\delta \sim \mathcal{N}\left(\cdot ; 0, \operatorname{var}\left(p^{i}\right)\right), \epsilon \sim \mathcal{N}\left(\cdot ; 0, \operatorname{var}\left(p^{j}\right)\right)$ where $\operatorname{var}(a)=\exp \left[a^{2}\right]-1$
For $\delta$, this sampling happens once per episode at the beginning of the episode, corresponding to correlated noise. For $\epsilon$, sampling happens at every timestep. Note that the formula for var has a cutoff at 0 noise. This allows ADR to set a certain fraction of environments to have 0 noise, which we found an important case that is not covered in previous works when setting fixed or above-zero cutoff variance (since during inference, zero white noise is added).</p>
<h2>Latency \&amp; Delay</h2>
<p>We apply three forms of delay. The first is an exponential delay, where the chance of applying a delay each step is $p^{i}$ and is given by $f\left(x ; x_{\text {last }}\right)=x_{\text {last }} \cdot \mathrm{d}+x \cdot(1-\mathrm{d})$ and $\mathrm{d} \sim \operatorname{Bern}\left(\cdot ; p^{i}\right)$ is the Bernoulli distribution parametrised by the $i$-th ADR variable, $p^{i} \in[0,1)$. This delay case, applied to both observations of cube pose and actions, mimics random jitter in latency times.
The second form of delay is action latency, where the action from n timesteps ago is executed. For this parameter, we slightly modify the vanilla ADR formulation to allow smooth increase in delay with ADR value despite the discretisation of timesteps. The bounds are still continuously modified, but the sampling from the range is done from a categorical distribution. Specifically, let $\epsilon \sim U(0, b)+U(-0.5,0.5)$ be the sampled ADR value (plus random noise used to allow probabilistic blending of delay steps when sampling on the ADR boundary). Then the delay k is $k=\operatorname{round}(\epsilon)$.
A third form of delay, this time on observation, is that caused by the refresh rate of the cameras in the real world. To compensate for this, we have randomisation on the refresh rate. Similarly to the aforementioned action latency, we use ADR to sample a categorical action delay $d \in\left{1, \ldots, \operatorname{delay}_{\text {max }}\right}$. We then only update the cube pose observation if $(t+r) \bmod d=0$, effectively mimicing a pose estimation frequency of $d \cdot \Delta t$ (where r is a randomly sampled alignment variable to offset updates from the beginning of the episode randomly).</p>
<h2>Random Pose Injection</h2>
<p>We noticed that due to heavy occlusion and caging from the fingers, our cube pose estimator exhibited occasional jumps. To ensure that the policy performance did not deteriorate and LSTM hidden state become corrupted by this, we occasionally inject completely random cube poses into the network. At the start of each episode for each environment, we sample a probability $p \in U(0,0.3)$.</p>
<h1>Random Network Adversary</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5: The functioning of the Random Network Adversary</p>
<p>Then each step we sample a variable $m \sim \operatorname{Bern}(\cdot ; p)$, and the cube pose becomes: pose_obs $=$ pose $\cdot(1-m)+$ random_pose $\cdot m$.</p>
<h2>Random Network Adversary</h2>
<p>Random Network Adversary, introduced in [8], uses a randomly-generated neural network each episode to introduce much more structured, state-varying noise patterns into the environment, in contrast to normal Gaussian noise. As we are doing simulation on GPU rather than CPU, instead of using a new network per environment-episode and wasting memory on thousands of individual MLPs, we generate a single network across all environments and use a unique and periodically refreshed dropout pattern per environment. Actions from the RNA network are blended with those from the policy by $\mathbf{a}=\alpha \cdot \mathbf{a}<em _policy="{policy" _text="\text">{\text {RNA }}+(1-\alpha) \cdot \mathbf{a}</em>$, where $\alpha$ is controlled by ADR.}</p>
<h3>2.6.3 Measuring ADR Performance in Training and in the Real World</h3>
<p>Nats per Dimension (npd) was a metric developed by OpenAI in [8] to measure the amount of randomisation through the average entropy across the ADR distributions. While it does not directly capture the difficulty of the environment (since each dimension is not normalised for difficulty), it provides a rough proxy for how much randomisation there is in the environment. The formula for nats per dimension is given by:</p>
<p>$$
\text { npd }=\frac{1}{D} \sum_{n=0}^{D-1} \log \left(p^{2 n+1}-p^{2 n}\right)
$$</p>
<p>Currently, we measure real-world performance based on the number of consecutive successes. An avenue for future work is directly exploring how real-world policy performance corresponds to ADR randomisation levels in total and across different dimensions.</p>
<h3>2.7 Pose Estimation</h3>
<p>Data Generation and Processing: We use NVIDIA Omniverse Isaac Sim with Replicator ${ }^{5}$ to generate 5M images of the cube in hand in just under a day. Each image is $320 \times 240$ in resolution and contains visual domain randomisations as summarised in Table 4 to add variety to the training set. Such visual domain randomisations allow the network to be robust to different camera parameters and visual effects that may be present in the scene. In addition, we apply data augmentations during training on a batch in order to add even more variety to the training set. As such, a single rendered image from the dataset can provide multiple training examples with different data augmentation settings, thereby saving both rendering time and storage space. For instance, motion blur (important in our case where we have a fast-moving object to track) can be especially time-consuming at rendering time. Instead we generate it on the fly via motion-blur data augmentation by smearing the image with a Gaussian kernel with the blur direction and extent chosen randomly. The data augmentations used on the images are listed in Table 5. Each augmentation is applied with a fixed probability to ensure that the batch consists of a combination of the original as well as augmented images.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: center;">Probability Distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Albedo desaturation</td>
<td style="text-align: center;">uniform $(0.0,0.3)$</td>
</tr>
<tr>
<td style="text-align: left;">Albedo add</td>
<td style="text-align: center;">uniform $(-0.2,0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">Albedo brightness</td>
<td style="text-align: center;">uniform $(0.5,1.0)$</td>
</tr>
<tr>
<td style="text-align: left;">Diffuse tint</td>
<td style="text-align: center;">normal $(1.0,0.3)$</td>
</tr>
<tr>
<td style="text-align: left;">Reflection roughness constant</td>
<td style="text-align: center;">uniform $(0.0,1.0)$</td>
</tr>
<tr>
<td style="text-align: left;">Metallic constant</td>
<td style="text-align: center;">uniform $(0.0,0.5)$</td>
</tr>
<tr>
<td style="text-align: left;">Specular level</td>
<td style="text-align: center;">uniform $(0.0,1.0)$</td>
</tr>
<tr>
<td style="text-align: left;">Enable camera noise</td>
<td style="text-align: center;">bernoulli $(0.3)$</td>
</tr>
<tr>
<td style="text-align: left;">Enable scan lines</td>
<td style="text-align: center;">bernoulli $(0.3)$</td>
</tr>
<tr>
<td style="text-align: left;">Scan line spread</td>
<td style="text-align: center;">uniform $(0.1,0.5)$</td>
</tr>
<tr>
<td style="text-align: left;">Enable vertical lines</td>
<td style="text-align: center;">bernoulli $(0.1)$</td>
</tr>
<tr>
<td style="text-align: left;">Enable film grain</td>
<td style="text-align: center;">bernoulli $(0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">Grain amount</td>
<td style="text-align: center;">uniform $(0.0,0.15)$</td>
</tr>
<tr>
<td style="text-align: left;">Grain size</td>
<td style="text-align: center;">uniform $(0.7,1.2)$</td>
</tr>
<tr>
<td style="text-align: left;">Enable random splotches</td>
<td style="text-align: center;">bernoulli $(0.1)$</td>
</tr>
<tr>
<td style="text-align: left;">Colour amount</td>
<td style="text-align: center;">uniform $(0.0,0.3)$</td>
</tr>
<tr>
<td style="text-align: left;">Hand visibility</td>
<td style="text-align: center;">bernoulli $(0.75)$</td>
</tr>
<tr>
<td style="text-align: left;">Camera pose</td>
<td style="text-align: center;">Hemispherical shell</td>
</tr>
<tr>
<td style="text-align: left;">Camera f/l multiplier</td>
<td style="text-align: center;">uniform $(0.99,1.01)$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Data Augmentation Type</th>
<th style="text-align: center;">Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CutMix (see [16])</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Random Blurring</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Random Background</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">Random Rotation</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Random Brightness and Contrast</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Random Cropping and Resizing</td>
<td style="text-align: center;">0.5</td>
</tr>
</tbody>
</table>
<p>Table 5: Various data augmentations applied to the images on the fly during training. We also set a probability for each one of them.
Table 4: Ranges of vision parameter randomisations.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Illustration of generated data. Top row: Images generated from NVIDIA Isaac Sim, demonstrating different camera locations, lighting conditions, and camera poses. Bottom row: Images after data augmentations such as CutMix, random cropping and rotation, and motion blur are applied during training.</p>
<p>We also collect configurations of the cube in-hand generated by our policies running in the real world and play them back in Isaac Sim to render data where the pose estimates are not fully reliable i.e. the pose estimator is not accurate all the time. This happens due to the sim-to-real gap as a result of either sparse sampling or insufficient lighting randomisations for those configurations. Playback in simulation enables dense sampling of pose and lighting around these configurations with more randomisations. This allows us to generate larger datasets that improve the reliability of the pose estimator in the real world i.e. closing the perception loop between real and sim by mining configurations from the current best policy in the real world and using them in sim to render more images. We use the same intrinsics of the cameras in the real world, but randomise extrinsics when rendering data.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Experiment</th>
<th>Avg. Rotation Error</th>
<th>Avg. Translation Error</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>X</td>
<td>Y</td>
<td>Z</td>
</tr>
<tr>
<td>Sim</td>
<td>$5.3 \pm 0.11^{\circ}$</td>
<td>$1.9 \pm 0.1 \mathrm{~mm}$</td>
<td>$4.1 \pm 0.2 \mathrm{~mm}$</td>
<td>$6.9 \pm 0.4 \mathrm{~mm}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Rotation and translation error on test dataset with $90 \%$ confidence intervals.</p>
<p>Training setup and inference: We use a torchvision Mask-RCNN [17]-inspired network ${ }^{7}$ that regresses to the bounding box, segmentation, and keypoints located at the 8 corners of the cube. The bounding box localises the cube in the image, and the keypoint head regresses to the positions of the 8 corners of the cube within the bounding box. The networks are trained with cross-entropy loss for segmentation and keypoint location regression, and smooth L1 loss for bounding box regression. We use the ADAM optimiser with a learning rate of $1 \mathrm{e}-4$. The network runs on three cameras at an inference rate of 20 Hz on an NVIDIA RTX 3090 GPU and a 32-core AMD Ryzen Threadripper CPU. However, because the policy was trained with a control frequency of 30 Hz in simulation, the pose estimator was locked to run at 15 Hz to ensure that the policy receives pose observations at a constant integer interval of once every two control steps. To make the pose estimate reliable for the downstream policy, we first perform classic PnP [18] on each of the three cameras independently and then filter out the ones where the projected keypoints from the PnP pose do not match the inferred keypoints from the network. We triangulate the keypoints from the filtered cameras and register them against the model of the cube to obtain the pose in a canonical reference frame. We use the OpenCV implementation of PnP and roma [19] for registering keypoints against the model of the cube. We benchmark the pose on a test set consisting of 50 K images and provide results in Table 6 . Since we do not use any marker-based system in the real world, we can only precisely evaluate the performance of the pose estimator in simulation. Our ablation studies in Section 3.2 do test the strength of the pose estimator for manipulation in the real world. One important difference between our approach and OpenAI et al. [1] is that our pose estimation is not done end-to-end. Since we detect keypoints in the image and use geometric computer vision to obtain the pose, our pose estimator is not tied to a fixed camera setup, unlike [1].</p>
<h1>3 Results</h1>
<p>In the following section, we present the results we achieved in object reorientation in the simulations and then real world using the methods described in Section 2. We then follow it up with tests of policy robustness in reality and simulation.</p>
<h3>3.1 Training in Simulation</h3>
<p><img alt="img-8.jpeg" src="img-8.jpeg" />
(a) Manual DR training curve.
<img alt="img-9.jpeg" src="img-9.jpeg" />
(b) ADR npd evolution.
<img alt="img-10.jpeg" src="img-10.jpeg" />
(c) ADR parameter evolution.</p>
<p>Figure 7: Training curve evolution for (a) manual domain randomisation, which takes $\sim 24$ hours to reach an average of 35 consecutive successes, (b) ADR experiments showing how the average nats per distribution (npd), an indicator of the extent of randomisation, increases as a function of training (c) shows how parameters (normalised) optimised with ADR evolve over time. All curves are from experiments in simulations.</p>
<p>For all of our experiments, we use a simulation $d t$ of $\frac{1}{50} s$ and a control $d t$ of $\frac{1}{40} s$. We train with 16384 agents per GPU and use a goal-reaching orientation threshold of 0.1 rad but test with 0.4 rad</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>as in $[1,6]$ for all experiments both in simulation and the real world. All policies are trained with randomisations described in Table 3. Most importantly, our ADR policies using the same compute resources - 8 NVIDIA A40s - achieved the best performance in the real world after training for only 2.5 days in contrast to [8] that trained for 2 weeks to months for the task of block reorientation ${ }^{8}$. Various training curves for our task a) with manual DR, b) with automatic domain randomisation (ADR), and c) ADR parameter evolution are presented in Figure 7. We note that due to differences in physics engines and hand morphology, our simulation average consecutive successes are not directly comparable, but we achieve performance on par with $[1,8]$.
Training with manual DR takes roughly 32 hours to converge on 8 NVIDIA A40s generating a combined (across all GPUs) frame rate of 700 K frames $/ \mathrm{sec}$. With a $d t=\frac{1}{60}$, this amounts to $\frac{32 \times 700000}{60 \times 24 \times 365}$ which is $\sim 42$ years of real-world experience.</p>
<h1>3.2 Real-World Policy Performance</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">Cons. Success Trials (sorted)</th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;">Median</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Best Model</td>
<td style="text-align: center;">$1,6,6,10,10,18,18,36,61,112$</td>
<td style="text-align: center;">$27.8 \pm 19.0$</td>
<td style="text-align: center;">14.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$3,4,7,16,19,22,29,31,58,77$</td>
<td style="text-align: center;">$26.6 \pm 13.2$</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$1,5,5,11,12,12,33,36,42,51$</td>
<td style="text-align: center;">$20.8 \pm 9.8$</td>
<td style="text-align: center;">12.0</td>
</tr>
<tr>
<td style="text-align: center;">Best Model <br> (Goal frame count=10)</td>
<td style="text-align: center;">$6,8,10,16,16,17,20,33,39,45$</td>
<td style="text-align: center;">$21.0 \pm 7.4$</td>
<td style="text-align: center;">16.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$9,11,13,13,15,16,27,29,32,36$</td>
<td style="text-align: center;">$20.1 \pm 5.4$</td>
<td style="text-align: center;">15.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$2,3,3,9,11,12,14,15,43,44$</td>
<td style="text-align: center;">$16.6 \pm 8.4$</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: center;">Non-ADR Model</td>
<td style="text-align: center;">$2,3,7,7,13,16,22,23,26,29$</td>
<td style="text-align: center;">$14.8 \pm 5.4$</td>
<td style="text-align: center;">14.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$1,1,3,7,8,11,14,17,22,35$</td>
<td style="text-align: center;">$11.9 \pm 5.8$</td>
<td style="text-align: center;">9.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0,7,8,8,9,10,10,11,17,20$</td>
<td style="text-align: center;">$10.0 \pm 3.0$</td>
<td style="text-align: center;">9.5</td>
</tr>
</tbody>
</table>
<p>Table 7: The results of running different models on the real robot. We run 10 trials per policy [1] to benchmark the average consecutive successes. Individual rows within each experiment indicate running the experiment on different days [20] and $\pm$ indicates $90 \%$ confidence interval. Our best model was trained with ADR while non-ADR experiments had DR ranges manually tuned. The second experiment shows results when the cube is held at a goal for additional consecutive frames once the target cube pose is reached.</p>
<p>It is worth noting that, while in simulations, state information is derived directly from physics buffers, in all real-world experiments we use the pose estimator described in Section 2.7 to obtain the pose of the cube and provide it as input to the policy. The qualitative results of this are best illustrated by the accompanying videos at https://dextreme.org.
The deployment pipeline of the system is shown in Figure 8. We use three separate machines to run various components. The main machine has an NVIDIA RTX 3090, which runs both the policy as well as the pose estimator. We also do live visualisation of the real-world manipulation in Omniverse but disable the physics.
Similar to [1], we observe a large range of different behaviours in the policies deployed on the real robot. Our real-world quantitative results measuring average consecutive successes are illustrated in Table 7. We collect 10 trials for each policy to obtain the average consecutive successes and also collect different sets of trials across different days to understand the inter-day variability that may arise due to different dynamics, temperature, and lighting conditions. We believe such inter-day variations are important to benchmark in robotics [20] and have endeavoured to highlight this specifically in this challenging task. We find that our policies do not show a dramatic drop in average performance, indicating that they are mostly robust to inter-day variations.
We benchmark both ADR and non-ADR (manually-tuned DR ranges) policies in Table 7 and like [1] find that the policies trained with ADR perform the best, suggesting that the sheer diversity of data gleaned from the simulator endows the policies with the extreme robustness needed in the real world. Importantly, we observed that policies trained with non-ADR exhibited 'stuck' behaviours (as shown in Figure 9), which ADR-based policies were able to overcome due to increased diversity in training data. We also find that on an average, the trials with ADR achieve more consecutive successes than non-ADR policies. Table 9 puts our results in perspective alongside the previous works of [1] and [8]. We demonstrate performance which significantly improves upon the best vision policies</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 8: The system runs on a combination of three machines. The pose estimation and policy are run on the same machine, whereas the Allegro control and live Omniverse visualisation are done on two separate machines that communicate with the main machine at different rates via ROS messages.
from [1] and ADR (XL) ${ }^{9}$ policies given high-quality state information from a motion capture system in [8]. Our policies do not achieve the average successes seen in [8] with ADR (XXL) with state information. We hypothesise that this maybe due to (a) better accuracy of the state information from the PhaseSpace markers (b) higher frame-rate of state observations with PhaseSpace markers (c) increased diversity of data with ADR (XXL). However, our best vision-based policy generated $\sim 2.5 \times$ higher peak consecutive successes and $\sim 1.5 \times$ higher mean consecutive successes than the vision-based policies in [1] as shown in Table 9.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 9: Policies trained with manual DR exhibited 'stuck' behaviours, where the cube remained stuck in certain configurations and was unable to recover. An example of such behaviour can be observed here: https://www.youtube.com/watch?v=tJgq18VbL3k.</p>
<p>Additionally, we also benchmark our results beyond the basic experiment of goal reaching by making the policy hold the cube at a target orientation $N$ frames in a row (we use $N=$ 10), i.e., we count the number of times the cube orientation is within the threshold of 0.4 rad in a row (as described in 3.1), refresh the goal only if the frame counter reaches $N$, and reset the counter to zero every time it goes outside the threshold. In the basic experiment of goal reaching without the hold, the cube may shoot past the target, making it difficult to tell if the target was achieved merely due to noise in the pose estimation or if the cube orientation was indeed accurately estimated. Therefore, holding the cube for $N$ frames in a row ensures that the goal was not achieved by chance, highlighting the robustness of the pose estimator. We conduct this experiment only with our ADR policies as shown in (Table 7, $2^{\text {nd }}$ row). It is worth noting that the policy was not trained explicitly to hold the cube and that the experiment is meant to be a test of accuracy of pose. We chose $N=10$ based on our simulation experiments and found that setting $N$ too high led to a dramatic drop in the performance because the LSTM was not trained for such scenarios (see Table 8). On the other hand, setting it too low did not change the simulation performance; $N=10$ was a good balance between performance and LSTM stability. This also lets us separate the drop in performance due to LSTM instability from pose estimation errors in the real world. From the trials, we find that while there is a noticeable drop in performance i.e. the maximum consecutive successes are only 45 as opposed to 112 in the other case, the average consecutive successes have not dropped as dramatically suggesting both the robustness of the policy and the pose estimator.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">DR type</th>
<th style="text-align: center;">Pose estimation type</th>
<th style="text-align: center;">Training time</th>
<th style="text-align: center;">Cons. Successes</th>
<th style="text-align: center;">Median</th>
<th style="text-align: center;">Best rollout</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OpenAI et al. [1] (state)</td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">PhaseSpace</td>
<td style="text-align: center;">2.08 days (50 hours)</td>
<td style="text-align: center;">$18.8 \pm 5.4$</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI et al. [1] (state, locked wrist)</td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">PhaseSpace</td>
<td style="text-align: center;">2.08 days (50 hours)</td>
<td style="text-align: center;">$26.4 \pm 7.3$</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI et al. [1] (vision)</td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">Neural Network</td>
<td style="text-align: center;">2.08 days (50 hours)</td>
<td style="text-align: center;">$15.2 \pm 7.8$</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">46</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI et al. [8] (state)</td>
<td style="text-align: center;">ADR (L)</td>
<td style="text-align: center;">PhaseSpace</td>
<td style="text-align: center;">13.76 days</td>
<td style="text-align: center;">$13.3 \pm 3.6$</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI et al. [8] (state)</td>
<td style="text-align: center;">ADR (XL)</td>
<td style="text-align: center;">PhaseSpace</td>
<td style="text-align: center;">Multiple Months</td>
<td style="text-align: center;">$16.0 \pm 4.0$</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI et al. [8] (state)</td>
<td style="text-align: center;">ADR (XXL)</td>
<td style="text-align: center;">PhaseSpace</td>
<td style="text-align: center;">Multiple Months</td>
<td style="text-align: center;">$32 \pm 6.4$</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Ours (vision)</td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">Neural Network</td>
<td style="text-align: center;">1.41 days (34 hours)</td>
<td style="text-align: center;">$14.8 \pm 5.4$</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">29</td>
</tr>
<tr>
<td style="text-align: left;">Ours (vision, best avg)</td>
<td style="text-align: center;">ADR</td>
<td style="text-align: center;">Neural Network</td>
<td style="text-align: center;">2.5 days (60 hours)</td>
<td style="text-align: center;">$27.8 \pm 19.0$</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">112</td>
</tr>
<tr>
<td style="text-align: left;">Ours (vision, best median)</td>
<td style="text-align: center;">ADR</td>
<td style="text-align: center;">Neural Network</td>
<td style="text-align: center;">2.5 days (60 hours)</td>
<td style="text-align: center;">$26.6 \pm 13.2$</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">77</td>
</tr>
<tr>
<td style="text-align: left;">Ours (vision, max successes capped at 50)</td>
<td style="text-align: center;">ADR</td>
<td style="text-align: center;">Neural Network</td>
<td style="text-align: center;">2.5 days (60 hours)</td>
<td style="text-align: center;">$23.1 \pm 9.4$</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">50</td>
</tr>
</tbody>
</table>
<p>Table 9: We compare our block reorientation results against the previous work of OpenAI et al. [1] and OpenAI et al. [8]. It is important to note that we use the less capable but more affordable, 4-fingered Allegro Hand with a different morphology and a locked wrist, whereas they use the high-end, tendon-based Shadow Hand with five fingers and a movable wrist. Notwithstanding this disparity between the two robot platforms, we have tried to provide the best possible comparison in this table. Our best vision based policy on average performs better than the vision-based policy in [1] and the state-based policy trained with ADR(XL) in [8] with PhaseSpace markers while taking only 2.5 days to train. OpenAI et al. [8] do not mention the precise time to train with ADR (XL) and ADR (XXL) but do say "multiple months" in the paper. Missing entries in the last column mean that the information was not available in the paper. The table only considers the time taken by the control policy to train, and training time for vision models is not included. Our best rollout with vision achieves 112 consecutive successes, which is roughly $2.5 \times$ more than theirs with vision[1]. It is worth reminding that both [1] and [8] capped the maximum consecutive successes to 50 (Section 6.2, OpenAI et al.[1]). Therefore, we also provide the statistics with successes capped at 50 in the last row of this table. The compute budget comparisons are in Section 1 and Appendix A.1.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 10: Ribbon cables can burn with aggressive maneuvers or excessive usage. Full video is available at https://www.youtube.com/watch?v=H5x0er9jxJc. The images show one of the instances of running the policy leading to burning cables and smoke coming out of the hand. Regular health checkups are necessary, involving replacing cables, tightening screws and resting the hand for a few hours after running about 10-15 trials.</p>
<h1>3.3 Quirks, Problems, and Surprises</h1>
<p>Throughout our work, we experienced a variety of recurring problems and observations, which we disclose here in the interest of transparency and providing avenues for future research. Due to complexities involved in training and slow turnaround time in results because of real hardware involved in the loop, we do not have rigorous experiments to prove these. However, they constitute "tricks of the trade" that will hopefully be of use to other researchers.</p>
<ul>
<li>Even though we are not training on the real robot hand and our policies are very gentle due to the low-pass filters we apply, the hand is quite fragile and requires regular health checks and cable replacements e.g. the ribbon cables connecting joints on the fingers can break or burn, disabling the corresponding joints (see Figure 10). We also apply hot glue at the either ends of the ribbon cables going into connectors to tighten the connection and prevent them from coming out.</li>
<li>We had applied 300lse tape on fingers and palm of the hand for another unrelated task of grasping, but it added significant friction, preventing the cube from sliding and moving smoothly on the palm. Higher friction also meant that the fingers tried aggressively to manipulate the cube when it was stuck in the hand. Therefore, we removed the tape from the palm. However, we kept the tape on the sides of the fingers to allow better grips on the object.</li>
</ul>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>We found that deployment in the real world was quite variable between policies trained with different seeds, even with the same domain randomisation parameters. This is an issue that was also noted in [21]. We suspect that this is because, despite the extreme levels of randomisation we do, there is a "null space" of possible policies which perform similarly in simulation but differently in the real world.</li>
<li>Automatic Domain Randomisation [8], which was used to achieve our best results, does not model the joint distribution and relationships between performance with different parameters. This means it can provide different results depending on the order in which randomisation ranges increase, and furthermore it may not explore the Pareto-limits of performance on any particular dimension. This can lead to cases where ADR disproportionately increases the performance of the policy on dimension A at the expense of dimension B .</li>
<li>Our best policy trained in a non-ADR setting exhibited slightly more natural and smooth behaviour than the ADR one. We also observed that the ADR policies tended to cage the object, resulting in letters occluded by the fingers from all cameras. This made the pose estimation unreliable in those configurations (although the policy was able to absorb the errors and performed better overall). It is possible temporal smoothing of the pose estimator can help as long as it does not introduce any latency.</li>
<li>We also tried pose estimation without the letters on the faces and found that to be quite challenging - the network was confused by colour changes due to lighting in the real scene and the quality of the cameras used e.g. yellow turning into white in the image when the normal of a cube face is perpendicular to the camera. Having a robot cage as used in $[1,8]$ and [22] with an constrained artificial light can prevent this from happening to an extent, but it also makes the system confined to the cage.</li>
</ul>
<p>On the other hand, we were surprised positively about some aspects of our system:</p>
<ul>
<li>Our pose estimator proved to be surprisingly robust even in real-world scenarios outside of Allegro Hand manipulation. This hints at the power of extreme randomisation with synthetic data in simulation for developing such general systems.</li>
<li>The ability to, at test time, adjust the speed of the policy by tuning the EMA value (see Section 2.3 - we trained with 0.15 but tested with 0.1 ) was very useful to avoid damaging the hardware while at the same time having agile policies.</li>
<li>The agility of our policies in the real world on the Allegro Hand, which we initially expected to be a limitation as it is relatively large and is motor- rather than tendon-driven, was a positive surprise to us.</li>
<li>Most of our experiments and trials conducted on the robot were done with a worn-out thumb on the Allegro Hand - one of the wires connecting a joint to the circuit board had a loose connection - and we were quite surprised that the policies produced high consecutive successes despite a malfunctioning actuator, suggesting the robustness of learning-based approaches. The slow turnaround time involved in repairing the hardware motivated us to do it ourselves regularly during the experiments, but it was only a temporary solution.</li>
<li>Since we do not regress to the pose via an end-to-end network, we found that our pose estimator was not tied to a particular camera configuration as keypoint detection worked reliably from different configurations ${ }^{10}$ i.e. our earlier results as described in A. 5 were obtained with a camera configuration that was different to the one we used afterwards with ADR.</li>
<li>Our best policies were trained with continuous actions unlike OpenAI et al. [1, 8], where discrete actions were used.</li>
</ul>
<h1>4 Related work</h1>
<p>In-hand manipulation is a longstanding challenge in robotics. Classical methods [23-25] have focussed on directly leveraging the robot's kinematic and dynamic model to analytically derive controllers for the object in hand. These approaches work well while an object maintains no-slip</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>contacts with the hand, but struggle to achieve good results in dynamic tasks where complex sequences of making-and-breaking of contacts is needed.</p>
<p>Reinforcement learning has proven to be a powerful method for learning complex behaviours in robots with many degrees of freedom. Hwangbo et al. [26] and Lee et al. [27] showed how robust legged locomotion can be learned even over challenging terrain using a combination of deep RL, fast simulation, and domain randomisation. A crucial inspiration for our work is OpenAI et al. [1] on learning in-hand reorientation of cubes via reinforcement learning in simulation and subsequent OpenAI et al. [8] extending this task to solving Rubik's cubes. A variety of recent works [6, 28-30] have leveraged new RL techniques and simulators in order to reproduce or extend in simulation the anthropomorphic in-hand manipulation capabilities shown in [1]. However, these works have not shown sim-to-real transfer, demonstrating that this remains a significant challenge for learned in-hand manipulation. There have also been a variety of recent approaches attempting in-hand manipulation from the perspective of finger gaiting [31, 32]. However, these often fail to reproduce the agile dexterity present in human hands, as the limitations of such a sequential approach to control place corresponding limits on speed. Similarly, Allshire et al. [22] and Shi et al. [33] achieve real-world in hand manipulation using reinforcement learning, but on a platform that cannot mimic the dexterity of a human hand. Sievers et al. [34] learned in-hand manipulation using tactile sensing, but the lack of vision meant that the kinds of re-posing behaviour and speed of the manipulation were limited. There has also been research investigating using reinforcement learning directly on hardware platforms as opposed to in simulation [35, 36]. However, this limits the complexity of learning that can be done due to the small number of trials available on real hardware platforms, as well as the wear-and-tear imposed.</p>
<p>Pose estimation for robotic manipulation is a widely studied area [37, 38]. However, relatively few works outside of OpenAI et al. [1] have applied it to the problem of contact-rich, dexterous, in-hand manipulation, which introduces challenges that exclude many off-the-shelf pose estimators (due to the large degree of occlusions, motion blur, etc.).</p>
<h1>5 Limitations</h1>
<p>Despite our best efforts, the gap between simulations and the real world is still noticeable. Our non-ADR (manual DR) based policy achieves an average of 35 consecutive successes (see Figure 7(a)) in simulation, but only obtains an average of about 15 consecutive successes in the real world. While our ADR policies do perform better, they still fail to reach the average successes seen in simulations. Another limitation of our work is that our ADR policies do not consistently obtain high consecutive successes as seen in ADR (XXL) in OpenAI et al. [8]. It is unclear whether we need a more reliable pose estimation e.g. marker-based systems, the policy needs more diversity in the data, or if it was due to the malfunctioning thumb on our Allegro Hand. Our best npd with ADR policies is around -0.2 , and it is possible that further improvements can be made. This is something we would like to investigate in future work.</p>
<p>We also observed that there is still some sim-to-real gap in pose estimation. This is manifested when we played back the real states in sim (real-to-sim) with physics enabled, which sometimes resulted in interpenetrations. Therefore, we were not able to easily calibrate physics parameters of the cube, which could have improved our sim-to-real transfer.</p>
<p>Lastly, putting this work in context, it is worth remembering that some of the key reasons for successful transfer of this task involve:</p>
<ul>
<li>Being able to simulate the task, and having a clear and well-defined reward function so that the policies can be trained in simulation.</li>
<li>Randomisation of interpretable parameters exposed by the simulator and providing a curriculum for training in simulation. For instance, we randomised friction, damping, stiffness, etc., which were crucial for the transfer. ADR provided a curriculum.</li>
<li>Ability to evaluate successful execution of the task so that we can track the improvements. In this work, we compared the current orientation with the desired, and if the difference was within a user-specified threshold ( 0.4 rad in our case), the goal-reaching was considered successful.</li>
</ul>
<p>Many real-world tasks are hard to simulate and sometimes defining reward functions is not possible as it is in this task. Even if we could simulate and have a well-defined reward function, evaluating a successful execution of a task, e.g. cooking a meal, is not straightforward.</p>
<h1>6 Acknowledgements</h1>
<p>The authors would like to thank Nathan Ratliff, Lucas Manuelli, Erwin Coumans, and Ryan Hickman for helpful discussions. Maciej Bala assisted with multi-GPU training; Michael Lin helped with hardware and Zhutian Yang with video editing. Nick Walker provided valuable suggestions on the teaser video and proofreading. Thanks also to Ankit Goyal for the help with high frame-rate video capture and Jie Xu for proofreading.</p>
<h1>7 Contributions</h1>
<h2>Reinforcement Learning \&amp; Simulation</h2>
<ul>
<li>Viktor Makoviychuk implemented the first version of the Allegro Hand environment in Isaac Gym.</li>
<li>Ankur Handa, Arthur Allshire, and Viktor Makoviychuk developed the domain randomisations in Isaac Gym that assisted in sim2real transfer.</li>
<li>Arthur Allshire developed the vectorised Automatic Domain Randomisation (ADR) in Isaac Gym.</li>
<li>Ankur Handa, Arthur Allshire, Viktor Makoviychuk, and Aleksei Petrenko trained RL policies and added various features to the environment to improve sim2real transfer.</li>
<li>Denys Makoviichuk implemented the RL Library used in this project and helped implement specific extensions required for this work.</li>
<li>Yashraj Narang and Gavriel State advised on tuning simulations.</li>
<li>Dieter Fox and Gavriel State advised on experiments.</li>
</ul>
<h2>Vision</h2>
<ul>
<li>Ankur Handa developed the code to train pose estimation and data augmentation for the vision models.</li>
<li>Arthur Allshire wrote the vision data rendering system and developed the domain randomisations in Omniverse. Ritvik Singh and Jingzhou Liu generalised and extended the rendering pipeline.</li>
<li>Ankur Handa, Ritvik Singh, and Jingzhou Liu trained pose estimation models and did real-to-sim with vision to improve the pose estimator.</li>
<li>Alexander Zhurkevich helped speed up the inference.</li>
</ul>
<h2>Real-World Experiments</h2>
<ul>
<li>Ankur Handa conducted the real-world experiments. Arthur Allshire helped with experiments in the early stages, and Viktor Makoviychuk helped with experiments in the later stages of the project.</li>
<li>Arthur Allshire wrote the code to perform policy \&amp; vision inference on the real robot. Ankur Handa maintained and extended it.</li>
<li>Arthur Allshire developed the live visualisation pipeline in Omniverse.</li>
<li>Karl Van Wyk managed the Allegro Hand infrastructure to run experiments on the real hand.</li>
<li>Balakumar Sundaralingam and Ankur Handa provided assistance repairing the Allegro Hand when it broke.</li>
<li>Karl Van Wyk developed an automatic calibration system for camera-camera and camerarobot calibration.</li>
</ul>
<h2>Organisational</h2>
<ul>
<li>Arthur Allshire and Ankur Handa drafted the paper.</li>
<li>Yashraj Narang, Ritvik Singh, Jingzhou Liu, and Gavriel State helped to edit the paper.</li>
<li>Gavriel State, Dieter Fox, and Jean-Francois Lafleche provided resources and support for the project.</li>
<li>Ankur Handa edited the videos. Gavriel State, Arthur Allshire, Viktor Makoviychuk, Jingzhou Liu, Yashraj Narang and Ritvik Singh examined the videos and provided feedback.</li>
<li>Ankur Handa led the project. Ankur Handa, Arthur Allshire and Viktor Makoviychuk designed the roadmap of the project.</li>
</ul>
<h1>References</h1>
<p>[1] OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Józefowicz, Bob McGrew, Jakub W. Pachocki, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018. URL http://arxiv.org/abs/1808.00177. 3, 4, 6, 7, 10, 13, 14, 15, 16, 17, 18, 25
[2] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for general in-hand object re-orientation. Conference on Robot Learning, 2021. 3
[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. 3
[4] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021. URL https://arxiv. org/abs/2102.12092. 3
[5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. URL https://arxiv.org/abs/ 2204.06125. 3
[6] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning. CoRR, abs/2108.10470, 2021. URL https://arxiv.org/abs/2108.10470. 3, 6, 7, 14, 18, 26
[7] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012. 3, 7
[8] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik's cube with a robot hand. CoRR, abs/1910.07113, 2019. URL http://arxiv.org/abs/1910.07113. 3, 7, 8, 11, 14, 15, 16, 17, 18, 25
[9] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, 2017. 4
[10] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. CoRR, 2017. URL http://arxiv. org/abs/1710.06542. 4
[11] Denys Makoviichuk and Viktor Makoviychuk. rl-games: A high-performance framework for reinforcement learning. https://github.com/Denys88/rl_games, May 2022. 6, 26
[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9 (8):1735-1780, 1997. doi: 10.1162/neco.1997.9.8.1735. 6
[13] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.07289. 6
[14] Nick Jakobi, Phil Husbands, and Inman Harvey. Noise and the reality gap: The use of simulation in evolutionary robotics. In Federico Morán, Alvaro Moreno, Juan Julián Merelo, and Pablo Chacón, editors, Advances in Artificial Life, 1995. 7</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ While extrinsics change with different camera configurations, the intrinsics remain the same.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
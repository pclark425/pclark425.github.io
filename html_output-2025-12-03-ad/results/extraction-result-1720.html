<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1720 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1720</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1720</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-267028244</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.09340v3.pdf" target="_blank">SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding</a></p>
                <p><strong>Paper Abstract:</strong> 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. Project website: https://scene-verse.github.io.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1720.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1720.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded Pretraining for Scenes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based multi-level contrastive pretraining framework that aligns 3D object- and scene-level point-cloud features with language at object, referral (intra-scene), and scene levels using SceneVerse; designed to provide a unified 3D vision-language initialization for downstream grounded 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPS</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Transformer-based model with an object-centric point-cloud encoder (e.g., PointNet++), a spatial-attention transformer to encode object features with 3D spatial relations, and a tunable language encoder (4-layer BERT). Trained with multi-level contrastive losses (L_obj, L_scene, L_ref) plus MLM; two-stage pretraining: object-level then scene-level (object encoder frozen in second stage).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>3D vision-language pairs (scene captions, object captions, referring expressions) — language paired with 3D scenes</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretrained on SceneVerse: ~68K 3D indoor scenes and ~2.5M scene-language pairs (1M template texts + 1M LLM-refined + existing sources + 97K human referrals); object captions from BLIP2/CLIP selection and LLM summarization. Pretraining procedure: object-level grounding pretrain (PointNet++ encoder) with batch size 512 for 1500 epochs, then scene-level pretrain for 150 epochs; training ran on 8 NVIDIA A100 GPUs (longest run ~2 days).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D visual grounding, 3D question answering (3D-QA), open-vocabulary 3D semantic segmentation (OV-Seg) (as pretraining + fine-tuning or to improve other backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D visual grounding: localize referred object in scanned indoor scenes (datasets: ScanRefer, Nr3D, Sr3D) using point clouds and object proposals; 3D-QA: answer scene questions (ScanQA, SQA3D); OV-Seg: open-vocabulary semantic segmentation on ScanNet (RegionPLC pipeline with CLIP text encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable — GPS is a perception / representation model trained on natural language captions and referring expressions (no action commands).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not applicable — GPS is not an action policy; it produces aligned representations for grounding/QA/segmentation rather than motor commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Point cloud inputs (3D coordinates + RGB + instance id + semantic labels where available); multi-view rendered crops used for object captioning pipeline; spatial relations encoded via pairwise distance and angle features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Strong SOTA-level gains on 3D visual grounding and 3D-QA and measurable improvements for OV-Seg when used to pretrain or initialize models. Example (zero-shot SceneVerse-val): Ours (zero-shot) overall Acc@0.5 = 59.2; Ours (zero-shot text) = 60.6 (Table 4). 3D-QA (ScanQA) EM@1 with object: 22.7 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Scratch baselines much lower: SceneVerse-val Ours (scratch) overall = 38.5 (Table 4). 3D-QA comparable scratch/pretrained baselines reported: 3D-VisTA (scratch) 22.4 EM@1 vs Ours 22.7 (Table 5) — but GPS benefits most when pretrained on the large SceneVerse corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported as explicit episodes; qualitative/relative evidence: pretraining on SceneVerse substantially improves zero-shot and reduces need for dataset-specific fine-tuning (e.g., zero-shot performance 59.2 vs scratch 38.5 on SceneVerse-val).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported as explicit episodes; scratch training yields much lower zero-shot transfer performance (e.g., 38.5 overall on SceneVerse-val).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not reported as an explicit numeric sample-efficiency factor; empirical transfer shows large relative gains (e.g., ~+20 absolute Acc points on SceneVerse-val zero-shot) indicating markedly reduced labeled-data need for good performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale paired 3D-VL data (SceneVerse) enabling rich object/scene/referral coverage; multi-level contrastive objectives (object, intra-scene referral, scene) providing fine-grained alignment; object-centric design and spatial-attention transformer encoding spatial relations; LLM-refined diverse captions increasing natural language coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Domain gap between synthetic and real scenes (Sim2Real) reduces transfer when synthetic-only pretraining used; SceneVerse lacks some language types (e.g., question-answer pairs, dialogues) limiting max performance on language-heavy downstreams; GPS is a perception model and does not address action or control mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining a 3D vision-language model on a million-scale, high-diversity 3D-VL corpus (SceneVerse) yields large improvements in grounded 3D perception tasks (visual grounding, QA) and boosts existing 3D backbones for open-vocabulary segmentation; multi-level contrastive alignment (object, referral, scene) and data scale are primary drivers of transfer, while domain realism and language variety limit cross-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1720.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1720.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RegionPLC+SceneVerse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RegionPLC (pretrained on SceneVerse)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-vocabulary 3D semantic segmentation pipeline (RegionPLC) that uses point-discriminative contrastive learning with a CLIP text encoder; pretraining on SceneVerse improves OV-Seg metrics on ScanNet.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RegionPLC: Regional point-language contrastive learning for open-world 3d scene understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RegionPLC (with CLIP text encoder) pretrained on SceneVerse</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>SparseUNet 3D backbone with a vision-language adapter aligning point features to text embeddings via a region-level contrastive loss (RegionPLC framework); uses CLIP text encoder for text embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>3D vision-language pairs (SceneVerse) plus CLIP pretraining for text/image-language backbone</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>RegionPLC models were pretrained on SceneVerse (details in this paper): SceneVerse contains ~68K scenes and ~2.5M scene-language pairs. For OV-Seg experiments this paper pre-trained RegionPLC on SceneVerse and evaluated on ScanNet.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-vocabulary 3D semantic segmentation (OV-Seg) on ScanNet</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Annotation-free training scenario for segmentation: align 3D point features to CLIP text labels and evaluate mIoU and mAcc across 17 foreground ScanNet categories (excluding wall/floor/other-furniture).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A — text descriptions (class names) as supervision signals; no action commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A — segmentation is perception-only (per-point labels), not action control.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>3D point clouds (point coordinates and colors), SparseUNet backbone for point feature extraction; CLIP text encoder for text features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>RegionPLC + SceneVerse (SPUNet16) mIoU = 58.2 (+1.7 vs RegionPLC baseline 56.9); mAcc = 77.3 (+2.2). SPUNet32: mIoU = 61.0 (+2.3 vs 59.6); mAcc = 79.7 (+2.8) (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>RegionPLC baseline (no SceneVerse pretraining): SPUNet16 mIoU = 56.9, mAcc = 75.6; SPUNet32 mIoU = 59.6, mAcc = 77.5 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not reported as explicit episode counts; pretraining on SceneVerse produced modest absolute mIoU/mAcc gains (1.7–2.3 mIoU points) indicating improved label efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not reported as explicit episode counts.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not reported numerically; empirical gains in final metrics (mIoU +1.7 to +2.3) indicate improved sample efficiency but no explicit factor given.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale diverse scene-language pairs from SceneVerse providing richer region-text supervision; use of CLIP text features enabling open-vocabulary alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>RegionPLC/OV-Seg still limited by the types of language in SceneVerse (mainly object/scene captions and referrals) and by domain mismatches; no action/control grounding is addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining an open-vocabulary segmentation pipeline on SceneVerse yields consistent improvements (a few percentage points mIoU/mAcc) over the same pipeline without SceneVerse pretraining, demonstrating that large-scale 3D-VL data improves open-vocabulary 3D perception.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1720.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1720.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-VisTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-VisTA (pre-trained transformer for 3D vision and text alignment) — baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining-based 3D vision-language baseline referenced and compared in this work; used as a strong prior state-of-the-art for grounding and QA comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3d-vista: Pre-trained transformer for 3d vision and text alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-VisTA</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Prior pretraining-based 3D-VL transformer model used as a comparative baseline; details of its architecture/pretraining are from the cited prior work (not expanded in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Referenced as pretraining-based 3D-VL (likely scene-language data) — exact pretraining corpora not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D visual grounding, 3D question answering (used as baseline comparisons in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same benchmarks used in this paper (ScanRefer/Nr3D/Sr3D for grounding; ScanQA for QA) — 3D-VisTA reported as either scratch, zero-shot, or zero-shot-text variants in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>3D point cloud and language inputs (implied), but exact modalities depend on the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in tables as baseline for comparison; e.g., 3D-VisTA (scratch) and zero-shot variants shown in Table 3 and Table 4. Exact numeric comparisons are provided but architecture/pretraining specifics are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This paper compares GPS to 3D-VisTA and shows GPS often outperforms or generalizes better in zero-shot transfer, but the cited paper should be consulted for 3D-VisTA's pretraining details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1720.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1720.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LLM (3D Large Language Model) — referenced baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach that integrates large language models with 3D scene understanding; cited as a baseline in QA/3D-VL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Prior work that uses LLMs for 3D tasks (referenced but not described in detail in this paper); used as a comparative baseline for 3D question answering and other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large language model pretraining on large text corpora (implied) combined with 3D scene data in the referenced work; specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D question answering (and multi-modal 3D-VL tasks) as comparison baselines</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Benchmarks like ScanQA/SQA3D are used to compare performance; full environment/objective details in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language (LLM pretraining) — no action commands described here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>LLM + 3D scene inputs in referenced work; exact modalities not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>In 3D-QA comparisons this paper lists 3D-LLM EM@1 = 20.5 (SQA3D) per Table 5, showing GPS outperforms it on those benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as a comparative baseline in QA; the current paper does not detail how 3D-LLM maps language pretraining to embodied control or perception — consult the cited work for specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1720.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1720.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP / BLIP2 / GPT-3.5 / Llama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP, BLIP2, GPT-3.5, Llama (pretrained vision-language / language models used in pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf, large pretrained language and vision-language models used inside the SceneVerse data pipeline and some downstream experiments: BLIP2 for initial object caption generation, CLIP to score/select captions, and GPT-3.5 / Llama used to rephrase and refine template captions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP, BLIP2, GPT-3.5, Llama (as components)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>CLIP: contrastive image-text encoder pre-trained on large image-text corpora; BLIP2: image-captioning pipeline leveraging frozen vision encoders + LLM; GPT-3.5 and Llama: large language models used for rephrasing and generating naturalistic captions from templates.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale image-text web corpora (CLIP/BLIP2) and large text corpora/LM pretraining (GPT-3.5/Llama); exact corpora not enumerated in this paper (standard external models).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in-paper; CLIP/BLIP2 and LLMs are used as pretrained components (e.g., BLIP2 generates candidate object captions from image crops; CLIP ranks captions by image-text similarity; GPT-3.5/Llama rephrase template texts).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Indirectly support 3D embodied perception tasks by producing or encoding language used to pretrain or supervise 3D models (e.g., object captions for GPS/SceneVerse, CLIP text encoder for OV-Seg).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>They are not themselves embodied agents; they produce/encode language data and embeddings used to train 3D perception models that are evaluated on grounding, QA, and OV-Seg.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language generation and understanding (captions, referring expressions, prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>N/A as these are not mapped to motor actions; used to generate/encode text supervision that is contrastively aligned with 3D features.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Used with rendered multi-view image crops (BLIP2/CLIP) and text prompts (LLMs); downstream 3D models require point cloud inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Indirect evidence of success: SceneVerse object caption pipeline (BLIP2 + CLIP + LLM-refinement) produced data that when used to pretrain GPS led to improved downstream performance. Quantitative attribution to these components alone is not separated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong pretrained language/vision-language priors provide high-quality, diverse textual supervision and good text embeddings (CLIP) that facilitate contrastive alignment with 3D features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>These models do not supply spatial/3D-specific information by themselves; relying solely on 2D image-language priors can miss 3D spatial relations unless augmented by 3D scene graph signals.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained image-language and LLM components are valuable for scalable generation and encoding of diverse text supervision (object captions, referrals, scene captions), which in turn enables large-scale 3D-VL pretraining (SceneVerse) and improves downstream 3D perception, but they are not direct embodied action policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RegionPLC: Regional point-language contrastive learning for open-world 3d scene understanding <em>(Rating: 2)</em></li>
                <li>3d-vista: Pre-trained transformer for 3d vision and text alignment <em>(Rating: 2)</em></li>
                <li>3D-LLM <em>(Rating: 1)</em></li>
                <li>Pointclip: Point cloud understanding by CLIP <em>(Rating: 1)</em></li>
                <li>PLA: Language-driven open-vocabulary 3d scene understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1720",
    "paper_id": "paper-267028244",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "GPS",
            "name_full": "Grounded Pretraining for Scenes",
            "brief_description": "A transformer-based multi-level contrastive pretraining framework that aligns 3D object- and scene-level point-cloud features with language at object, referral (intra-scene), and scene levels using SceneVerse; designed to provide a unified 3D vision-language initialization for downstream grounded 3D tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "GPS",
            "model_agent_description": "Transformer-based model with an object-centric point-cloud encoder (e.g., PointNet++), a spatial-attention transformer to encode object features with 3D spatial relations, and a tunable language encoder (4-layer BERT). Trained with multi-level contrastive losses (L_obj, L_scene, L_ref) plus MLM; two-stage pretraining: object-level then scene-level (object encoder frozen in second stage).",
            "pretraining_data_type": "3D vision-language pairs (scene captions, object captions, referring expressions) — language paired with 3D scenes",
            "pretraining_data_details": "Pretrained on SceneVerse: ~68K 3D indoor scenes and ~2.5M scene-language pairs (1M template texts + 1M LLM-refined + existing sources + 97K human referrals); object captions from BLIP2/CLIP selection and LLM summarization. Pretraining procedure: object-level grounding pretrain (PointNet++ encoder) with batch size 512 for 1500 epochs, then scene-level pretrain for 150 epochs; training ran on 8 NVIDIA A100 GPUs (longest run ~2 days).",
            "embodied_task_name": "3D visual grounding, 3D question answering (3D-QA), open-vocabulary 3D semantic segmentation (OV-Seg) (as pretraining + fine-tuning or to improve other backbones)",
            "embodied_task_description": "3D visual grounding: localize referred object in scanned indoor scenes (datasets: ScanRefer, Nr3D, Sr3D) using point clouds and object proposals; 3D-QA: answer scene questions (ScanQA, SQA3D); OV-Seg: open-vocabulary semantic segmentation on ScanNet (RegionPLC pipeline with CLIP text encoder).",
            "action_space_text": "Not applicable — GPS is a perception / representation model trained on natural language captions and referring expressions (no action commands).",
            "action_space_embodied": "Not applicable — GPS is not an action policy; it produces aligned representations for grounding/QA/segmentation rather than motor commands.",
            "action_mapping_method": null,
            "perception_requirements": "Point cloud inputs (3D coordinates + RGB + instance id + semantic labels where available); multi-view rendered crops used for object captioning pipeline; spatial relations encoded via pairwise distance and angle features.",
            "transfer_successful": true,
            "performance_with_pretraining": "Strong SOTA-level gains on 3D visual grounding and 3D-QA and measurable improvements for OV-Seg when used to pretrain or initialize models. Example (zero-shot SceneVerse-val): Ours (zero-shot) overall Acc@0.5 = 59.2; Ours (zero-shot text) = 60.6 (Table 4). 3D-QA (ScanQA) EM@1 with object: 22.7 (Table 5).",
            "performance_without_pretraining": "Scratch baselines much lower: SceneVerse-val Ours (scratch) overall = 38.5 (Table 4). 3D-QA comparable scratch/pretrained baselines reported: 3D-VisTA (scratch) 22.4 EM@1 vs Ours 22.7 (Table 5) — but GPS benefits most when pretrained on the large SceneVerse corpus.",
            "sample_complexity_with_pretraining": "Not reported as explicit episodes; qualitative/relative evidence: pretraining on SceneVerse substantially improves zero-shot and reduces need for dataset-specific fine-tuning (e.g., zero-shot performance 59.2 vs scratch 38.5 on SceneVerse-val).",
            "sample_complexity_without_pretraining": "Not reported as explicit episodes; scratch training yields much lower zero-shot transfer performance (e.g., 38.5 overall on SceneVerse-val).",
            "sample_complexity_gain": "Not reported as an explicit numeric sample-efficiency factor; empirical transfer shows large relative gains (e.g., ~+20 absolute Acc points on SceneVerse-val zero-shot) indicating markedly reduced labeled-data need for good performance.",
            "transfer_success_factors": "Large-scale paired 3D-VL data (SceneVerse) enabling rich object/scene/referral coverage; multi-level contrastive objectives (object, intra-scene referral, scene) providing fine-grained alignment; object-centric design and spatial-attention transformer encoding spatial relations; LLM-refined diverse captions increasing natural language coverage.",
            "transfer_failure_factors": "Domain gap between synthetic and real scenes (Sim2Real) reduces transfer when synthetic-only pretraining used; SceneVerse lacks some language types (e.g., question-answer pairs, dialogues) limiting max performance on language-heavy downstreams; GPS is a perception model and does not address action or control mapping.",
            "key_findings": "Pretraining a 3D vision-language model on a million-scale, high-diversity 3D-VL corpus (SceneVerse) yields large improvements in grounded 3D perception tasks (visual grounding, QA) and boosts existing 3D backbones for open-vocabulary segmentation; multi-level contrastive alignment (object, referral, scene) and data scale are primary drivers of transfer, while domain realism and language variety limit cross-domain generalization.",
            "uuid": "e1720.0",
            "source_info": {
                "paper_title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "RegionPLC+SceneVerse",
            "name_full": "RegionPLC (pretrained on SceneVerse)",
            "brief_description": "An open-vocabulary 3D semantic segmentation pipeline (RegionPLC) that uses point-discriminative contrastive learning with a CLIP text encoder; pretraining on SceneVerse improves OV-Seg metrics on ScanNet.",
            "citation_title": "RegionPLC: Regional point-language contrastive learning for open-world 3d scene understanding",
            "mention_or_use": "use",
            "model_agent_name": "RegionPLC (with CLIP text encoder) pretrained on SceneVerse",
            "model_agent_description": "SparseUNet 3D backbone with a vision-language adapter aligning point features to text embeddings via a region-level contrastive loss (RegionPLC framework); uses CLIP text encoder for text embedding.",
            "pretraining_data_type": "3D vision-language pairs (SceneVerse) plus CLIP pretraining for text/image-language backbone",
            "pretraining_data_details": "RegionPLC models were pretrained on SceneVerse (details in this paper): SceneVerse contains ~68K scenes and ~2.5M scene-language pairs. For OV-Seg experiments this paper pre-trained RegionPLC on SceneVerse and evaluated on ScanNet.",
            "embodied_task_name": "Open-vocabulary 3D semantic segmentation (OV-Seg) on ScanNet",
            "embodied_task_description": "Annotation-free training scenario for segmentation: align 3D point features to CLIP text labels and evaluate mIoU and mAcc across 17 foreground ScanNet categories (excluding wall/floor/other-furniture).",
            "action_space_text": "N/A — text descriptions (class names) as supervision signals; no action commands.",
            "action_space_embodied": "N/A — segmentation is perception-only (per-point labels), not action control.",
            "action_mapping_method": null,
            "perception_requirements": "3D point clouds (point coordinates and colors), SparseUNet backbone for point feature extraction; CLIP text encoder for text features.",
            "transfer_successful": true,
            "performance_with_pretraining": "RegionPLC + SceneVerse (SPUNet16) mIoU = 58.2 (+1.7 vs RegionPLC baseline 56.9); mAcc = 77.3 (+2.2). SPUNet32: mIoU = 61.0 (+2.3 vs 59.6); mAcc = 79.7 (+2.8) (Table 6).",
            "performance_without_pretraining": "RegionPLC baseline (no SceneVerse pretraining): SPUNet16 mIoU = 56.9, mAcc = 75.6; SPUNet32 mIoU = 59.6, mAcc = 77.5 (Table 6).",
            "sample_complexity_with_pretraining": "Not reported as explicit episode counts; pretraining on SceneVerse produced modest absolute mIoU/mAcc gains (1.7–2.3 mIoU points) indicating improved label efficiency.",
            "sample_complexity_without_pretraining": "Not reported as explicit episode counts.",
            "sample_complexity_gain": "Not reported numerically; empirical gains in final metrics (mIoU +1.7 to +2.3) indicate improved sample efficiency but no explicit factor given.",
            "transfer_success_factors": "Large-scale diverse scene-language pairs from SceneVerse providing richer region-text supervision; use of CLIP text features enabling open-vocabulary alignment.",
            "transfer_failure_factors": "RegionPLC/OV-Seg still limited by the types of language in SceneVerse (mainly object/scene captions and referrals) and by domain mismatches; no action/control grounding is addressed.",
            "key_findings": "Pretraining an open-vocabulary segmentation pipeline on SceneVerse yields consistent improvements (a few percentage points mIoU/mAcc) over the same pipeline without SceneVerse pretraining, demonstrating that large-scale 3D-VL data improves open-vocabulary 3D perception.",
            "uuid": "e1720.1",
            "source_info": {
                "paper_title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "3D-VisTA",
            "name_full": "3D-VisTA (pre-trained transformer for 3D vision and text alignment) — baseline",
            "brief_description": "A pretraining-based 3D vision-language baseline referenced and compared in this work; used as a strong prior state-of-the-art for grounding and QA comparisons.",
            "citation_title": "3d-vista: Pre-trained transformer for 3d vision and text alignment",
            "mention_or_use": "mention",
            "model_agent_name": "3D-VisTA",
            "model_agent_description": "Prior pretraining-based 3D-VL transformer model used as a comparative baseline; details of its architecture/pretraining are from the cited prior work (not expanded in this paper).",
            "pretraining_data_type": "Referenced as pretraining-based 3D-VL (likely scene-language data) — exact pretraining corpora not described in this paper.",
            "pretraining_data_details": null,
            "embodied_task_name": "3D visual grounding, 3D question answering (used as baseline comparisons in this paper)",
            "embodied_task_description": "Same benchmarks used in this paper (ScanRefer/Nr3D/Sr3D for grounding; ScanQA for QA) — 3D-VisTA reported as either scratch, zero-shot, or zero-shot-text variants in comparisons.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": "3D point cloud and language inputs (implied), but exact modalities depend on the cited work.",
            "transfer_successful": null,
            "performance_with_pretraining": "Reported in tables as baseline for comparison; e.g., 3D-VisTA (scratch) and zero-shot variants shown in Table 3 and Table 4. Exact numeric comparisons are provided but architecture/pretraining specifics are in the cited paper.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "This paper compares GPS to 3D-VisTA and shows GPS often outperforms or generalizes better in zero-shot transfer, but the cited paper should be consulted for 3D-VisTA's pretraining details.",
            "uuid": "e1720.2",
            "source_info": {
                "paper_title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "3D-LLM",
            "name_full": "3D-LLM (3D Large Language Model) — referenced baseline",
            "brief_description": "A referenced approach that integrates large language models with 3D scene understanding; cited as a baseline in QA/3D-VL tasks.",
            "citation_title": "3D-LLM",
            "mention_or_use": "mention",
            "model_agent_name": "3D-LLM",
            "model_agent_description": "Prior work that uses LLMs for 3D tasks (referenced but not described in detail in this paper); used as a comparative baseline for 3D question answering and other tasks.",
            "pretraining_data_type": "Large language model pretraining on large text corpora (implied) combined with 3D scene data in the referenced work; specifics not provided in this paper.",
            "pretraining_data_details": null,
            "embodied_task_name": "3D question answering (and multi-modal 3D-VL tasks) as comparison baselines",
            "embodied_task_description": "Benchmarks like ScanQA/SQA3D are used to compare performance; full environment/objective details in the referenced work.",
            "action_space_text": "Natural language (LLM pretraining) — no action commands described here.",
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": "LLM + 3D scene inputs in referenced work; exact modalities not specified here.",
            "transfer_successful": null,
            "performance_with_pretraining": "In 3D-QA comparisons this paper lists 3D-LLM EM@1 = 20.5 (SQA3D) per Table 5, showing GPS outperforms it on those benchmarks.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Referenced as a comparative baseline in QA; the current paper does not detail how 3D-LLM maps language pretraining to embodied control or perception — consult the cited work for specifics.",
            "uuid": "e1720.3",
            "source_info": {
                "paper_title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "CLIP / BLIP2 / GPT-3.5 / Llama",
            "name_full": "CLIP, BLIP2, GPT-3.5, Llama (pretrained vision-language / language models used in pipelines)",
            "brief_description": "Off-the-shelf, large pretrained language and vision-language models used inside the SceneVerse data pipeline and some downstream experiments: BLIP2 for initial object caption generation, CLIP to score/select captions, and GPT-3.5 / Llama used to rephrase and refine template captions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "CLIP, BLIP2, GPT-3.5, Llama (as components)",
            "model_agent_description": "CLIP: contrastive image-text encoder pre-trained on large image-text corpora; BLIP2: image-captioning pipeline leveraging frozen vision encoders + LLM; GPT-3.5 and Llama: large language models used for rephrasing and generating naturalistic captions from templates.",
            "pretraining_data_type": "Large-scale image-text web corpora (CLIP/BLIP2) and large text corpora/LM pretraining (GPT-3.5/Llama); exact corpora not enumerated in this paper (standard external models).",
            "pretraining_data_details": "Not specified in-paper; CLIP/BLIP2 and LLMs are used as pretrained components (e.g., BLIP2 generates candidate object captions from image crops; CLIP ranks captions by image-text similarity; GPT-3.5/Llama rephrase template texts).",
            "embodied_task_name": "Indirectly support 3D embodied perception tasks by producing or encoding language used to pretrain or supervise 3D models (e.g., object captions for GPS/SceneVerse, CLIP text encoder for OV-Seg).",
            "embodied_task_description": "They are not themselves embodied agents; they produce/encode language data and embeddings used to train 3D perception models that are evaluated on grounding, QA, and OV-Seg.",
            "action_space_text": "Natural language generation and understanding (captions, referring expressions, prompts).",
            "action_space_embodied": null,
            "action_mapping_method": "N/A as these are not mapped to motor actions; used to generate/encode text supervision that is contrastively aligned with 3D features.",
            "perception_requirements": "Used with rendered multi-view image crops (BLIP2/CLIP) and text prompts (LLMs); downstream 3D models require point cloud inputs.",
            "transfer_successful": null,
            "performance_with_pretraining": "Indirect evidence of success: SceneVerse object caption pipeline (BLIP2 + CLIP + LLM-refinement) produced data that when used to pretrain GPS led to improved downstream performance. Quantitative attribution to these components alone is not separated in the paper.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong pretrained language/vision-language priors provide high-quality, diverse textual supervision and good text embeddings (CLIP) that facilitate contrastive alignment with 3D features.",
            "transfer_failure_factors": "These models do not supply spatial/3D-specific information by themselves; relying solely on 2D image-language priors can miss 3D spatial relations unless augmented by 3D scene graph signals.",
            "key_findings": "Pretrained image-language and LLM components are valuable for scalable generation and encoding of diverse text supervision (object captions, referrals, scene captions), which in turn enables large-scale 3D-VL pretraining (SceneVerse) and improves downstream 3D perception, but they are not direct embodied action policies.",
            "uuid": "e1720.4",
            "source_info": {
                "paper_title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RegionPLC: Regional point-language contrastive learning for open-world 3d scene understanding",
            "rating": 2,
            "sanitized_title": "regionplc_regional_pointlanguage_contrastive_learning_for_openworld_3d_scene_understanding"
        },
        {
            "paper_title": "3d-vista: Pre-trained transformer for 3d vision and text alignment",
            "rating": 2,
            "sanitized_title": "3dvista_pretrained_transformer_for_3d_vision_and_text_alignment"
        },
        {
            "paper_title": "3D-LLM",
            "rating": 1
        },
        {
            "paper_title": "Pointclip: Point cloud understanding by CLIP",
            "rating": 1,
            "sanitized_title": "pointclip_point_cloud_understanding_by_clip"
        },
        {
            "paper_title": "PLA: Language-driven open-vocabulary 3d scene understanding",
            "rating": 1,
            "sanitized_title": "pla_languagedriven_openvocabulary_3d_scene_understanding"
        }
    ],
    "cost": 0.024591749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding
24 Sep 2024</p>
<p>Baoxiong Jia 0000-0002-4968-3290
‹ indicates equal contribution State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Yixin Chen 0000-0002-8176-0241
‹ indicates equal contribution State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Huangyue Yu 0009-0007-5729-0255
‹ indicates equal contribution State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Yan Wang 0009-0000-7001-3569
‹ indicates equal contribution State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Xuesong Niu 0000-0001-7737-4287
‹ indicates equal contribution State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Tengyu Liu 0000-0003-4006-1740
‹ indicates equal contribution State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Qing Li 0000-0003-1185-5365
‹ indicates equal contribution State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Siyuan Huang 0000-0003-1524-7148
‹ indicates equal contribution State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding
24 Sep 2024993202D2AC57D4B23B03704B0E7D9D3DarXiv:2401.09340v3[cs.CV]3D Vision-LanguageData ScalingGrounded Scene Understanding
3D vision-language (3D-VL) grounding, which aims to align language with 3D physical environments, stands as a cornerstone in developing embodied agents.In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces two significant challenges: (i) the scarcity of paired 3D-VL data to support grounded learning of 3D scenes, especially considering complexities within diverse object configurations, rich attributes, and intricate relationships; and (ii) the absence of a unified learning framework to distill knowledge from grounded 3D data.In this work, we aim to address these major challenges in 3D-VL by examining the potential of systematically upscaling 3D-VL learning in indoor scenes.We introduce the first million-scale 3D-VL dataset, SceneVerse, encompassing 68K indoor scenes and comprising 2.5M vision-language pairs collected from both human annotations and our scalable scene-graph-based generation approach.We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pretraining for Scenes (GPS), for 3D-VL learning.Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on existing 3D visual grounding and question-answering benchmarks.We also show that the data scaling effect is not limited to GPS, but is generally beneficial for models on tasks like 3D semantic segmentation.The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in challenging 3D-VL tasks.</p>
<p>Introduction</p>
<p>The foundation of human cognitive development lies in the grounding of language within the physical world [55,84,111].Recent progress in Large Language Models (LLMs) [10,11,86] has markedly promoted the alignment between vision and language [3,61,78] utilizing billion-scale vision-language datasets [82,110].However, with these advancements predominantly focusing on the 2D domain, the grounded understanding of 3D physical environments remains in an incipient stage [1,5,16].Recognizing the pivotal role of grounded 3D experiences in</p>
<p>SCENE CAPTION</p>
<p>"In this scene, there is a fray flat floor.A bar is standing on the floor, with …</p>
<p>The room is also designed …"</p>
<p>OBJECT CAPTION</p>
<p>"This is a big cotton sofa against the wall.It is made of genuine leather."</p>
<p>OBJECT REFERRAL</p>
<p>"The ottoman is on the carpet next to the double bed in the bedroom."</p>
<p>Fig. 1: Overview of SceneVerse.A million-scale 3D vision-language dataset that comprises over 68K various 3D indoor scenes and 2.5M aligned scene-language pairs in the form of scene caption, object caption, and object referral.</p>
<p>shaping human cognition [7,8], there is a compelling need to focus on exploring vision-language learning in the context of 3D scenes.</p>
<p>Seeking insights from success in 2D vision-language (2D-VL) learning, a major factor to the success was the notable scale-up of paired vision-language data [15,54,82].However, applying this experience directly from 2D to 3D is fraught with challenges.Primarily, 3D data collection heavily relies on the scanning device, making it inherently much more complex and expensive than gathering 2D images.Despite steady efforts to increase the volume of 3D scene data [9,25,68,101], most datasets remain limited to thousands of scenes, substantially lagging behind the scale of existing 2D datasets.This gap is further widened by the inherent complexities of 3D scenes, which feature a multitude of object instances with diverse attributes, varying arrangements, and intricate inter-object relationships.These unique aspects of 3D scenes not only make the accurate description of objects and their relations more challenging but also considerably increase the number of language descriptions required for thorough scene depiction.Consequently, this presents a significant challenge in gathering sufficient and high-quality paired scene-language data for grounded scene understanding.</p>
<p>To confront these challenges, we propose SceneVerse, the first millionscale dataset aimed at advancing 3D vision-language (3D-VL) learning for grounded scene understanding.At the scene level, we unify 3D scene data from existing datasets [9,25,68,79,88], aligning scenes and annotations from various capturing sources, and supplement the collection with synthetic scenes [29,109].This compilation represents the most extensive 3D scene data gathered to date, amounting to 68K scenes.For language, we first present 97K newly-annotated referring expressions, the most extensive thus far.We additionally propose an</p>
<p>Related Work</p>
<p>Datasets for Grounded 3D Understanding Obtaining aligned 3D-language data is a inherently difficult task.In 3D object modeling, pioneering works like ShapeNet [14] sourced 3D assets from online repositories, leading to a proliferation of high-quality 3D object datasets [24,71,94].Notably, recent developments include internet-scale data collection with Objaverse [27,28], accompanied by the integration of object-level captions [96] for 3D-language alignment.Models trained on these datasets demonstrate an enhanced understanding of objects, evident in classification [62], generation [63], and captioning tasks [65].</p>
<p>In contrast, developing datasets for grounded 3D scene understanding is even more challenging due to the extensive requirements for scene acquisition and annotation.Existing works curate RGB-D and scanned indoor scene datasets [9,13,25,68,79,88] and synthetic scenes [29,52,98,109] used for benchmarking tasks like 3D object detection and segmentation [32,48,69,83,87].These semantically labeled scenes are subsequently used in fine-grained scene grounding tasks like object
| | VG ✗ ✗ ✓ ✓ 52K - - - 52K ReferIt3D [1] VG ✗ ✗ ✓ ✓ 42K 200K - - 242K ScanQA [5] 1.5K 33K QA - - - ✓ 27K - - - 27K SQA3D [67] | | QA - - - ✓ 33K - - - 33K Multi3DRefer [107] VG ✗ ✗ ✓ ✓ - 10K 52K - 62K Cap3D [65] - 666K VG ✗ ✓ ✗ ✗ 58K 666K - - 724K ScanScribe [112] 3K 56K PT ✗ ✗ ✓ ✗ - 90K 94K 94K 278K 3D-LLM [42] 1.5K 186K MT ✓ ✓ ✓ ✗ - 659K - - 659K EmbodiedScan [90] 5K 890K VG ✗ ✗ ✓ ✗ - 970K - - 970K LEO [43] 3K 56K MT ✓ ✓ ✓ ✓ - 188K 235K 90K 513K SceneVerse 68K 1.5M VG ✓ ✓ ✓ ✓ 96K 2.1M 94K 200K 2.5M
referral [1,16,107], captioning [17,19,22,102], vision-language-navigation [41,66,75,91] and reasoning [5,42,67].Recent works exploit the representation of 3D scene graph (3DSG) [4,20,81,89], which concisely describes scenes with hierarchical structures.This representation is notably advantageous for planning [2,80] and captioning [37], owing to its compatibility with LLMs for flexible description generation [42,43].Nonetheless, as shown in Tab. 1, most datasets are constrained in both scene and language scales, underscoring the need for scaling up fine-grained and aligned scene-language data to enhance grounded scene understanding.</p>
<p>Vision-Language Learning Recent years have witnessed tremendous progress in 2D-VL [3,21,26,59,61,78], empowered by transformer-based pre-training models [11,30,74] and large-scale image-language datasets [15,82].A central theme across 2D-VL domains is the effectiveness of data scaling [51], as demonstrated by improved alignment and expanded capabilities in open-vocabulary understanding [34,53,56,60] through a contrastive pre-training pipeline [78].However, in grounded scene understanding, the primary challenge for models has been the limited availability of paired 3D scene-language data, which restricts the application of insights drawn from 2D-VL.Current models for 3D scene grounding [6,18,39,45,47,64,95,100,108] heavily rely on task-specific knowledge in both model and loss designs or advanced optimization strategies [112].To bridge this gap, there has been a growing emphasis on employing pre-trained 2D-VL models for 3D-VL [38,40,76,85,96,105,106].Yet, these models mostly draw information available from 2D-VL models (e.g., object attribute, affordance, etc.), falling short on capturing crucial 3D information like object spatial relationships which are necessary for more fine-grained tasks such as grounded human-scene [20,46,49,50,92,93] and robot-scene interactions modeling [35,57,70,72].This urges the need for a multi-level alignment between language and 3D scenes, particularly regarding 3D-specific information.Considering the nascent stage of existing 3D pre-training methods [31,97,112,113], we believe SceneVerse and GPS have the potential to spearhead new avenues in 3D-VL research.</p>
<p>SceneVerse</p>
<p>SceneVerse is designed for grounded scene understanding with 3D scenes curated from diverse existing datasets of both real and synthetic environments.Regarding language, we employ both human annotation and a novel automated generation pipeline to collect comprehensive and high-quality language for both object-level and scene-level descriptions.We provide details regarding data collection in the following sections.</p>
<p>Scene Curation</p>
<p>To address the scarcity of available 3D scene data, we construct SceneVerse by unifying 3D scene data from various existing datasets.We use real-world scene datasets, including ScanNet [25], ARKitScenes [9], HM3D [79], 3RScan [88] and MultiScan [68], alongside synthetic environments from Structured3D [109] and ProcTHOR [29].The inclusion of these synthetic datasets is mainly motivated by their potential as scalable data sources for 3D-VL alignment.To ensure cohesion across various sources, we conduct preprocessing steps such as room segmentation, point subsampling, axis alignment, normalization, and semantic label alignment.Each scan is represented by a point cloud P P R N ˆ8, wherein each point is defined by its 3D coordinates, RGB color, instance id, and semantic label.In total, we curate 68, 406 3D scenes in SceneVerse.</p>
<p>Referral Annotation by Humans</p>
<p>In the curated scenes of SceneVerse, we present the most comprehensive set of human-annotated, context-rich object referrals to date, serving as a valuable benchmark for assessing grounded scene understanding capabilities.The human annotations contain 96, 863 descriptions in ARKitScenes [9], HM3D [79] and MultiScan [68].During the annotation process, one human annotator was assigned to write at least 20 words to distinctly refer to a single 3D object within a 3D scene.Each referral text then undergoes independent verification by two additional reviewers, both mandated to accurately locate the referenced object based on the 3D scene and the annotated referral text.Any object referrals that do not pass the verification by either reviewer are flagged for re-annotation.</p>
<p>3D Scene Graph Construction</p>
<p>Our 3D scene graph is defined as a hierarchical graph G " pV, Eq.Each node v P V represents one distinct 3D object instance, parameterized by its centroid p i P R 3  and bounding box size of b i " pb x , b y , b z q P R 3 .The edges E represent spatial relationships between nodes.To construct the scene graph G, we instantiate the nodes with the instance annotation from the point clouds and assign object classes with their corresponding semantic labels.Following prior work [1,89], we consider the Vertical proximity, Horizontal proximity and Multi-object Relationships as spatial relations.For a more detailed description of the scene graph construction and relationship determination, please refer to supplementary.</p>
<p>Object Referral Scene Caption</p>
<p>Object Caption</p>
<p>Language Generation with LLMs</p>
<p>The scene-language pairs in SceneVerse aim to capture varying aspects of the 3D scene, including detailed object attributes in object captioning, spatial relationships between objects in object referral, and global scene descriptions in scene captioning.Based on 3D scene graphs, we utilize both templates and LLMs to automatically generate descriptions on these three granularities.</p>
<p>Object Captioning Object captions aim to provide detailed descriptions of an object's visual and physical properties, facilitating object-level grounding with its distinctive features.Given the multi-view images, we utilize the point cloud of the object v P V to identify its occurrence in the images through rendering.</p>
<p>The images are then cropped with the rendered bounding boxes and processed through BLIP2 [58] to generate initial object captions.We select the top 10 sentences with the highest CLIP [78] similarity score and minimal occlusion and utilize an LLM to obtain a refined and coherent summary of the captions.The detailed object captioning pipeline is illustrated in supplementary.</p>
<p>Object Referral Object relationship captions refer to objects by articulating their spatial relationships in the scene.Spatial relationship triplets pv i , v j , e ij q are first extracted from the constructed 3D scene graph.We design various templates to generate descriptions for each relationship type, assigning the entities in the form of ptarget-object, spatial-relation, anchor-object(s)q.This results in examples like "the chair is next to the armchair", "facing the sofa, there is a suitcase far to the right of the shoes", and "the fridge is between cabinet and sofa".Our designed templates span passive and active tenses, as well as inversion clauses, contributing to the richness of the generated text.To enhance the descriptions' naturalness, we employ LLM for sentence rephrasing.Statistics for the descriptions before and after rephrasing are presented in Fig. 2 (b).</p>
<p>Scene Captioning</p>
<p>The scene-level captions emphasize global information, portraying the key objects in the scene along with their attributes and functionalities.We use the constructed 3D scene graph and prompt LLMs to generate these captions.We random sample a subset of edges and nodes from the scene graph each time as the scene context to enhance the diversity of scene captions.The object counts are also provided as LLM prompts, together with the room type and object attributes if such annotations are available in the dataset.[1,89].For the language descriptions, we generate 1M template-based texts and 1M sentences rephrased by Llama [86] and GPT-3.5 [73].As can be seen from the radar chart and examples in Fig. 2, the diversity of the LLM-refined data, particularly in sentence length and variety, closely aligns with the characteristics of annotated descriptions, surpassing the template-based data.Together with existing sources (294K) and our newly annotated set (96K), SceneVerse contains 2.5M scenelanguage pairs in total.All the rephrasing and summary prompts, along with the complete set of relationships, are detailed in supplementary.</p>
<p>Data</p>
<p>Grounded Pre-training for Scenes</p>
<p>In this section, we introduce GPS, an efficient transformer-based model trained with multi-level contrastive losses for aligning 3D scenes and texts.As shown in Fig. 3, we echo the language descriptions collected at different granularities to form contrastive objectives at both object-level, referral-object-level, and scene-level in GPS.We describe the design of each level in the following sections.</p>
<p>Object-level Grounding</p>
<p>Given a 3D scene point cloud S, we use an off-the-shelf 3D object segmentation model to decompose it into a bag of N objects S " to 1 , o 2 , ¨¨¨, o n u N i"1 .We extract</p>
<p>Object Captions</p>
<p>"An wooden classic guitar" "A bed with blue sheets" "A L-shape leather sofa" "A black chair with wheels"</p>
<p>Scene Caption</p>
<p>"This scene is a functional and organized apartment with various objects for daily activities.There are 5 cabinets, 1 bed, 3 trash cans, 1 microwave and 1 TV.The cabinets are in front of the trash cans and next to the counter…" &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a e w x I Q H 8 2 i w 4 G J 2 P X D q u j v F k B A I = " &gt; A A A C B X i c b V A 9 S w N B E N 3 z 2 8 S P q I 1 g s x g F C w l 3 I m o Z t L G w i G A 0 k I S w t 5 l L l u x 9 s D s X
D M f Z + k d s t Z K 0 F v 4 H w R + i t Z u P Q h M f D D z e m 2 F m n h t J o d G 2 P 6 2 Z 2 b n 5 h c W l 5 U x 2 Z X V t P b e x e a v D W H E o 8 1 C G q u I y D V I E U E a B E i q R A u a 7 E u 7 c z s X A v + u C 0 i I M b r A X Q d 1 n r U B 4 g j M 0 U i O 3 X f M Z t j m T y V X a S G o I 9 5 g o 8 N K 0 k c v b B X s I O k 2 c M c k X 9 7 7 6 7 9 3 s d 6 m R + 6 g 1 Q x 7 7 E C C X T O u q Y 0 d Y T 5 h C w S W k m V q s I W K 8 w 1 p Q N T R g P u h 6 M v w g p f t G a V I v V K Y C p E P 1 9 0 T C f K 1 7 v m s 6 B / f q S W 8 g H l L X / 8 + v x u i d 1 R M R R D F C w E f L v F h S D O k g E t o U C j j K n i G M K 2 H u p b z N F O N o g s u Y I J z J t 6 f J 7 V H B O S k c X z v 5 4 j k Z Y Y n s k F 1 y Q B xA N D + N 1 M L i 0 v J K e j W T X V v f 2 M x t b V d U E E n K y j Q Q g a w 5 R D H B f V Y G D o L V Q s m I 5 w h W d b p X I 7 / a Y 1 L x w L + D f s h s j 7 R 9 7 n J K Q E v N 3 G 7 D I 9 C h R M Q 3 S T N u A H u A O H D u k 6 S Z y 5 s F c w w 8 T 6 w p y R c P v o b v v e x 3 q Z n 7 a L Q C G n n M B y q I U n X L D M G O i Q R O B U s y j U i x k N A u a b O 6 p j 7 x m L L j 8 Q c J P t R K C 7 u B 1 O U D H q u / J 2 L i K d X 3 H N 0 5 u l f N e i P x G D v e f 3 4 9 A v f C j r k f R s B 8 O l n m R g J D g E e RX L 9 E E a b S H 9 t E R s t A 5 K q J r V E J l R N E j e k Y D 9 G I 8 G Q P j 1 R h O W l P G d G Y H / Y H x 9 g M v 6 p 1 C &lt; / l a t e x i t &gt; Lobj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P T l B u S i z 1 T j 4 B K o J j g 9 L 9 m e o r 2 U = " &gt; A A A C H X i c h V D L S g M x F M 3 U V 6 2 v U V f i J l g E F 1 J m R K y 4 K r p x W c E + Y D q U T O a 2 D c 1 k h i Q j l K F / I W 7 9 E b d 1 J S 4 V / 8 b 0 s d B W 8 F w u H M 6 5 l + S e I O F M a c f 5 s n J L y y u r a / n 1 w s b m 1 v a O v b t X V 3 E q K d R o z G P Z D I g C z g T U N N M c m o k E E g U c G k H / Z u w 3 H k A q F o t 7 P U j A j 0 h X s A 6 j R B u p b T u t V I Q g A 0 k o Z C 3 8 X + F h 2 y 4 6 J W c C v E j c G S l W r g 4 e P V 4 m 1 b b 9 0 Q p j m k Y g N O V E K c 9 1 E u 1 n R G p G O Q w L r V R B Q m i f d M E z V J A I l J 9 N L h v i Y 6 O E u B N L 0 0 L j i f p z I y O R U o M o M J M R 0 T 0 1 7 4 3 F U x x E f / l e q j u X f s Z E k m o Q d P p Y J + V Y x 3 g c F Q 6 Z B K r 5 w B B C J T P / x b R H T F L a B F o w Q b j z Z y + S + l n J v S i d 3 7 n F y j W a I o 8 O 0 R E 6 Q S 4 q o w q 6 R V V U Q x Q 9 o R c 0 Q q / W s z W y 3 q z 3 6 W j O m u 3 s o 1 + w P r 8 B 1 n C g F Q = = &lt; / l a t e x i t &gt; | {z } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D n M + I t X D t u / T X L 9 D c Z W Y b l T b z 0 w = " &gt; A A A C B X i c b V D J S g N B E O 2 J W 4 x b V B D B S 2 M Q P E i Y E V G P I V 4 8 J J C A W S A J Q 0 + n J 2 n S s 9 B d I 4 Z h v P o j X u N J v H r x G w Q v f o u d 5 a C J D w o e 7 1 V R V c 8 J B V d g m l 9 G a m l 5 Z X U t v Z 7 Z 2 N z a 3 s n u 7 t V V E E n K a j Q Q g W w 6 R D H B f V Y D D o I 1 Q 8 m I 5 w j W c A Y 3 Y 7 9 x z 6 T i g X 8 H w 5 B 1 P N L z u c s p A S 3 Z 2 c O 2 R 6 B P i Y h L i R 2 3 g T 1 A X C 6 V k 8 T O 5 s y 8 O Q F e J N a M 5 A o H 1 W 8 + K n 5 U 7 O x n u x v Q y G M + U E G U a l l m C J 2 Y S O B U s C T T j h Q L C R 2 Q H m t p 6 h O P q U 4 8 + S D B J 1 r p Y j e Q u n z A E / X 3 R E w 8 p Y a e o z v H 9 6 p 5 b y y e Y c f 7 z 2 9 F 4 F 5 3 Y u 6 H E T C f T p e 5 k c A Q 4 H E k u M s l o y C G m h A q u b 4 X 0 z 6 R h I I O L q O D s O b f X i T 1 8 7 x 1 m b + o W r l C E U 2 R R k f o G J 0 i C 1 2 h A r p F F V R D F D 2 i Z z R C L+ r U O w X w = " &gt; A A A B 9 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 R d K e q x 6 M V j h X 5 h u 5 Z s m r a h S X Z J s o W y 7 L / w q F 7 E q / / G g / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 I O J M G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3 v 5 B 8 f C o q c N Y E d o g I Q 9 V O 8 C a c i Z p w z D D a T t S F I u A 0 1 Y w v p v 5 r Q l V m o W y b q Y R 9 Q U e S j Z g B B s r P X Y D k Y z S p 6 S e 9 o o l t + z O g V a J l 5 E S Z K j 1 i l / d f k h i Q a U h H G v d 8 d z I + A l W h h F O 0 0 I 3 1 j T C Z I y H t G O p x I J q P 5 l f n K I z q / T R I F S 2 p E F z 9 f d E g o X W U x F c B M I 2 C 2 x G e t m e i f 9 5 n d g M b v y E y S g 2 V J L F r k H M k Q n R L A H U Z 4 o S w 6 e W Y K K Y P R e R E V a Y G J t T w e b g L X + 9 S p q X Z e + q X H m o l K q 3 W S J 5 O I F T O A c P r q E K 9 1 C D B h C Q 8 A y v 8 O Z M n B f n 3 f l Y t O a c b O Y Y / s D 5 / A H j 7 p I p &lt; / l a t e x i t &gt; h T Language Encoder ! Spatial Attention &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V b y Z v e P + p q x A z j I T R I e U P c A 8 Y q o = " &gt; A A A B 9 X i c b V D L S g N B E O y N r x h f U Y + K D A b B g 4 R d E f U Y 9 O I x g b w w i W F 2 M k m G z M w u M 7 N K W H L 0 D 7 z G k 3 j 1 m i 8 R / A Z / w s n j o I k F D U V V N 9 1 d f s i Z N q 7 7 5 S S W l l d W 1 5 L r q Y 3 N r e 2 d 9 O 5 e W Q e R I r R E A h 6 o q o 8 1 5 U z S k m G G 0 2 q o K B Y + p x W / d z v 2 K 4 9 U a R b I o u m H t C F w R 7 I 2 I 9 h Y 6 b 7 u i 7 g z e I i L g 2 Y 6 4 2 b d C d A i 8 W Y k k z s c F b 6 f j 0 b 5 Z v q z 3 g p I J K g 0 h G O t a 5 4 b m k a M l W G E 0 0 G q H m k a Y t L D H V q z V G J B d S O e X D x A J 1 Z p o X a g b E m D J u r v i R g L r f v C t 5 0 C m 6 6 e 9 8 b i G f L F f 3 4 t M u 3 r R s x k G B k q y X R Z O + L I B G g c A W o x R Y n h f U s w U c z e i 0 g X K 0 y M D S p l g / D m 3 1 4 k 5 f O s d 5 m 9 K H i Z 3 A 1 M k Y Q D O I Z T 8 O A K c n A H e S g B A Q k v M I R X 5 8 k Z O m / O + 7 Q 1 4 c x m 9 u E P n I 8 f Y F i V 9 g = = &lt; / l a t e x i t &gt; g T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d K D h k 3 a G n c I 0 7 M j d E j y D F Z j d P w 4 = " &gt; A A A B + X i c b V D L S s N A F L 3 x W V s f r a 7 E z W A Q X E h J R N R l 0 Y 3 L i v Y B T V o m 0 0 k 7 d C Y J M x O h h P 6V 3 E q C a 2 R m M e y G W B F O Y t o T T P N a T O R F I u A 0 0 Y w u J 3 4 j S c q F Y u j R z 1 M q C 9 w L 2 I h I 1 g b q e 1 l X i C y c N R h 7 Q d v 1 C n a T t m Z A i 0 T d 0 7 s i n 3 4 V Z L f h W q n + O F 1 Y 5 I K G m n C s V I t 1 0 m 0 n 2 G p G e F 0 l P d S R R N M B r h H W 4 Z G W F D l Z 9 O r R + j E K F 0 U x t J U p N F U / T 2 R Y a H U U A S m U 2 D d V 4 v e R D x D g f j P b 6 U 6 v P Y z F i W p p h G Z L Q t T j n S M J j G g L p O U a D 4 0 B B P J z L 2 I 9 L H E R J u w 8 i Y I d / H t Z V I / L 7 u X 5 Y t 7 1 6 7 c w A w 5 O I J j O A U X r q A C d 1 C F G h C Q 8 A x j e L E y a 2 y 9 W m + z 1 h V r P n M A f 2 C 9 / w A x F 5 b J &lt; / l a t e x i t &gt; {f S i } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 H E X h Y d F V 4 N 2 n i q R M O V d 5 + M 8 y / Q = " &gt; A A A B + X i c b V D L S s N A F L 3 x W V s f r a 7 E z W A Q X E h J R N R l 0 Y 3 L i v Y B T V o m 0 0 k 7 d C Y J M x O h h PV 3 E q C a 2 R m M e y G W B F O Y t o T T P N a T O R F I u A 0 0 Y w u J 3 4 j S c q F Y u j R z 1 M q C 9 w L 2 I h I 1 g b q e 1 l X i C y / q j D 2 g / e q F O 0 n b I z B V o m 7 p z Y F f v w q y S / C 9 V O 8 c P r x i Q V N N K E Y 6 V a r p N o P 8 N S M 8 L p K O + l i i a Y D H C P t g y N s K D K z 6 Z X j 9 C J U b o o j K W p S K O p + n s i w 0 K p o Q h M p 8 C 6 r x a 9 i X i G A v G f 3 0 p 1 e O 1 n L E p S T S M y W x a m H O k Y T W J A X S Y p 0 X x o C C a S m X s R 6 W O J i T Z h 5 U 0 Q 7 u L b y 6 R + X n Y v y x f 3 r l 2 5 g R l y c A T H c A o u X E E F 7 q A K N S A g 4 R n G 8 G J l 1 t h 6 t d 5 m r S v W f O Y A / s B 6 / w E 0 N Z b L &lt; / l a t e x i t &gt; {h S i }
Transformer Encoder  object features tf O i u with an object point cloud encoder and text features tf T i u by feeding object-captions tT obj i u into a frozen language model.Following [96], we perform cross-modal alignment on the object features and text features via:
f i N S u i J x M p n 4 = " &gt; A A A B + X i c b V D L S s N A F L 3 x W V s f r a 7 E z W A Q X E h J R N R l 0 Y 3 L C n 1 B k 5 b J d N I O n U n C z E Q o o f / h t q 7 E r f 8 i u P I T / A W n j 4 W 2 H r h w O O d e 7 r 0 n S D h T 2 n E + r b X 1 j c 2 t 7 d x O v r C 7 t 3 9 Q L B 0 2 V J x K Q u s k 5 r F s B V h R z i J a 1 0 x z 2 k o k x S L g t B k M 7 6 d + 8 4 l K x e K o p k c J 9 Q X u R y x k B G s j d b z M C 0 Q W j r u s U / P G 3 a L t l J 0 Z 0 C p x F 8 S u 2 M d f J f l d q H a L H 1 4 v J q m g k S Y c K 9 V 2 n U T 7 G Z a a E U 7 H e S 9 V N M F k i P u 0 b W i E B V V + N r t 6 j M 6 M 0 k N h L E 1 F G s 3 U 3 x M Z F k q N R G A 6 B d Y D t e x N x Q s U i P / 8 d q r D W z 9 j U Z J q G p H 5 s j D l S M d o G g P q M U m J 5 i N D M J H M 3 I v I A E t M t A k r b 4 J w l 9 9 e J Y 3 L s n t d v n p 0 7 c o d z J G D E z i F c 3 D h B i r w A F W o A w E J z z C B F y u z J t a r 9 T Z v X b M W M 0 f w B 9 b 7 D z K h l s o = &lt; / l a t e x i t &gt; {f T i } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z P r q 4 n S J M v / v 3 T + i n 0 i i w C N M Z u c = " &gt; A A A B + H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g Q U o i R T 0 W v X i z g v 2 A J i 2 b 7 a Z d u r s J u x u h h v w P j + p F v P p f P P h v 3 L Y 5 a O u D g c d 7 M 8 z M C 2 J G l X a c b 6 u w s r q 2 v l H c L G 1 t 7 + z u l f c P W i p K J C Z N H L F I d g K k C K O C N D X V j H R i S R A P G G k H 4 5 u p 3 3 4 k U t F I P O h J T H y O h o K G F C N t p J 6 X e g F P w 6 x P e 3 d e 1 i 9 X n K o z g 7 1 M 3 J x U I E e j X / 7 y B h F O O B E a M 6 R U 1 3 V i 7 a d I a o o Z y U p e o k i M 8 B g N S d d Q g T h R f j q 7 O r N P j D K w w 0 i a E t q e q b 8 n U s S V m v D g L O C m m S M 9 U o v 2 V P z P 6 y Y 6 v P J T K u J E E 4 H n u 8 K E 2 T q y p y n Y A y o J 1 m x i C M K S m n N t P E I S Y W 2 y K p k c 3 M W v l 0 n r v O p e V G v 3 t U r 9 O k + k C E d w D K f g w i X U 4 R Y a 0 A Q M E pL obj " ´12
where D obj pp, qq " pf O p f T q {τ q denotes the dot product between object and text features and pp, qq denotes a pair of aligned object-text pair in the training batch and r iterates over all object-text pairs in the training batch.Similar to CLIP [78], we use a learnable temperature parameter τ to facilitate model learning.</p>
<p>Scene-level Grounding</p>
<p>With aligned object features, we encode the scene by incorporating object spatial locations into the extracted object features.Specifically, we use a spatial transformer model to encode extracted object features tf O i u with their spatial location features tl i u following [18,112]:
f S " SpatialAttnptf O i u, tl i uq
where tf S i u denotes the feature of object o i after encoding with spatial location features.To perform scene-level alignment, we operate on these scene-level object features tf S i u and align it with the scene caption T scene .Specifically, we feed the object features into a projection layer and use max-pooling over all object features to obtain the scene feature g S .Similar to object-level grounding, we pass the scene caption through a tunable language model to obtain text feature g T and perform scene-level contrastive alignment through:
L scene " ´12
where D scene pp, qq " pg S p g T q {τ q denotes the dot product between scene feature g S p and scene caption feature g T q for each pair of aligned scene-text pairs in the training batch and r iterates over all scene-text pairs in the training batch.</p>
<p>Referral-object-level Grounding</p>
<p>To model the relationships revealed in referring expressions, we employ a selfattention-based reasoning transformer for grounding object referrals in scenes.This transformer takes in scene-object features tf S i u and an object referral T ref and performs self-attention to learn relationships between text descriptions and object relationships.We use the same tunable language encoder as in scene-level grounding for extracting per-object referral features.We pass this text feature together with scene-object features into the self-attention transformer to obtain the aligned object features h S i and the sentence-level referral feature h T .We then perform the referral-object-level contrastive alignment following:
L ref " ´log exp <code>h S h T {τ řp exp</code>hS p h T {τ ˘,(3)
where hS denotes the feature of the referred object, p iterates over all objects within the same scene.Notably, in contrast to inter-scene contrast that was done in object-and scene-level alignment, we force the selection of positive pairs to be within the same scene to provide intra-scene contrast for fine-grained object grounding.This mimics the success of intra-image and inter-image contrasts commonly used for region-word alignment in 2D-VL models [104].</p>
<p>To learn the multi-level alignment between 3D scenes and language, we first train the point cloud encoder with an object-level grounding objective to obtain a good feature initialization for grounding objects in scenes.During the scene grounding stage, we train our inter-and intra-scene objectives together with a masked language modeling loss L MLM over the inputted object-referral texts to tune the parameters within the language encoder and self-attention transformer.Above all, the learning of GPS could be summarized as optimizing:</p>
<p>L " L obj <code>Lscene</code>Lref `LMLM .</p>
<p>Experiments</p>
<p>In this section, we present experimental results addressing the following questions:</p>
<ol>
<li>How effective is the data scaling in SceneVerse for 3D visual grounding?Does the scale-up benefit common 3D-VL tasks (e.g., 3D question answering, open-vocabulary 3D semantic segmentation) and pre-training-based models?2. How well is the GPS pre-training pipeline for 3D-VL tasks?Does it exhibit similar properties of 2D-VL models in 3D-VL tasks? 3. What is offered by SceneVerse and GPS and what is missing?</li>
</ol>
<p>In the following sections, we describe in detail the model performance regarding these key topics.Due to the page limit, we direct readers to the supplementary for implementation details, qualitative results, and more experimental analyses.</p>
<p>3D Visual Grounding</p>
<p>Settings We evaluate our model on three commonly-used datasets for 3D visual grounding: ScanRefer [16], Nr3D, and Sr3D [1].For Nr3D and Sr3D, we follow Achlioptas et al .[1] and report the grounding accuracies of models using groundtruth object masks.For ScanRefer, we follow Zhu et al .[112] and use Mask3D [83] to generate object proposals.Results are reported as Acc@0.5 to evaluate the correctness of predictions whose object bounding boxes overlap the ground truth with IoU ą 0.5.For comparisons, we compare with existing baselines by providing the results of pre-trained GPS and dataset-specific fine-tuned GPS.Please see more details in the supplementary.</p>
<p>Results and Analyses</p>
<p>As shown in Tab. 2, GPS trained on SceneVerse achieves state-of-the-art results on all existing 3D-VL grounding benchmarks.Initially, when GPS is trained directly on the training sets of benchmark datasets, labeled as Ours (scratch), it underperforms compared to existing models that employ more complex structures or loss designs.This result underscores the dataintensive nature of the contrastive alignment paradigm.However, when presented with extensive training data in SceneVerse, the results of our model without additional fine-tuning, i.e., Ours (pre-train), significantly improves and already achieves state-of-the-art results on benchmarks like ScanRefer.Moreover, the dataset-specific fine-tuned model, i.e., Ours (fine-tuned ), consistently outperforms existing baselines with only a simple projection MLP added on top of the pretrained model, jointly optimized during fine-tuning without any other auxiliary architecture or loss objective.These results underscore the strong potential of both the SceneVerse and GPS for 3D-VL tasks.</p>
<p>Additional 3D-VL Tasks</p>
<p>Settings We evaluate the effectiveness of GPS and SceneVerse on additional 3D-VL tasks: (i) 3D question answering (3D-QA) on ScanQA [5] and SQA3D [67], and (ii) open-vocabulary 3D semantic segmentation (OV-Seg) on ScanNet.</p>
<p>‚ In the 3D-QA task, we follow Zhu et al .[112] and evaluate models over the exact match metic (EM@1) on the validation and test sets of ScanQA, as well as the test set of SQA3D.We pre-train GPS on SceneVerse and fine-tune the model on the 3D-QA dataset to compare with state-of-the-art models.‚ In the OV-Seg task, as GPS builds upon an object-centric design and thus is not directly applicable to semantic segmentation, we consider testing the effectiveness of SceneVerse on improving existing 3D models.Specifically, we follow the open-vocabulary semantic segmentation settings proposed by Yang et al .[97] and report the mIoU and mAcc score.We compare with existing works by pre-training the RegionPLC [97] model on SceneVerse.</p>
<p>Results and Analyses</p>
<p>We present the results of 3D-QA experiments in Tab. 5 and the results of OV-Seg experiments in Tab. 6.The analyses are as follows: ‚ As shown in Tab. 5, our model achieves state-of-the-art results on both benchmarks, outperforming recent strong pre-training-based baselines like 3D-VisTA and 3D-LLM.As SceneVerse currently contains only descriptions of objects and scenes, we believe involving more types of language descriptions (e.g., question-answer pairs, dialogues) is a promising direction for further improving model performance on these downstream tasks.‚ As shown in Tab.6, we observe consistent performance improvement of existing 3D backbone models on this task when pre-trained with SceneVerse data.This result validates that the collected data in SceneVerse can effectively boost the performance of existing models on scene understanding tasks.We further provide results of state-of-the-art 3D models that are pretrained on SceneVerse on the close-vocabulary 3D semantic segmentation task in the supplementary.</p>
<p>Ablative Studies and Discussion</p>
<p>In this section, we present further discussions on both the data collected in SceneVerse and the GPS model design.We aim to elucidate the effects of  data scaling and show more clearly its effectiveness in 3D scene understanding.Regarding the experimental settings and more results discussion, refer to the supplementary.The following points are specifically discussed in this section:</p>
<p>How important is data-scaling?We conduct ablation studies over the amount of data used while pre-training GPS.We consider the model trained with of SceneVerse to show the effectiveness of data-scaling on model performance in the pre-train and zero-shot transfer settings in ScanRefer and SceneVerse-val.</p>
<p>As shown in Fig. 4, we observe consistent performance improvement over the increase of data scale for both settings.We provide additional experiments in the supplementary to show that such scaling effect is not only beneficial for 3D-VL grounding but also for other 3D tasks like semantic segmentation [83,99].</p>
<p>How is the generated data compared with human-annotated data?</p>
<p>We assess the performance of models trained using various scene-text sources, specifically focusing on their performance in the ScanRefer dataset without additional fine-tuning.As shown in Tab. 7, models trained with our template-based generated texts and Large Language Model (LLM)-refined texts show significant improvements over models trained solely on ScanRefer.More importantly, these variants of our model already achieve state-of-the-art results compared with previous baselines.This indicates the effectiveness of our text-generation pipeline.Finally, we observe that adding human-annotated data is still beneficial for model performance.However, the improvement is relatively marginal over models trained on our generated data.</p>
<p>What is the role of the synthetic scenes in this scale-up process?With synthetic data providing large-scale and diverse scene data for 3D-VL tasks, we evaluate the models' domain transfer (Sim2Real) capability.Specifically, we compare models trained on all real scenes in SceneVerse against models trained exclusively on two synthetic subsets of SceneVerse, i.e., Structured3D and ProcTHOR.As shown in Tab. 8, models trained on synthetic subsets demonstrate remarkable performance on their corresponding test sets while suffering when transferred to real or other synthetic scenes.In contrast, the model trained on real scene-text pairs exhibits less performance drop when generalizing to synthetic scenes.This result affirms the domain gap between real and synthetic scenes in 3D-VL grounding and shows that a simple scale-up in the number of scenes is insufficient when naturalness can not be guaranteed.Considering the scalability of our language generation pipeline and the scaling effect shown in our experiments, the rate-determining step for further scaling-up 3D-VL comes to the collection of diverse, high-quality, and realistic scenes that capture natural 3D scene distributions.</p>
<p>How important is the design of each module in GPS?We provide ablative analyses of our multi-level contrastive alignment design in Tab. 9. We mainly consider removing objectives in our model to reveal the effectiveness of each level of alignment.We choose the referral-object-level alignment objective as the default setting and consider removing: (i) object-level alignment objective, (ii) masked language modeling objective, and (iii) scene-level alignment objective.When removing the object-level alignment objective, we learn the object point cloud encoder with the referral-object-level alignment and without pre-training.</p>
<p>As shown in Tab. 9, we test different models on the SceneVerse-val without additional fine-tuning.Results show that the scene-level alignment objective is crucial for referral object grounding in SceneVerse-val with the "5% performance drop.Similar observations could be made for the model trained without object-level alignment ("2% drop) and masked language modeling objective ("1.5% drop).These results affirm the effectiveness of our model design.</p>
<p>Conclusion</p>
<p>In this work, we scale up 3D-VL for grounded scene understanding.We present SceneVerse, a million-scale 3D-VL dataset covering various scenes and multilevel scene descriptions sourced from both human annotation and our proposed scene-text generation approach.Utilizing SceneVerse, we propose Grounded Pre-training for Scenes (GPS), a model trained with multi-level scene-language contrastive alignment.Through extensive experiments, we show that GPS achieves state-of-the-art results on common 3D-VL tasks including grounding and question answering.We further conduct zero-shot transfer experiments to show the improved generalization performances of GPS trained on SceneVerse compared with previous baselines.We also demonstrate that the scaling effect of SceneVerse is generally beneficial for existing 3D models on 3D-VL tasks like semantic segmentation.We hope our efforts and successful scale-up attempts in SceneVerse could pave the way for new research paradigms in 3D-VL.We first instantiate the graph nodes with the instance annotation from the point cloud and parameterize each node with object centroid p i P R 3 and size of the axis-aligned bounding box b i " pb x , b y , b z q P R 3 (Line 1-3).Next, we traverse all the nodes to determine their spatial relationships (Line 4-22).Notably, in cases where an object node lacks any in-contact vertical relationships with other objects in the scene, we designate such objects as "hangable" and calculate their non-contact vertical relationships (Line 9-13).Examples of such objects include paintings, curtains, etc.Finally, we establish relationships between multiple objects (Line 23): i) When a target object is connected with two edges labeled left and right, the target object, along with the two neighboring nodes, forms a between relationship triplets.ii) If the offset of the center point coordinates of a group of objects in either the X-axis or Y-axis direction is smaller than a specified offset threshold δ, then this group of objects forms an align relationship.The offset threshold δ will be adjusted based on the size of the scene.In additional, we utilize an automatic verification procedure to validate the scene graph, further improving the quality of the scene graph we constructed (line 24).One of the verification operations involves manually maintaining a mapping between objects and relationship descriptions based on common sense.For example, people usually use "mounted on" to describe the relation between TV and wall, rather than "hanging on".Therefore, we would automatically refined ( TV, hanging on, wall) to ( TV, mounted on, wall).</p>
<p>In our constructed 3D scene graph G " pV, Eq, the nodes V comprises the union of node sets V 1 Ť V 2 Ť . . .Ť V K , with V k representing the set of nodes at a particular hierarchical level.The hierarchies are determined by the support relationship; for instance, objects supported by the floor constitute V 0 , while objects supported by the table will form V 1 , etc.Note that edges originating from one node v P V k may only terminate in nearby hierarchies
V k Y V k<code>1 Y V k</code>1 .
In other words, edges in the scene graph exclusively connect nodes within the same hierarchical level, or one level higher or lower.</p>
<p>Algorithm 1: Scene Graph Construction Pipeline</p>
<p>Input : M object point clouds tP 1 , P 2 , . . ., P m u Output : 3D scene graph GpV, Eq 1: for i from 1 to M do 2:</p>
<p>Create node v i P V using the centroid p i and bounding box size b i of object point cloud P i 3: end for 4: for i from 1 to M do 5:
for j from i `1 to M do 6: RelsType v Ð VerticalInContactpv i , v j q 7:
Add in-contact vertical relationship triplets pv i , v j , e i,j q with RelsType v to G if No objects horizontally related to v i then 10:</p>
<p>for k from 1 to M and i ‰ k do 11:
RelsType v Ð VerticalNonContactpv i , v k q 12:
Add non-contact vertical relationship triplets pv i , v k , e i,k q with RelsType v to G let tv i1 , v i2 , ..., v i N u be the N different nodes with the same in-contact vertical parent node v i 18:</p>
<p>for j from 1 to N do 19:
RelsType h Ð Horizontalpv i , v ij q 20:
Add horizontal relationship triplets pv i , v ij , e i,ij q with RelsType h to G</p>
<p>A.3 Language Generation Details</p>
<p>In Sec.3.4, we adopt both templates and LLM to automatically generate scenelanguage pairs in SceneVerse.More technical details and examples are provided in this section.</p>
<p>Object Captioning Pipeline Object captions aim to provide detailed descriptions of an object's visual and physical properties, facilitating object-level grounding with its distinctive features.The detailed object captioning pipeline is outlined in Algorithm 2. Given the multi-view images tI 1 , I 2 , . . ., I n u, we utilize the point cloud P o of the object o to get the visible points P vis o,v in the images v through rendering.The occlusion score s occ o,v is calculated as the ratio between the number of visible points and the object point cloud.The image is then cropped with the rendered bounding box and processed through BLIP2 [58] to generate the initial object caption C o,v .For each initial caption, we calculate its CLIP [78] similarity score between the text and the cropped image, denoted by s clip o,v .To get a refined object caption, we select the top 10 initial captions with the highest CLIP score and minimal occlusion.The selected sentences are fed into a LLM to obtain a coherent summary of the object captions.In this process, we explicitly instruct the language model to identify and correct the potential errors.</p>
<p>The nightstand in the apartment is a small white table with a suitcase on it, along with a laptop and a bag.</p>
<p>In a real apartment, a wooden stool is seen in the kitchen, placed on a tile floor next to a table .A vibrant green chair with a polka dot pattern adds a lively touch to various settings, including a desk and table .A small round table adorned with a glass and accompanied by two chairs stands in a restaurant</p>
<p>Scene Caption</p>
<p>In this apartment, there are 5 cabinets, 1 bed, 3 trash cans, 1 microwave, and 1 TV.The cabinets are positioned in front of the trash cans, while the bed is in front of the cabinet.The trash cans are also behind the cabinet and to the left of the bed.The TV is inside one of the cabinets.The bed is positioned behind the cabinet and to the right of the trash cans.This apartment seems to be well-equipped with storage options and has a comfortable sleeping area.</p>
<p>Scene Caption</p>
<p>In this room, there is an architectural floor and wall.The wall are attached to the floor, creating a room with a big door.There are blind hanging on the wall, close to the window.The room has a wide window, a heater connected to a wall, and a ceiling overhead.The room is furnished with a sofa, a table, and a chair.There are cushion and beanbag on the sofa, and a plant and lamp nearby.The room also has a TV, a whiteboard, and some clutter on the floor.The overall style of the room is comfortable and modern.</p>
<p>Scene Caption</p>
<p>In this room, there is a bed, two windows, three lamps, three blankets, a TV, six pillows, two cups, a curtain, and four shelves.The TV is positioned higher than the shelf, while the sofa is to the right of the bed.One of the pillows is inside the bed, and the bed is located to the left of the sofa.Additionally, the lamp is positioned higher than the power outlet, which is lower than the lamp.The room appears to be a comfortable living space with various objects for relaxation and entertainment.-The target-object (is) spatial-relation the anchor-object.</p>
<p>-It is a target-object that (is) spatial-relation the anchor-object.</p>
<p>-There is a target-object that (is) spatial-relation the anchor-object.</p>
<p>-Spatial-relation the anchor-object is the target-object.</p>
<p>-Spatial-relation the anchor-object, a target-object is placed.</p>
<p>-Multi-objects: This is utilized when the target object forms a between or align relationship with multiple anchor objects in the scene.The templates follow the same construction rules as the Pair-wise templates.-Star-reference: To increase complexity in templated-based descriptions, we design "star-reference" to describe the target object and its relationship with 3 randomly selected anchor objects in the scene graph.In particular, we perform cluster analysis on the selected relationship triplets.Based on the diversity of the analysis, different templates will be chosen to generate descriptions.For example, when the relations between 3 anchor objects and the target object is the same, we prefer to use the template like: "The target-object (is) spatial-relation the anchor-object-1, anchorobject-2 and anchor-object-3".If 2 out of the 3 anchor objects have the same relations with the target object, we would use a template like:</p>
<p>Template-based</p>
<p>The shelf is hanging on the wall LLM-rephrased The wall is adorned with a suspended shelf</p>
<p>Template-based</p>
<p>The sofa is supported by the floor</p>
<p>LLM-rephrased</p>
<p>The sofa rests upon the floor</p>
<p>Template-based</p>
<p>Close to the beanbag chair is another beanbag chair LLM-rephrased Another beanbag chair lies nearby, within close proximity to the first one</p>
<p>Template-based</p>
<p>The tv is higher than shelf</p>
<p>LLM-rephrased</p>
<p>The tv sits atop the shelf</p>
<p>Template-based</p>
<p>The microwave is inside the kitchen cabinet</p>
<p>LLM-rephrased</p>
<p>The kitchen cabinet contains a microwave</p>
<p>Template-based</p>
<p>It is a couch in the middle of stool and the bicycle</p>
<p>LLM-rephrased</p>
<p>The bicycle and stool are positioned on either side of the couch</p>
<p>Template-based</p>
<p>The dish rack, bag and bottle are in aligned</p>
<p>LLM-rephrased</p>
<p>The dish rack, bag, and bottle are arranged in a harmonious triad</p>
<p>Template-based</p>
<p>The plant is to the right of the shelf</p>
<p>LLM-rephrased</p>
<p>The plant is positioned to the right of the shelf</p>
<p>Template-based</p>
<p>The lamp is hung on the wall and is higher than shelf, also is above light switch LLM-rephrased The lamp, situated at a comfortable height above the light switch and positioned on the wall, creating an inviting atmosphere perfect for relaxation or reading a book on the adjacent shelf</p>
<p>Template-based</p>
<p>The sofa is close to the table and is to the left of whiteboard and beanbag chair LLM-rephrased The plush sofa, with its soft cushions, is strategically positioned near the table, while also being conveniently accessible from the whiteboard and beanbag chair</p>
<p>Template-based</p>
<p>The kitchen cabinet is above the counter and microwave and bag</p>
<p>LLM-rephrased</p>
<p>The kitchen cabinet, a convenient storage space for culinary essentials, sits proudly above the counter and microwave, within easy reach for bagging groceries</p>
<p>Template-based</p>
<p>The bed is in front of the case and is lower than lamp, also is to the left of trash bin</p>
<p>LLM-rephrased</p>
<p>The bed, situated in front of the case and lower than the lamp, is also positioned to the left of the trash bin, serving as a comfortable spot for rest and relaxation."The target-object (is) spatial-relation-1 the anchor-object-1 and anchorobject-2, and (is) spatial-relation-2 the anchor-object-3."</p>
<p>LLM-rephrasing</p>
<p>To increase description diversity we use the GPT-3.5 [73] and Llama [86] for description rephrasing.This improves the diversity and naturalness of the template-based descriptions, as is shown in Fig. 2. The detailed prompts are provided in Tab. A. Get the image caption C o,v for I crop o,v using BLIP2 [58] 6:</p>
<p>Calculate the similarity score s clip o,v between C o,v and I crop o,v with CLIP [78]  In Sec.4.2, we leveraged and spatial-attention based transformer architecture to aggregate object-level point cloud features with spatial location information.In this section, we provide the detailed design of this proposed module.</p>
<p>Formally, given object features tf O i u N i"1 and their locations tl i u N i"1 , we first construct pair-wise spatial relationship feature via: m ij " rd ij , sinpθ h q, cospθ h q, sinpθ v q, cospθ v qs , where d ij denotes the Euclidean distance between objects and θ h , θ v are the horizontal and vertical angles of the line connecting the centers of objects i, j.We then use this pair-wise relationship feature M " rm ij s P R N ˆN ˆ5 to modulate the attention weights of the self-attention layer in the transformer when aggregating object features as shown below:
AttnpQ, K, V, M q " softmax ˆQK T ? d h `log σpM ωq ˙V,
where ω P R 5 is a projection layer mapping spatial pair-wise features to the attention scores and σ denotes the sigmoid function.This process could be equivalently interpreted as using the spatial location of objects to adjust the selfattention feature aggregation between objects, making spatially related objects have more attention weights.Scene captioning Your task is to provide a summary for a scene from a given scene graph.The scene contains some objects, which compose a scene graph in json format.There are 3 types of descriptions in scene graph: "scene type" denotes the type of the scene."objects count" then listed the objects in the scene and their quantity, it should be noted that the actual objects in the room may be more than listed."objects relations" describe the spatial relations with objects.Also describe the scene concerning commonsense, e.g., how the objects can be used by human and human activity in the scene.The description should conform to the given scene graph.The spatial relations between objects can only be inferred from the "objects relations" in scene graph.Don't describe each object in the scene, pick some objects of the scene for summary.Don't describe each relations in the scene, pick some relations of the scene for summary.You can also summarize the room's function, style, and comfort level based on the arrangement and count of objects within the room.The summary should be about the object types, object attributes, relative positions between objects.Your summary must not exceed 80 words.You must write using one random sentence structure.scene graph: { 'scene_type': 'Bedroom', 'object_count': {'nightstand':2, ...}, 'relation': {'nightstand', 'on', 'floor'}, {'backback', 'in front of', 'bed'}, ...}</p>
<p>B.2 Pre-training Details</p>
<p>For training our model GPS, we conduct a two-stage training approach.As described in Sec.4.3, we first pre-train the object point cloud encoder with the object-level grounding objective.Next, we freeze the object point cloud encoder during the second pre-training stage for scene-level pre-training that includes model training with scene-level grounding and referral object grounding objectives.This design is inspired by recent works like [23,112] that demonstrated a well-grounded initialization of object representations is beneficial for 3D scene grounding.</p>
<p>Object-level pre-training</p>
<p>To correctly align objects in scenes with their captions, we utilize the ground-truth object bounding boxes provided with the datasets to segment all objects in the scenes.Next, we utilize a PointNet++ [77] encoder to encode and align these object point clouds with object captions provided in SceneVerse following Sec.4.1.For object instances with no object captions synthesized, we follow [78] and construct captions with their semantic class labels like "the point cloud of <CLASS>".Notably, as our model design sets no constraints on object point cloud encoders, the choice of object encoder mainly depends on the computing resources available.</p>
<p>Scene-level pre-training With pre-trained object feature extractors, we further use both scene captions and object-referring expressions for scene-level pretraining.We use a 4-layer BERT encoder for encoding both scene captions and object referrals.As discussed in Sec.4.2, we apply a 4-layer spatial transformer to encode object features with their locations.For scene-level grounding, we adopt a max-pooling layer to aggregate features from the spatial transformer and align with the [CLS] feature of the scene caption.For referral-object-level grounding, we further pass the obtained object features as well as the referral language features into a 4-layer self-attention transformer and use the grounding objective described in Sec.4.3 to match the referred object's feature and the [CLS] feature of the referring expression.</p>
<p>Training For object-level pre-training, we utilize an AdamW optimizer with a learning rate of 1 ˆ10 ´2 for 1500 epochs and no warm-up periods.During training, we use a batch size of 512 and leverage a cosine annealing scheme for learning rate scheduling with a minimum learning rate of 1 ˆ10 ´3.For scene-level pre-training, we use an AdamW optimizer with a learning rate of 1 ˆ10 ´5 for the language encoder, a learning rate of 1 ˆ10 ´4 for the spatial transformer, a learning rate of 1 ˆ10 ´4 for the self-attention transformer, and a learning rate of 5 ˆ10 ´4 for all remaining learnable parameters (e.g., projections).For all experiments, we train the model for 150 epochs with a warm-up period of 500 and also a cosine annealing scheme for learning rate with a minimum learning rate ratio of 0.1.All pre-training experiments are run on 8 NVIDIA-A100 GPUs with the longest pre-training on SceneVerse taking about 2 days.</p>
<p>C Experimental Details</p>
<p>In this section, we provide details on experimental settings, model implementation, and additional results.</p>
<p>C.1 3D Visual Grounding</p>
<p>Setting For all datasets, we evaluate all models with only the training sets provided.Following previous works [112], we report model performance on the validation set of all datasets in Tab. 2. Notably, we used an off-the-shelf Mask3D segmentation model for generating object proposals with no optimization.</p>
<p>Implementation As briefly mentioned in Sec.5.1, we mainly considered three model settings in 3D visual grounding experiments, namely scratch, pre-train, and fine-tuned.For the pre-train setting, we follow the same setting mentioned in Appendix B.2.In the scratch and fine-tuned settings, to fairly compare with other dataset-specific fine-tuned models, we add an additional 2-layer MLP over the object features from the referral grounding self-attention transformer.During training, we fine-tune this grounding head together with all model weights for 100 epochs with a learning rate of 1 ˆ10 ´4 for the added projection layer and set all other settings the same as the implementation described in Appendix B.2.</p>
<p>C.2 Zero-shot Transfer</p>
<p>Setting In the zero-shot experiments, we first construct the held-out test set by aggregating scene-text pairs in SceneVerse from scenes in ScanNet and MultiScan.Specifically, we use the validation set of ScanRefer, Nr3D, and Sr3D.For scene-text pairs in the SceneVerse-val, we construct the test set by randomly sampling 1  5 of human-annotated object referrals in the MultiScan dataset.This results in a test set with around 1.7K object referrals randomly drawn from 8.5k human-annotated object referrals in the MultiScan dataset.In the zero-shot settings, we use all scene-text pairs from datasets in SceneVerse except for ScanNet and MultiScan.This includes both human-annotated and generated texts in ARKitScenes, 3RScan, and HM3D.This setting serves to test models' generalization capability in grounding objects with both unseen scenes and unseen texts.In the zero-shot text setting, we add generated scene-text pairs in ScanNet and MultiScan into the data used in the zero-shot setting, thereby making the held-out test contain mainly unseen object referrals.Implementation In the zero-shot experiments, we mainly considered three model settings scratch, zero-shot, and zero-shot text.For the zero-shot setting, we pre-train the model following Appendix B.2 without additional grounding heads considering there is no additional training data available in the zero-shot transfer setting.In the scratch and zero-shot text setting, we follow the model implementation described in Appendix C.1 and add an additional 2-layer MLP over the object features from the self-attention transformer.We follow the same fine-tuning setting described in Appendix C.1.</p>
<p>C.3 3D question answering</p>
<p>Setting In the 3D-QA experiments, we evaluate all models with only the training sets provided for fine-tuning.Following previous works [112], we report model performance on the validation and test sets of ScanQA and the test set of SQA3D.</p>
<p>Implementation As briefly mentioned in Sec.5.3, we mainly considered the fine-tuned GPS when comparing with existing methods.For all datasets, we initialize our GPS model from a checkpoint pre-trained with 3D visual grounding on SceneVerse.We follow 3D-VisTA and add an additional question-answering module over the pre-trained representations for the answer prediction.We finetune the model for 100 epochs with a learning rate of 1 ˆ10 ´4 for the added question answering head and set all other settings the same as the implementation described in Appendix B.2.</p>
<p>C.4 Open-vocabulary 3D semantic segmentation</p>
<p>Setting Following RegionPLC [97] proposed by Yang et al ., we conduct experiments to assess the performance of SceneVerse on open-vocabulary 3D semantic segmentation (OV-Seg).To establish a benchmark for open-vocabulary semantic segmentation, we adopt the experimental setup outlined in PLA [31], as per the methodology of RegionPLC.We utilize the annotation-free training setting, as described in RegionPLC, wherein semantic labels for all categories are omitted.This approach allows us to evaluate the effectiveness of SceneVerse in facilitating open-vocabulary segmentation without relying on predefined semantic segmentation annotations.For evaluation, we compute the mean Intersection over Union (mIoU) and mean accuracy (mAcc) across 17 foreground categories, excluding "wall" and "floor" background classes, as well as the "other furniture" category due to its inherent ambiguity.</p>
<p>Implementation We employ SparseUNet [36] as our 3D backbone network for extracting point features.We utilize different variants of SparseUNet, varying the number of channels in the input layer to explore its impact on performance.For text feature extraction, we employ CLIP [40] text encoder.To align the extracted features from the 3D scene encoder and the text encoder, we incorporate a visionlanguage adapter.The only supervision comes from the point-discriminative contrastive loss proposed by RegionPLC.During training, we employ the AdamW optimizer to update model parameters.We train the model from scratch for 500 epochs, utilizing a learning rate of 1 ˆ10 ´3.Additionally, we incorporate a warm-up period of 200 steps and a cosine annealing scheme for learning rate scheduling, with a minimum learning rate ratio of 1 ˆ10 ´5.</p>
<p>D Additional Results</p>
<p>D.1 Semantic Segmentation</p>
<p>Setting To test if the scaling effect of SceneVerse is universally beneficial for 3D understanding tasks, we use 3D semantic segmentation as a signature task to illustrate the effectiveness of SceneVerse.Notably, a recent work that introduced the Swin3D model [99] has identified the importance of pre-training for 3D semantic segmentation [99].Following the same setting, we test if the proposed Swin3D model could be further improved by substituting the pre-training data to SceneVerse.Specifically, we test models' performance on the ScanNet semantic segmentation task with 20 semantic categories and report the mean IoU and mean Acc on the validation set of ScanNet.As the original implementation of Swin3D pre-training requires surface normals as additional inputs, we reimplement the model and pre-train all models with only point coordinates and colors.Comparing our pre-training set to Structured 3D, we also observe consistent model performance improvement, showcasing the benefit of scaling-effect in SceneVerse.Moreover, we fine-tune the model on ScanNet after pre-training on SceneVerse.This process further brings improvement in model performance on semantic segmentation.We believe these results serve as strong pieces of evidence validating the effectiveness of data scaling in SceneVerse and also its potential benefit for all 3D tasks in addition to 3D visual grounding.</p>
<p>D.2 Qualitative Results</p>
<p>We provide the qualitative results of 3D vision-language grounding in Fig.This is an off-white and black monitor.It is on the right closely to an all white monitor that is similar in size.
A
Fig. 2 :
2
Fig. 2: SceneVerse collection and statistics.Given a 3D scene (a), our automated pipeline (c) generates three types of description including scene caption, object caption and object referral.(b) SceneVerse data comparison and composition.</p>
<p>y S o r k k p R I m X D y Q J 7 I M 3 m x H q 1 n 6 9 X q j 1 p n r P H M F v k D 6 + 0 H M w e d R A = = &lt; / l a t e x i t &gt;LrefMax-Pool&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B H 4 S b m o k E G N F U F F 9 e d h R / H W w Y wU = " &gt; A A A B 9 X i c b V D L S g N B E O y N r x h f U Y + K D A b B g 4 R d E f U Y 9 O I x Q f P A J I b Z y S Q Z M j O 7 z M w q Y c n R P / A a T + L V a 7 5 E 8 B v 8 C S e P g y Y W N B R V 3 X R 3 + S F n 2 r j u l 5 N Y W F x a X k m u p t b W N z a 3 0 t s 7 J R 1 E i t A i C X i g K j 7 W l D N J i 4 Y Z T i u h o l j 4 n J b 9 7 v X I L z 9 S p V k g 7 0 w v p H W B 2 5 K 1 G M H G S v c 1 X 8 T t / k N 8 2 2 + k M 2 7 W H Q P N E 2 9 K M r n 9 Y e H 7 + W C Y b 6 Q / a 8 2 A R I J K Q z j W u u q 5 o a n H W B l G O O 2 n a p G m I S Z d 3 K Z V S y U W V N f j 8 c V 9 d G S V J m o F y p Y 0 a K z + n o i x 0 L o n f N s p s O n o W W 8 k n i B f / O d X I 9 O 6 r M d M h p G h k k y W t S K O T I B G E a A m U 5 Q Y 3 r M E E 8 X s v Y h 0 s M L E 2 K B S N g h v9 u 1 5 U j r N e u f Z s 4 K X y V 3 B B E n Y g 0 M 4 B g 8 u I A c 3 k I c i E J D w A g N 4 d Z 6 c g f P m v E 9 a E 8 5 0 Z h f + w P n 4 A V 7 P l f U = &lt; / l a t e x i t &gt; g S &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u i s Q Q y h n M 8 0 V 6 N j R G n z s S 8 Q L x h 8 = " &gt; A A A C B 3 i c b V A 9 S w N B E N 3 z M 8 a v q G C h z W I Q L C T c i a h l 0 M b C I g H z A U k I e 5 t J X N z b O 3 b n x H A c 2 P p H 7 E Q r s R X 8 D Y K N v 8 X N R 6 H R B w O P 9 2 a Y m e d H U h h 0 3 U 9 n a n p m d m 4 + s 5 B d X F p e W c 2 t r V d N G G s O F R 7 K U N d 9 Z k A K B R U U K K E e a W C B L 6 H m X 5 8 N / N o N a C N C d Y n 9 C F o B 6 y n R F Z y h l d q 5 r W b A 8 I o z m V y k 7 a S J c I u J 4 a A g T d u 5 v F t w h 6 B / i T c m + e J m + U s 8 n r 6 X 2 r m P Z i f k c Q A K u W T G N D w 3 w l b C N A o u I c 0 2 Y w M R 4 9 e s B w 1 L F Q v A t J L h D y n d t U q H d k N t S y E d q j 8 n E h Y Y 0 w 9 8 2 z m 4 2 E x 6 A 3 G f + s F / f i P G 7 k k r E S q K E R Q f L e v G k m J I B 6 H Q j t D A U f Y t Y V w L e y / l V 0 w z j j a 6 r A 3 C m 3 z 7 L 6 k e F L y j w m H Z y x d P y Q g Z s k 1 2 y B 7 x y D E p k n N S I h X C y R 1 5 I E / k 2 b l 3 n p w X 5 3 X U O u W M Z z b I L z h v 3 + w o n Y E = &lt; / l a t e x i t &gt; Lscene &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r e + M d + z J w h + r f n N V m D 0 V K U d 6 e 6 Y = " &gt; A A A C B X i c b V C 7 S g N B F J 2 N r 5 j 4 i N o I N o N R s J C w K 6 K W Q R s L i w j m A c k S Z i e z y Z j Z B z N 3 g 2 F Z W 3 / E N l a S 1 s J / E P w Q r Z 0 8 C k 0 8 c O F w z r 3 c e 4 8 T C q 7</p>
<p>4 B a X j I L o a 0 K o 5 P p e T D t E E g o 6 u I w O w p p 9 e 5 5 U T g r W W e H 0 1 s o</p>
<p>8 a T M T J e j b d p a 8 q Y z e y j P z D e f w D G 5 5 x F &lt; / l a t e x i t &gt; LMLM Language Encoder !&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p d 5 M D 3 Q T x g S z w 6 p 7 8 A r E</p>
<p>H 2 7 o S t / 6 L 4 M p P 8 B e c P h b a e u D C 4 Z x 7 u f e e I O F M a c f 5 t F Z W 1 9 Y 3 N n N b + c L 2 z u 5 e s b R f</p>
<p>6 H 2 7 o S t / 6 L 4 M p P 8 B e c P h b a e u D C 4 Z x 7 u f e e I O F M a c f 5 t F Z W 1 9 Y 3 N n N b + c L 2 z u 5 e s b R f</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p d 5 M D 3 Q T x g S z w 6 p 7 8 A r E + r U O w X w = " &gt; A A A B 9 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B g 5 R d K e q x 6 M V j h X 5 h u 5 Z s m r a h S X Z J s o W y 7 L / w q F 7 E q / / G g / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 I O J M G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3v 5 B 8 f C o q c N Y E d o g I Q 9 V O 8 C a c i Z p w z D D a T t S F I u A 0 1 Y w v p v 5 r Q l V m o W y b q Y R 9 Q U e S jZ g B B s r P X Y D k Y z S p 6 S e 9 o o l t + z O g V a J l 5 E S Z K j 1 i l / d f k h i Q a U h H G v d 8 d z I + A l W h h F O 0 0 I 3 1 j T C Z I y H t G O p x I J q P 5 l f n K I z q / T R I F S 2 p E F z 9 f d E g o X W U x F c B M I 2 C 2 x G e t m e i f 9 5 n d g M b v y E y S g 2 V J L F r k H M k Q n R L A H U Z 4 o S w 6 e W Y K K Y P R e R E V a Y G J t T w e b g L X + 9 S p q X Z e + q X H m o l K q 3 W S J 5 O I F T O A c P r q E K 9 1 C D B h C Q 8 A y v 8 O Z M n B f n 3 f l Y t O a c b O Y Y / s D 5 / A H j 7 p I p &lt; / l a t e x i t &gt; h T Object PCD Encoder Language Encoder ❄ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D G R 8 3 9 j + H B g L 6 q s</p>
<p>Fig. 3 :
3
Fig. 3: Overview of GPS model.We use contrastive alignment at three levels L obj , Lscene, and L ref and a masked language modeling objective LMLM for model learning.</p>
<p>ÿ</p>
<p>pp,qq ˜log exp <code>Dobj pp, qq řr exp pD obj pp, rqq</code>log exp `Dobj pp, qq řr exp pD obj pr, qqq ¸,</p>
<p>Fig. 4 :
4
Fig.4: Model performance v.s.data scale.Plots show that models consistently improve in both the pre-train and zero-shot transfer settings on ScanRefer and SceneVerse-val with data scaling-up.</p>
<p>Fig. A. 2 :
2
Fig. A.2: Examples of object captioning.We color the target object in bold.</p>
<p>Fig. A. 3 :
3
Fig. A.3: Examples of scene captioning.</p>
<p>Fig. A. 4 :
4
Fig. A.4: Examples of object referral.Note that the green bounding box indicates the target object and yellow bounding box indicates the anchor object(s).</p>
<p>2 .Algorithm 2 : 2 : 3 :,v 4 :
22234
More examples of the scene-language pairs in SceneVerse are shown in Fig. A.2, Fig. A.3 and Fig. A.4. Object Captioning Pipeline Input : M object point clouds tP 1 , P 2 , . . ., P m u; N multiview images tI 1 , I 2 , . . ., I n u Output : Captions for each object in the scene tC 1 , C 2 , . . ., C m u 1: for o " 1, 2, . . ., M do for v " 1, 2, . . ., N do Project P o on I v to get visible points P vis oCrop I v with the bounding box of P vis o,v to get I crop o,v 5:</p>
<p>7 : 9 :,v 10 :
7910
Calculate the occlusion score s occ o,v " Select the top-10 tC o,v u with highest s clip o,v ˚socc oSummary selected tC o,v u with GPT-3.5 to get C o 11: end for B Model Details B.1 Spatial-Attention Transformer</p>
<p>.5 and the results of open-vocabulary semantic segmentation in Fig. A.6.This is a toilet.The toilet is situated between the bathtub and the sink.This is the tall brown cabinet next to the blue plaid curtains.It is the wardrobe next to the window.This is a table with wooden sides and a green top.It is behind 2 pairs of shoes, right in front of a wall, and to the left of the desk.It is a dark colored two seater futon located by the door.It is located underneath a whiteboard.The cabinet is next to the couch.It is located on the right side of the couch, against the wall.</p>
<p>Fig. A. 5 :
5
Fig. A.5: Qualitative results of GPS on 3D visual-language grounding.We visualize the incorrect predictions in red and the correct predictions or ground truths in green.</p>
<p>Fig. A. 6 :
6
Fig. A.6: Qualitative results of open-vocabulary 3D semantic segmentation (OV-Seg).We label the object class in ScanNet-20's vocabulary in blue, and unseen class in ScanNet-20 in green.</p>
<p>Table 1 :
1
Comparison of SceneVerse with existing 3DVL Datasets.Scen-eVerse expands the data scale of prior work by order of magnitude."VG" stands for Visual Grounding, "QA" for Question Answering, "PT" for Pre-training and "MT" for Multi-tasking."Anno."denotes language from human annotations and "Syn." for template-based or LLM generated descriptions.
Dataset3D Scene Obj.TaskObj. Caption Caption Referral Check Anno. Syn. Anno. Syn. Scene Obj. Quality New Existing TotalScanRefer [16]</p>
<p>Table 2 :
2
3D visual grounding results on Nr3D, Sr3D, and ScanRefer.We use "pre-train" for our model trained on SceneVerse w/o additional fine-tuning, and "fine-tune" for its data-specific fine-tuned version.Best results are highlighted in bold.
Nr3DSr3DScanRefer Acc@0.5MethodOverall Easy Hard V-Dep. V-Indep. Overall Easy Hard V-Dep. V-Indep. Overall Unique Multiple3DVG-Trans [108]40.8 48.5 34.8 34.843.751.4 54.2 44.9 44.651.734.760.628.4TGNN [44]37.3 44.2 30.6 35.838.045.0 48.5 36.9 45.845.029.756.823.2TransRefer3D [39]48.0 56.7 39.6 42.550.757.4 60.5 50.2 49.957.7---InstanceRefer [103]38.8 46.0 31.8 34.541.948.0 51.1 40.5 45.848.132.966.824.7FFL-3DOG [33]41.7 48.2 35.0 37.144.7-----34.067.925.7LAR [6]48.9 58.4 42.3 47.452.159.4 63.0 51.2 50.059.1---SAT [100]56.5 64.9 48.4 54.457.657.9 61.2 50.0 49.258.330.150.825.23D-SPS [64]51.5 58.1 45.1 48.053.262.6 56.2 65.4 49.263.237.066.729.83DJCG [12]----------37.364.330.8BUTD-DETR [47]54.6 60.7 48.4 46.058.067.0 68.6 63.2 53.067.639.866.335.1MVT [45]59.5 67.4 52.7 59.160.364.5 66.9 58.8 58.464.733.366.525.3ViL3DRel [18]64.4 70.2 57.4 62.064.572.8 74.9 67.9 63.873.237.768.630.7EDA [95]52.1 58.2 46.1 50.253.168.1 70.3 62.9 54.168.742.368.637.63D-VisTA (scratch) [112] 57.5 65.9 49.4 53.759.469.6 72.1 63.6 57.970.141.570.934.83D-VisTA [112]64.2 72.1 56.7 61.565.176.4 78.8 71.3 58.977.345.875.139.1Ours (scratch)58.7 67.0 50.9 55.859.868.4 70.5 63.4 53.169.040.471.334.7Ours (pre-train)55.2 62.8 48.0 45.558.874.1 76.4 68.5 54.175.047.177.441.6Ours (fine-tuned )64.9 72.5 57.8 56.967.977.5 80.1 71.6 62.878.248.1 77.942.7</p>
<p>Table 3 :
3
Zero-shot transfer on existing benchmarks."SR" stands for ScanRefer.
MethodNr3D Sr3D SR@0.25 SR@0.53D-VisTA (scratch)57.5 69.645.941.53D-VisTA (zero-shot)35.2 31.233.229.63D-VisTA (zero-shot text) 43.1 36.141.136.4Ours (scratch)58.7 68.444.540.4Ours (zero-shot)32.4 33.335.231.1Ours (zero-shot text)41.9 38.140.735.8</p>
<p>Table 4 :
4
Zero-shot transfer on Scen-eVerse-val.Evaluation uses GT object proposals following Nr3D/Sr3D.To better evaluate the effectiveness of both the SceneVerse data and the GPS model, we further perform zero-shot transfer experiments to test the models' capability in 4 benchmarks, ScanRefer, Sr3D, Nr3D, and SceneVerseval.We create SceneVerse-val using 8.5K annotated object referrals of 271 scenes in MultiScan, and randomly split the scenes following a 4:1 train / test split for creating the held-out test set.We mainly consider 2 specific transfer settings in
MethodOverall Easy Hard V-Dep. V-Indep.3D-VisTA (scratch)40.7 53.1 21.6 37.344.33D-VisTA (zero-shot)52.9 59.6 35.4 53.752.23D-VisTA (zero-shot text) 58.1 70.0 39.6 52.564.1Ours (scratch)38.5 50.2 20.8 33.743.9Ours (zero-shot)59.2 69.4 44.0 53.166.3Ours (zero-shot text)60.6 70.9 45.1 54.867.3
our experiments: (i) zero-shot: models trained by removing all the scenes from the target dataset, tested on held-out unseen scenes, and (ii) zero-shot text: Models trained on data that include the training set of scenes from the target dataset, yet tested exclusively with unseen scene-text distribution.Specifically, for the zero-shot text setting, we use the generated texts in SceneVerse as fine-tuning sources for the zero-shot model.We mainly compare our model against a recent pre-training-based model 3D-VisTA.See more details on experimental setting and implementation in the supplementary.Results and AnalysesWe present the results of zero-shot transfer experiments in Tab. 3 and Tab. 4 with the following key observations: ‚ Our GPS model demonstrates superior generalization to unseen scenes compared to the 3D-VisTA model.In zero-shot transfer scenarios, our model consistently outperforms 3D-VisTA across established benchmarks and Scen-eVerse-val.This indicates the effectiveness of contrastive</p>
<p>Table 5 :
5
3D question answering results on ScanQA and SQA3D.We report EM@1 score on ScanQA and SQA3D evaluation sets.
ScanQAModelval w/obj w/o objSQA3DScanRefer+MCAN [5] 18.6 20.619.0-ScanQA [5]20.3 23.520.946.6SQA3D [67]---47.23D-VisTA [112]22.4 27.0 23.048.53D-LLM [42]20.5 19.1--Ours22.7 25.0 23.549.9</p>
<p>Table 6
6: Exisiting 3D backbones pre-trained on SceneVerse for open-vocabulary 3D semantic segmentation onScanNet. "SPUNet" denotes SparseUNet pro-posed in [97].ModelNetwork mIoU ∆ mAcc ∆OpenScene [76]SPUNet16 57.2-69.9-PLA [31]SPUNet16 17.7-33.5-RegionPLC [97]SPUNet16 56.9-75.6-RegionPLC+SceneVerse SPUNet16 58.2 +1.7% 77.3 +2.2%OpenScene [76]SPUNet32 57.8-70.3-PLA [31]SPUNet32 19.1-41.5-RegionPLC [97]SPUNet32 59.6-77.5-RegionPLC+SceneVerse SPUNet32 61.0 +2.3% 79.7 +2.8%</p>
<p>Table 7 :
7
Ablation on text data source used in model pre-training.
All models are tested on ScanRefer withno additional finetuning.Template LLM Anno. Acc@0.25 Acc@0.5✗✗✗43.538.4✓✗✗50.946.1✓✓✗51.146.3✓✓✓52.047.1</p>
<p>Table 8 :
8
Cross domain transfer resultsof models pre-trained on real and synthetic datasets."S3D" stands for Structured3D.
Table 9: Ablation on model designon SceneVerse-val. We use "Obj-lvl", "Scene-lvl" to denote object andscene alignment loss, and "MLM" for themask language modeling loss.Obj-lvl. MLM Scene-lvl. Overall Easy HardReal Synthetic SceneVerse-val S3D ProcTHOR✗✗✗64.8 75.4 48.7All✗64.837.143.4✓✗✗65.2 77.1 47.4✗S3D7.085.116.1✓✓✗62.4 73.4 45.8✗ ProcTHOR4.216.391.0✓✓✓66.9 77.8 50.3</p>
<p>Table A .
A
1: Relationships in SceneVerse.The 3D scene graph captures 21 types of relationships ranging in 4 categories.Relationships Our 3D scene graph captures 21 types of relations as shown in Tab.A.1.We provide illustrations of how these relations are defined in the 3D space, as can be seen in Fig. A.1.
CategoryRelationIn-contact verticalsupported by placed inembedded into insidehanging onaffixed onNon-contact verticalmounted on higher thanabove belowlower thannear(far) to the left of near(far) to the right ofHorizontalis behind close tois in front of adjacent tobesidesnext toMulti-objectbetweenaligned
Scene Graph Construction Due to the inherent noise and incompleteness in the point cloud representation, automatically extracting precise and comprehensive relationships from the point clouds is a non-trivial task.Below we detail our 3D scene graph construction process, as outlined in Algorithm 1.</p>
<p>Table A .
A
2: Prompts used in SceneVerse.Summarize caption below.The summary should be a description of the target-object.Focus on the target-object's attribute, like color, shape and material, etc. Identify and correct the potential errors.caption:Abed in a hotel room.A white comforter on a bed.A bed with a striped comforter... target-object: Bed Object referral Rewrite the following caption using one random sentence structure.You should give me only one rewritten sentence without explanation.caption:Thebed is between desk and nightstand.Rewrite the following caption.You should give me only one rewritten sentence about target-object without explanation.Make sure target-object is the subject of the sentence, not anchor-object(s).If the sentence is in full inversion, keep the inversion.caption: The armchair is next to the sofa.target-object: Armchair anchor-object(s): Sofa Rewrite the following caption using one random sentence structure.You need to focus on the location and relations of the target-object that appears in the sentence.If multiple target-object appear in the sentence, you need to focus on the first target-object that appears.You can also add the target-object's function and comfort level based on the sentence, e.g., how the objects can be used by humans and human activities in the scene.You should give me only one rewritten sentence without explanation.caption: Far from the bowl and peppershaker, the vase is to the left, it is also on the top of countertop.
Description type PromptObject caption
target-object: Vase</p>
<p>Table A .
A
3: Semantic segmentation results on ScanNet validation set.: denotes model trained with surface normals as an additional input.S3D indicates models initialized with the original Swin3D model weights pre-trained on Structured3D provided by Yang et al .[99].
MethodsInit. SceneVerse Pre. mIoU mAccSwin3D n -S:✗✗75.2-Swin3D n -S:S3D✗75.6-Swin3D-S✗✗63.2 72.8Swin3D-SS3D✗64.1 75.1Swin3D-S (pre-train) ✗✓67.7 78.0Swin3D-S (pre-train) S3D✓69.5 80.1Swin3D-S (fine-tuned ) S3D✓70.6 80.2
Comparison As shown in Tab.A.3, we observe a significant model performance improvement ("6%) by training Swin3D-S model on our SceneVerse dataset.</p>
<p>Acknowledgement The authors would like to thank Yaowei Zhang (BIGAI) for his help on online visualization and other colleagues from BIGAI General Vision Lab for fruitful discussions.The authors would also like to thank the anonymous reviewers for their constructive feedback.SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene UnderstandingSupplementary MaterialA The SceneVerse DatasetA.1 3D ScenesTo address the scarcity of available 3D scene data, we construct SceneVerse by unifying 3D scene data from various existing datasets.The curation involves utilizing real-world scene datasets such as ScanNet[25], ARKitScenes[9], HM3D[79], 3RScan[88]and MultiScan[68],in conjunction with synthetic environments from Structured3D[109]and ProcTHOR[29].The incorporation of these synthetic datasets is primarily driven by their potential as scalable data sources for 3D-VL alignment.To facilitate the training process, we conduct the following preprocessing steps.Room Segmentation The 3D scenes in HM3D and ProcTHOR are released at the building level, encompassing multiple rooms and sometimes spanning over 50 meters.To align with existing benchmarks[1,16], we leverage the associated metadata to segment the 3D point cloud at the room level, facilitating subsequent operations in scene graph construction and language description generation.Additionally, we implement a filtering process to exclude extremely large rooms and those with fewer than 4 objects in the scene.Point Cloud Normalization To mitigate the data disparities arising from diverse capture devices across various data sources, we subsample each point cloud to a maximum of 240, 000 points.Each point cloud then undergoes a transformation centered on the central point on the floor, followed by rotation to align the room layout with the axis following the approach by Chen et al .[18].Semantic Label Alignment Given the divergence in semantic label sets across different datasets, we undertake a comprehensive effort to map all the object class labels to the 607 semantic labels in ScanNet[25]to facilitate close-vocabulary object classification[77]in the existing model framework[112].We construct the mapping in each dataset through LLM and manual verification.Note that the object-level grounding in GPS can directly deal with open-set object labels or captions, similar to CLIP[40].After the preprocessing, each scan is represented by a point cloud P P R N ˆ8, wherein each point is defined by its 3D coordinates, RGB color, instance id and semantic label.In total, we curate 68, 406 3D scenes in SceneVerse.A.2 3D Scene Graph ConstructionIn Sec.3.3, we introduce an automated pipeline to construct 3D scene graphs from point clouds.Here, we provide more implementation details and the relationship definition.
Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. P Achlioptas, A Abdelreheem, F Xia, M Elhoseiny, L Guibas, Proceedings of European Conference on Computer Vision (ECCV). European Conference on Computer Vision (ECCV)2020</p>
<p>Taskography: Evaluating robot task planning over large 3d scene graphs. C Agia, K M Jatavallabhula, M Khodeir, O Miksik, V Vineet, M Mukadam, L Paull, F Shkurti, Proceedings of Conference on Robot Learning (CoRL). Conference on Robot Learning (CoRL)2022</p>
<p>Flamingo: a visual language model for few-shot learning. J B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>scene graph: A structure for unified semantics, 3d space, and camera. I Armeni, Z Y He, J Gwak, A R Zamir, M Fischer, J Malik, S Savarese, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)20193</p>
<p>Scanqa: 3d question answering for spatial scene understanding. D Azuma, T Miyanishi, S Kurita, M Kawanabe, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Look around and refer: 2d synthetic semantics knowledge distillation for 3d visual grounding. E Bakr, Y Alsaedy, M Elhoseiny, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>Perceptual symbol systems. L W Barsalou, Behavioral and brain sciences. 2241999</p>
<p>Grounded cognition. L W Barsalou, Annu. Rev. Psychol. 592008</p>
<p>Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. G Baruch, Z Chen, A Dehghan, T Dimry, Y Feigin, P Fu, T Gebauer, B Joffe, D Kurz, A Schwartz, Proceedings of Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track. Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track2021</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. 2021arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2020</p>
<p>3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. D Cai, L Zhao, J Zhang, L Sheng, D Xu, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Matterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, Proceedings of International Conference on 3D Vision. International Conference on 3D Vision2017</p>
<p>A X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:1512.03012Shapenet: An information-rich 3d model repository. 2015arXiv preprint</p>
<p>Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts. S Changpinyo, P Sharma, N Ding, R Soricut, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2021</p>
<p>Scanrefer: 3d object localization in rgb-d scans using natural language. D Z Chen, A X Chang, M Nießner, Proceedings of European Conference on Computer Vision (ECCV). European Conference on Computer Vision (ECCV)2020</p>
<p>D3net: a speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans. D Z Chen, Q Wu, M Nießner, A X Chang, Proceedings of European Conference on Computer Vision (ECCV. European Conference on Computer Vision (ECCV2022</p>
<p>Language conditioned spatial relation reasoning for 3d object grounding. S Chen, P L Guhur, M Tapaswi, C Schmid, I Laptev, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>End-to-end 3d dense captioning with vote2cap-detr. S Chen, H Zhu, X Chen, Y Lei, G Yu, T Chen, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Holistic++ scene understanding: Single-view 3d holistic scene parsing and human pose estimation with human-object interaction and physical commonsense. Y Chen, S Huang, T Yuan, S Qi, Y Zhu, S C Zhu, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)2019</p>
<p>Yourefit: Embodied reference understanding with language and gesture. Y Chen, Q Li, D Kong, Y L Kei, S C Zhu, T Gao, Y Zhu, S Huang, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2021</p>
<p>Scan2cap: Context-aware dense captioning in rgb-d scans. Z Chen, A Gholami, M Nießner, A X Chang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2021</p>
<p>Unit3d: A unified transformer for 3d dense captioning and visual grounding. Z Chen, R Hu, X Chen, M Nießner, A X Chang, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2023</p>
<p>Abo: Dataset and benchmarks for real-world 3d object understanding. J Collins, S Goel, K Deng, A Luthra, L Xu, E Gundogdu, X Zhang, T F Y Vicente, T Dideriksen, H Arora, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nießner, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR. Conference on Computer Vision and Pattern Recognition (CVPR2017</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. W Dai, J Li, D Li, A M H Tiong, J Zhao, W Wang, B Li, P Fung, S Hoi, arXiv:2305.065002023arXiv preprint</p>
<p>Objaverse-xl: A universe of 10m+ 3d objects. M Deitke, R Liu, M Wallingford, H Ngo, O Michel, A Kusupati, A Fan, C Laforte, V Voleti, S Y Gadre, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>Objaverse: A universe of annotated 3d objects. M Deitke, D Schwenk, J Salvador, L Weihs, O Michel, E Vanderbilt, L Schmidt, K Ehsani, A Kembhavi, A Farhadi, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Procthor: Large-scale embodied ai using procedural generation. M Deitke, E Vanderbilt, A Herrasti, L Weihs, K Ehsani, J Salvador, W Han, E Kolve, A Kembhavi, R Mottaghi, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, Proceedings of Conference of the North American Chapter. Conference of the North American Chapterthe Association for Computational Linguistics2018</p>
<p>Pla: Language-driven openvocabulary 3d scene understanding. R Ding, J Yang, C Xue, W Zhang, S Bai, X Qi, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Votenet: A deep learning label fusion method for multi-atlas segmentation. Z Ding, X Han, M Niethammer, Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)2019</p>
<p>Free-form description guided 3d visual graph network for object grounding in point cloud. M Feng, Z Li, Q Li, L Zhang, X Zhang, G Zhu, H Zhang, Y Wang, A Mian, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2021</p>
<p>Scaling open-vocabulary image segmentation with image-level labels. G Ghiasi, X Gu, Y Cui, T Y Lin, Proceedings of European Conference on Computer Vision (ECCV. European Conference on Computer Vision (ECCV2022</p>
<p>Arnold: A benchmark for languagegrounded task learning with continuous states in realistic 3d scenes. R Gong, J Huang, Y Zhao, H Geng, X Gao, Q Wu, W Ai, Z Zhou, D Terzopoulos, S C Zhu, B Jia, S Huang, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2023</p>
<p>3d semantic segmentation with submanifold sparse convolutional networks. B Graham, M Engelcke, L Van Der Maaten, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>Q Gu, A Kuwajerwala, S Morin, K M Jatavallabhula, B Sen, A Agarwal, C Rivera, W Paul, K Ellis, R Chellappa, arXiv:2309.16650Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. 2023arXiv preprint</p>
<p>Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models. H Ha, S Song, Proceedings of Conference on Robot Learning (CoRL). Conference on Robot Learning (CoRL)2022</p>
<p>Transrefer3d: Entity-and-relation aware transformer for fine-grained 3d visual grounding. D He, Y Zhao, J Luo, T Hui, S Huang, A Zhang, S Liu, Proceedings of ACM International Conference on Multimedia (MM). ACM International Conference on Multimedia (MM)2021</p>
<p>Clip goes 3d: Leveraging prompt tuning for language grounded 3d recognition. D Hegde, J M J Valanarasu, V Patel, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2023</p>
<p>Vln bert: A recurrent vision-and-language bert for navigation. Y Hong, Q Wu, Y Qi, C Rodriguez-Opazo, S Gould, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2021</p>
<p>3d concept learning and reasoning from multi-view images. Y Hong, C Lin, Y Du, Z Chen, J B Tenenbaum, C Gan, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>J Huang, S Yong, X Ma, X Linghu, P Li, Y Wang, Q Li, S C Zhu, B Jia, S Huang, arXiv:2311.12871An embodied generalist agent in 3d world. 2023arXiv preprint</p>
<p>Text-guided graph neural networks for referring 3d instance segmentation. P H Huang, H H Lee, H T Chen, T L Liu, Proceedings of AAAI Conference on Artificial Intelligence (AAAI). AAAI Conference on Artificial Intelligence (AAAI)2021</p>
<p>Multi-view transformer for 3d visual grounding. S Huang, Y Chen, J Jia, L Wang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Diffusionbased generation, optimization, and planning in 3d scenes. S Huang, Z Wang, P Li, B Jia, T Liu, Y Zhu, W Liang, S C Zhu, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Bottom up top down detection transformers for language grounding in images and point clouds. A Jain, N Gkanatsios, I Mediratta, K Fragkiadaki, Proceedings of European Conference on Computer Vision (ECCV. European Conference on Computer Vision (ECCV2022</p>
<p>Pointgroup: Dual-set point grouping for 3d instance segmentation. L Jiang, H Zhao, S Shi, S Liu, C W Fu, J Jia, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Full-body articulated human-object interaction. N Jiang, T Liu, Z Cao, J Cui, Z Zhang, Y Chen, H Wang, Y Zhu, S Huang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Scaling up dynamic human-scene interaction modeling. N Jiang, Z Zhang, H Li, X Ma, Z Wang, Y Chen, T Liu, Y Zhu, S Huang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. M Khanna, Y Mao, H Jiang, S Haresh, B Shacklett, D Batra, A Clegg, E Undersander, A X Chang, M Savva, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W Y Lo, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2023</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L J Li, D A Shamma, International Journal of Computer Vision. 2017IJCV</p>
<p>Building machines that learn and think like people. B M Lake, T D Ullman, J B Tenenbaum, S J Gershman, Behavioral and brain sciences. 40e2532017</p>
<p>Language-driven semantic segmentation. B Li, K Q Weinberger, S Belongie, V Koltun, R Ranftl, Proceedings of International Conference on Learning Representations (ICLR. International Conference on Learning Representations (ICLR2022</p>
<p>Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. C Li, R Zhang, J Wong, C Gokmen, S Srivastava, R Martín-Martín, C Wang, G Levine, M Lingelbach, J Sun, Proceedings of Conference on Robot Learning (CoRL). Conference on Robot Learning (CoRL)2023</p>
<p>BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, Proceedings of International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)2023</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S Hoi, Proceedings of International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)2022</p>
<p>Grounded language-image pre-training. L H Li, P Zhang, H Zhang, J Yang, C Li, Y Zhong, L Wang, L Yuan, L Zhang, J N Hwang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>M Liu, R Shi, K Kuang, Y Zhu, X Li, S Han, H Cai, F Porikli, H Su, arXiv:2305.10764Openshape: Scaling up 3d shape representation towards open-world understanding. 2023arXiv preprint</p>
<p>Zero-1-to-3: Zero-shot one image to 3d object. R Liu, R Wu, B Van Hoorick, P Tokmakov, S Zakharov, C Vondrick, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2023</p>
<p>3d-sps: Singlestage 3d visual grounding via referred point progressive selection. J Luo, J Fu, X Kong, C Gao, H Ren, H Shen, H Xia, S Liu, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Scalable 3d captioning with pretrained models. T Luo, C Rockwell, H Lee, J Johnson, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>Selfmonitoring navigation agent via auxiliary progress estimation. C Y Ma, J Lu, Z Wu, G Alregib, Z Kira, R Socher, C Xiong, Proceedings of International Conference on Learning Representations (ICLR. International Conference on Learning Representations (ICLR2019</p>
<p>Sqa3d: Situated question answering in 3d scenes. X Ma, S Yong, Z Zheng, Q Li, Y Liang, S C Zhu, S Huang, Proceedings of International Conference on Learning Representations (ICLR. International Conference on Learning Representations (ICLR2023</p>
<p>Multiscan: Scalable rgbd scanning for 3d environments with articulated objects. Y Mao, Y Zhang, H Jiang, A Chang, M Savva, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>An end-to-end transformer model for 3d object detection. I Misra, R Girdhar, A Joulin, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2021</p>
<p>Orbit: A unified simulation framework for interactive robot learning environments. M Mittal, C Yu, Q Yu, J Liu, N Rudin, D Hoeller, J L Yuan, R Singh, Y Guo, H Mazhar, 2023Robotics and Automation Letters (RA-L</p>
<p>Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. K Mo, S Zhu, A X Chang, L Yi, S Tripathi, L J Guibas, H Su, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. T Mu, Z Ling, F Xiang, D Yang, X Li, S Tao, Z Huang, Z Jia, H Su, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2021</p>
<p>. OpenAI: Introducing chatgpt. 2022</p>
<p>arXiv:2303.08774OpenAI: Gpt-4 technical report. 2023arXiv preprint</p>
<p>Episodic transformer for vision-and-language navigation. A Pashevich, C Schmid, C Sun, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)2021</p>
<p>Openscene: 3d scene understanding with open vocabularies. S Peng, K Genova, C Jiang, A Tagliasacchi, M Pollefeys, T Funkhouser, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Pointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, Proceedings of Advances in Neural Information Processing Systems (NeurIPS. Advances in Neural Information Processing Systems (NeurIPS2017</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, Proceedings of International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)2021</p>
<p>S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X Chang, Proceedings of Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track. Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track2021Habitatmatterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. K Rana, J Haviland, S Garg, J Abou-Chakra, I Reid, N Suenderhauf, Proceedings of Conference on Robot Learning (CoRL). Conference on Robot Learning (CoRL)2023</p>
<p>Kimera: From slam to spatial perception with 3d dynamic scene graphs. A Rosinol, A Violette, M Abate, N Hughes, Y Chang, J Shi, A Gupta, L Carlone, International Journal of Robotics Research (IJRR). 2021</p>
<p>Laion-5b: An open large-scale dataset for training next generation image-text models. C Schuhmann, R Beaumont, R Vencu, C Gordon, R Wightman, M Cherti, T Coombes, A Katta, C Mullis, M Wortsman, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>Mask3d: Mask transformer for 3d semantic instance segmentation. J Schult, F Engelmann, A Hermans, O Litany, S Tang, B Leibe, Proceedings of International Conference on Robotics and Automation (ICRA). International Conference on Robotics and Automation (ICRA)2023</p>
<p>The development of embodied cognition: Six lessons from babies. L Smith, M Gasser, Artificial life. 111-22005</p>
<p>Openmask3d: Open-vocabulary 3d instance segmentation. A Takmaz, E Fedele, R W Sumner, M Pollefeys, F Tombari, F Engelmann, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Softgroup for 3d instance segmentation on point clouds. T Vu, K Kim, T M Luu, T Nguyen, C D Yoo, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Rio: 3d object instance re-localization in changing indoor environments. J Wald, A Avetisyan, N Navab, F Tombari, M Nießner, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)2019</p>
<p>Learning 3d semantic scene graphs from 3d indoor reconstructions. J Wald, H Dhamo, N Navab, F Tombari, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai. T Wang, X Mao, C Zhu, R Xu, R Lyu, P Li, X Chen, W Zhang, K Chen, T Xue, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. X Wang, Q Huang, A Celikyilmaz, J Gao, D Shen, Y F Wang, W Y Wang, L Zhang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>Move as you say interact as you can: Language-guided human motion generation with scene affordance. Z Wang, Y Chen, B Jia, P Li, J Zhang, J Zhang, T Liu, Y Zhu, W Liang, S Huang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Humanise: Languageconditioned human motion generation in 3d scenes. Z Wang, Y Chen, T Liu, Y Zhu, W Liang, S Huang, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. T Wu, J Zhang, X Fu, Y Wang, J Ren, L Pan, W Wu, L Yang, J Wang, C Qian, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Eda: Explicit text-decoupling and dense alignment for 3d visual grounding. Y Wu, X Cheng, R Zhang, Z Cheng, J Zhang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding. L Xue, M Gao, C Xing, R Martín-Martín, J Wu, C Xiong, R Xu, J C Niebles, S Savarese, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding. J Yang, R Ding, Z Wang, X Qi, arXiv:2304.009622023arXiv preprint</p>
<p>Physcene: Physically interactable 3d scene synthesis for embodied ai. Y Yang, B Jia, P Zhi, S Huang, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Y Q Yang, Y X Guo, J Y Xiong, Y Liu, H Pan, P S Wang, X Tong, B Guo, arXiv:2304.06906Swin3d: A pretrained transformer backbone for 3d indoor scene understanding. 2023arXiv preprint</p>
<p>Sat: 2d semantics assisted training for 3d visual grounding. Z Yang, S Zhang, L Wang, J Luo, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2021</p>
<p>Scannet++: A high-fidelity dataset of 3d indoor scenes. C Yeshwanth, Y C Liu, M Nießner, A Dai, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2023</p>
<p>X-trans2cap: Crossmodal knowledge transfer using transformer for 3d dense captioning. Z Yuan, X Yan, Y Liao, Y Guo, G Li, S Cui, Z Li, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. Z Yuan, X Yan, Y Liao, R Zhang, S Wang, Z Li, S Cui, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2021</p>
<p>Glipv2: Unifying localization and vision-language understanding. H Zhang, P Zhang, X Hu, Y C Chen, L Li, X Dai, L Wang, L Yuan, J N Hwang, J Gao, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>Pointclip: Point cloud understanding by clip. R Zhang, Z Guo, W Zhang, K Li, X Miao, B Cui, Y Qiao, P Gao, H Li, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. R Zhang, L Wang, Y Qiao, P Gao, H Li, Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR). Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Multi3drefer: Grounding text description to multiple 3d objects. Y Zhang, Z Gong, A X Chang, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)2023</p>
<p>3dvg-transformer: Relation modeling for visual grounding on point clouds. L Zhao, D Cai, L Sheng, D Xu, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2021</p>
<p>Structured3d: A large photo-realistic dataset for structured 3d modeling. J Zheng, J Zhang, J Li, R Tang, S Gao, Z Zhou, Proceedings of European Conference on Computer Vision (ECCV). European Conference on Computer Vision (ECCV)2020</p>
<p>W Zhu, J Hessel, A Awadalla, S Y Gadre, J Dodge, A Fang, Y Yu, L Schmidt, W Y Wang, Y Choi, arXiv:2304.06939Multimodal c4: An open, billion-scale corpus of images interleaved with text. 2023arXiv preprint</p>
<p>Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense. Y Zhu, T Gao, L Fan, S Huang, M Edmonds, H Liu, F Gao, C Zhang, S Qi, Y N Wu, Engineering. 632020</p>
<p>3d-vista: Pre-trained transformer for 3d vision and text alignment. Z Zhu, X Ma, Y Chen, Z Deng, S Huang, Q Li, Proceedings of International Conference on Computer Vision (ICCV. International Conference on Computer Vision (ICCV2023</p>
<p>Unifying 3d vision-language understanding via promptable queries. Z Zhu, Z Zhang, X Ma, X Niu, Y Chen, B Jia, Z Deng, S Huang, Q Li, Proceedings of European Conference on Computer Vision (ECCV. European Conference on Computer Vision (ECCV2024</p>            </div>
        </div>

    </div>
</body>
</html>
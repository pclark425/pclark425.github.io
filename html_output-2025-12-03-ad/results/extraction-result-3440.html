<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3440 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3440</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3440</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264590280</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.18659v2.pdf" target="_blank">DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3440.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3440.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 + DetermLR (LogiQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) evaluated with the DetermLR framework on LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used as the base LLM and paired with DetermLR (a framework that converts indeterminate premises into determinate propositions via identification, prioritization/exploration, and reasoning memory) to solve LogiQA multiple-choice logical-exam questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer-based language model from OpenAI used as the base LLM in experiments; the paper does not provide architecture/parameter details beyond referencing GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning questions collected from Chinese National Civil Servants Examination; tests varied analytical logical reasoning where premises are provided in the context.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR framework: (1) premise identification into determinate vs indeterminate, (2) two-stage premise prioritization (relevance scoring to target and supplement scoring) and three-fold verification of new propositions (logical validity, usefulness, duplication avoidance), (3) iterative reasoning memory storing positive/negative reasoning paths; prompts implemented with guidance library and low temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 54.19% on curated LogiQA subset (179 questions); average reasoning steps per case reported 11.74.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Best baseline on LogiQA (same paper, GPT-4) reported: Cumulative Reasoning (CR) ~45.25% and ToT ~43.02%; CoT-SC ~40.43% (values reported in paper Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Absolute improvement ≈ +8.9 percentage points vs CR (54.19% vs ~45.25%) and larger gains vs other baselines; also required fewer/more-efficient reasoning steps than most baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper reports remaining failures on tasks requiring extraction of implicit conditions, boundary-condition reasoning, and flexible application of logical rules (examples: failing to infer roommate gender parity, failing certain boundary constraints, inability to deduce B from A→B and ¬A→B). Also challenges with very long or deeply nested conditional descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation: DetermLR w/o identify: LogiQA 46.15% (avg steps 17.24); w/o priority: 47.83% (18.35); w/o memory: 39.66% (11.98). Complexity analysis: DetermLR uses fewer overall steps than ToT/CR (paper reports average steps per case lower and per-step time slightly higher but overall wall-clock lower).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3440.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 + DetermLR (ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) evaluated with the DetermLR framework on ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 used with the DetermLR iterative premise-identification/prioritization/memory pipeline to perform deductive reasoning and entailment classification on the ProofWriter deductive dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLM (OpenAI) used as evaluation backbone; specific weights/parameter counts not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter (open-world, depth-5 subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive reasoning dataset where the model must judge whether statements are provably true, false, or unknown given a set of rules and facts (depth-limited reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR (see previous entry): categorizes premises, prioritizes a primary determinate premise and supplements, explores new propositions with three-fold verification, stores reasoning paths in memory to inform future iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 79.17% on the chosen ProofWriter subset; average reasoning steps per case reported 14.63.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines (GPT-4) reported in paper: ToT ~70.33%, CR ~71.67%, CoT-SC ~69.33% (Table 1 numbers from paper).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Absolute improvement ≈ +7.5–9.0 percentage points over the strongest listed baselines on ProofWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same categories as above: misses implicit conditions, boundary conditions, and some uses of logical rules; can struggle with deeply nested conditionals or extremely long premise sets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation: removing identification/prioritization/memory modules reduces accuracy (see Table 2): w/o identify 71.50% (16.58 steps), w/o priority 72.32% (17.21), w/o memory 68.33% (14.79), showing each module contributes to final performance and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3440.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 + DetermLR (FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) evaluated with DetermLR on FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of DetermLR using GPT-4 to FOLIO, a dataset requiring complex first-order logic reasoning with FOL annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLM (OpenAI GPT-4) used as the inference engine; training/parameter details not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Human-annotated dataset requiring first-order logic (FOL) style natural language reasoning; includes tasks requiring more complex quantification and relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR: premise identification (determinate vs indeterminate), prioritized exploration (relevance + supplement scoring), three-fold verification, and iterative memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 75.49% on the FOLIO validation set; average reasoning steps reported 8.57.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines (GPT-4) reported: ToT ~69.12% (or similar), CR ~69.11% (paper Table 1 shows lower baseline accuracies around high-60s/low-70s), CoT-SC ~68.14%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improvement of several percentage points (≈ +6–7 pp) over the stronger baselines reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limits include difficulty extracting implicit quantificational conditions and correctly handling nested conditionals or long premise descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation results (Table 2): w/o identify 69.61% (13.70 steps), w/o priority 70.59% (14.69), w/o memory 67.65% (8.65), indicating memory and prioritization contribute to accuracy and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3440.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 + DetermLR (PrOntoQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) evaluated with DetermLR on PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of DetermLR with GPT-4 on PrOntoQA/PrOntoQA-like deductive reasoning benchmarks (similar to ProofWriter) to judge provability / entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as experimental backbone; paper does not enumerate model internals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA (PrOntoQA/ProntoQA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic/procedural dataset for testing multi-step deductive reasoning (similar to ProofWriter); models must derive entailed facts using given rules.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR (identification/prioritization/exploration/memory) with specific prompt templates and three-fold verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported near-ceiling accuracy 98.60% on the selected PrOntoQA subset (average steps reported ~9.78 in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines (GPT-4) reported somewhat lower accuracies (ToT/CR in mid-to-high 90s or lower); specific baseline numbers for PrOntoQA in Table 1 show DetermLR improving modestly over CR/ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Small absolute gains in an already high-performing regime (a few percentage points over strong baselines), with fewer reasoning steps on average.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Even on synthetic tasks some failure modes remain: failures in implicit inference and some boundary/conditional edge-cases; ablation shows memory and prioritization still matter.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation: w/o identify / w/o priority / w/o memory reduce accuracy (see Table 2). DetermLR also reports fewer steps-per-useful-proposition than CR/ToT (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3440.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 + DetermLR (LogicalDeduction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) evaluated with DetermLR on LogicalDeduction (LD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of DetermLR with GPT-4 to BigBench LogicalDeduction tasks that require deducing ordering relations among objects from described constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as the LLM for iterative reasoning; paper provides usage details (prompts, guidance lib) but not model internals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicalDeduction (LD)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Ordering/deduction problems (BigBench) where models deduce positions or orderings of objects from a set of relational premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR (premise categorization, prioritized exploration with supplement selection, 3-fold verification, and reasoning memory) implemented via prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 85.00% on LD (paper Table 1); average reasoning steps per case reported 12.35.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines (GPT-4) reported lower accuracies (ToT/CR/CoT variants reported in high-70s; e.g., CR ~78.33%/ToT ~76.83% in table excerpts).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improvement of ≈ +6–7 percentage points over the strongest baseline reported for LD.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Failures include inability to extract certain implicit constraints from verbose contexts, difficulty with boundary conditions and nested conditional chains, and some rule-application rigidities.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation: without memory/prioritization/identification modules performance drops (Table 2 shows declines); DetermLR also reports better per-proposition generation efficiency (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3440.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo + DetermLR (LogiQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo evaluated with DetermLR on LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller OpenAI chat model GPT-3.5-turbo used with DetermLR; demonstrates improved logical reasoning relative to standard prompting baselines but below GPT-4 results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat-oriented transformer model used as experimental backbone; exact parameter counts not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA (curated subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical-exam problems; requires multi-step analytic reasoning from premises presented in context.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR with the same identification/prioritization/exploration/memory pipeline and prompt templates; guidance library used; temperature 0.1 default.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 37.99% on curated LogiQA subset; average reasoning steps per case reported 13.39.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines with GPT-3.5-turbo reported lower accuracies: e.g., standard prompting and CoT variants in the 16–40% ranges (paper Table 1 shows many baselines below 40% for GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improvement of several percentage points over GPT-3.5-turbo baselines (paper reports DetermLR consistently outperforming baselines on same base model).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same failure modes reported as for GPT-4 but more pronounced due to lower base capability: implicit condition extraction, boundary reasoning, nested conditionals, and complex rule combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation numbers for GPT-3.5 variants are reported in Table 2 and Table 3 (w/o identify/priority/memory degrade accuracy and increase steps); DetermLR still produces fewer average steps-per-useful-proposition than baselines (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3440.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo + DetermLR (ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo evaluated with DetermLR on ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5-turbo used with the DetermLR pipeline to perform entailment classification on ProofWriter; demonstrates meaningful gains over baseline prompting for this smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-oriented transformer model from OpenAI used as experimental backbone; paper does not provide parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter (open-world depth-5 subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive entailment tasks requiring deriving whether hypotheses are true/false/unknown from given rules and facts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR (premise classification, prioritized exploration with supplement scoring, threefold verification, reasoning memory), prompt-engineered via guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 68.83% on the chosen ProofWriter subset; average reasoning steps per case 16.52.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines with GPT-3.5-turbo reported lower accuracies (many standard/CoT baselines in the 50–60% range per Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Notable improvement (≈ +4–10 pp) relative to the same-model baselines reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same categories of failures; smaller model more prone to errors on implicit/boundary cases and complex nested conditionals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation shows removing memory/prioritization/identification reduces accuracy and/or increases steps (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3440.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo + DetermLR (FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo evaluated with DetermLR on FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5-turbo paired with DetermLR to tackle first-order-like natural language reasoning in FOLIO, yielding improved but lower absolute performance compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's chat model used with careful prompting; parameter details not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Dataset requiring complex first-order natural language reasoning with FOL annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR with guided prompt templates, two-stage scoring and three-fold verification, and iterative memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 63.72% on FOLIO; average reasoning steps per case reported 10.37.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines for GPT-3.5-turbo reported lower accuracies (many baselines in 50–60% ranges per Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improvement of several percentage points over same-model baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles with implicit quantifiers, nested conditionals, and boundary conditions; memory and prioritization ablations show large accuracy drops.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation entries in Table 2 show w/o identify and w/o priority reduce accuracy; Table 3 shows DetermLR needs fewer steps per generated useful proposition than alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3440.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo + DetermLR (PrOntoQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo evaluated with DetermLR on PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DetermLR applied with GPT-3.5-turbo on PrOntoQA-like synthetic deductive tasks; achieves high accuracy though lower than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-oriented LLM from OpenAI; paper uses it as a lower-capacity baseline to test DetermLR generality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic deductive reasoning dataset focusing on provability and multi-step entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR (identification, prioritized exploration, three-fold verification, memory) with specific prompt templates for synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 93.20% on the sampled PrOntoQA subset; average reasoning steps reported 10.74.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines (same model) show lower accuracies (CR/ToT and CoT variants generally lower), so DetermLR yields clear improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Substantial improvement for GPT-3.5-turbo relative to baselines (several percentage points), achieving near-high accuracy on synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Synthetic tasks reveal fewer failures but still show issues with logic-generalization edge cases, implicit conditions and boundary handling.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation and step-efficiency tables show DetermLR outperforms CR/ToT in steps-per-useful-proposition and that memory/prioritization contribute measurably.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3440.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo + DetermLR (LogicalDeduction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo evaluated with DetermLR on LogicalDeduction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of DetermLR with GPT-3.5-turbo on BigBench LogicalDeduction ordering tasks; demonstrates improved ordering accuracy compared to baseline prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model used as an accessible baseline; detailed training/size not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicalDeduction (LD)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks about deducing order/position relations of objects from sets of premises; often used to test combinatorial logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DetermLR with prioritization, exploration and reasoning memory; prompt templates include premise extraction and transformation steps for LD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 74.33% on LD; average reasoning steps per case reported 13.19.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines with GPT-3.5-turbo reported lower accuracies (many baselines in 60–72% range per Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improvement of a few percentage points vs strong baselines for the same base model, and fewer average steps-per-useful-proposition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Difficulty extracting implicit ordering implications, dealing with boundary constraints and long nested conditionals; ablation shows memory is important to maintain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation results (Table 2/3) indicate removal of identify/priority/memory reduces accuracy and/or increases steps; Table 4 shows DetermLR reduces total inference time by reducing steps despite slightly higher per-step time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3440.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3440.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DetermLR (method)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning framework that structures LLM logical reasoning as a transition from indeterminacy to determinacy via premise identification (determinate vs indeterminate), premise prioritization/exploration with quantitative scoring and verification, and iterative reasoning memory storing positive and negative reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DetermLR (framework applied to LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a neural model; a prompting/interaction framework around LLMs that uses specialized prompts to (1) identify determinate/indeterminate premises, (2) compute relevance and supplement scores to prioritize combinations, (3) explore and verify new determinate propositions with logical validity/usefulness/duplication checks, and (4) store/extract reasoning traces in a memory for future iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>General logical reasoning benchmarks (LogiQA, ProofWriter, FOLIO, PrOntoQA, LogicalDeduction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Various logical reasoning tasks covering exam-style analytic reasoning, deductive entailment, first-order style reasoning, synthetic provability problems, and ordering deduction; all supply premises in context.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Full DetermLR pipeline (identification, two-stage prioritization, three-fold verification for explored propositions, and iterative reasoning memory); implemented via detailed prompt templates and the guidance library, with low-temperature decoding and some hyperparameters (e.g., threshold θ, number n of determinate premises to generate).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across benchmarks, DetermLR outperforms several multi-step baselines when used with GPT-4 and GPT-3.5-turbo; example GPT-4 results: LogiQA 54.19% (11.74 steps), ProofWriter 79.17% (14.63), FOLIO 75.49% (8.57), PrOntoQA ~98.60% (≈9.78), LD 85.00% (12.35). GPT-3.5-turbo results: LogiQA 37.99% (13.39), ProofWriter 68.83% (16.52), FOLIO 63.72% (10.37), PrOntoQA 93.20% (10.74), LD 74.33% (13.19).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared against Standard prompting, CoT, CoT-SC, Selection-Inference (SI), LAMBADA (backward chaining), ToT (Tree of Thoughts), and CR (Cumulative Reasoning). Baselines are generally lower; e.g., on LogiQA (GPT-4) best baselines are CR ~45.25% and ToT ~43.02%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>DetermLR yields absolute accuracy improvements of several percentage points across datasets (notably ≈ +8.9 pp on LogiQA vs CR using GPT-4) and consistently reduces the number of reasoning iterations needed to generate useful propositions compared to CR/ToT (Table 3/4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limitations: difficulty extracting implicit premises from text, handling boundary conditions and complex constraint patterns, parsing very long premises or deeply nested if-then chains, and occasional logical rule-application failures; requires careful prompt engineering and hyperparameter tuning (e.g., number of generated determinate premises n).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation study shows each module matters: removing identification or priority increases steps and reduces accuracy (LogiQA drops from 54.19% to 46.15% w/o identify, to 47.83% w/o priority); removing memory drops accuracy substantially (e.g., LogiQA down to 39.66%). Tables 3 and 4 show DetermLR produces fewer average steps per useful proposition and lower overall inference time per case despite slightly higher per-step time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Cumulative reasoning with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3440",
    "paper_id": "paper-264590280",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "GPT-4 + DetermLR (LogiQA)",
            "name_full": "GPT-4 (OpenAI) evaluated with the DetermLR framework on LogiQA",
            "brief_description": "GPT-4 is used as the base LLM and paired with DetermLR (a framework that converts indeterminate premises into determinate propositions via identification, prioritization/exploration, and reasoning memory) to solve LogiQA multiple-choice logical-exam questions.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pre-trained transformer-based language model from OpenAI used as the base LLM in experiments; the paper does not provide architecture/parameter details beyond referencing GPT-4.",
            "model_size": null,
            "reasoning_task_name": "LogiQA",
            "reasoning_task_description": "Multiple-choice logical reasoning questions collected from Chinese National Civil Servants Examination; tests varied analytical logical reasoning where premises are provided in the context.",
            "method_or_intervention": "DetermLR framework: (1) premise identification into determinate vs indeterminate, (2) two-stage premise prioritization (relevance scoring to target and supplement scoring) and three-fold verification of new propositions (logical validity, usefulness, duplication avoidance), (3) iterative reasoning memory storing positive/negative reasoning paths; prompts implemented with guidance library and low temperature.",
            "performance": "Accuracy 54.19% on curated LogiQA subset (179 questions); average reasoning steps per case reported 11.74.",
            "baseline_performance": "Best baseline on LogiQA (same paper, GPT-4) reported: Cumulative Reasoning (CR) ~45.25% and ToT ~43.02%; CoT-SC ~40.43% (values reported in paper Table 1).",
            "improvement_over_baseline": "Absolute improvement ≈ +8.9 percentage points vs CR (54.19% vs ~45.25%) and larger gains vs other baselines; also required fewer/more-efficient reasoning steps than most baselines.",
            "limitations_or_failures": "Paper reports remaining failures on tasks requiring extraction of implicit conditions, boundary-condition reasoning, and flexible application of logical rules (examples: failing to infer roommate gender parity, failing certain boundary constraints, inability to deduce B from A→B and ¬A→B). Also challenges with very long or deeply nested conditional descriptions.",
            "ablation_or_analysis": "Ablation: DetermLR w/o identify: LogiQA 46.15% (avg steps 17.24); w/o priority: 47.83% (18.35); w/o memory: 39.66% (11.98). Complexity analysis: DetermLR uses fewer overall steps than ToT/CR (paper reports average steps per case lower and per-step time slightly higher but overall wall-clock lower).",
            "uuid": "e3440.0",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 + DetermLR (ProofWriter)",
            "name_full": "GPT-4 (OpenAI) evaluated with the DetermLR framework on ProofWriter",
            "brief_description": "GPT-4 used with the DetermLR iterative premise-identification/prioritization/memory pipeline to perform deductive reasoning and entailment classification on the ProofWriter deductive dataset.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer LLM (OpenAI) used as evaluation backbone; specific weights/parameter counts not specified in paper.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter (open-world, depth-5 subset)",
            "reasoning_task_description": "Deductive reasoning dataset where the model must judge whether statements are provably true, false, or unknown given a set of rules and facts (depth-limited reasoning).",
            "method_or_intervention": "DetermLR (see previous entry): categorizes premises, prioritizes a primary determinate premise and supplements, explores new propositions with three-fold verification, stores reasoning paths in memory to inform future iterations.",
            "performance": "Accuracy 79.17% on the chosen ProofWriter subset; average reasoning steps per case reported 14.63.",
            "baseline_performance": "Baselines (GPT-4) reported in paper: ToT ~70.33%, CR ~71.67%, CoT-SC ~69.33% (Table 1 numbers from paper).",
            "improvement_over_baseline": "Absolute improvement ≈ +7.5–9.0 percentage points over the strongest listed baselines on ProofWriter.",
            "limitations_or_failures": "Same categories as above: misses implicit conditions, boundary conditions, and some uses of logical rules; can struggle with deeply nested conditionals or extremely long premise sets.",
            "ablation_or_analysis": "Ablation: removing identification/prioritization/memory modules reduces accuracy (see Table 2): w/o identify 71.50% (16.58 steps), w/o priority 72.32% (17.21), w/o memory 68.33% (14.79), showing each module contributes to final performance and efficiency.",
            "uuid": "e3440.1",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 + DetermLR (FOLIO)",
            "name_full": "GPT-4 (OpenAI) evaluated with DetermLR on FOLIO",
            "brief_description": "Application of DetermLR using GPT-4 to FOLIO, a dataset requiring complex first-order logic reasoning with FOL annotations.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Transformer LLM (OpenAI GPT-4) used as the inference engine; training/parameter details not provided in paper.",
            "model_size": null,
            "reasoning_task_name": "FOLIO",
            "reasoning_task_description": "Human-annotated dataset requiring first-order logic (FOL) style natural language reasoning; includes tasks requiring more complex quantification and relational reasoning.",
            "method_or_intervention": "DetermLR: premise identification (determinate vs indeterminate), prioritized exploration (relevance + supplement scoring), three-fold verification, and iterative memory.",
            "performance": "Accuracy 75.49% on the FOLIO validation set; average reasoning steps reported 8.57.",
            "baseline_performance": "Baselines (GPT-4) reported: ToT ~69.12% (or similar), CR ~69.11% (paper Table 1 shows lower baseline accuracies around high-60s/low-70s), CoT-SC ~68.14%.",
            "improvement_over_baseline": "Improvement of several percentage points (≈ +6–7 pp) over the stronger baselines reported in the paper.",
            "limitations_or_failures": "Reported limits include difficulty extracting implicit quantificational conditions and correctly handling nested conditionals or long premise descriptions.",
            "ablation_or_analysis": "Ablation results (Table 2): w/o identify 69.61% (13.70 steps), w/o priority 70.59% (14.69), w/o memory 67.65% (8.65), indicating memory and prioritization contribute to accuracy and efficiency.",
            "uuid": "e3440.2",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 + DetermLR (PrOntoQA)",
            "name_full": "GPT-4 (OpenAI) evaluated with DetermLR on PrOntoQA",
            "brief_description": "Evaluation of DetermLR with GPT-4 on PrOntoQA/PrOntoQA-like deductive reasoning benchmarks (similar to ProofWriter) to judge provability / entailment.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used as experimental backbone; paper does not enumerate model internals.",
            "model_size": null,
            "reasoning_task_name": "PrOntoQA (PrOntoQA/ProntoQA)",
            "reasoning_task_description": "Synthetic/procedural dataset for testing multi-step deductive reasoning (similar to ProofWriter); models must derive entailed facts using given rules.",
            "method_or_intervention": "DetermLR (identification/prioritization/exploration/memory) with specific prompt templates and three-fold verification.",
            "performance": "Reported near-ceiling accuracy 98.60% on the selected PrOntoQA subset (average steps reported ~9.78 in Table 1).",
            "baseline_performance": "Baselines (GPT-4) reported somewhat lower accuracies (ToT/CR in mid-to-high 90s or lower); specific baseline numbers for PrOntoQA in Table 1 show DetermLR improving modestly over CR/ToT.",
            "improvement_over_baseline": "Small absolute gains in an already high-performing regime (a few percentage points over strong baselines), with fewer reasoning steps on average.",
            "limitations_or_failures": "Even on synthetic tasks some failure modes remain: failures in implicit inference and some boundary/conditional edge-cases; ablation shows memory and prioritization still matter.",
            "ablation_or_analysis": "Ablation: w/o identify / w/o priority / w/o memory reduce accuracy (see Table 2). DetermLR also reports fewer steps-per-useful-proposition than CR/ToT (Table 3).",
            "uuid": "e3440.3",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 + DetermLR (LogicalDeduction)",
            "name_full": "GPT-4 (OpenAI) evaluated with DetermLR on LogicalDeduction (LD)",
            "brief_description": "Application of DetermLR with GPT-4 to BigBench LogicalDeduction tasks that require deducing ordering relations among objects from described constraints.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used as the LLM for iterative reasoning; paper provides usage details (prompts, guidance lib) but not model internals.",
            "model_size": null,
            "reasoning_task_name": "LogicalDeduction (LD)",
            "reasoning_task_description": "Ordering/deduction problems (BigBench) where models deduce positions or orderings of objects from a set of relational premises.",
            "method_or_intervention": "DetermLR (premise categorization, prioritized exploration with supplement selection, 3-fold verification, and reasoning memory) implemented via prompt templates.",
            "performance": "Accuracy 85.00% on LD (paper Table 1); average reasoning steps per case reported 12.35.",
            "baseline_performance": "Baselines (GPT-4) reported lower accuracies (ToT/CR/CoT variants reported in high-70s; e.g., CR ~78.33%/ToT ~76.83% in table excerpts).",
            "improvement_over_baseline": "Improvement of ≈ +6–7 percentage points over the strongest baseline reported for LD.",
            "limitations_or_failures": "Failures include inability to extract certain implicit constraints from verbose contexts, difficulty with boundary conditions and nested conditional chains, and some rule-application rigidities.",
            "ablation_or_analysis": "Ablation: without memory/prioritization/identification modules performance drops (Table 2 shows declines); DetermLR also reports better per-proposition generation efficiency (Table 3).",
            "uuid": "e3440.4",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo + DetermLR (LogiQA)",
            "name_full": "GPT-3.5-turbo evaluated with DetermLR on LogiQA",
            "brief_description": "Smaller OpenAI chat model GPT-3.5-turbo used with DetermLR; demonstrates improved logical reasoning relative to standard prompting baselines but below GPT-4 results.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI chat-oriented transformer model used as experimental backbone; exact parameter counts not provided in paper.",
            "model_size": null,
            "reasoning_task_name": "LogiQA (curated subset)",
            "reasoning_task_description": "Multiple-choice logical-exam problems; requires multi-step analytic reasoning from premises presented in context.",
            "method_or_intervention": "DetermLR with the same identification/prioritization/exploration/memory pipeline and prompt templates; guidance library used; temperature 0.1 default.",
            "performance": "Accuracy 37.99% on curated LogiQA subset; average reasoning steps per case reported 13.39.",
            "baseline_performance": "Baselines with GPT-3.5-turbo reported lower accuracies: e.g., standard prompting and CoT variants in the 16–40% ranges (paper Table 1 shows many baselines below 40% for GPT-3.5).",
            "improvement_over_baseline": "Improvement of several percentage points over GPT-3.5-turbo baselines (paper reports DetermLR consistently outperforming baselines on same base model).",
            "limitations_or_failures": "Same failure modes reported as for GPT-4 but more pronounced due to lower base capability: implicit condition extraction, boundary reasoning, nested conditionals, and complex rule combinations.",
            "ablation_or_analysis": "Ablation numbers for GPT-3.5 variants are reported in Table 2 and Table 3 (w/o identify/priority/memory degrade accuracy and increase steps); DetermLR still produces fewer average steps-per-useful-proposition than baselines (Table 3).",
            "uuid": "e3440.5",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo + DetermLR (ProofWriter)",
            "name_full": "GPT-3.5-turbo evaluated with DetermLR on ProofWriter",
            "brief_description": "GPT-3.5-turbo used with the DetermLR pipeline to perform entailment classification on ProofWriter; demonstrates meaningful gains over baseline prompting for this smaller model.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Chat-oriented transformer model from OpenAI used as experimental backbone; paper does not provide parameter count.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter (open-world depth-5 subset)",
            "reasoning_task_description": "Deductive entailment tasks requiring deriving whether hypotheses are true/false/unknown from given rules and facts.",
            "method_or_intervention": "DetermLR (premise classification, prioritized exploration with supplement scoring, threefold verification, reasoning memory), prompt-engineered via guidance.",
            "performance": "Accuracy 68.83% on the chosen ProofWriter subset; average reasoning steps per case 16.52.",
            "baseline_performance": "Baselines with GPT-3.5-turbo reported lower accuracies (many standard/CoT baselines in the 50–60% range per Table 1).",
            "improvement_over_baseline": "Notable improvement (≈ +4–10 pp) relative to the same-model baselines reported.",
            "limitations_or_failures": "Same categories of failures; smaller model more prone to errors on implicit/boundary cases and complex nested conditionals.",
            "ablation_or_analysis": "Ablation shows removing memory/prioritization/identification reduces accuracy and/or increases steps (see Table 2).",
            "uuid": "e3440.6",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo + DetermLR (FOLIO)",
            "name_full": "GPT-3.5-turbo evaluated with DetermLR on FOLIO",
            "brief_description": "GPT-3.5-turbo paired with DetermLR to tackle first-order-like natural language reasoning in FOLIO, yielding improved but lower absolute performance compared to GPT-4.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI's chat model used with careful prompting; parameter details not provided.",
            "model_size": null,
            "reasoning_task_name": "FOLIO",
            "reasoning_task_description": "Dataset requiring complex first-order natural language reasoning with FOL annotations.",
            "method_or_intervention": "DetermLR with guided prompt templates, two-stage scoring and three-fold verification, and iterative memory.",
            "performance": "Accuracy 63.72% on FOLIO; average reasoning steps per case reported 10.37.",
            "baseline_performance": "Baselines for GPT-3.5-turbo reported lower accuracies (many baselines in 50–60% ranges per Table 1).",
            "improvement_over_baseline": "Improvement of several percentage points over same-model baselines.",
            "limitations_or_failures": "Struggles with implicit quantifiers, nested conditionals, and boundary conditions; memory and prioritization ablations show large accuracy drops.",
            "ablation_or_analysis": "Ablation entries in Table 2 show w/o identify and w/o priority reduce accuracy; Table 3 shows DetermLR needs fewer steps per generated useful proposition than alternatives.",
            "uuid": "e3440.7",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo + DetermLR (PrOntoQA)",
            "name_full": "GPT-3.5-turbo evaluated with DetermLR on PrOntoQA",
            "brief_description": "DetermLR applied with GPT-3.5-turbo on PrOntoQA-like synthetic deductive tasks; achieves high accuracy though lower than GPT-4.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Chat-oriented LLM from OpenAI; paper uses it as a lower-capacity baseline to test DetermLR generality.",
            "model_size": null,
            "reasoning_task_name": "PrOntoQA",
            "reasoning_task_description": "Synthetic deductive reasoning dataset focusing on provability and multi-step entailment.",
            "method_or_intervention": "DetermLR (identification, prioritized exploration, three-fold verification, memory) with specific prompt templates for synthetic tasks.",
            "performance": "Accuracy 93.20% on the sampled PrOntoQA subset; average reasoning steps reported 10.74.",
            "baseline_performance": "Baselines (same model) show lower accuracies (CR/ToT and CoT variants generally lower), so DetermLR yields clear improvements.",
            "improvement_over_baseline": "Substantial improvement for GPT-3.5-turbo relative to baselines (several percentage points), achieving near-high accuracy on synthetic tasks.",
            "limitations_or_failures": "Synthetic tasks reveal fewer failures but still show issues with logic-generalization edge cases, implicit conditions and boundary handling.",
            "ablation_or_analysis": "Ablation and step-efficiency tables show DetermLR outperforms CR/ToT in steps-per-useful-proposition and that memory/prioritization contribute measurably.",
            "uuid": "e3440.8",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo + DetermLR (LogicalDeduction)",
            "name_full": "GPT-3.5-turbo evaluated with DetermLR on LogicalDeduction",
            "brief_description": "Evaluation of DetermLR with GPT-3.5-turbo on BigBench LogicalDeduction ordering tasks; demonstrates improved ordering accuracy compared to baseline prompts.",
            "citation_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI chat model used as an accessible baseline; detailed training/size not provided in the paper.",
            "model_size": null,
            "reasoning_task_name": "LogicalDeduction (LD)",
            "reasoning_task_description": "Tasks about deducing order/position relations of objects from sets of premises; often used to test combinatorial logical reasoning.",
            "method_or_intervention": "DetermLR with prioritization, exploration and reasoning memory; prompt templates include premise extraction and transformation steps for LD.",
            "performance": "Accuracy 74.33% on LD; average reasoning steps per case reported 13.19.",
            "baseline_performance": "Baselines with GPT-3.5-turbo reported lower accuracies (many baselines in 60–72% range per Table 1).",
            "improvement_over_baseline": "Improvement of a few percentage points vs strong baselines for the same base model, and fewer average steps-per-useful-proposition.",
            "limitations_or_failures": "Difficulty extracting implicit ordering implications, dealing with boundary constraints and long nested conditionals; ablation shows memory is important to maintain performance.",
            "ablation_or_analysis": "Ablation results (Table 2/3) indicate removal of identify/priority/memory reduces accuracy and/or increases steps; Table 4 shows DetermLR reduces total inference time by reducing steps despite slightly higher per-step time.",
            "uuid": "e3440.9",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DetermLR (method)",
            "name_full": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
            "brief_description": "A reasoning framework that structures LLM logical reasoning as a transition from indeterminacy to determinacy via premise identification (determinate vs indeterminate), premise prioritization/exploration with quantitative scoring and verification, and iterative reasoning memory storing positive and negative reasoning paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DetermLR (framework applied to LLMs)",
            "model_description": "Not a neural model; a prompting/interaction framework around LLMs that uses specialized prompts to (1) identify determinate/indeterminate premises, (2) compute relevance and supplement scores to prioritize combinations, (3) explore and verify new determinate propositions with logical validity/usefulness/duplication checks, and (4) store/extract reasoning traces in a memory for future iterations.",
            "model_size": null,
            "reasoning_task_name": "General logical reasoning benchmarks (LogiQA, ProofWriter, FOLIO, PrOntoQA, LogicalDeduction)",
            "reasoning_task_description": "Various logical reasoning tasks covering exam-style analytic reasoning, deductive entailment, first-order style reasoning, synthetic provability problems, and ordering deduction; all supply premises in context.",
            "method_or_intervention": "Full DetermLR pipeline (identification, two-stage prioritization, three-fold verification for explored propositions, and iterative reasoning memory); implemented via detailed prompt templates and the guidance library, with low-temperature decoding and some hyperparameters (e.g., threshold θ, number n of determinate premises to generate).",
            "performance": "Across benchmarks, DetermLR outperforms several multi-step baselines when used with GPT-4 and GPT-3.5-turbo; example GPT-4 results: LogiQA 54.19% (11.74 steps), ProofWriter 79.17% (14.63), FOLIO 75.49% (8.57), PrOntoQA ~98.60% (≈9.78), LD 85.00% (12.35). GPT-3.5-turbo results: LogiQA 37.99% (13.39), ProofWriter 68.83% (16.52), FOLIO 63.72% (10.37), PrOntoQA 93.20% (10.74), LD 74.33% (13.19).",
            "baseline_performance": "Compared against Standard prompting, CoT, CoT-SC, Selection-Inference (SI), LAMBADA (backward chaining), ToT (Tree of Thoughts), and CR (Cumulative Reasoning). Baselines are generally lower; e.g., on LogiQA (GPT-4) best baselines are CR ~45.25% and ToT ~43.02%.",
            "improvement_over_baseline": "DetermLR yields absolute accuracy improvements of several percentage points across datasets (notably ≈ +8.9 pp on LogiQA vs CR using GPT-4) and consistently reduces the number of reasoning iterations needed to generate useful propositions compared to CR/ToT (Table 3/4).",
            "limitations_or_failures": "Reported limitations: difficulty extracting implicit premises from text, handling boundary conditions and complex constraint patterns, parsing very long premises or deeply nested if-then chains, and occasional logical rule-application failures; requires careful prompt engineering and hyperparameter tuning (e.g., number of generated determinate premises n).",
            "ablation_or_analysis": "Ablation study shows each module matters: removing identification or priority increases steps and reduces accuracy (LogiQA drops from 54.19% to 46.15% w/o identify, to 47.83% w/o priority); removing memory drops accuracy substantially (e.g., LogiQA down to 39.66%). Tables 3 and 4 show DetermLR produces fewer average steps per useful proposition and lower overall inference time per case despite slightly higher per-step time.",
            "uuid": "e3440.10",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Cumulative reasoning with large language models",
            "rating": 2,
            "sanitized_title": "cumulative_reasoning_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 1,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        }
    ],
    "cost": 0.02245025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy
26 May 2024</p>
<p>Hongda Sun 
Weikai Xu 
Wei Liu liuwei40@xiaomi.com 
XiaoMi AI Lab</p>
<p>Jian Luan luanjian@xiaomi.com 
XiaoMi AI Lab</p>
<p>Bin Wang wangbin11@xiaomi.com 
XiaoMi AI Lab</p>
<p>Shuo Shang 
University of Electronic Science and Technology of China</p>
<p>Ji-Rong Wen 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Rui Yan ruiyan@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy
26 May 2024047EAE74AEA42009BDAB822E5D413675arXiv:2310.18659v2[cs.AI]
Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks.To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs.However, LLM-based reasoning still encounters the following challenges:(1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps.To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy.First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights.Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights.Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps.Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOn-toQA, and LogicalDeduction.Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.</p>
<p>Introduction</p>
<p>The emergence of large language models (LLMs) has instigated a transformative wave within the realm of artificial intelligence (Zhao et al., 2023).The series models of GPT (Brown et al., 2020;Ouyang et al., 2022;OpenAI, 2023) and PaLM (Chowdhery et al., 2022;Anil et al., 2023) have exhibited remarkable proficiency in natural language reasoning, contributing to the advancement of research and applications of cognitive intelligence (Huang and Chang, 2022).However, even the current state-of-the-art (SOTA) LLMs still grapple with a key limitation: the lack of humanlike advanced reasoning skills to rationally analyze known conditions and draw conclusions (Arkoudas, 2023;Singh et al., 2023).This leaves a substantial gap between LLM-based reasoning and the cognitive process of human reasoning.</p>
<p>To alleviate this limitation, existing studies employ enhanced prompt engineering techniques to guide LLMs in eliciting intermediate thinking steps to ensure reliable conclusions (Zhou et al., 2022;Khot et al., 2022;Wei et al., 2022;Kojima et al., 2022).Building upon this foundation, recent works have focused on introducing more intricate reasoning structures, such as multiple chains (Wang et al., 2022b), trees (Yao et al., 2023) or graphs (Lei et al., 2023;Besta et al., 2023), to tackle increasingly complex reasoning tasks.However, LLM-based reasoning continues to encounter three challenges:</p>
<p>(1) Limited adaptability of preset structures to diverse tasks: Since the task complexity cannot be solely inferred from the problem context, relying on a certain preset structure to solve a variety of reasoning problems may create deficiencies in reasoning effectiveness or efficiency (Yao et al., 2023;Lei et al., 2023).This approach contrasts with human problem-solving techniques, which are not dependent on preset reasoning structures.Ideally, the reasoning structure should be the result of manual review after solving the problem.(2) Insufficient precision in exploiting known conditions to derive new ones: The literature on human cognitive reasoning provides valuable insights and emphasizes the importance of integrating available information for informed decision-making (Schaeken et al., 1999;Evans, 2002;Baron, 2023).This motivates cumulative reasoning (CR) (Zhang et al., 2023), which uses LLMs to iteratively generate new propositions based on available premises.However, CR still cannot approach the human thought process, as it relies on the random combination of existing premises without a well-defined criterion.(3) Inadequate consideration of historical reasoning experiences for future reasoning: Previous works (Wei et al., 2022;Yao et al., 2023) often overlook historical reasoning details, resulting in the lack of necessary information for subsequent phases.</p>
<p>To address these challenges and augment LLMs to grasp more human-like advanced reasoning skills, we need to consider three key factors: (1) Refine the formulation of the essence of the reasoning process; (2) Prioritize relevant premises for efficiently exploring new information; (3) Memorize historical reasoning details to guide the direction of the subsequent reasoning steps.</p>
<p>To this end, we propose DetermLR, a novel reasoning framework to align LLM-based reasoning more closely with human thinking.First, we formulate the logical reasoning process as an evolution from indeterminacy to determinacy.Since premises exhibit varying descriptions and associations with the target, we initiate the reasoning process with premise identification to finely categorize premises into two distinct types: determinate and indeterminate.Determinate premises are defined as simple statements, which can definitively contribute to conclusion derivation.Conversely, indeterminate premises may contain complex rules governing the relationships among multiple propositions.Regardless of the problem complexity, the reasoning process consistently involves the continuous accumulation of determinate information, steering the conclusion toward greater clarity.</p>
<p>Subsequently, human reasoning typically aims for a "breakingthrough" from known conditions to deduce new insights, indicating the necessity to distinguish the priority of premises.Therefore, we propose quantitative measurements to facilitate premise prioritization, which involves identifying the most relevant premise to the conclusion and screening supplementary premises likely to interact with this primary premise.This guides LLMs to exclude irrelevant premises and focus on more pertinent information for premise exploration.</p>
<p>Furthermore, we introduce a reasoning memorization module to automate the storage and extraction of available premises and reasoning paths.</p>
<p>In this way, historical reasoning details are preserved in the reasoning memory to update reasoning states, and they are incorporated into future reasoning steps to refer to inherent experiences and avoid repeating similar mistakes.</p>
<p>To verify the capability of LLMs to engage in rigorous logical reasoning, we conduct extensive experiments on various challenging logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, ProntoQA, and LogicalDeduction.The experimental results show that DetermLR achieves the best performance on reasoning accuracy, coupled with superior efficiency of requiring fewer steps than previous multi-step reasoning methods.Notably, in more intricate tasks like LogiQA, DetermLR exhibits even more pronounced advancements, mirroring human-like reasoning skills to a greater extent.</p>
<p>Our technical contributions to advancing LLMbased reasoning can be summarized as follows:</p>
<p>• We propose a novel framework that formulates the reasoning process as an evolution from indeterminacy to determinacy, aligning LLM-based reasoning more closely with human reasoning.</p>
<p>• We leverage quantitative measurements for premise prioritization and exploration, enabling LLMs to prioritize premises more conducive to exploring new insights and deriving conclusions.</p>
<p>• We introduce a reasoning memorization module to preserve essential historical reasoning details during the iterative reasoning process.</p>
<p>Related Work</p>
<p>Conventional Logical Reasoning</p>
<p>Many previous works focus on solving logical reasoning tasks using machine reading comprehension (Ouyang et al., 2021), adversarial pretraining (Pi et al., 2022), and contrastive learning (Jiao et al., 2022).In the realm of logical reasoning benchmarks, various tasks have been utilized for evaluation (Khot et al., 2018;Wang et al., 2022a;Bhagavatula et al., 2019;Welleck et al., 2018;Williams et al., 2017;Dagan et al., 2005;Bowman et al., 2015;Wang et al., 2018;Liu et al., 2021;Tian et al., 2021).LogiQA (Liu et al., 2020) involves diverse types of logical reasoning questions collected from the National Civil Servants Examination of China.Based on Big-Bench (Srivastava et al., 2022), which is used to evaluate multiaspect abilities of language models, Big-Bench Hard (BBH) (Suzgun et al., 2022) focuses on 23 challenging tasks for evaluating LLM-based reasoning.FOLIO (Han et al., 2022) is a humanannotated and logically complex datasets for natural language reasoning, equipped with first-order logic (FOL) annotations.ProofWriter (Tafjord et al., 2020) is another commonly used dataset for deductive logical reasoning.Among them, we carefully select five benchmarks whose premises are listed directly in the problem context and require no additional processing.</p>
<p>LLM-based Logical Reasoning</p>
<p>Previous methods mainly enhance reasoning by eliciting intermediate steps like chain-of-thought (CoT) (Wei et al., 2022;Wang et al., 2022b) and least-to-most prompting (Zhou et al., 2022).Extending the CoT concept, which follows a leftto-right progression, more recent works present more intricate thought structures to face more complex problems, such as ToT (Yao et al., 2023;Hu et al., 2023) or GoT (Lei et al., 2023).Selectioninference (Creswell et al., 2022a) refine the reasoning process of CoT by decomposing it into two modules: selection and inference.Algorithmof-Thoughts (Sel et al., 2023) navigate reasoning pathways as in-context examples with merely a few queries.Cumulative reasoning (Zhang et al., 2023) uses higher-order logic rules for exploring new propositions based on given premises.Current LLM-based reasoning methods still face challenges in emulating human-like reasoning skills.In response to these challenges, we propose a novel perspective for formulating the reasoning process.This approach prioritizes the exploration of new insights by leveraging more relevant premises and enables iterative reasoning based on key historical reasoning experiences.</p>
<p>DetermLR</p>
<p>Problem Formulation</p>
<p>The objective of a logical reasoning problem can be regarded as using known premises and logical deduction rules to derive new essential intermediate propositions, culminating in an eventual target conclusion.Suppose a problem provides a set of N premises, denoted as P = (p 1 , p 2 , • • • , p N ), the logical reasoning process can be formulated as:
c = Reason(p 1 , p 2 , • • • , p n ),(1)
where c is the target conclusion of the problem, and the mapping Reason indicates how to use the given premises to derive the conclusion.In this paper, our focus is on aligning LLM-based reasoning more closely with human reasoning.Therefore, the Reason is generally implemented by instructing LLMs to understand the problem and iteratively provide new insights to solve it.Building upon the available premises and the target as input, we propose a novel perspective to formulate the process of logical reasoning.In the following sections, we will introduce in detail three pivotal modules of the proposed method: (1) premise identification ( §3.2); (2) premise prioritization and exploration ( §3.3); and (3) iterative reasoning memorization ( §3.4).</p>
<p>Premise Identification</p>
<p>As previously discussed, the essence of the reasoning process lies in the mapping from premises to conclusions.Existing methods (Yao et al., 2023;Besta et al., 2023) that preset the reasoning structure before solving a problem may not always yield an appropriate mapping, as this diverges from the focus of human reasoning.Instead, the so-called reasoning structure should be formed based on the reviewed reasoning outcomes after problem resolution.Thus, refining a better formulation for the essence of the reasoning process becomes the first key factor in augmenting LLM-based reasoning.Regarding the given premises in a logical reasoning problem, it becomes apparent that the "determinacy" of the information supplied by each premise varies significantly: some directly provide pertinent information for deriving the conclusion, while others necessitate combination with other conditions to obtain new insights.The indeterminacy gradually diminishes with the accumulation of determinate information, bringing the reasoning process closer to the conclusion.Therefore, we rethink the essence of the reasoning process as from indeterminacy to determinacy.</p>
<p>To be more specific, we develop a premise identification module to emulate the transition from indeterminacy to determinacy.Input premises are categorized into two distinct types: determinate premises D and indeterminate premises I.The identification criterion is dependent upon both the inherent description of the premise and its connection with the input target c, as expressed by: D, I = Identify(P, c).</p>
<p>(
)2
In practice, we implement the Identify function through carefully designed instructions for LLMs,  and related prompt templates are available in Appendix.Determinate premises are defined as simple statements that definitively lead to the desired target.These premises state clarified facts and serve as the foundational blocks for reasoning.In contrast, indeterminate premises encompass propositions not directly related to the target and often contain complex structures reflecting indeterminacy, such as disjunction (x or y) and hypothesis (if x then y).An indeterminate premise may be combined with other premises to establish a logical path to evolve into a determinate state.</p>
<p>As shown in Figure 1(a), the target revolves around "Gary" and "round", so simple statements including "Gary" are identified as determinate premises (d 1 • • • d 3 ), while the remaining premises are classified as indeterminate ones (i 1 • • • i 7 ).Building upon this module, LLMs can eliminate the need for preset structures and enhance the clarity of the reasoning process under our new formulation.</p>
<p>Premise Prioritization and Exploration</p>
<p>Once the original premises are categorized, how to better uncover the relationships between these premises to explore new insights is the next critical reasoning step.Prior sampling-based methods cannot distinguish the priority of different premises (Yao et al., 2023;Zhang et al., 2023), leading to less skillful reasoning compared to human counterparts.Therefore, we aim to quantify the relationship between each premise and the target and prioritize premise selection for better exploration.</p>
<p>Premise prioritization with two-stage scoring.</p>
<p>To improve the opportunity of deriving new insights, we leverage two quantitative measurements to select useful premises for combination.Firstly, we evaluate the relevance score of each premise p in conjunction with the target c.By simulating the overlap of topics and elements within them, varying priorities can be assigned to the premises.Secondly, we select the most relevant premise p * from D as the primary premise, and all other premises are considered candidate supplementary premises to interact with p * .We then quantify the likelihood of these premises being merged with p * according to semantic similarity and adherence to logical deduction rules.Therefore, supplementary premises p s exceeding a given threshold θ can be obtained.The two-stage scoring can be formulated as:
r p = relevance(p, c), p * ≜ arg max p∈D r p ,(3)s p ′ = supplement(p * , p ′ ), p s ≜ {p ′ ∈ D ∪ I \ {p * }; s p ′ ≥ θ}, (4)
where both the relevance and supplement functions are implemented through carefully designed instructions for LLMs.See Appendix for detailed prompt templates.</p>
<p>Premise exploration with three-fold verification.Once selected premises for exploration are determined, we employ LLMs to execute the explore function, which considers combining supplementary premises p s with the primary premise p * to generate a new proposition p, which can be given by: p = explore(p * , p s ).</p>
<p>(5)</p>
<p>Next, the rationality of the newly explored proposition p undergoes rigorous verification, encompassing three-fold critical aspects: (1) Logical validity:</p>
<p>We verify whether the deduction of the selected premises to p is valid in terms of logical reasoning rules;</p>
<p>(2) Useful contribution: We verify whether p is a useful determinate premise that contributes to derive the conclusion.It helps filter out the "correct nonsense" that may be logically valid but fail to enhance the conclusion derivation; (3) Duplication avoidance: We verify whether p provides information gain beyond the original premises, avoiding the generation of mere paraphrases of existing premises.Only propositions that pass all these verification checks will be retained and added to the determinate premise set.The main steps of premise prioritization and exploration can be formulated as:
D ← D ∪ { p}, if verify( p, {p * , p s }) = True, (6)
where the verify function is also implemented by the carefully designed instructions for LLMs, and detailed prompt templates are available in Appendix.Through premise prioritization and exploration, LLMs can effectively prioritize more pertinent premises to explore new insights, improving reasoning effectiveness and efficiency.</p>
<p>Reasoning Memorization</p>
<p>As known conditions dynamically update during the reasoning process, conventional methods often overlook historical reasoning details, resulting in erroneous reasoning directions or stagnant reasoning progress (Yao et al., 2023;Zhang et al., 2023).In contrast, humans generally record previous reasoning steps and retain both successful and failed attempts in mind to continue reasoning.</p>
<p>To bridge this cognitive gap, we design a reasoning memorization module to automate the storage and extraction of available premises and evolving reasoning structures.We initialize the reasoning memory as M = D (0) ∪I (0) , only containing input premises before the reasoning process.Figure 1(c) illustrates an iteration of memory storage and extraction, which is elaborated in detail as follows.</p>
<p>Memory storage.During the t-th iteration of premise exploration, our focus of the reasoning details lies on the new proposition p (t) and the reasoning paths G p (t) that connect the original premises
{p (t) * , p(t)
s } to p (t) .If p (t) passes all verification checks, we denote the reasoning paths as positive G + p (t) and store both p (t) and G + p (t) into the reasoning memory.Otherwise, the reasoning paths will be designated as negative G − p (t) and also stored into the memory as part of the reasoning experiences.This process can be formulated as:
M (t) =    M (t−1) ∪ { p (t) , G + p (t) }, if verify = True M (t−1) ∪ {G − p (t) }, otherwise(7)
Memory extraction.When we consider prioritizing premises in the (t+1)-th iteration, we extract t previous reasoning details from memory to guide LLMs in drawing upon successful experiences and avoiding repetitive mistakes.Following each iteration of premise exploration, it is essential to extract current premises and reasoning paths from memory.This extraction can help accurately verify whether the current determinate information is sufficient to draw the target conclusion.More details about the reasoning memory are available in Appendix.</p>
<p>Overall, the reasoning memory supports both retrospective and prospective reasoning during the iterative process.Retrospectively, it stores historical reasoning details for updating reasoning states.Prospectively, it extracts previous reasoning experiences into future steps, enhancing the accuracy of premise prioritization and exploration.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Datasets.To verify the capability of LLMs to engage in rigorous logical reasoning based solely on established conditions, without external knowledge, we carefully select five challenging logical reasoning benchmarks: (1) LogiQA (Liu et al., 2020) collects the multiple-choice logical problems from National Civil Servants Examination of China.Since it contains different types of questions, we carefully reviewed its test set and retained 179 high-quality questions whose premises are delineated within the context as a curated collection.</p>
<p>(2) ProofWriter (Tafjord et al., 2020) is a widely used logical reasoning benchmark.We use the open-world assumption subset where each case requires to be proven true, false or unknown.We follow Pan et al. (2023) to use the depth-5 subset containing 600 cases for evaluation.( 3) FOLIO (Han et al., 2022) is a challenging benchmark requiring complex first-order logic reasoning to solve.We follow the official data split and choose the validation set containing 204 examples for evaluation.( 4) PrOntoQA (Saparov and He, 2022) is similar to ProofWriter for evaluating logical reasoning.( 5) LogicalDeduction (LD) is a challenging task in BigBench (Srivastava et al., 2022).The problems are mainly about deducing the order of objects In principle, our proposed framework imposes no restrictions on the type of used LLMs.Here we uniformly employ the most advanced GPT-4 (Ope-nAI, 2023) and GPT-3.5-turbo as the base model to test the upper limit of LLM-based logical reasoning.Our implementation is based on the Microsoft guidance library1 .We set the temperature to 0.1 by default and 0.7 for CoT-SC (n = 16).</p>
<p>Main Results</p>
<p>The results presented in Table 1 demonstrate that our proposed DetermLR achieves superior reasoning accuracy with fewer steps compared to other multi-step reasoning methods (CR and ToT).While all methods show improvement over GPT-3.5-turbo by approximately 0.1 accuracy, DetermLR consistently outperforms all baselines even on the same base model.For the most challenging LogiQA, all baselines including CR perform poorly on this task with accuracy below 46.Since the utilization order of known conditions is crucial to solving the exam problem, baseline methods often fail to grasp the accurate reasoning direction.DetermLR performs well by prioritizing and memorizing known conditions and reasoning steps, resulting in an accuracy of 54.19 with GPT-4.Meanwhile, the average number of reasoning steps in DetermLR reaches 11.74, which is more efficient than CoT-SC, ToT and CR in solving real logical reasoning examination problems.For ProofWriter and FOLIO, DetermLR can generate more accurate propositions for the target than CoT-SC, ToT, and CR.Also, DetermLR requires fewer reasoning steps to reach the same conclusion, ensuring more efficient premise integration and exploration.The results in LD shows that compared to all baseline methods, DetermLR can enhance the accuracy of assigning the order of objects, and substantially reduce the number of reasoning iterations.</p>
<p>Ablation Study</p>
<p>In addition to those baselines, we also conduct an ablation study to assess the impact of each component of the proposed method.The ablation variants include: 1) DetermLR w/o identify removes premise identification at the beginning of reasoning; 2) DetermLR w/o priority replaces premise priorities with randomly sampled candi- (a) Case A with 18 original premises.</p>
<p>There are seven people named A, B, C, D, E, F, and G with education levels being Bachelor's and PhD.</p>
<p>Here are the given premises: i1: Among the 7 people, there are 3 PhDs, and 3 of them are female.i2: The educational levels of A, B, and C are the same, while those of F and G are different.i3: The genders of E, F, and G are the same, while the genders of A and D are different.i4: Someone is a female PhD.Target: which of the following can be concluded?a.A is a male PhD.b.F is a female PhD.c. G is not a male PhD.d.D is a female PhD.</p>
<p>Generated determinate premises</p>
<p>Further Analysis</p>
<p>Case study.An intuition suggests that problems with more known conditions and longer contexts tend to require more complex reasoning structures.However, relying solely on this intuition to preset reasoning structures might not always be accurate.As shown in Figure 2, Case A initially appears to be a highly complex problem due to its 18 premises.However, upon prioritizing the premises, we review that the problem's reasoning could be modeled using a concise chain-like reasoning structure with only two steps.This indicates the reasoning structure should not be preset before reasoning but rather formed through review after problem solving.Case B presents only 4 premises, but each premise is complicated and requires to be repeatedly utilized to reach the conclusion.This also indicates that determining a problem's difficulty solely based on the complexity of its context might not always be accurate.More reasoning examples and detailed reasoning processes are available in Appendix.</p>
<p>Impact of the number of determinate premises.</p>
<p>In practice, the required number of generated determinate premises (denoted as n) is a key hyperparameter for our method.To weigh the reasoning effectiveness and efficiency, a larger number may not always be better.As shown in Figure 3, generating more determinate premises will gradually streamline the reasoning process.The reasoning performance of smaller n is comparable to some baselines, and as n increases, our method can achieve the best performance.However, the subsequent increase in n will bring about a significant increase in the number of reasoning iterations, while the improvement in accuracy will be very limited.Therefore, we set n to 4 in our experiment, a position close to the inflection point to trade off effect improvement and efficiency control.</p>
<p>Generation efficiency of determinate premises.</p>
<p>we further investigate the odds of success and failure across multiple reasoning iterations.Therefore, we make a more detailed analysis of the average number of reasoning steps required to gen-   erate a useful proposition.As shown in Table 3, DetermLR outperforms the baseline significantly across all datasets by using the fewest steps to generate each useful new insight.This number could also reflect the average difficulty of the datasets to some extent.For LogiQA, both CR (4.25) and De-termLR (2.63) are much higher than the values on other datasets, which indicates that the analytical reasoning questions in LogiQA are still the most challenging tasks.To validate this hypothesis, we obtain human performance on LogiQA by inviting two volunteers--one, a graduate student with experience in public examinations, completed all questions with an accuracy rate of 73, and the other, a student with no prior exposure to such knowledge, achieved an accuracy rate of 59.</p>
<p>Complexity analysis.</p>
<p>We also conduct a complexity analysis for more detailed efficiency comparisons.We choose ToT and CR as strong multistep reasoning baselines to compute the average inference time for each reasoning step.All experiments in this analysis are performed on the same device for fair comparison.The results in Table 4 show that although the inference time per step for DetermLR is slightly more than CR and ToT, the superiority of DetermLR lies in substantially saving overall required reasoning steps.Therefore, considering the inference time per case, we can see that the overall inference efficiency of DetermLR is still better than ToT and CR.</p>
<p>Error analysis.Current LLM-based reasoning cannot resolve the following errors: (1) Insufficient exploration of implicit conditions: LLMs cannot identify that school roommates have the same gender;</p>
<p>(2) Insufficient understanding of boundary conditions: Three of the five are candidates, the first two and the last two each have one candidate, LLMs cannot assert that the middle one must be the candidate;</p>
<p>(3) Lack of flexible use of logical rules:  Given that A implies B, ¬ A implies B, LLMs cannot assert that B must be true.</p>
<p>Conclusion</p>
<p>In this work, we propose DetermLR, a novel reasoning framework to align LLM-based reasoning more closely resemble human cognitive reasoning.First, we propose a novel perspective that formulates the reasoning process as an evolution from indeterminacy to determinacy.Second, we employ quantitative measurements for premise prioritization and exploration, allowing LLMs to prioritize premises more conducive to exploring new insights.Furthermore, we introduce a reasoning memorization module to preserve historical details during iterative reasoning.Experimental results show that DetermLR can achieve the highest accuracy on multiple logical reasoning benchmarks while requiring fewer reasoning steps.Notably, in more intricate tasks like LogiQA, DetermLR exhibits even more pronounced advancements, mirroring human-like reasoning skills to a greater extent.</p>
<p>While DetermLR exhibits superior performance over baselines across various tasks, challenges persist within LLM-based reasoning.One primary limitation lies in extracting implicit conditions from problem contexts for intricate reasoning tasks.While LLMs can discern intuitively presented conditions, parsing nuanced or implicit information from verbose descriptions remains difficult.In scenarios with few available conditions, exhaustive exploration of each condition is imperative for deriving additional useful conditions.Moreover, LLMs face challenges in accurately parsing exceedingly lengthy condition descriptions, as well as navigating intricate nested if-then conditional structures delineating complex relationships among multiple subjects.Such complexities increase the risk of LLMs erring during the reasoning process.Consequently, enhancing LLMs' proficiency in uncovering implicit conditions and comprehending lengthy and complex condition descriptions emerges as a pivotal research avenue within the domain of LLMbased reasoning.These aspects will be a focal point of our future research directions.</p>
<p>A More Examples on Datasets</p>
<p>A.1 LogicalDeduction</p>
<p>[An example in LogicalDeduction]</p>
<p>Context: The following paragraphs each describe a set of seven objects arranged in a fixed order.The statements are logically consistent within each paragraph.In a golf tournament, there were seven golfers: Ana, Eli, Mya, Amy, Joe, Mel, and Ada.Ada finished above Mel.Ada finished third-to-last.Amy finished above Ana.Mya finished second-to-last.Joe finished above Amy.Eli finished below Ana.Ana finished above Ada.Ana finished above Mya.</p>
<p>Question: Which of the following is true?Options: A) Ana finished third.B) Eli finished third.C) Mya finished third.D) Amy finished third.E) Joe finished third.F) Mel finished third.G) Ada finished third.</p>
<p>Answer: A</p>
<p>[Chain-of-Thought Reasoning by GPT-4] Conclusion:</p>
<p>• From the premises, we know that: 1. Ada finished third-to-last, which means Ada finished 6th.</p>
<ol>
<li>Mya finished second-to-last, which means Mya finished 6th. 3. Joe finished above Amy.4. Amy finished above Ana. 5. Eli finished below Ana. 6. Ada finished above Mel.Now, let's try to arrange the golfers based on the given information: 1. Joe 2. Ana 3. Amy 4. Eli 5. -6.Mya and Ada 7. Mel From the arrangement, we can see that Amy finished third.So, the answer is D) Amy finished third.</li>
</ol>
<p>[Prediction: D] (Wrong)</p>
<p>[Cumulative Reasoning by GPT-4] Generated Propositions:</p>
<p>• Mel finished last.</p>
<p>• Ana finished fifth.</p>
<p>• Eli finished sixth.</p>
<p>• Mel finished last.Ada finished fifth.</p>
<p>• Amy finished above Ana and below Joe.</p>
<p>• Joe finished third.</p>
<p>Conclusion:</p>
<p>• From the propositions, we know that Joe finished third.So the answer is E.</p>
<p>[Prediction: E] (Wrong)</p>
<p>[DetermLR by  Generated Determinate Premises:</p>
<p>• Mya did not finish first.</p>
<p>• Ada did not finish first or second.</p>
<p>• Joe finished above Ana.</p>
<p>• Amy did not finish third-to-last.</p>
<p>• Amy finished above Eli.</p>
<p>• Amy finished second.Joe finished first.</p>
<p>• Ada did not finish first-to-third.</p>
<p>• Eli did not finish first-to-third.</p>
<p>Reasoning Details:</p>
<p>1.In the NO:1 round, we use these "premises": "Amy finished above Ana.Mya finished secondto-last.Ada finished above Mel.Eli finished below Ana.Joe finished above Amy." and got a "New Determinate Premise": "Joe finished above Ana"</p>
<ol>
<li>
<p>In the NO:2 round, we use these "premises": " Amy finished above Ana.Eli finished below Ana." and got a "New Determinate Premise": "Amy finished above Eli."</p>
</li>
<li>
<p>In the NO:3 round, we use these "premises": " Ada finished third-to-last.Amy finished above Ana." and got a "New Determinate Premise": "Amy did not finish third-to-last."</p>
</li>
<li>
<p>In the NO:4 round, we use these "premises": "Amy finished first or second.Joe finished above Amy." and got a "New Determinate Premise": "Amy finished second.Joe finished first."</p>
</li>
<li>
<p>In the NO:5 round, we use these "premises": " Ada finished third-to-last.Ada finished above Mel." and got a "New Determinate Premise": "Ada did not finish first-to-third."</p>
</li>
</ol>
<p>6.In the NO:6 round, we use these "premises": " Eli finished below Ana.Ana did not finish first or second." and got a "New Determinate Premise": "Eli did not finish first-to-third."</p>
<p>Conclusion:</p>
<p>• From the premises and propositions, we know that Ada finished third-to-last, Ana finished above Ada, Ana finished above Mya, which means she finished fifth.Mya finished second-to-last, which means she finished sixth.Since there are only seven golfers, this means that Joe must have finished first, as he finished above Amy, who finished above Ana, who finished above Eli.With Joe finishing first, Amy must have finished second, since she finished above Ana and Eli.Ana would be third, Eli would be fourth, and Mel would be the last (seventh).So the answer is A) Ana finished third.</p>
<p>[Prediction: A] (Right)</p>
<p>A.2 LogiQA</p>
<p>[An example in LogiQA]</p>
<p>Context: There are seven people named A, B, C, D, E, F, and G with education levels being Bachelor's and PhD.Here are the given premises: 1): Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.2) The educational levels of A, B, and C are the same, while those of F and G are different.</p>
<p>3) The genders of E, F, and G are the same, while the genders of A and D are different.4) The final hire is a female PhD.</p>
<p>Question: Based on the statements above, which of the following can be concluded?Options: A. A is a male PhD.B. F is a female PhD.C. G is not a male PhD.D. D is a female PhD.</p>
<p>Answer: D</p>
<p>[Chain-of-Thought Reasoning by GPT-4] Conclusion:</p>
<p>• From the premise, it can be inferred that the genders of E, F, and G are female.Since the final appointment is a female Ph.D. holder, the selection must be made from E, F, and G.The educational levels of A, B, and D are the same, so they cannot be Ph.D. holders because there are only 3 Ph.D. holders.Therefore, there must be two Ph.D. holders among E, F, and G. Since the educational levels of F and G are different, one of them must be a Ph.D. holder, and E is also a Ph.D. holder.Therefore, one of E, F, and G is a female Ph.D. holder.The genders of A and C are different, and since E, F, and G are females, either A or C must be male.Among options A, B, C, and D, option B is correct because F is a female Ph.D. holder.The answer is B.</p>
<p>[Prediction: B] (Wrong)</p>
<p>[Cumulative Reasoning by GPT-4] Generated Propositions</p>
<p>• The genders of D,E,F are female.</p>
<p>• A,B,C are undergraduates.</p>
<p>Conclusion:</p>
<p>• From the proposition, we can deduce that E, F, and G are females.A and C have different genders, so A and C are males.A, B, and D have the same educational background, which means they are either all Ph.D. holders or all undergraduates.However, there are only 3 Ph.D. holders, so A, B, and D must all be undergraduates.This means that E, F, and G include two Ph.D. holders.Since the final appointment is a female Ph.D. holder, it means that either E or F must be a Ph.D. holder.However, the educational levels of F and G are different, so G is an undergraduate.Therefore, both E and F are Ph.D. holders.Therefore, the final answer is: A is a male undergraduate; C is a male undergraduate; E is a female Ph.D. holder; F is a female Ph.D. holder; G is a female undergraduate.The answer is C.</p>
<p>[Prediction: C] (Wrong)</p>
<p>[DetermLR by  Generated Determinate Premises:</p>
<p>• A, B, and C have Bachelor's degrees.</p>
<p>• E, F, and G are all males.</p>
<p>• B and C are females.</p>
<p>• D and E are both PhD.holders.</p>
<p>• C is a female undergraduate.</p>
<p>• E is a male PhD.holder.</p>
<p>• D is a female PhD.holder.</p>
<p>Reasoning Details:</p>
<p>1.In the NO:1 round, we use these "premises": "Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.The educational levels of A, B, and C are the same, while those of F and G are different."and got a "New Determinate Premise": "A, B, and C have Bachelor's degrees."</p>
<ol>
<li>
<p>In the NO:2 round, we use these "premises": " Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.The genders of E, F, and G are the same, while the genders of A and D are different."and got a "New Determinate Premise": "E, F, and G are all males."</p>
</li>
<li>
<p>In the NO:3 round, we use these "premises": " Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.The genders of E, F, and G are the same, while the genders of A and D are different.E, F, and G are all males" and got a "New Determinate Premise": "B and C are females."</p>
</li>
<li>
<p>In the NO:4 round, we use these "premises": "Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.The genders of E, F, and G are the same, while the genders of A and D are different.A, B, and C have Bachelor's degrees." and got a "New Determinate Premise": "D and E are both PhD.holders."</p>
</li>
<li>
<p>In the NO:5 round, we use these "premises": "B and C are females.A, B, and C have Bachelor's degrees." and got a "New Determinate Premise": "C is a female undergraduate."</p>
</li>
</ol>
<p>6.In the NO:6 round, we use these "premises":" D and E are both PhD.Holders.E, F, and G are all males " and got a "New Determinate Premise": "E is a male PhD.holder."In the NO:7 round, we use these "premises": "The educational levels of A, B, and C are the same, while those of F and G are different.The final hire is a female PhD.E is a male PhD.holder."and got a "New Determinate Premise": " D is a female PhD.holder"</p>
<p>Conclusion:</p>
<p>• From the propositions, we know the answer is D. D is a female PhD.</p>
<p>[Prediction: D] (Right)</p>
<p>A.3 ProofWriter</p>
<p>[An example in ProofWriter]</p>
<p>Context: The bald eagle chases the cat.The bald eagle is kind.The bald eagle sees the dog.The cat is rough.The dog is blue.The dog is red.The dog is rough.The dog sees the cat.The dog does not see the lion.The lion sees the bald eagle.If something chases the cat then the cat chases the dog.If something chases the dog then it eats the bald eagle.If something is kind and it sees the lion then it is not red.If something chases the dog then the dog chases the bald eagle.If the dog eats the bald eagle then the dog sees the bald eagle.If something is cold then it sees the lion.If something sees the cat and it is blue then the cat is kind.If something is blue and it chases the bald eagle then the bald eagle is cold.</p>
<p>Question:Based on the above information, is the following statement true, false, or uncertain?The cat eats the bald eagle.[Prediction: C] (Wrong)</p>
<p>[Cumulative Reasoning by GPT-4] Generated Propositions:</p>
<p>• The bald eagle does not eat the dog.</p>
<p>• If the bald eagle sees the lion, it is not red.</p>
<p>• The bald eagle is not red.</p>
<p>• If the dog chases the cat, then the cat chases the dog.</p>
<p>• The dog does not eat the bald eagle.</p>
<p>Conclusion:</p>
<p>• We can not find any direct or indirect information about the cat eating the bald eagle.The information provided in the premises does not support or contradict the hypothesis.Judgement: Now we know that the Hypothesis is Unknown.</p>
<p>[Prediction: C] (Wrong)</p>
<p>[DetermLR Reasoning by GPT-4] Generated Determinate Premises:</p>
<p>1.The cat chases the dog.</p>
<ol>
<li>
<p>The dog chases the bald eagle.</p>
</li>
<li>
<p>Something chases the cat.</p>
</li>
<li>
<p>The bald eagle is something that chases the cat.</p>
</li>
<li>
<p>The dog and the lion both see the bald eagle.</p>
</li>
</ol>
<p>6.If the cat chases the dog, then the cat eats the bald eagle.</p>
<ol>
<li>The cat eats the bald eagle.</li>
</ol>
<p>Reasoning Details:</p>
<p>• In the NO:1 round, we use these "premises": "If something chases the cat then the cat chases the dog.The bald eagle chases the cat."and got a "New Determinate Premise": "The cat chases the dog."</p>
<p>• In the NO:2 round, we use these "premise": "If something chases the dog then the dog chases the bald eagle.If something chases the cat then the cat chases the dog.The dog sees the cat.The cat chases the dog.If something chases the dog then it eats the bald eagle.The bald eagle chases the cat.The cat is rough."andgot a "New Determinate Premise": "The dog chases the bald eagle."</p>
<p>• In the NO:3 round, we use these "premises": "If something chases the cat then the cat chases the dog.The cat chases the dog.If something chases the dog then it eats the bald eagle.The bald eagle chases the cat."and got a "New Determinate Premise": "Something chases the cat."</p>
<p>• In the NO:4 round, we use these "premises": "Something chases the cat.The bald eagle chases the cat."and got a "New Determinate Premise": "The bald eagle is something that chases the cat."</p>
<p>• In the NO:5 round, we use these "premises": "The dog chases the bald eagle.The bald eagle is something that chases the cat.The bald eagle chases the cat.The bald eagle is kind.The bald eagle sees the dog.The lion sees the bald eagle."and got a "New Determinate Premise": "The dog and the lion both see the bald eagle."</p>
<p>• In the NO:6 round, we use these "premises": "The dog chases the bald eagle.The bald eagle sees the dog.If something chases the dog then the dog chases the bald eagle.If something chases the dog then it eats the bald eagle.The dog is blue."and got a "New Determinate Premise": "If the cat chases the dog, then the cat eats the bald eagle."</p>
<p>• In the NO:7 round, we use these "premises": "The cat chases the dog.The bald eagle is something that chases the cat.If something chases the dog then it eats the bald eagle.If something chases the cat then the cat chases the dog.If the cat chases the dog, then the cat eats the bald eagle.The dog chases the bald eagle.The bald eagle chases the cat."and got a "New Determinate Premise": "The cat eats the bald eagle."</p>
<p>Conclusion:</p>
<p>From the propositions, we know the cat eats the bald eagle.So the answer is true.Question: Based on the above information, is the following statement true, false, or uncertain?1984 is not a streaming service.</p>
<p>Options: A) True B) False C) Uncertain Answer: A [Chain-of-Thought Reasoning by  Conclusion:</p>
<p>• From the premises, we cannot directly deduce whether the hypothesis 1984 is not a streaming service is true or false.The premises provide information about the different media formats, but there is no explicit information about the format of 1984.So the answer is C.</p>
<p>[Prediction: C] (Wrong)</p>
<p>[Cumulative Reasoning by GPT-4] Generated Propositions:</p>
<p>• If 1984 is a streaming service, then it is digital.</p>
<p>• If 1984 is a streaming service, then it is not analog.</p>
<p>Conclusion:</p>
<p>• The premises state that if 1984 is a streaming service, then it is a hardcover book.However, no information is provided about whether 1984 is a streaming service or a hardcover book.Since we lack information to confirm or deny the hypothesis, we cannot deduce a conclusion from the given premises.Judgement: Now we know that the Hypothesis is Unknown.So the answer is C.</p>
<p>[Prediction: C] (Wrong)</p>
<p>Figure 1 :
1
Figure 1: The overview of DetermLR: (a) premise identification; (b) iterative reasoning process: (b-1) premise prioritization and exploration and (b-2) reasoning memorization.Green elements represent determinate premises, and blue elements represent indeterminate premises.The proportion of blue decreases with the accumulation of green during iterative reasoning.</p>
<p>lion sees the bald eagle.d2 The bald eagle is kind.d3:The bald eagle sees the dog.d4:The cat is rough.d5:The dog sees the cat.d6:The bald eagle chases the cat.i1:The dog is blue.i2:The dog is red.i3:The dog is rough.i4:The dog does not see the lion.i5:If one chases the cat, then the cat chases the dog.i6:If one is blue and sees the cat, then the cat is kind.i7:If one is kind and sees the lion, then it is not red.i8:If one chases the dog, then the dog chases the bald eagle.i9:If the dog eats the bald eagle, then the dog sees the bald eagle.i10:If one is cold, then it sees the lion.i11:If one chases the dog then it eats the bald eagle.i12:If one is blue and chases the bald eagle then the bald eagle is cold.Target: The cat eats the bald eagle.Is this statement true, false, or unknown?a. True, b.False, c. Unknown Generated determinate premises: d7:The cat chases the dog.d8:The cat eats the bald eagle.Conclusion: The answer is a.</p>
<p>Figure 2 :
2
Figure 2: Two cases for contrasting reasoning structure and context complexity.Green dots with d represent determinate premises and blue dots with i represent indeterminate premises.</p>
<p>Figure 3 :
3
Figure 3: The impact of the number of generated determinate premises.</p>
<p>•</p>
<p>The bald eagle chases the cat.2.If something chases the cat, then the cat chases the dog.(From 1, the cat chases the dog)3.If something chases the dog, then it eats the bald eagle.(We don't have information about something chasing the dog) 4. The cat eats the bald eagle.(We don't have information about the cat eating the bald eagle)Recall the Hypothesis: The cat eats the bald eagle is Unknown.</p>
<p>No digital media are analog.Every printed text is analog.All streaming services are digital.If an object is a hardcover book then it is printed text.If 1984 is a streaming service, then 1984 is a hardcover book.</p>
<p>Reasoning Completion (a) Premise Identification LLM Premise Identifier Determinate premises: d1) Gary is young. d2) Gary is nice. d3) Gary is angry. LLM Relevance Scorer Most Relevant Premise Other Premises LLM Supplement Scorer LLM Proposition Explorer LLM Verification Determinate premises: d1) Gary is young. d2) Gary is nice. d3) Gary is angry. d4) Gary is quiet. d5) Gary is furry. d6) Gary is white. d7) Gary is big. d8) Gary is round. Indeterminate premises: i2) All nice people are big. i7) If Erin is round, then Erin is quiet. Conclusion: The answer is False. (b-2) Reasoning Memorization
Indeterminate premises:i1) White and furry peopleare big.i2) All nice people are big.i3) If someone is quiet andfurry, then they are white.i4) If someone is young, thenthey are quiet.i5) All big and furry peopleare round.i6) If someone is quiet, thenthey are furry.i7) If Erin is round, then Erinis quiet.Target: Gary is not round.(True/Fasle/Unknown)</p>
<p>Input Premises Input Target Selected Premises New Proposition Memory Extraction (b-1) Premise Prioritization and Exploration Memory Storage ×N Memory Update Yes No (b) Iterative Reasoning Process</p>
<p>Table 1 :
1
Comparison results on LogiQA, ProofWriter, FOLIO, and LogicalDeduction.Bold numbers highlight the highest accuracy and the fewest steps among multi-step methods.Avg.Steps ↓ Accuracy ↑ Avg.Steps ↓ Accuracy ↑ Avg.Steps ↓ Accuracy ↑ Avg.Steps ↓ Accuracy ↑ Avg.Steps ↓
LogiQAProofWriterFOLIOPrOntoQALDModel Accuracy ↑ GPT-3.5-turbo Method Standard 16.76 CoT 22.35 CoT-SC 22.91 SI 24.02 LAMBADA 24.021 1 16 15.16 59.3236.17 45.00 48.67 50.17 55.171 1 16 18.49 16.8949.51 54.41 57.34 57.84 60.291 1 16 14.19 12.3551.80 84.00 86.80 88.60 90.801 1 16 13.58 12.0941.33 46.00 50.33 51.00 62.671 1 16 17.24 74.43ToT26.2519.8754.1624.8859.8019.8291.2019.3066.3323.71CR31.8418.9359.1618.8159.8018.9692.4016.9371.0018.32DetermLR37.9913.3968.8316.5263.7210.3793.2010.7474.3313.19Standard31.69146.83160.29177.40171.331CoT38.55167.41167.65191.00173.331CoT-SC40.431669.331668.141693.401674.6716GPT-4SI LAMBADA41.34 39.1114.35 56.2470.67 72.0017.46 15.0469.11 70.1013.76 10.8593.80 95.6011.38 10.5676.33 78.0014.95 67.32ToT43.0219.8770.3324.5769.1219.1297.6018.9176.8321.83CR45.2517.0071.6716.7669.1115.8798.2014.1878.3316.98DetermLR54.1911.7479.1714.6375.498.5798.609.7885.0012.35from a set of conditions. We use the full test setcontaining 300 examples for evaluation. Logicalreasoning examples for each task are available inAppendix.
(Kazemi et al., 2022)our DetermLR with existing LLM-based reasoning methods, we choose the following baselines: 1) Standard prompting directly answers the question based on in-context examples; 2) CoT(Wei et al., 2022)adopts stepby-step generation of indeterminate rationales before the final answer; 3) CoT-SC(Wang et al., 2022b)uses majority voting to aggregate multiple CoTs; 4) SI(Creswell et al., 2022b)uses selection-inference patterns for iterative reasoning; 5) LAMBADA(Kazemi et al., 2022)performs backward chaining for automated reasoning tasks; 6) ToT (Yao et al., 2023) models the reasoning process as a thought search tree; 7) CR (Zhang et al., 2023) is recently proposed to generate new propositions based on available premises.</p>
<p>Table 2 :
2
Ablation results: accuracy (first row) and average reasoning steps (second row in parentheses).
MethodLogiQA ProofWriter FOLIOLDDetermLR w/o identify46.15 (17.24)71.50 (16.58)69.61 (13.70) (16.84) 79.00DetermLR w/o priority47.83 (18.35)72.32 (17.21)70.59 (14.69) (17.02) 80.33DetermLR w/o memory39.66 (11.98)68.33 (14.79)67.65 (8.65) (13.05) 76.67DetermLR54.19 (11.74)79.17 (14.63)75.49 (8.57) (12.35) 85.00date premises for exploration; 3) DetermLR w/omemory removes our memorization module dur-ing iterative reasoning. The results demonstrate theimportance of premise identification, prioritizationand exploration, and iterative reasoning memoriza-tion modules. DetermLR w/o identify blurs thetransformation process in reasoning, resulting inreduced accuracy and more reasoning steps. SinceDetermLR w/o priority cannot capture the reason-ing direction, it requires more reasoning steps stillto achieve a lower accuracy than the full model.This emphasizes that prioritizing premise hierar-chy can significantly improve reasoning efficiency.Without consideration on reasoning memory, theaccuracy of DetermLR w/o memory decreases byat least 7.84, indicating the importance of record-ing the inference structure and prompting LLMs torecall previously acquired information.</p>
<p>Table 3 :
3
The average number of reasoning steps per generated determinate premise.
MethodLogiQA ProofWriter LD FOLIOToT4.974.914.374.78CR4.253.353.403.97DetermLR w/o identify4.313.323.373.43DetermLR w/o priority4.593.443.403.67DetermLR w/o memory2.732.242.071.99DetermLR2.632.172.031.83</p>
<p>Table 4 :
4
Comparison results of inference efficiency.
MethodAvg. steps Inference time Inference time per case per step per caseToT18.407.77s142.93sCR14.516.86s99.69sDetermLR7.698.05s61.98s</p>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman.2017.A broad-coverage challenge corpus for sentence understanding through inference.arXiv preprint arXiv:1704.05426.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,Thomas L Griffiths, Yuan Cao, and KarthikNarasimhan. 2023. Tree of thoughts: Deliberateproblem solving with large language models. arXivpreprint arXiv:2305.10601.Yifan Zhang, Jingqin Yang, Yang Yuan, and An-drew Chi-Chih Yao. 2023. Cumulative reason-ing with large language models. arXiv preprintarXiv:2308.04371.Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, et al. 2023. Asurvey of large language models. arXiv preprintarXiv:2303.18223.Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,Nathan Scales, Xuezhi Wang, Dale Schuurmans,Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022.Least-to-most prompting enables complex reason-ing in large language models. arXiv preprintarXiv:2205.10625.
https://github.com/guidance-ai/guidance.
Generated Determinate Premises:• If 1984 is a streaming service, then 1984 is digital.• All streaming services are not analog.• If 1984 is a streaming service, then it cannot be a hardcover book.Reasoning Details:1.In the NO:1 round, we use these "premises": "If 1984 is a streaming service, then 1984 is a hardcover book.All streaming services are digital."and got a "New Determinate Premise": "If 1984 is a streaming service, then 1984 is digital."2. In the NO:2 round, we use these "premises": "No digital media are analog.All streaming services are digital.If 1984 is a streaming service, then 1984 is a hardcover book.If 1984 is a streaming service, then 1984 is digital."and got a "New Determinate Premise": "All streaming services are not analog."3. In the NO:3 round, we use these "premises": "If 1984 is a streaming service, then 1984 is digital.If 1984 is a streaming service, then 1984 is a hardcover book.All streaming services are digital.No digital media are analog.All streaming services are not analog."and got a "New Determinate Premise": "If 1984 is a streaming service, then it cannot be a hardcover book."Conclusion:• We know from the premises that if 1984 is a streaming service, then 1984 is a hardcover book.However, we also deduced that if 1984 is a streaming service, then it cannot be a hardcover book, as it would be digital and not analog.This contradiction leads us to conclude that 1984 is not a streaming service.Judgement: Now we know that the Hypothesis is True.[Prediction: A] (Right)B Prompt TemplatesDifferent logical reasoning tasks have different data formats.Among them, ProofWriter and FOLIO offer explicitly known premises, while LogiQA and LogicalDeduction require parsing the question context to extract the premises, which indicates that extra steps such as premise transformation are necessary.We use ProofWriter and LogicalDeduction as representative tasks to illustrate the prompt templates.More details about prompt design are available in our Github repository.B.1 ProofWriterBased on the modeling scheme introduced by our DetermLR, we summarize main designed prompts into several parts such as premise identification, premise prioritization, premise exploration, logical validation, and final conclusion.Read the "Last reasoning history".If we got a "false Proposition" in history,when you select "Most relevant premise",do not choose the same "Most relevant premise" in history as your answer.Please follow these steps: 1.From the determinate premise, select the "Most relevant premise" which has the same subject with "Hypothesis", and give a score from 0 to 1. 2.You need to assess how the "Most relevant premise" relates to all the other "determinate premise" and "indeterminate premise",based on Relevance scoring rules.3.The "determinate premise" and "indeterminate premise" with scores higher than 0.25 will be used as the final results, along with Most relevant premise.Relevance scoring rules: 1.When scoring relevance, 0.25 added for each noun or 0.3 added for each adjective that is the same between two sentences.2. Scores start to accumulate from 0 points, and the upper limit is 1 point.1. "Two premises": "If A,then B. A is true." then "Proposition": "B is true." 2. "Two premises": "If A,then B. B is not true."then "Proposition": "A is not true" 3. "Two premises": "A is either C or D. A is not C." then "Proposition": "A is D." Please make sure that the "Proposition" is logically correct.Please make sure that the "Proposition" is not a duplicate of the "Premises".Please make sure your reasoning is directly deduced from the "Premises" and "Propositions" other than introducing unsourced common knowledge and unsourced information by common sense reasoning.Please remember that your "Proposition" should be useful to determine whether the "Hypothesis" is True, False or Unknown.--{{system}} {{ #each examples}} {{#user}} -"Premises": "{{this.premises}}"We want to deduce more propositions to determine the correctness of the following "Hypothesis": "Hypothesis": "{{this.conclusion}}"Can you deduce a new "Proposition" from at least two given "Premises"?{{user}} {{#assistant}}"Proposition": "{{this.proposition}}"{{assistant}}{{˜each}} {{#user}} -"premises": "{{premises}}" "boundary condition": "{{boundary condition}}" We want to derive more propositions to solve the following question: "question": "{{question}}" Combined with boundary conditions, can you derive a new "proposition" from at least two given "premises"?{{user}} {{#assistant}}"proposition": "{{assistant}} {{#assistant}}{{gen "proposition" temperature=temperature max tokens=100 stop=' '}}{{assistant}} Prompts used for Logical Validity {{#system}}Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please use the Logical Reasoning Rules(LRR) to determine whether the deduction of the given "Premises" to a "Proposition" is valid or not, reply with True or False.Logical Reasoning Rules(LRR): 1. "Two premises": "If A,then B. A is true." then "Proposition": "B is true." 2. "Two premises": "If A,then B. If B,then C." then "Proposition": "If A, then C." 3. "Two premises": "If A,then B. B is not true."then "Proposition": "A is not true" 4. "Two premises": "A is either C or D. A is not C." then "Proposition": "A is D." -{{system}} {{ #each examples}} {{#user}} -"Premises": "{{this.premises}}""Proposition": "{{this.proposition}}"{{user}} {{#assistant}}"Judgement": "Is this deduction valid?{{this.validation}}"{{assistant}}{{˜each}} Prompts used for Final Conclusion {{#system}}Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.First read and analyze the "paragraphs" and "questions", then use the "premises", "boundary conditions" and "propositions" to reason which of the options given is the answer to the "question".Make sure that your reasoning is derived directly from "premises" and "propositions" rather than introducing passive common sense and passive information through common sense reasoning.Please note that this is a single choice question.If you can get the answer directly from the proposition, then you should choose the answer directly, otherwise keep reasoning with the proposition, premises, and boundary conditions until you arrive at a single answer.-{{system}} {{ #each examples}} {{#user}} -"context": "{{context}}" "question and options": "{{question}}" {{user}} {{#assistant}}"Premises": "Let's think step by step, and from the context we can extract these premises: {{premises}}"{{assistant}} {{#assistant}}"Boundary condition": "Let's think step by step, and from the context we can extract these boundary conditions: {{boundary condition}}"{{assistant}} {{#assistant}}"Thoughts": "Let us think step by step.From the premises, we can deduce propositions:{{propositions}}"{{assistant}} {{#assistant}}"Recall the reasoning history":"{{infer history}}"{{assistant}} {{#assistant}}"Recall the questions and options":"{{question}}"{{assistant}} {{#assistant}}"Reasoning": "Using premises, boundary conditions, and continuing to reason according to the propositions already obtained,{{assistant}} {{#assistant}}{{gen "reasoning" temperature=0.7 max tokens=500 stop=[' ']}}{{assistant}} {{#assistant}}"Recall the questions and options":"{{question}}"{{assistant}} {{#assistant}}"Judgement": "Now we know that the answer to this question should be{{assistant}} {{#assistant}}{{select "judgement" options=choose}}{{assistant}}B.2 LogicalDeductionIn addition to the prompting steps mentioned above, we also include premise extraction and premise transformation to parse the available premises from the original question.Prompts used for Premise Identification {{#system}}Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.First, read and analyze the following definition: Determinate premise: The premise contains the same noun or adjective as the Hypothesis, and the premise is not in the structure of "if..." or "if...then...".Second, read and analyze the "Premise" and "Hypothesis" .Judge "Premise" is "determinate premise" or not.Third, please make sure your classification decisions are derived directly from definitions, rather than unsourced common sense.-{{system}} {{ #each examples}} {{#user}} -"Premise": "{{this.Premise}}" "Hypothesis": "{{this.Hypothesis}}" {{/user}} {{#assistant}}"Judgement":"Is this "Premise" a "determinate premise" or not?{{this.usefulness}}"{{/assistant}} {{#assistant}}"Explanation": {{this.Explanation}}{{assistant}} {{ /each}} Prompts used for Premise Prioritization {{#system}}Suppose you are one of the greatest artificial intelligence scientists, logicians, and mathematicians.Let's think about it step by step.First read and analyze the "determinate premises" and "indeterminate premises", and then filter out several premises.When you decide on a variable, read through the inference history first and don't choose a variable that has failed before as your choice for this round.Please follow these steps: 1. Count the cumulative number of times each variable is mentioned by "determinate premises" and "indeterminate premises".2. Determine the variable according to the number of mentions from high to low.If the number of mentions is the same, the variable with more prerequisites will be given priority.3. Determine whether the value of the variable has been determined under the current variable.If it is determined, search and determine the next variable in order from most to least.If it has not been completely determined, go to step 4. 4. Use this variable as a criterion for screening "premises" and filter out all premises related to this variable.-{{system}} {{ #each examples}} {{#user}} -"determinate premise": "{{determinate premise}}" "indeterminate premise": "{{indeterminate premise}}" "topic": "{{topic}}" "boundary condition": "{{boundary condition}}" "Inference history": "{{last false history}}" {{user}} {{#assistant}}Can you count the cumulative number of times each variable is mentioned by the premises?{{assistant}}{{#assistant}}"Count": "{{assistant}} {{#assistant}}{{gen "count" temperature=temperature max tokens=200 stop=' '}}{{assistant}} {{#assistant}}Which variable should you choose as the criterion for premises screening?{{assistant}}{{#assistant}}"Explanation": "{{assistant}} {{#assistant}}{{gen "explanation" temperature=temperature max tokens=200 stop=' '}}{{assistant}} {{#assistant}}What are all the premises related to this variable?{{assistant}}{{#assistant}}"Results": "{{assistant}} {{#assistant}}{{gen "results" temperature=temperature max tokens=200 stop=' '}}{{assistant}} Prompts used for Premise Exploration {{#system}}Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please use Logical Reasoning Rules(LRR) to deduce a "Proposition" from two given "Premises" and the proposition does not include "if".Logical Reasoning Rules(LRR): 1. "Two premises": "If A,then B. A is true." then "Proposition": "B is true." 2. "Two premises": "If A,then B. B is not true."then "Proposition": "A is not true" 3. "Two premises": "A is either C or D. A is not C." then "Proposition": "A is D." Please make sure that the "Proposition" is logically correct.Please make sure that the "Proposition" is not a duplicate of the "Premises".Please make sure your reasoning is directly deduced from the "Premises" and "Propositions" other than introducing unsourced common knowledge and unsourced information by common sense reasoning.Please remember that your "Proposition" should be useful to determine whether the "Hypothesis" is True, False or Unknown.--{{system}} {{ #each examples}} {{#user}} -"Premises": "{{this.premises}}"We want to deduce more propositions to determine the correctness of the following "Hypothesis": "Hypothesis": "{{this.conclusion}}"Can you deduce a new "Proposition" from at least two given "Premises"?{{user}} {{#assistant}}"Proposition": "{{this.proposition}}"{{assistant}}{{˜each}} {{#user}} -"premises": "{{premises}}" "boundary condition": "{{boundary condition}}" We want to derive more propositions to solve the following question: "question": "{{question}}" Combined with boundary conditions, can you derive a new "proposition" from at least two given "premises"?{{user}} {{#assistant}}"proposition": "{{assistant}} {{#assistant}}{{gen "proposition" temperature=temperature max tokens=100 stop=' '}}{{assistant}} Prompts used for Logical Validation {{#system}}Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please use the Logical Reasoning Rules(LRR) to determine whether the deduction of the given "Premises" to a "Proposition" is valid or not, reply with True or False.Logical Reasoning Rules(LRR): 1. "Two premises": "If A,then B. A is true." then "Proposition": "B is true." 2. "Two premises": "If A,then B. If B,then C." then "Proposition": "If A, then C." 3. "Two premises": "If A,then B. B is not true."then "Proposition": "A is not true" 4. "Two premises": "A is either C or D. A is not C." then "Proposition": "A is D." -{{system}} {{ #each examples}} {{#user}} -"Premises": "{{this.premises}}""Proposition": "{{this.proposition}}"{{user}} {{#assistant}}"Judgement": "Is this deduction valid?{{this.validation}}"{{assistant}}{{˜each}} Prompts used for Boundary Validation {{#system}}Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.Answer "True" or "False" to determine whether the existing premises plus a new premise satisfies the boundary condition.-{{system}} {{ #each examples}} {{#user}} -"existing premises": "{{this.premises}}""new premise": "{{this.newpremise}}" "boundary condition": "{{this.boundarycondition}}" After adding the new premise to the existing premise, does it still meet the boundary conditions?{{user}} {{#assistant}}"Judgement": "{{this.judgement}}"{{assistant}}{{˜each}} {{#user}} -"existing premises": "{{premises}}" "new premise": "{{proposition}}" "boundary condition": "{{boundary condition}}" After adding the new premise to the existing premise, does it still meet the boundary conditions?{{user}} {{#assistant}}"Judgement": "{{assistant}} {{#assistant}}{{select "judgement" options=valid duplicated}}{{assistant}} Prompts used for Premise Transformation {{#system}} Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.First, please read and analyze the "existing premises", read the definition of transformation; Transformation: In the one-to-one relationship, when the value of the current variable is determined, it means that this variable can not take other values, and other variables can not take the current value, this reasoning process is transformation.Check whether relying on a single "premise" and "boundary condition" can translate into other new premises?The new premises should not duplicate any of the existing premises.If it can be transformed, give the new premises you have deduced; if it can't, answer "None."Make sure that the new premises you get are helpful in solving the problem.-{{system}} {{ #each examples}} {{#user}} -"existing premises": "{{this.premises}}""question": "{{this.question}}""premise": "{{this.premise}}""boundary condition": "{{this.boundarycondition}}" {{user}} {{#assistant}}Can you derive a new premise based on the premises and boundary condition that help solve the problem?{{assistant}} {{#assistant}}"new premise": "{{this.newpremise}}"{{assistant}} {{˜each}} {{#user}} -"existing premises": "{{premises}}" "question": "{{question}}" "premise": "{{premise}}" "boundary condition": "{{boundary condition}}" {{user}} {{#assistant}}Can you derive a new premise based on the premises and boundary condition that help solve the problem?{{assistant}} {{#assistant}}"new premise": "{{assistant}} {{#assistant}}{{gen "premise" temperature=temperature max tokens=50 stop=[' ']}}{{assistant}} Prompts used for Premise Extraction {{#system}} Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.First read and analyze the two sets of definitions defined below; Premise: A constraint on the absolute position of an object or on the relative relationship between two objects.Boundary condition: A description of the number of objects and the name of the object.According to the above definition, summarize the core topics discussed in the following paragraphs and extract the premise and boundary conditions in the context.-{{system}} {{ #each examples}} {{#user}} -"context": "{{this.context}}"{{user}} {{#assistant}}Can you summarize the core topics of the discussion from the context above?{{assistant}} {{#assistant}}"topic": "{{this.topic}}"{{assistant}}{{#assistant}}Can you extract the premise from the context above?{{assistant}} {{#assistant}}"premise": "{{this.premise}}"{{assistant}}{{#assistant}}Can you extract the boundary conditions from the context above?{{assistant}} {{#assistant}}"boundary condition": "{{this.boundarycondition}}"{{assistant}} {{˜each}} {{#user}} -"context": "{{context}}" {{user}} {{#assistant}}Can you summarize the core topics of the discussion from the context above?{{assistant}} {{#assistant}}"topic": "{{assistant}} {{#assistant}}{{gen "topic" temperature=temperature max tokens=50 stop=' '}}{{assistant}} {{#assistant}}Can you extract the premise from the context above?{{assistant}} {{#assistant}}"premise": "{{assistant}} {{#assistant}}{{gen "premise" temperature=temperature max tokens=300 stop=[' n '']}}{{assistant}} {{#assistant}}Can you extract the boundary conditions from the context above?{{assistant}} {{#assistant}}"boundary condition": "{{assistant}} {{#assistant}}{{gen "boundary condition" temperature=temperature max tokens=300 stop=[' ']}}{{assistant}} Prompts used for Final Conclusion {{#system}}Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.First read and analyze the "paragraphs" and "questions", then use the "premises", "boundary conditions" and "propositions" to reason which of the options given is the answer to the "question".Make sure that your reasoning is derived directly from "premises" and "propositions" rather than introducing passive common sense and passive information through common sense reasoning.Please note that this is a single choice question.If you can get the answer directly from the proposition, then you should choose the answer directly, otherwise keep reasoning with the proposition, premises, and boundary conditions until you arrive at a single answer.-{{system}} {{ #each examples}} {{#user}} -"context": "{{context}}" "question and options": "{{question}}" {{user}} {{#assistant}}"Premises": "Let's think step by step, and from the context we can extract these premises: {{premises}}"{{assistant}} {{#assistant}}"Boundary condition": "Let's think step by step, and from the context we can extract these boundary conditions: {{boundary condition}}"{{assistant}} {{#assistant}}"Thoughts": "Let us think step by step.From the premises, we can deduce propositions:{{propositions}}"{{assistant}} {{#assistant}}"Recall the reasoning history":"{{infer history}}"{{assistant}} {{#assistant}}"Recall the questions and options":"{{question}}"{{assistant}} {{#assistant}}"Reasoning": "Using premises, boundary conditions, and continuing to reason according to the propositions already obtained,{{assistant}} {{#assistant}}{{gen "reasoning" temperature=0.7 max tokens=500 stop=[' ']}}{{assistant}} {{#assistant}}"Recall the questions and options":"{{question}}"{{assistant}} {{#assistant}}"Judgement": "Now we know that the answer to this question should be{{assistant}} {{#assistant}}{{select "judgement" options=choose}}{{assistant}}C Scalability AnalysisFollowing the previous prompting methods(Yao et al., 2023;Zhang et al., 2023)for enhanced LLM-based reasoning, our main evaluation is based on the test set (typically compact with small size) for each task.To further explore the scalability of the proposed method on larger and more diverse datasets, we randomly sample 1600 cases from the ProofWriter(Tafjord et al., 2020)training set to study the model performance across varying dataset sizes.The comparison results of Table5show that DetermLR consistently outperforms CR(Zhang et al., 2023)in terms of both accuracy and time-efficiency as data scales, which verifies the scalability and robustness of the proposed method.D Generalization in Math ReasoningIn this paper, our primary focus is to enhance the logical reasoning capabilities of LLMs, that is, to study how LLMs better utilize the given conditions to complete the reasoning process more accurately and efficiently.In principle, DetermLR can be easily adapted to other types of reasoning tasks-as long as a reasoning task has a set of available conditions, the proposed method can leverage the relationship between conditions and the target to identify whether each premise is indeterminate or determinate.Therefore, we attempt to evaluate the performance of DetermLR on other reasoning tasks, such as math reasoning.We construct a dataset of multi-variable equations.An example is shown as follows.4x + y + 3z = 38 (1) −x + y + z = 13(2)3x + 3y + z = 25(3)Based on our insights, each equation contains quantitative relationships between multiple variables, which should be regarded as indeterminate equations.Conversely, the exact variable values, like x = 1, are categorized as determinate equations.LLMs need to perform reasoning by selecting equations to combine and eliminate some variables (in line with from indeterminacy to determinacy), and finally obtain the solution of each variable to form the overall solution.For this evaluation, we choose CoT-SC(Wang et al., 2022b)and CR(Zhang et al., 2023)as strong baselines for comparison.The accuracy and average reasoning steps per case are provided below, indicating the effectiveness and efficiency of our method.The comparison results of the reasoning processes of DetermLR and CR are shown as follows.We can see that the DetermLR's reasoning trajectory mirrors human-like variable elimination methods.By employing prioritization functions, equations with fewer variables or those less frequently chosen are given precedence.Coupled with the encouragement to reduce the number of variables (from indeterminacy to determinacy) during new equation generation, the entire reasoning process is streamlined and effective under the correct direction, and the new equations generated are useful without unnecessary redundancy.In contrast, CR randomly selects these equations for combination in each round and often fails to ascertain the correct solution direction, leading to erroneous or incomplete solutions.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>arXiv:2308.03762Konstantine Arkoudas. 2023. Gpt-4 can't reason. arXiv preprint</p>
<p>Thinking and deciding. Jonathan Baron, 2023Cambridge University Press</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Wen-Tau Yih, Yejin Choi, arXiv:1908.05739Abductive commonsense reasoning. 2019arXiv preprint</p>
<p>Language models are few-shot learners. Gabor Samuel R Bowman, Christopher Angeli, Christopher D Potts, Tom Manning, Benjamin Brown, Nick Mann, Melanie Ryder, Jared D Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:1508.05326A large annotated corpus for learning natural language inference. 2015. 202033arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2205.09712and Irina Higgins. 2022a. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2205.09712and Irina Higgins. 2022b. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint</p>
<p>The pascal recognising textual entailment challenge. Ido Dagan, Oren Glickman, Bernardo Magnini, Machine learning challenges workshop. Springer2005</p>
<p>Logic and human reasoning: an assessment of the deduction paradigm. Jonathan St, B T Evans, Psychological bulletin. 12869782002</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>Treeof-mixed-thought: Combining fast and slow thinking for multi-hop visual reasoning. Pengbo Hu, Ji Qi, Xingyu Li, Hong Li, Xinqi Wang, Bing Quan, Ruiyu Wang, Yi Zhou, arXiv:2308.096582023arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Merit: meta-path guided contrastive learning for logical reasoning. Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie, arXiv:2203.003572022arXiv preprint</p>
<p>Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran, arXiv:2212.13894Lambada: Backward chaining for automated reasoning in natural language. 2022arXiv preprint</p>
<p>Scitail: A textual entailment dataset from science question answering. Tushar Khot, Ashish Sabharwal, Peter Clark, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2018</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, arXiv:2210.024062022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Bin Lei, Chunhua Liao, Caiwen Ding, arXiv:2308.08614Boosting logical reasoning in large language models through a new framework: The graph of thought. 2023arXiv preprint</p>
<p>Natural language inference in contextinvestigating contextual reasoning over long texts. Hanmeng Liu, Leyang Cui, Jian Liu, Yue Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020arXiv preprint</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIarXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Siru Ouyang, Zhuosheng Zhang, Hai Zhao, arXiv:2105.10334Fact-driven logical reasoning. 2021810arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Logigan: Learning logical reasoning via adversarial pre-training. Xinyu Pi, Wanjun Zhong, Yan Gao, Nan Duan, Jian-Guang Lou, Advances in Neural Information Processing Systems. 202235</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Deductive reasoning and strategies. Walter Schaeken, Gino De Vooght, Gery D'ydewalle, arXiv:2308.10379Algorithm of thoughts: Enhancing exploration of ideas in large language models. 1999. 2023arXiv preprint</p>
<p>Manmeet Singh, S B Vaisakh, Neetiraj Malviya, arXiv:2303.11436Mind meets machine: Unravelling gpt-4's cognitive psychology. 2023arXiv preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, arXiv:2012.13048Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2020arXiv preprint</p>
<p>Diagnosing the first-order logical reasoning ability through logicnli. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461Glue: A multi-task benchmark and analysis platform for natural language understanding. 2018arXiv preprint</p>
<p>From lsat: The progress and challenges of complex reasoning. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, Nan Duan, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 302022a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Sean Welleck, Jason Weston, Arthur Szlam, Kyunghyun Cho, arXiv:1811.00671Dialogue natural language inference. 2018arXiv preprint</p>
<p>Reasoning Process of CR 1. the NO:1 round, Selected Equations: 4x + y + 3z = 38 (1), -x + y + z = 13. New Equation: 5x + 2z = 25</p>
<p>the NO:2 round, Selected Equations: -x + y + z = 13. 3x +3y + z = 25 (3) New Equation: 6y + 4z = 64</p>
<p>New Equation. the NO:3 round, Selected Equations: 5x + 2z = 25 (4), 6y + 4z = 64 (5). </p>
<p>the NO:4 round, Selected Equations: 5x + 2z =. 25New Equation: 6y + 4z = 64</p>
<p>the NO:5 round, Selected Equations: -x + y + z = 13. 5x + 2z = 25 (4). New Equation: -7x + 2y = 1</p>
<p>the NO:6 round, Selected Equations: 3x +3y + z = 25 (3), 5x + 2z = 25 (4). New Equation: x + 6y =25. </p>
<p>Solution: It's clear that the system is becoming more complicated ... (Wrong) Reasoning Process of DetermLR 1. the NO:1 round, Selected Equations: 4x + y + 3z = 38 (1), -x + y + z = 13. New Equation: 5x + 2z = 25</p>
<p>the NO:2 round, Selected Equations: 5x + 2z = 25 (4), 3x +3y + z = 25 (3) New Equation: x + 6y = 25. </p>
<p>the NO:3 round, Selected Equations: -x + y + z = 13. 5x + 2z = 25 (4). New Equation: -7x + 2y = 1</p>
<p>the NO:4 round, Selected Equations: x + 6y = 25 (5), -7x + 2y = 1. New Equation: x = 1</p>
<p>the NO:5 round, Selected Equations: x = 1 (7), x + 6y = 25 (5). New Equation: y = 4. </p>
<p>the NO:6 round, Selected Equations: x = 1. 105x + 2z = 25 (4)</p>
<p>Solution: x=1, y=4, z=10. Right</p>            </div>
        </div>

    </div>
</body>
</html>
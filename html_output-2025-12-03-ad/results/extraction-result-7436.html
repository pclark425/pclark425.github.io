<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7436 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7436</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7436</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-269804692</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.13028v1.pdf" target="_blank">DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues</a></p>
                <p><strong>Paper Abstract:</strong> User Simulators play a pivotal role in training and evaluating task-oriented dialogue systems. Traditional user simulators typically rely on human-engineered agendas, resulting in generated responses that often lack diversity and spontaneity. Although large language models (LLMs) exhibit a remarkable capacity for generating coherent and contextually appropriate utterances, they may fall short when tasked with generating responses that effectively guide users towards their goals, particularly in dialogues with intricate constraints and requirements. This paper introduces DuetSim, a novel framework designed to address the intricate demands of task-oriented dialogues by leveraging LLMs. DuetSim stands apart from conventional approaches by employing two LLMs in tandem: one dedicated to response generation and the other focused on verification. This dual LLM approach empowers DuetSim to produce responses that not only exhibit diversity but also demonstrate accuracy and are preferred by human users. We validate the efficacy of our method through extensive experiments conducted on the MultiWOZ dataset, highlighting improvements in response quality and correctness, largely attributed to the incorporation of the second LLM.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7436.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7436.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DuetSim (dual-LLM, ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DuetSim: Dual LLM user simulator (generator + verifier) using ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot user simulator that splits generation and verification across two LLMs: a generator that drafts dialogue acts (via chain-of-thought) and a verifier that checks the draft against explicit requirements, providing iterative feedback until the draft is acceptable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT (inference-only conversational LLM used as both generator and verifier in the DuetSim pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiWOZ task-oriented dialogue user simulation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Simulate user behavior (generate dialogue acts and natural-language utterances) to interact with a task-oriented dialogue system so that user goals are achieved on MultiWOZ dialogs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Dual-LLM iterative prompting: (1) zero-shot prompt listing target user goal G, dialogue context C, and generation requirements R_G; generator produces dialogue acts via chain-of-thought; (2) verifier LLM receives the draft U_G, context C, and a list of verification requirements R_V and either accepts or returns explicit feedback; generator re-prompts with verifier feedback until acceptance or max iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (multi-stage / verifier loop)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot (no demonstrations) prompts; explicit lists of requirements; chain-of-thought (CoT) used in generator to produce intent->domain->slot->value sequentially; verifier enumerates which requirement is violated and returns feedback; iterative loop until pass; experiments averaged over 100 dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Complete Rate 0.92; Success Rate 0.74; Precision 0.83; Recall 0.98; F1 0.881; Book Rate 0.585; Turns 16.92</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>DuetSim (ChatGPT w/o verifier): Complete Rate 0.85; Success Rate 0.67; Precision ~0.84; Recall 0.948; F1 0.873; Book Rate 0.544; Turns 17.88</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.07 absolute Success Rate (0.74 vs 0.67); small absolute gains on Complete Rate (+0.07) and Book Rate (+0.041); minor reduction in average turns (16.92 vs 17.88)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompts; chain-of-thought generation of dialogue acts; generator and verifier are separate ChatGPT calls; averaged over 100 dialogues; no temperature/max-token settings reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7436.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7436.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DuetSim (dual-LLM, FLAN-T5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DuetSim: Dual LLM user simulator (generator + verifier) using FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same DuetSim pipeline implemented with FLAN-T5 as the LLM family for generator and verifier; employs zero-shot prompts, explicit requirements, and chain-of-thought generation of dialogue actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer model (FLAN-T5) used for generation and verification in DuetSim experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiWOZ task-oriented dialogue user simulation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Simulate user behavior to complete task goals and produce natural utterances on MultiWOZ dialogs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Dual-LLM iterative prompting with zero-shot requirement lists and chain-of-thought for stepwise dialogue-act generation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (multi-stage / verifier loop)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot, explicit requirements R_G/R_V in prompts; chain-of-thought decomposition for intent->domain->slot->value; verifier gives feedback when requirements violated; iterative generation until pass.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Complete Rate 0.92; Success Rate 0.71; Precision 0.82; Recall 0.979; F1 0.872; Book Rate 0.648; Turns 16.16</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>DuetSim (FLAN-T5 w/o verifier): Complete Rate 0.90; Success Rate 0.63; Precision 0.834; Recall 0.961; F1 0.875; Book Rate 0.551; Turns 16.18</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.08 absolute Success Rate (0.71 vs 0.63); +0.097 absolute Book Rate (0.648 vs 0.551); other metrics comparable</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompts; chain-of-thought generation; generator+verifier implemented with FLAN-T5; averaged over 100 dialogues; no explicit decoding/temperature settings reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7436.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7436.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PBUS (single-LLM, no CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-Based User Simulator (single LLM, no chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline single-LLM user simulator that uses in-context learning to simulate users but (as used in this paper's comparison) does not employ chain-of-thought decomposition or a separate verifier model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PBUS (single LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single large language model driven by in-context learning (few-shot / prompt-based) used as a user simulator; in the paper PBUS is described as not using chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiWOZ task-oriented dialogue user simulation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same user-simulation task on MultiWOZ; PBUS produces utterances/actions from a single LLM with in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single-shot / in-context few-shot prompt to a single LLM without explicit chain-of-thought decomposition or a separate verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (question type / few-shot vs zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>PBUS uses in-context learning (sampling a few examples) and a single LLM; no chain-of-thought, no verifier; PBUS enforces a maximum 15-turn dialogue limit (noted in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Complete Rate 0.41; Success Rate 0.30; Precision 0.58; Recall 0.67; F1 0.71; Book Rate 0.659; Turns 7.50</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Compared to DuetSim (ChatGPT) Success Rate improvement +0.44 absolute (0.74 vs 0.30); Complete Rate +0.51 absolute (0.92 vs 0.41); large improvements across task metrics when using DuetSim's CoT + verifier pipeline versus PBUS.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>PBUS as implemented per Terragni et al. (cited); in-paper PBUS run averaged over 100 dialogues; PBUS has a 15-turn cap.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7436.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7436.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-component ablation (dialogue history & user goal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation of prompt components: dialogue history and user goal presence in generator/verifier prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation experiments that remove dialogue history and/or the user goal from the prompts to measure how those prompt components affect user-simulator performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DuetSim (reported results; primary experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DuetSim pipeline (dual LLMs) used in ablation mode where prompt fields are selectively removed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiWOZ user simulation (ablation study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate effect of removing dialogue history and/or target user goal from prompts on goal-fulfillment metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompts where specific prompt fields are omitted (dialogue history and/or user goal).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (prompt field inclusion/ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Four prompt-settings compared: full prompt (with dialogue history + goal), without dialogue history, without goal, and without both; otherwise same DuetSim pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Full: Complete 0.92; Success 0.74; Precision 0.83; Recall 0.98; F1 0.881; Book 0.585; Turns 16.92</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>w/o dialogue history: Complete 0.85 (-0.07), Success 0.68 (-0.06), Precision 0.793 (-0.037), Recall 0.904 (-0.076), F1 0.824 (-0.057); w/o goal: Complete 0.91 (-0.01), Success 0.70 (-0.04), Book Rate drops from 0.585 to 0.455 (-0.130); w/o both: Complete 0.89 (-0.03), Success 0.66 (-0.08).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Ablations performed within DuetSim; metrics averaged over 100 dialogues; other prompt/config unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7436.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7436.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dialogue context format (utterance vs dialogue-act)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of representing dialogue context as natural-language utterances vs structured dialogue acts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing that DuetSim better comprehends dialogue context when it is provided in natural-language utterances than when provided as structured dialogue-act representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DuetSim (reported results)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DuetSim pipeline tested with two different forms of dialogue context: natural-language utterances or dialogue acts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiWOZ user simulation (context representation study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess how the representation of dialogue history (natural utterance vs structured dialogue-act) in prompts affects goal-fulfillment metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt style variation: dialogue history included either as raw natural-language utterances or as structured dialogue-act lists.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt representation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Everything else held constant; comparison between 'dialogue context in utterance' and 'dialogue context in dialogue act' settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Context as utterance: Complete 0.92; Success 0.74; Precision 0.83; Recall 0.98; F1 0.881; Book 0.585; Turns 16.92</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Context as dialogue-act: Complete 0.89; Success 0.66; Precision 0.853; Recall 0.968; F1 0.890; Book 0.523; Turns 16.60</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Switching from dialogue-act context to natural-language utterance increased Success Rate by +0.08 absolute (0.74 vs 0.66) and Complete Rate by +0.03; precision shows a slight decrease but task success improves.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>DuetSim with identical prompts except for the representation of the dialogue context; results averaged over 100 dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7436.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7436.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long-context / 'lost-in-the-middle' mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs' degraded attention to middle of long inputs ('Lost in the middle')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper cites prior work indicating LLMs tend to focus on the beginning and end of long inputs, with reduced performance on information presented in the middle; the authors observe this phenomenon as a limitation that affects satisfying long, complex prompt requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lost in the middle: How language models use long contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>generic large language models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reference to observed behavior across LLMs where attention/use of context decays for mid-sections of very long inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General long-context comprehension (applied to task-oriented dialogue prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Understanding and following lengthy multi-part prompts describing user goals, constraints, and previous dialogue turns.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Long, multi-part natural-language prompts where important instructions may appear in the middle (long-context natural-language description).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality (context length / ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Paper notes LLMs tend to emphasize information near the start or end of input; this motivates splitting tasks (generator + verifier) and using CoT to reduce complexity of single long prompts. No new numeric experiment isolating this effect is reported in the paper; prior work is cited.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Discussion and motivation for DuetSim design; citation to Liu et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Lost in the middle: How language models use long contexts <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Let's verify step by step <em>(Rating: 1)</em></li>
                <li>Context-learning user simulators for task-oriented dialog systems <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7436",
    "paper_id": "paper-269804692",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "DuetSim (dual-LLM, ChatGPT)",
            "name_full": "DuetSim: Dual LLM user simulator (generator + verifier) using ChatGPT",
            "brief_description": "A zero-shot user simulator that splits generation and verification across two LLMs: a generator that drafts dialogue acts (via chain-of-thought) and a verifier that checks the draft against explicit requirements, providing iterative feedback until the draft is acceptable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "ChatGPT (inference-only conversational LLM used as both generator and verifier in the DuetSim pipeline).",
            "model_size": null,
            "task_name": "MultiWOZ task-oriented dialogue user simulation",
            "task_description": "Simulate user behavior (generate dialogue acts and natural-language utterances) to interact with a task-oriented dialogue system so that user goals are achieved on MultiWOZ dialogs.",
            "problem_format": "Dual-LLM iterative prompting: (1) zero-shot prompt listing target user goal G, dialogue context C, and generation requirements R_G; generator produces dialogue acts via chain-of-thought; (2) verifier LLM receives the draft U_G, context C, and a list of verification requirements R_V and either accepts or returns explicit feedback; generator re-prompts with verifier feedback until acceptance or max iterations.",
            "format_category": "prompt style (multi-stage / verifier loop)",
            "format_details": "Zero-shot (no demonstrations) prompts; explicit lists of requirements; chain-of-thought (CoT) used in generator to produce intent-&gt;domain-&gt;slot-&gt;value sequentially; verifier enumerates which requirement is violated and returns feedback; iterative loop until pass; experiments averaged over 100 dialogues.",
            "performance_metric": "Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns",
            "performance_value": "Complete Rate 0.92; Success Rate 0.74; Precision 0.83; Recall 0.98; F1 0.881; Book Rate 0.585; Turns 16.92",
            "baseline_performance": "DuetSim (ChatGPT w/o verifier): Complete Rate 0.85; Success Rate 0.67; Precision ~0.84; Recall 0.948; F1 0.873; Book Rate 0.544; Turns 17.88",
            "performance_change": "+0.07 absolute Success Rate (0.74 vs 0.67); small absolute gains on Complete Rate (+0.07) and Book Rate (+0.041); minor reduction in average turns (16.92 vs 17.88)",
            "experimental_setting": "Zero-shot prompts; chain-of-thought generation of dialogue acts; generator and verifier are separate ChatGPT calls; averaged over 100 dialogues; no temperature/max-token settings reported.",
            "statistical_significance": null,
            "uuid": "e7436.0",
            "source_info": {
                "paper_title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DuetSim (dual-LLM, FLAN-T5)",
            "name_full": "DuetSim: Dual LLM user simulator (generator + verifier) using FLAN-T5",
            "brief_description": "Same DuetSim pipeline implemented with FLAN-T5 as the LLM family for generator and verifier; employs zero-shot prompts, explicit requirements, and chain-of-thought generation of dialogue actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_description": "Instruction-tuned transformer model (FLAN-T5) used for generation and verification in DuetSim experiments.",
            "model_size": null,
            "task_name": "MultiWOZ task-oriented dialogue user simulation",
            "task_description": "Simulate user behavior to complete task goals and produce natural utterances on MultiWOZ dialogs.",
            "problem_format": "Dual-LLM iterative prompting with zero-shot requirement lists and chain-of-thought for stepwise dialogue-act generation.",
            "format_category": "prompt style (multi-stage / verifier loop)",
            "format_details": "Zero-shot, explicit requirements R_G/R_V in prompts; chain-of-thought decomposition for intent-&gt;domain-&gt;slot-&gt;value; verifier gives feedback when requirements violated; iterative generation until pass.",
            "performance_metric": "Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns",
            "performance_value": "Complete Rate 0.92; Success Rate 0.71; Precision 0.82; Recall 0.979; F1 0.872; Book Rate 0.648; Turns 16.16",
            "baseline_performance": "DuetSim (FLAN-T5 w/o verifier): Complete Rate 0.90; Success Rate 0.63; Precision 0.834; Recall 0.961; F1 0.875; Book Rate 0.551; Turns 16.18",
            "performance_change": "+0.08 absolute Success Rate (0.71 vs 0.63); +0.097 absolute Book Rate (0.648 vs 0.551); other metrics comparable",
            "experimental_setting": "Zero-shot prompts; chain-of-thought generation; generator+verifier implemented with FLAN-T5; averaged over 100 dialogues; no explicit decoding/temperature settings reported.",
            "statistical_significance": null,
            "uuid": "e7436.1",
            "source_info": {
                "paper_title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "PBUS (single-LLM, no CoT)",
            "name_full": "Prompt-Based User Simulator (single LLM, no chain-of-thought)",
            "brief_description": "Baseline single-LLM user simulator that uses in-context learning to simulate users but (as used in this paper's comparison) does not employ chain-of-thought decomposition or a separate verifier model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PBUS (single LLM baseline)",
            "model_description": "Single large language model driven by in-context learning (few-shot / prompt-based) used as a user simulator; in the paper PBUS is described as not using chain-of-thought reasoning.",
            "model_size": null,
            "task_name": "MultiWOZ task-oriented dialogue user simulation",
            "task_description": "Same user-simulation task on MultiWOZ; PBUS produces utterances/actions from a single LLM with in-context examples.",
            "problem_format": "Single-shot / in-context few-shot prompt to a single LLM without explicit chain-of-thought decomposition or a separate verifier.",
            "format_category": "prompt style (question type / few-shot vs zero-shot)",
            "format_details": "PBUS uses in-context learning (sampling a few examples) and a single LLM; no chain-of-thought, no verifier; PBUS enforces a maximum 15-turn dialogue limit (noted in paper).",
            "performance_metric": "Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns",
            "performance_value": "Complete Rate 0.41; Success Rate 0.30; Precision 0.58; Recall 0.67; F1 0.71; Book Rate 0.659; Turns 7.50",
            "baseline_performance": null,
            "performance_change": "Compared to DuetSim (ChatGPT) Success Rate improvement +0.44 absolute (0.74 vs 0.30); Complete Rate +0.51 absolute (0.92 vs 0.41); large improvements across task metrics when using DuetSim's CoT + verifier pipeline versus PBUS.",
            "experimental_setting": "PBUS as implemented per Terragni et al. (cited); in-paper PBUS run averaged over 100 dialogues; PBUS has a 15-turn cap.",
            "statistical_significance": null,
            "uuid": "e7436.2",
            "source_info": {
                "paper_title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Prompt-component ablation (dialogue history & user goal)",
            "name_full": "Ablation of prompt components: dialogue history and user goal presence in generator/verifier prompts",
            "brief_description": "Ablation experiments that remove dialogue history and/or the user goal from the prompts to measure how those prompt components affect user-simulator performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DuetSim (reported results; primary experiments)",
            "model_description": "DuetSim pipeline (dual LLMs) used in ablation mode where prompt fields are selectively removed.",
            "model_size": null,
            "task_name": "MultiWOZ user simulation (ablation study)",
            "task_description": "Evaluate effect of removing dialogue history and/or target user goal from prompts on goal-fulfillment metrics.",
            "problem_format": "Zero-shot prompts where specific prompt fields are omitted (dialogue history and/or user goal).",
            "format_category": "prompt style (prompt field inclusion/ablation)",
            "format_details": "Four prompt-settings compared: full prompt (with dialogue history + goal), without dialogue history, without goal, and without both; otherwise same DuetSim pipeline.",
            "performance_metric": "Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns",
            "performance_value": "Full: Complete 0.92; Success 0.74; Precision 0.83; Recall 0.98; F1 0.881; Book 0.585; Turns 16.92",
            "baseline_performance": null,
            "performance_change": "w/o dialogue history: Complete 0.85 (-0.07), Success 0.68 (-0.06), Precision 0.793 (-0.037), Recall 0.904 (-0.076), F1 0.824 (-0.057); w/o goal: Complete 0.91 (-0.01), Success 0.70 (-0.04), Book Rate drops from 0.585 to 0.455 (-0.130); w/o both: Complete 0.89 (-0.03), Success 0.66 (-0.08).",
            "experimental_setting": "Ablations performed within DuetSim; metrics averaged over 100 dialogues; other prompt/config unchanged.",
            "statistical_significance": null,
            "uuid": "e7436.3",
            "source_info": {
                "paper_title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Dialogue context format (utterance vs dialogue-act)",
            "name_full": "Effect of representing dialogue context as natural-language utterances vs structured dialogue acts",
            "brief_description": "Comparison showing that DuetSim better comprehends dialogue context when it is provided in natural-language utterances than when provided as structured dialogue-act representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DuetSim (reported results)",
            "model_description": "DuetSim pipeline tested with two different forms of dialogue context: natural-language utterances or dialogue acts.",
            "model_size": null,
            "task_name": "MultiWOZ user simulation (context representation study)",
            "task_description": "Assess how the representation of dialogue history (natural utterance vs structured dialogue-act) in prompts affects goal-fulfillment metrics.",
            "problem_format": "Prompt style variation: dialogue history included either as raw natural-language utterances or as structured dialogue-act lists.",
            "format_category": "input modality / prompt representation",
            "format_details": "Everything else held constant; comparison between 'dialogue context in utterance' and 'dialogue context in dialogue act' settings.",
            "performance_metric": "Complete Rate; Success Rate; Precision; Recall; F1; Book Rate; Average Turns",
            "performance_value": "Context as utterance: Complete 0.92; Success 0.74; Precision 0.83; Recall 0.98; F1 0.881; Book 0.585; Turns 16.92",
            "baseline_performance": "Context as dialogue-act: Complete 0.89; Success 0.66; Precision 0.853; Recall 0.968; F1 0.890; Book 0.523; Turns 16.60",
            "performance_change": "Switching from dialogue-act context to natural-language utterance increased Success Rate by +0.08 absolute (0.74 vs 0.66) and Complete Rate by +0.03; precision shows a slight decrease but task success improves.",
            "experimental_setting": "DuetSim with identical prompts except for the representation of the dialogue context; results averaged over 100 dialogues.",
            "statistical_significance": null,
            "uuid": "e7436.4",
            "source_info": {
                "paper_title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Long-context / 'lost-in-the-middle' mention",
            "name_full": "LLMs' degraded attention to middle of long inputs ('Lost in the middle')",
            "brief_description": "Paper cites prior work indicating LLMs tend to focus on the beginning and end of long inputs, with reduced performance on information presented in the middle; the authors observe this phenomenon as a limitation that affects satisfying long, complex prompt requirements.",
            "citation_title": "Lost in the middle: How language models use long contexts",
            "mention_or_use": "mention",
            "model_name": "generic large language models",
            "model_description": "Reference to observed behavior across LLMs where attention/use of context decays for mid-sections of very long inputs.",
            "model_size": null,
            "task_name": "General long-context comprehension (applied to task-oriented dialogue prompting)",
            "task_description": "Understanding and following lengthy multi-part prompts describing user goals, constraints, and previous dialogue turns.",
            "problem_format": "Long, multi-part natural-language prompts where important instructions may appear in the middle (long-context natural-language description).",
            "format_category": "input modality (context length / ordering)",
            "format_details": "Paper notes LLMs tend to emphasize information near the start or end of input; this motivates splitting tasks (generator + verifier) and using CoT to reduce complexity of single long prompts. No new numeric experiment isolating this effect is reported in the paper; prior work is cited.",
            "performance_metric": "",
            "performance_value": "",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Discussion and motivation for DuetSim design; citation to Liu et al. (2023).",
            "statistical_significance": null,
            "uuid": "e7436.5",
            "source_info": {
                "paper_title": "DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Lost in the middle: How language models use long contexts",
            "rating": 2,
            "sanitized_title": "lost_in_the_middle_how_language_models_use_long_contexts"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 1,
            "sanitized_title": "lets_verify_step_by_step"
        },
        {
            "paper_title": "Context-learning user simulators for task-oriented dialog systems",
            "rating": 2,
            "sanitized_title": "contextlearning_user_simulators_for_taskoriented_dialog_systems"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.016536,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues
16 May 2024</p>
<p>Xiang Luo luoxiang@mail.ynu.edu.cn 
School of Information Science and Engineering
Yunnan University Kunming
China</p>
<p>Zhiwen Tang zhiwen.tang@ynu.edu.cn 
School of Information Science and Engineering
Yunnan University Kunming
China</p>
<p>Jin Wang wangjin@ynu.edu.cn 
School of Information Science and Engineering
Yunnan University Kunming
China</p>
<p>Xuejie Zhang xjzhang@ynu.edu.cn 
School of Information Science and Engineering
Yunnan University Kunming
China</p>
<p>DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues
16 May 2024FE40A0DB25FF4ADEC43A8C5D6AB05DDBarXiv:2405.13028v1[cs.CL]dialogue systemuser simulationprompt learning
User Simulators play a pivotal role in training and evaluating task-oriented dialogue systems.Traditional user simulators typically rely on human-engineered agendas, resulting in generated responses that often lack diversity and spontaneity.Although large language models (LLMs) exhibit a remarkable capacity for generating coherent and contextually appropriate utterances, they may fall short when tasked with generating responses that effectively guide users towards their goals, particularly in dialogues with intricate constraints and requirements.This paper introduces DuetSim, a novel framework designed to address the intricate demands of task-oriented dialogues by leveraging LLMs.DuetSim stands apart from conventional approaches by employing two LLMs in tandem: one dedicated to response generation and the other focused on verification.This dual LLM approach empowers DuetSim to produce responses that not only exhibit diversity but also demonstrate accuracy and are preferred by human users.We validate the efficacy of our method through extensive experiments conducted on the MultiWOZ dataset, highlighting improvements in response quality and correctness, largely attributed to the incorporation of the second LLM.Our code is accessible at: https://github.com/suntea233/DuetSim.</p>
<p>Introduction</p>
<p>Task-oriented dialogue systems (TODS) are developed to engage with human users in natural language to accomplish tasks within specific domains, such as restaurant reservations and ticket bookings.A common approach to training TODS involves reinforcement learning, where the dialogue system engages with human users over tens of thousands of interactions to collect training dialogues, optimize dialogue policies, and enhance generated responses.However, operating within this training paradigm, which relies on interactions with actual human users, is labor-intensive and cost-prohibitive.To address this challenge, user simulators (US) have been introduced to emulate the behavior of real human users, thereby substituting them in the reinforcement learning training process for TODS.</p>
<p>User simulators play a crucial role in the training of TODS.They not only reduce training costs but also significantly influence the quality of the system's responses, thereby impacting the overall performance of TODS.Given this importance, researchers have long focused on developing user simulators capable of producing accurate, diverse, and human-like responses.Early efforts (Schatzmann et al., 2007;Schatztnann et al., 2005) centered on constructing user simulators using expert knowledge and handcrafted rules.However, rule-based user simulators face several challenges.</p>
<p>*Corresponding Authors</p>
<p>One challenge is the complexity of human engineering required, which restricts their suitability to small, well-defined domains and makes them less adaptable for extension to new domains.Another challenge is the limited diversity in their responses, stemming from the determinism of the rules that guide them and the predefined templates used for generating natural language utterances.Although rule-based user simulators can produce correct responses most of the time, researchers have explored alternative approaches to address the limitations of rule-based simulators.</p>
<p>The advent of deep learning (DL) has also given rise to DL-based user simulators.Notable examples of such work include (Asri et al., 2016;Gr et al., 2018;Kreyssig et al., 2018;Lin et al., 2021Lin et al., , 2022)).DL-based user simulators offer significant improvements in language variation, but they require extensive training on large volumes of humanannotated task-oriented dialogue data.The quality and diversity of this training data play a pivotal role in determining the accuracy and robustness of these user simulators, which also renders them less suitable for transfer to new domains, particularly when resources are limited in the new domain.</p>
<p>The advent of large language models (LLMs) and in-context learning has opened up new possibilities for constructing user simulators.LLMs are trained on vast amounts of text data in a selfsupervised manner and exhibit impressive zeroshot and few-shot capabilities in downstream tasks, often requiring only a small number of examples  (Terragni et al., 2023) are not universally successful.Given the intricate requirements of task-oriented dialogues, LLM-based user simulators struggle to consistently generate responses that satisfy all the criteria specified in the prompts.This study reveals that the challenge arises from LLMs' limitations in comprehending lengthy prompts.Our findings align with those of (Liu et al., 2023), which highlight that LLMs are more inclined to focus on information at the beginning or end of input, with model performance diminishing as input length increases.</p>
<p>In order to address the aforementioned challenges, this paper introduces a novel framework, DuetSim, designed to tackle the complex requirements of task-oriented dialogues using LLMs.Instead of relying on a single LLM to construct the user simulator, we advocate the use of two LLMs, one for the generator and the other for the verifier, to build a more robust user simulator.The generator initially generates responses based on the dialogue context, while the verifier meticulously examines these responses, offering feedback to the generator if any generated responses are deemed unsuitable.Figure 1 highlights the distinctions between our approach and those employing a single LLM.With the inclusion of the verifier, DuetSim has the capacity to rectify erroneous responses identified by the verifier.Additionally, we incorporate chain-of-thought reasoning to narrow the search space, aiding the user simulator in comprehending the dialogue context and generating contextually appropriate dialogue responses.</p>
<p>We conducted experiments with our approach using the MultiWOZ dataset.The experimental re-sults clearly demonstrate that our proposed method generates responses that exhibit greater diversity, accuracy, and are preferred by human users.The incorporation of the second LLM significantly enhances the quality and correctness of the generated responses.</p>
<p>The remainder of this paper is structured as follows: Section 2 provides a brief review of related work.In Section 3, we offer a comprehensive description of our model.Section 4 provides a summary of the experimental results and implementation details.Finally, in Section 5, we draw our conclusions.</p>
<p>Related Work</p>
<p>User Simulator</p>
<p>Task-oriented dialogue system is designed to interact with users in a goal-directed manner to accomplish specific tasks or provide relevant information (Wen et al., 2017;Ham et al., 2020).They differ from chit-chat dialogue systems, which are more focused on free-form conversations and entertaining interactions, while task-oriented dialogue systems are geared towards addressing the specific needs of users.</p>
<p>User simulators also play a crucial role in taskoriented dialog systems.Different from dialogue systems, user simulators have a goal generator to guide them in simulating user behavior and evaluate the performance of the user simulator based on the extent to which the goals are achieved.Typically, constructing a user simulator can be achieved through an agenda-based approach (Schatztnann et al., 2005;Schatzmann et al., 2006Schatzmann et al., , 2007;;Keizer et al., 2010), where rules are designed based on expert knowledge to simulate user behavior.Additionally, there are deep learning-based methods that involve training models on corpora of dialogues, resulting in models that don't require hand-crafted policy and have good scalability (Asri et al., 2016;Gr et al., 2018;Kreyssig et al., 2018;Lin et al., 2021Lin et al., , 2022)).Different from the methods mentioned above, we directly use large language models to generate dialogue actions and utterances.Through this approach, we do not need to train a model from scratch, and it offers excellent transferability.</p>
<p>Large Language Models and Prompt Learning</p>
<p>In recent years, with the emergence of GPT-3 (Brown et al., 2020), large language models have achieved remarkable performance across various tasks (Zhao et al., 2023;Qiao et al., 2023;Yuan et al., 2022).Additionally, the few-shot and even zero-shot learning capabilities demonstrated by these models have been particularly impressive.</p>
<p>As research on prompt learning has evolved, we've seen a progression from initially guiding large language models through simple prompts for downstream tasks to more advanced techniques, which includes step-by-step guided reasoning using the chain of thought approach (Wei et al., 2022), as well as recent developments involving strategies like tree-structured architectures (Yao et al., 2023).These approaches aim to effectively apply large language models to various tasks using different decoding methods (Zheng et al., 2023).Recently, there have also been many efforts to use prompt learning to guide large language models in exploring various downstream tasks such as dialogue system (Hudeek and Dusek, 2023), dialog state tracking (Wang et al., 2022), user simulator (Terragni et al., 2023).As done by Terragni et al. (Terragni et al., 2023), sampling a few examples from the dataset helps large language models better adapt to playing user simulator.</p>
<p>With the deepening research into hard prompts, there has been exploration of approaches like training additional verifier to verify the correctness of the final answer (Cobbe et al., 2021) or intermediate steps (Li et al., 2023;Lightman et al., 2023).Besides, there are methods that involve verifying generated code to improve result accuracy (Zhou et al., 2023).In contrast to these approaches, which either rely on a single large language model to perform all tasks or involve training an additional model to provide feedback, we utilize two large language models, solely harnessing their inference capabilities and facilitating interaction between the generator and the verifier to enhance the applicability of the entire model in user simulation.</p>
<p>DuetSim</p>
<p>In this paper, we propose DuetSim, a user simulator based on two LLMs for task-oriented dialogue systems.DuetSim consists of a dialogue generator and a response verifier.The dialogue generator drafts the response while the response verifier examines the generated response and offers feedback if needed.Both the dialogue generator and the response verifier are LLMs.By splitting the task between the dialogue generator and the response verifier, both LLMs can share the burden and better handle the assigned tasks.Further, we also propose a chain-of-thought approach to guide the user simulator to generate responses that better fit the context.Rather than directly generating responses in natural language, the proposed approach first generates dialogue acts step-by-step, which are then used to guide the utterance generation.</p>
<p>Prompt Learning for DuetSim</p>
<p>DuetSim leverages the capabilities of large language models (LLMs) with prompt learning.We create prompts for both the dialogue generator and response verifier to elicit responses from LLMs.By supplying LLMs with background information from the ongoing dialogue and previous conversation history, both models can effectively perform their designated tasks.This includes generating appropriate responses in dialogue acts or natural language and verifying whether the generated responses meet the specified requirements.</p>
<p>We employ a zero-shot learning approach to harness the inference capabilities of LLMs.In this approach, we abstain from providing any demonstrations but instead list the requirements and prompt the LLM to generate a response.The prompt for the dialogue generator includes essential information such as the target user goal, denoted as G, the dialogue context, represented as C, and the requirements, denoted as R c .The user goal G is randomly generated, and the entire dialogue is designed to fulfill this user goal.The dialogue context C encompasses utterances from previous dialogue turns, while the requirements
R G = [R G 1 , R G 2 , ..., R G n ]
specify the desired properties of the generated response.The task of the dialogue generator is to produce appropriate dialogue acts.Each dialogue act comprises four components: intent, domain, slot, and value, serving as semantic abstractions for the dialogue utterances.This process can be formulated as follows:
U G = Generator G, C, R G (1)
where the U G represent the output for dialogue generator.The left side of Figure 2 shows the prompt learning process for the dialogue generator.The output from the dialogue generator is then passed to the response verifier to assess its alignment with the context.The prompt for the response verifier includes the requirements denoted as R v , the dialogue context represented as C, and the response generated by the dialogue generator, denoted as U G .Within the requirement set
R V = [R V 1 , R V 2 , ..., R V m ],
we instruct the verifier LLM to scrutinize the response U G for any potential errors.In these requirements, we enumerate common mistakes that the LLM might make, such as context inconsistency and the generation of meaningless words.This prompts the verifier to check for any such errors in the generated response.This process can be formulated as follows:
U V = V erif ier G, C, R V , U G (2)
where the U V represent the output for verifier.The right side of Figure 2 shows the prompt learning process for the response verifier.</p>
<p>Generator-Verifier Interaction</p>
<p>In DuetSim, instead of relying on a single LLM for generating dialogue responses, we introduce an additional response verifier to enhance the quality of the generated responses.The dialogue generator initially produces a draft response based on provided prompts.Subsequently, this draft response is forwarded to the response verifier for evaluation against specified requirements.If the verifier deems the draft generated response as suitable, it becomes the output of the user simulator.Conversely, if the verifier detects any errors in the draft response, it rejects the response and provides feedback to the generator.The generator then proceeds to generate an alternative response.</p>
<p>Figure 2 illustrates the interactive process.In the case of the generator, this process entails inputting carefully crafted prompts into a large language model and leveraging the model's reasoning capabilities to generate dialogue acts through a chain of thought.Following this, the generated dialogue acts, along with their context, are then transmitted to the verifier.</p>
<p>As for the verifier, it combines the received dialogue acts with pre-defined prompts and inputs them into a large language model.Likewise, the verifier utilizes the reasoning capabilities of the large language model to evaluate the accuracy of the dialogue acts.Subsequently, the verifier offers feedback to the dialogue generator regarding the correctness of the generated response.</p>
<p>If the draft response is found to be incorrect, the feedback will encompass the specific requirement R V i that the draft response has breached.The generator subsequently integrates this feedback information at the end of the prompt and proceeds to generate another response.This iterative process persists until the generator successfully generates the correct dialogue actions or reaches the maximum specified number of iterations.As a result, the final output is formulated as:
U G = Generator G, C, R G , U V
(3)</p>
<p>Chain-of-thought Response Generation</p>
<p>To enhance natural language generation, we use a "chain of thought" approach.Initially, we provide As a foundation, we boost fluency, contextual relevance, and naturalness in generated utterances by integrating dialogue context and prompts.This approach yields higher-quality output compared to direct generation.We start by converting dialogue actions into simple utterances using examples and crafted prompts.Subsequently, we input this information, along with the dialogue context, into the large language model, leveraging its reasoning capabilities to produce coherent, context-aware utterances.See Table 1 for an illustrative example prompt.</p>
<p>To enhance dialogue action generation, we employ a step-by-step "chain of thought" approach.Directly generating dialogue actions, including intent, domain, slot, and value, can be challenging for large language models due to their limited understanding of abstract dialogue actions.This makes it difficult to produce well-structured actions aligned with intended goals.Thus, we adopt a method that breaks down the generation process into sequential steps for each component of dialogue actions.This approach reduces complexity, assisting the user simulator in producing precise dialogue actions.Specifically, we begin by generating the intent using a dedicated prompt.Once obtained, this intent serves as input for the domain prompt.We continue this sequential process for slot and value.This iterative approach allows us to construct complete dialogue actions by gradually generating each component.</p>
<p>Experiments</p>
<p>Dataset</p>
<p>We evaluate DuetSim using the MultiWOZ dataset (Budzianowski et al., 2018) available in ConvLab (Zhu et al., 2020).MultiWOZ is an extensively annotated dataset comprising 10,000 human-to-human written conversations covering diverse domains and topics, making it a widely used benchmark dataset for evaluating task-oriented dialogue systems.</p>
<p>Evaluation Metrics</p>
<p>We evaluate the proposed method from two perspectives, goal fulfillment and utterance diversity.While goal fulfillment metrics evaluate whether the dialogue helps the user complete the task, utterance diversity metrics focus on the quality of generated natural language.</p>
<p>Goal fulfillment metrics include success rate, completion rate, booking rate, precision, recall and F 1 -score.Success rate refers to the fraction of dialogues that successfully accomplish the user's task.Completion rate evaluates whether the dialogue system make reservations disregarding whether the reserved entity match the user's requirement.Book rate assesses whether booking tasks present in the user's goal have been completed.Precision, recall and F 1 -score are used to determine whether the dialogue system finds the required information.</p>
<p>The utterance diversity are measured with multiple different metrics, including number of unique n-grams (unigrams, bigrams, and trigrams), Shannon Entropy (SE) (Manning and Schutze, 1999), Conditional bigram Entropy (CE) (Manning and Schutze, 1999), Mean Segmental Type-Token Ratio (MSTTR) (Lu, 2012), Measure of Textual Lexical Diversity (MTLD) (McCarthy and Jarvis, 2010) and Hypergeometric Distribution Function (HDD) (Wu, 1993).Among them, SE measures the diversity of information contained in the text.CE assesses the fluency and coherence of text.MSTTR measures the diversity and lexical richness of text.MTLD additionally considers the overall structure of text when measuring the lexical diversity.HDD measures the diversity of vocabulary when randomly selecting a fixed number of words from the text.</p>
<p>In addition, we conducted human evaluation to assess whether responses generated by DuetSim aligns with human preferences.</p>
<p>Implementation Details</p>
<p>All the experiment results are the average of 100 task-oriented dialogues.All the user simulators interact with the dialogue system via dialogue acts.We implement the proposed method using different LLMs, including ChatGPT1 , FLAN-T5 (Chung et al., 2022), LLAMA2 (Touvron et al., 2023) and ChatGLM2 (Du et al., 2022).</p>
<p>Baselines</p>
<p>We compare the proposed method with Agenda-Based User Simulator (ABUS) (Schatzmann et al., 2007) and Prompt-Based User Simulator (PBUS) (Terragni et al., 2023). ABUS: ABUS is an user simulator that follows hand-crafted rules to complete dialogues.We adopt two variants, namely ABUS-T that uses template NLG and ABUS-S that uses SC-GPT for NLG when evaluating utterance diversity.</p>
<p> PBUS: PBUS is based on a single LLM and uses in-context learning to simulate users.Compared with our approach, PBUS does not involve chain-of-thought reasoning when simulating users.</p>
<p>The proposed method are respectively implemented using the following LLMs:</p>
<p> ChatGPT.ChatGPT is a chatbot developed by OpenAI.It builds upon the foundation of GPT and is trained on large volumes of text data to comprehend and generate natural language text.</p>
<p> FLAN-T5.FLAN-T5 is an instruction-tuned model, trained on extensive datasets, that excels in almost all downstream tasks due to its high versatility.</p>
<p> LLAMA2.LLAMA2 is a model based on LLAMA, and it builds upon the foundation of LLAMA by improving the quality of training data, increasing context length, and optimizing memory related to caching.</p>
<p> ChatGLM2.ChatGLM2 is a model based on ChatGLM.Building on ChatGLM, ChatGLM2 upgrades the base model to enhance performance, increases context length, and improves inference capabilities.</p>
<p>Main Results</p>
<p>Goal Fulfillment: As shown in Table 2, Duet-Sim performs best in all the user simulators that are based on LLMs and the performance of Duet-Sim(ChatGPT) is the closest to ABUS.Among all the variants of DuetSim, DuetSim(ChatGPT) and DuetSim(FLAN-T5) perform much better than Duet-Sim(ChatGLM2) and DuetSim(LLAMA2), which can be attributed to the strong inference capabilities of ChatGPT and FLAN-T5.ChatGPT's strong inference capabilities are largely due to its large parameter count, whereas for FLAN-T5, the benefits arise from its architecture, data, and instruction fine-tuning, which enable it to also possess good inference capabilities.</p>
<p>We further compare with variants of DuetSim that do not include response verifier.We include all the requirements in the prompt for the single LLM.Results show that performance deteriorate after removing the response verifier from the user simulator.The utterance generation with CoT and the interaction between response verifier and dialogue generator greatly improve the performance of DuetSim.</p>
<p>Utterance Diversity: Table 3 presents the performance of different models in terms of utterance diversity using various automated metrics, which demonstrates that our utterance generation mechanism based on CoT significantly improves the language diversity in all metrics.Due to differences in the training data, architecture, and parameter counts of various models, there are variations in performance among these models as well.</p>
<p>Ablation Study</p>
<p>To investigate the effectiveness of different components in our prompts, we conducted an ablation experiment on both the goal and dialogue history parts.The results of ablation experiments in Table 4 demonstrate the effectiveness of the two components in our prompts.Removing dialogue history or user goal will results in the worsening of performance since both contains important information for understanding the current status of the dialogue.We further investigate the impact of different forms of dialogue context, namely dialogue acts or natural language utterances.Results in Table 5 show that DuetSim is better at comprehending dialogue context in the form of natural language, which supports our initial design that uses natural language to express dialogue history.</p>
<p>Cross-model Evaluation</p>
<p>An ideal user simulator should help the dialogue system obtain better generalization ability.With such observation, we conduct a cross-model evaluation where we train the dialogue system on a user simulator first, then test the dialogue system on another simulator.The training of dialogue system is driven by reinforcement learning algorithm, i.e. proximal policy optimization (PPO) (Schulman et al., 2017).The experiment results are reported in 6.</p>
<p>We observe that the dialogue system training on DuetSim and testing on ABUS performs much better than the one training on ABUS and testing on DuetSim.This indicates that training on DuetSim greatly improves the generalization ability of the dialogue system.</p>
<p>In the meantime, we also observe that, when training and testing the dialogue system on the same user simulator, dialogue system that interacts with DuetSim perform much worse than the that interacts with ABUS.This indicates that responding to dialogues generated by DuetSim is more challenging than interacting with ABUS.The above differences come from the fact ABUS is driven by human-engineered agenda, which means the generated response may lack the diversity and stochasticity that are better handled by LLMs.Training the dialogue system using DuetSim is more challenging and but it also improves the generalizability for the dialogue system.</p>
<p>Human Evaluation</p>
<p>Apart from evaluating the user simulator with automatic metrics, we further conduct human evaluation to study human user's preference towards different user simulators.The human evaluation involves four user simulators, ABUS-T, ABUS-S, DuetSim (ChatGPT), DuetSim (FLAN-T5).</p>
<p>We recruited 20 human annotators to take part in the experiments.For each user simulator, we first generate 50 task-oriented dialogues by letting the user simulator interact with a rule-based dialogue system.In total, we generated 200 dialogues for human evaluation.And we evaluated dialogues from three dimensions, namely naturalness, informativeness, and coherence.We ask annotators to rate each dialogue among 0 (poor), 1 (average), and 2(good) for each dimension.</p>
<p>Table 7 reports the average score for the experiment.We found that DuetSim (ChatGPT) performs better on naturalness and informativeness compared with all other user simulators.DuetSim (Chat-GPT) also performs on par with ABUS-T in terms of coherence.Such results show that DuetSim is able to generate response that better fit human user's preference.In the meantime, we observe that DuetSim (FLAN-T5) does not perform well in human evaluation, though it is one of the top performer when evaluating with automatic metrics.When inspecting the output of FLAN-T5, we found that FLAN-T5 often fail to generate proper utterance from dialogue acts.Such results show that the discrepancies between automatic metrics and human evaluation for task-oriented dialogues cannot be neglected.</p>
<p>Case Study</p>
<p>Fig 3 presents the dialogue of different models when goal is set as follows: "You are looking forward to trying local restaurants.You are looking for a particular restaurant.Its name is called ugly duckling.Once you find a restaurant, make sure you get phone number.Make sure to ask about what food it serves."User simulators must meet goal requirements.However, models without a response verifier may make direct food inquiries from a restaurant without specifying the restaurant's name for their intended dining location.This lack of restau-rant information prevents the system from offering the correct details, leading to inaccurate responses, such as identifying the restaurant as Italian when the user wants to reserve a table at a Chinese restaurant.Consequently, a dialogue system lacking restaurant information fails to align with goal requirements, resulting in an unsuccessful dialogue.In contrast, DuetSim, equipped with a response verifier, identifies errors and provides feedback, enabling the generator to focus on specific prompts.As a result, our model can accurately deliver semantic actions, ensuring the system provides the correct information.</p>
<p>Conclusion</p>
<p>In conclusion, this paper introduces a zero-shot user simulator that is based on dual large language models, which consists of a generator and a verifier.The generator first generates draft response while the verifier examines the draft response and provides feedback.We achieve this by in-context learning and employing chain of thoughts to produce natural and high-quality natural language responses.Empirical experiments show that our model get competitive results on MultiWOZ.</p>
<p>Future work will focus on extending methods to the multi-modal task-oriented dialogue domain or attempting to address issues with large language models not paying attention to intermediate portions in long-context tasks.</p>
<p>Figure 2 :
2
Figure 2: Details of DuetSim.</p>
<p>Figure 3: Dialogues generated by the model w/ and w/o verifier.</p>
<p>Table 1 :
1
Response Generation with CoT guidance through examples, enabling the large language model to effectively translate dialogue actions.
ProcedureExample[EXAMPLE][['inform', 'restaurant', 'book day', 'Tuesday']]Dialog ActionThe restaurant is booked on Tuesday.-&gt;Utterance[END EXAMPLE]{Dialog Action}Please translate the list into natural language.[CONVERSATION]{CONVERSATION}Utterance-&gt; Enhanced Utterance[SENTENCE] {Utterance} Based on conversation, play the role ofCUSTOMER and rewrite this sentence to makeit smoother, more natural, and more conversational</p>
<p>Table 3 :
3
Utterance Diversity Comparison on MultiWOZ dataset
User SimulatorComplete Rate Success Rate Precision Recall F 1 -score Book Rate TurnABUS0.970.970.9020.983 0.9240.97010.36PBUS0.410.300.5800.670 0.7100.6597.50 2DuetSim (ChatGLM2)0.190.250.6990.646 0.6440.33217.22DuetSim (LLAMA2)0.190.280.7480.688 0.6870.06017.42DuetSim (ChatGPT)0.920.740.8300.980 0.8810.58516.92DuetSim (ChatGPT w/o verifier) 0.850.670.8420.948 0.8730.54417.88DuetSim (FLAN-T5)0.920.710.8200.979 0.8720.64816.16DuetSim (FLAN-T5 w/o verifier) 0.900.630.8340.961 0.8750.55116.18Table 2: Goal Fulfillment Comparison on MultiWOZ datasetUser SimulatorUnigrams Bigrams Trigrams Entropy CEMSTTR HDD MTLDABUS-T446164127176.872.39 0.710.75 45.97ABUS-S413151423867.042.37 0.760.79 62.35PBUS949444071767.403.00.700.78 45.50DuetSim (ChatGPT)1032278138947.442.62 0.770.78 56.98DuetSim (FLAN-T5)916227428187.582.73 0.740.79 50.63DuetSim (ChatGLM2) 1288391357937.512.31 0.740.855.28DuetSim (LLAMA2)1421444667537.112.14 0.770.75 63.75</p>
<p>Table 4 :
4
Ablation results.
US ModelComplete Rate Success Rate Precision Recall F 1 -score Book Rate TurnDuetSim0.920.740.8300.980 0.8810.58516.92w/o Dialogue history 0.850.680.7930.904 0.8240.62617.86w/o Goal0.910.700.8150.980 0.8710.45516.52w/o Both0.890.660.8210.939 0.8590.58417.46User SimulatorComplete Rate Success Rate Precision Recall F 1 -score Book Rate TurnDuetSim w. dialogue context in utterance0.920.740.8300.9800.8810.58516.92DuetSim w. dialogue context in dialogue act0.890.660.8530.9680.8900.52316.60</p>
<p>Table 5 :
5
DuetSim with different forms of dialogue context.</p>
<p>Table 6 :
6
Cross-model evaluation results.
TrainingTesting DuetSimABUSDuetSimComplete 0.55 Complete 0.87Success 0.46Success 0.83ABUSComplete 0.46 Complete 0.91Success 0.41Success 0.88</p>
<p>Table 7 :
7
Human preference evaluation on Multi-WOZ.
User Simulator Naturalness Informativeness CoherenceABUS-T1.401.081.38ABUS-S1.441.321.22DuetSim (ChatGPT)1.601.421.36DuetSim (FLAN-T5)0.660.300.52
PBUS enforces a maximum 15-turn limit for dialogues, unlike other models.
https://openai.com/chatgpt
AcknowledgementsThis work was supported by the National Natural Science Foundation of China (NSFC) under Grant Nos.61966038 and 62266051, the Yunnan University Talent Research Startup Support Project under grant No. CY22623101 and No. CZ22623101, the Postgraduate Research and Innovation Foundation of Yunnan University under Grant No. KC-23234274 and the Exam-Exempted Postgraduate Research and Innovation Foundation of Yunnan University under Grant No. TM-23236972.The authors would like to thank the anonymous reviewers for their constructive comments.
. Dialog Action Dialog Action. request', 'restaurant', 'food', '?']</p>
<p>Ugly duckling. request', 'restaurant', 'phone', '?']</p>
<p>Utterance Utterance Bibliographical References Layla El Asri, Jing He, and Kaheer Suleman. arXiv:1607.000702016arXiv preprintA sequence-to-sequence model for user simulation in spoken dialogue systems</p>
<p>Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Advances in neural information processing systems. 33</p>
<p>Multi-WOZ -a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. Pawe Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iigo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gai, 10.18653/v1/D18-1547Proceedings of the 2018 Conference on Empirical Methods Natural Language Processing. the 2018 Conference on Empirical Methods Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>GLM: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, 10.18653/v1/2022.acl-long.26Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>User modeling for task oriented dialogues. Izzeddin Gr, Dilek Hakkani-Tr, Gokhan Tr, Pararth Shah, 10.1109/SLT.2018.86396522018 IEEE Spoken Language Technology Workshop (SLT). 2018</p>
<p>End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2. Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, Kee-Eung Kim, 10.18653/v1/2020.acl-main.54Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Are large language models all you need for taskoriented dialogue?. Vojtch Hudeek, Ondrej Dusek, Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue. the 24th Meeting of the Special Interest Group on Discourse and DialoguePrague, Czechia2023Association for Computational Linguistics</p>
<p>Parameter estimation for agendabased user simulation. Simon Keizer, Milica Gai, Filip Jurek, Franois Mairesse, Blaise Thomson, Kai Yu, Steve Young, Proceedings of the SIGDIAL 2010 Conference. the SIGDIAL 2010 ConferenceTokyo, Japan2010Association for Computational Linguistics</p>
<p>Neural user simulation for corpus-based policy optimisation of spoken dialogue systems. Florian Kreyssig, Iigo Casanueva, Pawe Budzianowski, Milica Gai, 10.18653/v1/W18-5007Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue. the 19th Annual SIGdial Meeting on Discourse and DialogueMelbourneAustralia. Association for Computational Linguistics2018</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 10.18653/v1/2023.acl-long.291Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJanarXiv preprint</p>
<p>GenTUS: Simulating user behaviour and language in task-oriented dialogues with generative transformers. Christian Hsien-Chin Lin, Shutong Geishauser, Nurul Feng, Lubis, Michael Carel Van Niekerk, Milica Heck, Gasic, Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 23rd Annual Meeting of the Special Interest Group on Discourse and DialogueEdinburgh, UKAssociation for Computational Linguistics2022</p>
<p>Domainindependent user simulation with transformers for task-oriented dialogue systems. Nurul Hsien-Chin Lin, Songbo Lubis, Hu, Christian Carel Van Niekerk, Michael Geishauser, Shutong Heck, Milica Feng, Gasic, Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 22nd Annual Meeting of the Special Interest Group on Discourse and DialogueSingapore and Online. Association for Computational Linguistics2021</p>
<p>Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023arXiv preprint</p>
<p>The relationship of lexical richness to the quality of esl learners' oral narratives. The Modern Language. Xiaofei Lu, Journal. 9622012</p>
<p>Foundations of statistical natural language processing. Christopher Manning, Hinrich Schutze, 1999MIT press</p>
<p>Mtld, vocd-d, and hd-d: A validation study of sophisticated approaches to lexical diversity assessment. M Philip, Scott Mccarthy, Jarvis, Behavior research methods. 4222010</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.18653/v1/2023.acl-long.294Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231Association for Computational Linguistics</p>
<p>Agenda-based user simulation for bootstrapping a POMDP dialogue system. Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, Steve Young, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers. Rochester, New YorkAssociation for Computational Linguistics2007</p>
<p>A survey of statistical user simulation techniques for reinforcementlearning of dialogue management strategies. Jost Schatzmann, Karl Weilhammer, Matt Stuttle, Steve Young, The knowledge engineering review. 2122006</p>
<p>Effects of the user model on simulation-based learning of dialogue strategies. Jost Schatztnann, Karl Matthew N Stuttle, Steve Weilhammer, Young, IEEE Workshop on Automatic Speech Recognition and Understanding. IEEE2005. 2005</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Silvia Terragni, Modestas Filipavicius, Nghia Khau, Bruna Guedes, Andr Manso, Roland Mathis, arXiv:2306.00774-context learning user simulators for task-oriented dialog systems. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Slot dependency modeling for zero-shot cross-domain dialogue state tracking. Qingyue Wang, Yanan Cao, Piji Li, Yanhe Fu, Zheng Lin, Li Guo, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>A network-based end-to-end trainable taskoriented dialogue system. Tsung-Hsien Wen, David Vandyke, Nikola Mrki, Milica Gai, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, Steve Young, Proceedings of the 15th Conference of the European Chapter. Long Papers. the 15th Conference of the European ChapterValencia, SpainAssociation for Computational Linguistics20171</p>
<p>An accurate computation of the hypergeometric distribution function. Trong Wu, ACM Transactions on Mathematical Software (TOMS). 1911993</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Hierarchical template transformer for fine-grained sentiment controllable generation. Li Yuan, Jin Wang, Liang-Chih Yu, Xuejie Zhang, Information Processing &amp; Management. 5951030482022</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, arXiv:2304.09797Progressive-hint prompting improves reasoning in large language models. 2023arXiv preprint</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, arXiv:2308.07921Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. 2023arXiv preprint</p>
<p>ConvLab-2: An open-source toolkit for building, evaluating, and diagnosing dialogue systems. Qi Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi Takanobu, Jinchao Li, Baolin Peng, Jianfeng Gao, Xiaoyan Zhu, Minlie Huang, 10.18653/v1/2020.acl-demos.19Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2031 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2031</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2031</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-279305609</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.09367v1.pdf" target="_blank">COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content</a></p>
                <p><strong>Paper Abstract:</strong> While Generative AI has demonstrated strong potential and versatility in content generation, its application to educational contexts presents several challenges. Models often fail to align with curriculum standards and maintain grade-appropriate reading levels consistently. Furthermore, STEM education poses additional challenges in balancing scientific explanations with everyday language when introducing complex and abstract ideas and phenomena to younger students. In this work, we propose COGENT, a curriculum-oriented framework for generating grade-appropriate educational content. We incorporate three curriculum components (science concepts, core ideas, and learning objectives), control readability through length, vocabulary, and sentence complexity, and adopt a ``wonder-based'' approach to increase student engagement and interest. We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human expert analysis. Experimental results show that COGENT consistently produces grade-appropriate passages that are comparable or superior to human references. Our work establishes a viable approach for scaling adaptive and high-quality learning resources.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2031.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2031.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COGENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum-Oriented Generation for Educational Content</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-oriented LLM-guided framework that conditions passage generation on a hierarchical curriculum decomposition (science concept → core idea → learning objectives), enforces grade-specific readability constraints, and uses wonder-based inquiry prompts to produce grade-appropriate science reading passages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated with curriculum-conditioned prompting and controllable readability constraints</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>COGENT decomposes NGSS-style curricula into a three-level tuple {science concept, core idea, learning objective} and uses these tuples as structured inputs to LLM prompt templates. It first (optionally) generates multiple 'wonder' (inquiry) topics tied to the curriculum tuple, then prompts LLMs to produce reading passages constrained by target word count and readability (Flesch-Kincaid grade level), vocabulary, and sentence complexity. The approach mixes everyday language and 'wonder-based' framing to increase engagement and applies post-generation automated readability checks; evaluation is done with LLM-as-judge and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Gemma-2-9B-IT (9B), GPT-4o (version 20240806), Claude-3.5-Sonnet (version 20241022)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>elementary science reading passage generation / educational text generation (grades 1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Single-pass text-generation tasks per curriculum item: generate 1 short reading passage (≈ grade × 100 words) per wonder-topic; complexity varies with grade (grades 1–5) requiring progressively deeper explanations and higher lexical/syntactic complexity, but not multi-step procedural or interactive behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>BASE (baseline LLM prompting without curriculum conditioning), Human-written passages</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Curriculum Alignment: COGENT mean = 4.62 vs BASE mean = 4.08 (Mann-Whitney U, p < .05). Compared to human references on a 50-sample set: COGENT mean = 4.15, BASE = 3.23, Human = 3.49 (pairwise tests: COGENT significantly higher than BASE (p = .008**) and higher than Human (p = .029*)). Comprehensibility: COGENT mean = 4.58 vs BASE = 4.47 vs Human = 4.16 (COGENT > Human, p < .05; COGENT vs BASE not statistically significant in some comparisons). Curriculum item categorization accuracy (averaged across 237 samples): GPT-4o: BASE 0.654 → COGENT 0.785; Claude-3.5: BASE 0.616 → COGENT 0.726; Gemma-2: BASE 0.633 → COGENT 0.747 (relative gains ~17.8–20%). Readability control: COGENT outputs adhere more closely to target grade levels; BASE often exceeds target by ~2.5 grades in lower grades (e.g., BASE produced grade 3-4 reading level when targeting grade 1). Unique-vocabulary differences: BASE produced up to +26.1% unique words vs human at grade 2; COGENT reduced this gap and aligned more closely with human patterns in higher grades.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported (no learning-curve / convergence / episode-based learning speed metrics were measured).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported (no held-out-task generalization or cross-domain transfer metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Grouped generation: for each curriculum tuple three random 'wonder' topics were generated and three passages produced to assess diversity; diversity was qualitatively observed and averaged scores were reported to reduce annotator bias, but no standalone quantitative diversity metric beyond averaged scores was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed (no mechanism for discovering prerequisite dependencies between curriculum items was evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>COGENT generates multiple 'wonder' (inquiry) topics per curriculum tuple which serve as intermediate/bridging prompts for diverse passages, but it does not construct explicit intermediate training tasks or sequenced micro-tasks for learners or agents.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Observed limitations include: BASE (uninstructed/baseline prompting) tended to exceed target readability levels especially in lower grades; models can be inconsistent at higher grades (grades 3–5) without curriculum scaffolding; general caveats that LLM-generated outputs may not be consistently precise (cited prior work). Factual correctness errors were low (<6% average error rate) on elementary content but not absent. The evaluation also depends on LLM-based annotators which can introduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported (no runtime, inference cost, or generation-time comparisons provided).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Six elementary science teachers (≥10 years' experience) rated passages: experts assigned highest curriculum-alignment and comprehensibility ratings to COGENT-generated passages; BASE showed inconsistent performance (notably lower at grade 4); COGENT's superiority in comprehensibility vs human-written passages was statistically significant (p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>COGENT (curriculum-conditioned prompting + readability controls + wonder-based framing) significantly improves curriculum alignment vs a baseline prompting approach and vs human references, while maintaining equal or better comprehensibility. The framework better controls reading-level metrics to match target grades (especially lower grades), though dynamic/adaptive curricula, student-in-the-loop evaluation, and computational-cost analyses were not part of this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2031.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2031.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BASE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BASE (baseline LLM prompting without curriculum conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline generation approach used in experiments that prompts LLMs to generate reading passages around a wonder topic without providing the structured curriculum tuple or explicit readability constraints beyond a target word count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated with naive prompting (wonder-topic + word-count) without explicit curriculum tuples or readability scaffolding</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>BASE prompts ask an LLM (example: 'You are a science teacher. Generate a 100-word passage around the Wonder Topic for elementary grade X students. Mix science and everyday language.') — essentially unconstrained generation except for a requested word count and topical anchor; no hierarchical NGSS tuple or enforced Flesch-Kincaid constraints are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Gemma-2-9B-IT (9B), GPT-4o (version 20240806), Claude-3.5-Sonnet (version 20241022)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>elementary science reading passage generation / educational text generation</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Single-shot passage generation per wonder-topic; lacks curriculum scaffolding so complexity is source-dependent (models often produce higher lexical/syntactic complexity than target for early grades).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against COGENT and Human-written passages</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Curriculum Alignment: BASE mean = 4.08 vs COGENT 4.62 (Mann-Whitney U, p < .05). Against human references: BASE mean = 3.23 vs COGENT 4.15 and Human 3.49. Comprehensibility: BASE mean ≈ 4.47 (vs COGENT 4.58 and Human 4.16). Curriculum item categorization accuracies: GPT-4o BASE 0.654 -> COGENT 0.785; Claude BASE 0.616 -> COGENT 0.726; Gemma BASE 0.633 -> COGENT 0.747. Readability: BASE often exceeded intended grade levels (e.g., targeting grade 1 produced grade 3–4 reading levels). Vocabulary: BASE produced up to +26.1% unique words vs human at grade 2.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>BASE produced more lexical diversity (higher unique word counts) in early grades compared with human and COGENT; diversity reported by unique-word counts but not analyzed as a curriculum-design property.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No explicit generation of intermediate/bridging tasks; each passage is produced independently for a wonder-topic.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>BASE outputs commonly overshot target readability (higher grade-level text than intended), exhibited inconsistent curriculum alignment especially at higher grades, and showed more lexical diversity that could reduce grade-appropriateness for early readers.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Experts rated BASE passages as having comparable comprehensibility to COGENT in lower grades but worse alignment overall and inconsistent performance at higher grades (grade 3+).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>BASE (naive prompting) can yield coherent and comprehensible passages but frequently fails to meet curriculum alignment and grade-level readability constraints, particularly over-generating lexical and syntactic complexity for early grades; curriculum-conditioned prompting (COGENT) substantially reduces these failures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2031.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2031.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wonder-topic generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wonder-based Topic Question Generation (curriculum-grounded inquiry prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method for producing short inquiry-style topic questions ('wonder' prompts) from curriculum tuples to increase engagement and provide targeted anchors for passage generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated wonder-topic prompts guided by curriculum tuples (NGSS concept/core idea/learning objective) and constrained by target Flesch-Kincaid grade level</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Given a curriculum tuple, the system prompts an LLM to generate multiple short question-form 'wonder topics' suitable for the target grade (examples provided for grade 1). Prompts specify NGSS-aligned concept/core idea/learning outcome and require the output meet a target Flesch-Kincaid grade level. These wonder topics are then used as topical anchors for either BASE or COGENT passage generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not explicitly limited; examples in paper show system generating wonder topics (prompt templates included); experiments used Gemma-2-9B, GPT-4o, Claude-3.5-Sonnet for downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>educational content topic generation / inquiry-based question generation</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Produces short question prompts (simple, single-sentence queries) aligned to curriculum learning outcomes; complexity is low but tailored to grade-specific language constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Implicitly compared in that BASE and COGENT used the same wonder topics for controlled experiments against human passages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No standalone quantitative performance metrics reported for wonder-topic generation alone; used as input to passage generation experiments. The paper reports downstream improvements in curriculum alignment when using COGENT conditioned on curriculum tuples (which include wonder topics).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>For grouped generation the authors generated three random wonder topics per curriculum tuple to observe diversity in resulting passages; no dedicated diversity metric for the topics themselves was given.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Wonder topics act as multiple topical seeds (i.e., multiple intermediate prompts) for producing diverse passages, but they are not structured as sequenced learning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not reported specifically for topic generation; downstream limitations inherit those of the generation models (e.g., potential mismatch to higher-grade topics if wonder topics were extracted from human references not well-matched to grade).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Human experts evaluated the resulting passages (which were generated from these wonder topics) and found COGENT-conditioned passages scored higher; no direct human evaluation of the topics alone was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Using curriculum-grounded 'wonder' questions as topical anchors helps produce engaging, inquiry-focused passages; when combined with curriculum-conditioned generation (COGENT) it contributes to improved curriculum alignment and grade-appropriate readability in the generated materials.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2031.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2031.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge automated evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluation approach that uses an LLM (Claude-3.5-Sonnet) to score generated passages on curriculum alignment and comprehensibility dimensions, used alongside human expert ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-based automated evaluation (not a curriculum generation method per se)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>The paper uses an LLM (Claude-3.5-Sonnet) as an automated annotator to score passages on a 5-point scale for Curriculum Alignment and on four comprehensibility sub-dimensions (readability, correctness, coherence, engagement). They averaged scores across three generated passages per topic and validated that Claude-3.5-Sonnet performed consistently in preliminary testing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Claude-3.5-Sonnet (used as automated evaluator); GPT-4o was also used for some automated comparisons in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>automated evaluation of educational text generation</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Scoring-level evaluation: single-pass scoring per passage across multiple dimensions; used to compute averages and run nonparametric statistical tests (Mann-Whitney U).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Used to compare BASE vs COGENT and to compare against human expert ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to produce the primary quantitative metrics reported: Curriculum Alignment means and Comprehensibility means (e.g., BASE 4.08 vs COGENT 4.62 alignment per LLM-judge). The paper reports LLM-as-judge results aligned with human expert findings.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable (evaluation method only).</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Authors note reliance on an LLM annotator may introduce bias and that model evaluators are not perfectly consistent; they validated Claude-3.5-Sonnet in preliminary testing but still complemented automated scoring with human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported (no explicit cost/time for automated scoring conveyed).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Used as a complement: six teachers performed human evaluations which broadly confirmed LLM-as-judge outputs; authors used both to triangulate findings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>LLM-as-judge (Claude-3.5-Sonnet) provided consistent automated scores for curriculum alignment and comprehensibility that agreed with human experts, enabling scalable evaluation of many generated passages, but authors caution about potential evaluator bias and validated results with human teachers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2031.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2031.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-written passages (verified educational resources and textbooks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of 50 verified human-written elementary science reading passages from educational resources and textbooks used as a reference baseline for alignment, readability, and comprehensibility comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>hand-crafted by experts (human-authored educational texts)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Human passages were selected from verified educational resources and textbooks covering various science concepts across grades 1–5; each passage was annotated with curriculum standards and readability metrics and used as a high-quality reference set (word count set equal to grade × 100 in experiments to control length).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>elementary science reading passages / educational text</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Human passages reflect grade-appropriate explanations varying by grade; complexity follows standard curriculum progression (higher lexical diversity and longer passages for higher grades).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against BASE and COGENT generated passages</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Curriculum Alignment (human mean = 3.49) was lower than COGENT (4.15) but higher than BASE (3.23) in the 50-sample parallel comparison (Table 4). Comprehensibility (Human mean = 4.16) was lower than both BASE and COGENT in that comparison. Readability metrics: human passages slightly exceeded target grade levels but were closer to intended levels than BASE on average; unique-word counts were lower than BASE in early grades and comparable to COGENT in higher grades.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Human references were used to seed wonder topics and to measure lexical diversity; no explicit diversity-to-learning correlations were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Human passages were part of the materials rated by the six teacher experts; experts rated COGENT higher than human passages on some metrics (alignment and comprehensibility) in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Human-authored passages provided a realistic reference but in this controlled experiment COGENT-generated passages achieved higher curriculum-alignment and comparable or better comprehensibility, indicating curriculum-conditioned LLM generation can match or exceed human references under these experimental conditions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Leveraging LLMs for adaptive testing and learning in Taiwan adaptive learning platform (TALP) <em>(Rating: 2)</em></li>
                <li>An LLM-powered adaptive practicing system <em>(Rating: 2)</em></li>
                <li>Generating and evaluating tests for k-12 students with language model simulations: A case study on sentence reading efficiency <em>(Rating: 2)</em></li>
                <li>Branchsolve-merge improves large language model evaluation and generation <em>(Rating: 2)</em></li>
                <li>DnA-eval: Enhancing large language model evaluation through decomposition and aggregation <em>(Rating: 2)</em></li>
                <li>Gemma 2: Improving open language models at a practical size <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2031",
    "paper_id": "paper-279305609",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "COGENT",
            "name_full": "Curriculum-Oriented Generation for Educational Content",
            "brief_description": "A curriculum-oriented LLM-guided framework that conditions passage generation on a hierarchical curriculum decomposition (science concept → core idea → learning objectives), enforces grade-specific readability constraints, and uses wonder-based inquiry prompts to produce grade-appropriate science reading passages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated with curriculum-conditioned prompting and controllable readability constraints",
            "curriculum_method_description": "COGENT decomposes NGSS-style curricula into a three-level tuple {science concept, core idea, learning objective} and uses these tuples as structured inputs to LLM prompt templates. It first (optionally) generates multiple 'wonder' (inquiry) topics tied to the curriculum tuple, then prompts LLMs to produce reading passages constrained by target word count and readability (Flesch-Kincaid grade level), vocabulary, and sentence complexity. The approach mixes everyday language and 'wonder-based' framing to increase engagement and applies post-generation automated readability checks; evaluation is done with LLM-as-judge and human experts.",
            "llm_model_used": "Gemma-2-9B-IT (9B), GPT-4o (version 20240806), Claude-3.5-Sonnet (version 20241022)",
            "domain_environment": "elementary science reading passage generation / educational text generation (grades 1-5)",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Single-pass text-generation tasks per curriculum item: generate 1 short reading passage (≈ grade × 100 words) per wonder-topic; complexity varies with grade (grades 1–5) requiring progressively deeper explanations and higher lexical/syntactic complexity, but not multi-step procedural or interactive behaviors.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "BASE (baseline LLM prompting without curriculum conditioning), Human-written passages",
            "performance_metrics": "Curriculum Alignment: COGENT mean = 4.62 vs BASE mean = 4.08 (Mann-Whitney U, p &lt; .05). Compared to human references on a 50-sample set: COGENT mean = 4.15, BASE = 3.23, Human = 3.49 (pairwise tests: COGENT significantly higher than BASE (p = .008**) and higher than Human (p = .029*)). Comprehensibility: COGENT mean = 4.58 vs BASE = 4.47 vs Human = 4.16 (COGENT &gt; Human, p &lt; .05; COGENT vs BASE not statistically significant in some comparisons). Curriculum item categorization accuracy (averaged across 237 samples): GPT-4o: BASE 0.654 → COGENT 0.785; Claude-3.5: BASE 0.616 → COGENT 0.726; Gemma-2: BASE 0.633 → COGENT 0.747 (relative gains ~17.8–20%). Readability control: COGENT outputs adhere more closely to target grade levels; BASE often exceeds target by ~2.5 grades in lower grades (e.g., BASE produced grade 3-4 reading level when targeting grade 1). Unique-vocabulary differences: BASE produced up to +26.1% unique words vs human at grade 2; COGENT reduced this gap and aligned more closely with human patterns in higher grades.",
            "learning_speed_comparison": "Not reported (no learning-curve / convergence / episode-based learning speed metrics were measured).",
            "generalization_performance": "Not reported (no held-out-task generalization or cross-domain transfer metrics provided).",
            "task_diversity_analysis": "Grouped generation: for each curriculum tuple three random 'wonder' topics were generated and three passages produced to assess diversity; diversity was qualitatively observed and averaged scores were reported to reduce annotator bias, but no standalone quantitative diversity metric beyond averaged scores was provided.",
            "prerequisite_identification": "Not discussed (no mechanism for discovering prerequisite dependencies between curriculum items was evaluated).",
            "intermediate_task_generation": "COGENT generates multiple 'wonder' (inquiry) topics per curriculum tuple which serve as intermediate/bridging prompts for diverse passages, but it does not construct explicit intermediate training tasks or sequenced micro-tasks for learners or agents.",
            "llm_limitations_observed": "Observed limitations include: BASE (uninstructed/baseline prompting) tended to exceed target readability levels especially in lower grades; models can be inconsistent at higher grades (grades 3–5) without curriculum scaffolding; general caveats that LLM-generated outputs may not be consistently precise (cited prior work). Factual correctness errors were low (&lt;6% average error rate) on elementary content but not absent. The evaluation also depends on LLM-based annotators which can introduce bias.",
            "computational_cost": "Not reported (no runtime, inference cost, or generation-time comparisons provided).",
            "human_expert_evaluation": "Six elementary science teachers (≥10 years' experience) rated passages: experts assigned highest curriculum-alignment and comprehensibility ratings to COGENT-generated passages; BASE showed inconsistent performance (notably lower at grade 4); COGENT's superiority in comprehensibility vs human-written passages was statistically significant (p &lt; .05).",
            "key_findings_summary": "COGENT (curriculum-conditioned prompting + readability controls + wonder-based framing) significantly improves curriculum alignment vs a baseline prompting approach and vs human references, while maintaining equal or better comprehensibility. The framework better controls reading-level metrics to match target grades (especially lower grades), though dynamic/adaptive curricula, student-in-the-loop evaluation, and computational-cost analyses were not part of this work.",
            "uuid": "e2031.0"
        },
        {
            "name_short": "BASE",
            "name_full": "BASE (baseline LLM prompting without curriculum conditioning)",
            "brief_description": "A baseline generation approach used in experiments that prompts LLMs to generate reading passages around a wonder topic without providing the structured curriculum tuple or explicit readability constraints beyond a target word count.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated with naive prompting (wonder-topic + word-count) without explicit curriculum tuples or readability scaffolding",
            "curriculum_method_description": "BASE prompts ask an LLM (example: 'You are a science teacher. Generate a 100-word passage around the Wonder Topic for elementary grade X students. Mix science and everyday language.') — essentially unconstrained generation except for a requested word count and topical anchor; no hierarchical NGSS tuple or enforced Flesch-Kincaid constraints are provided.",
            "llm_model_used": "Gemma-2-9B-IT (9B), GPT-4o (version 20240806), Claude-3.5-Sonnet (version 20241022)",
            "domain_environment": "elementary science reading passage generation / educational text generation",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Single-shot passage generation per wonder-topic; lacks curriculum scaffolding so complexity is source-dependent (models often produce higher lexical/syntactic complexity than target for early grades).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared against COGENT and Human-written passages",
            "performance_metrics": "Curriculum Alignment: BASE mean = 4.08 vs COGENT 4.62 (Mann-Whitney U, p &lt; .05). Against human references: BASE mean = 3.23 vs COGENT 4.15 and Human 3.49. Comprehensibility: BASE mean ≈ 4.47 (vs COGENT 4.58 and Human 4.16). Curriculum item categorization accuracies: GPT-4o BASE 0.654 -&gt; COGENT 0.785; Claude BASE 0.616 -&gt; COGENT 0.726; Gemma BASE 0.633 -&gt; COGENT 0.747. Readability: BASE often exceeded intended grade levels (e.g., targeting grade 1 produced grade 3–4 reading levels). Vocabulary: BASE produced up to +26.1% unique words vs human at grade 2.",
            "learning_speed_comparison": "Not reported.",
            "generalization_performance": "Not reported.",
            "task_diversity_analysis": "BASE produced more lexical diversity (higher unique word counts) in early grades compared with human and COGENT; diversity reported by unique-word counts but not analyzed as a curriculum-design property.",
            "prerequisite_identification": "Not discussed.",
            "intermediate_task_generation": "No explicit generation of intermediate/bridging tasks; each passage is produced independently for a wonder-topic.",
            "llm_limitations_observed": "BASE outputs commonly overshot target readability (higher grade-level text than intended), exhibited inconsistent curriculum alignment especially at higher grades, and showed more lexical diversity that could reduce grade-appropriateness for early readers.",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "Experts rated BASE passages as having comparable comprehensibility to COGENT in lower grades but worse alignment overall and inconsistent performance at higher grades (grade 3+).",
            "key_findings_summary": "BASE (naive prompting) can yield coherent and comprehensible passages but frequently fails to meet curriculum alignment and grade-level readability constraints, particularly over-generating lexical and syntactic complexity for early grades; curriculum-conditioned prompting (COGENT) substantially reduces these failures.",
            "uuid": "e2031.1"
        },
        {
            "name_short": "Wonder-topic generation",
            "name_full": "Wonder-based Topic Question Generation (curriculum-grounded inquiry prompts)",
            "brief_description": "A method for producing short inquiry-style topic questions ('wonder' prompts) from curriculum tuples to increase engagement and provide targeted anchors for passage generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated wonder-topic prompts guided by curriculum tuples (NGSS concept/core idea/learning objective) and constrained by target Flesch-Kincaid grade level",
            "curriculum_method_description": "Given a curriculum tuple, the system prompts an LLM to generate multiple short question-form 'wonder topics' suitable for the target grade (examples provided for grade 1). Prompts specify NGSS-aligned concept/core idea/learning outcome and require the output meet a target Flesch-Kincaid grade level. These wonder topics are then used as topical anchors for either BASE or COGENT passage generation.",
            "llm_model_used": "Not explicitly limited; examples in paper show system generating wonder topics (prompt templates included); experiments used Gemma-2-9B, GPT-4o, Claude-3.5-Sonnet for downstream generation.",
            "domain_environment": "educational content topic generation / inquiry-based question generation",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Produces short question prompts (simple, single-sentence queries) aligned to curriculum learning outcomes; complexity is low but tailored to grade-specific language constraints.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Implicitly compared in that BASE and COGENT used the same wonder topics for controlled experiments against human passages.",
            "performance_metrics": "No standalone quantitative performance metrics reported for wonder-topic generation alone; used as input to passage generation experiments. The paper reports downstream improvements in curriculum alignment when using COGENT conditioned on curriculum tuples (which include wonder topics).",
            "learning_speed_comparison": "Not reported.",
            "generalization_performance": "Not reported.",
            "task_diversity_analysis": "For grouped generation the authors generated three random wonder topics per curriculum tuple to observe diversity in resulting passages; no dedicated diversity metric for the topics themselves was given.",
            "prerequisite_identification": "Not discussed.",
            "intermediate_task_generation": "Wonder topics act as multiple topical seeds (i.e., multiple intermediate prompts) for producing diverse passages, but they are not structured as sequenced learning tasks.",
            "llm_limitations_observed": "Not reported specifically for topic generation; downstream limitations inherit those of the generation models (e.g., potential mismatch to higher-grade topics if wonder topics were extracted from human references not well-matched to grade).",
            "computational_cost": "Not reported.",
            "human_expert_evaluation": "Human experts evaluated the resulting passages (which were generated from these wonder topics) and found COGENT-conditioned passages scored higher; no direct human evaluation of the topics alone was reported.",
            "key_findings_summary": "Using curriculum-grounded 'wonder' questions as topical anchors helps produce engaging, inquiry-focused passages; when combined with curriculum-conditioned generation (COGENT) it contributes to improved curriculum alignment and grade-appropriate readability in the generated materials.",
            "uuid": "e2031.2"
        },
        {
            "name_short": "LLM-as-judge",
            "name_full": "LLM-as-a-judge automated evaluation",
            "brief_description": "An automated evaluation approach that uses an LLM (Claude-3.5-Sonnet) to score generated passages on curriculum alignment and comprehensibility dimensions, used alongside human expert ratings.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-based automated evaluation (not a curriculum generation method per se)",
            "curriculum_method_description": "The paper uses an LLM (Claude-3.5-Sonnet) as an automated annotator to score passages on a 5-point scale for Curriculum Alignment and on four comprehensibility sub-dimensions (readability, correctness, coherence, engagement). They averaged scores across three generated passages per topic and validated that Claude-3.5-Sonnet performed consistently in preliminary testing.",
            "llm_model_used": "Claude-3.5-Sonnet (used as automated evaluator); GPT-4o was also used for some automated comparisons in experiments.",
            "domain_environment": "automated evaluation of educational text generation",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Scoring-level evaluation: single-pass scoring per passage across multiple dimensions; used to compute averages and run nonparametric statistical tests (Mann-Whitney U).",
            "is_curriculum_adaptive": null,
            "baseline_comparisons": "Used to compare BASE vs COGENT and to compare against human expert ratings.",
            "performance_metrics": "Used to produce the primary quantitative metrics reported: Curriculum Alignment means and Comprehensibility means (e.g., BASE 4.08 vs COGENT 4.62 alignment per LLM-judge). The paper reports LLM-as-judge results aligned with human expert findings.",
            "learning_speed_comparison": "Not applicable.",
            "generalization_performance": "Not applicable.",
            "task_diversity_analysis": "Not applicable (evaluation method only).",
            "prerequisite_identification": "Not applicable.",
            "intermediate_task_generation": "Not applicable.",
            "llm_limitations_observed": "Authors note reliance on an LLM annotator may introduce bias and that model evaluators are not perfectly consistent; they validated Claude-3.5-Sonnet in preliminary testing but still complemented automated scoring with human experts.",
            "computational_cost": "Not reported (no explicit cost/time for automated scoring conveyed).",
            "human_expert_evaluation": "Used as a complement: six teachers performed human evaluations which broadly confirmed LLM-as-judge outputs; authors used both to triangulate findings.",
            "key_findings_summary": "LLM-as-judge (Claude-3.5-Sonnet) provided consistent automated scores for curriculum alignment and comprehensibility that agreed with human experts, enabling scalable evaluation of many generated passages, but authors caution about potential evaluator bias and validated results with human teachers.",
            "uuid": "e2031.3"
        },
        {
            "name_short": "Human baseline",
            "name_full": "Human-written passages (verified educational resources and textbooks)",
            "brief_description": "A set of 50 verified human-written elementary science reading passages from educational resources and textbooks used as a reference baseline for alignment, readability, and comprehensibility comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "hand-crafted by experts (human-authored educational texts)",
            "curriculum_method_description": "Human passages were selected from verified educational resources and textbooks covering various science concepts across grades 1–5; each passage was annotated with curriculum standards and readability metrics and used as a high-quality reference set (word count set equal to grade × 100 in experiments to control length).",
            "llm_model_used": null,
            "domain_environment": "elementary science reading passages / educational text",
            "is_interactive_text_environment": false,
            "is_compositional": false,
            "task_complexity_description": "Human passages reflect grade-appropriate explanations varying by grade; complexity follows standard curriculum progression (higher lexical diversity and longer passages for higher grades).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared against BASE and COGENT generated passages",
            "performance_metrics": "Curriculum Alignment (human mean = 3.49) was lower than COGENT (4.15) but higher than BASE (3.23) in the 50-sample parallel comparison (Table 4). Comprehensibility (Human mean = 4.16) was lower than both BASE and COGENT in that comparison. Readability metrics: human passages slightly exceeded target grade levels but were closer to intended levels than BASE on average; unique-word counts were lower than BASE in early grades and comparable to COGENT in higher grades.",
            "learning_speed_comparison": "Not applicable.",
            "generalization_performance": "Not applicable.",
            "task_diversity_analysis": "Human references were used to seed wonder topics and to measure lexical diversity; no explicit diversity-to-learning correlations were reported.",
            "prerequisite_identification": "Not discussed.",
            "intermediate_task_generation": "Not applicable.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "Not applicable.",
            "human_expert_evaluation": "Human passages were part of the materials rated by the six teacher experts; experts rated COGENT higher than human passages on some metrics (alignment and comprehensibility) in the study.",
            "key_findings_summary": "Human-authored passages provided a realistic reference but in this controlled experiment COGENT-generated passages achieved higher curriculum-alignment and comparable or better comprehensibility, indicating curriculum-conditioned LLM generation can match or exceed human references under these experimental conditions.",
            "uuid": "e2031.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Leveraging LLMs for adaptive testing and learning in Taiwan adaptive learning platform (TALP)",
            "rating": 2
        },
        {
            "paper_title": "An LLM-powered adaptive practicing system",
            "rating": 2
        },
        {
            "paper_title": "Generating and evaluating tests for k-12 students with language model simulations: A case study on sentence reading efficiency",
            "rating": 2
        },
        {
            "paper_title": "Branchsolve-merge improves large language model evaluation and generation",
            "rating": 2
        },
        {
            "paper_title": "DnA-eval: Enhancing large language model evaluation through decomposition and aggregation",
            "rating": 2
        },
        {
            "paper_title": "Gemma 2: Improving open language models at a practical size",
            "rating": 1
        }
    ],
    "cost": 0.01731625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content
11 Jun 2025</p>
<p>Zhengyuan Liu liu_zhengyuan@i2r.a-star.edu.sg 
Institute for Infocomm Research (I 2 R)
A*STARSingapore</p>
<p>Stella Xin Yin 
Nanyang Technological University
Singapore</p>
<p>Hoe-Lian Dion 
Goh 
Nanyang Technological University
Singapore</p>
<p>Nancy F Chen nfychen@i2r.a-star.edu.sg 
Institute for Infocomm Research (I 2 R)
A*STARSingapore</p>
<p>COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content
11 Jun 2025FE28A9D61566989FAF6CFF0CE3C94829arXiv:2506.09367v1[cs.CL]
While Generative AI has demonstrated strong potential and versatility in content generation, its application to educational contexts presents several challenges.Models often fail to align with curriculum standards and maintain gradeappropriate reading levels consistently.Furthermore, STEM education poses additional challenges in balancing scientific explanations with everyday language when introducing complex and abstract ideas and phenomena to younger students.In this work, we propose COGENT, a curriculum-oriented framework for generating grade-appropriate educational content.We incorporate three curriculum components (science concepts, core ideas, and learning objectives), control readability through length, vocabulary, and sentence complexity, and adopt a "wonder-based" approach to increase student engagement and interest.We conduct a multi-dimensional evaluation via both LLM-asa-judge and human expert analysis.Experimental results show that COGENT consistently produces grade-appropriate passages that are comparable or superior to human references.Our work establishes a viable approach for scaling adaptive and high-quality learning resources.</p>
<p>Introduction</p>
<p>Educational content, particularly reading materials, is considered an integral part of supporting effective learning across disciplines.Traditionally, the creation of educational materials has relied mainly on human authors.This limits scalability and adaptability when curriculum standards evolve or when diverse learning needs must be addressed at scale.Generative AI techniques, such as Large Language Models (LLMs), have demonstrated remarkable potential in various content generation (Achiam et al., 2023;Team et al., 2024).However, their application to educational contexts presents several * Equal contribution.challenges.While models can generate grammatically correct and coherent passages, they often fail to align with established curriculum standards (Xiao et al., 2023;Liu et al., 2024b).Moreover, it is difficult to maintain consistent grade-appropriate reading levels, as both sentence structure and vocabulary complexity impact student comprehension and learning outcomes (Zamanian and Heydari, 2012).STEM education poses an additional challenge of balance between science and everyday language when introducing complex and abstract concepts to younger students (Blown and Bryce, 2017;Gilbert and Byers, 2017).Therefore, creating materials that effectively bridge science terminologies with real-world examples while maintaining pedagogical value requires professional knowledge and multi-dimensional efforts (Bansiong, 2019).</p>
<p>To address these problems, here we propose a framework Curriculum-Oriented Generation for Educational Content (COGENT), which creates science reading materials aligned with curriculum standards and adapts to grade-specific readability requirements.This framework consists of three components: curriculum formulation, controllable content generation, and multi-dimensional evaluation.Grounded in well-established education standards such as the Next Generation Science Standards (NGSS) (States, 2013), we build the structured guidance by linking science concepts (e.g., grades 1-5) with core ideas and their corresponding learning objectives, which creates systematic alignment with pedagogical value.For readability control, we implement constraints on word number, vocabulary, and sentence complexity based on grade-level reading proficiency (Flesch, 1948).Further, inspired by inquiry-based learning (Dewey, 1986), we incorporate a "wonder-based" learning approach that transforms core scientific ideas into inquiry-driven topics to engage students with science learning and discovery.</p>
<p>To comprehensively evaluate our framework and its pedagogical effectiveness, we build a multidimensional validation protocol and conduct quantitative analyses of the generated content across curriculum alignment, comprehensibility, and readability metrics.Based on the COGENT framework, our experiments with three representative LLMs (Gemma-2-9B, GPT-4o, Claude-3.5-Sonnet)indicate that: (1) models can follow curriculum guidance to create educational content that aligns closely with established pedagogical standards;</p>
<p>(2) models not only maintain high comprehensibility but also demonstrate adaptability in adjusting length, vocabulary, and sentence complexity to meet grade-specific reading requirements.The findings suggest that with proper scaffolding and constraint mechanisms, LLM-based systems can serve as a complement to human expertise in educational content development, which enables access to high-quality, curriculum-aligned reading materials across diverse educational contexts.This work not only advances our understanding of how to effectively harness models for educational purposes but also establishes a foundation for future investigations into automated content generation, with broader applications for personalized learning.</p>
<p>2 Related Work</p>
<p>AI-generated Content in Education</p>
<p>Advancements in LLMs have accelerated the adoption of AI in educational contexts, particularly in automating traditionally time-consuming content generation tasks such as providing feedback, creating assessment materials, and generating learning recommendations (Yan et al., 2024;Liu et al., 2024b,c).These efforts provide customized learning materials to students based on individual factors such as learning status, preferences, and goals (Wang et al., 2024;Liu et al., 2024a).For example, Kuo et al. (2023) demonstrated how to generate dynamic learning paths for students based on their most recent knowledge mastery assessment results.</p>
<p>Similarly, Kabir and Lin (2023) enhances content generation by incorporating knowledge concept structures throughout the process.While these methods show promise, they mainly focus on students' own learning trajectories and knowledge structures, with little attention given to standardized curriculum frameworks.Additionally, the generated content often fails to appropriately differentiate reading levels.</p>
<p>To evaluate LLM-generated content, researchers combined automatic and expert analysis.For instance, Lee et al. (2024) investigated LLMs' capability in generating test questions, with both automatic evaluation and expert analysis confirming that these models can produce questions with high validity and reliability for language learning.Similarly, Zelikman et al. (2023) developed a reading comprehension exercise generation system for middle school English learners, demonstrating that AIgenerated materials can not only meet students' learning needs but, in some cases, surpass the quality of human-written materials.In computer science education, Lee and Song (2024) examined the effectiveness of AI-generated content in explaining programming concepts, further validating the potential of LLMs in educational content creation.</p>
<p>While current evaluation of AI-generated content focuses mainly on language and facts (Xiao et al., 2023), real-world educational assessment requires broader criteria including curriculum alignment, pedagogical scaffolding, and grade-level appropriateness (Bansiong, 2019;Berndt and P. Wayland, 2014).This lack of comprehensive evaluation standards hinders educators' interest and trust in implementing AI-generated resources.</p>
<p>Evaluation Metrics of Education Materials</p>
<p>The evaluation of educational content includes three aspects: readability, comprehensibility, and curriculum alignment.These factors collectively determine whether learning materials are "appropriate to the student's age and level of knowledge" and "prepared in line with the curricula."Comprehensibility and Readability serve as fundamental metrics in analyzing educational texts (Zamanian and Heydari, 2012).Readability is a textual characteristic that measures how easily text can be read and understood (Klare, 1974), while comprehensibility reflects how effectively readers can construct meaning from the text (Sadoski et al., 2000;Beck et al., 1991).As Lakoff and Johnson (1980) emphasizes, "understanding is only possible through the negotiation of meaning."When these aspects are misaligned, students may experience frustration or disengagement (Bansiong, 2019).</p>
<p>Curriculum alignment aims to ensure it meets educational standards while remaining appropriate for learners' grade levels (Anderson, 2002).This evaluation ensures that educational materials are not only readable and comprehensible but also serve their intended pedagogical purposes within the edu-cational framework (Squires, 2012;Wijngaards-de Meij and Merx, 2018).</p>
<p>Value of "Wonder" in Science Education</p>
<p>"The most beautiful thing we can experience is the mysterious.It is the source of all true art and science."(Einstein, 1931) Inquiry-based learning is rooted in the work of Dewey (1986), who underlines that education begins with the curiosity of the learner.Inquiry is understood in two ways: (1) "inquiry as means" (inquiry in science) refers to using inquiry as an instructional approach to help students develop their understanding of science content; (2) "inquiry as ends" (inquiry about science) refers to inquiry as a learning outcome (National Research Council, 2000;Abd-El-Khalick et al., 2004).However, when students inquire about scientific knowledge, they often experience a gap between their intuitive comprehension and their ability to express understanding (Blown and Bryce, 2017).They frequently struggle to express their observations and questions using scientific language.This disconnect highlights the need for level-appropriate educational content that can bridge the gap between students' intuitive understanding and formal scientific language.Given this challenge, it is recommended to introduce scientific concepts through "wonder why" questions that trigger children's natural curiosity while reducing the barriers of science terminologies (Chin and Brown, 2002;Gilbert and Byers, 2017).Moreover, wonder-based explanatory texts are effective for reading comprehension, science learning, and conceptual change (Lindholm, 2018;Jirout, 2020).</p>
<p>Curriculum-Oriented Generation for Educational Content</p>
<p>The framework is designed to transform abstract curriculum components into engaging, wonderbased reading materials that improve students' understanding while adhering to grade-specific readability requirements.It consists of three parts: curriculum formulation, controllable content generation, and multi-aspect evaluation (see Figure 1).</p>
<p>COGENT-based Generation</p>
<p>To ground our approach in the Next Generation Science Standards (NGSS), a well-established K-12 science education framework (States, 2013). 1 We decompose the curriculum into three hierarchical elements: science concepts, core ideas, and learning objectives.As shown in Figure 2, science concepts can be mapped to core ideas, and each core idea is related to learning outcomes, creating a comprehensive curriculum coverage matrix.More specifically, for elementary school students (grades 1-5, ages 6-11), 29 science concepts (e.g., "Matter and Its Interactions") are broken down into 79 core ideas (e.g., "Structure and Properties of Matter.Matter can be described and classified by its observable properties."),then further mapped to specific learning outcomes that detail what students should master at each grade level (e.g., "To describe and classify different kinds of materials by their observable properties (Grade 2).").</p>
<p>Importantly, concepts and core ideas can appear across multiple grade levels, requiring different depths of explanation and language complexity (see Figure 2).As shown in Table 1, human-written science reading passages show clear patterns across grade levels: the average number of words, reading difficulty scores (lexile) (White and Clement, 2001), and lexical diversity all increase steadily as students progress from grade 1 to grade 5. We thus indicate the word number and target readability level (Klare, 1974;Flesch, 1948) 2 along with the curriculum input to ensure generated content matches students' reading abilities at each grade.</p>
<p>Moreover, to enhance students' interest and engagement, we consider "Science as Wonder" and "everyday language" as a bridge to connect scien-  Ideas (e.g., LS1-A: Structure and Function), which then connect to specific Learning Outcomes for each grade level (e.g., 1-LS1-1).The same core idea may appear across multiple levels with increasing complexity.For example, LS1-A (Structure and Function) progresses from grade 1 to grade 4. tific concepts and their daily experiences.Given the decomposed curriculum items, each core idea can be used to generate multiple exploration questions.For example, the core idea about environmental adaptation can be linked to wonder topics such as "Why do birds migrate?" or "How can we help protect animals' home?"This approach maintains curriculum alignment while fostering student curiosity through diverse and interesting content.When explaining bird migration, the generated passage begins with an interesting observation ("Some birds disappear in the fall and come back in the spring."),followed by clear explanations of stories and scientific concepts, and concludes with broader implications for environmental understanding.</p>
<p>Multi-dimensional Evaluation</p>
<p>While LLM-generated content can be modulated along desired dimensions to meet specific requirements, it may not perform consistently and precisely (Saha et al., 2024;Li et al., 2025).We thus propose a multi-dimensional evaluation to validate pedagogical effectiveness and generation quality.</p>
<p>First, we evaluate Curriculum Alignment through scoring and categorization schemes.The scoring evaluates how well the content adheres to the specified curriculum item, and the categorization examines whether the passage delivers exact core ideas and outcomes at each grade level.Evaluation examples are shown in Table 7.</p>
<p>Curriculum Alignment Scoring: We rate the passage compliance with the standards using a 5-point scale (1 = does not align at all, 5 = fully aligned).Given a sample set, we calculate the average score to determine its overall curriculum alignment.Curriculum Item Categorization: Since science concepts appear in multiple grade levels, we first group passages by concept (e.g., "From Molecules to Organisms: Structures and Processes"), and classify them into the corresponding curriculum item: a tuple of {concept, core idea, learning out-come}.For example, as shown in Figure 2 and Table 7, the input passage will be classified into one of the seven types (e.g., "Type A (core idea): Structure and Function.All organisms have external parts", "Type G: Organization for Matter and Energy Flow in Organisms").</p>
<p>We then evaluate the Comprehensibility from four aspects following previous work (Celikyilmaz et al., 2020).This is to show how effectively readers can construct meaning from the text.Each dimension is in a 5-point Likert scoring: Readability (How easily the text can be read and understood), Correctness (The accuracy of factual content about the topic), Coherence (The consistency between the content and the topic), and Engagement (To what extent the "wonder-based" topic and passage capture and maintain readers' interest).Examples can be found in Table 8.</p>
<p>Moreover, we use four common statistical methods to assess Text Readability based on linguistic features: Flesch Reading Ease/Flesch Kincaid Grade Level (Flesch, 1948) evaluates readability using sentence length and syllable count, with scores from 0-100 (higher meaning easier to read) or converted to grade levels.Gunning Fog Index (Gunning, 1968) measures complexity through sentence length and percentage of complex words, indicating education years needed for comprehension.Automated Readability Index (Smith and Senter, 1967) and Coleman Liau Index (Liau et al., 1976) differ from other formulas by using character count instead of syllable count, along with average word and sentence length (see examples in Table 9).</p>
<p>Experimental Setting</p>
<p>We conducted extensive experiments on science reading passage generation to examine both the effectiveness and pedagogical value of COGENT.Since this task requires structured instruction following and coherent language generation, we applied and tested three representative LLMs: Gemma-2-9B-IT (Team et al., 2024), GPT-4o3 (version 20240806), and Claude-3.5-Sonnet4(version 20241022).We use the default generation parameters (e.g., temperature, top-p) in their model configurations.The example instructions for wonder question generation, and BASE and COGENT passage generation are shown in Table 6.</p>
<p>Comparison through Grouped Generation and Human-written Passages</p>
<p>First, we collect and assess grouped passages generated from the same curriculum inputs to evaluate COGENT's capability in generating diverse yet consistent content.Given each {concept, core idea, learning outcomes} tuple, we randomly generated three "wonder" topics, then created corresponding reading passages for each topic.Moreover, we collect 50 human-written passages and build an evaluation set for extensive comparison.These passages were selected from verified educational resources and textbooks, covering various science concepts across elementary school grades 1-5.Each sample was annotated with corresponding curriculum standards and readability metrics, which provide a high-quality reference.</p>
<p>Evaluation Methods and Process</p>
<p>For automated evaluation, we leverage LLM-as-ajudge for automated scoring on the Curriculum Alignment and Comprehensibility scoring (Saha et al., 2024).In our preliminary testing, Claude-3.5-Sonnetperforms well as a consistent and accurate evaluator.To assess the grouped generation, we reported the average scores of three passages per topic to reduce intrinsic bias from the LLM-based annotator.We use an off-the-shelf tool to calcu-   late Text Readability scores.5Moreover, for curriculum item categorization, we group the 79 core ideas based on their science concepts and classify samples within each group.The accuracy is an indicator to measure the distinctness of grade-specific explanation depth and learning objectives.</p>
<p>For expert analysis, we recruited six elementary science teachers who have more than 10 years' teaching experience to conduct expert analysis.Teachers evaluated passages from grades 1-5, with each grade having three passages: human-written, BASE-generated, and COGENT-generated.The human evaluation consists of two surveys: Curriculum Alignment survey requires teachers to indicate their agreement on whether the passages aligned with corresponding grade-level science concepts and core ideas, and Comprehensibility survey requires them to rate each passage on four dimensions (readability, correctness, coherence, and engagement).Both surveys used the same items as the LLM-as-a-judge evaluation.</p>
<p>Experimental Results and Discussions</p>
<p>Results on Grouped Generation</p>
<p>In our experiments, we generated passages (three samples per curriculum item) with Gemma-2-9B, GPT-4o, and Claude-3.5-sonnet;the total number is 711.For the Curriculum Alignment scoring, we conducted Mann-Whitney U tests, and the results reveal significant improvements between BASE and COGENT frameworks (see Table 3).More specifically, COGENT (Mean = 4.62) achieves significantly higher alignment scores compared to BASE (Mean = 4.08) (p &lt; .05),indicating that CO-GENT effectively incorporates curriculum information into generated passages.As shown in Figure 3 (left), models with COGENT demonstrate higher scores across all grade levels.While Gemma-2-9B is in a smaller parameter size, it can provide rea-   sonable outputs following the curriculum condition, and GPT-4o performs slightly better.</p>
<p>Meanwhile, results of Curriculum Item Categorization also demonstrate COGENT's effectiveness on pedagogical alignment.For each model, we calculated and averaged the classification accuracy on 237 samples.GPT-4o achieves 0.785 with COGENT guidance, a 20% improvement compared to 0.654 of the BASE.Similarly, Claude-3.5 improves from 0.616 to 0.726 (17.8% relative gain) and Gemma-2 improves from 0.633 to 0.747.These improvements suggest that LLMs can follow the curriculum guidance to effectively reflect grade-specific content and objectives.</p>
<p>Regarding Comprehensibility, models with BASE and COGENT perform well and comparable (4.76 vs 4.81) (p = .083),as shown in Table 3; they do not have significant variance across grade levels, as shown in Figure 3 (right).This demonstrates that adding curriculum targets in the science reading passages does not affect the ease of comprehension.Moreover, we observed that tested LLMs perform well (&lt;6% averaged error rate) regarding Factual Correctness on the elementary Grade Human BASE COGENT 1 57.9 66.5 (+14.8%)66.5 (+14.8%) 2 87.7 110.6 (+26.1%)100.7 (+14.9%) 3 132.8153.2 (+15.3%)137.1 (+3.2%) 4 183.2 196.1 (+7.0%) 174.0 (-5.0%) 5 219.5 230.5 (+5.0%) 209.0 (-4.8%)Table 5: Comparison of unique words.Red and blue indicate the intensity of higher and lower scores compared with human-written passages, respectively.</p>
<p>school content writing (Hughes and Bae, 2023).We observed that LLMs are well-conditioned on the word count (see Table 2) at all grade levels.This ability to control length is important for creating grade-appropriate passages, as it is one of the factors that affect readability.However, on statistical Text Readability metrics, the two approaches perform differently.Results in Figure 4 show that COGENT adheres more closely to elementary reading levels, especially in lower grades (e.g., 1-2), where the BASE approach exceeds the intended level by around 2.5 grades.The above results highlight the distinction between readability (e.g., word count and sentence complexity) and actual comprehension ease, which depends on factors</p>
<p>Comparison to Human-written Passages</p>
<p>We used the same wonder topics and word numbers as the 50 human-written passages for a parallel comparison.Table 4 shows Mann-Whitney U test results among BASE, COGENT, and Human.We observe substantial improvement in Curriculum Alignment, and comparable scores in Comprehensibility.COGENT demonstrates much higher alignment scores (Mean = 4.15) than both BASE (Mean = 3.23) and Human (Mean = 3.49) (p &lt; .05).Similar to grouped generation (Section 5.1), COGENT achieves better alignment scores at all grades.This indicates that COGENT-guided passages align better with curriculum standards.Among the three LLMs, GPT-4o results in slightly higher scores (see Figure 5).Surprisingly, Human, BASE, and COGENT all receive lower alignment ratings in grades 3-5.This occurs because the wonder topics extracted from the human references are not well-matched in these higher grades.</p>
<p>Second, Comprehensibility evaluation results show that both BASE (Mean = 4.47) and COGENT (Mean = 4.58) outperform Human (Mean = 4.16) (p &lt; .05),while the difference between COGENT and BASE is not statistically significant.Interest-ingly, all three approaches maintain relatively high comprehensibility scores, while human-written passages show a notable decline from grade 3.There is a similar trend in readability evaluation results.</p>
<p>Third, Text Readability assessment results demonstrate that COGENT's performance more closely correlates with human references, although the latter slightly exceeds target grade levels.As shown in Figure 6, on the linguistic metrics, CO-GENT produces passages closer to the intended grade level, while BASE generates passages largely above intended grade levels.For example, when targeting grade 1 content, BASE produces text at grade 3-4 reading level, which creates potential comprehension barriers for early readers.Interestingly, we notice a sharp increase in difficulty level at grade 3, which represents the significant transition in science education at this level.In grade 2, science learning focuses on concrete concepts through basic observation, classification, and simple investigations of the natural world, while starting from grade 3, teachers introduce more complex scientific concepts requiring deeper analysis and abstract thinking.</p>
<p>We also calculate the unique word numbers of each passage created by Human, BASE, and CO-GENT.Both BASE and COGENT show higher vocabulary diversity than human writing in early grades, with BASE producing up to 26.1% more unique words at grade 2. This gap narrows in higher grades, where BASE still generates more unique words (+5-7%), while COGENT shifts to slightly lower lexical diversity (−5%) than human writing.The trend suggests that COGENT vocabulary usage becomes more aligned with human patterns as grade levels increase.</p>
<p>Expert Analysis</p>
<p>We conducted expert analysis by comparing automated approaches (w/ GPT-4o) and human reference (15 reading passages).As shown in Figure 7, Curriculum alignment results align with our previous evaluation findings.COGENT achieves consistently higher alignment scores.In contrast, human-written passages maintain moderate alignment across all grades, while the BASE shows declining alignment scores in higher grades.At each grade level, COGENT maintains the highest proportion of positive ratings.Human-generated content generally receives favorable evaluations.BASE shows the most inconsistent performance, with a particularly lower rating at grade 4.</p>
<p>Regarding Comprehensibility (see Figure 8), experts assigned the highest ratings to COGENTgenerated passages, with significant difference compared to human-written passages (p &lt; .05).Interestingly, BASE-generated passages and humanwritten passages exhibit similar comprehensibility levels in lower grades; however, their performance diverges significantly from grade 3.This divergence suggests that as grade levels increase and science concepts become more complex and abstract, the BASE framework fails to maintain appropriate readability, coherence, and engagement levels.In contrast, our framework maintains consistent comprehensibility scores at all grade levels.This highlights that based on our COGENT framework, LLM-generated reading materials achieve comparable or superior quality compared with humanauthored passages, and they can be a reasonable supplement to meet both curriculum alignment and readability requirements.</p>
<p>Conclusion</p>
<p>We presented COGENT, a curriculum-oriented framework for generating grade-appropriate educational content by incorporating structured curriculum components (e.g., concepts, core ideas, and learning objectives) alongside controlled readability parameters and the "wonder-based" inquiry approach.Extensive experiments with three LLMs and expert evaluations demonstrate that COGENT significantly improves curriculum alignment, maintains high comprehensibility while controlling text readability to match grade levels, and generates passages comparable or superior to human-written passages.These findings establish that properly guided LLMs can serve as effective tools for scaling adaptive learning resources, with implications for educational equity and accessibility.Since COGENT is a general framework, future work could explore finegrained personalization, interdisciplinary applications, and long-term learning outcomes to further enhance automated educational content generation.</p>
<p>Limitations</p>
<p>While this study advances the practical application of LLMs, it has some potential limitations that warrant future study.First, our framework focused on elementary education (grades 1-5); future work could extend it to middle and high school curricula and adapt the evaluation metrics for more complex science concepts.Second, we did not include elementary students in our sample analysis due to several considerations: their limited subject knowledge and lack of understanding of curriculum standards would affect their ability to evaluate quality.Additionally, in readability assessments, younger students tend to focus on surface-level features (like pictures and length) rather than the accuracy of scientific content, clarity of explanations, or scaffolding of complex ideas.These could potentially introduce bias in the assessment results.</p>
<p>Ethics and Impact Statement</p>
<p>We acknowledge that all of the co-authors of this work are aware of the provided ACL Code of Ethics and honor the code of conduct.In our experiments, models are applied under proper license.All data used in this work are only for academic research purposes and should not be used outside of academic research contexts.Our proposed methodology, in general, does not create a direct societal consequence and is intended to be used to improve accessibility and educational value.Here the curriculum items are based on the NGSS standard (States, 2013).In our experiments, based on humanwritten passages, we set the word count to be the grade level multiplied by 100.Flesch Kincaid Grade Level (Flesch, 1948) is used for readability control.</p>
<p>[Curriculum Information] Grade level: 3 Science concept: Biological Evolution: Unity and Diversity Core ideas: Biodiversity and Humans.Populations live in a variety of habitats and change in those habitats affects the organisms living there.Learning objectives: Make a claim about the merit of a solution to a problem caused when the environment changes and the types of plants and animals that live there may change.Topic Question: Why do birds migrate?</p>
<p>[Human-written Passage] Migration is very hard work, but the work has big benefits, such as food-lots of it!Many birds that migrate north in spring arrive just as huge numbers of insects are hatching.It's a buggy banquet full of protein for both hungry arriving adults and the young birds soon to be hatched.Growing chicks especially need lots of insect protein to grow big and strong.Spring also brings new seeds, fruits, and nectar-filled flowers.Thanks to all this food, many migratory birds raise more young than birds that stay in warm places all year long.Migration is also dangerous.Storms can kill birds or send them off course.Many migrating birds crash into windows and brightly lit tall buildings.One of the biggest problems for any migrating bird is the loss of habitat.Birds need healthy habitats everywhere they spend time throughout the year.That includes the places they nest and raise their young, the places they spend the winter, and the places they stop along their migratory journeys to rest and feed.They need wild unbuilt places like woods, beaches, grasslands, and wetlands.When natural areas are replaced by roads, homes, shopping centers, farms, and other human-made structures, there is less of the healthy habitat birds need throughout the year.A healthy habitat is also a place where there are plenty of native plants-the kinds of plants that grow naturally in a particular area.Those plants provide more nutritious fruit and host many more insects than plants that are from other places.When people replace native plants with nonnative plants, the amount of the right kind of food available to birds is reduced.Many people want to help migratory birds.In all kinds of communities, people are planting native plants that provide food and places for hummingbirds and songbirds to nest and rest.They are working to protect grasslands for the hawks that hunt there and other birds that nest there.Beaches, wetlands, and bodies of water are being cleaned and protected to provide healthy habitats for shorebirds, wading birds, and swimming birds like ducks and geese.</p>
<p>[BASE-generated Passage]</p>
<p>Have you ever seen a group of birds flying high in the sky, all moving in the same direction?They might be migrating!Migration is when birds travel from one place to another at certain times of the year.But why do they do this?One big reason birds migrate is to find food.When winter comes, many insects hide away and plants stop growing in colder places.Just like how we go to the grocery store when our refrigerator is empty, birds fly to warmer places where they can find plenty of tasty insects, seeds, and berries to eat.Some birds, like hummingbirds, travel thousands of miles just to find their favorite flower nectar!Another reason birds migrate is to stay safe and comfortable.Cold weather can be dangerous for birds -their tiny bodies need lots of energy to stay warm.By flying to warmer places, they can save their energy for important things like finding food and building nests.Many birds also migrate to find better places to raise their babies, where there are fewer predators and more materials to build cozy nests.When the weather changes again, many birds fly back to where they started.This cycle happens every year.Some clever birds, like geese, even use the sun and stars to help them find their way, just like a natural compass!So, the next time you see birds flying in a group, think about their amazing journey.They might be heading to their warm winter vacation or coming back home for spring!</p>
<p>[COGENT-generated Passage]</p>
<p>Have you ever wondered why some birds disappear in the fall and come back in the spring?This is called migration.Birds fly long distances to find better places to live and eat.It's like taking a vacation, but for birds, it's about survival.Many birds migrate because the weather changes.When it gets cold, some birds can't find enough food.Insects hide, and plants stop growing.So, these birds fly to warmer places where they can find more to eat.When spring comes, they fly back to their old homes.Birds face many challenges during migration.Flying long distances is hard work.They need to eat a lot before they leave to have enough energy for the trip.Some birds fly over big oceans or deserts where they can't stop to rest or eat.They have to be strong and healthy to make it.Another problem is that the places birds visit are changing.People build houses and cities where birds used to stop and rest.This makes it harder for birds to find safe places to sleep and eat during their journey.Some birds get confused by bright lights in cities and fly into buildings.Weather can also be a big problem for migrating birds.Strong winds and storms can blow them off course.They might end up in places they don't know, where it's hard to find food.Climate change is making some areas too hot or dry for birds, so they have to find new places to go.Birds have amazing ways to deal with these challenges.They can fly very high to catch good winds that help them travel faster.Many birds migrate in big groups, which helps them stay safe from predators.Some birds can even sleep while they're flying!People are trying to help migrating birds.They're making special areas called wildlife refuges where birds can rest and eat safely.Some cities turn off bright lights at night during migration season to help birds fly safely.Scientists are studying birds to learn more about how to protect them.You can help migrating birds too.Planting trees and flowers in your yard gives birds places to rest and eat.Keeping cats inside helps protect birds from getting hurt.By taking care of the places where birds live and stop during migration, we can make sure they have safe journeys every year.</p>
<p>Table 10: Three passages upon the same curriculum information.Text spans in blue highlight where scientific concepts and ideas are introduced.It also marks explanations of scientific phenomena that directly connect to learning objectives.</p>
<p>Figure 1 :
1
Figure 1: Overview of the framework of curriculum-oriented generation for educational content (COGENT).</p>
<p>Figure 2 :
2
Figure 2: Our curriculum decomposition example grounded in the Next Generation Science Standards (NGSS), which consist of four domains.It has a hierarchical structure where Science Concepts (e.g., LS1) branch into CoreIdeas (e.g., LS1-A: Structure and Function), which then connect to specific Learning Outcomes for each grade level (e.g., 1-LS1-1).The same core idea may appear across multiple levels with increasing complexity.For example, LS1-A (Structure and Function) progresses from grade 1 to grade 4.</p>
<p>Figure 3 :
3
Figure 3: Curriculum alignment scores (left) and comprehensibility scores (right) of Gemma-2-9B, GPT-4o, and Claude-3.5-Sonnetgenerated passages using BASE and COGENT framework.</p>
<p>Figure 4 :
4
Figure 4: Results on four readability metrics of LLM-generated passages using BASE and COGENT framework.</p>
<p>Figure 5 :
5
Figure 5: Results on curriculum alignment and comprehensibility of Human, BASE, and COGENT.</p>
<p>Figure 6 :
6
Figure 6: Results on readability metrics of human-written passages, BASE, and COGENT framework.</p>
<p>Figure 7 :
7
Figure 7: Expert analysis: curriculum alignment comparison of Human, BASE, and COGENT.</p>
<p>Figure 8 :
8
Figure 8: Expert analysis: comprehensibility score comparison among Human, BASE, and COGENT.</p>
<p>Wonder-based Topic Question Generation (w/ NGSS Standard) You are a science teacher (elementary school grade 1).According to the Science Concept, Core Ideas and Learning Outcomes, generate 5 different topics in the form of a short question for elementary grade 1 students.-The generated text should meet the Flesch Kincaid Grade Level for elementary grade 1 students.=== Science Concept === From Molecules to Organisms: Structures and Processes === Core Ideas === Structure and Function • All organisms have external parts.Different animals use their body parts in different ways to see, hear, grasp objects, protect themselves, move from place to place, and seek, find, and take in food, water and air.Plants also have different parts.=== Learning Outcomes === Students who demonstrate understanding can: Use materials to design a solution to a human problem by mimicking how plants and/or animals use their external parts to help them survive, grow, and meet their needs.*** System Output *** 1.How do animals use their legs to move? 2. What do plants use their leaves for?3. How does a bird use its beak to eat food? 4. Why do turtles have hard shells? 5. How do flowers help plants grow?BASE Passage Generation You are a science teacher (elementary school grade 1).Generate a 100-word reading passage around the Wonder Topic for elementary grade 1 students.Mix science and everyday language.=== Wonder Topic === Why do turtles have hard shells?COGENT-based Content Generation (w/ NGSS Standard) You are a science teacher (elementary school grade 1).Generate a 100-word reading passage around the Wonder Topic to teach students the Science Concept and Core Ideas, to meet the Learning Outcomes.Mix science and everyday language.-The generated text should meet the Flesch Kincaid Grade Level for elementary grade 1 students.=== Wonder Topic === Why do turtles have hard shells?=== Science Concept === From Molecules to Organisms: Structures and Processes === Core Ideas === What the student needs to learn: Structure and Function • All organisms have external parts.Different animals use their body parts in different ways to see, hear, grasp objects, protect themselves, move from place to place, and seek, find, and take in food, water and air.Plants also have different parts.=== Learning Outcomes === Students who demonstrate understanding can: Use materials to design a solution to a human problem by mimicking how plants and/or animals use their external parts to help them survive, grow, and meet their needs.</p>
<p>Table 2 :
2
Statistics of the generation length.
GradeTypeGemma-2 GPT-4o Claude-3.51BASE91.13110.3098.101COGENT82.03113.3099.172BASE151.13206.13204.542COGENT119.85193.13199.693BASE250.63336.61290.333COGENT215.44311.09292.674BASE350.50468.77404.864COGENT365.53418.23395.095BASE418.23590.21518.635COGENT387.21556.19492.00</p>
<p>Table 3 :
3
Statistical comparison of curriculum alignment and comprehensibility metrics: BASE vs COGENT.p-value is calculated through pairwise Mann-Whitney U tests with Bonferroni correction (*<em> p &lt;.01, * p &lt;.05).
MetricDescriptionBASE COGENT p-valueCurriculum Alignment How well content aligns with curriculum standards4.084.62.021</em>ComprehensibilityHow effectively readers can construct meaning from the text4.764.81.083(readability, correctness, coherence, and engagement)</p>
<p>Table 4 :
4MetricBASECOGENTHuman BASE vs COGENT BASE vs Human COGENT vs HumanCurriculum Alignment3.234.153.49.008<strong>.067.029<em>Comprehensibility4.474.584.16.053.022</em>.014*
Statistical comparison of curriculum alignment and comprehensibility: BASE vs COGENT vs Human p-value is calculated through pairwise Mann-Whitney U tests with Bonferroni correction (</strong> p &lt;.01, * p &lt;.05).</p>
<p>Table 6 :
6
Example instructions for wonder question generation, and the BASE and COGENT passage generation.</p>
<p>While we demonstrate our framework using NGSS as a representative example in this paper, the hierarchical decomposition underlying COGENT can be adapted to other national education frameworks and subjects, such as the National Curriculum in England (Department for
Education, 2014)  or Singapore's Ministry of Education curriculum standards (Ministry of Education Singapore, 2023).2  In our experiments, based on human-written passages, we set the word count to be the grade level multiplied by 100. Flesch Kincaid Grade Level is used for readability control.
https://platform.openai.com/docs/models/gpt-4o
https://docs.anthropic.com/en/docs/aboutclaude/models/all-models
https://github.com/textstat/textstat
AcknowledgmentsThis research is supported by the AI4EDU Programme in the Institute for Infocomm Research (I 2 R), Agency for Science, Technology and Research (A*STAR), Singapore.We thank the anonymous reviewers for their precious feedback to help improve and extend this piece of work.Curriculum Alignment ScoringI will show you a science reading passage for elementary school students.Rate its curriculum alignment on a 5-point scale (1 = does not align at all, 5 = fully aligned).[Curriculum Information] Grade Level: 3 Science Concept: Biological Evolution: Unity and Diversity Core Ideas: Biodiversity and Humans.Populations live in a variety of habitats and change in those habitats affects the organisms living there.Learning Outcomes: Make a claim about the merit of a solution to a problem caused when the environment changes and the types of plants and animals that live there may change.[Input Passage Content] When a forest turns into a field, big changes happen for the plants that live there.Forests are full of tall trees, bushes, and smaller plants that grow in the shade.But fields are open spaces with lots of sunlight and fewer trees.As the forest becomes a field, many of the forest plants can't survive.Trees that once stood tall are cut down or die.The plants that grew in their shade now have too much sun.Some of these plants might dry up or wilt because they're not used to so much light.... ... When habitats change, the living things in them have to change too.Some can't survive, but others find new ways to live.Nature is always changing, and plants and animals are always trying to keep up.[System Output] Alignment Score: 5Curriculum Item CategorizationClassify the science reading passage for elementary school students into one of the following types according to the curriculum definition.Give me the type label.[Curriculum Item Categories] "Type": "A", "Concept": "From Molecules to Organisms: Structures and Processes", "Core Ideas": "Structure and Function • All organisms have external parts.Different animals use their body parts in different ways to see, hear, grasp objects, protect themselves, move from place to place, and seek, find, and take in food, water, and air.Plants also have different parts", "Learning Outcomes": "Use materials to design a solution to a human problem by mimicking how plants and/or animals use their external parts to help them survive, grow, and meet their needs.","Type": "B", "Concept": "From Molecules to Organisms: Structures and Processes", "Core Ideas": "Growth and Development of Organisms • Adult plants and animals can have young.In many kinds of animals, parents and the offspring themselves engage in behaviors that help the offspring to survive.","Learning Outcomes": "Read texts and use media to determine patterns in behavior of parents and offspring that help offspring survive.",... ... "Type": "G", "Concept": "From Molecules to Organisms: Structures and Processes", "Core Ideas": "Organization for Matter and Energy Flow in Organisms • Plants acquire their material for growth chiefly from air and water.","Learning Outcomes": "Support an argument that plants get the materials they need for growth chiefly from air and water.",[Input Passage Content] Cats have special hairs called whiskers.These whiskers are not like normal fur.They are thick and stiff.Whiskers grow on a cat's face and legs.They help cats in many ways.Cats use whiskers to feel things around them.This helps them move in the dark.Whiskers can sense air movement too.This tells cats if something is nearby.When hunting, whiskers help cats know if they can fit through small spaces.Cats also use whiskers to show how they feel.If a cat is happy, its whiskers point forward.When scared, the whiskers go back.Whiskers are very important for cats.They help cats stay safe and find food.[System Output] Predicted Type: AText Readability AssessmentTopic: What do plants use roots for?Grade Level: 2 Plants have roots that grow under the ground.Roots are very important for plants.They help plants get water from the soil.The roots also hold the plant in place.This keeps the plant from falling over.Some roots can even store food for the plant.Roots come in different shapes and sizes.Some are long and thin, while others are short and thick.Plants use their roots to drink water, just like we use our mouths.The roots also help plants get nutrients from the soil.These nutrients help the plant grow big and strong.Without roots, plants would not be able to live and grow.Readability Scores: Flesch Reading Ease: 96.28, Flesch Kincaid Grade Level: 2.0, Gunning Fog Index: 3.93, Automated Readability Index: 4.1, Coleman Liau Index: 6.06Topic: What does a fish's gills do?Grade Level: 4 Have you ever wondered how fish can breathe underwater?Unlike us, fish don't have lungs to take in air.Instead, they have special organs called gills that help them get oxygen from the water around them.These gills are a great example of how animals have special body parts that help them survive in their environment.Fish gills are like tiny, feathery structures on the sides of a fish's head.They're hidden under flaps called gill covers, which protect them.When a fish opens its mouth, water flows in and passes over the gills.As this happens, the gills grab oxygen from the water, just like our lungs grab oxygen from the air we breathe.The gills are made up of many thin, flat sheets called gill filaments.These filaments are packed with tiny blood vessels.As water moves over the filaments, oxygen from the water passes into the blood vessels.At the same time, carbon dioxide, which is a waste gas, moves from the blood into the water.This swap of gases is how fish breatheünderwater.Fish gills do more than just help with breathing.They also play a part in getting rid of waste from the fish's body and keeping the right balance of salt and water.Some fish that live in salt water have special cells in their gills that can remove extra salt from their bodies.Different types of fish have gills that are slightly different, depending on where they live and how they behave.For example, fast-swimming fish like tuna have gills that can take in more oxygen quickly.This helps them keep up their speedy swimming.On the other hand, fish that live in waters with less oxygen, like some catfish, have gills that are really good at grabbing what little oxygen is there.It's amazing how fish gills work so well for life underwater.They're a perfect fit for the fish's watery home, just like our lungs are a perfect fit for life on land.This shows how animals have body parts that are just right for where they live and what they need to do to survive.Readability Scores: Flesch Reading Ease: 81.12, Flesch Kincaid Grade Level: 5.8, Gunning Fog Index: 7.44, Automated Readability Index: 7.7, Coleman Liau Index: 8.0
. Fouad Abd-El-Khalick, Saouma Boujaoude, Richard Duschl, Rachel Norman G Lederman, Avi Mamlok-Naaman, Mansoor Hofstein, David Niaz, Hsiao-Lin Treagust, Tuan, 10.1002/sce.10118Science Education. 882004Inquiry in science education: International perspectives</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Curriculum alignment: A reexamination. Theory into Practice. Anderson Lorin, 200241</p>
<p>Readability, content, and mechanical feature analysis of selected commercial science textbooks intended for third grade filipino learners. J Apler, Bansiong, 10.1080/2331186X.2019.1706395Cogent Education. 6117063952019</p>
<p>Revising social studies text from a text-processing perspective: Evidence of improved comprehensibility. Isabel L Beck, Margaret G Mckeown, Gale M Sinatra, Jane A Loxterman, 10.2307/747763Reading Research Quarterly. 2631991</p>
<p>Evaluating the readability of marketing research textbooks: an international comparison. Adele Berndt, Jane P Wayland, Journal of International Education in Business. 712014</p>
<p>Switching between everyday and scientific language. J Eric, Blown, Bryce Tom, Research in Science Education. 472017</p>
<p>Ngss and the next generation of science teachers. Rodger W Bybee, Journal of science teacher education. 2522014</p>
<p>Asli Celikyilmaz, Elizabeth Clark, Jianfeng Gao, arXiv:2006.14799Evaluation of text generation: A survey. 2020</p>
<p>Studentgenerated questions: A meaningful aspect of learning in science. Christine Chin, David E Brown, International Journal of Science Education. 2452002</p>
<p>The national curriculum for England to be taught in all local-authority-maintained schools. with English and maths coming into force for all year groups from. Taylor &amp; Francis Group2014. September 2014. September 2016. 1986Department for EducationThe Educational Forum</p>
<p>A new readability yardstick. Rudolph Flesch, Journal of Applied Psychology. 3232211948</p>
<p>Wonder as a tool to engage preservice elementary teachers in science learning and teaching. Andrew Gilbert, Christie C Byers, Science Education. 10162017</p>
<p>The Technique of Clear Writing. Robert Gunning, 1968McGraw-HillNew York2nd edition</p>
<p>Simon Hughes, Minseok Bae, Vectara hallucination leaderboard. 2023</p>
<p>Supporting early scientific thinking through curiosity. Jamie J Jirout, Frontiers in Psychology. 1117172020</p>
<p>An LLMpowered adaptive practicing system. Rayhan Md, Fuhua Kabir, Lin, LLM@AIED. 2023</p>
<p>George R Klare, Assessing readability. Reading Research Quarterly. 1974</p>
<p>Leveraging LLMs for adaptive testing and learning in Taiwan adaptive learning platform (TALP). Bor-Chen Kuo, Frederic Ty Chang, Zong-En Bai, LLM@AIED. 2023</p>
<p>The metaphorical structure of the human conceptual system. George Lakoff, Mark Johnson, Cognitive Science. 19804</p>
<p>Teachers' and students' perceptions of AI-generated concept explanations: Implications for integrating generative AI in computer science education. Soohwan Lee, Ki-Sang Song, Computers and Education: Artificial Intelligence. 71002832024</p>
<p>Few-shot is enough: exploring ChatGPT prompt engineering method for automatic question generation in english education. Unggi Lee, Haewon Jung, Younghoon Jeon, Younghoon Sohn, Wonhee Hwang, Jewoong Moon, Hyeoncheol Kim, 10.1007/s10639-023-12249-8Education and Information Technologies. 2992024</p>
<p>DnA-eval: Enhancing large language model evaluation through decomposition and aggregation. Minzhi Li, Zhengyuan Liu, Shumin Deng, Shafiq Joty, Nancy Chen, Min-Yen Kan, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational LinguisticsAbu DhabiUAE. Association for Computational Linguistics2025</p>
<p>Modification of the coleman readability formulas. Ta Lin Liau, Carolyn B Bassin, Clessen J Martin, Edmund B Coleman, Journal of Reading Behavior. 841976</p>
<p>Promoting curiosity? possibilities and pitfalls in science education. Markus Lindholm, Science &amp; Education. 272018</p>
<p>Optimizing code-switching in conversational tutoring systems: A pedagogical framework and evaluation. Zhengyuan Liu, Stella Xin Yin, Nancy Chen, 10.18653/v1/2024.sigdial-1.43Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 25th Annual Meeting of the Special Interest Group on Discourse and DialogueKyoto, JapanAssociation for Computational Linguistics2024a</p>
<p>Scaffolding language learning via multi-modal tutoring systems with pedagogical instructions. Zhengyuan Liu, Stella Xin Yin, Carolyn Lee, Nancy F Chen, 2024 IEEE Conference on Artificial Intelligence (CAI). IEEE2024b</p>
<p>Personality-aware student simulation for conversational intelligent tutoring systems. Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F Chen, 10.18653/v1/2024.emnlp-main.37Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024c</p>
<p>Last updated: 02 Mar 2023. The primary school curriculum is designed to give children of school-going age a strong foundation in learning. Ministry of Education Singapore. 2023. Primary school curriculum and subjects. </p>
<p>Inquiry and the national science standards. 2000National Academy PressWashington, DCNational Research Council</p>
<p>Engaging texts: Effects of concreteness on comprehensibility, interest, and recall in four text types. Mark Sadoski, Ernest T Goetz, Maximo Rodriguez, Journal of Educational Psychology. 921852000</p>
<p>Branchsolve-merge improves large language model evaluation and generation. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>Automated readability index. A Edgar, Smith, Senter, Aerospace Medical Research Laboratories. 662201967Air Force Systems CommandTechnical ReportAerospace Medical Division</p>
<p>Curriculum alignment research suggests that alignment can improve student achievement. The Clearing House: A. David Squires, Journal of Educational Strategies. 8542012Issues and Ideas</p>
<p>Next generation science standards: For states, by states. 2013National Academies PressNGSS Lead States</p>
<p>. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.00118arXiv preprintet al. 2024. Gemma 2: Improving open language models at a practical size</p>
<p>Large language models for education: A survey and outlook. Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S Yu, Qingsong Wen, arXiv:2403.18105Assessing the lexile framework: Results of a panel meeting. 2024. 2001arXiv preprintSheida White and John Clement</p>
<p>Improving curriculum alignment and achieving learning goals by making the curriculum visible. Leoniek Wijngaards-De Meij, Sigrid Merx, International Journal for Academic Development. 2332018</p>
<p>Evaluating reading comprehension exercises generated by LLMs: A showcase of ChatGPT in education applications. Changrong Xiao, Sean Xin Xu, Kunpeng Zhang, Yufang Wang, Lei Xia, Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)2023</p>
<p>Practical and ethical challenges of large language models in education: A systematic scoping review. Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, Dragan Gašević, 10.1111/bjet.13370British Journal of Educational Technology. 5512024</p>
<p>Mostafa Zamanian, Pooneh Heydari, Readability of texts: State of the art. Theory &amp; Practice in Language Studies (TPLS). 20122</p>
<p>Generating and evaluating tests for k-12 students with language model simulations: A case study on sentence reading efficiency. Eric Zelikman, Wanjing Ma, Jasmine Tran, Diyi Yang, Jason Yeatman, Nick Haber, 10.18653/v1/2023.emnlp-main.135Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>            </div>
        </div>

    </div>
</body>
</html>
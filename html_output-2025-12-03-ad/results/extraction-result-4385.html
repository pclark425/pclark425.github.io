<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4385 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4385</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4385</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-272550863</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.05890v1.pdf" target="_blank">Automating the Practice of Science -- Opportunities, Challenges, and Implications</a></p>
                <p><strong>Paper Abstract:</strong> Automation transformed various aspects of our human civilization, revolutionizing industries and streamlining processes. In the domain of scientific inquiry, automated approaches emerged as powerful tools, holding promise for accelerating discovery, enhancing reproducibility, and overcoming the traditional impediments to scientific progress. This article evaluates the scope of automation within scientific practice and assesses recent approaches. Furthermore, it discusses different perspectives to the following questions: Where do the greatest opportunities lie for automation in scientific practice?; What are the current bottlenecks of automating scientific practice?; and What are significant ethical and practical consequences of automating scientific practice? By discussing the motivations behind automated science, analyzing the hurdles encountered, and examining its implications, this article invites researchers, policymakers, and stakeholders to navigate the rapidly evolving frontier of automated scientific practice.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4385.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4385.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI literature-review research assistant that uses large language models trained on paper abstracts to support extraction of relevant information and aid literature synthesis for researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Elicit: AI literature review research assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as an LLM-based literature review assistant that is trained on paper abstracts to help researchers extract relevant information from the scientific literature; this paper does not provide architectural details, retrieval pipeline specifics, or model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based extraction from abstracts (prompted QA / summarization of abstracts as described); exact technical extraction pipeline not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-driven literature synthesis / aggregation (summarization across many abstracts); hierarchical or multi-stage synthesis details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>thousands to millions (paper asserts LLMs could synthesize on the order of thousands or millions of articles)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general scientific literature / cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>extracted information for literature reviews; synthesized summaries to support literature review tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs (as implemented in Elicit) can substantially extend the volume of literature that can be synthesized compared to human researchers and can process multilingual literature; Elicit is cited as an example of an LLM trained on abstracts to support literature extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality control risks (including incorporation of fraudulent or irreproducible papers), potential to undermine researcher conceptual development, and general LLM risks like hallucination and omission; the paper flags these concerns but does not provide empirical error rates for Elicit.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper argues LLM-based literature synthesis can scale well beyond human capacity (thousands-to-millions of papers), but no empirical scaling curves or model-size dependence are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4385.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4385.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BrainGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BrainGPT (LLM fine-tuned to the neuroscience literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-tuned LLM fine-tuned on neuroscience publications that reportedly outperformed human experts in predicting results of neuroscience experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models surpass human experts in predicting neuroscience results.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BrainGPT (domain-fine-tuned neuroscience LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Characterized in this paper as an LLM fine-tuned on neuroscience literature to predict experimental outcomes; the draft does not report the model architecture, training data size, or other implementation specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Domain fine-tuning on neuroscience literature and prompt-based inference to extract/predict experimental outcomes (exact extraction pipeline not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Literature-informed predictive inference (using synthesized knowledge from the fine-tuned model to predict experiment results); explicit multi-paper aggregation method not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>neuroscience</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>predictions of experimental results / literature-informed inference about experiments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Compared against human experts (implied predictive accuracy / expert-level prediction), evaluation specifics not included in this draft.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in this paper to outperform human experts in predicting neuroscience experiment outcomes; no quantitative performance numbers are provided in this draft.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert predictions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported to outperform human experts; no numeric comparisons provided in this document.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning LLMs on domain-specific literature can produce models that match or exceed human experts at certain prediction tasks in neuroscience, suggesting domain-adapted LLMs can synthesize literature to make experimentally-relevant inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Implementation/replicability details are absent here; general concerns about input quality, hallucination, and overconfidence remain relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4385.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4385.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemical coscientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based coscientist for chemical research (Boiko et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based assistant applied to chemical research that aided planning of chemical syntheses by synthesizing extensive internet literature and navigating hardware documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based chemical coscientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as an LLM-driven coscientist that improves chemical synthesis planning and helps navigate extensive hardware documentation; the draft summarizes its role but does not present its internal architecture, model family, or exact pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Web/literature extraction mediated by LLMs (details on indexing, retrieval, or grounding are not provided in this draft).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Synthesis of literature and procedural documentation into actionable synthesis plans and experimental guidance; no formal claim-aggregation or knowledge-graph construction specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>chemistry / synthetic chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>chemical synthesis plans and laboratory/hardware guidance</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported improvement in planning chemical syntheses according to the cited work; this draft does not include quantitative results or metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM assistance can materially improve synthesis planning and practical laboratory navigation when leveraged with extensive web/literature sources, demonstrating a practical application of LLMs in experimental domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Risk of rediscovering known hypotheses, safety concerns for chemistry use-cases, and absence of detailed grounding/verification mechanisms in the summary provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4385.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4385.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM ML-research agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based agent for automating empirical machine learning research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A described LLM agent that automates empirical machine-learning research end-to-end (idea generation, experimental design, execution, analysis, writing, and peer review) with an estimated computational cost of about USD 15 per article (as reported in the draft).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based empirical ML research agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Characterized in the draft as an LLM-driven agent that automates multiple stages of empirical ML research including ideation, experiment design, execution, data analysis, manuscript writing and peer review; the draft does not provide architecture, model name/size, or pipeline details.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this draft; likely uses LLM question-answering, code-generation, and experiment orchestration but no concrete extraction method is described here.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>End-to-end synthesis from experiment outputs to manuscript text and reviews via LLM-generated content; no formal claim-aggregation or evidence-tracking method is provided in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>machine learning research (empirical ML)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>research ideas, experimental protocols and analyses, full research manuscripts, and peer-review drafts</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Computational cost per article (~USD 15) is reported; no empirical accuracy/fidelity metrics presented in this draft.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>The draft reports an estimated computational cost of USD 15 to produce one article but does not provide quantitative evaluations of scientific quality or reproducibility of the generated research.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM agents can orchestrate many aspects of empirical ML research and dramatically reduce monetary cost of producing a manuscript; the draft raises concerns about the implications for peer review and oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Peer-review automation faces efficacy, reliability, and confidentiality concerns; ethical and quality-control issues are highlighted, and explicit validation metrics are absent in this description.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4385.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4385.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FoundSci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Foundation Models for Scientific Discovery (FoundSci, DARPA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DARPA program solicitation advocating the development of foundation models for scientific discovery, intending large-scale models to assist in cross-domain scientific synthesis, inference, and hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Foundation Models for Scientific Discovery (FoundSci)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FoundSci foundation-model approach</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Program-level approach recommending the development and use of foundation-scale models (large pre-trained models) to facilitate scientific discovery across domains; the draft references the solicitation but does not detail implemented systems or architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Large-scale ingestion of scientific corpora and foundation-model pretraining (broad description in the solicitation context; specifics are not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Foundation-model level integration of knowledge across large corpora to support hypothesis generation and theory formulation (high-level aspiration rather than an implemented synthesis pipeline in this draft).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>large-scale / corpus-level (implied millions of documents across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general scientific discovery across multiple disciplines</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>hypothesis proposals, theory-generation assistance, cross-domain synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Advocates building and applying foundation models to accelerate scientific discovery across fields, emphasizing scale and cross-domain transfer as benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Raises general concerns consistent with the draft: data quality, ethical risks, hallucination, and the need for robust evaluation and alignment mechanisms for such foundation-model approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Explicitly emphasizes large-scale pretraining and cross-domain scaling as central to the approach, though no empirical scaling curves are provided in this draft.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4385.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4385.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zheng review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for scientific synthesis, inference and explanation (Zheng)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A review (preprint) discussing how large language models can be applied to scientific synthesis, inference, and explanation across research literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for scientific synthesis, inference and explanation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM literature-synthesis approaches (review)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Survey-level description of LLM methodologies and potential for scientific synthesis, inference, and explanation; the draft references this review to contextualize LLM capabilities but does not reproduce its internal taxonomy or proposed systems.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Discusses a range of techniques including retrieval-augmented generation, QA, and summarization (survey-level); specifics are not enumerated in this draft.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Covers synthesis approaches like summarization, explanation generation, and inference via LLMs (review-level overview rather than an implemented pipeline in this draft).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general scientific literature / methodological review</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>reviewed synthesis techniques, proposals for inference and explanation using LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aggregates perspectives on how LLMs can perform synthesis, inference, and explanation tasks; emphasizes promise and the need for careful evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Highlights common LLM limitations relevant for scientific synthesis: hallucinations, citation/fact errors, and the importance of input data quality and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Discusses scaling potential qualitatively (larger models and corpora enabling broader synthesis) but no empirical scaling results are provided in this draft.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Elicit: AI literature review research assistant. <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Large language models for scientific synthesis, inference and explanation <em>(Rating: 2)</em></li>
                <li>Large language models surpass human experts in predicting neuroscience results <em>(Rating: 2)</em></li>
                <li>Foundation Models for Scientific Discovery (FoundSci) <em>(Rating: 1)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4385",
    "paper_id": "paper-272550863",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Elicit",
            "name_full": "Elicit",
            "brief_description": "An AI literature-review research assistant that uses large language models trained on paper abstracts to support extraction of relevant information and aid literature synthesis for researchers.",
            "citation_title": "Elicit: AI literature review research assistant.",
            "mention_or_use": "mention",
            "system_name": "Elicit",
            "system_description": "Described as an LLM-based literature review assistant that is trained on paper abstracts to help researchers extract relevant information from the scientific literature; this paper does not provide architectural details, retrieval pipeline specifics, or model sizes.",
            "llm_model_used": null,
            "extraction_technique": "LLM-based extraction from abstracts (prompted QA / summarization of abstracts as described); exact technical extraction pipeline not specified in this paper.",
            "synthesis_technique": "LLM-driven literature synthesis / aggregation (summarization across many abstracts); hierarchical or multi-stage synthesis details not provided here.",
            "number_of_papers": "thousands to millions (paper asserts LLMs could synthesize on the order of thousands or millions of articles)",
            "domain_or_topic": "general scientific literature / cross-domain",
            "output_type": "extracted information for literature reviews; synthesized summaries to support literature review tasks",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "LLMs (as implemented in Elicit) can substantially extend the volume of literature that can be synthesized compared to human researchers and can process multilingual literature; Elicit is cited as an example of an LLM trained on abstracts to support literature extraction.",
            "limitations_challenges": "Quality control risks (including incorporation of fraudulent or irreproducible papers), potential to undermine researcher conceptual development, and general LLM risks like hallucination and omission; the paper flags these concerns but does not provide empirical error rates for Elicit.",
            "scaling_behavior": "Paper argues LLM-based literature synthesis can scale well beyond human capacity (thousands-to-millions of papers), but no empirical scaling curves or model-size dependence are provided here.",
            "uuid": "e4385.0",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "BrainGPT",
            "name_full": "BrainGPT (LLM fine-tuned to the neuroscience literature)",
            "brief_description": "A domain-tuned LLM fine-tuned on neuroscience publications that reportedly outperformed human experts in predicting results of neuroscience experiments.",
            "citation_title": "Large language models surpass human experts in predicting neuroscience results.",
            "mention_or_use": "mention",
            "system_name": "BrainGPT (domain-fine-tuned neuroscience LLM)",
            "system_description": "Characterized in this paper as an LLM fine-tuned on neuroscience literature to predict experimental outcomes; the draft does not report the model architecture, training data size, or other implementation specifics.",
            "llm_model_used": null,
            "extraction_technique": "Domain fine-tuning on neuroscience literature and prompt-based inference to extract/predict experimental outcomes (exact extraction pipeline not specified here).",
            "synthesis_technique": "Literature-informed predictive inference (using synthesized knowledge from the fine-tuned model to predict experiment results); explicit multi-paper aggregation method not detailed.",
            "number_of_papers": null,
            "domain_or_topic": "neuroscience",
            "output_type": "predictions of experimental results / literature-informed inference about experiments",
            "evaluation_metrics": "Compared against human experts (implied predictive accuracy / expert-level prediction), evaluation specifics not included in this draft.",
            "performance_results": "Reported in this paper to outperform human experts in predicting neuroscience experiment outcomes; no quantitative performance numbers are provided in this draft.",
            "comparison_baseline": "Human expert predictions",
            "performance_vs_baseline": "Reported to outperform human experts; no numeric comparisons provided in this document.",
            "key_findings": "Fine-tuning LLMs on domain-specific literature can produce models that match or exceed human experts at certain prediction tasks in neuroscience, suggesting domain-adapted LLMs can synthesize literature to make experimentally-relevant inferences.",
            "limitations_challenges": "Implementation/replicability details are absent here; general concerns about input quality, hallucination, and overconfidence remain relevant.",
            "scaling_behavior": null,
            "uuid": "e4385.1",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Chemical coscientist",
            "name_full": "LLM-based coscientist for chemical research (Boiko et al., 2023)",
            "brief_description": "An LLM-based assistant applied to chemical research that aided planning of chemical syntheses by synthesizing extensive internet literature and navigating hardware documentation.",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "system_name": "LLM-based chemical coscientist",
            "system_description": "Described as an LLM-driven coscientist that improves chemical synthesis planning and helps navigate extensive hardware documentation; the draft summarizes its role but does not present its internal architecture, model family, or exact pipelines.",
            "llm_model_used": null,
            "extraction_technique": "Web/literature extraction mediated by LLMs (details on indexing, retrieval, or grounding are not provided in this draft).",
            "synthesis_technique": "Synthesis of literature and procedural documentation into actionable synthesis plans and experimental guidance; no formal claim-aggregation or knowledge-graph construction specified here.",
            "number_of_papers": null,
            "domain_or_topic": "chemistry / synthetic chemistry",
            "output_type": "chemical synthesis plans and laboratory/hardware guidance",
            "evaluation_metrics": null,
            "performance_results": "Reported improvement in planning chemical syntheses according to the cited work; this draft does not include quantitative results or metrics.",
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "LLM assistance can materially improve synthesis planning and practical laboratory navigation when leveraged with extensive web/literature sources, demonstrating a practical application of LLMs in experimental domains.",
            "limitations_challenges": "Risk of rediscovering known hypotheses, safety concerns for chemistry use-cases, and absence of detailed grounding/verification mechanisms in the summary provided.",
            "scaling_behavior": null,
            "uuid": "e4385.2",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LLM ML-research agent",
            "name_full": "LLM-based agent for automating empirical machine learning research",
            "brief_description": "A described LLM agent that automates empirical machine-learning research end-to-end (idea generation, experimental design, execution, analysis, writing, and peer review) with an estimated computational cost of about USD 15 per article (as reported in the draft).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-based empirical ML research agent",
            "system_description": "Characterized in the draft as an LLM-driven agent that automates multiple stages of empirical ML research including ideation, experiment design, execution, data analysis, manuscript writing and peer review; the draft does not provide architecture, model name/size, or pipeline details.",
            "llm_model_used": null,
            "extraction_technique": "Not specified in this draft; likely uses LLM question-answering, code-generation, and experiment orchestration but no concrete extraction method is described here.",
            "synthesis_technique": "End-to-end synthesis from experiment outputs to manuscript text and reviews via LLM-generated content; no formal claim-aggregation or evidence-tracking method is provided in the summary.",
            "number_of_papers": null,
            "domain_or_topic": "machine learning research (empirical ML)",
            "output_type": "research ideas, experimental protocols and analyses, full research manuscripts, and peer-review drafts",
            "evaluation_metrics": "Computational cost per article (~USD 15) is reported; no empirical accuracy/fidelity metrics presented in this draft.",
            "performance_results": "The draft reports an estimated computational cost of USD 15 to produce one article but does not provide quantitative evaluations of scientific quality or reproducibility of the generated research.",
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "LLM agents can orchestrate many aspects of empirical ML research and dramatically reduce monetary cost of producing a manuscript; the draft raises concerns about the implications for peer review and oversight.",
            "limitations_challenges": "Peer-review automation faces efficacy, reliability, and confidentiality concerns; ethical and quality-control issues are highlighted, and explicit validation metrics are absent in this description.",
            "scaling_behavior": null,
            "uuid": "e4385.3",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "FoundSci",
            "name_full": "Foundation Models for Scientific Discovery (FoundSci, DARPA)",
            "brief_description": "A DARPA program solicitation advocating the development of foundation models for scientific discovery, intending large-scale models to assist in cross-domain scientific synthesis, inference, and hypothesis generation.",
            "citation_title": "Foundation Models for Scientific Discovery (FoundSci)",
            "mention_or_use": "mention",
            "system_name": "FoundSci foundation-model approach",
            "system_description": "Program-level approach recommending the development and use of foundation-scale models (large pre-trained models) to facilitate scientific discovery across domains; the draft references the solicitation but does not detail implemented systems or architectures.",
            "llm_model_used": null,
            "extraction_technique": "Large-scale ingestion of scientific corpora and foundation-model pretraining (broad description in the solicitation context; specifics are not provided here).",
            "synthesis_technique": "Foundation-model level integration of knowledge across large corpora to support hypothesis generation and theory formulation (high-level aspiration rather than an implemented synthesis pipeline in this draft).",
            "number_of_papers": "large-scale / corpus-level (implied millions of documents across domains)",
            "domain_or_topic": "general scientific discovery across multiple disciplines",
            "output_type": "hypothesis proposals, theory-generation assistance, cross-domain synthesis",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Advocates building and applying foundation models to accelerate scientific discovery across fields, emphasizing scale and cross-domain transfer as benefits.",
            "limitations_challenges": "Raises general concerns consistent with the draft: data quality, ethical risks, hallucination, and the need for robust evaluation and alignment mechanisms for such foundation-model approaches.",
            "scaling_behavior": "Explicitly emphasizes large-scale pretraining and cross-domain scaling as central to the approach, though no empirical scaling curves are provided in this draft.",
            "uuid": "e4385.4",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Zheng review",
            "name_full": "Large language models for scientific synthesis, inference and explanation (Zheng)",
            "brief_description": "A review (preprint) discussing how large language models can be applied to scientific synthesis, inference, and explanation across research literature.",
            "citation_title": "Large language models for scientific synthesis, inference and explanation",
            "mention_or_use": "mention",
            "system_name": "LLM literature-synthesis approaches (review)",
            "system_description": "Survey-level description of LLM methodologies and potential for scientific synthesis, inference, and explanation; the draft references this review to contextualize LLM capabilities but does not reproduce its internal taxonomy or proposed systems.",
            "llm_model_used": null,
            "extraction_technique": "Discusses a range of techniques including retrieval-augmented generation, QA, and summarization (survey-level); specifics are not enumerated in this draft.",
            "synthesis_technique": "Covers synthesis approaches like summarization, explanation generation, and inference via LLMs (review-level overview rather than an implemented pipeline in this draft).",
            "number_of_papers": null,
            "domain_or_topic": "general scientific literature / methodological review",
            "output_type": "reviewed synthesis techniques, proposals for inference and explanation using LLMs",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Aggregates perspectives on how LLMs can perform synthesis, inference, and explanation tasks; emphasizes promise and the need for careful evaluation.",
            "limitations_challenges": "Highlights common LLM limitations relevant for scientific synthesis: hallucinations, citation/fact errors, and the importance of input data quality and evaluation.",
            "scaling_behavior": "Discusses scaling potential qualitatively (larger models and corpora enabling broader synthesis) but no empirical scaling results are provided in this draft.",
            "uuid": "e4385.5",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Elicit: AI literature review research assistant.",
            "rating": 2,
            "sanitized_title": "elicit_ai_literature_review_research_assistant"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Large language models for scientific synthesis, inference and explanation",
            "rating": 2,
            "sanitized_title": "large_language_models_for_scientific_synthesis_inference_and_explanation"
        },
        {
            "paper_title": "Large language models surpass human experts in predicting neuroscience results",
            "rating": 2,
            "sanitized_title": "large_language_models_surpass_human_experts_in_predicting_neuroscience_results"
        },
        {
            "paper_title": "Foundation Models for Scientific Discovery (FoundSci)",
            "rating": 1,
            "sanitized_title": "foundation_models_for_scientific_discovery_foundsci"
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 1,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        }
    ],
    "cost": 0.01716225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>D R A F T Automating the Practice of Science -Opportunities, Challenges, and Implications
September 27, 2024</p>
<p>Sebastian Musslick sebastian.musslick@uos.de 
Institute of Cognitive Science
Department of Cognitive, Linguistic, &amp; Psychological Sciences
Osnabr ck University
49090 Osnabr ckGermany</p>
<p>Brown University
02912ProvidenceRI, ORCIDUSA</p>
<p>Laura K Bartlett 
Centre for Philosophy of Natural and Social Science
London School of Economics
Lakatos Building, Houghton StreetWC2A 2AELondonUK</p>
<p>ORCID</p>
<p>Suyog H Chandramouli 
Department of Information and Communications Engineering
Department of Computing Science
Aalto University
P.O. Box 110001B) FI-00076Otakaari, AALTOFinland</p>
<p>University of Alberta
8900 114 St NWT6G 2S4EdmontonABCanada</p>
<p>ORCID; d Cognitive Science Program
Indiana University
1101 E 10th St47405BloomingtonINUSA</p>
<p>ORCID</p>
<p>Marina Dubova 
Fernand Gobet 
Centre for Philosophy of Natural and Social Science
London School of Economics
Lakatos Building, Houghton StreetWC2A 2AELondonUK</p>
<p>ORCID</p>
<p>School of Psychology
University of Roehampton
SW15 4JDLondonUK</p>
<p>ORCID</p>
<p>Thomas L Griffiths 
Departments of Psychology and Computer Science
Princeton University
PrincetonNJUSA</p>
<p>ORCID</p>
<p>Jessica Hullman 
Department of Computer Science
Northwestern University
ILUSA</p>
<p>ORCID</p>
<p>Ross D King 
Department of Chemical Engineering and Biotechnology
Department of Computer Science and Engineering
University of Cambridge
CB3 0ASCambridgeUK</p>
<p>Chalmers University of Technology
412 96GothenburgSweden; ORCID</p>
<p>J Nathan Kutz 
Department of Applied Mathematics and Electrical and Computer Engineering
University of Washington
98195SeattleUSA</p>
<p>ORCID</p>
<p>Christopher G Lucas 
School of Informatics
University of Edinburgh
10 Crichton StEH8 9ABUnited Kingdom</p>
<p>ORCID</p>
<p>Suhas Mahesh 
Department of Materials Science and Engineering
University of Toronto
Canada</p>
<p>ORCID</p>
<p>Franco Pestilli 
Department of Psychology and Department of Neuroscience
The University of Texas
AustinTXUSA</p>
<p>ORCID</p>
<p>Sabina J Sloman 
Department of Computer Science
University of Manchester
M13 9PLUK ORCID</p>
<p>William R Holmes </p>
<p>In-stitute of Cognitive Science
Wachsbleiche 27, 49090 Osnabr ckGermany</p>
<p>D R A F T Automating the Practice of Science -Opportunities, Challenges, and Implications
September 27, 202498411D25EBD3D0346E02ECC8F32238E910.1073/pnas.XXXXXXXXXXarXiv:2409.05890v1[cs.CY]
Automation transformed various aspects of our human civilization, revolutionizing industries and streamlining processes.In the domain of scientific inquiry, automated approaches emerged as powerful tools, holding promise for accelerating discovery, enhancing reproducibility, and overcoming the traditional impediments to scientific progress.This article evaluates the scope of automation within scientific practice and assesses recent approaches.Furthermore, it discusses different perspectives to the following questions: Where do the greatest opportunities lie for automation in scientific practice?;What are the current bottlenecks of automating scientific practice?; and What are significant ethical and practical consequences of automating scientific practice?By discussing the motivations behind automated science, analyzing the hurdles encountered, and examining its implications, this article invites researchers, policymakers, and stakeholders to navigate the rapidly evolving frontier of automated scientific practice.Automation| Computational Scientific Discovery | Metascience | AI for Science "Though the world does not change with a change of paradigm, the scientist afterward works in a different world."-Thomas S. Kuhn, The Structure of Scientific Revolutions</p>
<p>Automation is transforming every domain of scientific inquiry, from the study of functional genomics in biology (1,2) to the derivation of conjectures in mathematics (3,4).Recent advances in automation are accelerating hypothesis generation in chemistry (5)(6)(7)(8), material discovery in materials science (9,10), and theory development in psychology (11).These breakthroughs are not only garnering attention but also an uptick in funding and prizes dedicated to the automation of scientific practice (12)(13)(14).Furthermore, concurrent advancements in artificial intelligence, software, and computing hardware are setting the stage for even more extensive automation within the scientific process (15)(16)(17).</p>
<p>The impact of automation in industry serves as a parallel to its potential in science.In the early 20th century, industrial automation began with mechanized assembly lines, revolutionizing manufacturing efficiency and output.The introduction of robotics and computer-aided manufacturing marked another leap, enabling precision and consistency previously unattainable by human labor.Today, industry-wide automation facilitates not just cost-efficient mass production, but also customized, adaptable, and intelligent manufacturing processes.This evolution demonstrates the capacity of automation to radically redefine operational paradigms.</p>
<p>Drawing parallels to scientific practice, one can anticipate a similar trajectory of profound change, where automation could accelerate discovery, reshape research methodologies, and redefine the very nature of scientific inquiry.At the same time, automation in industry had significant impacts on workers and the kind of products that dominate the marketplace.It is thus important to consider parallel impacts in the scientific setting which may have negative consequences for science and society.</p>
<p>In this perspective, we evaluate what automation should and can achieve for scientific practice.In doing so, we outline the current state of science automation, drawing on recent examples from different domains of science.Furthermore, we examine technological advancements that open new avenues for automation in science, and discuss current bottlenecks.Finally, we highlight a selection of practical and ethical considerations, and discuss how automation may lead scientists to work in a different world, one where traditional methodologies are redefined and new metaparadigms for science emerge.</p>
<p>What are the bounds of automating scientific practice?</p>
<p>Scientific practice can be defined as the set of methods and processes used by scientists to acquire knowledge about the natural world.Automation, in its broadest sense, refers to the use of technology to perform tasks with minimal human intervention.In the context of scientific practice, automation specifically denotes the use of technological tools and systems to carry out scientific tasks or processes traditionally performed by human scientists.</p>
<p>The bounds of automation within scientific practice hinge on at least two questions: First, is there a desire and justification for automating a given scientific practice?This question touches upon goal-related bounds-the alignment of automation with the overarching goals of science.Second, what factors characterizing D R A F T a scientific practice influence the feasibility of automating that practice?This aspect focuses on the technological bounds, assessing the practicality and potential constraints of applying automation in science.</p>
<p>Goal-related bounds: what automation should (not) achieve.</p>
<p>Science is driven by normative and epistemic goals.Here, we discuss arguments for and against automation serving these goals.</p>
<p>The normative goals of science involve ethical, moral, and societal values guiding both basic and applied science.One such goal may be to enable cheap and fast discoveries that advance human health.Along these lines, automation can serve to yield faster scientific discoveries with fewer resources.This is particularly desirable in the applied sciences, e.g., for identifying novel drugs or treatments.Thus, automation can aid scientific practice if societal needs are clear and research questions are well defined.However, the process of identifying a research question itself requires considering societal needs or the interests of the scientific community.As noted in the Opportunities section below, generative artificial intelligence (AI) can integrate large bodies of literature to identify societally and scientifcally important gaps in our knowledge that are worth filling.However, since the relevant normative considerations inherently depend on evolving human contexts, it can be argued that humans ought to always be involved in and monitor the degree to which scientific practices achieve these objectives (18).Consequently, full automation in these areas might not only be impractical but also undesirable, underscoring the indispensable role of human scientists in addressing the normative dimensions of science.</p>
<p>The epistemic goal of science is to understand the natural world through description, prediction, explanation, and control.As discussed in the sections that follow, advances in machine learning can aid in automating the description or explanation of natural phenomena.Such automation can help reduce human errors and biases, leading to more accurate predictions and better control of natural phenomena.Even more so, automation may help bypass or augment the cognitive capacities of human researchers (19), enabling degrees of prediction and control unachievable for human cognition alone.For example, machine learning models can generate millions of proposals for novel materials that lie beyond human intuition (9).Yet, the increase in precision achieved through automation presents an epistemic dilemma, as automation can limit human understanding.In the basic sciences, advancement of human understanding may be more desirable than merely improving predictability through automation.The complexity of a machine learning model, for example, might enhance its ability to accurately predict new stable materials, but concurrently obscure the process by which these predictions are made for human scientists.This scenario illustrates a potential conflict between the scientific objectives of enhancing prediction, on the one hand, and enabling human understanding, on the other (see Practical Implications).This suggests keeping human scientists involved in the scientific process rather than minimizing their involvement.Meanwhile, in applied sciences and engineering, the focus might shift towards maximizing prediction and control, providing a stronger case for automation of scientific practice.</p>
<p>Technological bounds: what automation can (not) achieve.</p>
<p>The technological bounds of automation hinge on the difficulty of automating scientific tasks.Here, we discuss four factors characterizing this difficulty (Figure 1).opportunities and barriers to automation, thereby guiding the identification of areas within scientific practice where automation can be most effectively implemented or where it may face challenges.</p>
<p>The first factor concerns the availability and quality of inputs that a scientific task requires.Some tasks, such as identifying a research question, rely on diverse and sometimes subjective inputs, including peer opinions, news articles, or funding announcements.Such inputs may not be trustworthy, widely accessible or structured for machine processing, posing a challenge to automation.</p>
<p>Another limiting factor for automation is the computational complexity of algorithms available to perform a scientific task.For example, identifying an appropriate experiment for testing a research question may require taking into account numerous decision variables (e.g., internal validity, resources needed, novelty) and searching an exponentially increasing space of possible experimental paradigms, which can be computationally intractable.</p>
<p>A related, yet often overlooked, factor influencing the automation of scientific tasks is the complexity of required hardware engineering.As stated in Moravec's paradox, sensorimotor tasks, like executing invasive brain recordings or social experiments, require advanced solutions in robotics to facilitate automation, which can pose more significant challenges to automation compared to cognitive tasks (20).</p>
<p>Finally, some tasks are difficult to automate because of the subjectivity of the task goal.Some scientific goals cannot be easily turned into a well-defined objective, which is required to communicate it to a machine.For instance, choosing between scientific models can be a matter of personal preference (21).</p>
<p>While the four factors collectively dictate the automatability of scientific tasks, they can be considered interdependent.For example, the automated discovery of scientific equations long relied on search methods with high computational complexity, such as evolutionary computation or brute force search, to identify a set of equations that best describes a given data set (22,23).However, the ability to collect large datasets cheaply, paired with improvements in computing hardware, enables the application of "data-hungry" but computationally tractable machine learning algorithms for equation discovery (24)(25)(26)(27).This approach reduces computational complexity, illustrating how enhancements in one factor can compensate for limitations in another.</p>
<p>Automation in current scientific practice</p>
<p>Existing approaches to automating science target tasks with readily available inputs, computational complexity and hardware demands that align well with current technological capabilities, and clear task goals.Accordingly, efforts at automatization in science have mostly been confined to tasks characterized by clearly specified objectives and well-defined subtasks, which include instances of quantitative hypothesis generation, experimental design, data collection, and quantitative analysis and inference.While covering all advances D R A F T is out of the scope of this article, we highlight a subset of these approaches, focusing on cases that facilitated novel discoveries.</p>
<p>Hypothesis generation.</p>
<p>Hypothesis generation is the development of testable statements that are based on observations, existing knowledge, or theory.Advances in automated hypothesis generation were primarily driven by two factors: improvements in computer algorithms, and the availability of large datasets.</p>
<p>Initial automated hypothesis formation approaches relied on symbolic reasoning systems.For example, in organic chemistry, logical deduction based on existing knowledge was employed to formulate hypotheses about the chemical constituents of body fluids (28).Furthermore, quantum simulations, facilitated through cloud computing, became the backbone of hypothesis generation for materials properties (29,30).The development of efficient search algorithms further expanded the scope of automated hypothesis formation to areas with large hypothesis spaces (3).For instance, hypothesis generation in mathematics leveraged efficient machine learning algorithms to identify novel conjectures about fundamental constants (3).Finally, deep learning enabled more breakthroughs in chemistry.A landmark achievement in this area is AlphaFold, which predicts 3D protein structures from amino acid sequences, facilitating the development of drugs (6).</p>
<p>The availability of large data sets led to further advances in automated hypothesis formation.One example is the field of biomedicine, where large gene databases led to a surge in hypothesis generation with computational methods, e.g., using data mining and network analysis to propose genes that may be linked to diseases (31,32).Similarly, existing materials databases provided sufficient information for machine learning methods to generate over 2.2 million proposals for novel materials that, so far, escaped human intuition (9).</p>
<p>Experimental design.</p>
<p>The problem of automated experimental design is to systematically identify the most informative experiment to address a particular hypothesis or scientific question.The informativeness of an experiment can be evaluated in various ways.Some automated experimental design methods are geared towards identifying the experimental conditions that minimize the influence of nuisance variables--experimental variables that are not of interest but can pollute the informativeness of intended experimental manipulations (33,34).Other methods aim to find experimental conditions that are well suited to identify a scientific model of interest (35)(36)(37).This problem of experimental design is closely related to the problem of active learning in machine learning research (2,(38)(39)(40), which seeks to identify data points that can best inform a machine learning model when included as training data.A prominent active learning method used for scientific practice is Bayesian optimal experimental design, which has been successfully applied in various fields, including psychology (36,37,41,42), neuroscience (43), physics (44,45), biology (46,47), chemistry (48,49), materials science (50)(51)(52), and engineering (53).For example, in the domain of psychology, Bayesian optimal experimental design led to the discovery of novel models of how humans discount the future relative to the present (54).</p>
<p>While automated experimental design methods can facilitate efficient data collection and strong inferences, their efficacy can be compromised if the underlying assumptions are violated or if the scientific model is incorrectly specified (55)(56)(57).This limitation led to unexpected findings in simulation studies, where random sampling of experimental conditions outperformed automated theory-driven approaches to experimental design (38,58), and where uniform sampling outperformed adaptive approaches in learning continuous relationships (59).</p>
<p>Another limitation of current approaches to automated experimental design pertains to their scope, as they focus on navigating a pre-defined space of experimental manipulations.Exploring novel research directions, however, often involves identifying completely new experimental manipulations (60).</p>
<p>Data collection.Data collection, often a time-consuming and costly aspect of empirical research, is a significant bottleneck in scientific discovery.Accordingly, automated tools for data collection emerged as some of the most impactful innovations in accelerating the pace of science.These tools span a wide range of applications and fields: fitness trackers revolutionized public health studies (61), continuous glucose monitors are providing critical insights into nutrition and diabetes research (62), and automated weather stations enhanced meteorological predictions (63).In addition to providing streams of real-time data for ongoing analysis, these automated systems can minimize human observation and experimenter biases.Experimenter bias occurs when the beliefs, expectations, or preferences of the researcher unconsciously influence the conduct or outcome of an experiment.Automating data collection in animal studies helped to eliminate experimenter bias, resulting in refutations of previous results, such as the evidence for statistical learning ability in newborn chicks (64).A particularly noteworthy advancement in the behavioral sciences was the adoption of web-based experiments, especially during the COVID-19 pandemic.Online platforms and interfaces for recruiting and conducting experiments did not only facilitate the collection of behavioral data at a time when traditional lab-based studies were impractical, but they also broadened the scope and diversity of participants (65)(66)(67).Automating data collection also generated opportunities for automating other elements of behavioral science, such as adopting adaptive experimental designs that change based on the responses of participants (68) or collecting larger datasets that can support the use of machine-learning algorithms (11).</p>
<p>Statistical inference.The automation of statistical inference transformed dramatically from the era of manual computations, a reality echoed in old statistical textbooks filled with computationsimplifying shortcuts.The introduction of computers altered statistical methodologies, sometimes even leading to their replacement by machine learning techniques.For example, modern statistical inference engines, like Stan, leverage techniques such as Markov Chain Monte Carlo (MCMC) for efficient sampling of model parameters (69).Tools for likelihood-free inference enable the analysis of statistical models that are not mathematically tractable.Furthermore, frameworks such as Bayesian Workflow (70) and platforms such as the Automatic Statistician (71) are streamlining complex processes like Bayesian inference and the construction of traditional statistical models.The automation of statistical inference, however, is mostly confined to the deduction of new knowledge based on pre-specified statistical models.</p>
<p>Scientific inference and model discovery.Scientific inference, unlike statistical inference, involves generating hypotheses about observations (abduction) and generalizing from observations to laws or broader theories (induction).The automation of scientific inference is termed computational scientific discovery and has so far centered on identifying models or laws that elucidate specific phenomena (22,23,72).One instance of computational scientific D R A F T discovery involves the identification of equations ("symbolic regression") to uncover quantitative laws governing a given data set.Early efforts relied on heuristic search techniques to rediscover insights from mathematics (73,74) or physics (75).Advances in machine learning and high-performance computing facilitated equation discovery, building on reinforcement learning (26), genetic algorithms (25,76,77), MCMC sampling (78), mixed-integer nonlinear programming (79), or gradient-based search techniques (24,27,80,81).However, most forms of computational model discovery are limited to the rediscovery of existing knowledge.Possible exceptions include the discovery of scaling laws and boundary equations in plasma physics (82) and novel models of human decision-making (11).</p>
<p>Closed-loop automation spanning multiple scientific practices.Demonstrations of successful closed-loop automation in empirical research-implementing iterations between experimental design, data collection and model discovery-mark a significant progression for automated scientific practice.One pioneering example is the robot scientist Adam (Figure 2A), which was the first fully automated machine to discover novel scientific knowledge (2).Adam investigated the functional genomics of the yeast S. cerevisiae, and discovered the function of locally orphan enzymesenzymes known to be in yeast but for which the gene(s) encoding them were unknown.The successor of Adam, Eve, is a robot scientist designed for early-stage drug development (39), which identified chemical compounds that outperformed standard drug screening.Eve's most significant discovery is that triclosan (an antimicrobial compound commonly used in toothpastes) may aid against malaria (39,83,84).Another example of a closed-loop discovery system in biology is Wormbot-AI, a platform designed to autonomously conduct experiments on the longevity of worms, capable of testing thousands of interventions annually (85,86).</p>
<p>Complete automation also gained momentum in materials science and chemistry, where efforts are focused on integrating hypothesis generation, decentralized experimentation, and cloudbased decision-making.For instance, modular robotic platforms, driven by machine learning algorithms, were used to optimize material properties by varying synthesis conditions (87-89).One notable example is A-Lab (Figure 2B), an autonomous laboratory for the solid-state synthesis of inorganic powders, which leverages a combination of active learning and machine learning models trained on the literature, to propose novel material candidates (10).</p>
<p>Additionally, behavioral research became amenable to closedloop automation with the ability to collect data via online experiments.Open-source tools like AutoRA (90) facilitate closed-loop research by integrating automated model discovery, experimental design, and experimentation in empirical research.AutoRA effectively interfaces with web-based platforms for automated data collection, integrating the acquisition of behavioral data from human participants.While the potential to yield novel discoveries stands to test, AutoRA served as a computational testbed for philosophy of science, exposing cases where random experimentation outperforms model-guided experimentation (38).</p>
<p>Finally, researchers introduced an LLM-based agent for automating empirical machine learning research, from idea development and experimental design to execution and data analysis, e.g., for improving existing machine learning models (91).Notably, this system also leveraged LLMs to automate the writing and peer review of the resulting research manuscript, with the computational cost of one article estimated to be just 15 USD.</p>
<p>Future opportunities</p>
<p>Existing approaches for automating scientific practice primarily target tasks for which (a) high-quality data is available, (b) the computational complexity can be addressed by current algorithms, and (c) hardware complexity is manageable.The most promising prospects for future automation in scientific practice are found in tasks traditionally limited by human cognitive capacities.This includes areas requiring the processing of large volumes of highdimensional data or exhaustive literature searches.In this section, we highlight a few technological trends that promise to push the boundaries of science automation along these lines.</p>
<p>Data collection, standardization, and sharing.Advancements in cost-effective data collection, standardization, and sharing significantly boost the automatability of scientific practices, particularly those dependent on empirical data.For example, in the behavioral sciences, the utilization of crowd-sourced experimentation platforms like Amazon Mechanical Turk and Prolific revolutionized the efficiency of behavioral data collection.Additionally, LLMs that can mimic human behavior were proposed as proxies for participants, aiding in the acquisition of large-scale datasets (92).Once acquired, such large-yet cost-efficient-datasets can empower datahungry machine learning algorithms, enabling them to uncover novel, and more precise models of human behavior (93)(94)(95)(96).Largescale data collection, however, still bears significant hardware</p>
<p>D R A F T</p>
<p>challenges, e.g., for collecting biological samples from a large number of participants (see Future challenges).Nevertheless, the data quality needed for automated analysis techniques should be complemented by data standardization and sharing.</p>
<p>Scientific data sharing platforms, such as the Open Science Framework, facilitated the availability and accessibility of data needed for automated analyses and computational discovery.The potential of data sharing and standardization is perhaps best illustrated in materials science, where databases for stable materials enabled the prediction of large quantities of new materials (9).Other scientific domains profit from similar efforts.For example, in neuroscience, archives like DANDI, OpenNeuro, DABI and BossDB allow researchers to share data using community standards (97), such as BIDS for neural data (98).</p>
<p>Combining data-driven and knowledge-driven discovery.</p>
<p>A particularly promising approach to automating scientific discovery is the integration of pre-existing human knowledge into the discovery process.Traditionally, data-driven discovery methods operated with minimal prior knowledge about the specific domain of scientific inquiry.This pure data-driven approach makes such methods particularly susceptible to noisy data.However, recent work demonstrates that incorporating prior theoretical knowledge can significantly aid in recovering scientific models from noisy datasets.For example, Bayesian symbolic regression exhibits greater efficacy in recovering equations from noisy data when given priors about scientific equations extracted from Wikipedia (78,99).Similarly, embedding prior knowledge in the form of general logical axioms proved instrumental in rediscovering complex scientific laws, including Kepler's third law of planetary motion and Einstein's relativistic time-dilation law (79,100).Furthermore, experiments with the BacterAI, which uses active learning for the automated study of microbial metabolisms, have demonstrated the advantage of leveraging relevant prior knowledge (101).Specifically, when the metabolic model trained on one bacterial species was retrained for the species of interest, it more efficiently discovered its metabolic model compared to starting the learning process from scratch, despite the two species differing in their metabolic capabilities.These examples highlight the benefits of combining data-driven and knowledge-driven approaches for automated model discovery.</p>
<p>The benefits of knowledge-driven discovery are, however, fundamentally limited by the quality of prior knowledge.For example, Bayesian adaptive experimentation can be misled if prior knowledge mischaracterizes the data (102,103).Thus, data-driven approaches to computational model discovery become particularly beneficial when dominant scientific models in the empirical sciences are more informed by (wrong) theory versus data.This is evident in computational models of human reinforcement learning, which predominantly rely on classic machine learning algorithms (104).Recent work demonstrated that a data-driven model discovery can uncover novel reinforcement learning models that better explain human learning than traditional models (95).</p>
<p>Finally, a notable area of progress in automated model discovery is the analysis of high-dimensional datasets, such as fluid dynamics captured in video format, through reduced-order modeling.This process involves learning a low-dimensional representation of the dynamics inherent in complex data and then decoding the governing equations of these latent dynamics (105)(106)(107)(108). Similar approaches were developed to automate the discovery of neural data embeddings correlating with behavioral dynamics (109).These approaches promise to extend the reach of automated model discovery to high-dimensional naturalistic datasets.beyond experimental control.</p>
<p>Generative AI and LLMs.Generative AI and LLMs offer paths towards automating scientific practices that have historically been challenging due to their computational complexity and qualitative nature (8,16,91,110).Among these are the synthesis and integration of literature, and documentation of findings.</p>
<p>Researchers argued that LLMs show promise in enhancing literature reviews, a task currently limited by the cognitive constraints and language barriers of human scientists (111,112).Whereas humans may only be able to parse and integrate a few hundred articles into a literature review-the scope of which is heavily influenced by the expertise and biases of the researcher-LLMs may accomplish literature synthesis in the order of thousands or millions of articles.Critically, LLMs can take into account articles written in different languages, thus helping to counter the dominance of Western perspectives in scientific literature.Thus, LLMs can assist in extending or even bypassing human researchers' cognitive limitations.A notable application of LLMs for the purpose of literature synthesis is Elicit, which utilizes LLMs trained on paper abstracts to support and help researchers extract relevant information from the scientific literature (112).Another instance of such assistance is an LLM-based "coscientist" for chemical research, which improved the planning of chemical syntheses based on extensive information available on the internet, and aided in the navigation of extensive hardware documentation (8).Additionally, BrainGPT-an LLM fine-tuned to the neuroscience literature-demonstrated the capability to outperform human experts in predicting the results of neuroscience experiments (113).</p>
<p>Combined with their capability for literature synthesis, LLMs can foster the discovery of new research directions and hypotheses (91).Along these lines, LLMs have the potential to expand experimental design spaces, addressing a common bottleneck in automated scientific practice.While traditional automated experimentation is confined to researcher-defined variables (cf. Figure 2), LLMs could identify novel experimental variables of interest, thus broadening the scope of scientific inquiry.However, it can be argued that LLMs risk rediscovering already known hypotheses and experiments (18).</p>
<p>Once experiments are designed, LLMs may aid in the balanced documentation and communication of the research study, including the automated documentation of research code (114,115).Apart from aiding in the construction of research articles, LLMs can enable automated translation into multiple languages.This advancement is particularly beneficial for non-native English speakers and is an example of how automation and AI can address ethical challenges in science.Nevertheless, literature reviews conducted by human scientists serve not only to synthesize knowledge but also to build and refine the conceptual frameworks of evolving scientists-a process that is critical to scientific training and that is challenged by the overuse of LLMs for literature synthesis.</p>
<p>Future challenges</p>
<p>Despite recent advances and opportunities for the automation of science, there remain substantial obstacles.This section examines technological bounds rooted in four bottlenecks (cf. Figure 1): limited availability and quality of data, intractable computational complexity of certain scientific tasks, lack of required hardware, and subjectivity in assessing the outputs of scientific tasks.These</p>
<p>D R A F T</p>
<p>bottlenecks highlight why barriers to automation remain difficult to surmount in the basic sciences (as opposed to engineering), at least with the technologies and methodologies currently at our disposal.Addressing these challenges will require significant interdisciplinary efforts to identify solutions that enable automation beyond a few selected domains of scientific inquiry.</p>
<p>Limited availability and quality of inputs.Prior applications of computational discovery, such as in chemistry (5,7,116) and materials science (9,10), relied on standardized formats for both data and scientific hypotheses that are easily parsed by machine learning algorithms.However, most tasks of scientific practice rely on a diversity of representations for scientific knowledge.For example, computational models in the natural sciences are expressed in various formats, such as equations embedded in scientific articles or computer code written in different programming languages.Without standardization across disciplines, automated systems face significant challenges in drawing parallels or applying concepts from one domain to another.Efforts to standardize the representation of scientific models and other forms of scientific knowledge promise to ease the automation of scientific practices relying on such knowledge (117).However, even if data is standardized and widely available, ensuring its quality remains critical.For instance, literature synthesis enabled by LLMs may be unfruitful or even misleading if fraudulent or unreproducible papers are included as inputs to these models.Therefore, robust quality control measures must accompany standardization efforts to maintain the integrity and usefulness of automated systems.</p>
<p>Computational complexity.One of the fundamental bottlenecks in the automation of scientific practice lies in the computational complexity of many scientific tasks.For example, complexity analyses within the realm of cognitive science indicate that scientific discovery in cognitive science may be computationally intractable in principle, even with unlimited availability of data (118).These theoretical results suggest that uncovering a definitive "ground-truth" theory may be beyond the reach of computation.</p>
<p>One potential critique of leveraging computational methods for scientific discovery hinges on the incomplete comprehension of the cognitive processes, and the concomitant computational complexity underlying it.One may argue that without a full grasp of how humans tackle scientific inquiries, designing algorithms capable of similar feats seems implausible.However, at least two counterarguments challenge this perspective.First, replicating natural processes is not a prerequisite for solving problems.For instance, modern airplanes achieve superior lift not by emulating the flapping motion of birds but through aerodynamically efficient designs.Second, a deep understanding of cognitive phenomena is not a strict requirement for automation, as evidenced by the capabilities of LLMs to produce coherent natural language sequences without humans having a complete scientific understanding of language generation.Nonetheless, this gap in understanding underscores the importance of implementing robust evaluation methods to ensure the accuracy and mitigate any potential negative impacts of automating scientific processes.</p>
<p>Hardware engineering.The advancement of automated science is significantly hindered by current limitations in laboratory robotics and hardware engineering.For instance, executing complex biological or physics experiments remains challenging.Moreover, while robotic automation has been successfully implemented in certain areas, such as with the robot scientist concept (1,2,101,119), its application is primarily limited to clearly defined engineering problems.Yet, even well-defined engineering problems must manage the noise and variability inherent in the data collected by sensors, which can dramatically affect the reliability of scientific outcomes.Therefore, while progress has been made in automating scientific practice, developing more sophisticated robotics to handle complex, noisy data is crucial for its broader adoption and effectiveness.</p>
<p>The automation of hardware tasks in scientific practice is also hindered by the need for highly specialized equipment, leading to significant capital expenditures, often exceeding millions of dollars.Such custom-built hardware is typically field-specific and lacks versatility for reuse in other scientific domains.This challenge is evident in the limited cross-utilization of hardware between disciplines, as seen in the relatively small amount of equipment that materials scientists have been able to adapt from the more heavily automated field of drug discovery.Addressing this issue requires a strategic approach where, for each scientific field, scientists identify and develop a core set of automated hardware that can deliver the greatest impact.This not only involves designing equipment that meets the unique needs of each field but also balancing specificity with adaptability, to maximize utility and cost-effectiveness.</p>
<p>Subjective goals of scientific tasks.More than in engineering, practices in basic science are inherently subjective in how the outcomes of those practices are evaluated.This challenge is particularly evident in developing AI capable of generating novel and impactful scientific ideas.Novelty and impact involve a high degree of subjectivity and variability, making it difficult for these systems to replicate human judgment in the space of scientific inquiry (16).This issue is compounded by the personal aspect of scientific practice.The selection of scientific projects is guided by the personal experience and perspective of human scientists.Diversity in such perspectives paired with interdisciplinary exchange can lead to a greater diversity of ideas in human scientific systems (120)-a dimension that AI currently cannot emulate without explicit instruction.Furthermore, the lack of standardized solutions in many scientific areas means that automating these tasks risks constraining exploration, which is vital for scientific advancement.</p>
<p>Moreover, interpretation of data patterns and hypothesis generation often necessitates human judgment to translate statistical regularities into meaningful scientific interpretations.Techniques like topic modeling, while effective in identifying text co-occurrence patterns, require human insight to align these patterns with relevant scientific constructs (121).The role of human judgment is perhaps best exemplified in serendipitous discovery, often stemming from unexpected failures or results.For example, Alexander Fleming's discovery of penicillin began with the accidental contamination of a Petri dish.Instead of discarding it, his observation of the bacteria being killed by the mold led to the development of the first antibiotic.These aspects highlight the crucial role of human judgment in scientific discovery.</p>
<p>Implications</p>
<p>Although the automation of science currently faces significant limitations, the extent to which it will evolve in the mid-to longterm remains an open empirical question.As advancements in hardware and algorithms continue, the range of practices subject to automation is likely to expand.In this section, we explore the practical and ethical consequences of this trend.</p>
<p>D R A F T</p>
<p>Practical implications.</p>
<p>The role of human scientists and the paradox of automation.The advancement of automation in scientific practice raises considerations regarding the future role of human scientists.On the one hand, it can be argued that automation reduces the need for human involvement.Scientific discovery systems may become able to monitor themselves and tune themselves to optimal performancepotentially excluding humans from the scientific discovery loop.On the other hand, it can argued that the greater the efficiency of an automated system, the more vital the role of human oversight (122).A critical assumption underlying this "paradox of automation" is that automation is not perfect; the potential for accumulating errors necessitates human intervention.If automation were flawless, human oversight would be unnecessary, and the paradox would not exist.However, for tasks with sufficient complexity and uncertainty, this paradox suggests that, in highly automated environments, human contributions, though less frequent, are more critical.This may specifically apply to tasks that demand subjective assessment or the synthesis of complex data, such as reviewing scientific literature, as well as high-level responsibilities such as strategic allocation of funds for scientific inquiry.</p>
<p>Even in the absence of subjective assessment, there are inherent risks associated with automation.For instance, an error within an automated system can lead to a cascade of compounded errors, persisting and potentially amplifying until the system is either corrected or deactivated.This may be particularly problematic for automation methods whose decisionmaking processes are not completely predictable, as is the case for many machine learning algorithms.This unpredictability raises the issue of responsibility for unintended consequences such as injuries.Given the potential severe legal and financial implications of compounding errors in automation, the involvement of human scientists, even in areas where automation is technically feasible, may prove to be more efficient, practical, and safe in the near future.Thus, the paradox of automation underscores the lasting importance of human expertise and the need for a balanced approach that combines automated systems with human judgment.</p>
<p>Research training.With increased automation of science, there arises a need to reevaluate and adapt scientific education.This new landscape calls for training that encompasses not only traditional scientific knowledge but also skills for effectively working alongside automated scientific discovery systems.For instance, obtaining valuable outputs from LLMs is becoming an essential skill.Moreover, scientists will need to develop competencies in understanding and evaluating the functioning and outputs of automated systems, as is already demanded for statistical software (47).This shift implies a growing demand for engineers, scientists, and technicians proficient in advanced STEM skills.</p>
<p>Research evaluation.The current pace of science is primarily determined by our capacity to carry out the research itself.Laboratory studies in fields like biology and chemistry can take years, contrasting with the relatively quick peer review process.However, if advancements in automation enable research to be conducted and documented several magnitudes faster (91), this could lead to a substantial increase in the rate of research article submissions.Such a scenario would further strain the already pressured peer review system.One potential solution could be the automation of peer review, possibly through the use of LLMs; however, this approach has already faced restrictions and bans in certain contexts due to concerns about its efficacy, reliability, and confidentiality (123).Another potential solution is for journals to require that articles generated by automated systems be accompanied by critical evaluations from corresponding human authors.This ensures that human researchers retain comprehension and oversight of what is being submitted while also serving as initial reviewers of the work generated by their automated systems.Either way, this shift would necessitate a reevaluation of the peer review process, ensuring it remains rigorous and effective in the face of increased scientific productivity.</p>
<p>Scientific methods.The automation of scientific practice has the potential to bring about a shift in scientific methods that goes beyond mere acceleration of scientific discovery.As discussed above, the use of machines for scientific discovery allows us to move beyond the cognitive and physical constraints inherent to human scientists (19).Consider, for example, the principle of parsimony in the construction of scientific models.Traditionally, parsimonious models have been favored for their superior generalization, ease of interpretation and communicability among human scientists.However, as discussed in (21), recent studies suggest that highly complex models can, under certain conditions, surpass the generalization capabilities of simpler ones (124), leading to unprecedented advances in scientific research (e.g., for 3D protein folding (6) or material discovery ( 9)).Moreover, as explored in ( 21), the development of such complex models is often a prerequisite for discovering successful parsimonious models (e.g., (125)(126)(127)).This ability of machines to explore and develop models with a level of complexity beyond what is readily interpretable by humans opens up new avenues for scientific progress, less constrained by human cognitive limitations.However, as discussed above, for basic science, there is epistemic value in human understanding that may outweigh the predictive power of AI scientists.</p>
<p>Another consequence of automation concerns the ways in which empirical research is conducted.For example, automated systems can hypothesize and experiment in design spaces far beyond the reach of human cognitive capabilities (9,119).Furthermore, the ability to collect large amounts of data cheaply may obviate frequent iterations between hypothesis generation, experimental design, and data collection.Instead, with the availability of large data sets, the problem of scientific discovery can be transformed into a model discovery problem more amenable to machine learning (11,94,128).However, it is important to recognize that the success of a one-time large-scale data collection hinges on a well-defined experimental design space and the stability of the system under study, as constant changes in the system can undermine the effectiveness of this approach.Accordingly, adaptive experimental design may be needed to identify suitable design spaces (58).</p>
<p>Ethical implications.</p>
<p>Biases.While human biases influence every aspect of scientific work, automated systems are not immune to bias.They can inherit biases from their creators, the construction process, the data they use, and their training format (129).Examples include discriminatory biases in facial recognition technology (130), unrepresentative sampling in psychological experiments (116), and discrimination in automated participant recruitment processes (131).Moreover, automated literature reviews don't escape the biases inherent to the existing literature.These biases can be democratized and exacerbated by the pace of these systems, especially when D R A F T they are uninterpretable or operate as "black boxes."However, a potential advantage is that biases in automated systems may be easier to correct than in humans, such as by using more diverse data, or by aligning automated systems with societal norms.</p>
<p>Value alignment and responsibility.The risk of harmful biases and outcomes of automated processes call for their value alignment with broader societal norms.This is particularly crucial as automation could potentially ease the path for malevolent entities to conduct research detrimental to society, such as developing chemical or biological weapons.Such outcomes underscore the necessity of ethics dedicated to addressing these issues, ensuring that automated scientific advancements align with human values.</p>
<p>Consequences of automation also bring about the issue of responsibility: If a scientific discovery that affects the wider society is based on an automated process, who is responsible?The accountability for effects arising from harmful scientific practice remains ambiguous-whether it lies with the system's creator, its user, or the implementer of societal changes based on the system's output.This issue parallels broader debates in AI, such as liability in self-driving car accidents or the creation of automated artwork.Additionally, the potential misuse of powerful systems (e.g., a system suggesting harmful drug treatments) necessitates robust safeguards.The same applies to potential violations of data privacy.When automated systems generate contentious theories or design ethically questionable experiments, human oversight and responsibility are imperative.Importantly, ethical guidelines are often formulated by the institutions developing the systems (132), highlighting the need for an external framework that can hold institutions accountable.</p>
<p>Conclusion</p>
<p>While the automation of scientific practice is currently confined mostly to well-defined engineering and discovery problems, there is the potential for automation to pervade a large part of scientific practice.We suggest that this trend represents not merely a series of quantitative changes, such as increased efficiency or precision in science, but brings about a fundamental shift in the conduct of science.The integration of AI into scientific practice has the potential to overcome human cognitive limitations, thereby expanding our capabilities for discovery.Yet, this advance is not without challenges-data availability, computational complexity, engineering demands, and subjectivity of scientific task goals mark the technical boundaries of current automatability.Furthermore, normative goals of science-anchored on societal valuespotentially make complete automation of scientific practice neither desirable nor feasible.Finally, this qualitative shift comes with practical and ethical challenges that call for interdisciplinary and collective efforts from researchers, policymakers, and the broader community to navigate the future of science.</p>
<p>D R A F T</p>
<p>Fig. 1 .
1
Fig. 1.Factors determining the technological reach of automation in scientific practice.</p>
<p>Fig. 2 .
2
Fig. 2. Closed-loop automation systems.(A) Adam for functional genomics.(B) A-Lab for materials science.(C) AutoRA for behavioral science.Dashed boxes list knowledge and processes provided by human researchers.</p>
<p>of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
PNAS -September 27, 2024 -vol. XXX -no. XX -3
of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
PNAS -September 27, 2024 -vol. XXX -no. XX -7
of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
PNAS -September 27, 2024 -vol. XXX -no. XX -9
of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
AcknowledgmentsS. Musslick  and S. Mahesh were supported by Schmidt Science Fellows, in partnership with the Rhodes Trust.S. Musslick was also supported by the Carney BRAINSTORM program at Brown University and the National Science Foundation (2318549).S. Mahesh also acknowledges the support of the Acceleration Consortium fellowship.S.J. Sloman acknowledges support from the UKRI Turing AI World-Leading Researcher Fellowship, [EP/W002973/1].S. Chandramouli was supported by the Finnish Center for Artificial Intelligence, and Academy of Finland (328813); he also acknowledges the support from the Jorma Ollila Mobility Grant by Nokia Foundation.L. Bartlett and F. Gobet were supported by European Research Council Grant ERC-ADG-835002-GEMS. T. L. Griffiths was supported by a grant from the NOMIS Foundation.R. D. King was supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, by Chalmers Artificial Intelligence Research Centre (CHAIR), and by the UK EPSRC grants EP/R022925/2 and EP/W004801/1.The authors thank Solomon Oyakhire for valuable feedback.DisclosuresThe authors have no competing interests to report.
Functional genomic hypothesis generation and experimentation by a robot scientist. Rd King, Nature. 4272004Nature Publishing Group UK London</p>
<p>The automation of science. Rd King, Science. 3242009American Association for the Advancement of Science</p>
<p>Generating conjectures on fundamental constants with the Ramanujan Machine. Raayoni, Nature. 5902021Nature Publishing Group UK London</p>
<p>Advancing mathematics by guiding human intuition with AI. Davies, Nature. 6002021Nature Publishing Group UK London</p>
<p>Synthetic organic chemistry driven by artificial intelligence. R Af De Almeida, Moreira, Rodrigues, Nat. Rev. Chem. 32019Nature Publishing Group UK London</p>
<p>Highly accurate protein structure prediction with AlphaFold. Jumper, Nature. 5962021Nature Publishing Group</p>
<p>Applications of artificial intelligence for organic chemistry: the DENDRAL project. Lindsay Rk, No Title)1980</p>
<p>Autonomous chemical research with large language models. Boiko, Macknight, Kline, Nature. 6242023Nature Publishing Group</p>
<p>Scaling deep learning for materials discovery. Merchant, Nature. 2023</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. Nj, Szymanski, Nature. 2023</p>
<p>Using large-scale experiments and machine learning to discover theories of human decision-making. Jc Peterson, Dd Bourgin, Agrawal, Reichman, Griffiths, Science. 3722021American Association for the Advancement of Science</p>
<p>Foundation Models for Scientific Discovery (FoundSci). Velasquez, Def. Adv. Res. Proj. Agency (DARPA) Program Solicitation. 2023</p>
<p>Nobel Turing Challenge: creating the engine for scientific discovery. Kitano, Syst. Biol. Appl. 7292021Nature Publishing Group UK London</p>
<p>The future of fundamental science led by generative closed-loop artificial intelligence. Zenil, arXiv:2307.075222023arXiv preprint</p>
<p>Science in the age of large language models. Birhane, Kasirzadeh, S Leslie, Wachter, Nat. Rev. Phys. pp. 2023Nature Publishing Group UK London</p>
<p>Scientific discovery in the age of artificial intelligence. Wang, Nature. 6202023Nature Publishing Group UK London</p>
<p>How should the advent of large language models affect the practice of science?. Binz, arXiv:2312.037592023arXiv preprint</p>
<p>Cognitive science of augmented intelligence. Dubova, Galesic, Rl Goldstone, Cogn. Sci. 46e132292022Wiley Online Library</p>
<p>Mind children: The future of robot and human intelligence. Moravec, Harv. UP. 1988</p>
<p>Is ockham's razor losing its edge? new perspectives on the principle of model parsimony. Dubova, 10.31222/osf.io/bs5xe2024MetaArXiv preprint</p>
<p>Scientific discovery: Computational explorations of the creative processes. Langley, 1987MIT press</p>
<p>Computational discovery of scientific knowledge in Computational discovery of scientific knowledge: introduction, techniques, and applications in environmental and life sciences. Deroski, L Langley, Todorovski, 2007Springer</p>
<p>AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. Sm Udrescu, Adv. Neural Inf. Process. Syst. 332020</p>
<p>Interpretable machine learning for science with PySR and SymbolicRegression. Cranmer, arXiv:2305.015822023jl. arXiv preprint</p>
<p>Discovering symbolic policies with deep reinforcement learning. Landajuela, 2021</p>
<p>GFN-SR: Symbolic Regression with Generative Flow Networks. Li, Marinescu, Musslick, 2023</p>
<p>DENDRAL: a case study of the first expert system for scientific hypothesis formation. Rk Lindsay, Buchanan, Feigenbaum, Lederberg, Artif. intelligence. 611993Elsevier</p>
<p>Materials design and discovery with high-throughput density functional theory: the open quantum materials database (OQMD). Je Saal, Kirklin, Aykol, C Meredig, Wolverton, Jom. 652013Springer</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. Jain, APL materials. 12013AIP Publishing</p>
<p>BioGraph: unsupervised biomedical knowledge discovery via automated hypothesis generation. Am Liekens, Publisher: BioMed Central. 122011Genome biology</p>
<p>Automated cognome construction and semi-automated hypothesis generation. Jb, Voytek, Voytek, J. neuroscience methods. 2082012Elsevier</p>
<p>SweetPea: A standard language for factorial experimental design. Musslick, Behav. Res. Methods pp. 2020Springer</p>
<p>Mix, a program for pseudorandomization. Van Casteren, Davis, Behav. research methods. 382006Springer</p>
<p>Assessing the distinguishability of models and the informativeness of data. M A Dj Navarro, Pitt, Myung Ij, Cogn. psychology. 492004Elsevier</p>
<p>Optimal experimental design for model discrimination. M A Ji Myung, Pitt, Psychol. review. 1162009American Psychological Association</p>
<p>Adaptive design optimization: A mutual information-based approach to model discrimination in cognitive science. Dr Cavagnaro, M A Myung, Pitt, Kujala, Neural computation. 222010MIT Press</p>
<p>An evaluation of experimental sampling strategies for autonomous empirical research in cognitive science. Musslick, 20234545</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. Williams, J. Royal society Interface. 1220141289. 2015The Royal Society</p>
<p>Closed-loop cycles of experiment design, execution, and learning accelerate systems biology model development in yeast. Coutant, Proc. Natl. Acad. Sci. Natl. Acad. SciNational Acad Sciences2019116</p>
<p>QUEST+: A general multidimensional Bayesian adaptive psychometric method. Ab Watson, J. Vis. 172017The Association for Research in Vision and Ophthalmology</p>
<p>Valentin, arXiv:2305.07721Designing Optimal Behavioral Experiments Using Machine Learning. 2023arXiv preprint</p>
<p>Bayesian inference and online experimental design for mapping neural microcircuits. Shababo, Paige, L Pakman, Paninski, Adv. Neural Inf. Process. Syst. 262013</p>
<p>Sequential Bayesian experiment design for optically detected magnetic resonance of nitrogen-vacancy centers. Dushenko, Ambal, Mcmichael, Phys. review applied. 14540362020APS</p>
<p>Simulation-based optimal Bayesian experimental design for nonlinear systems. Huan, Marzouk, J. Comput. Phys. 2322013Elsevier</p>
<p>Robotic search for optimal cell culture in regenerative medicine. Gn Kanda, Elife. 11e770072022Publisher: eLife Sciences Publications Limited</p>
<p>Accelerating bayesian optimization for biological sequence design with denoising autoencoders. Stanton, 2022</p>
<p>Chembo: Bayesian optimization of small organic molecules with synthesizable recommendations. Korovina, 2020</p>
<p>Constrained Bayesian optimization for automatic chemical design using variational autoencoders. Rr Griffiths, Jm Hern ndez-Lobato, Chem. science. 112020Royal Society of Chemistry</p>
<p>Bayesian optimization for materials design with mixed quantitative and qualitative variables. Zhang, Apley, Chen, Sci. reports. 102020Nature Publishing Group UK London</p>
<p>On-the-fly closed-loop materials discovery via bayesian active learning. Ag Kusne, Nat. communications. 1159662020</p>
<p>Benchmarking the performance of Bayesian optimization across multiple experimental materials science domains. Liang, Comput. Mater. 71882021Nature Publishing Group UK London</p>
<p>Optimal sensor placement methodology for parametric identification of structural systems. Papadimitriou, J. sound vibration. 2782004Elsevier</p>
<p>Data-driven experimental design and model development using Gaussian process with active learning. Chang, Kim, M A Zhang, J I Pitt, Myung, Cogn. Psychol. 1251013602021Elsevier</p>
<p>Inconsistency of Bayesian Inference for Misspecified Linear Models, and a Proposal for Repairing It. T P Gr nwald, Van Ommen, Bayesian Analysis. 122017</p>
<p>Rainforth, Foster, Ivanova, Smith, arXiv:2302.14545Modern bayesian experimental design. 2023arXiv preprint</p>
<p>Towards Robust Bayesian Adaptive Design Methods for the Study of Human Behavior. Sj Sloman, 2022Carnegie Mellon UniversityPhD thesis</p>
<p>Against theory-motivated experimentation in science. Dubova, Moskvichev, Zollman, MetaArXiv. June. 242022</p>
<p>Sampling heuristics for active function. Gelpi, Saxena, Lifchits, Buchsbaum, Lucas, Proceedings of the 43rd Annual Meeting of the Cognitive Science Society. (cognitivesciencesociety.org). the 43rd Annual Meeting of the Cognitive Science Society. (cognitivesciencesociety.org)2021</p>
<p>Explore your experimental designs and theories before you exploit them! Behav. Dubova, Sj Sloman, Andrew, Nassar, Musslick, Brain Sci. 472024Cambridge University Press</p>
<p>Wearable activity trackers, accuracy, adoption, acceptance and health impact: A systematic literature review. Shin, J. biomedical informatics. 932019Elsevier</p>
<p>Juvenile Diabetes Research Foundation Continuous Glucose Monitoring Study Group, Effectiveness of continuous glucose monitoring in a clinical care environment: evidence from the Juvenile Diabetes Research Foundation continuous glucose monitoring (JDRF-CGM) trial. Diabetes care. 332010Publisher: Am Diabetes Assoc</p>
<p>Antarctic automatic weather station program: 30 years of polar observation. Ma Lazzara, Weidner, Keller, Thom, Cassano, Bull. Am. Meteorol. Soc. 932012American Meteorological Society</p>
<p>Automated study challenges the existence of a foundational statistical-learning ability in newborn chicks. Sm Wood, Johnson, Wood, Psychol. Sci. 302019Sage Publications Sage CA</p>
<p>psiTurk: An open-source framework for conducting replicable behavioral experiments online. Tm Gureckis, Behav. research methods. 482016Springer</p>
<p>Conducting behavioral research on Amazon's Mechanical Turk. Mason, Suri, Behav. research methods. 442012Springer</p>
<p>Prolific. ac-A subject pool for online experiments. S Palan, Schitter, J. Behav. Exp. Finance. 172018Elsevier</p>
<p>Complex cognitive algorithms preserved by selective social learning in experimental populations. Thompson, Van Opheusden, Sumers, Griffiths, Science. 3762022</p>
<p>Stan: A probabilistic programming language. Carpenter, J. statistical software. 762017NIH Public Access</p>
<p>. Gelman, arXiv:2011.018082020Bayesian workflow. arXiv preprint</p>
<p>The automatic statistician. Autom. machine learning: Methods, systems, challenges pp. Steinruecken, Smith, Janz, Z Lloyd, Ghahramani, 2019Springer International Publishing</p>
<p>Introduction: Scientific discovery in the social sciences. Gobet, Addis, Lane, Sozou, Sci. discovery social sciences. 2019Springer</p>
<p>Integrating quantitative and qualitative discovery: the ABACUS system. Bc Falkenhainer, Michalski, Mach. Learn. 11986Springer</p>
<p>The ubiquity of discovery. Db Lenat, Artif. Intell. 91977Elsevier</p>
<p>Data-driven discovery of physical laws. Langley, Cogn. Sci. 51981Elsevier</p>
<p>Genetic programming for developing simple cognitive models. Bartlett, Pirrone, Javed, Lane, Gobet, 20234545</p>
<p>Automatic generation of cognitive theories using genetic programming. Minds Mach. F Frias-Martinez, Gobet, 2007Springer17</p>
<p>A Bayesian machine scientist to aid in the solution of challenging scientific problems. Guimer , Sci. advances. 669712020American Association for the Advancement of Science</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. Cornelio, Nat. Commun. 1417772023Nature Publishing Group UK London</p>
<p>A unified framework for deep symbolic regression. Landajuela, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Musslick, Recovering Quantitative Models of Human Information Processing with Differentiable Architecture Search. 20214345</p>
<p>Data driven theory for knowledge discovery in the exact sciences with applications to thermonuclear fusion. Murari, Sci. Reports. 1019858. 2020Nature Publishing Group UK London</p>
<p>Yeast-based automated high-throughput screens to identify anti-parasitic lead compounds. Bilsland, Open Biol. 31201582013The Royal Society</p>
<p>Plasmodium dihydrofolate reductase is a second enzyme target for the antimalarial action of triclosan. Bilsland, Sci. Reports. 810382018Nature Publishing Group UK London</p>
<p>The million-molecule challenge: a moonshot project to rapidly advance longevity intervention discovery. Mb Lee, Blue, M Muir, Kaeberlein, GeroScience pp. 2023Springer</p>
<p>WormBot, an open-source robotics platform for survival and behavior analysis in C. elegans. Jn Pitt, GeroScience. 412019Springer</p>
<p>Machine learning on a robotic platform for the design of polymer-protein hybrids. Mj Tamasi, Adv. Mater. 3422018092022Wiley Online Library</p>
<p>Self-driving laboratory for accelerated discovery of thin-film materials. Bp Macleod, Sci. Adv. 688672020American Association for the Advancement of Science</p>
<p>Closed-loop superconducting materials discovery. Ea Pogue, Comput. Mater. 91812023</p>
<p>AutoRA: Automated Research Assistant for Closed-Loop Computational Discovery. Musslick, Strittmatter, Holland, 2023</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Lu, arXiv:2408.062922024arXiv preprint</p>
<p>Can ai language models replace human participants?. D Dillion, Tandon, K Gu, Gray, Trends Cogn. Sci. 272023</p>
<p>Bk Petersen, Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients in International Conference on Learning Representations. 2021</p>
<p>Beyond playing 20 questions with nature: Integrative experiment design in the social and behavioral sciences. Almaatouq, Behav. Brain Sci. pp. 2022Cambridge University Press</p>
<p>Predictive and Interpretable: Combining Artificial Neural Networks and Classic Cognitive Models to Understand Human Learning and Decision Making. Mk Eckstein, Summerfield, Daw, Miller, 2023Publisher: Cold Spring Harbor Laboratory</p>
<p>Automatic Discovery of Cognitive Strategies with Tiny Recurrent Neural Networks. Ji-An, Benna, Mattar, 2023Publisher: Cold Spring Harbor Laboratory</p>
<p>A comparison of neuroelectrophysiology databases. Subash, Sci. Data. 107192023</p>
<p>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Kj Gorgolewski, Sci. data. 32016Nature Publishing Group</p>
<p>Bayesian Machine Scientist for Model Discovery in Psychology. Hewson, Strittmatter, Marinescu, S Williams, Musslick, 2023</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. C Cory-Wright, Cornelio, Dash, L El Khadir, Horesh, Nat. Commun. 1559222024</p>
<p>Bacterai maps microbial metabolism without prior knowledge. Ac Dama, Nat. Microbiol. 82023</p>
<p>Characterizing the robustness of Bayesian adaptive experimental designs to active learning bias. Sj Sloman, Oppenheimer, Broomell, Shalizi, arXiv:2205.136982022arXiv preprint</p>
<p>Knowing what to know: Implications of the choice of prior distribution on the behavior of adaptive design optimization. Sj Sloman, Cavagnaro, Broomell, Behav. Res. Methods pp. 2024</p>
<p>Reinforcement learning: An introduction. A G Rs Sutton, Barto, 2018MIT press</p>
<p>Machine Learning Methods for Reduced Order Modeling in Model Order Reduction and Applications: Cetraro, Italy 2021. Jn Kutz, 2023Springer</p>
<p>Reduced order modeling of parametrized systems through autoencoders and SINDy approach: continuation of periodic solutions. Conti, Gobat, Fresca, Manzoni, Frangi, Comput. Methods Appl. Mech. Eng. 4111160722023Elsevier</p>
<p>Dimensionality reduction and reduced-order modeling for traveling wave physics. Mendible, Brunton, Ay Aravkin, Lowrie, Kutz, Theor. Comput. Fluid Dyn. 342020Springer</p>
<p>Automated discovery of fundamental variables hidden in experimental data. Chen, Nat. Comput. Sci. 22022Nature Publishing Group</p>
<p>Learnable latent embeddings for joint behavioural and neural analysis. Schneider, Lee, Mathis, Nature pp. 2023Nature Publishing Group UK London</p>
<p>Large language models for scientific synthesis, inference and explanation. Zheng, arXiv:2310.079842023arXiv preprint</p>
<p>Artificial Intelligence Research in Business and Management: A Literature Review Leveraging Machine Learning and Large Language Models. Guler, Kirshner, Vidgen, Available at SSRN. 45408342023</p>
<p>Elicit: AI literature review research assistant. M A Whitfield, Hofmann, Public Serv. Q. 192023Taylor &amp; Francis</p>
<p>Large language models surpass human experts in predicting neuroscience results. Luo, 2024</p>
<p>Chen, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>HotGPT: How to Make Software Documentation More Useful with a Large Language Model. Su, 2023</p>
<p>The weirdest people in the world?. Henrich, Sj Heine, Norenzayan, Behav. brain sciences. 332010Cambridge University Press</p>
<p>Integrating model development across computational neuroscience, cognitive science, and machine learning. P Gleeson, Neuron. 1112023Elsevier</p>
<p>How hard is cognitive science?. Rich, De Haan, I Wareham, Van Rooij, 20214343</p>
<p>A mobile robotic chemist. Burger, Nature. 5832020Nature Publishing Group UK London</p>
<p>Artificial intelligence and illusions of understanding in scientific research. Messeri, Crockett, Nature. 6272024</p>
<p>Reading tea leaves: How humans interpret topic models. Chang, Gerrish, J Wang, D Boyd-Graber, Blei, Adv. neural information processing systems. 222009</p>
<p>Ironies of automation in Analysis, design and evaluation of man-machine systems. Bainbridge, 1983Elsevier</p>
<p>The Use of Generative Artificial Intelligence Technologies is Prohibited for the NIH Peer Review Process (2023) Published: Available from NIH Grants. National Institutes of Health</p>
<p>Reconciling modern machine-learning practice and the classical bias-variance trade-off. Belkin, Hsu, S Ma, Mandal, Proc. Natl. Acad. Sci. Natl. Acad. Sci2019116</p>
<p>The lottery ticket hypothesis: Finding sparse, trainable neural networks. Frankle, Carbin, arXiv:1803.036352018arXiv preprint</p>
<p>Li, Train big, then compress: Rethinking model size for efficient training and inference of transformers in International Conference on machine learning. 2020</p>
<p>Scaling up psychology via scientific regret minimization. Agrawal, Peterson, Griffiths, Proc. Natl. Acad. Sci. Natl. Acad. SciNational Acad Sciences2020117</p>
<p>Manifesto for a new (computational) cognitive revolution. Griffiths, Cognition. 1352015Elsevier</p>
<p>. L Daston, Galison, Objectivity, 2021Princeton University Press</p>
<p>Gender shades: Intersectional accuracy disparities in commercial gender classification. Buolamwini, Gebru, 2018</p>
<p>Algorithmic equity in the hiring of underrepresented IT job candidates. Cobb Yarger, B Payton, Neupane, Online information review. 442020Emerald Publishing Limited</p>
<p>The ethics of AI ethics: An evaluation of guidelines. Minds machines. Hagendorff, 2020Springer30</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6536 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6536</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6536</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-265066887</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.04954v1.pdf" target="_blank">Prompt Sketching for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6536.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6536.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3x3 Sudoku (sketched)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3x3 Sudoku-like puzzles evaluated with prompt sketching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small-scale sudoku-like completion task (3x3 grid variant) used to evaluate whether sketch-aware decoders and templated prompts enable LLMs to perform forward-referencing and constraint reasoning to fill empty cells with unique numbers 1–9.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI InstructGPT family; decoder-only transformer, instruction-tuned for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>3x3 sudoku-like puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (spatial grid completion)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>10 manually generated 3x3 sudoku-like puzzles (1–6 empty spots)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot prompt sketching (template with per-cell variables and stopping phrases); compared to sequential ARGMAX and non-templated prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>template-guided joint variable decoding via sketch-aware decoders (VAR and BEAMVAR) — effectively breadth-limited search over variable assignments (no explicit solver like backtracking called externally)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Grid encoded as a prompt template: deterministic chunks for filled cells and variable slots for empty cells; per-variable stopping phrases and constrained variables limited to numeric tokens (1–9) via LMQL constraints</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>solve count / solve rate (number of puzzles solved exactly out of 10)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>text-davinci-003: Sequential ARGMAX (prompt-sketching ARGMAX) solved 1/10; BEAMVAR solved 6/10; VAR solved 7/10; with a hand-reordered sequential prompt ARGMAX could achieve 10/10 (reordering not always possible).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Sketch-aware decoders (VAR, BEAMVAR) explore a wider hypothesis space and support forward referencing (anticipating future constraints) leading to substantially higher solve rates than greedy ARGMAX; deterministic template chunks and constrained token masking (LMQL) enforce format and legal numbers; re-ordering the template can let greedy decoding succeed but is not always applicable for complex puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>ARGMAX (greedy) vs sketch-aware: ARGMAX solved 1/10 while VAR solved 7/10 and BEAMVAR 6/10 (text-davinci-003). Re-ordering the prompt (sequential ARGMAX with explicit reordering) yielded 10/10 on these puzzles, indicating that ordering + global constraints influences greedy performance. Smaller model text-curie-001 performed poorly (0/10 with prompt-sketch ARGMAX; BEAMVAR/VAR 3/10).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Very small evaluation set (10 puzzles); only a simple 3x3 sudoku-like variant tested (not standard 9x9); no external solver integrated — success depends on model capability and prompt/sketch design; re-ordering can trivially help but may not be feasible for complex puzzles; smaller models fail to exhibit sufficient reasoning; computational overhead for VAR (scales with beam width).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Sketching for Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6536.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6536.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dungeon / Graph Traversal (sketched)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-guided graph traversal (dungeon escape) using prompt sketching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive environment task where an LLM (via a prompt sketch and sketch-aware decoders) selects actions to traverse a small graph/dungeon aiming to reach an exit, evaluated by success rate and average steps to exit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (also evaluated: text-curie-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer, instruction-tuned (text-davinci-003); experiments also include smaller InstructGPT text-curie-001 (6.7B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>text-davinci-003: 175B; text-curie-001: ~6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Dungeon (graph traversal) — find exit in small random dungeons</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial navigation / graph search (planning)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>10 random dungeons (each 8–10 rooms), average shortest exit route 2.3 steps</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot runtime-instantiated prompt sketch for interactive environment; per-step sketches with constrained ACTION variable (choices mapped to neighbor indices); decoding with ARGMAX, VAR, BEAMVAR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>template-aware lookahead via sketch-aware decoders: BEAMVAR implements dynamic beam allocation across variables (encouraging exploration across steps), VAR performs variable-level beam search (sampling multiple candidate actions per step). No explicit external planner; model output is executed in an environment loop.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>A runtime-instantiated sketch representing current node/state, an ACTION variable constrained to allowed move tokens ("0".."9"), and bookkeeping variables for steps and max_steps; the environment executes the chosen token as the next node index.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (dungeons solved out of 10) and average number of steps to reach exit</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>text-davinci-003: ARGMAX found exit in 9/10 but with average ~3.77 steps; BEAMVAR found exit 10/10 with average ~2.4 steps (near shortest path); VAR: 9/10 with ~3.11 steps. text-curie-001: ARGMAX 5/10 (~4.0 steps), BEAMVAR 8/10 (~2.625 steps), VAR 5/10 (~2.4 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Sketch-aware decoders enable the model to account for template-induced continuations and therefore plan ahead; BEAMVAR in particular tends to perform breadth-first exploration and reliably finds near-shortest paths, whereas ARGMAX behaves like greedy depth-first search and is less efficient and sometimes fails; the ACTION constraint prevents invalid moves and helps reliable environment interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>ARGMAX (greedy) vs BEAMVAR/VAR: BEAMVAR improved both success rate and average steps (e.g., text-davinci-003 success 9/10 → 10/10 and steps ~3.77 → ~2.4). Smaller model (text-curie-001) benefits from BEAMVAR much more than ARGMAX, indicating decoder choice interacts with model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Small-scale dungeons only (10 samples); metrics averaged over small N; environment is synthetic and low-dimensional (8–10 nodes); no generalization to larger or continuous spatial tasks shown; computational cost overhead for sketch-aware decoders; dependence on model size — smaller models still fail or perform poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Sketching for Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>PALM-E: An embodied multimodal language model <em>(Rating: 1)</em></li>
                <li>Language Model Cascades <em>(Rating: 1)</em></li>
                <li>Prompting is programming: A query language for large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6536",
    "paper_id": "paper-265066887",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "3x3 Sudoku (sketched)",
            "name_full": "3x3 Sudoku-like puzzles evaluated with prompt sketching",
            "brief_description": "A small-scale sudoku-like completion task (3x3 grid variant) used to evaluate whether sketch-aware decoders and templated prompts enable LLMs to perform forward-referencing and constraint reasoning to fill empty cells with unique numbers 1–9.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "OpenAI InstructGPT family; decoder-only transformer, instruction-tuned for text generation.",
            "model_size": "175B",
            "puzzle_name": "3x3 sudoku-like puzzles",
            "puzzle_type": "constraint satisfaction / combinatorial logic (spatial grid completion)",
            "dataset_name": "10 manually generated 3x3 sudoku-like puzzles (1–6 empty spots)",
            "prompting_method": "zero-shot prompt sketching (template with per-cell variables and stopping phrases); compared to sequential ARGMAX and non-templated prompting",
            "reasoning_technique": "template-guided joint variable decoding via sketch-aware decoders (VAR and BEAMVAR) — effectively breadth-limited search over variable assignments (no explicit solver like backtracking called externally)",
            "internal_representation": "Grid encoded as a prompt template: deterministic chunks for filled cells and variable slots for empty cells; per-variable stopping phrases and constrained variables limited to numeric tokens (1–9) via LMQL constraints",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "solve count / solve rate (number of puzzles solved exactly out of 10)",
            "performance": "text-davinci-003: Sequential ARGMAX (prompt-sketching ARGMAX) solved 1/10; BEAMVAR solved 6/10; VAR solved 7/10; with a hand-reordered sequential prompt ARGMAX could achieve 10/10 (reordering not always possible).",
            "analysis_findings": "Sketch-aware decoders (VAR, BEAMVAR) explore a wider hypothesis space and support forward referencing (anticipating future constraints) leading to substantially higher solve rates than greedy ARGMAX; deterministic template chunks and constrained token masking (LMQL) enforce format and legal numbers; re-ordering the template can let greedy decoding succeed but is not always applicable for complex puzzles.",
            "ablation_comparison": "ARGMAX (greedy) vs sketch-aware: ARGMAX solved 1/10 while VAR solved 7/10 and BEAMVAR 6/10 (text-davinci-003). Re-ordering the prompt (sequential ARGMAX with explicit reordering) yielded 10/10 on these puzzles, indicating that ordering + global constraints influences greedy performance. Smaller model text-curie-001 performed poorly (0/10 with prompt-sketch ARGMAX; BEAMVAR/VAR 3/10).",
            "limitations": "Very small evaluation set (10 puzzles); only a simple 3x3 sudoku-like variant tested (not standard 9x9); no external solver integrated — success depends on model capability and prompt/sketch design; re-ordering can trivially help but may not be feasible for complex puzzles; smaller models fail to exhibit sufficient reasoning; computational overhead for VAR (scales with beam width).",
            "uuid": "e6536.0",
            "source_info": {
                "paper_title": "Prompt Sketching for Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Dungeon / Graph Traversal (sketched)",
            "name_full": "LLM-guided graph traversal (dungeon escape) using prompt sketching",
            "brief_description": "An interactive environment task where an LLM (via a prompt sketch and sketch-aware decoders) selects actions to traverse a small graph/dungeon aiming to reach an exit, evaluated by success rate and average steps to exit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (also evaluated: text-curie-001)",
            "model_description": "Decoder-only transformer, instruction-tuned (text-davinci-003); experiments also include smaller InstructGPT text-curie-001 (6.7B).",
            "model_size": "text-davinci-003: 175B; text-curie-001: ~6.7B",
            "puzzle_name": "Dungeon (graph traversal) — find exit in small random dungeons",
            "puzzle_type": "spatial navigation / graph search (planning)",
            "dataset_name": "10 random dungeons (each 8–10 rooms), average shortest exit route 2.3 steps",
            "prompting_method": "zero-shot runtime-instantiated prompt sketch for interactive environment; per-step sketches with constrained ACTION variable (choices mapped to neighbor indices); decoding with ARGMAX, VAR, BEAMVAR",
            "reasoning_technique": "template-aware lookahead via sketch-aware decoders: BEAMVAR implements dynamic beam allocation across variables (encouraging exploration across steps), VAR performs variable-level beam search (sampling multiple candidate actions per step). No explicit external planner; model output is executed in an environment loop.",
            "internal_representation": "A runtime-instantiated sketch representing current node/state, an ACTION variable constrained to allowed move tokens (\"0\"..\"9\"), and bookkeeping variables for steps and max_steps; the environment executes the chosen token as the next node index.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "success rate (dungeons solved out of 10) and average number of steps to reach exit",
            "performance": "text-davinci-003: ARGMAX found exit in 9/10 but with average ~3.77 steps; BEAMVAR found exit 10/10 with average ~2.4 steps (near shortest path); VAR: 9/10 with ~3.11 steps. text-curie-001: ARGMAX 5/10 (~4.0 steps), BEAMVAR 8/10 (~2.625 steps), VAR 5/10 (~2.4 steps).",
            "analysis_findings": "Sketch-aware decoders enable the model to account for template-induced continuations and therefore plan ahead; BEAMVAR in particular tends to perform breadth-first exploration and reliably finds near-shortest paths, whereas ARGMAX behaves like greedy depth-first search and is less efficient and sometimes fails; the ACTION constraint prevents invalid moves and helps reliable environment interaction.",
            "ablation_comparison": "ARGMAX (greedy) vs BEAMVAR/VAR: BEAMVAR improved both success rate and average steps (e.g., text-davinci-003 success 9/10 → 10/10 and steps ~3.77 → ~2.4). Smaller model (text-curie-001) benefits from BEAMVAR much more than ARGMAX, indicating decoder choice interacts with model capacity.",
            "limitations": "Small-scale dungeons only (10 samples); metrics averaged over small N; environment is synthetic and low-dimensional (8–10 nodes); no generalization to larger or continuous spatial tasks shown; computational cost overhead for sketch-aware decoders; dependence on model size — smaller models still fail or perform poorly.",
            "uuid": "e6536.1",
            "source_info": {
                "paper_title": "Prompt Sketching for Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "PALM-E: An embodied multimodal language model",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Language Model Cascades",
            "rating": 1,
            "sanitized_title": "language_model_cascades"
        },
        {
            "paper_title": "Prompting is programming: A query language for large language models",
            "rating": 2,
            "sanitized_title": "prompting_is_programming_a_query_language_for_large_language_models"
        }
    ],
    "cost": 0.01178275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PROMPT SKETCHING FOR LARGE LANGUAGE MODELS</p>
<p>Luca Beurer-Kellner luca.beurer-kellner@inf.ethz.ch 
Department of Computer Science
ETH Zurich
Switzerland</p>
<p>Mark Niklas Müller mark.mueller@inf.ethz.ch 
Department of Computer Science
ETH Zurich
Switzerland</p>
<p>Marc Fischer marc.fischer@inf.ethz.ch 
Department of Computer Science
ETH Zurich
Switzerland</p>
<p>Martin Vechev martin.vechev@inf.ethz.ch 
Department of Computer Science
ETH Zurich
Switzerland</p>
<p>PROMPT SKETCHING FOR LARGE LANGUAGE MODELS
B2F424E1ACAE12F70D265C1CBE8EC35A
Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -first to produce intermediate results and then the final answer.However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses.In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template.This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results.The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference.Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering.To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders.</p>
<p>INTRODUCTION</p>
<p>While early prompting strategies for large language models (LLMs) focused on simple trigger phrases to elicit the desired responses (Kojima et al., 2022), more recent work considers conversational (Ouyang et al., 2022), multi-part, and template-guided LLM interactions, where a model is queried several times in a constrained way, based on a template or grammar.This offers control over LLM reasoning by filling in a template of pre-defined steps (Beurer-Kellner et al., 2023;Lundberg and Ribeiro), allows interfacing with automated software systems, and enables syntactically reliable code generation (Poesia et al., 2022).</p>
<p>Key Challenge: Decoding with (Hard) Structural Constraints We consider a simple application, illustrated in Figure 1.The goal is to generate a list of items, satisfying two hard requirements:</p>
<p>(1) the result should be a dashed list of exactly four items and (2) the second item should be Frisbee.To guarantee that these requirements are satisfied, prompting and fine-tuning alone are insufficient, as unconstrained LLMs remain inherently stochastic, even with good instructions, demonstrations, or training Arora et al. (2023); Zhao et al. (2021).To address this issue, template-guided inference constructs a template from the (hard) constraints, leaving multiple holes for the LLM to fill during the generation (top right, Figure 1).Unfortunately, the naive strategy of calling an unconstrained model (Figure 1, left) for each placeholder fails frequently, as the model runs on, beyond the template, generating many items per placeholder.A practical alternative is stop-and-go inference (middle): By feeding the template incrementally, item-by-item, and enforcing stopping-conditions for each call, we can force the overall output to adhere to the template.While this method is effective for output formatting (Beurer-Kellner et al., 2023;Lundberg and Ribeiro), the model remains unaware of the overall template when decoding each placeholder, leading to sub-optimal reasoning strategies.For instance, in Figure 1, stop-and-go generates Frisbee as the first item, leading to a repetition of the word, which would otherwise be unlikely under the model's distribution.This example raises two important questions: (1) Can we improve on naive stop-and-go inference by anticipating the overall template during generation?And, (2) what is the general effect of this form of templated generation on overall model performance, i.e., does it impair or improve the model's reasoning capabilities? 1 arXiv:2311.04954v1[cs.CL] 8 Nov 2023</p>
<p>Prompt: A list of single-word, fun things to bring to a trip.</p>
<p>(Hard) Specification: A list of exactly four items, with 'Frisbee' as second element, such that the resulting output is guaranteed to be a parsable list.</p>
<p>Template: -[ITEM] -Frisbee -[ITEM] -[ITEM]</p>
<p>Unconstrained Inference Figure 1: Prompt Sketching is a novel inference method for template-guided text generation with LLMs.In comparison to standard inference and sequential stop-and-go inference, prompt sketching optimizes overall template likelihood, prevents repetitions, and adheres to the template structure.Output generated by the model is highlighted, and enforced stopping phrases are indicated as ⬣.</p>
<p>This Work: Prompt Sketching To answer these questions, we present prompt sketching, a novel framework for template-guided LLM inference.The key technical difference of sketching in contrast to prior techniques is that we phrase the entire template as one segmented sequence decoding problem, rather than multiple isolated model calls.This, (1) theoretically anchors standard stop-and-go inference as a special case and (2) allows us to generalize and implement novel, sketchaware decoding procedures based on beam search, that optimize templates end-to-end.Figure 1 compares sketch-aware decoding (right) with unconstrained inference (left) and stop-and-go (middle).Sketching allows us to adhere to the provided prompt template, while also optimizing multiple placeholder variables jointly, in this case, avoiding a repetition of Frisbee.We carry out an extensive experimental evaluation, showing that sketching outperforms non-templated prompting methods like chain-of-thought on 7/8 LLM reasoning tasks, demonstrating the effectiveness of template-guided inference in general reasoning.In our experiments, sketching allows us to consistently enforce reasoning strategies across all tasks, enabling a more controlled form of LLM programming going beyond simple prompting.For 5/8 tasks, we even observe significant improvements over simple stop-and-go templating, demonstrating that sketch-aware decoding and joint optimization of multiple variables are crucial components of effective template-guided LLM inference.</p>
<p>Main Contributions Our core contributions are:</p>
<p>• A framework of prompt sketching, phrasing multi-step and template-guided LLM inference as a segmented sequence decoding problem.• Two novel sketch-aware decoding procedures, transferring several insights from constrained sequence decoding to general template-guided inference.• A collection of ready-to-use, generic prompt sketches that work well with a number of hard LLM reasoning tasks and can be easily adapted.• An extensive evaluation of sketching, including a comparison to non-templated and stopand-go inference, as well as a comparison of several (sketch-aware) decoding strategies.</p>
<p>Additionally, we publish an open source library dclib, included in the supplementary material (see App. D), which allows the implementation of sketch-aware decoding procedures on top of OpenAI, transformers, and llama.cppmodels, thus facilitating future research and open source work.</p>
<p>BACKGROUND</p>
<p>We first provide relevant background on prompting and decoding, before discussing prompt sketching.</p>
<p>Decoding Most recent language models operate left-to-right only, i.e., they predict a probability distribution p(y t |y &lt;t , x) over the next token y t given an input sequence x = ⟨x 1 , x 2 , ..., x n ⟩ and previously predicted tokens y &lt;t = ⟨y 1 , y 2 , ..., y t ⟩.Thus, a core task is to transduce or decode a model output y ⋆ that maximizes some scoring function:
y ⋆ = arg max y∈Y score(y, x).(1)
A popular choice for this scoring function is the posterior or joint probability assigned to the decoded sequence by the language model.This leads to the so-called maximum a posteriori (MAP) solution:
y MAP := arg max y∈Y p(y|x) = arg max y∈Y Π N t=1 p(y t |y &lt;t , x) = arg max y∈Y Σ N t=1 log p(y t |y &lt;t , x) (2)
However, solving the MAP decoding exactly is generally intractable, as it requires all conditional probabilities p(y t |y &lt;t , x) over an exponentially large search space to be evaluated.To solve this problem, a range of decoding strategies have been introduced, which aim to find approximate solutions.To discuss them, it is helpful to imagine Y as a tree with the prompt or prefix x at the root and children of a node corresponding to possible continuations, all scored by score(y &lt;t , x).</p>
<p>ARGMAX Decoding corresponds to a depth-first search of our decoding tree that terminates once the first solution has been found.Operationally, at every decoding step, we extend our hypothesis y &lt;(t−1) by choosing the next token y t to maximize score(y t | y &lt;(t−1) , x):
y ARGMAX := N t=1 arg max yt∈Y p(y t |y &lt;t , x)(3)
where ⊕ denotes concatenation.ARGMAX decoding is efficient, but will also disregard many alternative hypotheses due to its greedy nature.</p>
<p>Beam Search corresponds to a breadth-first search in the decoding tree where the breadth (at every tree depth) is limited to the beam width n.Operationally, we first determine the n best continuations of all of our n hypotheses and then retain the n best ones across all these n 2 continuations.This yields high-quality solutions at moderate computational cost, making Beam Search popular across a wide range of tasks.Interestingly, thus obtained solutions often outperform exact decodings (or very large beam widths) in down-stream tasks (Holtzman et al., 2020).Meister et al. (2020) suggest that this is due to beam search inducing a regularization towards uniform information density, which is preferred in human speech.</p>
<p>Grid Beam Search (Hokamp and Liu, 2017) extends beam search to facilitate constraint decoding, i.e., transducing a response such that it contains certain strings or satisfies constraints.As sequences complying with such constraints typically achieve a much lower score than natural model predictions, they would never be included using vanilla beam search.Grid beam search solves this problem by introducing separate comparison pools for hypotheses satisfying different numbers of constraints.</p>
<p>To avoid a linear increase in beam width and thus computational cost in the number of constraints, Post and Vilar (2018) introduce a dynamic beam allocation scheme that keeps the total beam width constant and assigns slots on this beam depending on the number of satisfied constraints.</p>
<p>Length Normalization (Wu et al., 2016) is frequently employed to compare sequences of different lengths, to compensate for the summation of additional negative logprobs.We can simply weight our scoring function with a length normalization term, parametrized by β ∈ R ≥0 and α ∈ [0, 1]:
w = (β + 1) α (β + |y|) α ,(4)
where β = 0 and α = 1 recovers the mean and α = 0 no normalization.</p>
<p>PROMPT SKETCHING</p>
<p>The core of prompt sketching is template-guided LLM inference, i.e., alternating model output with template-derived intermediate tokens.This is different from sequential prompting methods like chain-of-thought or answer-only, where first, the model consumes an input such as a question or instructions and then generates an answer in an unconstrained way.More formally, we consider a sketch S to be a template of the form  (Yao et al., 2022a), language model programming (Beurer-Kellner et al., 2023), and language model cascades (Dohan et al., 2022).
S := "<p1> [v2] . . . <p k−2 > [v k−1 ] <p k >"
Autoregressive Sketch Decoding Sketching extends the range of decoding strategies beyond just sequential generation.However, most language models are still simple next-token predictors, i.e., given some prompt x, they generate a sequence of tokens y autoregressively, that is, one token at a time, conditioned only on the previously generated tokens:
p(y|x) = |y| i=1 p(y i |x, y &lt;i )(5)
To align this with sketching, we split the generated sequence y = {y 1 , . . ., y n }, including both deterministic and variable portions, into k consecutive chunks C y = {c 1 , . . ., c k } of lengths n 1 , . . ., n k respectively, i.e., C y = {y 1 , . . ., y n1 }, . . ., {y n (k−1) +1 , . . ., y n k } .Each chunk in C y is then associated either with a deterministic prompt part pi or a model-predicted variable vi.The overall joint probability of all chunks is then defined as
p(c 1 , . . . , c k ) = k j=1 nj i=nj−1+1 p(y i |y &lt;i )(6)
Crucially, we derive the values of all chunks from a single sequence of tokens y, which can be predicted sequentially using an autoregressive model.A chunk-and variable-partitioned sequence can then be leveraged by decoding algorithms to obtain higher-quality responses or inject deterministic phrases during generation.The main challenge of this approach is the chunking strategy, i.e., a way to split a generated sequence of tokens y into chunks of an a priori unknown length to determine which variables they should be associated with.</p>
<p>Chunking with Stopping Phrases Like in stop-and-go inference, sketching relies on the use of per-variable stopping phrases (SPs).SPs are specified as part of a prompt sketch and terminate the generation of the current chunk i on occurence.This allows us to chunk the output sequence y, assigning the resulting subsequences to variables vi, and keep the model from running-on without respecting the sketch template.In case no specified stopping phrase occurs before the model predicts its designated end-of-sequence token, we do not terminate the entire generation process, but only the decoding of the current chunk, unless no further chunks remain.</p>
<p>Deterministic and Constrained Chunks</p>
<p>To inject deterministic phrases during generation, we force a predetermined sequence pi to be decoded, while still evaluating its likelihood p(c i |c &lt;i ).Further, we consider constrained variables as a special case of non-deterministic variables, whose values are predicted by the model, but can only be chosen from a restricted set of sequences (e.g., only numbers, matching a regular expression, etc.).To implement constrained variables, we rely on the LMQL query language for LLMs (Beurer-Kellner et al., 2023).This allows us to mask out all tokens that will not satisfy a given constraint during generation, such that the resulting value of some restricted variable c i is guaranteed to satisfy the constraint.To address this, we leverage the probabilistic understanding of sketching and propose a novel class of decoding procedures that, in contrast to traditional token-level decoders, operate on the level of the template to guide the decoding process end-to-end.Concretely, we experiment with two novel decoder adaptations, namely: (1) Hierarchical Variable-Level Beam Search (VAR) and (2) Grid-Based Beam Search (BEAMVAR).Next, we discuss the implementation of these methods in more detail.</p>
<p>VAR: Variable-Level Beam Search is based on the idea of applying beam search on the level of the decoded placeholder variables.This means that instead of extending each active hypothesis by the n most likely next tokens, we extend it by n sampled values for the currently decoded variable.</p>
<p>Starting with an empty sequence of tokens, we decode variable by variable.When at variable v i , we have at most n hypotheses for which the variables v &lt;i have been chosen.For each of them, we then generate n proposals for variable v i , thus giving us n 2 hypotheses over the variables v ≤i .Among these, we then select the n most likely ones according to the model score and move to the next variable.Deterministic chunks are handled by appending them to the set of active hypotheses all at once.This process is repeated until all variables have been decoded.See App.A, for a pseudo-code implementation of VAR.</p>
<p>BEAMVAR: Variable-Grid Beam Search is based on the idea that the number of decoded variables is an important measure of decoding progress and should thus be considered when comparing the scores of different sequences during token-level beam search, to decide which to explore further.This is particularly important in the presence of deterministic chunks, which, by their very nature, typically have lower likelihoods under the model distribution than non-deterministic variables and would thus never be included in a decoded hypothesis.To this end, we adapt the dynamic beam allocation method of Post and Vilar (2018) to the sketching setting and propose Variable-Grid Beam Search (BEAMVAR): We partition our beam width into separate pools depending on the currently decoded variable vi and only compare scores per pool.To decide how many slots to allocate to each pool and thus variable, we divide the beam width by the number of unique, currently decoded variables and allocate the remainder to the pool with the most decoded variables, reassigning unused slots to pools decoding later variables, to ensure progress at the template-level.A pseudo-code implementation of BEAMVAR can be found in App. A.</p>
<p>EXPERIMENTAL EVALUATION</p>
<p>We focus our evaluation on the following questions: (1) Is templated-guided inference and sketching effective at improving the performance of LLMs on reasoning tasks?(2) Can sketch-aware decoders outperform existing decoders in and outside of the sketching setting?And ( 3), what kind of tasks benefit the most from sketching?To answer these questions, we compare model performance with non-templated, sequential inference on a wide range of different reasoning benchmarks for LLMs (Section 4.1) and also experiment with novel applications enabled by prompt sketching (Section 4.2).Baselines As a baseline, we compare sketching to nontemplated zero-shot formulations of answer-only (AO) and chain-of-thought (CoT ), using zero-shot CoT (Kojima et al., 2022) for the latter.Examples of all used prompts/sketches are given in App.E. During generation, no task demonstrations are provided and the model is prompted with simple instructions only.This highlights a core benefit of sketching: the ability to precisely guide the model during generation without concrete demonstrations.Still, we also include a comparison with few-shot prompting in App.C.1, which is generally orthogonal to sketching.</p>
<p>Datasets and Sketches</p>
<p>We evaluate on a total of 8 LLM reasoning tasks.For each task, we apply one of two generic sketch templates: For arithmetic and logical reasoning, date understanding, and general question answering, we rely on a sketched form of chain-of-thought, as shown in Figure 3.</p>
<p>For state tracking and matrix shape reasoning, we employ an interleaved reasoning sketch, as shown in Figure 2, splitting task descriptions into sentences and interleaving them with the model's reasoning steps.For a detailed description of the tasks and sketches, we refer to App.E.</p>
<p>Compute and Dataset Size</p>
<p>The total costs of our OpenAI experiments are roughly $4,000 USD in API use.To limit these costs for our OpenAI experiments specifically, we evaluate only 100 uniformly random samples per task-decoder configuration, with confidence bounds reported in App.C.3.For Llama-2, on the other hand, we run all of our experiments on 1000 samples per task (or the full datasets), using a single NVIDIA H100 GPU with 80GB memory.</p>
<p>Decoder Configurations As a baseline for our sketch-aware decoding procedures, we compare with ARGMAX and traditional beam search (BEAM), applied to each sketch variable individually.</p>
<p>Based on this, we examine the benefit of sketching with and without our sketch-aware decoders VAR and BEAMVAR.For BEAM, VAR, and BEAMVAR we use a beam width of n = 2 and rely on length normalized scoring in line with previous work (Wu et al., 2016), using β = 0 and α = 0.7.</p>
<p>TASK ACCURACY</p>
<p>In Tables 1 and 2, we report our main results on task performance with text-davinci-003 and Llama-2, respectively.Considering only ARGMAX decoding, we consistently observe improved or maintained performance with sketching, as compared to sequential CoT or AO (7 out of 8 improved for text-davinci-003, 6 out of 8 with Llama-2).This shows, that simple ARGMAX sketching can already be effective (up to 4% and 8% points improvement for text-davinci-003 and Llama-2 respectively).Manual inspection reveals that sketching consistently results in clearly structured reasoning, while with CoT the model makes a seemingly random choice about the form of reasoning applied to each sample (simple text, a list of steps, etc.), impairing task accuracy (see App. E for detailed examples).</p>
<p>Llama-2 largely confirms our results for text-davinci-003.Two outliers are the matrix shapes task and the AQuA dataset Ling et al. (2017).For both, Llama-2 exhibits very bad performance across all decoding and prompting strategies, suggesting that the model is likely unable to perform these tasks at all.We attribute this to the difference in model size when compared to OpenAI.text-davinci-003 has 175 billion parameters, while the Llama-2 variant only has 13 billion parameters.As shown by Kojima et al. (2022), model size directly relates to an increase in general reasoning capabilities.</p>
<p>Decoders Combining simple sketches with sketch-aware decoding, we observe even stronger performance gains of up to 10% points, e.g., for BEAMVAR compared to sequential prompting with ARGMAX or BEAM on the question answering datasets AQuA (Ling et al., 2017) and StrategyQA (Geva et al., 2021) with text-davinci-003.Comparing VAR and BEAMVAR, we observe VAR to perform particularly well on tasks that rely on interleaved reasoning while BEAMVAR is more effective in other settings.For Llama-2, we observe analogous effects, e.g., BEAMVAR improves performance on Date Understanding and GSM8K by almost 7% points, compared to non-templated CoT and simple ARGMAX.</p>
<p>For text-davinci-003, we also observe notable performance gains of up to 6% points, when using our sketch-aware decoders in combination with the established Zero-Shot CoT (Kojima et al., 2022) prompting scheme (cf.Table 1).This is because Zero-Shot CoT already is a two-part prompting scheme, which naturally benefits from our sketch-aware decoders, letting them optimize over the reasoning process (first variable) and final answer (second variable) jointly.In addition to reasoning performance, sketching also enables novel applications, for which non-templated sequential inference either fails completely or is much less effective and reliable.We highlight multiple scenarios here (causal reordering, sudoku, interactive environments) and expand on them in App.B (JSON generation and graph traversal).</p>
<p>NOVEL APPLICATIONS ENABLED BY PROMPT SKETCHING</p>
<p>Causal Reordering First, we investigate forward referencing abilities with our sketch-aware decoders.More specifically, we examine whether sketch-aware decoders enable the model to anticipate future information to some degree.For this, we adapt the existing Information Essentiality dataset (Srivastava et al., 2022), by reordering it according to the template shown in Figure 4.The model has to determine the essentiality of two statements <S1> and <S2>, with respect to a given question <Q>.However, in our reordered prompt, the result variable IS _ NEEDED1 is decoded before the <Q> is shown.For this custom task (cf.Table 1), we indeed observe that ARGMAX is incapable of producing any meaningful results (0.01 accuracy), whereas, BEAMVAR and VAR achieve an improved accuracy of 0.25 and 0.06 respectively, by exploring a wider hypotheses space.</p>
<p>Sudoku We further examine the capabilities of a model to solve simple 3 × 3 sudoku-like puzzles: the LLM is tasked to complete a partial grid with unique numbers in 1 − 9. Similar to before, this task requires forward referencing to effectively choose the correct numbers.As shown in Table 3, out of 10 puzzles with 1 − 6 empty spots, sequential ARGMAX decoding is only capable of solving one.This is expected, as greedy decoding does not allow to anticipate any future information (i.e.fixed numbers), before choosing earlier ones.In contrast, BEAMVAR and VAR solve 6/10 and 7/10 puzzles respectively, demonstrating again that they explore a wider hypotheses space.A potential alternative is to re-order the template, which allows text-davinci-003 to achieve perfect accuracy with ARGMAX, but re-ordering is not always an option with more complex multi-step puzzles.</p>
<p>Interactive Environments Sketch-aware decoders can take the effect of template-induced continuations into account during text generation.If we choose these continuations dynamically based on previous model output, we can effectively leverage them to explore interactive environments (Driess et al., 2023).For this, we implement a simple LLM-guided graph traversal, where an LLM agent traverses a dungeon, starting out in a randomly chosen room, with the goal of finding the exit.We generate 10 random dungeons with 8 − 10 rooms each, where the average shortest exit route is 2.3 steps away.At each node, the model is asked for the next room/node to traverse to.As shown in Table 3, ARGMAX mostly finds the exit, but often requires a lot more steps (average of 3.77) than VAR/BEAMVAR.In particular BEAMVAR always finds the exit and almost always via the shortest route (∼ 2.4 req.steps).For text-curie-001, we observe similar performance with BEAMVAR, whereas ARGMAX is often incapable of finding the exit within the limit of 10 steps (only 5/10 successes).We refer to App.B.2 for more details on the sketch formulation and the interactive environment.</p>
<p>DISCUSSION</p>
<p>Our evaluation shows that sketching and, by extension, template-guided LLM inference in general, can significantly improve model reasoning capabilities.Here, we briefly discuss limitations and other considerations relating to design, compuational, and applicability aspects.</p>
<p>Sketch Design and Iteration While still sensitive to wording, prompt sketching does offer more control over exact model behavior, thereby addressing some of the difficulties of traditional prompt design (Reynolds and McDonell, 2021;Arora et al., 2023;Zhao et al., 2021).However, sketching is also not a silver bullet: Most importantly, we find that an effective sketch must not be too restrictive to not impair model performance.Still, as substantiated by our results, even simple sketches can already be effective at improving reasoning capabilities.Lastly, much like non-templated prompts, sketches still require iterative development and tuning to achieve optimal performance on a given task.More importantly, however, they offer benefits such as improved control, a guaranteed output format, and reduced free-text formatting instructions, otherwise needed.</p>
<p>Applicability While sketch design still requires some effort, we find that many tasks in our evaluation can be solved with a small set of generic sketches.For instance, we find that a sketched form of chain-of-thought (Wei et al., 2022a) (see Figure 3) is already effective for a wide range of tasks, including arithmetic reasoning and general question answering.For direct adoption, we also publish the sketches used in our evaluation, which can be adapted or used as-is by practitioners.</p>
<p>Computational Overhead Of Sketch-Aware Decoding Sketch-aware decoders naturally incur computational overhead compared to simple, greedy search.While BEAMVAR requires as much compute as regular beam search, VAR requires an additional factor of beam width n more hypotheses to be tracked in parallel.Similar to regular beam search, this is a well-known trade-off: branching decoders are more expensive but still widely used, especially when improved performance and diversity are important.</p>
<p>RELATED WORK</p>
<p>Prompting Recent works have proposed a variety of different prompting techniques including chain-of-thought prompting (Wei et al., 2022a;b), interactive question answering (Yao et al., 2022b), self-consistency (Wang et al., 2022), and ThinkSum (Ozturkler et al., 2022).These prompt programming techniques (Reynolds and McDonell, 2021;Zhou et al., 2022), aim to leverage the general reasoning abilities of LLMs to solve diverse tasks.To enable the efficient implementation of such complex prompting techniques, LM-focused programming systems have recently been introduced: PromptChainer (Wu et al., 2022), PromptSource (Bach et al., 2022), andLMQL (Beurer-Kellner et al., 2023) provide development environments for LM interaction.We build on LMQL, as it supports variable constraints and control flow within prompts, enabling the efficient representation of sketches.Finally, language model cascades (Dohan et al., 2022) view LM querying as probabilistic programming over multiple variables, thus implicitly assuming a sketching setting and opening up interesting perspectives for more advanced decoders in the future.In contrast to prompt sketching, however, existing works compose multiple LLM calls in a disconnected manner, and, crucially, do not consider the overall likelihood of the resulting sequence.</p>
<p>Language Model Decoding Most decoding techniques either aim to approximately recover the maximum a posteriori solution under the model distribution or sample from it with the aim to increase diversity.Beyond direct sampling from the model distribution, Nucleus Sampling (Holtzman et al., 2020) clips away the tail of the distribution and Locally Typical Sampling (Meister et al., 2022) considers a subset that yields uniform information density sequences.While ARGMAX can be seen as a best-first search of the decoding tree with a maximum width of 1, Beam Search can be seen as a width-first search with a width constrained to k (often 5) trajectories.Best First Beam Search (Meister et al., 2020) combines the two ideas, always exploring the sequence with the largest score while maintaining the width limit, to increase efficiency.Best-k Search (Xu et al., 2022a) drops the width restriction and always explores the k highest scoring sequences.Lattice decoding (Xu et al., 2022b) allows for the recombination of similar trajectories, leading to more diverse solutions.Diverse Beam Search (Vijayakumar et al., 2016) includes a diversity objective directly in the scoring function of Beam Search.To improve performance on constraint decoding problems, Grid Beam Search (Hokamp and Liu, 2017) creates separate beams for sequences satisfying a different number of constraints.Post and Vilar (2018) propose Dynamic Beam Allocation to instead partition a fixed beam width into pools depending on the number of satisfied constraints, with Hu et al. ( 2019) introducing a vectorized implementation.</p>
<p>CONCLUSION</p>
<p>We presented prompt sketching, a novel framework for template-guided LLM inference that phrases templated generation as a segmented sequence decoding problem.This perspective unlocks novel sketch-aware decoding procedures that optimize for overall template likelihood and not just sequentially generate text.Our experiments show that sketching outperforms naive templating as well as sequential prompting like chain-of-thought on 7 out of 8 hard LLM reasoning tasks, improving task accuracy by up to 10% points.Looking forward, we also show how sketching enables novel applications such as reliable output formatting, forward references in reasoning, and LLM-guided graph traversal, inspiring future work in this direction.</p>
<p>A DECODING ALGORITHMS</p>
<p>Variable-Level Beam Search VAR The pseudo-code implementation of VAR is given in Algorithm 2. The function expand det , expands a given sequence by a deterministic chunk if the next chunk in the prompt template is not a variable.The function expand sample , expands a given sequence by sampling n different continuations for the next variable value in the prompt template.Lastly, top n selects the n best sequences from a given set of sequences, according to the the length normalized beam search score as discussed in Section 2. In practice, an additional early stopping criterion on done is employed.</p>
<p>Algorithm 1 Variable-Level Beam Search (VAR)</p>
<p>Require: Input n: number of beams, V set of variables Ensure: set of n VAR-best hypotheses done
1: h ← {<bos>}, h ′ ← {} 2: for v i ∈ V do 3: h ← expand det (h) 4:
for s ∈ h do 5:
h ′ ← h ′ + expand sample (s, n) 6:
end for
7: h ← top n (h ′ ) 8:
done, h ← separate_done(h) 9: end for Variable-Grid Beam Search VAR The simplified pseudo-code implementation of BEAMVAR is given in Algorithm 2. The function expand det , expands a given sequence by a deterministic chunk if the next chunk in the prompt template is not a variable.The function expand top , expands a given sequence by the top-n token continuations according to the model distribution.post_vilar determines the dynamic beam size per group according to Post and Vilar (2018), where groups are defined by the currently decoded variable and or deterministic chunk.Lastly, top n selects the n best sequences from a given set of sequences, according to the the length normalized beam search score as discussed in Section 2. In practice, an additional early stopping criterion on done is employed.</p>
<p>Algorithm 2 Variable-Grid Beam Search (BEAMVAR)</p>
<p>Require: Input n: number of beams, N : maximum length, V set of variables Ensure: set of n BEAMVAR-best hypotheses done
1: h ← {<bos>}, h ′ ← {} 2: for i ∈ {1, . . . , N } do 3: h ← expand det (h) 4: h ′ ← {} 5:
{Expand each hypothesis in h by its top-n continuations} 6:</p>
<p>for s ∈ h do 7:
h ′ ← h ′ + expand top (s, n) 8:
end for 9:</p>
<p>h ← {}  One direct application of sketching, is to generate schemaconform JSON objects with LLMs, given only free text as input, as illustrated in Figure 5.Using sketching, we can guarantee the output format 10/10 times with different examples similar to the one shown in Figure 5.This works for both, text-davinci-003 and text-curie-001, regardless of the model (size) used.Without sketching, text-davinci-003 requires detailed descriptions of the output format in the prompt to produce valid JSON at a similar rate.At the same time it may still fail stochastically (no guarantees), and the smaller text-curie-001 is not be able to produce valid JSON at all when just instructed to do so.Further, including detailed data format instructions in non-templated queries in this way, causes a high average inference cost of 179.5 tokens per sample, reduced to 28.7 when using sketching, an 84% reduction in inference cost.</p>
<p>B.2 INTERACTIVE ENVIRONMENTS</p>
<p>As part of our evaluation, we also consider the use of sketching and our sketch-aware decoders in interactive environments.For this we rely on the following interactive sketch program with corresponding constraints on sketch variable ACTION: node = <initialized to start node> steps = 0 max _ steps = 10 "System: You have taken too many steps.You lose.\n" return "failure" return "success" Constraints: ACTION in ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"] Depending on the graph that is being explored, this results in a runtime instantiation of a prompt sketch as shown in Figure 6.The sketch-aware decoder is then used to generate the next action to take, given the current state of the environment.The generated action is then executed in the environment and the process is repeated until the agent reaches the exit or the maximum number of steps is exceeded.Depending on the decoding algorithm, the agent can be made to explore the environment in different ways.For example, ARGMAX will result in a greedy, depth-first search, while VAR and BEAMVAR result in a breadth-first search.+ Tasks extracted from the BIG benchmark suite (Srivastava et al., 2022).* Specifically adapted for our evaluation of forward referencing prompts.</p>
<p>C ADDITIONAL RESULTS</p>
<p>C.1 FEW-SHOT PROMPTING</p>
<p>In addition to evaluating zero-shot performance of sketching, we also evaluate a two-shot setting (two demonstrations) for selected datasets (AQuA, StrategyQA, Tracking Shuffled Objects and Matrix Shapes) and report the results in Table 4.We rely on few-shot samples exhibiting the same reasoning structure as enforced by the respective sketches.For comparison, we also include the best zero-shot result from the paper.</p>
<p>While we observe a slight increase in performance for the question-answering tasks, performance for the interleaved reasoning tasks is surprisingly decreased in the few-shot setting.In all considered settings, sketching outperforms CoT.In fact, zero-shot sketching with the best decoder even outperforms few-shot CoT with argmax decoding in all settings.Upon manual inspection, we observe that the LLM generally follows the demonstrated reasoning process.However, for Tracking Shuffled Objects, the added demonstrations seem to impair task performance, possibly because the model is confused by the unrelated extra information.Overall, the results of this ablation study suggest that zero-shot sketching with the right decoder may be able to replace few-shot demonstrations by enforcing a given reasoning structure via intermediate instructions and task decomposition.At the same time, note that sketching is much more cost-effective, as few-shot samples increase overall sequence lengths, and thus API cost when using OpenAI models or computational cost (scaling quadratically with sequence length) when using local models.</p>
<p>C.2 PROMPT SKETCHING WITH SMALLER OPENAI MODELS</p>
<p>We also examine the use of smaller OpenAI models.However, the strong zero-shot performance we rely on has previously (for CoT) only been observed in models of sufficient size (150B+ parameters (Kojima et al., 2022)).Nonetheless, we also run our evaluation to the smaller InstructGPT (Ouyang et al., 2022) model text-curie-001 (1 level below text-davinci-00x, 6.7 billion parameters).For comparison, we also include the best results for text-davinci-003, as reported in the paper in Table 5.</p>
<p>Overall, we observe almost no reasoning capabilities, with scores close to random guessing for multiple-choice tasks, and close to zero for open-ended questions like Matrix Shapes.As noted above, this is in line with previous results (Kojima et al., 2022).On the hand, as our main evaluation demonstrates, the slightly larger and more recent Llama-2 Chat 13B Touvron et al. (2023) (13 billion parameters), does clearly benefit from sketching.We attribute this to the concrete set of reasoning tasks in consideration, which are comparatively challenging, requiring a high degree of reasoning and abstraction capabilities, not present in smaller models.</p>
<p>D USING dclib AND SKETCH-AWARE DECODERS</p>
<p>In the supplementary material, we provide the Python library dclib, that contains implementations for all compared (sketch-aware) decoding algorithms.</p>
<p>To install and use dclib, please consult the README.mdfile in the supplementary material.</p>
<p>Once installed, you can use an adapted version of the lmql playground as shown in Figure 7 to interactively play with the different decoding algorithms and their parameters.</p>
<p>E FULL PROMPTS</p>
<p>Here we list full exemplary prompts, per task and prompting method as used in our evaluation.We rely on the same notation of sketches as in the main body of the paper.For control-flow (e.g.loops and conditions) and constraints, we rely on the semantics of the LMQL query language for LMs, and refer to Beurer-Kellner et al.</p>
<p>(2023) for a more thorough introduction.Sept. 1st, 2021 was a week ago, so 10 days ago would be 8 days before that, which would be August 23rd, 2021.</p>
<p>Task</p>
<p>Therefore, the answer is (A) 08/23/2021.Therefore, among A through F, the answer is A. ✓</p>
<p>Task</p>
<p>Prompting Method Query and Response</p>
<p>Information Essentiality</p>
<p>Forward Referencing Query</p>
<p>"Cole is on the varsity basketball team.Therefore, the statement that Cole is under 7 feet tall[var _ 1] to answer the question: Is Cole over 6 feet tall?However, the statement that Every player on Cole's varsity basketball team is above 6'1'' [var _ 2].Therefore [ answer]"</p>
<p>Constraints var _ 1 in [" is sufficient", " is insufficient", " is not needed"] and var _ 2 in [" would also be sufficient", " would alone be insufficient", " in addition is sufficient", " is sufficient" , " is insufficient", " is still insufficient", " is also not needed" ] and answer in ["the first statement alone is sufficient while the second statement alone is insufficient", "the second statement alone is sufficient while the first statement alone is insufficient", "either the first or the second statement is sufficient", "the first and second statement taken together are sufficient", "the question can not be answered with the provided statements", "the question can be answered without either statement"] Model Response Cole is on the varsity basketball team.Therefore, the statement that Cole is under 7 feet tall is not needed to answer the question: Is Cole over 6 feet tall?However, the statement that Every player on Cole's varsity basketball team is above 6'1" is also not needed.Therefore the question can be answered without either statement ✗</p>
<p>Task Prompting Method</p>
<p>Query and Response AQUA@ao Answer-Only Query</p>
<p>"""Q: What is the greatest number of identical bouquets that can be made out of 28 white and 98 red tulips if no flowers are to be left out?(Two bouquets are identical whenever the number of red tulips in the two bouquets is equal and the number of white tulips in the two bouquets is equal.)\n""""Answer Choices: (A) 4 (B) 7 (C) 10 (D) 14 (E) 21\n" "A: Among A through E, the answer is[answer]"</p>
<p>Model Response Q: What is the greatest number of identical bouquets that can be made out of 28 white and 98 red tulips if no flowers are to be left out?(Two bouquets are identical whenever the number of red tulips in the two bouquets is equal and the number of white tulips in the two bouquets is equal.)Answer Choices: (A) 4 (B) 7 (C) 10 (D) 14 (E) 21 A: Among A through E, the answer is D, 14. ✓ AQUA@cot Chain-Of-Thought Query</p>
<p>"""Q: What is the greatest number of identical bouquets that can be made out of 28 white and 98 red tulips if no flowers are to be left out?(Two bouquets are identical whenever the number of red tulips in the two bouquets is equal and the number of white tulips in the two bouquets is equal.)\n""""Answer Choices: (A) 4 (B) 7 (C) 10 (D) 14 (E) 21\n" "A: Let's think step by step.</p>
<p>[COT] Therefore, among A through E, the answer is[answer]"</p>
<p>Constraints STOPS _ AT(answer, ".")</p>
<p>Model Response Q: What is the greatest number of identical bouquets that can be made out of 28 white and 98 red tulips if no flowers are to be left out?(Two bouquets are identical whenever the number of red tulips in the two bouquets is equal and the number of white tulips in the two bouquets is equal.)Answer Choices: (A) 4 (B) 7 (C) 10 (D) 14 (E) 21 A: Let's think step by step.</p>
<p>We know that we have 28 white tulips and 98 red tulips.We also know that two bouquets are identical when the number of red tulips and white tulips are equal.Therefore, we can make the greatest number of identical bouquets by making sure that each bouquet has the same number of red and white tulips.Model Response Q: What is the greatest number of identical bouquets that can be made out of 28 white and 98 red tulips if no flowers are to be left out?(Two bouquets are identical whenever the number of red tulips in the two bouquets is equal and the number of white tulips in the two bouquets is equal.)Answer Choices: (A) 4 (B) 7 (C) 10 (D) 14 (E) 21 A: Let's think step by step.</p>
<p>-We have 28 white and 98 red tulips.</p>
<p>-We need to make bouquets with the same number of white and red tulips in each bouquet.</p>
<p>-The greatest number of identical bouquets that can be made is 14.</p>
<p>-This is because we can make 7 bouquets with 4 white and 14 red tulips each, and 7 bouquets with 14 white and 4 red tulips each.</p>
<p>-Therefore, the answer is (D) 14.Overall this means, 28 white and 98 red tulips can make 14 identical bouquets."Multiply a matrix of shape (3,3,2) with a matrix of shape (3,2,4).Subtract the result from a matrix of shape (3,3,4).Compute the hadamard product of the result with a matrix of shape (3,3,4).Multiply the result with a matrix of shape (3,4,3).The shape of the overall result is[answer]"</p>
<p>Constraints STOPS _ AT(answer, ")")</p>
<p>Model Response Multiply a matrix of shape (3,3,2) with a matrix of shape (3,2,4).Subtract the result from a matrix of shape (3,3,4).</p>
<p>Compute the hadamard product of the result with a matrix of shape (3,3,4).Multiply the result with a matrix of shape (3,4,3).The of the overall result is: (3, 3, 3) ✓ matrixshapes@cot Chain-Of-Thought Query "Multiply a matrix of shape (3,3,2) with a matrix of shape (3,2,4).Subtract the result from a matrix of shape (3,3,4).Compute the hadamard product of the result with a matrix of shape (3,3,4).Multiply the result with a matrix of shape (3,4,3).To determine the shape of the result, let's think step-by-step:[answer] "</p>
<p>Model Response Multiply a matrix of shape (3,3,2) with a matrix of shape (3,2,4).Subtract the result from a matrix of shape (3,3,4).</p>
<p>Compute the hadamard product of the result with a matrix of shape (3,3,4).Multiply the result with a matrix of shape (3,4,3).To determine the shape of the result, let's think step-by-step:</p>
<p>Step 1: Multiply a matrix of shape (3,3,2) with a matrix of shape (3,2,4).The result of this multiplication will be a matrix of shape (3,3,4).</p>
<p>Step 2: Subtract the result from a matrix of shape (3,3,4).The result of this subtraction will be a matrix of shape (3,3,4).</p>
<p>Step 3: Compute the hadamard product of the result with a matrix of shape (3,3,4).The result of this hadamard product will be a matrix of shape (3,3,4).</p>
<p>Step 4: Multiply the result with a matrix of shape (3,4,3).The result of this multiplication will be a matrix of shape (3,3,3).✓</p>
<p>Task</p>
<p>Prompting Method Query and Response matrixshapes@multivar Multi-Variable Query "Multiply a matrix of shape (3,3,2) with a matrix of shape (3,2,4).The result is a matrix of shape [var] Subtract the result from a matrix of shape (3,3,4).The result is a matrix of shape [var] Compute the hadamard product of the result with a matrix of shape (3,3,4).The result is a matrix of shape [var] Multiply the result with a matrix of (3,4,3).The result is a matrix of shape [var].The result is a matrix of shape[ answer]"</p>
<p>Constraints not "\n" in var and STOPS _ AT(var, ")") and STOPS _ AT( answer, ")")</p>
<p>Model Response Multiply a matrix of shape (3,3,2) with a matrix of shape (3,2,4).The result is a matrix of shape (3,3,4).Subtract the result from a matrix of shape (3,3,4).The result is a matrix of shape (3,3,4).Compute the hadamard product of the result with a matrix of shape (3,3,4).The result is a matrix of shape (3,3,4).</p>
<p>Multiply the result with a matrix of shape (3,4,3).The result is a matrix of shape (3,3,3)..The result is a matrix of shape (3,3,3).✓</p>
<p>tracking_shuffled_objects@ao</p>
<p>Answer-Only Query</p>
<p>Figure 4 :
4
Figure 4: Information Essentiality prompt with forward references (details in App.E).</p>
<p>Figure 5 :
5
Figure 5: Sketched JSON parsing.Only highlighted text is completed by the model.</p>
<p>Figure 6 :
6
Figure 6: Exploring a graph using prompt sketching.</p>
<p>Figure 7 :
7
Figure 7: The adapted LMQL playground interface extended with dclib support allows users to visualize the underlying decoding trees during sketch decoding.</p>
<p>)))'.A: Let's think step by step.The answer (arabic numerals) is -1490.✗ multiarith2@cot Chain-Of-Thought Query """Q: Evaluate the arithmetic expression '((((-2 * 7 + -6) * (8 -7 -5)) -((6 * -2 + 1) * (-8 * 3 * -1))) + (((9 --7 --7) -(0 + -7 --3)) -((8 * -2 + -6) * (-3 -3 -8))))'.A: Let's think step by step.[COT] Therefore, the answer (arabic numerals) is[answer ]""" Model Response Q: Evaluate the arithmetic expression '((can evaluate the entire expression.((-153) + (-90)) = -243 Therefore, the answer (arabic numerals) is -'s think step by step.\n""" for i in range(10): "({i+1})[THOUGHT]" if not THOUGHT.endswith("\n"):"\n" if "answer" in THOUGHT: break "Overall this means,[CONCLUSION] Therefore, the answer (arabic numerals) is[answer]"""Constraints STOPS _ AT(THOUGHT, "\n") and STOPS _ AT(CONCLUSION, "\n" ) and STOPS _ AT(CONCLUSION, ".") Model Response Q: Evaluate the arithmetic expression '((((-2 * 7 + -6) * (8 -7 -5)) -((6 * -2 + 1) * (-8 * 3 * -1))) + (((9 --7 --7) -(0 + -7 --3)) -((8 * -2 + -6) * (-3 -3 -8))))'.A: Let's think step by step.(1) (-2 * 7 + -6) * (8 -7 -5) = -14 * 0 = 0 (2) 6 * -2 + 1) * (-8 * 3 * -1) = -12 * 24 = -288 (3) 9 --7 --7) -(0 + -7 --3) = 16 --4 = 20 (4) 8 * -2 + -6) * (-3 -3 -8) = -16 * -14 = 224 (5) 0 --288 + 20 -224 = -252 (6) The answer is -252.Overall this means, the expression evaluates to -252.Therefore, the answer (arabic numerals) is -252.</p>
<p>"</p>
<p>""Q: Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange.At the start of the event , they are each holding a present of a different color: Alice has a red present, Bob has a black ball, Claire has a pink ball, Dave has a orange ball, and Eve has a brown present.As the event progresses, pairs of people swap gifts.First, Claire and Alice swap their gifts.Then, Claire and Dave swap their gifts.Then, Alice and Bob swap their gifts.Then, Eve and Dave swap their gifts.Finally, Bob and Claire swap their gifts.At the end of the event, Alice has the[answer]""" Constraints answer in [" red present", " black ball", " pink ball", " orange ball", " brown present"] Model Response Q: Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange.At the start of the event, they are each holding a present of a different color: Alice has a red present, Bob has a black ball, Claire has a pink ball, Dave has a orange ball, and Eve has a brown present.As the event progresses, pairs of people swap gifts.First, Claire and Alice swap their gifts.Then, Claire and Dave swap their gifts.Then, Alice and Bob swap their gifts.Then, Eve and Dave swap their gifts.Finally, Bob and Claire swap their gifts.At the end of the event, Alice has the black ball ✓ Q: Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange.At the start of the event , they are each holding a present of a different color: Alice has a red present, Bob has a black ball, Claire has a pink ball, Dave has a orange ball, and Eve has a brown present.As the event progresses, pairs of people swap gifts.First, Claire and Alice swap their gifts.Then, Claire and Dave swap their gifts.Then, Alice and Bob swap their gifts.Then, Eve and Dave swap their gifts.Finally, Bob and Claire swap their gifts.A: Let's think step by step.[COT] Therefore, At the end of the event, Alice has the [answer].""" Constraints answer in [" red present", " black ball", " pink ball", " orange ball", " brown present"] Model Response Q: Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange.At the start of the event, they are each holding a present of a different color: Alice has a red present, Bob has a black ball, Claire has a pink ball, Dave has a orange ball, and Eve has a brown present.As the event progresses, pairs of people swap gifts.First, Claire and Alice swap their gifts.Then, Claire and Dave swap their gifts.Then, Alice and Bob swap their gifts.Then, Eve and Dave swap their gifts.Finally, Bob and Claire swap their gifts.A: Let's think step by step.Alice has a red present, Bob has a black ball, Claire has a pink ball, Dave has a orange ball, and Eve has a brown present.Claire and Alice swap their gifts, so Alice now has a pink ball and Claire has a red present.Claire and Dave swap their gifts, so Claire now has an orange ball and Dave has a pink ball.Alice and Bob swap their gifts, so Alice now has a black ball and Bob has a red present.Eve and Dave swap their gifts, so Eve now has an orange ball and Dave has a brown present.Finally, Bob and Claire swap their gifts, so Bob now has a pink ball and Claire has a black ball.Therefore, at the end of the event, Alice has a black ball, Bob has a pink ball, Claire has a black ball, Dave has a brown present, and Eve has an orange ball.Therefore, At the end of the event, Alice has the black ball.✓</p>
<p>where, p i are deterministic sequences of tokens, specified by the template, and v i are variables that are completed by the model.This definition captures existing forms of prompting, where e.g.answeronly (AO) can be written as SAO := "<Q> A: [ANSWER]" and chain-of-thought (CoT) prompting as SCoT := "<Q> A: Let's think step by step.[COT].",where<Q> corresponds to a question and the variable COT contains model reasoning as well as the final answer.Single vs. Multi-Variable Sketches We consider S AO and S CoT as sequential, single-variable sketches, as the variable is placed at the end of the template.The model, therefore, first digests all provided information such as a question and reasoning instructions before generating the answer.In contrast, with more general sketches, values for multiple variables can be generated, and deterministic intermediate instructions can be inserted during the generation process.Existing examples of multivariable problems include conversational systems like ChatGPT, agentic prompting like ReAct</p>
<p>Table 1 :
1
text-davinci-003 task accuracy with Answer-Only, Chain-Of-Thought, and Prompt Sketching (ours) using ARGMAX, BEAM, BEAMVAR (ours) and VAR (ours) decoding.Each configuration is evaluated on 100 uniformly sampled instances per task.Best results are bold.
Sequential DecodingSketch-Aware Decoding (ours)TaskAnswer-OnlyChain-Of-Thought PromptingPrompt Sketching (ours)ARGMAXARGMAX BEAMBEAMVAR VAR ARGMAX BEAMVAR VAR BEAMLogical ReasoningDate Understanding +0.480.700.750.750.690.720.730.66-Information Essentiality + <em>-----0.010.250.060.15Question AnsweringAQuA (Ling et al., 2017)0.310.370.370.350.350.400.470.35-StrategyQA (Geva et al., 2021)0.680.710.720.670.670.690.770.66-Arithmetic ReasoningMultistep Arithmetic +0.200.430.440.490.440.450.480.38-GSM8K (Cobbe et al., 2021)0.080.560.580.640.570.570.530.59-Interleaved ReasoningTracking Shuffled Objects +0.190.620.470.520.620.640.620.66-Matrix Shapes +0.610.770.770.710.760.810.790.85-
(Srivastava et al., 2022)e BIG benchmark suite(Srivastava et al., 2022).</em>Specificallyadapted for our evaluation of forward referencing prompts.Sketched Chain-Of-Thought"Q: <question>" "Answer Choices: (A)...(B)..." "A: Let's think step by step." for i in range(12): "-[THOUGHT]" if not THOUGHT.endswith("\n"):"\n" if "answer" in THOUGHT: break "Overall this means,[CONCLUSION] Therefore, among A through E, the answer is[answer]" Figure 3: A sketched formulation of chain-of-though reasoning.Models We use OpenAI's text-davinci-003 InstructGPT model (175B parameters; Ouyang et al. (2022)) and Llama-2 Chat (13B parameters; Llama-2 in the following; Touvron et al. (2023)) to evaluate.While text-davinci-003 clearly is the more capable model, we find that Llama-2 provides an interesting comparison point for the applicability of sketching to smaller, more open models.We also experimented with OpenAI's smaller text-curie-001 model, but, overall, found that its instruction following capabilities were insufficient to support sketching (comparative study in App.C.2).</p>
<p>Table 2 :
2
Llama-2 Chat 13B results, analogous to Table1.Each configuration is evaluated on 1, 000 samples or the full datasets.
Answer-Only Chain-Of-ThoughtPrompt Sketching (ours)ARGMAXARGMAXARGMAXBEAMVARVARLogical ReasoningDate Understanding +0.49590.59080.63410.65880.58Information Essentiality + *--0.08820.13240.1324Question AnsweringAQuA (Ling et al., 2017)0.2310.29080.2390.2550.243StrategyQA (Geva et al., 2021)0.5640.5550.6380.6380.64Arithmetic ReasoningMultistep Arith. +0.0380.1330.12610.1420.103GSM8K (Cobbe et al., 2021)0.0490.2760.320.3530.331Interleaved ReasoningTracking Shuffled Objects +0.1970.1960.2270.210.234Matrix Shapes +0.2270.0870.2050.20.193</p>
<p>Table 3 :
3
Sketch-aware decoding enables sudoku solving and more efficient graph traversal.
Sequential PromptingPrompt Sketching(ours)ARGMAXARGMAXBEAMVARVARSudoku (3x3)text-davinci-00310/10 (reordered)1/106/107/10text-curie-0010/10 (reordered)0/103/103/10Dungeon Escapetext-davinci-003-9/10 (∼3.77 steps)10/10 (∼2.4 steps)9/10 (∼3.11 steps)text-curie-001-5/10 (∼4.0 steps)8/10 (∼2.625 steps)5/10 (∼2.4 steps)</p>
<p>Table 4 :
4
Few-Shot Task Accuracy with Answer-Only, Chain-Of-Thought and Prompt Sketching (ours) using ARGMAX, BEAM, BEAMVAR (ours) and VAR (ours) decoding.The best results are highlighted in bold.
Two-ShotZero-ShotTaskSequential DecodingSketch-AwareTaskAnswer-OnlyCoTPrompt Sketching (ours)ARGMAXARGMAX ARGMAXVAR BEAMVAR VARQuestion AnsweringAQuA (Ling et al., 2017)0.290.450.460.440.470.35StrategyQA (Geva et al., 2021)0.670.740.780.780.770.66Interleaved ReasoningTracking Shuffled Objects +0.10.460.570.570.620.66Matrix Shapes +0.670.760.810.770.790.85</p>
<p>Table 5 :
5
(Srivastava et al., 2022)urie-001 with Answer-Only, Chain-Of-Thought and Prompt Sketching (ours) using ARGMAX, BEAM, BEAMVAR (ours) and VAR (ours) decoding, compared to the results with text-davinci-003.The best results are highlighted in bold.Tasks extracted from the BIG benchmark suite(Srivastava et al., 2022).
text-curie-001text-davinci-003TaskSequential DecodingSketch-AwareTaskAnswer-OnlyCoTPrompt Sketching (ours)ARGMAXARGMAX ARGMAXVAR BEAMVAR VARQuestion AnsweringAQuA (Ling et al., 2017)0.160.240.270.170.470.35StrategyQA (Geva et al., 2021)0.460.530.580.520.770.66Interleaved ReasoningTracking Shuffled Objects +0.180.190.220.240.620.66Matrix Shapes +0.040.070.010.00.790.85
+ * Specifically adapted for our evaluation of forward referencing prompts.</p>
<p>Model Response Q: While on a liquid diet, are there some types of soup you cannot eat?A: Let's think step by step.First, what is a liquid diet?A liquid diet is a diet that consists of only liquids, such as water, juices, and soups.Now, are there some types of soup you cannot eat while on a liquid diet?Generally speaking, no, there are no types of soup that you cannot eat while on a liquid diet.However, it is important to note that some soups may contain ingredients that are not allowed on a liquid diet, such as dairy products, nuts, and certain types of meat.Therefore, it is important to read the ingredients list of any soup you are considering eating while on a liquid diet to make sure it does not contain any of these ingredients.Therefore, the answer (yes or no) is no ✗
Task Task TaskPrompting Method Prompting Method Prompting MethodQuery and Response Query and Response Query and ResponseStrategyQA@ao StrategyQA@multivar Multi-Variable Answer-Only multiarith2@ao Answer-OnlyQuery Query Query"""Q: While on a liquid diet, are there some types of """Q: While on a liquid diet, are there some types ofsoup you cannot eat?\n""" soup you cannot eat?\n""""A: The answer (yes or no) is[answer]" "A: Let's think step by step.\n"for i in range(5): Constraints "\n-[THOUGHT]"answer in [' yes', ' no', ' false', ' true'] if not THOUGHT.endswith("\n"): "\n"if "Therefore" in THOUGHT: breakModel Response Q: While on a liquid diet, are there some types of if "According" in THOUGHT: breaksoup you cannot eat? A: The answer (yes or no) is yes ✓ " Overall this means,[CONCLUSION] Therefore, the answer(yes or no) is[answer]"StrategyQA@cotChain-Of-ThoughtQuery Constraints"""Q: While on a liquid diet, are there some types of soup you cannot eat?\n""" STOPS _ AT(THOUGHT, "\n") and STOPS _ AT(THOUGHT, ".") andanswer in [' yes', ' no'] and "A: Let's think step by step.\n[COT] \n Therefore, the answer (yes or no) is[answer]" STOPS _ AT(CONCLUSION, "\n") and STOPS _ AT(CONCLUSION, ".")Constraints Model Response Q: While on a liquid diet, are there some types ofanswer in [' yes', ' no', ' false', ' true'] soup you cannot eat? A: Let's think step by step.-First, what is a liquid diet? A liquid diet is a diet that consists ofonly liquids, such as juices, smoothies, and soups.-Second, are there some types of soup you cannot eat on a liquiddiet? Yes, there are some types of soup that are not suitable for aliquid diet.-Examples of soups that are not suitable for a liquid diet includecream-based soups, chunky soups, and soups with large chunks ofvegetables or meat.-Additionally, some soups may contain ingredients that are notallowed on a liquid diet, such as dairy, sugar, or processed foods.-Finally, it is important to check with your doctor or nutritionistbefore starting any type of diet to make sure it is safe and appropriatefor your individual needs. Overall this means, yes, there are sometypes of soup you cannot eat while on a liquid diet. Therefore, theanswer (yes or no) is yes ✓Therefore, among Athrough E, the answer is D. ✓</p>
<p>""Q: Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange.At the start of the event , they are each holding a present of a different color: Alice has a red present, Bob has a black ball, Claire has a pink ball, Dave has a orange ball, and Eve has a brown present.As the event progresses, pairs of people swap gifts.First, Claire and Alice swap their gifts.\n-Thismeans,now[state _ var] Whereas [state _ var]\nThen, Claire and Dave swap their gifts.\n-Thismeans,now[ state _ var] Whereas [state _ var]\nThen, Alice and Bob swap their gifts.\n-Thismeans,now[state _ var] Whereas [state _ var]\nThen, Eve and Dave swap their gifts.\n-Thismeans,now[state _ var] Whereas [ state _ var]\nFinally, Bob and Claire swap their gifts.A: Overall this means[CONCLUSION] Therefore, At the end of the event, Alice has the[answer]"""Constraintsanswer in [" red present", " black ball", " pink ball", " orange ball", " brown present"] and STOPS _ AT( state _ var, ".") and STOPS _ AT(CONCLUSION, "\n") and STOPS _ AT(CONCLUSION, ".") Model Response Q: Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange.At the start of the event, they are each holding a present of a different color: Alice has a red present, Bob has a black ball, Claire has a pink ball, Dave has a orange ball, and Eve has a brown present.As the event progresses, pairs of people swap gifts.First, Claire and Alice swap their gifts.-This means, now Alice has a pink present and Claire has a red present.Whereas Bob has a black ball, Dave has an orange ball, and Eve has a brown present.Then, Claire and Dave swap their gifts.-This means, now Claire has an orange present and Dave has a pink present.Whereas Alice has a pink present, Bob has a black ball, and Eve has a brown present.Then, Alice and Bob swap their gifts.-This means, now Alice has a black present and Bob has a pink present.Whereas Claire has an orange present, Dave has a pink present, and Eve has a brown present.Then, Eve and Dave swap their gifts.-This means, now Eve has an orange present and Dave has a brown present.Whereas Alice has a black present, Bob has a pink present, and Claire has an orange present.Finally, Bob and Claire swap their gifts.A: Overall this means, now Alice has a black present, Bob has an orange present, Claire has a pink present, Dave has a brown present, and Eve has an orange present.Therefore, At the end of the event, Alice has the black ball ✓
TaskPrompting MethodQuery and Responsetracking_shuffled_ob-Multi-VariableQueryjects@multivar2
"</p>
<ul>
<li>Tasks extracted from the BIG benchmark suite(Srivastava et al., 2022).
C.3 CONFIDENCE BOUNDS AND SCALING OF OPENAI-SPECIFIC RESULTSTo check for significance of our smaller scale OpenAI-specific experiments, we additionally examine the corresponding confidence bounds.For this, we report all main OpenAI results with a Clopper-Pearson 95% two-sided confidence interval in Table6.Lastly, we scale our best OpenAI results for the Matrix Shapes taskSrivastava et al. (2022), by evaluating with 1000 instead of 100 samples, sampled uniformly from the original dataset.Doing so, we can confirm our main result in Table7, i.e. that prompt sketching and interleaved reasoning specifically are effective at improving LLM reasoning performance on this task.Due to cost considerations, we cannot scale all OpenAI experiments like this, but expect similar results, similar to the trends we observe in our large scale experiments with Llama-2 Chat 13B( Touvron et al. (2023)(see Section 4).Table6: Task accuracy of text-davinci-003 with Clopper-Pearson 95% two-sided confidence intervalsClopper and Pearson (1934).Model Response Q: The girls are trying to raise money for a carnival.Kim raises $320 more than Alexandra, who raises $430, and Maryam raises $400 more than Sarah, who raises $300.How much money, in dollars, did they all raise in total?A: Let's think step by step.Kim raised $320 more than Alexandra, so Kim raised $750 ($430 + $320).Maryam raised $400 more than Sarah, so Maryam raised $700 ($300 + $400).Therefore, the total amount of money the girls raised is $1,450 ($750 + $700).Therefore, the answer (arabic numerals) is 1,450 dollars.✗TaskPrompting Method Query and Response GSM8@multivar Multi-Variable Query"""Q: The girls are trying to raise money for a carnival.Kim raises \$320 more than Alexandra, who raises \$430, and Maryam raises \$400 more than Sarah, who raises \$300.How much money, in dollars, did they all raise in total?\n""" "A: Let's think step by step.\n" for i in range(10)Model Response Q: The girls are trying to raise money for a carnival.Kim raises $320 more than Alexandra, who raises $430, and Maryam raises $400 more than Sarah, who raises $300.How much money, in dollars, did they all raise in total?A: Let's think step by step.(1) Sarah raised $300 (2) Maryam raised $400 more than Sarah, so Maryam raised $700 (3) Alexandra raised $430 (4) Kim raised $320 more than Alexandra, so Kim raised $750 (5) The total amount of money they all raised is $300 + $700 + $430 + $750 = $2,280 (6) Therefore, the girls raised a total of $2,280 in dollars.(7) Answer: The girls raised a total of $2,280 in dollars.(8) Note: You can also solve this problem using algebra.Let x = Sarah's amount, then Maryam raised x + 400, Alexandra raised x + 430, and Kim raised x + 750.The total amount is then x + x + 400 + x + 430 + x + 750 = 3x + 1580 = 2,280.(9) Therefore, the girls raised a total of $2,280 in dollars.(10)Answer: The girls raised a total of $2,280 in dollars.Overall this means, the girls raised a total of $2,280 in dollars.Therefore, the answer (arabic numerals) is 2,280.✓
Large Language Models are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, ArXiv preprint. 2205.11916, 2022</li>
</ul>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Proc. of NeurIPS. of NeurIPS2022</p>
<p>Prompting is programming: A query language for large language models. L Beurer-Kellner, M Fischer, M Vechev, Proceedings of the ACM on Programming Languages. 2023PLDI</p>
<p>Guidance-ai/guidance: A guidance language for controlling large language models. S Lundberg, M T C Ribeiro, </p>
<p>Synchromesh: Reliable code generation from pre-trained language models. G Poesia, A Polozov, V Le, A Tiwari, G Soares, C Meek, S Gulwani, Proc. of ICLR. of ICLR2022</p>
<p>Ask me anything: A simple strategy for prompting language models. S Arora, A Narayan, M F Chen, L J Orr, N Guha, K Bhatia, I Chami, C Ré, Proc. of ICLR. of ICLR2023</p>
<p>Calibrate before use: Improving few-shot performance of language models. Z Zhao, E Wallace, S Feng, D Klein, S Singh, Proc. of ICML. of ICML2021</p>
<p>The curious case of neural text degeneration. A Holtzman, J Buys, L Du, M Forbes, Y Choi, Proc. of ICLR. of ICLR2020</p>
<p>If beam search is the answer, what was the question?. C Meister, R Cotterell, T Vieira, Proc. of EMNLP. of EMNLP2020</p>
<p>Lexically constrained decoding for sequence generation using grid beam search. C Hokamp, Q Liu, Proc. of ACL. of ACL2017</p>
<p>Fast lexically constrained decoding with dynamic beam allocation for neural machine translation. M Post, D Vilar, Proc. of NAACL-HLT. of NAACL-HLT2018</p>
<p>Google's neural machine translation system: Bridging the gap between human and machine translation. Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, J Klingner, A Shah, M Johnson, X Liu, L Kaiser, S Gouws, Y Kato, T Kudo, H Kazawa, K Stevens, G Kurian, N Patil, W Wang, C Young, J Smith, J Riesa, A Rudnick, O Vinyals, G Corrado, M Hughes, J Dean, abs/1609.08144ArXiv preprint. 2016</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, abs/2210.03629ArXiv preprint. 2022</p>
<p>Language Model Cascades. D Dohan, W Xu, A Lewkowycz, J Austin, D Bieber, R G Lopes, Y Wu, H Michalewski, R A Saurous, J Sohl-Dickstein, K Murphy, C Sutton, ArXiv preprint. 2022</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. W Ling, D Yogatama, C Dyer, P Blunsom, Proc. of ACL. of ACL2017</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. M Geva, D Khashabi, E Segal, T Khot, D Roth, J Berant, 2021Transactions of the Association for Computational Linguistics</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, ArXiv preprint. 2110.14168, 2021</p>
<p>. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, A Kluska, A Lewkowycz, A Agarwal, A Power, A Ray, A Warstadt, A W Kocurek, A Safaya, A Tazarv, A Xiang, A Parrish, A Nie, A Hussain, A Askell, A Dsouza, A Rahane, A S Iyer, A Andreassen, A Santilli, A Stuhlmüller, A M Dai, A La, A K Lampinen, A Zou, A Jiang, A Chen, A Vuong, A Gupta, A Gottardi, A Norelli, A Venkatesh, A Gholamidavoodi, A Tabassum, A Menezes, A Kirubarajan, A Mullokandov, A Sabharwal, A Herrick, A Efrat, A Erdem, A Karakas, ArXiv preprint. 2022Beyond the imitation game: Quantifying and extrapolating the capabilities</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, abs/2307.09288ArXiv preprint. 2023</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, abs/2303.03378ArXiv preprint. 2023</p>
<p>Prompt programming for large language models: Beyond the fewshot paradigm. L Reynolds, K Mcdonell, CHI '21: CHI Conference on Human Factors in Computing Systems, Virtual Event / Yokohama Japan. Extended Abstracts. May 8-13, 2021. 2021</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, abs/2201.11903ArXiv preprint. 2022</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, Proc. of ICLR. of ICLR2022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, abs/2210.03629ArXiv preprint. 2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, D Zhou, abs/2203.11171ArXiv preprint. 2022</p>
<p>Thinksum: Probabilistic reasoning over sets using large language models. B Ozturkler, N Malkin, Z Wang, N Jojic, abs/2210.01293ArXiv preprint. 2022</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, abs/2211.01910ArXiv preprint. 2022</p>
<p>Promptchainer: Chaining large language model prompts through visual programming. T Wu, E Jiang, A Donsbach, J Gray, A Molina, M Terry, C J Cai, CHI '22: CHI Conference on Human Factors in Computing Systems. New Orleans, LA, USA29 April 2022 -5 May 2022. 2022Extended Abstracts</p>
<p>PromptSource: An integrated development environment and repository for natural language prompts. S Bach, V Sanh, Z X Yong, A Webson, C Raffel, N V Nayak, A Sharma, T Kim, M S Bari, T Fevry, Z Alyafeai, M Dey, A Santilli, Z Sun, S Ben-David, C Xu, G Chhablani, H Wang, J Fries, M Al-Shaibani, S Sharma, U Thakker, K Almubarak, X Tang, D Radev, M T .-J, A Jiang, Rush, Proc. of ACL. of ACL2022</p>
<p>Locally typical sampling. C Meister, T Pimentel, G Wiher, R Cotterell, abs/2202.00666ArXiv preprint. 2022</p>
<p>Best-k search algorithm for neural text generation. J Xu, C Xiong, S Savarese, Y Zhou, abs/2211.11924ArXiv preprint. 2022</p>
<p>Massive-scale decoding for text generation using lattices. J Xu, S Jonnalagadda, G Durrett, Proc. of NAACL-HLT. of NAACL-HLT2022</p>
<p>Diverse beam search: Decoding diverse solutions from neural sequence. A K Vijayakumar, M Cogswell, R R Selvaraju, Q Sun, S Lee, D J Crandall, D Batra, abs/1610.02424ArXiv preprint. 2016</p>
<p>Improved lexically constrained decoding for translation and monolingual rewriting. J E Hu, H Khayrallah, R Culkin, P Xia, T Chen, M Post, B Van Durme, Proc. of NAACL-HLT. of NAACL-HLT2019</p>
<p>Task Prompting Method Query and Response date_under-standing@multivar2 Multi-Variable Query. C J Clopper, E S Pearson, Q: It was Sept. 1st, 2021 a week ago. What is the date 10 days ago in MM/DD/YYYY? Answer Choices. 1934Constraints STOPS _ AT(THOUGHT, "\n") and STOPS _ AT(THOUGHT. and STOPS _ AT(CONCLUSION, "\n") and STOPS _ AT(CONCLUSION, " .") Model Response Q: It was Sept. 1st, 2021 a week ago. What is the date 10 days ago in MM/DD/YYYY? Answer Choices: (A) 08/29/2021 (B) 08/28/2021 (C) 08/29/1925 (D) 08/30/2021 (E) 05/25/2021 (F) 09/19/2021 A: Let's think step by step. -Sept. -1st -2021 -a week ago -10 days ago Overall this means, 10 days before Sept. Therefore, among A through F, the answer is B: 08/28/2021</p>            </div>
        </div>

    </div>
</body>
</html>
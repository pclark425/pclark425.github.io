<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3979 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3979</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3979</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-04365f0f1db4c659c3297cb8e70c39b38ed3b487</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/04365f0f1db4c659c3297cb8e70c39b38ed3b487" target="_blank">Self-Evaluation Improves Selective Generation in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> ICBINB</p>
                <p><strong>Paper TL;DR:</strong> This work reformulates open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level, and demonstrates that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
                <p><strong>Paper Abstract:</strong> Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3979.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3979.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeqLik</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-level likelihood (log p(y|x))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A score computed as the sum (or length-normalized sum) of token log-probabilities assigned by an LLM to a generated sequence, commonly used as a confidence/ranking score for generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predictive ranking of output quality (used as a confidence score); accuracy when used to select among candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute log p(y|x) or length-normalized log p(y|x) for sampled candidate outputs and use these values to rank/select outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to TruthfulQA and TL;DR in experiments to compare against self-evaluation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy (%), Calibration-AUC (%), Selective-AUC (%); reported specific values (e.g., TruthfulQA PaLM-2: Accuracy 48.23%, Calibration-AUC 39.80%, Selective-AUC 33.63 for raw sequence likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>No direct human-in-the-loop required to compute the score; ground-truth correctness labels for evaluation come from human labels or models fine-tuned on human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Biased by sequence length (underestimates longer sequences), poor correlation with actual quality for open-ended generations (can be negatively correlated), not reliable for selective generation; length-normalization helps but does not fully fix calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Used to rank/sample answers to TruthfulQA questions and TL;DR summaries—e.g., comparing sequence-likelihood-ranked answers against human-judged truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Sequence-level likelihood performed worst among compared scores for selective generation; e.g., on TruthfulQA (PaLM-2) had low accuracy and very low Calibration-AUC/Selective-AUC, indicating poor predictive power for output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3979.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3979.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SampleAndSelect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample-and-Select (multi-choice reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert free-form generation to a multiple-choice task by sampling multiple candidate answers, labeling them A,B,C..., and asking the model to select the best token (character) corresponding to the best candidate; optionally de-bias via shuffling and add a 'NONE OF THE ABOVE' option.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Within-question selection accuracy (choose the best candidate) and cross-question calibration (ranking of selected outputs when used for selective generation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Sample n candidate answers per question, present them as choices with letters, query model for p(c_i | x, {c y}), optionally use un-normalized logits to avoid probability dispersion, shuffle+average to reduce position bias, and add 'NONE OF THE ABOVE' to allow rejection.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on TruthfulQA and TL;DR sampling setups (n=4) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy (% selecting correct answer among candidates), Calibration-AUC (%), Selective-AUC (%); example: PaLM-2 TruthfulQA Sample-and-Select accuracy 58.26%, Calibration-AUC 53.17%, Selective-AUC 48.59 (improved vs sequence-likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Ground-truth correctness labels used for evaluation come from human-evaluation proxies (GPT-judge or reward models) or human labels; human effort not required for the selection operation itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Position bias (selection score depends on answer ordering), probability dispersion when multiple sampled answers are true (softmax probability spreads across true answers), forced choice when none are true (over-confidence), shuffle-and-average de-biasing is computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Selecting the best answer among multiple sampled answers to a TruthfulQA question presented as A/B/C/D, and using the model's choice as the selected output.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Sample-and-Select improved accuracy relative to sequence-likelihood but suffered on calibration metrics; adding 'NONE OF THE ABOVE' improved calibration substantially. Position-bias has small experimental effect when not de-biased (per Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3979.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3979.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SampleAndEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample-and-Eval (pointwise true/false reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert free-form generation to a pointwise true/false evaluation task by asking the model, for each candidate (x,y), whether the candidate is correct (Yes/No) and using p(Yes|x,y) as the confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Calibration for predicting correctness across questions (ranking performance), and accuracy when used to select the best candidate (if used for selection).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Sample candidate answers, then for each candidate query model with a prompt that asks 'Is this answer factual/concise/etc.?' and take the softmax-normalized p(Yes) as a score; optionally include other candidates in the prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used on TruthfulQA and TL;DR; compared directly with Sample-and-Select and sequence-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy, Calibration-AUC, Selective-AUC; example: PaLM-2 TruthfulQA Sample-and-Eval achieved Accuracy 59.12%, Calibration-AUC 73.79%, Selective-AUC 58.19, notably higher calibration than sequence-level scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Evaluation labels for correctness are provided by human-derived proxies (GPT-judge, reward model) or human raters; the pointwise evaluation itself is zero-shot self-evaluation by the LLM (no separate human panel used at inference time).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pointwise score is independent of other candidates (a strength), but initial selection among many candidates may still require a separate selection method; may be slightly lower in raw selection accuracy compared to Sample-and-Select but better calibrated.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>For a sampled answer to a TruthfulQA question, the model is prompted 'Is the above answer factual, informative, unbiased, and safe? A) Yes B) No' and p(Yes) is used to score the candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Sample-and-Eval yielded the best calibration metrics among single methods (high Calibration-AUC) and strong Selective-AUC, demonstrating superior correlation between score and true quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3979.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3979.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid (Sample-and-Select then Sample-and-Eval scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage approach: use Sample-and-Select to pick the best candidate among sampled answers, then discard the multi-choice score and compute a pointwise p(Yes|x, y_hat) for the selected answer as the final confidence score; optionally penalize by the 'nota' uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Combine selection accuracy (choose best candidate within question) with cross-question calibrated confidence for selective generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Sample n candidates, run multi-choice selection to choose y_hat, then run pointwise true/false evaluation to score y_hat; final score optionally adjusted by p(nota) where 'NONE OF THE ABOVE' is used to quantify uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on TruthfulQA and TL;DR with PaLM-2 and partially on GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy, Calibration-AUC, Selective-AUC; example: Hybrid w/ nota on TruthfulQA (PaLM-2) Calibration-AUC 75.34% and Selective-AUC 58.10%, showing overall strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Same as Sample-and-Eval: ground-truth labels used for evaluation originate from human evaluations or human-trained proxies; hybrid method itself is automated (self-evaluation by LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Higher inference cost (1-2x increase) due to multiple calls; selection-phase biases remain potential issue though mitigated; complexity and cost trade-offs versus single-stage methods.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Select best candidate summary for TL;DR via Sample-and-Select, then compute p(Yes|text, selected_summary) as confidence and abstain if below threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Hybrid variants achieved competitive accuracy and the best overall calibration/Selective-AUC when combined with 'NONE OF THE ABOVE', balancing selection quality and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3979.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3979.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selective-generation and calibration metrics (Accuracy, Calibration-AUC, Selective-AUC, ROC, ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of evaluation metrics used to assess whether a model's confidence scores predict output quality and support abstention strategies: Accuracy, Calibration-AUC (ROC AUC for correctness vs score), Selective-AUC (area under selective generation curve), ROC curves, and a discussion of ECE limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy measures selection correctness; Calibration-AUC measures ranking ability of confidence scores to discriminate correct vs incorrect outputs; Selective-AUC measures performance when abstaining low-confidence outputs (area under accuracy vs abstention curve); ECE measures calibration of predicted probabilities to empirical accuracy but is limited here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute selected-answer accuracy, compute ROC and AUC treating correctness as binary label vs confidence score (Calibration-AUC), sort samples by score and plot accuracy as a function of abstention rate (Selective-AUC = area under that curve), discuss ECE applicability and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to model outputs on TruthfulQA and TL;DR; ROC and selective-generation curves plotted (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Reported numeric percentages for Accuracy, Calibration-AUC, Selective-AUC across methods and models; e.g., tables show values for PaLM-2 and GPT-3 across scoring strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Metrics require ground-truth correctness judgments derived from human labels or automated proxies tuned to human feedback; computing ECE requires probabilistic scores and reliable ground truth, which is challenging for sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ECE not applicable if scores are not true probabilities; obtaining ground truth for open-ended generation is expensive/difficult; Calibration-AUC measures ranking only (not absolute calibration), and selective metrics depend on the availability of reliable correctness labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Used to evaluate whether confidence scores allow safe abstention (output 'I don't know') to improve deployed quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Token-level self-evaluation methods substantially improved Calibration-AUC and Selective-AUC relative to sequence-level likelihood; selective curves show model accuracy increases as low-score outputs are abstained.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3979.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3979.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TruthfulQA and TL;DR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two benchmark datasets used for evaluating model output quality: TruthfulQA tests susceptibility to false human misconceptions (817 validation Qs); TL;DR is a Reddit-mined abstractive summarization dataset (15,240 test examples, 1k sampled).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Truthfulness and factuality for QA (TruthfulQA); concision and comprehensiveness for summaries (TL;DR).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Generate n candidate answers/summaries per item, apply selection/scoring strategies (Sequence-likelihood, Sample-and-Select, Sample-and-Eval, Hybrid), and compare scores against ground-truth quality labels (human or proxy).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>TruthfulQA: 817 validation questions assessing truthfulness vs misconceptions; TL;DR: Reddit-mined summarization dataset, 15,240 examples, 1,000 randomly sampled for cost savings.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Reported Accuracy, Calibration-AUC, Selective-AUC per scoring method and model (PaLM-2, GPT-3); example tables list numeric results for each combination.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>TruthfulQA labels: used GPT-judge (GPT-3 fine-tuned with human feedback) to approximate human ratings (90–95% accuracy vs human); TL;DR labels: used a reward model fine-tuned on human feedback (71.34% accuracy vs human).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Proxy labelers (GPT-judge, reward model) imperfect approximations of human judgment; sampling to reduce cost (1k TL;DR samples) may affect statistical power; API limitations for GPT-3 restricted some evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Example TruthfulQA prompt/answers (e.g., 'What happens to you if you eat watermelon seeds?') used to elicit candidate answers which are then scored and evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across both benchmarks, token-level self-evaluation approaches (Sample-and-Eval and Hybrid with nota) outperformed sequence-level scoring on calibration and selective-generation metrics; adding 'NONE OF THE ABOVE' generally improved calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3979.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3979.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanProxyModels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-feedback proxy evaluators (GPT-judge, reward model, RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models and training approaches used to approximate human judgments: GPT-judge (GPT-3 fine-tuned on human feedback) for truthfulness labels, reward models fine-tuned on human feedback for summarization quality, and RLHF discussed as a training alignment method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predict human judgments of correctness/truthfulness or summary quality to provide ground-truth labels for metric computation and to train aligned models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Use models fine-tuned on human preference labels to judge outputs (GPT-judge for TruthfulQA; reward model for TL;DR); mention RLHF and other human-feedback fine-tuning approaches as means to improve model calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used to label outputs on TruthfulQA (GPT-judge) and TL;DR (reward model trained on human feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Proxy labeler accuracy vs human: GPT-judge reported 90-95% accuracy predicting human truthfulness judgments; reward model for TL;DR reported 71.34% accuracy predicting human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-collected preference data used to fine-tune proxy evaluators and reward models; direct human evaluation is expensive—models act as scalable proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Proxy evaluators are imperfect and can inject bias or errors; human feedback data is expensive to collect; some improvements (e.g., RLHF) require large human-labeling investment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Using GPT-judge to assign binary truthfulness labels to candidate TruthfulQA answers which are then used to compute Calibration-AUC and Selective-AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Proxy evaluators enabled large-scale evaluation but their imperfect accuracy is acknowledged; results based on these proxies show token-level self-evaluation improves calibration, but depend on the quality of the proxy labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3979.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3979.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LimitationsAndChallenges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Practical limitations, biases and open challenges in evaluating LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of identified issues that complicate evaluation and calibration of LLM-generated outputs, including biases in scores, sampling effects, evaluation cost, and metric applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N/A (this entity summarizes limitations affecting the reliability of evaluation methods and metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Analysis and ablations in paper: position-bias experiments, ablation of de-biasing vs vanilla, discussion of API limitations and computational cost, and experiments with self-critique & revise.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Limitations observed across experiments on TruthfulQA and TL;DR using PaLM-2 and GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Empirical observations: position bias had limited effect in experiments; de-biasing by averaging permutations is computationally expensive; adding 'NONE OF THE ABOVE' improved calibration; self-evaluation increases inference time by 1–2x.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human evaluation is expensive; reliance on proxy models (GPT-judge/reward models) introduces uncertainty about ground truth; collecting human labels remains a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sequence-level scores poorly correlated with quality; position bias, probability dispersion among multiple correct answers, 'no true answer' forcing overconfidence; ECE not applicable to non-probabilistic scores; API limitations (GPT-3 logprob token limits) prevent full evaluation; increased inference cost for self-evaluation; model verbalized confidence often overconfident.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>No direct scientific-theory examples; limitation impacts any open-ended generation including hypothetical scientific claims where ground-truth is difficult to obtain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper documents that token-level self-evaluation reduces several of these problems and improves calibration and selective-generation performance, but trade-offs remain (cost, remaining biases, proxy label noise).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models (mostly) know what they know <em>(Rating: 2)</em></li>
                <li>Robots that ask for help: Uncertainty alignment for large language model planners <em>(Rating: 2)</em></li>
                <li>Measuring and improving model-moderator collaboration using uncertainty estimation <em>(Rating: 2)</em></li>
                <li>On calibration of modern neural networks <em>(Rating: 1)</em></li>
                <li>On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study <em>(Rating: 1)</em></li>
                <li>Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3979",
    "paper_id": "paper-04365f0f1db4c659c3297cb8e70c39b38ed3b487",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "SeqLik",
            "name_full": "Sequence-level likelihood (log p(y|x))",
            "brief_description": "A score computed as the sum (or length-normalized sum) of token log-probabilities assigned by an LLM to a generated sequence, commonly used as a confidence/ranking score for generated outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Predictive ranking of output quality (used as a confidence score); accuracy when used to select among candidates.",
            "evaluation_methods": "Compute log p(y|x) or length-normalized log p(y|x) for sampled candidate outputs and use these values to rank/select outputs.",
            "benchmark_or_dataset": "Applied to TruthfulQA and TL;DR in experiments to compare against self-evaluation methods.",
            "metrics_reported": "Accuracy (%), Calibration-AUC (%), Selective-AUC (%); reported specific values (e.g., TruthfulQA PaLM-2: Accuracy 48.23%, Calibration-AUC 39.80%, Selective-AUC 33.63 for raw sequence likelihood).",
            "human_involvement": "No direct human-in-the-loop required to compute the score; ground-truth correctness labels for evaluation come from human labels or models fine-tuned on human feedback.",
            "limitations_or_challenges": "Biased by sequence length (underestimates longer sequences), poor correlation with actual quality for open-ended generations (can be negatively correlated), not reliable for selective generation; length-normalization helps but does not fully fix calibration.",
            "llm_theory_example": "Used to rank/sample answers to TruthfulQA questions and TL;DR summaries—e.g., comparing sequence-likelihood-ranked answers against human-judged truthfulness.",
            "evaluation_results": "Sequence-level likelihood performed worst among compared scores for selective generation; e.g., on TruthfulQA (PaLM-2) had low accuracy and very low Calibration-AUC/Selective-AUC, indicating poor predictive power for output quality.",
            "uuid": "e3979.0",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SampleAndSelect",
            "name_full": "Sample-and-Select (multi-choice reduction)",
            "brief_description": "Convert free-form generation to a multiple-choice task by sampling multiple candidate answers, labeling them A,B,C..., and asking the model to select the best token (character) corresponding to the best candidate; optionally de-bias via shuffling and add a 'NONE OF THE ABOVE' option.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Within-question selection accuracy (choose the best candidate) and cross-question calibration (ranking of selected outputs when used for selective generation).",
            "evaluation_methods": "Sample n candidate answers per question, present them as choices with letters, query model for p(c_i | x, {c y}), optionally use un-normalized logits to avoid probability dispersion, shuffle+average to reduce position bias, and add 'NONE OF THE ABOVE' to allow rejection.",
            "benchmark_or_dataset": "Evaluated on TruthfulQA and TL;DR sampling setups (n=4) in experiments.",
            "metrics_reported": "Accuracy (% selecting correct answer among candidates), Calibration-AUC (%), Selective-AUC (%); example: PaLM-2 TruthfulQA Sample-and-Select accuracy 58.26%, Calibration-AUC 53.17%, Selective-AUC 48.59 (improved vs sequence-likelihood).",
            "human_involvement": "Ground-truth correctness labels used for evaluation come from human-evaluation proxies (GPT-judge or reward models) or human labels; human effort not required for the selection operation itself.",
            "limitations_or_challenges": "Position bias (selection score depends on answer ordering), probability dispersion when multiple sampled answers are true (softmax probability spreads across true answers), forced choice when none are true (over-confidence), shuffle-and-average de-biasing is computationally expensive.",
            "llm_theory_example": "Selecting the best answer among multiple sampled answers to a TruthfulQA question presented as A/B/C/D, and using the model's choice as the selected output.",
            "evaluation_results": "Sample-and-Select improved accuracy relative to sequence-likelihood but suffered on calibration metrics; adding 'NONE OF THE ABOVE' improved calibration substantially. Position-bias has small experimental effect when not de-biased (per Table 4).",
            "uuid": "e3979.1",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SampleAndEval",
            "name_full": "Sample-and-Eval (pointwise true/false reduction)",
            "brief_description": "Convert free-form generation to a pointwise true/false evaluation task by asking the model, for each candidate (x,y), whether the candidate is correct (Yes/No) and using p(Yes|x,y) as the confidence score.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Calibration for predicting correctness across questions (ranking performance), and accuracy when used to select the best candidate (if used for selection).",
            "evaluation_methods": "Sample candidate answers, then for each candidate query model with a prompt that asks 'Is this answer factual/concise/etc.?' and take the softmax-normalized p(Yes) as a score; optionally include other candidates in the prompt context.",
            "benchmark_or_dataset": "Used on TruthfulQA and TL;DR; compared directly with Sample-and-Select and sequence-likelihood.",
            "metrics_reported": "Accuracy, Calibration-AUC, Selective-AUC; example: PaLM-2 TruthfulQA Sample-and-Eval achieved Accuracy 59.12%, Calibration-AUC 73.79%, Selective-AUC 58.19, notably higher calibration than sequence-level scores.",
            "human_involvement": "Evaluation labels for correctness are provided by human-derived proxies (GPT-judge, reward model) or human raters; the pointwise evaluation itself is zero-shot self-evaluation by the LLM (no separate human panel used at inference time).",
            "limitations_or_challenges": "Pointwise score is independent of other candidates (a strength), but initial selection among many candidates may still require a separate selection method; may be slightly lower in raw selection accuracy compared to Sample-and-Select but better calibrated.",
            "llm_theory_example": "For a sampled answer to a TruthfulQA question, the model is prompted 'Is the above answer factual, informative, unbiased, and safe? A) Yes B) No' and p(Yes) is used to score the candidate.",
            "evaluation_results": "Sample-and-Eval yielded the best calibration metrics among single methods (high Calibration-AUC) and strong Selective-AUC, demonstrating superior correlation between score and true quality.",
            "uuid": "e3979.2",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Hybrid",
            "name_full": "Hybrid (Sample-and-Select then Sample-and-Eval scoring)",
            "brief_description": "A two-stage approach: use Sample-and-Select to pick the best candidate among sampled answers, then discard the multi-choice score and compute a pointwise p(Yes|x, y_hat) for the selected answer as the final confidence score; optionally penalize by the 'nota' uncertainty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Combine selection accuracy (choose best candidate within question) with cross-question calibrated confidence for selective generation.",
            "evaluation_methods": "Sample n candidates, run multi-choice selection to choose y_hat, then run pointwise true/false evaluation to score y_hat; final score optionally adjusted by p(nota) where 'NONE OF THE ABOVE' is used to quantify uncertainty.",
            "benchmark_or_dataset": "Evaluated on TruthfulQA and TL;DR with PaLM-2 and partially on GPT-3.",
            "metrics_reported": "Accuracy, Calibration-AUC, Selective-AUC; example: Hybrid w/ nota on TruthfulQA (PaLM-2) Calibration-AUC 75.34% and Selective-AUC 58.10%, showing overall strong performance.",
            "human_involvement": "Same as Sample-and-Eval: ground-truth labels used for evaluation originate from human evaluations or human-trained proxies; hybrid method itself is automated (self-evaluation by LLM).",
            "limitations_or_challenges": "Higher inference cost (1-2x increase) due to multiple calls; selection-phase biases remain potential issue though mitigated; complexity and cost trade-offs versus single-stage methods.",
            "llm_theory_example": "Select best candidate summary for TL;DR via Sample-and-Select, then compute p(Yes|text, selected_summary) as confidence and abstain if below threshold.",
            "evaluation_results": "Hybrid variants achieved competitive accuracy and the best overall calibration/Selective-AUC when combined with 'NONE OF THE ABOVE', balancing selection quality and calibration.",
            "uuid": "e3979.3",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Metrics",
            "name_full": "Selective-generation and calibration metrics (Accuracy, Calibration-AUC, Selective-AUC, ROC, ECE)",
            "brief_description": "Set of evaluation metrics used to assess whether a model's confidence scores predict output quality and support abstention strategies: Accuracy, Calibration-AUC (ROC AUC for correctness vs score), Selective-AUC (area under selective generation curve), ROC curves, and a discussion of ECE limitations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Accuracy measures selection correctness; Calibration-AUC measures ranking ability of confidence scores to discriminate correct vs incorrect outputs; Selective-AUC measures performance when abstaining low-confidence outputs (area under accuracy vs abstention curve); ECE measures calibration of predicted probabilities to empirical accuracy but is limited here.",
            "evaluation_methods": "Compute selected-answer accuracy, compute ROC and AUC treating correctness as binary label vs confidence score (Calibration-AUC), sort samples by score and plot accuracy as a function of abstention rate (Selective-AUC = area under that curve), discuss ECE applicability and limitations.",
            "benchmark_or_dataset": "Applied to model outputs on TruthfulQA and TL;DR; ROC and selective-generation curves plotted (Figure 3).",
            "metrics_reported": "Reported numeric percentages for Accuracy, Calibration-AUC, Selective-AUC across methods and models; e.g., tables show values for PaLM-2 and GPT-3 across scoring strategies.",
            "human_involvement": "Metrics require ground-truth correctness judgments derived from human labels or automated proxies tuned to human feedback; computing ECE requires probabilistic scores and reliable ground truth, which is challenging for sequences.",
            "limitations_or_challenges": "ECE not applicable if scores are not true probabilities; obtaining ground truth for open-ended generation is expensive/difficult; Calibration-AUC measures ranking only (not absolute calibration), and selective metrics depend on the availability of reliable correctness labels.",
            "llm_theory_example": "Used to evaluate whether confidence scores allow safe abstention (output 'I don't know') to improve deployed quality.",
            "evaluation_results": "Token-level self-evaluation methods substantially improved Calibration-AUC and Selective-AUC relative to sequence-level likelihood; selective curves show model accuracy increases as low-score outputs are abstained.",
            "uuid": "e3979.4",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Benchmarks",
            "name_full": "TruthfulQA and TL;DR",
            "brief_description": "Two benchmark datasets used for evaluating model output quality: TruthfulQA tests susceptibility to false human misconceptions (817 validation Qs); TL;DR is a Reddit-mined abstractive summarization dataset (15,240 test examples, 1k sampled).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Truthfulness and factuality for QA (TruthfulQA); concision and comprehensiveness for summaries (TL;DR).",
            "evaluation_methods": "Generate n candidate answers/summaries per item, apply selection/scoring strategies (Sequence-likelihood, Sample-and-Select, Sample-and-Eval, Hybrid), and compare scores against ground-truth quality labels (human or proxy).",
            "benchmark_or_dataset": "TruthfulQA: 817 validation questions assessing truthfulness vs misconceptions; TL;DR: Reddit-mined summarization dataset, 15,240 examples, 1,000 randomly sampled for cost savings.",
            "metrics_reported": "Reported Accuracy, Calibration-AUC, Selective-AUC per scoring method and model (PaLM-2, GPT-3); example tables list numeric results for each combination.",
            "human_involvement": "TruthfulQA labels: used GPT-judge (GPT-3 fine-tuned with human feedback) to approximate human ratings (90–95% accuracy vs human); TL;DR labels: used a reward model fine-tuned on human feedback (71.34% accuracy vs human).",
            "limitations_or_challenges": "Proxy labelers (GPT-judge, reward model) imperfect approximations of human judgment; sampling to reduce cost (1k TL;DR samples) may affect statistical power; API limitations for GPT-3 restricted some evaluations.",
            "llm_theory_example": "Example TruthfulQA prompt/answers (e.g., 'What happens to you if you eat watermelon seeds?') used to elicit candidate answers which are then scored and evaluated.",
            "evaluation_results": "Across both benchmarks, token-level self-evaluation approaches (Sample-and-Eval and Hybrid with nota) outperformed sequence-level scoring on calibration and selective-generation metrics; adding 'NONE OF THE ABOVE' generally improved calibration.",
            "uuid": "e3979.5",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "HumanProxyModels",
            "name_full": "Human-feedback proxy evaluators (GPT-judge, reward model, RLHF)",
            "brief_description": "Models and training approaches used to approximate human judgments: GPT-judge (GPT-3 fine-tuned on human feedback) for truthfulness labels, reward models fine-tuned on human feedback for summarization quality, and RLHF discussed as a training alignment method.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Predict human judgments of correctness/truthfulness or summary quality to provide ground-truth labels for metric computation and to train aligned models.",
            "evaluation_methods": "Use models fine-tuned on human preference labels to judge outputs (GPT-judge for TruthfulQA; reward model for TL;DR); mention RLHF and other human-feedback fine-tuning approaches as means to improve model calibration.",
            "benchmark_or_dataset": "Used to label outputs on TruthfulQA (GPT-judge) and TL;DR (reward model trained on human feedback).",
            "metrics_reported": "Proxy labeler accuracy vs human: GPT-judge reported 90-95% accuracy predicting human truthfulness judgments; reward model for TL;DR reported 71.34% accuracy predicting human ratings.",
            "human_involvement": "Human-collected preference data used to fine-tune proxy evaluators and reward models; direct human evaluation is expensive—models act as scalable proxies.",
            "limitations_or_challenges": "Proxy evaluators are imperfect and can inject bias or errors; human feedback data is expensive to collect; some improvements (e.g., RLHF) require large human-labeling investment.",
            "llm_theory_example": "Using GPT-judge to assign binary truthfulness labels to candidate TruthfulQA answers which are then used to compute Calibration-AUC and Selective-AUC.",
            "evaluation_results": "Proxy evaluators enabled large-scale evaluation but their imperfect accuracy is acknowledged; results based on these proxies show token-level self-evaluation improves calibration, but depend on the quality of the proxy labels.",
            "uuid": "e3979.6",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LimitationsAndChallenges",
            "name_full": "Practical limitations, biases and open challenges in evaluating LLM outputs",
            "brief_description": "A collection of identified issues that complicate evaluation and calibration of LLM-generated outputs, including biases in scores, sampling effects, evaluation cost, and metric applicability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "N/A (this entity summarizes limitations affecting the reliability of evaluation methods and metrics).",
            "evaluation_methods": "Analysis and ablations in paper: position-bias experiments, ablation of de-biasing vs vanilla, discussion of API limitations and computational cost, and experiments with self-critique & revise.",
            "benchmark_or_dataset": "Limitations observed across experiments on TruthfulQA and TL;DR using PaLM-2 and GPT-3.",
            "metrics_reported": "Empirical observations: position bias had limited effect in experiments; de-biasing by averaging permutations is computationally expensive; adding 'NONE OF THE ABOVE' improved calibration; self-evaluation increases inference time by 1–2x.",
            "human_involvement": "Human evaluation is expensive; reliance on proxy models (GPT-judge/reward models) introduces uncertainty about ground truth; collecting human labels remains a bottleneck.",
            "limitations_or_challenges": "Sequence-level scores poorly correlated with quality; position bias, probability dispersion among multiple correct answers, 'no true answer' forcing overconfidence; ECE not applicable to non-probabilistic scores; API limitations (GPT-3 logprob token limits) prevent full evaluation; increased inference cost for self-evaluation; model verbalized confidence often overconfident.",
            "llm_theory_example": "No direct scientific-theory examples; limitation impacts any open-ended generation including hypothetical scientific claims where ground-truth is difficult to obtain.",
            "evaluation_results": "Paper documents that token-level self-evaluation reduces several of these problems and improves calibration and selective-generation performance, but trade-offs remain (cost, remaining biases, proxy label noise).",
            "uuid": "e3979.7",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 2
        },
        {
            "paper_title": "Robots that ask for help: Uncertainty alignment for large language model planners",
            "rating": 2
        },
        {
            "paper_title": "Measuring and improving model-moderator collaboration using uncertainty estimation",
            "rating": 2
        },
        {
            "paper_title": "On calibration of modern neural networks",
            "rating": 1
        },
        {
            "paper_title": "On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study",
            "rating": 1
        },
        {
            "paper_title": "Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs",
            "rating": 1
        }
    ],
    "cost": 0.014897249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Evaluation Improves Selective Generation in Large Language Models</h1>
<p>Jie Ren<em>, Yao Zhao</em>, Tu Vu ${ }^{\dagger}$, Peter J. Liu<em>, Balaji Lakshminarayanan</em><br>{jjren, yaozhaoyz, ttvu, peterjliu, balajiln}@google.com<br>Google DeepMind ${ }^{\star}$, Google Research ${ }^{\dagger}$</p>
<h4>Abstract</h4>
<p>Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequencelevel probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate openended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a "None of the above" option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PalM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) are often pre-trained on a vast corpus of text and then fine-tuned on supervised data to follow instructions [Devlin et al., 2018, Radford et al., 2018, Raffel et al., 2020, Adiwardana et al., 2020, Wei et al., 2021, Ouyang et al., 2022, Chung et al., 2022]. Having the ability to tell when a language model's output is trustworthy is important for safe deployment of language models. For example, the model's trustworthiness can be used as signal to selectively generate answers based on how confident the LLM is in the quality of its output.</p>
<p>Prior research has demonstrated that the distance to the training distribution in the embedding space predicts output quality for conditional generative models [Ren et al., 2023b]. Extending this work to large language models is challenging because their training distribution is too large to estimate and extracting embeddings from well-integrated LLM systems requires significant engineering effort.</p>
<p>Alternatively, a straightforward approach to estimating a language model's confidence in its output is to calculate the sequence probability or the length-normalized sequence probabilities [Adiwardana et al., 2020]. However, studies have shown that language models' sequence probabilities on openended generations do not reliably rank-order their outputs by quality [Liu et al., 2022, Ren et al., 2023b]. Human feedback can be used to fine-tune language models to better align with human-judged quality, such as with Reinforcement Learning from Human Feedback (RLHF) [Stiennon et al., 2020], SLiC-HF [Zhao et al., 2023] and DPO [Rafailov et al., 2023], resulting in better quality-calibrated models.</p>
<p>Since human feedback data is expensive to obtain, we explore leveraging the self-evaluation ability of LLMs to improve quality-calibration. Despite the poor calibration on sequence-level likelihood, recent work has shown that LLM token-level probability can be quite well-calibrated on choosing the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Demonstration of our approach.</p>
<p>correct option of multi-choice question answering and true/false questions [Kadavath et al., 2022, OpenAI, 2023, Robinson et al., 2022]. This suggests that evaluating language model's generation with token-level probabilities using an appropriate prompt format might be better for selective generation than sequence-level likelihood.</p>
<p>In this study, we focus on obtaining a confidence score that is quality-calibrated on free-form generation tasks. We propose reducing the sequence-level scoring problem to token-level scoring by designing different self-evaluation tasks and propose a variety of scores. We focus on evaluating model's quality-calibration for use in selective generation, and not just predictive accuracy. We show that our proposed confidence estimation significantly improves the quality calibration, and can be used to abstain poor quality outputs using the TRUTHFULQA and TL;DR benchmarks.</p>
<h2>2 Methods</h2>
<p><strong>Background: sequence likelihood</strong> Given a question <em>x</em> and answer <em>y</em>, <em>y</em> = <em>y</em><sup>1</sup><em>y</em><sup>2</sup> . . . <em>y</em><sup>t</sup>, we have sequence-level likelihood score,</p>
<p>$$
\log p(\mathbf{y}|\mathbf{x}) = \sum_{t=1}^{l} \log p(y^{t}|y^{1} \dots y^{t-1}, \mathbf{x}). \qquad \text{(Sequence likelihood)}
$$</p>
<p>Though log <em>p</em>(<strong>y</strong>|<strong>x</strong>) is statistically meaningful, it has been shown that it is biased towards sequence length, i.e. models tend to underestimate sequence likelihood of longer sentences [Wu et al., 2016]. The length normalized likelihood is an alternative score to use,</p>
<p>$$
\log \bar{p}(\mathbf{y}|\mathbf{x}) = \frac{1}{l} \sum_{t=1}^{l} \log p(y^{t}|y^{1} \dots y^{t-1}, \mathbf{x}). \qquad \text{(Length normalized sequence likelihood)}
$$</p>
<p>Although sequence-level scores have weak predictive power, the previous results show that LLMs are well-calibrated on multiple choice question answer tasks and true/false evaluation tasks [Kadavath et al., 2022, OpenAI, 2023], suggesting the model has better calibration on token-level scores. Inspired by this, we propose to reduce free-form generation to multiple-choice and true/false evaluation tasks, in order to leverage token-level calibration to improve the calibration of free-form generation, as shown in Figure 1. Ren et al. [2023a] propose a similar idea but their focus was on robotics planning, while we focus on the general question answer settings.</p>
<p>To convert free-form generation to multi-choice question answer task, we first sample multiple candidate answers. For a given question <em>x</em>, we sample <em>n</em> answers {<strong>y</strong><sub><em>i</em></sub>}, <em>i</em> = 1, . . . , <em>n</em> from an LLM. We tried using a prompt to instruct the model to generate multiple different answers all at once, but the quality of the batch generated answers were not as good as sampling one at a time.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The issues of position bias, probability dispersion, and no true answers in the Sample and Select setup. The question examples are from [Lin et al., 2021, Agarwal et al., 2023].</p>
<h3>2.1 Sample and Select: reduce free-form generation to multi-choice question answer task</h3>
<p>Given a question and a set of candidate answers {y}<em i="i">{n}, we append alphabet characters, c = A, B, C, ..., to the answers and form it into a multiple choice format. A straightforward score could be the softmax probability for the characters, p(c</em>). However, there are a few issues with that score:}|x, {cq}), which was used in Ren et al. [2023a]. The selected answer would be the one with the highest softmax probability, y˜ = y_{r}, r = arg max_{i} p(c_{i}|x, {cq</p>
<p><strong>Position bias</strong> The score could change as the position of the candidate answers change. See Figure 2 (left). This phenomenon was also reported in other work [Robinson et al., 2022, Zheng et al., 2023]. A simple "shuffle and average" could de-bias and correct for the scores, while more sophisticated method to estimate the prior was proposed by Zheng et al. [2023]. In our work, we use the simple shuffle and average de-bias method. The ablation study of the effect of position bias is in Table 4.</p>
<p><strong>Probability dispersion</strong> among multiple true answers. Unlike the pre-designed multiple choice QA task where only one true answer provided, in the free-form generation there is no such guarantee that only one of the sampled answers is true. When more than one true answers are in the candidate list, the probability of the true is dispersed among the true answers, see Figure 2 (middle). This is an undesired property for comparing across questions, since different questions could generate different number of true answers. Probability dispersion is not a unique problem in LLMs; similar issue was discovered in the ImageNet classification where an image can map to multiple classes, and unnormalized logit was preferred than softmax probability to avoid the probability dispersion [Hendrycks et al., 2019]. Therefore we propose,</p>
<p>$$
\log p(c_i | x, {c y}), c = {A, B, \dots}. \qquad \text{(Sample and Select)}
$$</p>
<p><strong>No answer is true</strong> It is possible that when the model does not know the answer, none of the sampled answers is true. If only wrong answers are provided, the model will be forced to choose one from them, resulting in over-confident prediction. See Figure 2 (right). To mitigate that, we add "NONE OF THE ABOVE" as an additional candidate answer to give model a chance to reject the sampled answers, {y}_{nota} = {y} ∪ {nota}. This is similar to adding "An option not listed here" to the robotic planning task [Ren et al., 2023a]. We obtain the score corresponding to the "NONE OF THE ABOVE" answer,</p>
<p>$$
p(c_{\text{nota}} | x, {c y}_{+ \text{nota}}) \qquad \text{(Sample and Select w/ NONE OF THE ABOVE)}
$$</p>
<p>A higher nota score indicates that the selected answer is less likely to be correct. So we use -p(cnota|x, {cq}+nota) as the confidence score of the selected answer, yˆ = y_{r}, r = arg max_{i} p(c_{i}|x, {cq}). Note that the selected answer is still the answer with the highest score within the original answer set {y} excluding the nota answer.</p>
<h3>2.2 Sample and Eval: reduce free-form generation to true/false evaluation task</h3>
<p>We can also evaluate a question and an answer pair using pointwise evaluation format. We ask the model if the candidate answer is correct or not, as shown in Figure 1. Since the task is a binary</p>
<p>classification task, we can normalize the output score using softmax function to a probability,</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \boldsymbol{y}_{i})
$$</p>
<p>(Sample and Eval)
This is similar the P(True) proposed in [Kadavath et al., 2022]. They also propose to include candidate answers in the prompt,</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \boldsymbol{y}_{i},{\boldsymbol{y}})
$$</p>
<p>(Sample and Eval w/ other candidates)
But that work focuses on the scaling law of the score's calibration, and did not compare it with sequence-level score and Sample and Select score.</p>
<h1>2.3 Combining the best of both worlds: select the answer via multi-choice evaluation and score the selected answer via pointwise evaluation</h1>
<p>Sample and Select and Sample and Eval have their own pros and cons. In Sample and Select, although the un-normalized logit is better than softmax probability for calibration purpose, the logit score is still dependent on the other candidate answers. For fairly comparing across $(\boldsymbol{x}, \boldsymbol{y})$ pairs, a good score should measure the confidence to the $(\boldsymbol{x}, \boldsymbol{y})$ itself, not dependent on other candidate answers. Sample and Eval score $p\left(\right.$ Yes $\left.\mid \boldsymbol{y}<em r="r">{i}, \boldsymbol{x}\right)$ is indeed independent of other answers. On the other hand, Sample and Select provides the opportunity for comparing different answers and select the best. Therefore, we combine the best of both: We first use Sample and Select to select the best answer within a given question. The answer with the highest softmax probability score is selected, $\hat{\boldsymbol{y}}=\boldsymbol{y}</em>, r=\arg \max <em i="i">{i} p\left(c</em>)$.} \mid \boldsymbol{x},{c \boldsymbol{y}}\right)$. After selection, we discard the score because it is not good for cross question comparison. We score the selected answer via Sample and Eval $p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}}), \text { where } \hat{\boldsymbol{y}}=\boldsymbol{y}<em i="i">{r}, r=\arg \max </em>}\right)
$$} p\left(c_{i} \mid \boldsymbol{x},{c \boldsymbol{y</p>
<p>In the case where None of the above answer is added, we penalize the confidence score $p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}})$ with the uncertainty score for the nota answer, that is $p\left(\right.$ Yes $\mid \boldsymbol{x}, \hat{\boldsymbol{y}}$ ) $-p\left(c_{\text {nota }} \mid \boldsymbol{x},{c \boldsymbol{y}}{ }_{+\text {nota }}\right)$. We call this hybrid strategy "Sample and Select and Eval". See details in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Hybrid</span><span class="w"> </span><span class="s">&quot;Sample and Select and Eval&quot;</span>
<span class="w">    </span><span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Question</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">multi</span><span class="o">-</span><span class="kd">choice</span><span class="w"> </span><span class="nx">selection</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">F</span><span class="p">}</span><span class="err">\</span><span class="p">),</span>
<span class="w">        </span><span class="nx">pointwise</span><span class="w"> </span><span class="nx">evaluation</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Use</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">stackrel</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">iid</span><span class="w"> </span><span class="p">}}{=}</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">})</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Append</span><span class="w"> </span><span class="s">&quot;None of the above&quot;</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}=</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">nota</span><span class="err">\</span><span class="p">}.</span><span class="w"> </span><span class="o">|</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="o">|=</span><span class="nx">n</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Compose</span><span class="w"> </span><span class="nx">selection</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">F</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">})</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">),</span><span class="w"> </span><span class="nx">feed</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">obtain</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">softmax</span><span class="w"> </span><span class="nx">probabi</span><span class="o">-</span>
<span class="w">        </span><span class="nx">ity</span><span class="w"> </span><span class="nx">scores</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">best</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">among</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">sampled</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="p">(</span><span class="nx">exclude</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">post</span><span class="o">-</span><span class="nx">hoc</span><span class="w"> </span><span class="nx">added</span><span class="w"> </span><span class="nx">nota</span><span class="w"> </span><span class="nx">answer</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}}=</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">r</span><span class="p">},</span><span class="w"> </span><span class="nx">r</span><span class="p">=</span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="nx">neq</span><span class="w"> </span><span class="nx">n</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Obtain</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">uncertainty</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">nota</span><span class="w"> </span><span class="nx">answer</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}=</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Compose</span><span class="w"> </span><span class="nx">pointwise</span><span class="w"> </span><span class="nx">evaluation</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}})</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">feed</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">obtain</span><span class="w"> </span><span class="nx">output</span>
<span class="w">        </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="p">=</span><span class="nx">p</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Yes</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}})</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">The</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="nx">confidence</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="p">=</span><span class="nx">s</span><span class="o">-</span><span class="nx">s_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">its</span><span class="w"> </span><span class="nx">confidence</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="err">\</span><span class="p">).</span>
</code></pre></div>

<h2>3 Evaluation metrics for selective generation</h2>
<p>Suppose $\mathcal{D}={\boldsymbol{x}}<em n="n">{m}$ is a dataset containing $m$ questions to evaluate. Given a LLM model $\mathcal{M}$, for each question $\boldsymbol{x}$, we randomly sample $n$ answers ${\boldsymbol{y}}</em>}=\left{\boldsymbol{y<em 2="2">{1}, \boldsymbol{y}</em>}, \ldots, \boldsymbol{y<em i="i">{n}\right}$, where $\boldsymbol{y}</em>)$ pair, we would like evaluate how well the score could be used for selective generation, besides the accuracy.
Accuracy For a fixed question $\boldsymbol{x}$ and a set candidate answers ${\boldsymbol{y}}} \stackrel{\text { iid }}{=} \mathcal{M}(\boldsymbol{x})$. Suppose the ground truth $h(\boldsymbol{x}, \boldsymbol{y})={0,1}$ for each answer's correctness (or quality) is available, either through human evaluation or an auto-evaluation model to approximate human rating. Given a confidence score function $s(\boldsymbol{x}, \boldsymbol{y})$ measuring the confidence of a $(\boldsymbol{x}, \boldsymbol{y<em r="r">{n}$ to $\boldsymbol{x}$, we could use the confidence score to select the final answer $\hat{\boldsymbol{y}}$ to the question $\boldsymbol{x}$. We assess if the selected answer is correct, i.e. $h(\boldsymbol{x}, \hat{\boldsymbol{y}})=1, \hat{\boldsymbol{y}}=\boldsymbol{y}</em>, r=\arg \max <em i="i">{i=1}^{n} s\left(\boldsymbol{x}, \boldsymbol{y}</em>\right)$.</p>
<p>Accuracy evaluates if the score can be used to choose the best answer among the candidate answers within a given question. For selective generation, we compare across questions. Given the $m$ question and its selected best answer, ${(\boldsymbol{x}, \hat{\boldsymbol{y}})}_{m}$, we would abstain poor quality pairs to ensure better overall generation quality, aka selective generation. Suppose for each pair we have a confidence score, $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$. If the score is predictive for the quality, we could rank the pairs by the score, and abstain those with the lowest scores, and selectively only output answers with high scores. For the abstained low quality answers, we could instead output "SORRY, I DON'T KNOW". An honest "I don't know" answer is better then a wrong answer. To quantitatively evaluate the scores on selective generation, we use Calibration-AUC and Selective-AUC as defined below.</p>
<p>Calibration-AUC AUC metric for a binary prediction task where the binary label is the correctness $h(\boldsymbol{x}, \hat{\boldsymbol{y}})$, and the prediction score is the confidence score $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$ [Kivlichan et al., 2021]. Since Calibration-AUC measures the ranking performance, it cannot be simply tricked using the post-hoc calibration heuristics such as the temperature scaling.</p>
<p>Selective generation curve and AUC Selective generation curve measures the correctness $h(\boldsymbol{x}, \hat{\boldsymbol{y}})$ as a function of abstention rate $\alpha \%$, where the samples are sorted by $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$ and samples with the lowest $\alpha \%$ scores are abstained [Ren et al., 2023b]. At $\alpha=0$ no sample is abstained, so the curve starts from the conventionally defined accuracy. As $\alpha$ increases, if the score is predictive of correctness, low quality samples will be abstained first, and the remaining samples will have higher overall quality. Therefore we expect the curve to increase. To quantitatively measure the performance, we compute the area under the selective generation curve, Selective-AUC.</p>
<p>Distinction to Expected Calibration Error (ECE) ECE [Guo et al., 2017] is commonly used to measure if the predictive probability value matches the ground truth accuracy. ECE computation is straightforward for categorical prediction. However, for sequence generation, even though it is possible to define sequence-level ECE [Zablotskaia et al., 2023], getting the ground truth is challenging. Also ECE can only be applied to probabilistic scores. The confidence scores we propose are not necessarily probabilities, so therefore ECE is not applicable there. In this study, we focus on a more general setting that apply to any confidence scores: assessing if the confidence score is predictive of the output quality. Therefore we use the calibration-AUC and selective generation instead of ECE.</p>
<h1>4 Experiments</h1>
<h3>4.1 Experiment setup</h3>
<p>LLMs PALM-2 LARGE is mainly used in our experiments. For each question, we sample $n=4$ answers at temperature 1.0. We de-duplicate the answers to reduce the chance of probability dispersion. We also consider GPT-3 (text-davinci-003) model for evaluation. Due to the OpenAI API limitation, we cannot evaluate all the methods and obtain complete results for GPT-3. We can neither evaluate methods on GPT-3.5 and GPT-4 models because OpenAI API does not provide output log-probabilities for them.</p>
<p>Benchmark datasets TruthfulQA [Lin et al., 2021] is a dataset for assessing model's ability to generate truthful answers against false belief or misconception. It contains 817 questions in the validation split. To label the quality of generated answers, we use the GPT-judge, which is a GPT-3 model fine-tuned on human feedback data, provided by Lin et al. [2021]. It is shown that GPT-judge has $90-95 \%$ accuracy in predicting human evaluations of truthfulness.</p>
<p>TL;DR is a summarization benchmark dataset mined from Reddit website [Völske et al., 2017]. It contains 15,240 examples in the test split. We randomly sampled 1000 examples to save inference cost. To label the quality of the generated summaries, we use a reward model fine-tuned on human feedback data, as used by [Zhao et al., 2023]. The prediction accuracy of human rating of the reward model is $71.34 \%$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison of different scores for the accuracy and calibration metrics on TruthfulQA for PALM-2 LARGE and GPT-3 models. The numbers are in percentage.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PALM-2 LARGE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">48.23</td>
<td style="text-align: center;">39.80</td>
<td style="text-align: center;">33.63</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">52.75</td>
<td style="text-align: center;">50.09</td>
<td style="text-align: center;">42.15</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">53.17</td>
<td style="text-align: center;">48.59</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">58.13</td>
<td style="text-align: center;">72.59</td>
<td style="text-align: center;">56.61</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">$\mathbf{5 9 . 1 2}$</td>
<td style="text-align: center;">$\underline{73.79}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 1 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\underline{59.00}$</td>
<td style="text-align: center;">68.78</td>
<td style="text-align: center;">55.70</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">73.76</td>
<td style="text-align: center;">57.38</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">58.14</td>
<td style="text-align: center;">$\mathbf{7 5 . 3 4}$</td>
<td style="text-align: center;">$\underline{58.10}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">40.50</td>
<td style="text-align: center;">49.76</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">42.06</td>
<td style="text-align: center;">50.22</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">$\mathbf{7 2 . 2 4}$</td>
<td style="text-align: center;">47.97</td>
<td style="text-align: center;">56.75</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">67.83</td>
<td style="text-align: center;">48.47</td>
<td style="text-align: center;">53.28</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\underline{68.48}$</td>
<td style="text-align: center;">$\underline{51.36}$</td>
<td style="text-align: center;">$\underline{55.28}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">$\mathbf{7 2 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 4 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
</tbody>
</table>
<h1>4.2 Results</h1>
<p>The performance of the different scores evaluated using accuracy, calibration-AUC, and selectiveAUC are shown in Table 1. It is clear to see that, sequence-level likelihood is not good for both accuracy and calibration. It has even below 0.5 AUC suggesting sequence likelihood is negatively correlated with correctness. Length normalization could improve the performance but AUC is still below 0.5 . The strategy of reducing sequence-level score to token-level scores via self-evaluation improve both the accuracy and calibration over sequence likelihood. Considering all metrics together, the hybrid strategy with NONE OF THE ABOVE added, achieves overall better performance.
Comparing the two strategies, Sample and Select and Sample and Eval, Sample and Select has decent accuracy, but suffers from the calibration metrics. Adding NONE OF THE ABOVE helps improve calibration. On the other hand, Sample and Eval is better on calibration metrics, but it has a bit lower accuracy. This trend is more clear in GPT-3. Therefore we propose the hybrid strategy to combine the best of both. The ROC curves for binary classification of correct and incorrect answers using different scores, and the selective generation curves can be found in Figure 3. Calibration-AUC and Selective-AUC are the area under the two curves respectively.
In addition, we show that self-evaluation is complementary to self-critique and revise, a technique to self-improve the answer quality [Bai et al., 2022]. We first apply that technique to improve each of the sampled answers. Then we compute the scores on the revised answers, instead of on the original answers. In Table 2, it is clear that on the revised answers, we see similar patterns that sequence-level scores are not well suited for selective generation, and the token-level scores achieves better performance.</p>
<p>Table 2: Self-critique and revise further improves the model's accuracy, calibration, and selective generation on TruthfulQA on PALM-2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">54.83</td>
<td style="text-align: center;">38.96</td>
<td style="text-align: center;">38.40</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">59.12</td>
<td style="text-align: center;">49.64</td>
<td style="text-align: center;">47.03</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">64.87</td>
<td style="text-align: center;">50.41</td>
<td style="text-align: center;">52.40</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">64.60</td>
<td style="text-align: center;">66.92</td>
<td style="text-align: center;">58.69</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">$\underline{66.34}$</td>
<td style="text-align: center;">70.55</td>
<td style="text-align: center;">$\mathbf{6 1 . 8 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\mathbf{6 6 . 7 1}$</td>
<td style="text-align: center;">64.69</td>
<td style="text-align: center;">59.44</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">64.87</td>
<td style="text-align: center;">$\underline{71.35}$</td>
<td style="text-align: center;">61.11</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">64.50</td>
<td style="text-align: center;">$\mathbf{7 2 . 7 2}$</td>
<td style="text-align: center;">$\underline{61.44}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ROC curves for binary classification and selective generation curves, evaluated on TruthFULQA. The left most point of the selective generation curves (abstention rate $\alpha=0$ ) is the accuracy reported in Table 1. The area under the ROC curve is calibration-AUC, and the area under the selective generation curve is selective-AUC.</p>
<h1>4.3 Self-evaluation improves calibration on TL;DR summarization</h1>
<p>TL;DR is a summarization benchmark dataset mined from Reddit website [Völske et al., 2017]. Evaluating the different scores on that dataset shows again that the sequence-level scores are not suitable for calibration. Self-evaluation based token-level scores improve the both accuracy and calibration performance (Table 3). Sample and Select has higher accuracy but lower calibration-AUC than Sample and Eval, and adding None of the above option helps to improve Calibration-AUC without sacrificing much the accuracy. Hybrid methods in general have decent performance.</p>
<p>Table 3: Comparison of different scores: accuracy and calibration on TL;DR for PaLM-2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">65.80</td>
<td style="text-align: center;">49.75</td>
<td style="text-align: center;">52.63</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">69.40</td>
<td style="text-align: center;">$\underline{53.20}$</td>
<td style="text-align: center;">56.93</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">46.65</td>
<td style="text-align: center;">54.68</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">$\mathbf{7 0 . 8 0}$</td>
<td style="text-align: center;">49.54</td>
<td style="text-align: center;">56.56</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">68.70</td>
<td style="text-align: center;">52.34</td>
<td style="text-align: center;">56.09</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">$\mathbf{5 5 . 1 9}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 9 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">$\underline{70.70}$</td>
<td style="text-align: center;">52.19</td>
<td style="text-align: center;">$\underline{57.56}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">$\mathbf{7 0 . 8 0}$</td>
<td style="text-align: center;">52.05</td>
<td style="text-align: center;">$\underline{57.55}$</td>
</tr>
</tbody>
</table>
<h3>4.4 Effect of position bias</h3>
<p>We assess the effect of position bias on the performance. We compare the vanilla setting where the answers are ordered by default, and the de-biased setting where the answer scores are averaged across all $n$ ! possible permutations. The difference on the performance is not that significant. Given</p>
<p>the de-bias process through shuffle and average is very computational expensive, we use the vanilla setting by default.</p>
<p>Table 4: Effect of position bias on metrics. The results are based on PALM-2 LARGE.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TruthFULQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, vanilla</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">53.17</td>
<td style="text-align: center;">48.59</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, de-biased</td>
<td style="text-align: center;">58.87</td>
<td style="text-align: center;">52.13</td>
<td style="text-align: center;">48.58</td>
</tr>
<tr>
<td style="text-align: left;">TL;DR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, vanilla</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">46.65</td>
<td style="text-align: center;">54.68</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, de-biased</td>
<td style="text-align: center;">70.70</td>
<td style="text-align: center;">43.94</td>
<td style="text-align: center;">53.86</td>
</tr>
</tbody>
</table>
<h1>5 Related work</h1>
<p>The calibration of LLMs on multiple choice question answer tasks is studied in Kadavath et al. [2022]. Robinson et al. [2022] show that the sequence level probability is worse than the token-level probability (e.g. A, B, C, etc) for predicting the correctness. But those studies use the multiple choice question answering datasets where the answers are pre-defined and not generated from LLMs. Our work focuses on the calibration of free-form generation tasks. We transform free-form generation to multiple choice task by generating answer candidates by itself. Another distinction to [Kadavath et al., 2022] is that we care more on the ranking performance measured by AUC than the exact value match to ground truth probability measured by ECE.
In terms of estimating language models' confidence or uncertainty, Tian et al. [2023], Lin et al. [2022] propose to ask model to express uncertainty in words along with the generated answer, but it is shown that LLMs often exhibit a high degree of overconfidence when verbalizing their confidence [Xiong et al., 2023]. Kuhn et al. [2023] propose to use semantic entropy among a set of sampled answers to estimate model's uncertainty. The semantic similarity is inferred using a separate natural language inference classification system (NLI). Cole et al. [2023] find the degree of repetition in sampled answers is a good score for selectively answering ambiguous questions. The distinctions between our work and the above are that, we focus on estimating the confidence of long sequence free-form generation tasks, where the repetition can not be easily measured. Also, we are interested in zero-shot self-evaluation based scores, without utilized a separate model for inference. The true/ false evaluation method proposed by Kadavath et al. [2022] is one of them. In our work, we compare this score with several other scores and have a comprehensive assessment on selective generation of free-form generation tasks
Prior studies have proposed generating multiple candidate responses for free-form generation tasks and then selecting the best. The final answer is selected using a variety of methods, including: (1) simple sequence likelihood [Adiwardana et al., 2020], (2) ranking model trained on human preference data [Nichols et al., 2020], (3) self-consistency i.e. if an answer is the most consensus one [Wang et al., 2022, Chen et al., 2023] and (4) models' self-evaluation ability to choose the final response based on its own evaluation of the responses [Ren et al., 2023a]. However, the focus of most prior work except for [Ren et al., 2023a] are on improving accuracy, not on confidence estimation or calibration. [Ren et al., 2023a] is similar to our work in the sense that it not only proposes to generate multiple options and then ask the model to choose one, but also estimate uncertainty to ask for clarification. However they focus on robotics planning, while we focus on more general question answer. Also, they directly use the multiple choice score output, while we identified the position bias and probability dispersion problems in the scores, and propose hybrid method to address them</p>
<h2>6 Discussion</h2>
<p>We show that although generic sequence-level scores are not well suited for selective generation (even negatively correlated with the the quality) for free-form generation, asking the model again to self-evaluate could reduce the sequence-level score to token-levels scores, improving quality calibration. Self-evaluation is though at the cost of increasing inference time by 1 or 2 (hybrid mode) times. Alternative to this post-hoc method, how to improve the quality calibration of the sequence-level score during training and finetuning is one of our future work.</p>
<h1>Acknowledgements</h1>
<p>We would like to thank Denny Zhou, Zelda Mariet, Sharat Chikkerur, Jasper Snoek, and Alexander D'Amour from Google DeepMind for helpful discussions for insightful discussion and providing valuable feedback for this work. We would also like to express our appreciation towards Lyric Doshi, Xuezhi Wang, and Michael W. Dusenberry from Google DeepMind for their technical support.</p>
<h2>References</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like opendomain chatbot. arXiv preprint arXiv:2001.09977, 2020.</p>
<p>Ayushi Agarwal, Nisarg Patel, Neeraj Varshney, Mihir Parmar, Pavan Mallina, Aryan Bhavin Shah, Srihari Raju Sangaraju, Tirth Patel, Nihar Thakkar, and Chitta Baral. Can NLP models' identify', 'distinguish', and'justify'questions that don't have a definitive answer? arXiv preprint arXiv:2309.04635, 2023.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Jeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions. arXiv preprint arXiv:2305.14613, 2023.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321-1330. PMLR, 2017.</p>
<p>Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. arXiv preprint arXiv:1911.11132, 2019.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.</p>
<p>Ian D Kivlichan, Zi Lin, Jeremiah Liu, and Lucy Vasserman. Measuring and improving modelmoderator collaboration using uncertainty estimation. arXiv preprint arXiv:2107.04212, 2021.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.</p>
<p>Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. Brio: Bringing order to abstractive summarization. arXiv preprint arXiv:2203.16804, 2022.</p>
<p>Eric Nichols, Leo Gao, and Randy Gomez. Collaborative storytelling with large-scale neural language models, 2020.</p>
<p>OpenAI. GPT-4 technical report. arXiv, pages 2303-08774, 2023.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback, 2022. URL https://arxiv. org/abs/2203.02155, 13, 2022.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. arXiv preprint arXiv:2307.01928, 2023a.</p>
<p>Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter J. Liu. Out-of-distribution detection and selective generation for conditional language models, 2023b.</p>
<p>Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language models for multiple choice question answering. arXiv preprint arXiv:2210.12353, 2022.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.</p>
<p>Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023.</p>
<p>Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, 2017.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.</p>
<p>Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. arXiv preprint arXiv:2306.13063, 2023.</p>
<p>Polina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, and Jeremiah Liu. On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study. arXiv preprint arXiv:2304.08653, 2023.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.</p>
<p>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models' selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023.</p>
<h1>A Prompts used in the study</h1>
<p>In the following task, you will receive a question. Please generate one factual, informative, unbiased, and safe answer to the question.
$==$
Question:
What happens to you if you eat watermelon seeds?
$==$
Answer:
Watermelon seeds will pass through your digestive system without being digested and will eventually be excreted in your stool.
$==$
Question:
(question)
$==$
Answer:</p>
<p>Figure 4: The prompt for sampling an answer in TruthfulQA.</p>
<p>In the following task, you will receive a question with multiple answers. Please choose the most factual, informative, unbiased, and safe answer. A honest "I don't know" answer is preferable to a wrong answer. Only output the capitalized alphabet letter corresponding to the answer.
$==$
Question:
(question)
$==$
Candidate answers:
(answer_list)
$==$
Letter:</p>
<p>Figure 5: The prompt for multi-choice selection in TruthfulQA.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: The prompt for True/False evaluation in TruthfulQA.</p>
<p>In the following task, you will receive a question, a few candidate answers, and a proposed answer. $==$
Question:
(question)
$==$
Candidate answers:
(answer_list)
The above candidate answers can be either true or false. Please judge if the following proposed answer is factual, informative, unbiased, and safe.
$==$
Answer:
(answer)
$==$
Is the above answer factual, informative, unbiased, and safe?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 7: Prompt for True/False evaluation with other candidates in TruthFULQA</p>
<p>In the following task, you will receive a question and an answer. Please generate an explanation why this answer would be non-factual. Say None if you don't have any critique.
$==$
Question:
What happens to you if you eat watermelon seeds?
$==$
Answer:
Watermelon seeds can cause a watermelon to grow in your stomach.
$==$
This answer could be wrong because the stomach is an acidic environment that is not suitable for watermelon seeds to grow.
$==$
Question:
(question)
$==$
Answer:
(answer)
$==$
This answer could be wrong because</p>
<p>Figure 8: The prompt for generating a critique in TruthfulQA.</p>
<p>In the following task, you will receive a question, an answer, and a critique that identifies hallucinated statements and explains why they are non-factual. Please rewrite the answer so that it is factual, informative, unbiased, and safe.
$==$
Question:
When was Ronald Reagan assassinated?
$==$
Answer:
Ronald Reagan was assassinated by John Hinckley Jr. in Washington, D.C., on March 30, 1981
$==$
Critique:
Ronald Reagan was shot by Hinkley but he survived, so he was not assassinated.
$==$
New answer:
Ronald Reagan was not assassinated. He was shot and wounded by John Hinckley Jr. in Washington, D.C., on March 30, 1981, but he survived the attack.
$==$
Question:
(question)
$==$
Answer:
(answer)
$==$
Critique:
(critique)
$==$
New answer:</p>
<p>Figure 9: The prompt for generating a revised answer given the critique in TruthFULQA.</p>
<p>In the following task, you will receive a text. Please generate a summary TDLR.
$==$
Text:
(question)
$==$
TLDR:</p>
<p>Figure 10: The prompt for sampling an answer in TL;DR.</p>
<p>In the following task, you will receive a text and a few candidate summaries. Please choose the most concise and comprehensive summary. Only output the capitalized alphabet letter corresponding to the answer.
$==$
Text:
(question)
$==$
Candidate summaries:
(answer_list)
$==$
Letter:</p>
<p>Figure 11: The prompt for multi-choice selection in TL;DR.</p>
<p>In the following task, you will receive a text and a proposed summary. Please judge if the summary is concise and comprehensive.
$==$
Text:
(question)
$==$
Summary:
(answer)
$==$
Is the above summary concise and comprehensive?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 12: The prompt for pointwise evaluation in TL;DR.</p>
<p>In the following task, you will receive a text, a few candidate summaries, and a proposed summary.
$==$
Text:
{question}
$=$
Candidate summaries:
{answer_list}
The above candidate summaries can be either good or bad. Please judge if the following proposed summary is concise and comprehensive.
$==$
Summary:
{answer}
$=$
Is the above summary concise and comprehensive?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 13: The prompt for pointwise evaluation with other candidates in TL;DR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For GPT-3 model, the API can only output log-probability for up to 5 most likely tokens. Because of this limitation, a few methods cannot be evaluated on GPT-3. For example, the most likely tokens in the multi-response evaluation setting are not necessarily A, B, C etc., but the most likely letter and its variants such as 'A', ' $\mathrm{A}^{\prime}$, 'A', or 'A'n'. Therefore the maximum token prediction and its log-probability are always available, but the log-probability for a specific token such as 'E' for the "None of the above" answer is not available.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
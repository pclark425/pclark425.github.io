<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6473 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6473</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6473</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-272525276</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.05591v3.pdf" target="_blank">MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation</a></p>
                <p><strong>Paper Abstract:</strong> Processing long contexts presents a significant challenge for large language models (LLMs). While recent advancements allow LLMs to handle much longer contexts than before (e.g., 32K or 128K tokens), it is computationally expensive and can still be insufficient for many applications. Retrieval-Augmented Generation (RAG) is considered a promising strategy to address this problem. However, conventional RAG methods face inherent limitations because of two underlying requirements: 1) explicitly stated queries, and 2) well-structured knowledge. These conditions, however, do not hold in general long-context processing tasks. In this work, we propose MemoRAG, a novel RAG framework empowered by global memory-augmented retrieval. MemoRAG features a dual-system architecture. First, it employs a light but long-range system to create a global memory of the long context. Once a task is presented, it generates draft answers, providing useful clues for the retrieval tools to locate relevant information within the long context. Second, it leverages an expensive but expressive system, which generates the final answer based on the retrieved information. Building upon this fundamental framework, we realize the memory module in the form of KV compression, and reinforce its memorization and cluing capacity from the Generation quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG achieves superior performances across a variety of long-context evaluation tasks, not only complex scenarios where traditional RAG methods struggle, but also simpler ones where RAG is typically applied.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6473.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6473.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoRAG: Global Memory-Enhanced Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-system RAG framework that builds a compact, global KV memory from a very long input context using a lightweight memory model to generate draft answer 'clues' which guide retrieval; a heavier generator then produces the final answer from retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-stage agent: (1) a memory model (lightweight, long-range) compresses the long context into a global KV memory and, when given a task, generates draft answers (clues); (2) a generator LLM uses the clues to retrieve evidence from the original long context and generate the final answer. The memory is implemented as KV-compressed memory tokens and is trained via pretraining, SFT and a reinforcement-style RLGF stage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Memory model default: Mistral-7B-instruct-v0.2-32K (7B) used for memory; generator: Phi-3-mini-128K (≈3B) by default; experiments also evaluate Qwen2-7B and Llama3.1-8B-128K as underlying LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Compact KV-compressed global memory (memory-token based KV cache); also supports a light global memory variant (full/sparse KV cache).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Key-value pairs (KV cache) for memory tokens: [K_m_cache, V_m_cache] representing compressed summaries of context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Memory model forms KV memory during prefill (insert memory tokens per context window); at query time the memory model generates draft answer clues (y = Θ_mem(q|θ_mem)); retriever Γ uses these clues as queries to retrieve passages; memory is updated/formed by concatenating per-window memory K/V (and optimized via RLGF to favor clues that yield high final-answer rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LongBench, InfiniteBench, UltraDomain (examples: NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, MuSiQue, GovReport, En.SUM, MultiNews, En.QA, and 20 UltraDomain datasets across finance, law, textbooks, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context question answering, query-focused summarization, long-document understanding/aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper reports consistent gains over baselines across evaluated datasets; example from Table 1: NarrativeQA entry shows MemoRAG (27.5) > Full long-LLM baseline (21.4) (dataset-specific metric as reported in Table 1). Overall MemoRAG outperforms standard RAG, advanced RAG variants (HyDE, RQ-RAG, GraphRAG), and direct full-context long LLMs on the listed benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baselines without this global memory include: Full (direct full-context long LLMs) and standard RAG pipelines. Example from Table 1: Full (direct long LLM) NarrativeQA 21.4; standard RAG (BGE-M3 retriever) NarrativeQA ~20.3 (table rows). (Metrics are dataset-specific as reported in Table 1.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Dataset-specific evaluation metrics (varies by dataset; metrics reported in Table 1 and appendix of paper).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Indexing (memory formation) latency is higher than standard RAG due to global memory construction, and retrieval latency increases because the memory model must generate clues; however MemoRAG is faster and uses less GPU memory than full long-LLM prefill. GPU memory: MemoRAG and standard RAG process 128K contexts with under ~60 GiB, while long LLMs require substantially more. Compression ratio (ρ) trades off semantic richness vs KV memory: smaller ρ preserves semantics but uses more KV cache; high ρ increases efficiency but reduces recall/quality (performance declines as ρ increases, stabilizing around ρ=32).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports that light/global-KV baselines (sparse-attention/light memory) are inferior; very high compression ratios degrade performance; MemoRAG's memory formation step makes indexing slower than standard RAG. No specific dataset type is reported where MemoRAG consistently underperforms baselines, but ablations show removing compact memory or RLGF degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Qian H., Liu Z., Zhang P., Mao K., Lian D., Dou Z., Huang T., 2025. MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6473.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6473.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Compact Global Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compact Global Memory (KV‑compressible memory tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The memory-module design used by MemoRAG that inserts a small number of learned memory tokens per context window and stores only the KV projections for those tokens, producing a compressed global KV memory that scales to much longer contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoRAG memory module (compact)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture that inserts k memory tokens after each context window and maintains only the memory tokens' KV cache (K_m_cache, V_m_cache) while discarding regular-token KV caches, achieving configurable compression ratios to reduce KV memory while preserving essential semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Underlying LLM size varies; newly-initialized memory parameters depend on base model (example: ~1.1B new params when using Qwen2-7B-instruct as base). Compression ratios ρ ∈ {4,8,16,32,64} are evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>KV-compressed, token-based global KV cache (compact episodic/global memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Compact key-value pairs for memory tokens: [K_m_cache, V_m_cache], concatenated across windows to form global memory θ_mem.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Memory tokens are attended to alongside current queries (attention uses [Q; Q_m] against K including K_m_cache); retriever uses draft clues generated from this compact memory. Memory is created by concatenating per-window memory KV pairs and is read via attention and used to generate draft clues.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same long-context benchmarks as MemoRAG (LongBench, InfiniteBench, UltraDomain).</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context QA and summarization / long-document understanding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Ablations show compact global memory outperforms light global memory and improves end-to-end MemoRAG performance; performance declines gradually with higher compression but stabilizes (empirically) at ρ=32.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>When replaced with light global memory or omitted (zero-shot/no training) performance degrades (ablation results in Figures 4 and 5), exact per-dataset numeric deltas for the compact-vs-light comparison are reported in the paper's ablation figures rather than in a single table.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Varies by dataset (paper reports dataset-specific metrics in Table 1; ablation figures show relative gains).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Compression ratio provides a clear trade-off: lower compression (small ρ) retains richer semantics but uses more KV cache (higher GPU memory); higher compression (large ρ) reduces memory cost and enables longer context handling but reduces semantic fidelity and performance until stabilizing at an observed point (ρ≈32).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Excessive compression (large ρ) reduces semantic richness and harms task performance; requires separate pretraining and SFT for memory parameters (added engineering/training cost); initial global-memory formation increases indexing latency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Qian et al., 2025. (component introduced and evaluated in the MemoRAG paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6473.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6473.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Light Global Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Light Global Memory (full KV cache / sparse-attention long-context techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline memory option that reuses the full key-value cache or leverages light long-context techniques (e.g., sparse attention via MInference or hierarchical attention via SelfExtend) to extend context, but without KV compression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Light global memory configuration (MemoRAG baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses the full KV cache produced during prefill or uses sparse/bi-level attention techniques (MInference, SelfExtend) to extend the effective context window, keeping more of the raw KV information but without compression.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Depends on the underlying LLM (examples used: Mistral-7B-32K with SelfExtend; Phi/Llama variants with MInference).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Full KV cache or sparse-attention long-context cache (non-compressed KV memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Full key-value pairs [K, V] for all tokens in the native context windows (or sparse approximations thereof).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Reuses the prefill KV cache during decoding and may rely on sparse/bi-level attention mechanisms (as implemented by MInference or SelfExtend) to scale to longer inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as a baseline on LongBench, InfiniteBench, UltraDomain in ablation/comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context processing (QA, summarization, document understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Empirically inferior to Compact Global Memory in ablations: the paper reports light memory performs worse due to native context limits and sparse-attention semantic loss (exact numbers in ablation figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared against compact memory and standard RAG; compact memory gives higher performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Varies by dataset; comparisons shown in paper ablations and main table.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Light memory is easier to implement but constrained by native context size; sparse attention reduces compute but can compromise semantic completeness; maintaining full KV caches consumes substantial GPU memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Constrained by LLM native context window (limits extreme long-context handling); sparse attention implementations can lose semantic completeness and underperform compact memory in retrieval-guided tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Qian et al., 2025. (baseline described and evaluated within MemoRAG paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6473.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6473.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLGF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning with Generation Feedback (RLGF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An optimization method that assigns rewards to draft answer clues based on their usefulness for producing high-quality final answers, and uses a preference-ranking loss to reinforce the memory model to generate better clues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RLGF (training/optimization applied to MemoRAG memory model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A reinforcement/preference-based training stage: generate multiple candidate clues, evaluate their downstream contribution to final-answer quality (reward R(y)), then apply a ranking loss to push the memory model toward generating clues that yield higher end-to-end rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Applied to newly-initialized memory parameters (size dependent on base model); RLGF stage uses a sampled set of 2,000 instances from the SFT dataset in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Optimization/learning mechanism for the memory module's outputs (not a distinct memory storage type).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Does not change stored KV representation; optimizes generation policy for clues produced from the compact memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Generates multiple candidate clues per instance, retrieves evidence for each, measures downstream final-answer reward, forms preference pairs (y+, y-) and applies a margin-ranking loss L_RLGF to update memory parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used during training for long-context tasks that MemoRAG targets (SFT samples created from long documents; evaluated on LongBench/InfiniteBench/UltraDomain).</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Training optimization for long-context QA/summarization pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Ablation study shows RLGF improves end-to-end answer quality over pretraining+SFT alone (paper reports improved results in Figures 4a/4b), exact numeric deltas are shown in ablation figures rather than a single aggregated table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without RLGF (i.e., after only pretraining+SFT), the memory module yields lower downstream performance per ablation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Varies by dataset; improvements shown in ablation figures comparing Zero-shot / Pretrain / SFT / RLGF stages.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Requires constructing reward signals from downstream generation (thus needs additional retrieval+generation during training) and human-reviewed preference data; uses a modestly sized RLGF dataset (2,000 instances) which adds training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Effectiveness depends on the quality of the reward signals and the SFT-generated clue candidates; constructing reliable end-to-end rewards is resource-intensive and may not scale trivially to very large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6473.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6473.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MInference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparse-attention prefill technique that speeds up the KV prefill stage and serves as a light memory baseline in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MInference (sparse-attention prefill technique)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Applies dynamic sparse attention to accelerate the prefill stage of long-context LLMs, enabling faster reuse of KV caches for long inputs; used as a light global-memory baseline in MemoRAG experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Technique applicable to multiple LLM sizes; used with models in baseline comparisons (e.g., Mistral/Phi/Llama variants).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Sparse-attention KV cache (light memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Key-value pairs produced in prefill stage; sparsified for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Dynamic sparse attention during prefill and reuse of KV cache in decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as baseline on LongBench, InfiniteBench, UltraDomain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context processing baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in Table 1 as a baseline (example value seen in table: NarrativeQA ~20.7 for MInference row), generally inferior to MemoRAG and sometimes slightly below Full baseline on certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>N/A (this is a method variant used as a baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Varies by dataset (see Table 1 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Gains in prefill speed and reduced computation, but sparse attention may lose semantic completeness and limit effective long-context comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sparse attention can compromise semantic completeness; limited adaptability to extremely long contexts compared to compact memory compression.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6473.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6473.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfExtend</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-level hierarchical attention technique to extend an LLM's effective context window without full model tuning; used as a light-memory baseline in MemoRAG evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SelfExtend (hierarchical attention long-context extension)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Creates a bi-level hierarchical attention mechanism to expand the native context window of an LLM without full tuning; in the MemoRAG paper it is used as a comparison baseline for light global memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Applied to underlying LLMs (examples in paper: Mistral-7B used with SelfExtend to extend context).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Hierarchical-attention KV cache (light memory extension)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>KV caches organized with bi-level hierarchical attention across chunks/windows.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Bi-level hierarchical attention to connect and attend to tokens across extended context windows; KV caches are maintained to support decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as baseline on LongBench, InfiniteBench, UltraDomain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context processing baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper as a baseline (table entries show SelfExtend row); empirical results indicate SelfExtend improves full-LM context handling but is outperformed by MemoRAG compact-memory approach on many long-context tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>N/A (this is an extension technique used as a baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Varies by dataset (reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Enables longer contexts without full model tuning but can still be limited by inherent semantic completeness and GPU memory costs compared to compressed memory designs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>While extending context window, hierarchical approaches can still be less efficient or lose some semantics compared to compact memory; paper's ablations find it inferior to compact memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6473.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6473.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related-work paper (referenced) proposing a neurobiologically inspired long-term memory mechanism for LLMs; mentioned in the MemoRAG related work as another attempt to endow LLMs with external/memory-like components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HippoRAG (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-augmented LLM architecture inspired by biological long-term memory mechanisms (referenced in MemoRAG related work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Neurobiologically inspired long-term memory (referenced paper’s approach)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-term memory augmentation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Referenced in MemoRAG related work (Jiménez Bernal et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention <em>(Rating: 2)</em></li>
                <li>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning <em>(Rating: 2)</em></li>
                <li>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models <em>(Rating: 2)</em></li>
                <li>A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts <em>(Rating: 2)</em></li>
                <li>In-Context Retrieval-Augmented Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6473",
    "paper_id": "paper-272525276",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "MemoRAG",
            "name_full": "MemoRAG: Global Memory-Enhanced Retrieval-Augmented Generation",
            "brief_description": "A dual-system RAG framework that builds a compact, global KV memory from a very long input context using a lightweight memory model to generate draft answer 'clues' which guide retrieval; a heavier generator then produces the final answer from retrieved evidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemoRAG",
            "agent_description": "Two-stage agent: (1) a memory model (lightweight, long-range) compresses the long context into a global KV memory and, when given a task, generates draft answers (clues); (2) a generator LLM uses the clues to retrieve evidence from the original long context and generate the final answer. The memory is implemented as KV-compressed memory tokens and is trained via pretraining, SFT and a reinforcement-style RLGF stage.",
            "model_size": "Memory model default: Mistral-7B-instruct-v0.2-32K (7B) used for memory; generator: Phi-3-mini-128K (≈3B) by default; experiments also evaluate Qwen2-7B and Llama3.1-8B-128K as underlying LLMs.",
            "memory_used": true,
            "memory_type": "Compact KV-compressed global memory (memory-token based KV cache); also supports a light global memory variant (full/sparse KV cache).",
            "memory_representation": "Key-value pairs (KV cache) for memory tokens: [K_m_cache, V_m_cache] representing compressed summaries of context windows.",
            "memory_access_mechanism": "Memory model forms KV memory during prefill (insert memory tokens per context window); at query time the memory model generates draft answer clues (y = Θ_mem(q|θ_mem)); retriever Γ uses these clues as queries to retrieve passages; memory is updated/formed by concatenating per-window memory K/V (and optimized via RLGF to favor clues that yield high final-answer rewards).",
            "task_name": "LongBench, InfiniteBench, UltraDomain (examples: NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, MuSiQue, GovReport, En.SUM, MultiNews, En.QA, and 20 UltraDomain datasets across finance, law, textbooks, etc.)",
            "task_category": "Long-context question answering, query-focused summarization, long-document understanding/aggregation",
            "performance_with_memory": "Paper reports consistent gains over baselines across evaluated datasets; example from Table 1: NarrativeQA entry shows MemoRAG (27.5) &gt; Full long-LLM baseline (21.4) (dataset-specific metric as reported in Table 1). Overall MemoRAG outperforms standard RAG, advanced RAG variants (HyDE, RQ-RAG, GraphRAG), and direct full-context long LLMs on the listed benchmarks.",
            "performance_without_memory": "Baselines without this global memory include: Full (direct full-context long LLMs) and standard RAG pipelines. Example from Table 1: Full (direct long LLM) NarrativeQA 21.4; standard RAG (BGE-M3 retriever) NarrativeQA ~20.3 (table rows). (Metrics are dataset-specific as reported in Table 1.)",
            "has_comparative_results": true,
            "performance_metric": "Dataset-specific evaluation metrics (varies by dataset; metrics reported in Table 1 and appendix of paper).",
            "tradeoffs_reported": "Indexing (memory formation) latency is higher than standard RAG due to global memory construction, and retrieval latency increases because the memory model must generate clues; however MemoRAG is faster and uses less GPU memory than full long-LLM prefill. GPU memory: MemoRAG and standard RAG process 128K contexts with under ~60 GiB, while long LLMs require substantially more. Compression ratio (ρ) trades off semantic richness vs KV memory: smaller ρ preserves semantics but uses more KV cache; high ρ increases efficiency but reduces recall/quality (performance declines as ρ increases, stabilizing around ρ=32).",
            "limitations_or_failure_cases": "Paper reports that light/global-KV baselines (sparse-attention/light memory) are inferior; very high compression ratios degrade performance; MemoRAG's memory formation step makes indexing slower than standard RAG. No specific dataset type is reported where MemoRAG consistently underperforms baselines, but ablations show removing compact memory or RLGF degrades performance.",
            "citation": "Qian H., Liu Z., Zhang P., Mao K., Lian D., Dou Z., Huang T., 2025. MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation.",
            "uuid": "e6473.0",
            "source_info": {
                "paper_title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Compact Global Memory",
            "name_full": "Compact Global Memory (KV‑compressible memory tokens)",
            "brief_description": "The memory-module design used by MemoRAG that inserts a small number of learned memory tokens per context window and stores only the KV projections for those tokens, producing a compressed global KV memory that scales to much longer contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemoRAG memory module (compact)",
            "agent_description": "Architecture that inserts k memory tokens after each context window and maintains only the memory tokens' KV cache (K_m_cache, V_m_cache) while discarding regular-token KV caches, achieving configurable compression ratios to reduce KV memory while preserving essential semantics.",
            "model_size": "Underlying LLM size varies; newly-initialized memory parameters depend on base model (example: ~1.1B new params when using Qwen2-7B-instruct as base). Compression ratios ρ ∈ {4,8,16,32,64} are evaluated.",
            "memory_used": true,
            "memory_type": "KV-compressed, token-based global KV cache (compact episodic/global memory)",
            "memory_representation": "Compact key-value pairs for memory tokens: [K_m_cache, V_m_cache], concatenated across windows to form global memory θ_mem.",
            "memory_access_mechanism": "Memory tokens are attended to alongside current queries (attention uses [Q; Q_m] against K including K_m_cache); retriever uses draft clues generated from this compact memory. Memory is created by concatenating per-window memory KV pairs and is read via attention and used to generate draft clues.",
            "task_name": "Same long-context benchmarks as MemoRAG (LongBench, InfiniteBench, UltraDomain).",
            "task_category": "Long-context QA and summarization / long-document understanding",
            "performance_with_memory": "Ablations show compact global memory outperforms light global memory and improves end-to-end MemoRAG performance; performance declines gradually with higher compression but stabilizes (empirically) at ρ=32.",
            "performance_without_memory": "When replaced with light global memory or omitted (zero-shot/no training) performance degrades (ablation results in Figures 4 and 5), exact per-dataset numeric deltas for the compact-vs-light comparison are reported in the paper's ablation figures rather than in a single table.",
            "has_comparative_results": true,
            "performance_metric": "Varies by dataset (paper reports dataset-specific metrics in Table 1; ablation figures show relative gains).",
            "tradeoffs_reported": "Compression ratio provides a clear trade-off: lower compression (small ρ) retains richer semantics but uses more KV cache (higher GPU memory); higher compression (large ρ) reduces memory cost and enables longer context handling but reduces semantic fidelity and performance until stabilizing at an observed point (ρ≈32).",
            "limitations_or_failure_cases": "Excessive compression (large ρ) reduces semantic richness and harms task performance; requires separate pretraining and SFT for memory parameters (added engineering/training cost); initial global-memory formation increases indexing latency.",
            "citation": "Qian et al., 2025. (component introduced and evaluated in the MemoRAG paper)",
            "uuid": "e6473.1",
            "source_info": {
                "paper_title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Light Global Memory",
            "name_full": "Light Global Memory (full KV cache / sparse-attention long-context techniques)",
            "brief_description": "A baseline memory option that reuses the full key-value cache or leverages light long-context techniques (e.g., sparse attention via MInference or hierarchical attention via SelfExtend) to extend context, but without KV compression.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Light global memory configuration (MemoRAG baseline)",
            "agent_description": "Uses the full KV cache produced during prefill or uses sparse/bi-level attention techniques (MInference, SelfExtend) to extend the effective context window, keeping more of the raw KV information but without compression.",
            "model_size": "Depends on the underlying LLM (examples used: Mistral-7B-32K with SelfExtend; Phi/Llama variants with MInference).",
            "memory_used": true,
            "memory_type": "Full KV cache or sparse-attention long-context cache (non-compressed KV memory)",
            "memory_representation": "Full key-value pairs [K, V] for all tokens in the native context windows (or sparse approximations thereof).",
            "memory_access_mechanism": "Reuses the prefill KV cache during decoding and may rely on sparse/bi-level attention mechanisms (as implemented by MInference or SelfExtend) to scale to longer inputs.",
            "task_name": "Used as a baseline on LongBench, InfiniteBench, UltraDomain in ablation/comparisons.",
            "task_category": "Long-context processing (QA, summarization, document understanding)",
            "performance_with_memory": "Empirically inferior to Compact Global Memory in ablations: the paper reports light memory performs worse due to native context limits and sparse-attention semantic loss (exact numbers in ablation figures).",
            "performance_without_memory": "Compared against compact memory and standard RAG; compact memory gives higher performance.",
            "has_comparative_results": true,
            "performance_metric": "Varies by dataset; comparisons shown in paper ablations and main table.",
            "tradeoffs_reported": "Light memory is easier to implement but constrained by native context size; sparse attention reduces compute but can compromise semantic completeness; maintaining full KV caches consumes substantial GPU memory.",
            "limitations_or_failure_cases": "Constrained by LLM native context window (limits extreme long-context handling); sparse attention implementations can lose semantic completeness and underperform compact memory in retrieval-guided tasks.",
            "citation": "Qian et al., 2025. (baseline described and evaluated within MemoRAG paper)",
            "uuid": "e6473.2",
            "source_info": {
                "paper_title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RLGF",
            "name_full": "Reinforcement Learning with Generation Feedback (RLGF)",
            "brief_description": "An optimization method that assigns rewards to draft answer clues based on their usefulness for producing high-quality final answers, and uses a preference-ranking loss to reinforce the memory model to generate better clues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RLGF (training/optimization applied to MemoRAG memory model)",
            "agent_description": "A reinforcement/preference-based training stage: generate multiple candidate clues, evaluate their downstream contribution to final-answer quality (reward R(y)), then apply a ranking loss to push the memory model toward generating clues that yield higher end-to-end rewards.",
            "model_size": "Applied to newly-initialized memory parameters (size dependent on base model); RLGF stage uses a sampled set of 2,000 instances from the SFT dataset in the paper.",
            "memory_used": true,
            "memory_type": "Optimization/learning mechanism for the memory module's outputs (not a distinct memory storage type).",
            "memory_representation": "Does not change stored KV representation; optimizes generation policy for clues produced from the compact memory.",
            "memory_access_mechanism": "Generates multiple candidate clues per instance, retrieves evidence for each, measures downstream final-answer reward, forms preference pairs (y+, y-) and applies a margin-ranking loss L_RLGF to update memory parameters.",
            "task_name": "Used during training for long-context tasks that MemoRAG targets (SFT samples created from long documents; evaluated on LongBench/InfiniteBench/UltraDomain).",
            "task_category": "Training optimization for long-context QA/summarization pipelines",
            "performance_with_memory": "Ablation study shows RLGF improves end-to-end answer quality over pretraining+SFT alone (paper reports improved results in Figures 4a/4b), exact numeric deltas are shown in ablation figures rather than a single aggregated table.",
            "performance_without_memory": "Without RLGF (i.e., after only pretraining+SFT), the memory module yields lower downstream performance per ablation experiments.",
            "has_comparative_results": true,
            "performance_metric": "Varies by dataset; improvements shown in ablation figures comparing Zero-shot / Pretrain / SFT / RLGF stages.",
            "tradeoffs_reported": "Requires constructing reward signals from downstream generation (thus needs additional retrieval+generation during training) and human-reviewed preference data; uses a modestly sized RLGF dataset (2,000 instances) which adds training complexity.",
            "limitations_or_failure_cases": "Effectiveness depends on the quality of the reward signals and the SFT-generated clue candidates; constructing reliable end-to-end rewards is resource-intensive and may not scale trivially to very large datasets.",
            "uuid": "e6473.3",
            "source_info": {
                "paper_title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MInference",
            "name_full": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
            "brief_description": "A sparse-attention prefill technique that speeds up the KV prefill stage and serves as a light memory baseline in the paper's comparisons.",
            "citation_title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
            "mention_or_use": "use",
            "agent_name": "MInference (sparse-attention prefill technique)",
            "agent_description": "Applies dynamic sparse attention to accelerate the prefill stage of long-context LLMs, enabling faster reuse of KV caches for long inputs; used as a light global-memory baseline in MemoRAG experiments.",
            "model_size": "Technique applicable to multiple LLM sizes; used with models in baseline comparisons (e.g., Mistral/Phi/Llama variants).",
            "memory_used": true,
            "memory_type": "Sparse-attention KV cache (light memory)",
            "memory_representation": "Key-value pairs produced in prefill stage; sparsified for efficiency.",
            "memory_access_mechanism": "Dynamic sparse attention during prefill and reuse of KV cache in decoding.",
            "task_name": "Used as baseline on LongBench, InfiniteBench, UltraDomain.",
            "task_category": "Long-context processing baseline",
            "performance_with_memory": "Reported in Table 1 as a baseline (example value seen in table: NarrativeQA ~20.7 for MInference row), generally inferior to MemoRAG and sometimes slightly below Full baseline on certain tasks.",
            "performance_without_memory": "N/A (this is a method variant used as a baseline).",
            "has_comparative_results": true,
            "performance_metric": "Varies by dataset (see Table 1 in paper).",
            "tradeoffs_reported": "Gains in prefill speed and reduced computation, but sparse attention may lose semantic completeness and limit effective long-context comprehension.",
            "limitations_or_failure_cases": "Sparse attention can compromise semantic completeness; limited adaptability to extremely long contexts compared to compact memory compression.",
            "uuid": "e6473.4",
            "source_info": {
                "paper_title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "SelfExtend",
            "name_full": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
            "brief_description": "A bi-level hierarchical attention technique to extend an LLM's effective context window without full model tuning; used as a light-memory baseline in MemoRAG evaluations.",
            "citation_title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
            "mention_or_use": "use",
            "agent_name": "SelfExtend (hierarchical attention long-context extension)",
            "agent_description": "Creates a bi-level hierarchical attention mechanism to expand the native context window of an LLM without full tuning; in the MemoRAG paper it is used as a comparison baseline for light global memory.",
            "model_size": "Applied to underlying LLMs (examples in paper: Mistral-7B used with SelfExtend to extend context).",
            "memory_used": true,
            "memory_type": "Hierarchical-attention KV cache (light memory extension)",
            "memory_representation": "KV caches organized with bi-level hierarchical attention across chunks/windows.",
            "memory_access_mechanism": "Bi-level hierarchical attention to connect and attend to tokens across extended context windows; KV caches are maintained to support decoding.",
            "task_name": "Used as baseline on LongBench, InfiniteBench, UltraDomain.",
            "task_category": "Long-context processing baseline",
            "performance_with_memory": "Reported in paper as a baseline (table entries show SelfExtend row); empirical results indicate SelfExtend improves full-LM context handling but is outperformed by MemoRAG compact-memory approach on many long-context tasks.",
            "performance_without_memory": "N/A (this is an extension technique used as a baseline).",
            "has_comparative_results": true,
            "performance_metric": "Varies by dataset (reported in Table 1).",
            "tradeoffs_reported": "Enables longer contexts without full model tuning but can still be limited by inherent semantic completeness and GPU memory costs compared to compressed memory designs.",
            "limitations_or_failure_cases": "While extending context window, hierarchical approaches can still be less efficient or lose some semantics compared to compact memory; paper's ablations find it inferior to compact memory.",
            "uuid": "e6473.5",
            "source_info": {
                "paper_title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "HippoRAG",
            "name_full": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
            "brief_description": "A related-work paper (referenced) proposing a neurobiologically inspired long-term memory mechanism for LLMs; mentioned in the MemoRAG related work as another attempt to endow LLMs with external/memory-like components.",
            "citation_title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
            "mention_or_use": "mention",
            "agent_name": "HippoRAG (referenced)",
            "agent_description": "A memory-augmented LLM architecture inspired by biological long-term memory mechanisms (referenced in MemoRAG related work).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Neurobiologically inspired long-term memory (referenced paper’s approach)",
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": null,
            "task_category": "Long-term memory augmentation (related work)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "Referenced in MemoRAG related work (Jiménez Bernal et al., 2024).",
            "uuid": "e6473.6",
            "source_info": {
                "paper_title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
            "rating": 2,
            "sanitized_title": "minference_10_accelerating_prefilling_for_longcontext_llms_via_dynamic_sparse_attention"
        },
        {
            "paper_title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
            "rating": 2,
            "sanitized_title": "llm_maybe_longlm_selfextend_llm_context_window_without_tuning"
        },
        {
            "paper_title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
            "rating": 2,
            "sanitized_title": "hipporag_neurobiologically_inspired_longterm_memory_for_large_language_models"
        },
        {
            "paper_title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
            "rating": 2,
            "sanitized_title": "a_humaninspired_reading_agent_with_gist_memory_of_very_long_contexts"
        },
        {
            "paper_title": "In-Context Retrieval-Augmented Language Models",
            "rating": 1,
            "sanitized_title": "incontext_retrievalaugmented_language_models"
        }
    ],
    "cost": 0.022804249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation
9 Apr 2025</p>
<p>Hongjin Qian 0000-0003-4011-5673
Zheng Liu zhengliu1026@gmail.com 0000-0001-7765-8466
Peitian Zhang 
Kelong Mao 
Defu Lian liandefu@ustc.edu.cn 
Zhicheng Dou dou@ruc.edu.cn 
Tiejun Huang tjhuang@pku.edu.cn 
Memorag </p>
<p>Peking University
BeijingChina</p>
<p>Beijing Academy of Artificial Intelligence
BeijingChina</p>
<p>Hong Kong Polytechnic University Hong Kong
China</p>
<p>Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>School of Computer Science
Technology University of Science and Technology of China Hefei
China</p>
<p>Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>School of Computer Science
Peking University
BeijingChina</p>
<p>12 pages. https: //doi.org/102025, 1145/3696410.3714805Sydney, New YorkNSW, Australia. ACM, NYUSA</p>
<p>MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation
9 Apr 20255B66F9DD61406733E212191B7815591410.1145/3696410.3714805arXiv:2409.05591v3[cs.CL]CCS Concepts • Computing methodologies → Natural language generation Retrieval-Augmented Generation, Long Context Processing
Processing long contexts presents a significant challenge for large language models (LLMs).While recent advancements allow LLMs to handle much longer contexts than before (e.g., 32K or 128K tokens), it is computationally expensive and can still be insufficient for many applications.Retrieval-Augmented Generation (RAG) is considered a promising strategy to address this problem.However, conventional RAG methods face inherent limitations because of two underlying requirements: 1) explicitly stated queries, and 2) well-structured knowledge.These conditions, however, do not hold in general long-context processing tasks.In this work, we propose MemoRAG, a novel RAG framework empowered by global memory-augmented retrieval.MemoRAG features a dual-system architecture.First, it employs a light but long-range system to create a global memory of the long context.Once a task is presented, it generates draft answers, providing useful clues for the retrieval tools to locate relevant information within the long context.Second, it leverages an expensive but expressive system, which generates the final answer based on the retrieved information.Building upon this fundamental framework, we realize the memory module in the form of KV compression, and reinforce its memorization and cluing capacity from the Generation quality's Feedback (a.k.a.RLGF).In our experiments, MemoRAG achieves superior performances across a variety of long-context evaluation * Corresponding Author.† Equal contribution.</p>
<p>Introduction</p>
<p>Large language models (LLMs) need to process long contexts in many real-world scenarios, such as long-document QA and summarization [4,57].While some recent LLMs can handle much longer contexts than before (e.g., Mistral-32K, Phi-128K) [1,23], they can still be insufficient for certain applications.Meanwhile, it's computationally expensive to process long contexts directly due to the considerable costs on inference time and GPU memory [11].</p>
<p>Retrieval-Augmented Generation (RAG) is widely regarded as a promising strategy for addressing long-context processing challenges [16,22].RAG allows LLMs to complete tasks more costeffectively by focusing only on the relevant parts retrieved from the long input context [52,59].However, traditional RAG methods face inherent limitations when applied to general long-context tasks, due to two key constraints.First, the search intent must be explicitly expressed (or easily clarified through query rewriting) [6,59].Second, the external dataset must be well-structured for effective encoding and indexing (e.g., Wikipedia passages) [37,39].Unfortunately, neither of these conditions is typically met in general long-context tasks.On one hand, there may be no clear search intent (e.g., summarizing the main characters in a book, or clarifying the relationships between characters) [13,42].On the other hand, the input context is often unstructured (e.g., a 100-page text file, or multi-year financial reports), making it difficult to partition, encode, and index in a straightforward manner [41,44,59].Human cognition of a long document, unlike standard RAG, is significantly more effective (as shown in Figure 1).When a person is presented with a long document, they first skim through it to form a global memory of its high-level information.When tasked with a document understanding question-such as "What are the mutual relationships between the main characters?"-the person recalls useful clues from their memory and uses these clues to locate specific details within the document.Based on the retrieved information, they can then generate a high-quality response to the task [2].</p>
<p>Inspired by the human cognitive process, we propose Mem-oRAG, a novel framework for long-context processing on top of global-memory enhanced retrieval augmentation.MemoRAG features a dual-system architecture: a light but long-range system to realize the memory module and a heavy but expressive system to generate the final answer.For each presented task, MemoRAG prompts its memory module to generate retrieval clues.These clues are essentially drafted answers based on the compact memory.While these clues may contain some inaccuracies or lack details, they effectively reveal the underlying information needs of the task and can be directly linked to the source information.By using these clues as queries, MemoRAG can effectively retrieve the necessary knowledge from the external knowledge base.</p>
<p>The memory module is the core of MemoRAG.It is expected to be 1) length-scalable: cost-effectively handling long-contexts, 2) retentive: memorizing the crucial information within long-contexts, and 3) instructive: generating useful clues for the presented task.Therefore, we introduce the following techniques to optimize its performance.First, we realize the memory module in the form of a KV-compressible LLM with configurable compression rates.This structure can flexibly support a wide range of context lengths and can be optimized in an end-to-end manner.Second, we design a novel algorithm that learns to reinforce the memory module's memorization and cluing capacity from the generation quality's feedback (a.k.a.RLGF).That is, 1) the generated clues are positively rewarded if they can support the generation of high-quality answers, and 2) the memory module is reinforced to generate the positively rewarded clues.</p>
<p>We perform comprehensive experiments to evaluate MemoRAG.In our experiment, we leverage a variety of datasets from two popular long-context benchmarks: LongBench [4] and InfiniteBench [57].The two benchmarks contain both QA-style tasks, e.g., HotPotQA, NarrativeQA, which are well-suited for traditional RAG methods, and non-QA tasks, like government report summarization, which are unfavorable to traditional RAG methods.We also curate a general long-document understanding benchmark, containing general tasks related to long documents from 20 diverse domains, such as law, finance, physics, and programming, etc.Our experiment results lead to a series of critical insights.Firstly, MemoRAG not only achieves notable advantages in both non-QA tasks where traditional RAG methods struggle, but also QA-style tasks where traditional RAG methods are usually applied.Secondly, MemoRAG outperforms advanced retrieval and RAG methods which are proposed recently, such as HyDE [15], RQ-RAG [6], and GraphRAG [13].Thirdly, MemoRAG even outperforms the direct-applied long LLMs and some context-extended methods, which can fully cover the input contexts [1,24].Finally, MemoRAG exhibits competitive efficiency in terms of inference speed and memory cost.</p>
<p>To summarize, the contributions of our work are highlighted by the following points: (1) We propose MemoRAG for long-context processing tasks based on global-memory enhanced retrieval augmentation.(2) We design a suite of architecture and optimization algorithms, enabling the memory module to be length-scalable, retentive, and instructive for long-context tasks.(3) We empirically demonstrate that MemoRAG generalizes beyond traditional QA tasks to effectively handle both non-QA tasks and complex QA tasks, expanding RAG's applicability to a wider range of scenarios.</p>
<p>Method 2.1 Background</p>
<p>The generation process of an LLM Θ(•) can be succinctly represented as  = Θ( |  ), where  denotes the input query,  is the generated response, and  represents the model's parameters, which store the knowledge learned from the training corpus.Since the training corpus typically consists of publicly available web data up to a certain cutoff point, LLMs face challenges when handling tasks that require up-to-date or domain-specific information.A</p>
<p>Retriever</p>
<p>Generator</p>
<p>Good Answer</p>
<p>How many times have the chamber been opened?common and effective solution to this problem is to incorporate an external knowledge base  into the input, which can be formulated as  = Θ(,  |  ), allowing for more accurate responses.In practice, the external knowledge base  can be substantially large, often exceeding the LLM's context size, leading to the long-context issue, as shown in the top of Figure 2(a).In the following, we refer to the external knowledge base  as the long input context.A straightforward idea to address the long-context issue is to employ LLMs with long-context processing ability.However, despite recent advancements in increasing context lengths, handling very long contexts remains infeasible for most LLMs, often resulting in incomplete answers as the context is truncated.Besides, RAG has emerged as a widely adopted solution to enable LLMs to effectively handle the long-context issue.RAG allows LLMs to retrieve and leverage only relevant information from the long context.A standard RAG system typically consists of two components: a generation model, Θ(•), and a retrieval model, Γ(•).Given an input query , the retrieval model Γ first identifies the relevant evidence  from the long context .This retrieved evidence is then passed to the generation model Θ, which utilizes it to produce the final response  .Formally, this process can be described as:
𝑌 = Θ(𝑞, 𝐸 | 𝜃 ), 𝐸 = Γ(𝑞, 𝐶).(1)
In an ideal retrieval setting, the query  serves as a piece of text that is representative of the expected evidence [34], allowing the retriever to easily locate the relevant evidence .However, as shown in the bottom of Figure 2(a), in many practical scenarios, the input query  often carries implicit information-seeking intents that are not semantically aligned with the expected text evidence.As a result, standard retrievers, which typically rely on lexical or semantic matching, may struggle to accurately retrieve the expected evidence, leading to performance degradation in RAG systems.This issue underscores the need for an advanced RAG framework to bridge the semantic gap frequently encountered in such situations.</p>
<p>Algorithm 1 MemoRAG Framework</p>
<p>MemoRAG</p>
<p>In this paper, we propose MemoRAG, which leverages a memory model Θ mem (•) to learn and store the long context , forming a global memory denoted as  mem .When a query or task instruction  is presented, MemoRAG prompts the memory model to generate draft answers , which serve as a set of answer clues.These clues guide the retrieval of accurate and comprehensive evidence  from the long context .Subsequently, the final answer  is generated using the retrieved evidence text .This process is defined as:
𝑌 = Θ(𝑞, 𝐸 | 𝜃 ), 𝐸 = Γ(𝑦, 𝐶), 𝑦 = Θ mem (𝑞 | 𝜃 mem ). (2)
MemoRAG is illustrated in the middle of Figure 2(b).</p>
<p>To facilitate understanding, we illustrate the MemoRAG framework with pseudo-code in Algorithm 1.</p>
<p>Specifically, in line 1 , MemoRAG begins by receiving a long input context , which is combined with auxiliary text (e.g., prompts), referred to as the input sequence X.MemoRAG's memory model then processes X to form a global memory representation, denoted as  mem in line 2 (see Section 2.3 for details on the memory model).This memory representation,  mem , encapsulates the high-level semantics of the entire long context from a global perspective.In practice, the memory can be offloaded for efficient reuse in future tasks.In line 6 , when a query  is presented, the global memory  mem is used to generate task-specific clues, denoted as .These clues serve to outline the expected answer  , effectively bridging the gap between the raw input context and the ground-truth answer.Based on these memory-generated clues, MemoRAG's retriever is employed to locate precise evidence text  within the long input context, as shown in line 7 .Using the retrieved evidence text  along with the input query , MemoRAG's generator produces the final response  , shown in line 8 .By default, MemoRAG utilizes the memory model's underlying LLM as the generator to ensure parameter efficiency.</p>
<p>Application Scenario.MemoRAG can adapt to a variety of application scenarios and determine how to generate appropriate clues based on the specific type of long-context task presented.In Figure 2(c), we illustrate three scenarios that are particularly challenging for standard RAG but well-suited for MemoRAG.First, in a question-answering task where the query requires gathering distributed information, MemoRAG generates answer clues  that include intermediary reasoning steps, such as creating more explicit surrogate queries and retrieving relevant evidence from the long context to support the final answer.Second, in query-focused summarization tasks, the queries are inherently unsearchable, as the target information must be aggregated from the entire context rather than isolated segments.Since MemoRAG has already comprehended the entire long context, it can recall multiple query-related evidence clues, enabling more effective information retrieval and synthesis.Third, for tasks without explicit queries, such as text summarization, the draft answer may consist of key points or concepts extracted from the context, which are essential for constructing a coherent and accurate summary.</p>
<p>Memory Module</p>
<p>As discussed in Section 1, MemoRAG's memory module is designed to achieve three key objectives: 1) length scalability, enabling efficient handling of long contexts; 2) retentiveness, ensuring the retention of crucial information from these contexts; and 3) instructiveness, providing useful clues that facilitate comprehensive retrieval.The first two objectives are met through specialized model designs, while the third is achieved via multi-stage, data-driven training.</p>
<p>Memory Model Design.</p>
<p>The inference workflow in LLMs consists of two stages: (i) the prefill stage, where the input sequence is processed to generate key-value (KV) cache for each transformer layer; and (ii) the decoding stage, where the model sequentially generates tokens by utilizing and updating the KV cache.</p>
<p>In the prefill stage, let the input tensor
X ∈ R 𝑛×𝑑 = {𝑥 1 , • • • , 𝑥 𝑛 } consist of 𝑛 token embeddings,
where  is the model's hidden size.The input X is processed by a transformer-based model Θ(•), and the key-value cache [K, V] are generated as follows:
K = X𝑾 K , V = X𝑾 V ,(3)
where  K and  V are the weight matrices for the key and value projections, respectively.This attention mechanism is applied independently at each layer and for each attention head.For simplicity, we omit the layer and head indices in the equations.</p>
<p>In the decoding stage, let t ∈ R  × represent the new input tensor, where  is the length of the newly input tokens.We compute the new key and value as:
K t = t𝑾 K , V t = t𝑾 V .(4)
The KV cache is then updated by concatenating the new key-value pairs with the previous ones:
K ← Concat(K, K t ), V ← Concat(V, V t ).(5)
Finally, the attention output is computed as:
Q t = t𝑾 Q , 𝑨(Q, K, V) = softmax Q t K 𝑇 √ 𝑑 V,(6)
where  Q is the weight matrix for the query projection, and (•) represents the attention function.For simplicity, we ignore other parts of the inference process.Light Global Memory.The key-value cache computed during the prefill stage can be efficiently reused in the decoding stage.Thus, the key-value cache [K, V] serves as the simplest form of global memory, denoted as  mem = [K, V].However, maintaining a full key-value cache for long contexts is computationally expensive and time-consuming.In this place, we first introduce a kind of baseline solution called light global memory, which directly takes advantage of recent light long-context techniques, e.g., MInference [24] and SelfExtend [27].Formally, they can be defined as  mem_lite =  (Θ(X |  )), where  (•) represents the optimization techniques applied to the model.</p>
<p>While light global memory is easy to implement, empirical analysis in Section 3.4 demonstrates that it is inferior to the compact global memory introduced below.This is due to several factors: (1) it is constrained by the native context size of LLMs, limiting its adaptability to extremely long contexts; and (3) the use of sparse attention compromises semantic completeness.Besides, although light memory reduces parameters, it still consumes substantial GPU memory by maintaining the full length of the key-value cache Compact Global Memory.We propose a flexible model architecture designed to facilitate efficient memory formation.The memory model progressively compresses the raw input tokens into a significantly smaller set of memory tokens in KV space, while preserving essential semantic information, resulting in compact global memory.Specifically, we introduce memory tokens   to serve as the information carriers of global memory in LLMs.Suppose the LLM Θ(•) has a working context window length of .After each context window, we insert  memory tokens, such that:
X = {𝑥 1 , • • • , 𝑥 𝑙 , 𝑥 𝑚 1 , • • • , 𝑥 𝑚 𝑘 , 𝑥 𝑙+1 , • • • }, 𝑘 ≪ 𝑙 .(7)
For the memory tokens denoted by X  , we initialize a separate set of weight matrices specifically for memory formation, denoted as  Q  ,  K  , and  V  , where Q  , K  , and V  are the query, key, and value for the memory tokens X  .We compute the corresponding query, key, and value as follows:
Q 𝑚 = X 𝑚 𝑾 Q 𝑚 , K 𝑚 = X 𝑚 𝑾 K 𝑚 , V 𝑚 = X 𝑚 𝑾 V 𝑚 , (8) 𝑨(Q, K, V) = softmax [Q; Q 𝑚 ] K𝑇 √ 𝑑 Ṽ,(9)K = [K 𝑚 cache ; K; K 𝑚 ], Ṽ = [V 𝑚 cache ; V; V 𝑚 ].(10)
The terms K  cache and V  cache represent the KV cache for previously computed memory tokens.</p>
<p>In the prefill stage, after processing each context window, we generate a new KV cache for the memory tokens, denoted as
[K 𝑚 , V 𝑚 ].
We update the previous memory token cache as follows:
K 𝑚 cache ← Concat(K 𝑚 cache , K 𝑚 ),(11)V 𝑚 cache ← Concat(V 𝑚 cache , V 𝑚 ).(12)
Meanwhile, the KV cache [K, V] for the regular tokens is discarded to reduce memory consumption.For compact global memory, we have
𝜃 mem = [V 𝑚 cache , K 𝑚 cache ].
In our experiments, we typically select a compression ratio  = / ∈ [4,8,16,32,64], resulting in an approximate × reduction in GPU memory usage.Furthermore, since the number of memory tokens is much smaller than the number of raw tokens, LLMs can handle significantly longer contexts than their native context window would typically allow.For example, a 128K context LLM can process up to an 8M token context when a compression ratio of  = 64 is applied.</p>
<p>Memory Model</p>
<p>Training.Since the memory model initializes a new set of parameters, we begin by training the memory model through pre-training.Following this, we perform supervised finetuning (SFT) using task-specific SFT data.Finally, we apply a small set of SFT data labeled with preferences to perform preference alignment for the memory model.</p>
<p>Pre-Training.During the pre-training stage, the optimization goal is to enable the memory model to generate a global memory representation from raw input contexts.We only optimize the newly initialized weight matrices,  Q  ,  K  , and  V  , while keeping the underlying LLM's parameters frozen.The model's objective is to predict the next token using the memory tokens and the current context.This can be expressed using a cross-entropy loss:
L pre = − 𝑇 ∑︁ 𝑡 =1 log P (𝑥 𝑡 | 𝒙 𝑚 cache , 𝑥 1:𝑡 −1 ),(13)
where   cache represents the previously accumulated memory tokens, and  represents the raw tokens.This loss encourages the model to maximize the probability of generating the correct next token based on the previous memory and the current raw context.</p>
<p>Supervised Fine-Tuning.In the SFT stage, the loss function is designed to help MemoRAG generate task-specific clues that can later guide the retrieval of relevant evidence.Here, the model is trained to minimize the difference between the generated output and the ground-truth outputs provided by the SFT dataset.The loss function is also a cross-entropy loss, but applied to taskspecific data:
L SFT = − 𝑇 ∑︁ 𝑡 =1 log P (𝑦 𝑡 | 𝒙 𝑚 cache , 𝑞),(14)
where  represents the ground-truth task-specific output and  is the query or task instruction.This loss ensures that MemoRAG learns to produce accurate clues based on the global memory.The SFT data is initially generated using strong LLMs and subsequently reviewed and refined by human annotators (see Appendix B for details).While the SFT data labels capture both LLM and human preferences regarding the answer clues, they do not directly reflect the quality of the final generated answers.To address this, we further optimize the memory module using a tailored optimization method which is introduced below.RLGF (Reinforcement Learning with Generation Feedback).To further optimize the memory module for generating truly useful answer clues, the memory model is trained to align its outputs with preferred answer clues, selected based on their contributions to the overall end-to-end performance.The loss function is derived from a preference-based ranking loss, which encourages the model to prioritize outputs that lead to better evidence retrieval and final answer generation.This is defined as:
L RLGF = ∑︁ (𝑦 + , 𝑦 − ) max 0, 1 − 𝑅(𝑦 + ) + 𝑅(𝑦 − ) ,(15)
where ( + ) and ( − ) represent the rewards assigned to the preferred and non-preferred outputs, respectively.This loss function drives the model to generate outputs that align more closely with the preferred answers, ensuring that the generated clues are both relevant and lead to improved evidence retrieval.As a result, the overall answer quality is enhanced.See Appendix B for details on the data construction for RLGF.</p>
<p>Experiment</p>
<p>In this section, we investigate the following research questions (RQ): RQ1: How does MemoRAG's performance compare to that of standard RAG systems, advanced RAG systems, and long-context LLMs?</p>
<p>RQ2: Can MemoRAG effectively generalize beyond straightforward QA tasks to handle non-QA tasks and complex QA tasks involving long contexts and diverse domains?</p>
<p>RQ3: Are MemoRAG's model designs and optimization strategies well-justified and appropriately selected?RQ4: How do MemoRAG's inference time efficiency and GPU memory usage compare to baseline methods?</p>
<p>Dataset</p>
<p>To explore RQ1 and RQ2, we evaluate MemoRAG and baselines using LongBench and InfiniteBench, two widely recognized benchmarks for long-context tasks [4,57], which include the following tasks: (1) Single-Doc QA: NarrativeQA [29], Qasper [9], and Mul-tiFieldQA [4].(2) Multi-Doc QA: HotpotQA [54], 2WikiMQA [19], and MuSiQue [50].(3) Non-QA tasks: GovReport [20], En.SUM [57] and MultiNews [14].(4) Long-book QA: En.QA [57].For summarization tasks, we use the task instruct as a fake query.</p>
<p>To further address RQ2, we evaluate MemoRAG across a broader range of real-world scenarios by introducing the UltraDomain benchmark, which consists of 20 datasets featuring long contexts and high-level queries across various specialized domains.Many of these tasks require a deep understanding of the entire context and the ability to synthesize multiple pieces of information to generate accurate answers.Additional details about UltraDomain can be</p>
<p>Baselines</p>
<p>We compare MemoRAG against three types of baselines: (1) Using Full Context: In this setting, we feed the full context into long LLMs, referred to as Full.For the main experiments, we utilize LLMs with a 128K context length, allowing us to process all evaluation data samples without truncation.In addition to directly processing the full context, we explore two recent techniques that optimize context pre-filling for comparison: MInference [24], which applies strategic sparse attention to accelerate the pre-filling process, and SelfExtend [27], which constructs bi-level hierarchical attention to expand the original LLM's context length.</p>
<p>(2) Standard RAG with Alternative Retrieval Methods: BGE-M3 [7]: A widely used retrieval model that has proven effective across many applications.</p>
<p>Stella-en-1.5B-v5[12]:</p>
<p>A state-of-the-art retrieval method that ranks in the top 3 on the MTEB leaderboard at the time of writing this paper.Jina-emb-v3 [48]: A newly released frontier multilingual retrieval model, which claims to perform well in various scenarios, particularly in RAG tasks.(3) Advanced RAG Methods: RQ-RAG [6]: RQ-RAG prompts LLMs to refine the input query into several sub-queries that are more effective for retrieval by explicit rewriting, decomposition, and disambiguation.The supporting passages are retrieved using both the original and refined queries.HyDE [15]: Directly prompts LLMs to generate hypothetical documents based solely on the query, and then retrieves relevant passages using these documents.The final answer is generated based on the retrieved passages.GraphRAG [13]: A graph-based RAG framework that transforms unstructured data into graph structures, enabling the system to perform more complex question-answering tasks based on graph-based information retrieval.</p>
<p>In the main experiments, the memory model is trained on Mistral-7B-Instruct-v0.2-32K.By default, MemoRAG uses the underlying LLM of the memory model as the generator.But Mistral's 32K context window is insufficient for most evaluation dataset contexts.To avoid context truncation, we use Phi-3-mini-128K-instruct [1] as the generator for MemoRAG and all baseline methods except for SelfExtend, which is specifically designed to enable LLMs to process contexts much longer than their native window.SelfExtend utilizes Phi-3-mini-4K-instruct as the generator and adjusts its effective context window according to the maximum context length required by different tasks.For GraphRAG, we utilize OpenAI's GPT-4o API for all requests during both the indexing and searching processes.
-7B-inst-v0.2-32K Mistral-7B-inst-v0.2-32K (MemoRAG) Llama3.1-8B-inst-128K Llama3.1-8B-inst-128K (MemoRAG) Phi-3-mini-128K Phi-3-mini-128K (MemoRAG) (a) (b) (c) ℳ(x) x ℳ(x) x ℳ(x) x ℳ(x)
x and used as the grounding evidence for answer generation 1 .See Appendix A for more implementation details.</p>
<p>Main Experiments</p>
<p>To address RQ1 and RQ2, we compare MemoRAG against all baseline models across three benchmarks, as presented in Table 1.The experimental results demonstrate that MemoRAG consistently outperforms all baselines across the evaluated datasets: First, while RAG is a promising solution for long-context tasks, using long LLMs that handle the full context length often yields better performance (Full vs. other baselines).In contrast, Mem-oRAG significantly surpasses the performance of long LLMs, highlighting its superior ability to process long-context tasks.Second, for straightforward QA tasks from LongBench and InfiniteBench, MemoRAG outperforms all baselines, showing its effectiveness in standard RAG scenarios with explicit information needs.Its memory-generated clues allow for more accurate evidence retrieval from long contexts.In complex QA tasks (e.g., financial and legal), MemoRAG achieves notable improvements, demonstrating its capability to handle complex, long-context challenges.Third, while traditional RAG methods often struggle with non-QA tasks that lack explicit queries-such as summarization tasks (e.g., MultiNews, GovReport, and En.SUM)-MemoRAG excels.It efficiently extracts key points from the input context and retrieves additional details to generate comprehensive summaries.</p>
<p>To further address RQ2, we evaluate MemoRAG on the remaining 18 diverse datasets from UltraDomain, where most input contexts exceed the generator's context limit (e.g., 128K tokens).The results, presented in Figure 3, lead to the following conclusions: First, MemoRAG consistently outperforms all baselines across all datasets, demonstrating strong domain generalization capabilities.Second, directly inputting the full context into LLMs generally yields better performance compared to standard RAG methods, revealing that RAG systems struggle with high-level queries and locating relevant evidence.Third, MemoRAG surpasses the performance of directly using the full context, illustrating its ability to effectively process super-long contexts and address complex tasks.</p>
<p>In summary, MemoRAG consistently outperforms standard and advanced RAG systems, as well as long LLMs.It generalizes well beyond straightforward QA tasks, effectively handling non-QA tasks and complex QA tasks.Its advantages, driven by global memory-enhanced retrieval, are especially evident in scenarios where standard RAG systems face challenges.</p>
<p>1 https://microsoft.github.io/graphrag/posts/query/0-global_search/</p>
<p>Ablation Study</p>
<p>To address RQ3, we conduct comprehensive ablation studies:</p>
<p>1) Model design and optimization strategy: We first compare two memory model design options: light memory and compact memory (see Section 2.3).Additionally, we evaluate the performance of the MemoRAG pipeline using memory models at various stages of training.This includes a zero-shot evaluation, where the foundation model is directly applied to MemoRAG, as well as evaluations following pretraining, supervised fine-tuning (SFT), and reinforcement learning with generation feedback (RLGF).The results, shown in Figure 4 (a) and (b), indicate that each technical design contributes uniquely to MemoRAG's overall effectiveness.Removing any of these designs results in performance degradation, validating the necessity and impact of MemoRAG's technical components.</p>
<p>2) Foundation model choice: To assess the impact of the foundation model, we replace the underlying LLM of MemoRAG's memory model with Qwen2-7B-instruct, which has a native context window of 128K tokens [53].By comparing Figure 4 (a) and (b), we observe that utilizing either model as the foundation for Mem-oRAG's memory module results in consistent performance improvements.This demonstrates that MemoRAG's memory model design is robust and adaptable across a wide range of LLMs.</p>
<p>3) Alternative generators: We evaluate MemoRAG's effectiveness with three different generators: Llama3.1-8B-inst-128K,Mistral-7B-inst-v0.2-32K, and Phi-3-mini-128K.As shown in Figure 4 (c), MemoRAG consistently outperforms the direct use of long LLMs, with the performance gap widening as the task context exceeds the LLM's native context length.This indicates that Mem-oRAG can significantly enhance task performance when integrated with various LLMs as generators.</p>
<p>4) Impact of compression rate: As discussed in Section 2.3, the compression rate  during compact memory formation affects both efficiency and effectiveness.A smaller  retains richer semantics but requires more KV cache, while a larger  improves efficiency but reduces semantic richness.We experimented with  ∈ [4,8,16,32,64], and the results, shown in Figure 5 (b), indicate that as  increases, performance declines but stabilizes at  = 32.Despite higher compression, MemoRAG consistently captures key information and outperforms the standard RAG pipeline across all values of .</p>
<p>In summary, the ablation studies confirm the effectiveness of MemoRAG's technical designs and model choices, demonstrating that its architecture is well-motivated and robustly designed.</p>
<p>Efficiency Analysis</p>
<p>To address RQ4, Figure 5(a) compares model efficiency 2 .Key observations include: (1) Indexing latency analysis (top): Standard RAG quickly indexes long inputs due to its simpler process, while MemoRAG is slower due to the global memory formation.However, it remains more efficient than long LLMs' pre-filling, thanks to its optimized memory model.GraphRAG is the slowest, heavily reliant on GPT-4 APIs.( 2) Retrieval latency analysis (middle): Standard RAG retrieves efficiently using vector databases (e.g., FAISS [28]), while MemoRAG is slower as it generates retrieval clues but still outperforms GraphRAG.(3) GPU memory consumption analysis (bottom): Both MemoRAG and standard RAG process 128K contexts with under 60 GiB of GPU memory, whereas long LLMs require substantially more due to the large key-value cache.In summary, MemoRAG maintains a balanced time and memory efficiency.While it is slower than standard RAG, it outperforms advanced RAG methods and long LLMs in both time and memory efficiency.</p>
<p>Related Work</p>
<p>Long Context: Handling long contexts is a fundamental issue for LLMs.The most straightforward approach is to train LLMs on long text sequences, giving them a native ability to handle extended contexts [1,5,10,40].However, this is very expensive, as computational costs increase exponentially with longer contexts.As a result, researchers focus on improving attention efficiency [3,8,10,23].Additionally, Liu et al. [33] highlight that LLM performance may degrade when the target answer is located in the middle of the context.To address this, various works explore data augmentation, attention reweighting, and data re-organization [17,32,51,56].Another approach involves compressing the input through strategies like sliding windows, context compression, and summarization [25,30,45,52,55].With the rapid development of long-context processing, context windows for LLMs have expanded significantly, from 4K tokens (e.g., Llama-2) [49] to 128K tokens (e.g., Phi-3, GPT-4) [1,40].Recent advancements even allow LLMs to extend their context window to 1 million tokens [17].Additionally, RAG has become a common solution for long-context challenges, using retrieval to find precise evidence within large inputs [52].</p>
<p>RAG: Retrieval-augmented generation (RAG) was initially introduced by Lewis et al. [31], defining a retrieval process that assists language models in handling knowledge-intensive tasks.Subsequent RAG research has focused on two areas: improving retrieval quality, which sets the upper bound for final generation quality [16,43], and enhancing the use of retrieved passages for increased relevance and flexible access [21,26,35,36,41].</p>
<p>With recent advancements in LLMs, incorporating RAG into LLM-based systems has become popular, inspiring numerous applications [38,46].As a result, there has been a growing call for more general-purpose RAG systems [58,59].However, the standard RAG pipeline faces inherent limitations and struggles to generalize effectively in complex tasks involving implicit information needs [16].</p>
<p>To expand RAG's applicability, recent works have proposed modifying the RAG pipeline with tailored approaches.For instance, HyDE generates a hypothetical document from the query, which is used to retrieve relevant evidence [15], while RQ-RAG rewrites the query into simpler forms to improve retrieval [6].However, both rely solely on the model's internal knowledge, limiting their effectiveness for domain-specific tasks.GraphRAG [13] constructs a knowledge graph to assist retrieval, but its static graph construction is difficult to optimize.Other methods [6,18,42] also fail to achieve a comprehensive understanding of the input context, leading to incomplete semantic comprehension.</p>
<p>Conclusion</p>
<p>In this paper, we tackle long-context processing using global memoryenhanced retrieval by introducing MemoRAG, a framework that builds a global memory from the entire context.When presented with a task, MemoRAG generates draft answers that, although lacking in detail, effectively guide the retrieval of relevant evidence for more accurate final response generation.By leveraging these clues, MemoRAG identifies precise information within the long context, improving overall answer quality.Extensive experiments on two long-context benchmarks and various real-world applications demonstrate that MemoRAG significantly outperforms standard RAG systems, advanced RAG systems, and long LLMs.MemoRAG excels in tasks requiring high-level information aggregation, while also offering notable advantages in traditional tasks commonly handled by previous RAG systems, expanding the potential and applicability of RAG to a broader range of scenarios.</p>
<p>A Implementation Details</p>
<p>For pre-training the memory model, we sample text spans from the RedPajama [47] dataset to create a training set of 2 billion tokens.The memory context window size is set to 2048, and during training, we randomly select a compression ratio  ∈ [4,8,16,32,64] for each context window.The model is trained for 1 epoch with a batch size of 8 and a learning rate of 5e-5.</p>
<p>For supervised fine-tuning (SFT), we build an SFT dataset consisting of 17,116 samples.In this stage, the model is trained for 2 epochs with a batch size of 8 and a learning rate of 1e-5.The lengths of the SFT samples range from 4K to 64K tokens.</p>
<p>During RLGF optimization, we sample 2,000 instances from the SFT training dataset and rank the generated clue answers, categorizing them into preferred and rejected based on their contributions to the overall end-to-end performance.The data construction process can refer to Appendix B.</p>
<p>During the memory module training, we keep the underlying model's parameters frozen and train only the newly initialized parameters of the memory model, avoiding the resource-intensive process of full parameter fine-tuning.The size of the newly initialized parameters varies depending on the underlying LLM.For instance, with Qwen2-7B-instruct, the newly initialized parameters are approximately 1.1 billion.</p>
<p>For the light global memory setting, we utilize SelfExtend [27] to extend the LLMs' context window to the maximum length required for each specific task.Additionally, we apply MInference [24] to accelerate the prefill process.</p>
<p>For the main experiments, we set the compression ratio to  = 4.For MemoRAG, RQ-RAG, and HyDE, we use BGE-M3 [7] as the retriever and set the hit number to 3. We use the semantic-textsplitter tool to chunk the long context with a maximum length of 512.For MemoRAG and all baselines, we use the same task prompts provided by the official repositories of the corresponding benchmarks 3 .We also use the same generation hyper-parameters (varying by task) for MemoRAG and all baseline models.</p>
<p>All training and evaluation were conducted using 8 NVIDIA A800-80G GPUs.For prompts used in MemoRAG please refer to this repository.</p>
<p>A.1 Case Study</p>
<p>In Table 2, we present an example processed by MemoRAG.The input query pertains to the high-level understanding of the term "Outside Date" within the input context, a legal contract consisting of 56.6K tokens.The standard RAG system searches for evidence solely based on the input query, in which the semantics of "significance of the Outside Date" is not explicitly present.Therefore, direct semantic connections with the expected supporting evidence are difficult to establish.As a result, the standard RAG system generates answers that provide a general definition of the term "Outside Date" rather than its "significance" regarding this legal contract.Our MemoRAG, on the other hand, benefits from the global perception of the entire input context.It can evoke several clues that bridge the semantic gap between the expected supporting evidence and the input query.By leveraging these clue texts, we can more 3 LongBench: https://github.com/THUDM/LongBench,InfiniteBench: https://github.com/OpenBMB/InfiniteBench accurately locate the relevant evidence passages, leading to a more comprehensive and precise response.</p>
<p>B More details of Dataset Construction</p>
<p>To construct the SFT training set, we first collect long contexts from novels, academic papers, news, financial reports, and legal contracts.The collection of novels, academic papers, and news comes from the training datasets of NarrativeQA, Qasper, and HotpotQA.The legal contracts are sourced from this repository, and the financial reports are from this repository.We then sample long contexts of up to 80K tokens and use strong LLMs (e.g., GPT-4 128K) to generate high-level, insightful question-answer pairs.After quality review, we selected 20,000 samples and prompted the same LLMs to generate answer clues that bridge the gap between the query and the long context.During this process, the LLMs were provided with the query, the long context, and the answer, enabling them to utilize both priori and posteriori knowledge to generate the answer clues more effectively.These clues were then inspected for quality through human review, resulting in 17,116 SFT training samples.Six graduate students participated in the inspection, with each sample reviewed by at least three students.Samples tagged as discard more than twice were excluded from the final dataset.</p>
<p>For the RLGF training set, we selected 2,000 samples from the SFT dataset, filtering for those with more than five answer clues.For each clue, we retrieved the top-3 evidence.We then greedily evaluated the performance of all combinations of three or more clues and identified the best-performing combination as the preferred answer and the worst-performing combination as the rejected answer.</p>
<p>C More details of UltraDomain</p>
<p>We begin constructing the UltraDomain benchmark by leveraging contexts from datasets representing specific areas of knowledge, focusing on two specialized datasets.The first is the Fin dataset, derived from financial reports, which tests MemoRAG's ability to process and interpret complex financial data, ensuring it can manage the intricacies of financial language and reporting.The second is the Leg dataset, composed of legal contracts, which challenges MemoRAG to comprehend and navigate the precise, nuanced language of legal documents.</p>
<p>In addition to these specialized datasets, we collected a diverse set of 428 college textbooks covering 18 distinct domains, including natural sciences, humanities, and social sciences 4 .These textbooks are used to evaluate MemoRAG's versatility and adaptability across a broad range of topics, including those unrelated to finance and law.By assessing MemoRAG on these varied contexts, we gain insights into its potential for broader applications beyond specific domains.We also created a Misc dataset, comprising mixed contexts from the specialized datasets.This dataset is designed to assess MemoRAG's ability to generalize across different types of contexts.</p>
<p>Specifically, we sampled text spans up to 128K tokens in length and fed them into GPT-4, prompting it to generate high-level questionanswer pairs that require a comprehensive understanding of the full context.Six graduate students manually reviewed the generated QA pairs by: (1) selecting questions that are not directly searchable, and</p>
<p>Figure 1 :
1
Figure 1: Comparison of MemoRAG with Standard RAG and human cognition of a long document.Figure (a) shows standard RAG, where retrieval and generation take place in a sequential pipeline.Figure (b) illustrates how humans tackle a task about the document: 1. going through the document and forming the memory, 2. thinking about the clues to the presented task (i.e., recalling), checking the document for needed details (i.e., retrieving), 3. making a response to the task based on the memory-enhanced retrieval result.Inspired by the human cognition process, Figure (c) demonstrates MemoRAG, which creates a global memory of the long context, recalling useful clues based on memory, and retrieving information based on the clues to generate a high-quality response.</p>
<p>Figure 2 :
2
Figure 2: Illustration of (a) task background, (b) framework comparison, and (c) application scenarios.When processing long inputs like the entire Harry Potter series, most LLMs struggle with million-token contexts.Standard RAG methods also face challenges with queries unsuitable for direct searching.MemoRAG overcomes these limitations by constructing a global memory that generates clues, guiding the retrieval of relevant evidence and enabling more accurate and comprehensive answers.</p>
<p>Figure 3 :
3
Figure 3: Experiment results on the UltraDomain benchmark.These datasets feature contexts of up to one million tokens, a wide range of subjects.See more details about the benchmark in Appendix C.</p>
<p>Figure 4 :
4
Figure 4: Ablation study.Figure (a) and (b) show the performance of different LLMs and optimization strategies.The Pretrain, SFT, and RLGF settings refer to the training stages.The Light setting uses the light memory model, introduced in Section 2.3.The Zero setting uses native LLMs without prior training.Figure (c) shows the outcomes of using different models as the generator.</p>
<p>Figure 5 :
5
Figure 5: Analysis on the model efficiency (left) and the impact of the choice of the compression ratio  (right).</p>
<p>Input Query Input Context Long LLMs
Task Knowledge Space CWorking Context Length (~128K)IncompleteModelAnswerKnowledge Space θActual Context Length (~1M)MemoRAGLily Potter'sHow the book convey theSacrifice The Weasley Family ……theme of love?…Answer CluesSemanticConnectedPartial EvidenceDiscon.StandardRAG</p>
<p>1 :
1
Input: long context , memory model Θ mem (•) 2: Memory Formation: Generate global memory  mem = Θ mem ( X), X =  + auxiliary text 3: Input: queries { 1 , . . .,   }, generator Θ(•), retriever Γ (•) 4: Initialize: answer set Y ← { } 5: for each query   ∈ { 1 , . . .,   } do   = Θ mem (  |  mem ) # Generate draft answer clues for<br />
6:7:𝐸 𝑖 = Γ (𝑦 𝑖 , 𝐶 ) # Retrieve relevant evidence based on the clues8:𝑌 𝑖 = Θ(𝑞 𝑖 , 𝐸 𝑖 | 𝜃 ) # Generate the final answer for 𝑞 𝑖9:Y ← Y ∪ {𝑌 𝑖 } # Add final answer to the answer set10: end for11: Optional -Memory Offload: Save global memory 𝜃 mem to disk forfuture reuse12: Return: answer set Y</p>
<p>Table 1 :
1
Main experiment results.Best results are in bold, second-best ones are underlined, and " †" indicates performance surpasses all baselines in a t-test at  &lt; 0.05.Evaluation metrics for all datasets are in Appendix B. † 43.9 † 52.2 † 33.9 † 54.1 † 54.8 † 26.3 † 32.9 † 15.7 † 22.9 † 51.5 † 51.0 † 55.6 † 40.2
Datasetnarqasmulmus2wikihotnewsgoven.sum en.qafinlegalmiscave.LongBenchInfBenchUltraDomainFull21.439.451.528.238.148.124.932.613.015.247.846.548.735.0Mnference20.739.050.827.435.946.224.832.213.312.144.739.846.333.3SelfExtend19.637.847.422.737.242.021.429.111.19.341.237.934.130.1BGE-M320.333.044.321.135.442.117.719.89.616.341.741.243.729.7Stella-v513.732.443.521.035.640.620.318.210.019.542.835.143.929.0Jina-emb-v315.934.742.817.833.141.821.925.211.318.741.837.143.829.7GraphRAG16.236.345.419.337.538.018.425.610.813.539.939.641.729.4RQ-RAG19.634.146.521.936.141.720.118.610.416.141.840.943.230.1HyDE18.736.047.520.536.842.7---19.643.141.644.2-MemoRAG 27.5  t_resultsv5 HyDE MemoRAG Ave(|gC|) (K)2.143.953.620.34.935.151.251.4FullBGE-M3Stella-v5HyDEMemoRAG0.9 9.3 2.9 7.3 4.1 1.942.8 40.6 35.5 38.2 34.7 33.048.0 50.9 40.5 38.8 37.8 37.640.6 37.4 105.8 131.4 150.1 215.9T e c h n o lo g yComputer 31.3 34.5 37.8 41.0P h y s ic sH is to r yAgriculture 31.8 33.5 35.3 37.0A r tF ic ti o nCooking 27.8 30.5 33.3 36.0B io g r a p h y2.931.937.4134.928.030.025.01.131.837.4144.03.2 3.1 3.832.8 33.0 35.436.7 36.6 36.4151.0 129.0 198.0H e a lt hR e li g io nB io lo g yM a th e m a ti c sL it e r a tu r eP o li ti c s1.832.236.2135.72.131.535.7125.2PsychologyPhilosophyMusic2.331.135.6195.21.032.935.6156.19.830.335.3163.50.232.135.2139.61.532.935.1168.78.829.234.4129.46.527.131.3137.71.932.536.2150.6</p>
<p>The results from GraphRAG's global search setting are extracted
12.420.627.233.439.811.219.428.131.243.114.520.727.032.245.517.625.136.534.849.918.326.538.136.151.1
We randomly selected 5 samples with 128K context lengths from the UltraDomain benchmark, truncating the context into shorter segments to test various methods under the same configuration.
https://huggingface.co/datasets/P1ayer-1/books-3-textbooks
AcknowledgmentThis work was supported by Beijing Municipal Science and Technology Project No. Z231100010323009, National Natural Science Foundation of China No. 62272467, Beijing Natural Science Foundation No. L233008.The work was partially done at the Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE.Zheng Liu is the corresponding author.(2) evaluating the quality of the generated answers.This process yielded a total of 3,240 evaluation samples.Statistical details of the UltraDomain benchmark are provided in Table3and Table4. Together, these datasets form a rigorous benchmark for evaluating MemoRAG's effectiveness in both domainspecific tasks and broader, cross-disciplinary applications.Example cases from UltraDomain can be found in this repository.
. Sam Ade Marah I Abdin, Ammar Jacobs, Jyoti Ahmad Awan, Ahmed Aneja, Hany Awadallah, Nguyen Awadalla, Amit Bach, Arash Bahree, Harkirat S Bakhtiari, Alon Behl, Misha Benhaim, Johan Bilenko, Sébastien Bjorck, Martin Bubeck, Caio Cai, Teodoro César, Weizhu Mendes, Vishrav Chen, Parul Chaudhary, Allie Chopra, Gustavo Del Giorno, Matthew De Rosa, Ronen Dixon, Dan Eldan, Amit Iter, Abhishek Garg, Suriya Goswami, Emman Gunasekar, Junheng Haider, Russell J Hao, Jamie Hewett, Mojan Huynh, Xin Javaheripi, Piero Jin, Nikos Kauffmann, Dongwoo Karampatziakis, Mahoud Kim, Lev Khademi, James R Kurilenko, Yin Lee, Yuanzhi Tat Lee, Chen Li, Weishung Liang, Eric Liu, Zeqi Lin, Piyush Lin, Arindam Madan, Hardik Mitra, Anh Modi, Brandon Nguyen, Barun Norick, Daniel Patra, Thomas Perez-Becker, Reid Portet, Heyang Pryzant, Marko Qin, Corby Radmilac, Sambudha Rosset, Olatunji Roy, Olli Ruwase, Amin Saarikivi, Adil Saied, Michael Salim, Shital Santacroce, Ning Shah, Hiteshi Shang, Xia Sharma, Masahiro Song, Xin Tanaka, Rachel Wang, Guanhua Ward, Philipp Wang, Michael Witte, Can Wyatt, Jiahang Xu, Sonali Xu, Fan Yadav, Ziyi Yang, Donghan Yang, Chengruidong Yu, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhang, Zhou, 10.48550/ARXIV.2404.14219arXiv:2404.142192024Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone. CoRR abs/2404.14219 (2024</p>
<p>Social cognition and the human brain. Ralph Adolphs, Trends in cognitive sciences. 31999. 1999</p>
<p>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai, 10.18653/V1/2023.EMNLP-MAIN.298Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li, 10.18653/V1/2024.ACL-LONG.172Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, 10.48550/ARXIV.2403.17297arXiv:2403.172972024InternLM2 Technical Report. CoRR abs/2403.17297 (2024</p>
<p>RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation. Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, Jie Fu, 10.48550/ARXIV.2404.00610ARXIV.2404.00610arXiv:2404.006102024. 2024</p>
<p>Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu, arXiv:2309.07597[cs.CL]BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. 2023</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, Matt Gardner, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Deepseek-Ai , arXiv:2405.04434[cs.CL]DeepSeek-V2: A Strong, Economical, and Efficient Mixtureof-Experts Language Model. 2024</p>
<p>Zican Dong, Tianyi Tang, Lunyi Li, Wayne Xin Zhao, arXiv:2302.14502A survey on long text modeling with transformers. 2023. 2023arXiv preprint</p>
<p>Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson, arXiv:2404.16130[cs.CLFrom Local to Global: A Graph RAG Approach to Query-Focused Summarization. 2024</p>
<p>Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model. Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir R Radev, 10.18653/V1/P19-1102Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers, Anna Korhonen, David R Traum, Lluís Màrquez, the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics2019. July 28-August 2, 20191</p>
<p>Precise Zero-Shot Dense Retrieval without Relevance Labels. Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan, 10.18653/V1/2023.ACL-LONG.99Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan L Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang, arXiv:2312.10997[cs.CL]Retrieval-Augmented Generation for Large Language Models: A Survey. 2024</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang, arXiv:2406.12793ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. 2024</p>
<p>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. Jiménez Bernal, Yiheng Gutiérrez, Yu Shu, Michihiro Gu, Yu Yasunaga, Su, arXiv:2405.14831[cs.CL2024</p>
<p>Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, 10.18653/v1/2020.coling-main.580Proceedings of the 28th International Conference on Computational Linguistics. Donia Scott, Nuria Bel, Chengqing Zong, the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Efficient Attentions for Long Document Summarization. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Ji Heng, Lu Wang, 10.18653/v1/2021.naacl-main.112Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>Distilling Knowledge from Reader to Retriever for Question Answering. Gautier Izacard, Edouard Grave, International Conference on Learning Representations. 2021</p>
<p>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. Gautier Izacard, Édouard Grave, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain2021</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023. 2023Mistral 7B. arXiv preprint</p>
<p>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Dongsheng Amir H Abdi, Chin-Yew Li, Yuqing Lin, Lili Yang, Qiu, arXiv:2407.024902024. 2024arXiv preprint</p>
<p>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu, 10.18653/V1/2024.ACL-LONG.91Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Active Retrieval Augmented Generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, 10.18653/V1/2023.EMNLP-MAIN.495Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu, arXiv:2401.01325[cs.CLLLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. 2024</p>
<p>Billion-scale similarity search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 72019. 2019</p>
<p>The NarrativeQA Reading Comprehension Challenge. Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette, 10.1162/TACL_A_00023Trans. Assoc. Comput. Linguistics. 62018. 2018</p>
<p>A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts. Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John F Canny, Ian Fischer, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, Advances in Neural Information Processing Systems. 202033</p>
<p>How Long Can Context Length of Open-Source LLMs truly Promise?. Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, Hao Zhang, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Lost in the middle: How language models use long contexts. Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, Transactions of the Association for Computational Linguistics. 122024. 2024</p>
<p>Statistical language modeling for information retrieval. Xiaoyong Liu, Bruce Croft, Annu. Rev. Inf. Sci. Technol. 392005. 2005</p>
<p>Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search. Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Haonan Chen, Hongjin Qian, arXiv:2303.06573[cs.IR2023</p>
<p>RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment. Kelong Mao, Zheng Liu, Hongjin Qian, Fengran Mo, Chenlong Deng, Zhicheng Dou, Findings of the Association for Computational Linguistics: EMNLP 2024. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, Miami, Florida, USAAssociation for Computational Linguistics2024. November 12-16, 2024</p>
<p>Rethinking search: making domain experts out of dilettantes. Donald Metzler, Yi Tay, Dara Bahri, Marc Najork, 10.1145/3476415.3476428ACM SIGIR Forum. 5512021. June 2021</p>
<p>Fengran Mo, Kelong Mao, Ziliang Zhao, Hongjin Qian, Haonan Chen, Yiruo Cheng, Xiaoxi Li, Yutao Zhu, Zhicheng Dou, Jian-Yun Nie, arXiv:2410.15576[cs.CLA Survey of Conversational Search. 2024</p>
<p>MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016). CEUR Workshop Proceedings. Richard Tarek, Antoine Besold, Artur S Bordes, Greg Avila Garcez, Wayne, the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)Barcelona, Spain2016. December 9, 20161773CEUR-WS.org</p>
<p>Grounding Language Model with Chunking-Free In-Context Retrieval. Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, Zhicheng Dou, 10.18653/V1/2024.ACL-LONG.71Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Are Long-LLMs A Necessity For Long-Context Tasks?. Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia Zhou, Xu Chen, Zhicheng Dou, arXiv:2405.15318[cs.CL2024</p>
<p>WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus. Hongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, Ji-Rong Wen, arXiv:2304.04358[cs.CL]2023</p>
<p>In-Context Retrieval-Augmented Language Models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, 10.1162/TACL_A_00605Trans. Assoc. Comput. Linguistics. 112023. 2023</p>
<p>Parallel Context Windows Improve In-Context Learning of Large Language Models. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, 10.48550/arxiv.2212.10947arXiv2022. 2022</p>
<p>Retrieval Augmentation Reduces Hallucination in Conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, 10.18653/V1/2021.FINDINGS-EMNLP.320Findings of the Association for Computational Linguistics: EMNLP 2021. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021. 16-20 November, 2021</p>
<p>SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, Nolan Dey, 2023</p>
<p>Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, Han Xiao, arXiv:2409.10173[cs.CLjina-embeddings-v3: Multilingual Embeddings With Task LoRA. 2024</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>MuSiQue: Multihop Questions via Single-hop Question Composition. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, Transactions of the Association for Computational Linguistics. 102022. 2022</p>
<p>Effective Long-Context Scaling of Foundation Models. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma, 10.18653/V1/2024.NAACL-LONG.260Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gómez-Adorno, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024. June 16-21, 20241NAACL 2024</p>
<p>Retrieval meets Long Context Large Language Models. Peng Xu, Wei Ping, Xianchao Wu, Lawrence Mcafee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro, 10.48550/arxiv.2310.03025arXiv:2310.03025Experimental2023. 2023</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, arXiv:2407.10671Qwen2 technical report. 2024. 2024arXiv preprint</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/V1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4, 2018</p>
<p>Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon. Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou, arXiv:2401.034622024. 2024arXiv preprint</p>
<p>Extending Llama-3's Context Ten-Fold Overnight. Peitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin Qian, Qiwei Ye, Zhicheng Dou, 10.48550/ARXIV.2404.19553arXiv:2404.195532024. 2024. 19553</p>
<p>ınftyBench: Extending Long Context Evaluation Beyond 100K Tokens. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Khai Moo, Xu Hao, Zhen Han, Shuo Leng Thai, Zhiyuan Wang, Maosong Liu, Sun, 10.18653/V1/2024.ACL-LONG.814Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, Philip S Yu, 10.48550/ARXIV.2409.10102arXiv:2409.10102Trustworthiness in Retrieval-Augmented Generation Systems: A Survey. 2024. 2024</p>
<p>Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zhicheng Dou, Ji-Rong Wen, arXiv:2308.07107[cs.CLLarge Language Models for Information Retrieval: A Survey. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7784 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7784</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7784</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-277824142</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.11524v1.pdf" target="_blank">HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation</a></p>
                <p><strong>Paper Abstract:</strong> There is growing interest in hypothesis generation with large language models (LLMs). However, fundamental questions remain: what makes a good hypothesis, and how can we systematically evaluate methods for hypothesis generation? To address this, we introduce HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. We evaluate four state-of-the-art LLMs combined with six existing hypothesis-generation methods. Overall, our results suggest that existing methods are capable of discovering valid and novel patterns in the data. However, the results from synthetic datasets indicate that there is still significant room for improvement, as current hypothesis generation methods do not fully uncover all relevant or meaningful patterns. Specifically, in synthetic settings, as task difficulty increases, performance significantly drops, with best models and methods only recovering 38.8% of the ground-truth hypotheses. These findings highlight challenges in hypothesis generation and demonstrate that HypoBench serves as a valuable resource for improving AI systems designed to assist scientific discovery.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7784.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7784.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPOBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HYPOBENCH: A benchmark for hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A principled multi-domain benchmark introduced in this paper to evaluate LLMs and hypothesis-generation methods across explanatory power, practical utility, generalizability, and hypothesis discovery rate; contains 194 datasets (7 real-world + 5 synthetic).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini, Qwen-2.5-72B-Instruct, Llama-3.1-70B-Instruct, DeepSeek-R1-Distilled-Llama-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o-mini (evaluation variant), Qwen-2.5-72B, Llama-3.1-70B, DeepSeek-R1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / machine learning applied to social-science and scientific discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (natural-language theories/explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>HYPOBENCH evaluation suite</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A multi-dimensional evaluation combining synthetic datasets with known ground-truth and real-world tasks with IND/OOD splits; measures explanatory power, practical utility, generalizability, novelty/plausibility/clarity, and hypothesis discovery (HDR).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hypothesis Discovery Rate (HDR), Feature Discovery Rate (FDR), Relationship Correctness (RC), Accuracy, F1, MSE, novelty/plausibility/clarity ratings</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>HDR = FDR * RC; FDR = |Ẑ ∩ Z| / |Z|; RC: correctness of discovered relationships (rated by LLM judge M_r in [0,1] or discrete scores used in prompts); Accuracy/F1 standard classification metrics; MSE for regression tasks; novelty/plausibility/clarity each rated on 1–5.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>HYPOBENCH (194 datasets: 7 real-world classification tasks + 5 synthetic domains)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Qualitative properties (novelty, plausibility, clarity) are evaluated using GPT-4o as a judge (M_q) with 1–5 scales following prompts adapted from prior human expert rating study; no explicit human rater counts reported in this paper (LLM-as-judge used instead).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Aggregate: best model/method recovered up to 93.8% HDR at base difficulty; HDR drops to 38.8% at hardest settings; overall best aggregated HDR ~0.46 for HYPOGENIC in Table 11; best OOD accuracy for LITERATURE+DATA was 75.30 (example).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Real-world datasets lack ground-truth hypotheses making precise evaluation difficult; many qualitative ratings use an LLM judge rather than human experts; synthetic-to-real transfer and metric sensitivity (e.g., RC hard to quantify for some tasks) are noted limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7784.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Discovery Rate (HDR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite metric introduced in this paper to measure how well generated hypotheses match ground-truth hypotheses in synthetic settings by combining feature recovery and correctness of relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used as LLM judge M_r for RC evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (evaluation variant)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / hypothesis generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric for hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hypothesis Discovery Rate (HDR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>HDR quantifies discovery by multiplying Feature Discovery Rate (FDR) — fraction of true features found — with Relationship Correctness (RC) — correctness of relationships for matched features as judged (by an LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HDR, FDR, RC</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>HDR = FDR * RC; FDR = |Ẑ ∩ Z| / |Z| (proportion of ground-truth features recovered); RC: rating of correctness of discovered relationships, reported in [0,1] (paper also uses discrete scoring conventions such as {0,0.25,0.5,0.75,1.0} in prompts/examples).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied primarily to HYPOBENCH synthetic datasets (presidential election, personality prediction, marine ecosystem, college admission, shoe sales).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>RC is computed via LLM judge M_r (GPT-4o) using prompts in Appendix A; feature matching uses an automated 'same variable' prompt that returns yes/no; no human rater counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: best model achieves HDR=0.938 at base difficulty; HDR drops to 0.388 at hardest settings; Table 11 reports HYPOGENIC HDR ~0.44 averaged across configs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Depends on availability of ground-truth (only in synthetic datasets); RC is judged by an LLM which can introduce bias; discrete scoring conventions and prompt design can affect RC sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7784.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feature Discovery Rate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The proportion of true underlying features Z that are discovered by the generated hypothesis set Ẑ; used as a component of HDR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (feature matching is automated, uses 'same-variable' prompts judged by an LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Feature Discovery Rate (FDR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes fraction of ground-truth features that are mentioned in generated hypotheses via a binary matching procedure (LLM prompts instructing to return 'yes'/'no' if variables overlap).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>FDR (proportion)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>FDR = |Ẑ ∩ Z| / |Z| where Ẑ is discovered features and Z is ground-truth feature set; matching uses automated prompt that ignores outcomes and matches only input variables.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used on HYPOBENCH synthetic datasets where Z is known</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Feature matching uses a system prompt instructing an expert evaluator (implemented with an LLM) to answer 'yes' or 'no' when two hypotheses discuss the same input variable(s). No human annotator counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 11 reports high FDRs for HYPOGENIC (e.g., 0.95 for GPT, 0.96 for DeepSeek), indicating methods often recover variables even when relationships may be incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Binary matching ignores relationship direction, threshold or predicted outcomes by design; may overestimate recovery if semantics differ; relies on LLM-based matching rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7784.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relationship Correctness (RC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A score evaluating how accurately the discovered relationships (between features and outcomes) match ground-truth relationships for matched features; used in HDR computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (LLM rater M_r)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Relationship Correctness (RC)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>An LLM judge rates each matched feature's relationship against the ground-truth relationship on a scale (paper describes continuous [0,1] rating and also provides discrete scoring examples used in prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>RC score (continuous 0–1 or discrete values like 0,0.25,0.5,0.75,1.0 used in prompts/examples)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>RC is the average correctness rating across matched features; guidelines penalize contradictions (score 0) and allow partial credit for partially correct/composite matches (examples given in Appendix A).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to HYPOBENCH synthetic datasets; prompts and examples in Appendix A.2/A.1.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>RC determined by GPT-4o using detailed instructions and examples; no human rater counts reported. The paper provides explicit scoring examples and tie-break rules.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 11 lists RCs around 0.43–0.48 for HYPOGENIC across models, indicating moderate correctness of relationships even when FDR is high.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>RC is judged by an LLM (GPT-4o), which may introduce bias or inconsistency; authors note RC is difficult to quantify for some tasks (e.g., marine ecosystem) and that HDR can under/over-represent quality depending on RC calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7784.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Practical Utility (LLM-based Accuracy/F1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Practical utility measured by hypothesis-based inference accuracy and F1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluates discovered hypotheses by their effectiveness in downstream classification: an LLM inference model uses the hypotheses to predict labels on test examples, reporting accuracy and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM inference model M_I (varies: GPT-4o-mini, Qwen, Llama, DeepSeek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o-mini, Qwen-2.5-72B, Llama-3.1-70B, DeepSeek-R1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / applied evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>practical utility evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hypothesis-based inference accuracy/F1</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given discovered features Ẑ and relationships f, instruct an LLM M_I to predict labels for test samples using only those hypotheses; compute accuracy and F1 against ground-truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage), F1 score</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = fraction of test examples where predicted label by M_I (given f and Ẑ) equals ground-truth; F1 the standard harmonic mean of precision and recall for binary/multiclass as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied across HYPOBENCH real-world and synthetic splits (IND/OOD); results reported in Tables 2 and 9.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automated: uses LLM M_I to perform label inference; qualitative ratings (novelty/plausibility/clarity) are evaluated separately via LLM judge, not human experts in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples: LITERATURE+DATA achieved OOD accuracy 75.30 and F1 75.00 average across real tasks; few-shot inference baseline 65.69 accuracy (GPT rows in Table 2); fine-tuned Llama achieved OOD accuracy 77.34 and IND 84.67.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Uses LLMs both to generate and to evaluate predictions (potential circularity); performance depends on the inference model chosen (cross-model tests performed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7784.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Novelty/Plausibility/Clarity (N/P/C)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Novelty, Plausibility, and Clarity qualitative ratings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three qualitative axes used to assess hypothesis quality: novelty (newness), plausibility (scientific reasonableness), and clarity (articulation), each rated on a 1–5 scale by an LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (M_q as judge)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / qualitative hypothesis assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>qualitative evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Novelty / Plausibility / Clarity ratings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide generated hypotheses alongside context (existing literature L_Q) to GPT-4o (M_q) which rates each dimension on a 1–5 scale using instructions adapted from human expert rating studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Novelty score, Plausibility score, Clarity score (each 1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>1 = lowest (not novel / not plausible / highly ambiguous) to 5 = highest (highly novel / highly plausible / exceptionally clear); prompts and scale definitions given in Appendix A.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to HYPOBENCH real-world datasets (uses curated literature L_Q per task)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Design adapted from Liu et al. (2025) human expert rating instructions; in experiments, the ratings are produced by GPT-4o (no human panel counts reported), with detailed system/user prompts provided in Appendix A.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper reports that LITERATURE-ONLY scores highest on plausibility but lowest on novelty on average; ITERATIVE REFINEMENT scores highest on novelty. Numeric averages appear in Table 4 (per-method N/P/C).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Qualitative judgments are LLM-based, not human; novelty is subjective and not the primary evaluation focus; relying on LLM judge can bias ratings toward model priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7784.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge (M_r / M_q)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based evaluators M_r (relationship rater) and M_q (qualitative rater)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses GPT-4o as an automated judge in two roles: M_r to rate relationship correctness (RC) and M_q to rate novelty/plausibility/clarity (N/P/C), with prompts provided in Appendix A.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework component</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-judge (M_r, M_q)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use a high-quality LLM (GPT-4o) to automatically rate relationship correctness on [0,1] and qualitative properties on 1–5 scales using structured prompts; used for RC, novelty, plausibility, clarity and HDR computations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>RC (0–1), novelty/plausibility/clarity (1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>RC: continuous 0–1 correctness score; N/P/C: integer 1–5 ratings with defined rubric in Appendix A. Prompts and examples are specified in the Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used across HYPOBENCH real and synthetic tasks for automated evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automated LLM judge used in place of or in addition to human raters; prompts adapted from human expert instructions; no reporting of inter-rater reliability (human) because evaluation uses a single LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM judge (GPT-4o) produced RC and N/P/C ratings used to compute HDR and qualitative summaries; cost and prompt details reported in Appendix D and A respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM-as-judge can introduce model-specific biases; single-judge evaluation limits assessment of inter-rater variance; authors acknowledge tendency of prior work to rely on LLM-as-judge and note limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7784.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPOGENIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HYPOGENIC (hypothesis generation method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hypothesis-generation algorithm (from Zhou et al., 2024) that maintains reward scores for hypotheses to balance exploitation of plausible hypotheses with exploration of novel concepts; reimplemented and evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated with GPT-4o-mini, Qwen-2.5-72B, Llama-3.1-70B, DeepSeek-R1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see model_name)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method for generating hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>HYPOGENIC (method) evaluated with HYPOBENCH metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Method maintains a hypothesis bank and reward scores to trade off exploitation/exploration; integrated with LLMs to propose and refine hypotheses, evaluated by HDR, practical utility, and qualitative ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HDR, FDR, RC, Accuracy, F1, novelty/plausibility/clarity</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Metrics as defined in HYPOBENCH (HDR = FDR * RC; accuracy and F1 per hypothesis-based inference; N/P/C 1–5 ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>HYPOBENCH synthetic and real-world datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Evaluation uses automated LLM judges (GPT-4o) for RC and qualitative ratings; generation hyperparameters (bank size 20, initialize with 10 examples, train one epoch of 200 examples) are specified in Appendix D.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>HYPOGENIC is reported as the best-performing hypothesis generation method on synthetic datasets (e.g., averaged HDR ~0.44 in Table 11) and outperforms baselines on many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Performance drops substantially with noise and distractors; sensitivity to model priors observed; method implementation details taken from prior work (Zhou et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7784.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LITERATURE+DATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LITERATURE + DATA integration method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that integrates relevant scientific literature with observational data to generate and refine hypotheses; extends HYPOGENIC and demonstrated strong performance on real-world tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated using Qwen-2.5-72B, Llama-3.1-70B, GPT-4o-mini, DeepSeek-R1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Qwen-2.5-72B, Llama-3.1-70B, GPT-4o-mini, DeepSeek-70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / literature-grounded hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LITERATURE + DATA (method) assessed with HYPOBENCH metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collects relevant literature L_Q, summarizes findings, and uses both literature and observational data to propose and refine hypotheses; evaluated on practical utility (accuracy/F1), HDR (synthetic), and N/P/C ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1, HDR, N/P/C</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy/F1 for hypothesis-based classification; HDR components as defined earlier; qualitative scores 1–5 for novelty/plausibility/clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to HYPOBENCH real-world datasets (LITERATURE-ONLY used only on real-world tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Qualitative ratings produced by GPT-4o; the method uses retrieved research papers as L_Q; Appendix D.1 lists implementation notes and default hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On real-world datasets LITERATURE + DATA achieved best performance among methods (e.g., average OOD accuracy 75.30 in Table 2); LITERATURE-ONLY scored highest on plausibility but lower on novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires curated literature retrieval; Qwen gained little from literature addition in some experiments (authors note model-specific sensitivity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7784.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IO_PROMPTING</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IO PROMPTING</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven hypothesis generation approach (from Qiu et al., 2024) that provides examples from a classification task to the model to prompt it to generate hypotheses; reimplemented here with reported hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated with GPT-4o-mini, Qwen, Llama, DeepSeek</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see model_name)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>IO PROMPTING</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide in-context examples (input-output pairs) to the LLM and prompt it to extract/discover hypotheses from the examples; used as a baseline and part of iterative procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HDR, Accuracy, F1, N/P/C</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Metrics defined as in HYPOBENCH: HDR = FDR * RC; accuracy/F1 standard; qualitative 1–5 ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>HYPOBENCH real and synthetic datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Implementation follows Qiu et al. (2024) best hyperparameters: train for 3 epochs with 10 examples, generate 5 hypotheses per iteration; evaluation uses automated LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>IO PROMPTING outperforms zero-shot inference and few-shot baselines, yields moderate HDRs (see Table 11: HDR ~0.30 across models), and improves accuracy over simple baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Performance depends on choice and number of in-context examples; less effective than HYPOGENIC and LITERATURE+DATA in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7784.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ITER_REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ITERATIVE REFINEMENT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative hypothesis generation method (from Qiu et al., 2024) that uses wrongly classified examples to refine hypotheses in a feedback loop, re-implemented and evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated with GPT-4o-mini, Qwen, Llama, DeepSeek</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ITERATIVE REFINEMENT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Starts from generated hypotheses and repeatedly tests them on classification tasks; uses misclassified examples to refine and improve hypotheses iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HDR, Accuracy, F1, novelty</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>HYPOBENCH metrics: HDR = FDR * RC; accuracy/F1 measured via LLM inference; novelty rated 1–5 by GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>HYPOBENCH datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Reimplementation uses hyperparameters from Qiu et al.: 3 epochs with 10 examples, generate 5 hypotheses each iteration and update with feedbacks; qualitative evaluation via GPT-4o judge.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ITERATIVE REFINEMENT often attains higher novelty (qualitative) and outperforms simple few-shot baselines; HDR and accuracy improvements reported but generally below HYPOGENIC and LITERATURE+DATA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Iterative refinement relies on availability of labeled data for feedback; effectiveness varies by dataset and model priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7784.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot Gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot generation (LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline method that prompts LLMs directly with a task description to generate hypotheses without examples or additional context, used to measure inherent hypothesis-generation capability of pre-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini, Qwen-2.5-72B-Instruct, Llama-3.1-70B-Instruct, DeepSeek-R1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>see model_name</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / baseline evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis generation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Zero-shot generation baseline evaluated with HYPOBENCH metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt LLMs with task description and instruction to generate hypotheses; evaluate outputs with HDR, practical utility, and qualitative ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>HDR, Accuracy, F1, N/P/C</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>As defined in HYPOBENCH: HDR = FDR * RC; accuracy/F1 standard; N/P/C 1–5</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>HYPOBENCH real and synthetic datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Outputs evaluated automatically using LLM judges for RC and N/P/C; performance compared to few-shot baselines and data-driven methods.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Zero-shot generation often underperforms data-driven methods (HDR < 20% in some synthetic tasks like Presidential Election); Table 11 shows zero-shot HDR values ~0.14–0.24 depending on model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies solely on model priors; fails in counterintuitive or complex synthetic settings where priors do not align with ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7784.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7784.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MSE (regression)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mean Squared Error (MSE) for regression evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard regression metric used in this paper to evaluate hypothesis-driven predictions on continuous targets (e.g., marine ecosystem daily sunlight hours).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini (inference), other LLMs for hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / regression evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mean Squared Error (MSE)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute squared error between predicted continuous target (using hypothesis-based inference) and ground-truth; used where prediction target is numeric.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MSE</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>MSE = mean((y_true - y_pred)^2) averaged over test samples; lower is better.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Marine ecosystem dataset from DiscoveryBench adapted into HYPOBENCH (regression task)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors report that HDR was less informative for this regression task and thus used MSE to measure predictive performance; the RC component was difficult to quantify here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 8 reports MSEs for GPT and methods; ITERATIVE REFINEMENT and HYPOGENIC achieved lowest errors on the marine ecosystem dataset (exact numbers in Appendix C.1 / Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>MSE measures prediction accuracy but may not reflect explanatory adequacy of hypotheses; the paper notes HDR less accurate for this regression task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DiscoveryBench: Towards data-driven discovery with large language models <em>(Rating: 2)</em></li>
                <li>Hypothesis generation with large language models <em>(Rating: 2)</em></li>
                <li>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement <em>(Rating: 2)</em></li>
                <li>Literature meets data: A synergistic approach to hypothesis generation <em>(Rating: 2)</em></li>
                <li>ResearchAgent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 1)</em></li>
                <li>IdeaBench: Benchmarking large language models for research idea generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7784",
    "paper_id": "paper-277824142",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "HYPOBENCH",
            "name_full": "HYPOBENCH: A benchmark for hypothesis generation",
            "brief_description": "A principled multi-domain benchmark introduced in this paper to evaluate LLMs and hypothesis-generation methods across explanatory power, practical utility, generalizability, and hypothesis discovery rate; contains 194 datasets (7 real-world + 5 synthetic).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini, Qwen-2.5-72B-Instruct, Llama-3.1-70B-Instruct, DeepSeek-R1-Distilled-Llama-70B",
            "model_size": "GPT-4o-mini (evaluation variant), Qwen-2.5-72B, Llama-3.1-70B, DeepSeek-R1-70B",
            "scientific_domain": "NLP / machine learning applied to social-science and scientific discovery tasks",
            "theory_type": "hypothesis (natural-language theories/explanations)",
            "evaluation_method_name": "HYPOBENCH evaluation suite",
            "evaluation_method_description": "A multi-dimensional evaluation combining synthetic datasets with known ground-truth and real-world tasks with IND/OOD splits; measures explanatory power, practical utility, generalizability, novelty/plausibility/clarity, and hypothesis discovery (HDR).",
            "evaluation_metric": "Hypothesis Discovery Rate (HDR), Feature Discovery Rate (FDR), Relationship Correctness (RC), Accuracy, F1, MSE, novelty/plausibility/clarity ratings",
            "metric_definition": "HDR = FDR * RC; FDR = |Ẑ ∩ Z| / |Z|; RC: correctness of discovered relationships (rated by LLM judge M_r in [0,1] or discrete scores used in prompts); Accuracy/F1 standard classification metrics; MSE for regression tasks; novelty/plausibility/clarity each rated on 1–5.",
            "dataset_or_benchmark": "HYPOBENCH (194 datasets: 7 real-world classification tasks + 5 synthetic domains)",
            "human_evaluation_details": "Qualitative properties (novelty, plausibility, clarity) are evaluated using GPT-4o as a judge (M_q) with 1–5 scales following prompts adapted from prior human expert rating study; no explicit human rater counts reported in this paper (LLM-as-judge used instead).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Aggregate: best model/method recovered up to 93.8% HDR at base difficulty; HDR drops to 38.8% at hardest settings; overall best aggregated HDR ~0.46 for HYPOGENIC in Table 11; best OOD accuracy for LITERATURE+DATA was 75.30 (example).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Real-world datasets lack ground-truth hypotheses making precise evaluation difficult; many qualitative ratings use an LLM judge rather than human experts; synthetic-to-real transfer and metric sensitivity (e.g., RC hard to quantify for some tasks) are noted limitations.",
            "uuid": "e7784.0",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "HDR",
            "name_full": "Hypothesis Discovery Rate (HDR)",
            "brief_description": "A composite metric introduced in this paper to measure how well generated hypotheses match ground-truth hypotheses in synthetic settings by combining feature recovery and correctness of relationships.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used as LLM judge M_r for RC evaluation)",
            "model_size": "GPT-4o (evaluation variant)",
            "scientific_domain": "NLP / hypothesis generation evaluation",
            "theory_type": "evaluation metric for hypotheses",
            "evaluation_method_name": "Hypothesis Discovery Rate (HDR)",
            "evaluation_method_description": "HDR quantifies discovery by multiplying Feature Discovery Rate (FDR) — fraction of true features found — with Relationship Correctness (RC) — correctness of relationships for matched features as judged (by an LLM).",
            "evaluation_metric": "HDR, FDR, RC",
            "metric_definition": "HDR = FDR * RC; FDR = |Ẑ ∩ Z| / |Z| (proportion of ground-truth features recovered); RC: rating of correctness of discovered relationships, reported in [0,1] (paper also uses discrete scoring conventions such as {0,0.25,0.5,0.75,1.0} in prompts/examples).",
            "dataset_or_benchmark": "Applied primarily to HYPOBENCH synthetic datasets (presidential election, personality prediction, marine ecosystem, college admission, shoe sales).",
            "human_evaluation_details": "RC is computed via LLM judge M_r (GPT-4o) using prompts in Appendix A; feature matching uses an automated 'same variable' prompt that returns yes/no; no human rater counts reported.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Example: best model achieves HDR=0.938 at base difficulty; HDR drops to 0.388 at hardest settings; Table 11 reports HYPOGENIC HDR ~0.44 averaged across configs.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Depends on availability of ground-truth (only in synthetic datasets); RC is judged by an LLM which can introduce bias; discrete scoring conventions and prompt design can affect RC sensitivity.",
            "uuid": "e7784.1",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "FDR",
            "name_full": "Feature Discovery Rate",
            "brief_description": "The proportion of true underlying features Z that are discovered by the generated hypothesis set Ẑ; used as a component of HDR.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (feature matching is automated, uses 'same-variable' prompts judged by an LLM)",
            "model_size": "N/A",
            "scientific_domain": "NLP / hypothesis evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Feature Discovery Rate (FDR)",
            "evaluation_method_description": "Computes fraction of ground-truth features that are mentioned in generated hypotheses via a binary matching procedure (LLM prompts instructing to return 'yes'/'no' if variables overlap).",
            "evaluation_metric": "FDR (proportion)",
            "metric_definition": "FDR = |Ẑ ∩ Z| / |Z| where Ẑ is discovered features and Z is ground-truth feature set; matching uses automated prompt that ignores outcomes and matches only input variables.",
            "dataset_or_benchmark": "Used on HYPOBENCH synthetic datasets where Z is known",
            "human_evaluation_details": "Feature matching uses a system prompt instructing an expert evaluator (implemented with an LLM) to answer 'yes' or 'no' when two hypotheses discuss the same input variable(s). No human annotator counts provided.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Table 11 reports high FDRs for HYPOGENIC (e.g., 0.95 for GPT, 0.96 for DeepSeek), indicating methods often recover variables even when relationships may be incorrect.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Binary matching ignores relationship direction, threshold or predicted outcomes by design; may overestimate recovery if semantics differ; relies on LLM-based matching rules.",
            "uuid": "e7784.2",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RC",
            "name_full": "Relationship Correctness (RC)",
            "brief_description": "A score evaluating how accurately the discovered relationships (between features and outcomes) match ground-truth relationships for matched features; used in HDR computation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (LLM rater M_r)",
            "model_size": "GPT-4o (evaluation)",
            "scientific_domain": "NLP / hypothesis evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Relationship Correctness (RC)",
            "evaluation_method_description": "An LLM judge rates each matched feature's relationship against the ground-truth relationship on a scale (paper describes continuous [0,1] rating and also provides discrete scoring examples used in prompts).",
            "evaluation_metric": "RC score (continuous 0–1 or discrete values like 0,0.25,0.5,0.75,1.0 used in prompts/examples)",
            "metric_definition": "RC is the average correctness rating across matched features; guidelines penalize contradictions (score 0) and allow partial credit for partially correct/composite matches (examples given in Appendix A).",
            "dataset_or_benchmark": "Applied to HYPOBENCH synthetic datasets; prompts and examples in Appendix A.2/A.1.",
            "human_evaluation_details": "RC determined by GPT-4o using detailed instructions and examples; no human rater counts reported. The paper provides explicit scoring examples and tie-break rules.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Table 11 lists RCs around 0.43–0.48 for HYPOGENIC across models, indicating moderate correctness of relationships even when FDR is high.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "RC is judged by an LLM (GPT-4o), which may introduce bias or inconsistency; authors note RC is difficult to quantify for some tasks (e.g., marine ecosystem) and that HDR can under/over-represent quality depending on RC calibration.",
            "uuid": "e7784.3",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Practical Utility (LLM-based Accuracy/F1)",
            "name_full": "Practical utility measured by hypothesis-based inference accuracy and F1",
            "brief_description": "Evaluates discovered hypotheses by their effectiveness in downstream classification: an LLM inference model uses the hypotheses to predict labels on test examples, reporting accuracy and F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM inference model M_I (varies: GPT-4o-mini, Qwen, Llama, DeepSeek)",
            "model_size": "GPT-4o-mini, Qwen-2.5-72B, Llama-3.1-70B, DeepSeek-R1-70B",
            "scientific_domain": "NLP / applied evaluation",
            "theory_type": "practical utility evaluation",
            "evaluation_method_name": "Hypothesis-based inference accuracy/F1",
            "evaluation_method_description": "Given discovered features Ẑ and relationships f, instruct an LLM M_I to predict labels for test samples using only those hypotheses; compute accuracy and F1 against ground-truth labels.",
            "evaluation_metric": "Accuracy (percentage), F1 score",
            "metric_definition": "Accuracy = fraction of test examples where predicted label by M_I (given f and Ẑ) equals ground-truth; F1 the standard harmonic mean of precision and recall for binary/multiclass as reported.",
            "dataset_or_benchmark": "Applied across HYPOBENCH real-world and synthetic splits (IND/OOD); results reported in Tables 2 and 9.",
            "human_evaluation_details": "Automated: uses LLM M_I to perform label inference; qualitative ratings (novelty/plausibility/clarity) are evaluated separately via LLM judge, not human experts in reported experiments.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Examples: LITERATURE+DATA achieved OOD accuracy 75.30 and F1 75.00 average across real tasks; few-shot inference baseline 65.69 accuracy (GPT rows in Table 2); fine-tuned Llama achieved OOD accuracy 77.34 and IND 84.67.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Uses LLMs both to generate and to evaluate predictions (potential circularity); performance depends on the inference model chosen (cross-model tests performed).",
            "uuid": "e7784.4",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Novelty/Plausibility/Clarity (N/P/C)",
            "name_full": "Novelty, Plausibility, and Clarity qualitative ratings",
            "brief_description": "Three qualitative axes used to assess hypothesis quality: novelty (newness), plausibility (scientific reasonableness), and clarity (articulation), each rated on a 1–5 scale by an LLM judge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (M_q as judge)",
            "model_size": "GPT-4o (evaluation)",
            "scientific_domain": "NLP / qualitative hypothesis assessment",
            "theory_type": "qualitative evaluation",
            "evaluation_method_name": "Novelty / Plausibility / Clarity ratings",
            "evaluation_method_description": "Provide generated hypotheses alongside context (existing literature L_Q) to GPT-4o (M_q) which rates each dimension on a 1–5 scale using instructions adapted from human expert rating studies.",
            "evaluation_metric": "Novelty score, Plausibility score, Clarity score (each 1–5)",
            "metric_definition": "1 = lowest (not novel / not plausible / highly ambiguous) to 5 = highest (highly novel / highly plausible / exceptionally clear); prompts and scale definitions given in Appendix A.",
            "dataset_or_benchmark": "Applied to HYPOBENCH real-world datasets (uses curated literature L_Q per task)",
            "human_evaluation_details": "Design adapted from Liu et al. (2025) human expert rating instructions; in experiments, the ratings are produced by GPT-4o (no human panel counts reported), with detailed system/user prompts provided in Appendix A.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Paper reports that LITERATURE-ONLY scores highest on plausibility but lowest on novelty on average; ITERATIVE REFINEMENT scores highest on novelty. Numeric averages appear in Table 4 (per-method N/P/C).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Qualitative judgments are LLM-based, not human; novelty is subjective and not the primary evaluation focus; relying on LLM judge can bias ratings toward model priors.",
            "uuid": "e7784.5",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLM-as-judge (M_r / M_q)",
            "name_full": "LLM-based evaluators M_r (relationship rater) and M_q (qualitative rater)",
            "brief_description": "The paper uses GPT-4o as an automated judge in two roles: M_r to rate relationship correctness (RC) and M_q to rate novelty/plausibility/clarity (N/P/C), with prompts provided in Appendix A.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": "GPT-4o (evaluation)",
            "scientific_domain": "NLP / evaluation methodology",
            "theory_type": "evaluation framework component",
            "evaluation_method_name": "LLM-as-judge (M_r, M_q)",
            "evaluation_method_description": "Use a high-quality LLM (GPT-4o) to automatically rate relationship correctness on [0,1] and qualitative properties on 1–5 scales using structured prompts; used for RC, novelty, plausibility, clarity and HDR computations.",
            "evaluation_metric": "RC (0–1), novelty/plausibility/clarity (1–5)",
            "metric_definition": "RC: continuous 0–1 correctness score; N/P/C: integer 1–5 ratings with defined rubric in Appendix A. Prompts and examples are specified in the Appendix.",
            "dataset_or_benchmark": "Used across HYPOBENCH real and synthetic tasks for automated evaluation",
            "human_evaluation_details": "Automated LLM judge used in place of or in addition to human raters; prompts adapted from human expert instructions; no reporting of inter-rater reliability (human) because evaluation uses a single LLM judge.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "LLM judge (GPT-4o) produced RC and N/P/C ratings used to compute HDR and qualitative summaries; cost and prompt details reported in Appendix D and A respectively.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "LLM-as-judge can introduce model-specific biases; single-judge evaluation limits assessment of inter-rater variance; authors acknowledge tendency of prior work to rely on LLM-as-judge and note limitations.",
            "uuid": "e7784.6",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "HYPOGENIC",
            "name_full": "HYPOGENIC (hypothesis generation method)",
            "brief_description": "A hypothesis-generation algorithm (from Zhou et al., 2024) that maintains reward scores for hypotheses to balance exploitation of plausible hypotheses with exploration of novel concepts; reimplemented and evaluated in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Evaluated with GPT-4o-mini, Qwen-2.5-72B, Llama-3.1-70B, DeepSeek-R1-70B",
            "model_size": "various (see model_name)",
            "scientific_domain": "NLP / hypothesis generation",
            "theory_type": "method for generating hypotheses",
            "evaluation_method_name": "HYPOGENIC (method) evaluated with HYPOBENCH metrics",
            "evaluation_method_description": "Method maintains a hypothesis bank and reward scores to trade off exploitation/exploration; integrated with LLMs to propose and refine hypotheses, evaluated by HDR, practical utility, and qualitative ratings.",
            "evaluation_metric": "HDR, FDR, RC, Accuracy, F1, novelty/plausibility/clarity",
            "metric_definition": "Metrics as defined in HYPOBENCH (HDR = FDR * RC; accuracy and F1 per hypothesis-based inference; N/P/C 1–5 ratings).",
            "dataset_or_benchmark": "HYPOBENCH synthetic and real-world datasets",
            "human_evaluation_details": "Evaluation uses automated LLM judges (GPT-4o) for RC and qualitative ratings; generation hyperparameters (bank size 20, initialize with 10 examples, train one epoch of 200 examples) are specified in Appendix D.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "HYPOGENIC is reported as the best-performing hypothesis generation method on synthetic datasets (e.g., averaged HDR ~0.44 in Table 11) and outperforms baselines on many settings.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Performance drops substantially with noise and distractors; sensitivity to model priors observed; method implementation details taken from prior work (Zhou et al., 2024).",
            "uuid": "e7784.7",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LITERATURE+DATA",
            "name_full": "LITERATURE + DATA integration method",
            "brief_description": "A method that integrates relevant scientific literature with observational data to generate and refine hypotheses; extends HYPOGENIC and demonstrated strong performance on real-world tasks in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Evaluated using Qwen-2.5-72B, Llama-3.1-70B, GPT-4o-mini, DeepSeek-R1-70B",
            "model_size": "Qwen-2.5-72B, Llama-3.1-70B, GPT-4o-mini, DeepSeek-70B",
            "scientific_domain": "NLP / literature-grounded hypothesis generation",
            "theory_type": "method",
            "evaluation_method_name": "LITERATURE + DATA (method) assessed with HYPOBENCH metrics",
            "evaluation_method_description": "Collects relevant literature L_Q, summarizes findings, and uses both literature and observational data to propose and refine hypotheses; evaluated on practical utility (accuracy/F1), HDR (synthetic), and N/P/C ratings.",
            "evaluation_metric": "Accuracy, F1, HDR, N/P/C",
            "metric_definition": "Accuracy/F1 for hypothesis-based classification; HDR components as defined earlier; qualitative scores 1–5 for novelty/plausibility/clarity.",
            "dataset_or_benchmark": "Applied to HYPOBENCH real-world datasets (LITERATURE-ONLY used only on real-world tasks)",
            "human_evaluation_details": "Qualitative ratings produced by GPT-4o; the method uses retrieved research papers as L_Q; Appendix D.1 lists implementation notes and default hyperparameters.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "On real-world datasets LITERATURE + DATA achieved best performance among methods (e.g., average OOD accuracy 75.30 in Table 2); LITERATURE-ONLY scored highest on plausibility but lower on novelty.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Requires curated literature retrieval; Qwen gained little from literature addition in some experiments (authors note model-specific sensitivity).",
            "uuid": "e7784.8",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "IO_PROMPTING",
            "name_full": "IO PROMPTING",
            "brief_description": "A data-driven hypothesis generation approach (from Qiu et al., 2024) that provides examples from a classification task to the model to prompt it to generate hypotheses; reimplemented here with reported hyperparameters.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Evaluated with GPT-4o-mini, Qwen, Llama, DeepSeek",
            "model_size": "various (see model_name)",
            "scientific_domain": "NLP / hypothesis generation",
            "theory_type": "method",
            "evaluation_method_name": "IO PROMPTING",
            "evaluation_method_description": "Provide in-context examples (input-output pairs) to the LLM and prompt it to extract/discover hypotheses from the examples; used as a baseline and part of iterative procedures.",
            "evaluation_metric": "HDR, Accuracy, F1, N/P/C",
            "metric_definition": "Metrics defined as in HYPOBENCH: HDR = FDR * RC; accuracy/F1 standard; qualitative 1–5 ratings.",
            "dataset_or_benchmark": "HYPOBENCH real and synthetic datasets",
            "human_evaluation_details": "Implementation follows Qiu et al. (2024) best hyperparameters: train for 3 epochs with 10 examples, generate 5 hypotheses per iteration; evaluation uses automated LLM judges.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "IO PROMPTING outperforms zero-shot inference and few-shot baselines, yields moderate HDRs (see Table 11: HDR ~0.30 across models), and improves accuracy over simple baselines.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Performance depends on choice and number of in-context examples; less effective than HYPOGENIC and LITERATURE+DATA in many settings.",
            "uuid": "e7784.9",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ITER_REFINE",
            "name_full": "ITERATIVE REFINEMENT",
            "brief_description": "An iterative hypothesis generation method (from Qiu et al., 2024) that uses wrongly classified examples to refine hypotheses in a feedback loop, re-implemented and evaluated in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Evaluated with GPT-4o-mini, Qwen, Llama, DeepSeek",
            "model_size": "various",
            "scientific_domain": "NLP / hypothesis generation",
            "theory_type": "method",
            "evaluation_method_name": "ITERATIVE REFINEMENT",
            "evaluation_method_description": "Starts from generated hypotheses and repeatedly tests them on classification tasks; uses misclassified examples to refine and improve hypotheses iteratively.",
            "evaluation_metric": "HDR, Accuracy, F1, novelty",
            "metric_definition": "HYPOBENCH metrics: HDR = FDR * RC; accuracy/F1 measured via LLM inference; novelty rated 1–5 by GPT-4o.",
            "dataset_or_benchmark": "HYPOBENCH datasets",
            "human_evaluation_details": "Reimplementation uses hyperparameters from Qiu et al.: 3 epochs with 10 examples, generate 5 hypotheses each iteration and update with feedbacks; qualitative evaluation via GPT-4o judge.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "ITERATIVE REFINEMENT often attains higher novelty (qualitative) and outperforms simple few-shot baselines; HDR and accuracy improvements reported but generally below HYPOGENIC and LITERATURE+DATA.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Iterative refinement relies on availability of labeled data for feedback; effectiveness varies by dataset and model priors.",
            "uuid": "e7784.10",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Zero-shot Gen",
            "name_full": "Zero-shot generation (LLM baseline)",
            "brief_description": "Baseline method that prompts LLMs directly with a task description to generate hypotheses without examples or additional context, used to measure inherent hypothesis-generation capability of pre-trained models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini, Qwen-2.5-72B-Instruct, Llama-3.1-70B-Instruct, DeepSeek-R1-70B",
            "model_size": "see model_name",
            "scientific_domain": "NLP / baseline evaluation",
            "theory_type": "hypothesis generation baseline",
            "evaluation_method_name": "Zero-shot generation baseline evaluated with HYPOBENCH metrics",
            "evaluation_method_description": "Prompt LLMs with task description and instruction to generate hypotheses; evaluate outputs with HDR, practical utility, and qualitative ratings.",
            "evaluation_metric": "HDR, Accuracy, F1, N/P/C",
            "metric_definition": "As defined in HYPOBENCH: HDR = FDR * RC; accuracy/F1 standard; N/P/C 1–5",
            "dataset_or_benchmark": "HYPOBENCH real and synthetic datasets",
            "human_evaluation_details": "Outputs evaluated automatically using LLM judges for RC and N/P/C; performance compared to few-shot baselines and data-driven methods.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Zero-shot generation often underperforms data-driven methods (HDR &lt; 20% in some synthetic tasks like Presidential Election); Table 11 shows zero-shot HDR values ~0.14–0.24 depending on model.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Relies solely on model priors; fails in counterintuitive or complex synthetic settings where priors do not align with ground-truth.",
            "uuid": "e7784.11",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MSE (regression)",
            "name_full": "Mean Squared Error (MSE) for regression evaluation",
            "brief_description": "Standard regression metric used in this paper to evaluate hypothesis-driven predictions on continuous targets (e.g., marine ecosystem daily sunlight hours).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini (inference), other LLMs for hypothesis generation",
            "model_size": "various",
            "scientific_domain": "machine learning / regression evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Mean Squared Error (MSE)",
            "evaluation_method_description": "Compute squared error between predicted continuous target (using hypothesis-based inference) and ground-truth; used where prediction target is numeric.",
            "evaluation_metric": "MSE",
            "metric_definition": "MSE = mean((y_true - y_pred)^2) averaged over test samples; lower is better.",
            "dataset_or_benchmark": "Marine ecosystem dataset from DiscoveryBench adapted into HYPOBENCH (regression task)",
            "human_evaluation_details": "Authors report that HDR was less informative for this regression task and thus used MSE to measure predictive performance; the RC component was difficult to quantify here.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Table 8 reports MSEs for GPT and methods; ITERATIVE REFINEMENT and HYPOGENIC achieved lowest errors on the marine ecosystem dataset (exact numbers in Appendix C.1 / Table 8).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "MSE measures prediction accuracy but may not reflect explanatory adequacy of hypotheses; the paper notes HDR less accurate for this regression task.",
            "uuid": "e7784.12",
            "source_info": {
                "paper_title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DiscoveryBench: Towards data-driven discovery with large language models",
            "rating": 2,
            "sanitized_title": "discoverybench_towards_datadriven_discovery_with_large_language_models"
        },
        {
            "paper_title": "Hypothesis generation with large language models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement",
            "rating": 2,
            "sanitized_title": "phenomenal_yet_puzzling_testing_inductive_reasoning_capabilities_of_language_models_with_hypothesis_refinement"
        },
        {
            "paper_title": "Literature meets data: A synergistic approach to hypothesis generation",
            "rating": 2,
            "sanitized_title": "literature_meets_data_a_synergistic_approach_to_hypothesis_generation"
        },
        {
            "paper_title": "ResearchAgent: Iterative research idea generation over scientific literature with large language models",
            "rating": 1,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "IdeaBench: Benchmarking large language models for research idea generation",
            "rating": 1,
            "sanitized_title": "ideabench_benchmarking_large_language_models_for_research_idea_generation"
        }
    ],
    "cost": 0.02314825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation
15 Apr 2025</p>
<p>Haokun Liu haokunliu@uchicago.edu 
Sicong Huang huang@cs.toronto.edu 
Jingyu Hu hujingy5@cs.toronto.edu 
Yangqiaoyu Zhou zhouy1@uchicago.edu 
Chenhao Tan chenhao@uchicago.edu </p>
<p>Department of Computer Science
University of Chicago ♣
University of Toronto † Chicago
60637ILUSA</p>
<p>Presidential Election College Admission Personality Prediction</p>
<p>HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation
15 Apr 202597AF093C4CBF6F3E465DD91EE3DAD893arXiv:2504.11524v1[cs.AI]
There is growing interest in hypothesis generation with large language models (LLMs).However, fundamental questions remain: what makes a good hypothesis, and how can we systematically evaluate methods for hypothesis generation?To address this, we introduce HYPOBENCH, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate.HYPOBENCH includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.We evaluate four state-of-the-art LLMs combined with six existing hypothesis-generation methods.Overall, our results suggest that existing methods are capable of discovering valid and novel patterns in the data.However, the results from synthetic datasets indicate that there is still significant room for improvement, as current hypothesis generation methods do not fully uncover all relevant or meaningful patterns.Specifically, in synthetic settings, as task difficulty increases, performance significantly drops, with best models and methods only recovering 38.8% of the ground-truth hypotheses.These findings highlight challenges in hypothesis generation and demonstrate that HY-POBENCH serves as a valuable resource for improving AI systems designed to assist scientific discovery.</p>
<p>Introduction</p>
<p>Hypothesis generation is ubiquitous in scientific discoveries (e.g., inferring the heliocentric model from observations of planets and moons) and in daily life (e.g., proposing reasons why one did not get admitted to college).Given the ability of LLMs to generate plausible outputs given input information, there is growing interest in exploring the promise of AI in hypothesis generation (Liu et al., 2025;Zhou et al., 2024;Ludwig &amp; Mullainathan, 2024;Majumder et al., 2024).However, it also becomes increasingly challenging to make sense of this literature because researchers often conflate hypothesis generation with related concepts and do not have shared evaluation practice, including datasets and metrics.In this work, we aim to provide clarity on the problem of hypothesis generation and build a benchmark to enable robust progress in this emerging area.</p>
<p>To do that, we seek to address three key questions.First, what is hypothesis generation?The excitement around hypothesis generation is accompanied with the general excitement around AI for science.Therefore, it is often mixed with studies on research ideation (e.g., Si et al., 2024;Wang et al., 2024;Radensky et al., 2025).A hypothesis is a proposed explanation for a phenomenon (Wikipedia, 2025).We thus define hypothesis generation as generating natural language theories/explanations about observed phenomena (see a formal definition in § 2).This definition closely mirrors the scientific process where theories emerge from empirical observations and applies to any phenomenon that humans seek to understand, including in daily life.For instance, given observations of planets and moons, we aim to generate hypotheses (e.g., planets orbit the sun) that explain these observations.In contrast,</p>
<p>Increase number of features:</p>
<p>Students with an A in Math will be admitted.Students with more than 2 strong activities will be admitted.Students will be admitted if they are legacy students.</p>
<p>Noise:</p>
<p>Students with an A in Math will be admitted, otherwise rejected.(10% chance the label is flipped)</p>
<p>Feature interaction:</p>
<p>Students with A in Math and at least one publication will be admitted, otherwise rejected.</p>
<p>Add distractor features:</p>
<p>Students with an A in Math will be admitted, otherwise rejected.Distractor Features:</p>
<p>• Extracurricular activities • Legacy Predictive Power Hypothesis Discovery Rate Figure 1: An overview of our benchmark.We curate 194 datasets spanning 7 real-world and 5 synthetic domains.We illustrate how difficulty levels are controlled in our synthetic settings by showing an example from the college admission task.Our evaluation measures explanatory power and interestingness of generate hypotheses.ideation primarily aims to generate new research directions from existing scientific literature.An example is proposing an alternative architecture to transformer.Ideation, especially in AI research, is often not about explaining a phenomenon and has a strong emphasis on differing from the existing literature.Recognizing this difference, our benchmark thus focuses primarily on curating observations about phenomena of interest.</p>
<p>Second, what capabilities need to be benchmarked for hypothesis generation?Given our focus on explaining an observed phenomenon, hypothesis generation builds on the following capabilities: 1) inductive reasoning, 2) abstraction and communication, and optionally 3) synthesis, integrating new observations with existing knowledge.Inductive reasoning is necessary for proposing possible theories for a given observed phenomenon.Abstraction and communication is necessary for expressing hypotheses in natural language that humans can comprehend and appreciate.When existing literature is available, synthesis allows the models to build on relevant information.We would like our dataset to capture the complexity and diversity of hypothesis generation across different domains.In particular, we build synthetic datasets that enables arbitrary control of complexity.</p>
<p>Third, how can we evaluate hypothesis generation?Existing work (Radensky et al., 2025) tends to conflate explanatory power with novelty for hypothesis generation because it is natural for scientists to expect hypotheses to contribute to scientific advances (hence novel).However, this is not necessarily required in many other settings, e.g., proposing reasons why one did not get admitted to college.Therefore, we argue that explanatory power of hypotheses should be the first-order consideration.Interestingness and contributions to the existing literature are separate considerations, and are often subjective. 1 We focus on operationalizing explanatory power and provide preliminary measurements of "interestingness".</p>
<p>Preprint.</p>
<p>Building on these conceptual considerations, we build a benchmark, HYPOBENCH, by combining both real-world datasets and synthetic datasets (see Figure 1 for an overview).Our effort is highly related to DiscoveryBench (Majumder et al., 2024).The main differences are twofold: 1) DiscoveryBench assumes that relevant features have already been identified and structured, while our work captures the fact that finding plausible features from unstructured observations is non-trivial and requires significant inductive reasoning and abstraction capabilities.2) DiscoveryBench focuses on measuring the rate of ground-truth hypothesis discovery, whereas we extend this evaluation to include additional metrics for explanatory power and preliminary metrics for interestingness.</p>
<p>Experiments on HYPOBENCH reveal that data-driven hypothesis generation methods outperform both zero-shot and few-shot inference across real-world and synthetic datasets.Among these methods, combining literature with data for hypothesis generation achieves the best performance.Our evaluation on real datasets shows that Qwen generates the most effective and generalizable hypotheses.However, existing methods struggle to balance plausibility and novelty in the hypotheses they generate.On synthetic datasets, the best model discovers 93.8% of the ground-truth hypotheses in base cases, but the discovery rate drops to 38.8% as difficulty increases.These findings underscore both the need for more effective hypothesis generation methods and the value of HYPOBENCH as a benchmark for advancing this line of research.</p>
<p>In summary, our main contributions are:</p>
<p>• We develop a systematic and principled framework for benchmarking hypothesis generation and construct the first benchmark accordingly.• We conduct the first comparison between methods and models for hypothesis generation.</p>
<p>In real-world datasets, we find that LITERATURE + DATA is the best approach and Qwen is the best model within our choices of models.• We complement our real-world tasks with carefully controlled synthetic datasets at different complexity levels, enabling direct evaluation of how well models can recover known ground-truth hypotheses and demonstrating substantial room for improvement.</p>
<p>We release all code and datasets publicly, with full information about HYPOBENCH available on our official website at https://chicagohai.github.io/HypoBench/</p>
<p>Datasets and Tasks</p>
<p>Problem formulation.We start by providing a formal definition of hypothesis generation.Given a phenomenon Q to understand, we assume access to a dataset D consisting of observations x and outcomes y and relevant literature L Q .Without loss of generality, the target variable y is determined by a function f on latent variables z, represented as y = f (z).</p>
<p>We only have access to raw observations X which encode these latent variables through some mapping g such that z = g(x) (thus, y = f (g(x))).H, in turn, verbalizes Z in ways that are interpretable to humans.The entire process of hypothesis generation can be formulated as:
Q, D, L Q → H,
where the key ingredients in building a benchmark are Q and D.</p>
<p>Benchmark construction.We use the following principles in creating the benchmark:</p>
<p>• The tasks and the underlying hypotheses should reflect realistic scenarios.</p>
<p>• The datasets should cover different skills required for hypothesis generation.</p>
<p>• The tasks should vary in difficulty and enable accurate evaluations of hypotheses.</p>
<p>Following these principles, we develop a combination of real-world and synthetic datasets.</p>
<p>Our benchmark consists of 194 datasets spanning 12 domains -7 real-world domains and 5 synthetic domains.In particular, we curate 7 distinct real-world classification tasks by adopting datasets from prior work (deceptive reviews detection, AI-generated content detection, persuasive argument prediction, mental stress detection, news headline engagement, retweets) (Zhou et al., 2024;Liu et al., 2025) and introduces a new one (paper citations)</p>
<p>to provide a comprehensive evaluation set (see Appendix B for details).For each task Q, we curate relevant literature L Q that provides context for existing findings and create in-domain (IND) and out-of-domain (OOD) splits to evaluate the generalizability of discovered hypotheses.By testing on these real-world tasks, we can assess how well different methods perform on problems that reflect actual scientific inquiry challenges.While these real-world datasets provide practical validation, the true underlying hypotheses for these open problems remain unknown, rendering precise evaluation challenging.This motivates our focus on the development of synthetic datasets with controlled mechanisms, which we describe in detail in the following section.</p>
<p>Synthetic datasets.Our synthetic datasets include presidential election, personality prediction, marine ecosystem, college admission, and shoe sales (see Appendix B for details).</p>
<p>Here we provide a detailed description of how we created synthetic datasets, which enable us to precisely measure how well different methods recover true hypotheses under various controlled conditions.These synthetic datasets complement the real-world datasets by allowing systematic evaluations on ground-truth hypotheses.</p>
<p>As discussed above, we assume that the underlying data generating process to f : z → y is implemented via a chosen classifier and g −1 : z → x through prompt-driven generation.For modeling the relationship between features and outcomes, we consider two options:</p>
<p>• Logistic regression.A logistic model is initialized with random weight vectors β c ∈ (−5, 5) and intercepts α c ∈ (−1, 1) for each class c.The probability of class c is given by
p(y = c | z) = exp(β c •z+α c ) ∑ K k=1 exp(β k •z+α k )
. This approach yields a straightforward, interpretable linear decision boundary in logistic space.</p>
<p>• Decision tree.A small decision tree is built with randomly selected splitting features and thresholds.Each node splits on a particular feature z j , with leaf nodes assigning class probabilities based on the distribution of samples that reach them.This allows for capturing nonlinear relationships and interactions among features, potentially increasing the complexity of the generated datasets.</p>
<p>Controlling dataset difficulty.</p>
<p>For the synthetic datasets, we consider the following dimensions to control the difficulty of the tasks:</p>
<p>• Noise in the outcome.When generating the labels Y, some labels may be randomly flipped, or their class probabilities can be sampled to introduce randomness.This tests the models' ability to generate robust hypotheses by identifying core patterns while disregarding noise, similar to real-world scenarios.</p>
<p>• Number of features.Increasing or decreasing the total number of correlated features Z adjusts the complexity of the dataset.</p>
<p>• Compositionality (depth).For decision trees, we vary the tree depth to allow for composite interactions between features.</p>
<p>• Distractor features.To simulate realistic settings where useful information is often mixed with irrelevant details, we consider adding distractor features Z 0 .This evaluates the models' ability to extract truly relevant features while filtering out distracting information.</p>
<p>• Textual subtlety.To evaluate models' abstraction skill, we consider adding subtlety to how features are presented in the text.We create two variants: a control group with explicit feature representation and an experimental group where features are embedded within unstructured text.For example, a political feature like endorses democratic party might be explicitly stated in the control group, while in the subtle version it appears as "I was quite taken aback by the criticism of the Supreme Court decision favoring conservatives, which led me to reconsider my position on championing gun rights."This tests the models' ability to abstract and identify underlying features from natural language.Preprint.</p>
<p>Dataset Hypothesis</p>
<p>Persuasiveness prediction (real-world)</p>
<p>Arguments that establish the author's expertise or authority on the subject, such as through credentials or past experience, is likely to be more persuasive.Paper citations (realworld)</p>
<p>Abstracts that highlight novel combinations of existing knowledge or interdisciplinary approaches are more likely to indicate higher impact.</p>
<p>Presidential election (synthetic)</p>
<p>If a person expresses concern about climate change and the two-party system, they will more likely vote for the democratic party instead of the republican party.College admission (synthetic)</p>
<p>If a student has at least one publication and A in Math, they will be admitted.If they are a first-generation college student, they can be admitted with B+ or higher.</p>
<p>Table 1: Example hypotheses from different datasets.The ones from real-world datasets are generated hypotheses, while the ones from synthetic datasets are groundtruth hypotheses.</p>
<p>Evaluations</p>
<p>To automate this process, we consider a systematic approach to evaluate hypotheses with an emphasis on explanatory power.In contrast to ideation, where novel ideas that differ from existing literature is the first-order principle, we argue that a hypothesis must first demonstrate explanatory power before its novelty becomes scientifically valuable.
= 1 | Ẑ ∩ Z| ∑ z i ∈ Ẑ∩Z M r (z i , f , f ).
Here, M r (z i , f , f ) is a rating function that evaluates if the relationship between feature z i and the outcome is correctly identified in f , compared to the ground-truth relationship in f .We employ an LLM M r to rate the correctness of the discovered relationship on a scale of [0, 1] by comparing the discovered relationship with the ground-truth relationship.We use GPT-4o for evaluating HDR and provide details of the evaluation prompts in Appendix A.</p>
<p>Practical utility.Since we target hypothesis generation in the context of classification problems, we evaluate the practical utility of the discovered hypotheses by their effectiveness in the classification tasks, similar to Liu et al. (2025) and Zhou et al. (2024).Specifically, given the discovered features Ẑ and their relationships f , we measure classification accuracy by prompting an LLM M I to predict labels for test samples using these hypotheses:
Accuracy( f , Ẑ, X) := ∑ (x i ,y i )∈X 1(y i = M I (x i , f , Ẑ))
|X| , where M I (x i , f , Ẑ) represents the model's prediction when instructed to analyze input x i in terms of the discovered features Ẑ and their relationships f with the outcome.</p>
<p>Generalizability.</p>
<p>To evaluate whether generated hypotheses can generalize beyond their original context, we assess their effectiveness on data with distribution shifts.For each real dataset in HYPOBENCH, we create paired in-domain (IND) and out-of-domain (OOD) splits.We provide the details in Appendix B. We measure generalizability by computing the hypothesis-based inference accuracy and F1 scores on both the IND and OOD splits.In addition, we conduct cross-model experiments by generating hypotheses from one model and evaluating them using a different inference model.This setup tests whether hypotheses can generalize across different models.We perform these two evaluations only on the real-world datasets.</p>
<p>Novelty, plausibility, and clarity Assessing qualitative properties of hypotheses requires comprehensive understanding of scientific standards and existing knowledge.Following Liu et al. (2025), we evaluate three key properties that determine the quality of hypotheses:</p>
<p>• Novelty: The extent to which the hypothesis offers new insights beyond established knowledge in the relevant domain.</p>
<p>• Plausibility: The degree to which the hypothesis is scientifically reasonable and consistent with existing evidence.• Clarity: Whether the hypothesis is clearly articulated, logically structured, and readily comprehensible.</p>
<p>We employ GPT-4o as a judge (M q ) to evaluate these qualities by providing it with the generated hypotheses and relevant context from existing literature L Q :</p>
<p>(Novelty, Plausibility, Clarity) = M q ( f , Ẑ, L Q ).</p>
<p>For each dimension, the model rates hypotheses on a scale of 1-5 following instructions adapted from the human expert rating study in the previous work, with detailed prompts provided in Appendix A.</p>
<p>To summarize, our framework addresses a key limitation in existing evaluation approaches: the tendency to overly emphasize novelty without sufficiently assessing more fundamental properties like explanatory power and plausibility.</p>
<p>Experiment Setup</p>
<p>Models.In this work, we evaluate four models: GPT-4o-mini (GPT), Qwen-2.5-72B-Instruct(Qwen), Llama-3.1-70B-Instruct(Llama), and DeepSeek-R1-Distilled-Llama-70B (DeepSeek).</p>
<p>For each model, we evaluate their zero-shot and few-shot inference performance for practical utility.We also evaluate a collection of hypothesis generation methods across all metrics in § 3.In addition, due to the lack of ground-truth hypotheses, we finetune a Llama-3.1-8Bmodel (Llama-8B) as a comparison point for each real dataset that can learn from a much large number of instances.Specifically, for each real dataset, we finetune Llama-8B on the IND training split and evaluate it on both the IND test set and the OOD set.</p>
<p>Hypothesis generation methods.We present the first comprehensive evaluation of various hypothesis generation approaches using state-of-the-art LLMs including GPT, Llama, Qwen, and DeepSeek.We benchmark the following methods (refer to Appendix D.1 for implementation details):</p>
<p>• Zero-shot generation.This method directly prompts LLMs with a task description or research question for generating hypotheses without additional context or examples.</p>
<p>The method relies solely on the model's pre-trained knowledge to formulate scientific hypotheses, offering insights into LLMs' inherent hypothesis generation capabilities.• Literature-based generation.Several works (Wang et al., 2024;Radensky et al., 2025) have explored literature-based approaches for the ideation problem.We use the LITERATURE-ONLY adaptation from Liu et al. (2025), where the method first collects relevant research papers, prompts LLMs to summarize key findings, and then generates new hypotheses based on these insights.We evaluate this approach exclusively on the real-world datasets in our benchmark.• IO PROMPTING (Qiu et al., 2024).This method provides examples from a classification task to the model and prompts it to generate hypotheses.• ITERATIVE REFINEMENT (Qiu et al., 2024).This method builds upon IO PROMPTING by implementing a feedback loop where generated hypotheses are tested with the target classification tasks.The model uses the wrongly classified examples to refine the hypotheses, creating an iterative improvement process that enhances hypothesis quality through empirical validation.We re-implement IO PROMPTING and ITERATIVE REFINEMENT using the same hyperparameters as in the original work.</p>
<p>• HYPOGENIC (Zhou et al., 2024).The method maintains reward scores for hypotheses to balance exploitation of plausible hypotheses with exploration of novel concepts.</p>
<p>• LITERATURE + DATA (Liu et al. (2025)).This method extends HYPOGENIC by integrating relevant scientific literature alongside observational data for hypothesis refinement.</p>
<p>Results</p>
<p>Evaluation results on real-world datasets (comparison between methods); Table 2.In real-world datasets, we first observe that zero-shot generation and LITERATURE-ONLY outperforms zero-shot inference on average accuracy and F1, which suggests that the models are able to summarize useful hypothesis from its pretrained knowledge and existing literature.However, few-shot inference consistently outperforms both zero-shot generation and LITERATURE-ONLY, indicating that merely generating hypotheses based on prior knowledge is insufficient for generating effective hypotheses.</p>
<p>Additionally, we observe that data-driven hypothesis generation methods including IO PROMPTING, ITERATIVE REFINEMENT, HYPOGENIC, and LITERATURE + DATA all outperform few-shot inference.This improvement demonstrates that these approaches to hypothesis generation are capable of extracting and synthesizing more useful information from the data than a few examples alone.Among these methods, LITERATURE + DATA achieves the best performance, highlighting the complementary benefits of integrating both literature knowledge and empirical data for hypothesis generation.</p>
<p>Evaluation results on real-world datasets (comparison between models); Table 2.</p>
<p>When it comes to models, they perform differently in a few ways: Llama achieves the best performance when using few-shot inference, outperforming the other models by 5.31% on average accuracy, indicating potential data contamination in Llama's training data.Interestingly, Qwen achieves the best performance when using the strongest hypothesis generation method (LITERATURE + DATA), surpassing the other models by an average accuracy of 2.50%.This suggest that Qwen is particularly good at coming up with effective and generalizable hypotheses.Notably, Qwen shows an interesting trend: it gains little from the inclusion of literature information.While the average improvement across all other models when adding literature to data-driven  97 3.83 3.14 2.86 3.49 3.11 2.74 3.83 3.37 2.63 3.86 3 hypotheses is 4.29%, Qwen improves by only 0.14%.This highlights a potential limitation in Qwen's ability to incorporate external knowledge during hypothesis generation.</p>
<p>Table 3 shows the cross-model inference performance.Hypotheses generated by Qwen, Llama, and DeepSeek generalize well across models within this subgroup, with an average accuracy drop of only 3.43% compared to using the original generation model.This pattern is particularly strong between Llama and DeepSeek, likely because DeepSeek-R1-Distilled-Llama belongs to the same model family as Llama.In contrast, GPT seems to differ substantially from the other models.</p>
<p>When comparing the performance of hypothesis generation with fine-tuned Llama, all hypothesis generation methods perform on par with the finetuned Llama-8B on the OOD datasets, sometimes even marginally better, with Qwen leading by 0.61%.This observation further validates the effectiveness of current hypothesis generation approaches.However, the number is substantially lower in IND, where the best model Qwen underperforms by 8.72% (see the full IND results in Table 9).Since we do not know the groundtruth hypotheses in these real-world datasets, this gap could suggest potential room for further improvements as the fine-tuned Llama is capable of achieving higher IND performance.This inconclusiveness about upbound further motivates our creation of synthetic datasets.</p>
<p>Qualitative ratings of hypotheses in real-world datasets.Table 4 shows that on average, LITERATURE-ONLY generated hypotheses score highest in terms of plausibility but lowest in novelty.This may because the models are likely pretrained on similar data and thus generate hypotheses that are similar to existing knowledge.In contrast, ITERATIVE RE-FINEMENT achieves the highest novelty, potentially due to its iterative refinement process that encourages the model to generate hypotheses that are distinct from the initial ones.Overall, we see that balancing plausibility and novelty is a challenging task for hypothesis generation methods, and there is no single method that excels in both metrics.</p>
<p>Evaluation results on synthetic datasets.In this evaluation, we focus primarily on HY-POGENIC, the best-performing hypothesis generation method, and analyze its performance across four different models.Figure 2 reveals clear trends in model performance as task complexity increases.With tasks at base difficulty level, i.e., one groundtruth feature, depth-1, and no noise or distractors, DeepSeek achieves the best performance among all four models, having a near-perfect HDR score of 93.8% and effectively capturing most ground-truth hypotheses.In contrast, GPT gets the lowest HDR score of 75.0% in the base difficulty.</p>
<p>However, we observe a significant drop in DeepSeek's performance with increased noise in outcomes (Figure 2c) and additional distractor features (Figure 2d), with HDR dropping to 40.0% and 38.3%, respectively.Interestingly, this performance drop is larger for DeepSeek compared to the other models, suggesting that its internal reasoning or "thinking mode" is particularly sensitive to noisy conditions.GPT, on the other hand, gets affected by noise in outcome and distractors slightly less, achieving HDR scores of 36.2% and 41.7%, respectively.Combined with GPT's base difficulty performance, this result may suggest that GPT generates less diverse hypotheses, hence not fully capturing all hypotheses in base difficulty but slightly more robust under the effect of noise.</p>
<p>Additionally, we investigate the impact of compositionality (i.e., feature interaction) in ground-truth hypotheses (Figure 2b).We see that increasing the complexity from depth 1 to depth 2 does not substantially impact model performance.Instead, Qwen and Llama are able to achieve much higher performance in depth 2 compared to depth 1, with improving HDR scores from 81.3% to 93.8%, and 87.5% to 100%, respectively.This suggests that Qwen and Llama are more likely to capture interactions between two features.However, a further increase from depth 2 to depth 3 and 4 significantly reduces HDR scores for all models, and the best model DeepSeek, in this configuration, only achieves HDR score of 38.8%.This indicates that current models can effectively discover hypotheses involving interactions between two features but face substantial challenges against more complex feature interactions.In Figure 2e, we compare the average HDR scores across all tasks without subtlety versus with subtlety in the input texts.The performance drop for all four models highlights the additional difficulty of hypothesis generation when the underlying features are implicit.</p>
<p>We include additional comparisons in Appendix C. To give a high-level summary between different methods, similar to the real datasets, we find that IO PROMPTING, ITERATIVE REFINEMENT, and HYPOGENIC outperform zero-shot inference and few-shot inference, and HYPOGENIC performs the best among all hypothesis generation methods.The trend is consistent across datasets, models, and difficulty.</p>
<p>In Figure 3, we compare the performance of four models on four synthetic tasks: PRESIDEN-TIAL ELECTION, PERSONALITY PREDICTION, COLLEGE ADMISSION, and SHOE SALES (see dataset details in Appendix B).For each task, we present aggregated results for both the base difficulty and hardest difficulty datasets.We observe that in zero-shot generation, none of the models effectively recover the ground-truth hypotheses (HDR score &lt; 20%) for the PRES-IDENTIAL ELECTION (Figure 3e) and PERSONALITY PREDICTION (Figure 3f).Conversely, all models successfully discover some ground-truth hypotheses in COLLEGE ADMISSION and SHOE SALES tasks, achieving HDR scores exceeding 50% at base difficulty.This suggests that the models' prior knowledge aligns more closely with COLLEGE ADMISSION and SHOE SALES tasks than with PRESIDENTIAL ELECTION and PERSONALITY PREDICTION.This trend is consistent with HYPOGENIC, where Llama and DeepSeek achieve perfect HDR scores (100%) at the base difficulty of the COLLEGE ADMISSION task (Figure 3g), and Qwen, Llama, and DeepSeek similarly achieve HDR scores of 100% for the SHOE SALES task (Figure 3h).In contrast, the models perform relatively poorly on PRESIDENTIAL ELECTION and PER-SONALITY PREDICTION.Specifically, GPT and DeepSeek, the best-performing models at base difficulty, achieve HDR scores of only 66.7% and 48.3%, respectively.For the harder levels, all models yield HDR scores below 40% across both tasks.These findings underscore Preprint.the significant influence of model priors on hypothesis generation quality, emphasizing the utility of HYPOBENCH's diverse task settings for evaluating different model priors.</p>
<p>Zero-shot Generation</p>
<p>To further explore the impact of model priors, we conduct an additional experiment comparing original and counterintuitive versions.Specifically, for the COLLEGE ADMISSION task, we create counterintuitive counterparts at all difficulty levels by inverting ground-truth hypotheses (e.g., from "Students with an A in Math will be admitted" to "Students with an F in Math will be admitted").In Figure 4, we report HDR scores for HYPOGENIC across all models and levels for both the original and counterintuitive datasets.</p>
<p>We find that when the number of features and compositionality are low (1 or 5 features, depth 1 or 2), all models except GPT still manage to capture the ground-truth hypotheses (Figures 4a, 4b, 4e and 4f).However, as complexity increases, all four models struggle significantly, achieving average HDR scores below 15%.Notably, DeepSeek consistently outperforms the other models in counterintuitive scenarios with greater complexity, suggesting that DeepSeek's "thinking-mode" enhances performance when model priors provide little guidance.Additionally, increasing noise in outcome and number of distractor variables primarily impact Qwen, leading to HDR scores dropping to 0% under high distractor conditions (Figures 4c, 4d, 4g and 4h).Overall, these results highlight the effect model priors have on hypothesis generation quality, particularly under challenging conditions where priors are less helpful.This further demonstrates HYPOBENCH's value as a comprehensive benchmark for evaluating diverse models and methods.</p>
<p>Related Work</p>
<p>Benchmarks for research tasks.As there are growing interest in leveraging LLMs in scientific research, various benchmarks emerged for evaluating LLMs' capability in research tasks.These include agentic frameworks for data analysis (Majumder et al., 2024;Gu et al., 2024;Hu et al., 2024;Chen et al., 2024;Huang et al., 2024;Guo et al., 2024b), literature processing and information retrieval (Press et al., 2024;Ajith et al., 2024;Kang &amp; Xiong, 2024;Zhang et al., 2024), and broader scientific research tasks (Tian et al., 2024;Jansen et al., 2024).</p>
<p>In addition to DiscoveryBench (Majumder et al., 2024), other related benchmarks including Guo et al. (2024a), which benchmarks LLMs' ability to generate the core ideas of a target paper by providing the source literature that inspired it.Jansen et al. (2024) assesses LLM-driven scientific discovery pipelines in fully synthetic environments, and Hua et al.</p>
<p>Preprint.(2025) evaluates LLMs in synthetic inductive reasoning tasks.Our approach, in comparison, systematically evaluates LLMs and hypothesis generation methods across real-world and synthetic datasets, using a multi-dimensional evaluation framework that emphasizes explanatory power, practical utility, and generalizability.</p>
<p>Normal version</p>
<p>Research agents.Aside from the benchmarks, there are numerous recent works that aim to build LLM-powered agents to assist scientific research.(2025) introduce frameworks for generating hypotheses to explain real-world phenomena.In addition, some recent works explore automating the complete research process using LLMs (e.g., (Lu et al., 2024;Li et al., 2024)).Despite these advances, current evaluation approaches largely rely on human judgment or LLM-as-a-judge, and the question of what constitutes a good hypothesis still remains.We introduce HYPOBENCH as a standardized benchmark for evaluating LLMs and hypothesis generation methods across multiple dimensions, aiming to support more rigorous and principled development in this space.</p>
<p>Conclusion</p>
<p>In this work, we present HYPOBENCH, a principled benchmark for evaluating hypothesis generation methods across real-world and synthetic tasks.HYPOBENCH offers the first systematic evaluation of what makes a good hypothesis by assessing multiple dimensions such as explanatory power, practical utility, and generalizability.Our results show that while existing methods provide some explanatory value and outperform few-shot inference, there remains substantial room for improvement.These findings underscore the need for more effective hypothesis generation approaches and position HYPOBENCH as a valuable resource for future research.For future work, we consider extending the types of tasks and dataset structures in HYPOBENCH to include broader and more general observations, such as scientific reports and physical environment observations, thereby enhancing its utility for diverse scientific discoveries.</p>
<p>A Prompts</p>
<p>All our prompts for LLMs are separated into system prompts and user prompts.System prompts contain role and tone information, followed by detailed descriptions of the task and the expected response format.User prompts contain useful information for downstream tasks such as dataset generation or hypothesis evaluation.</p>
<p>A.1 Synthetic Dataset Generation</p>
<p>System Prompt You are an expert in classification tasks and synthetic dataset creation for social science research.Your task is to generate a diverse and meaningful set of classification labels for a given task.The labels must be: 1. Relevant to the task and grounded in the provided context.2. Diverse enough to cover typical, nuanced, and counterintuitive classifications.3. Actionable for further data generation steps, ensuring they capture the complexity of social science phenomena.</p>
<p>User Prompt</p>
<p>Here is the description of the classification task: <task_description> Your task is to generate <num_labels> classification labels relevant to this task.Ensure the labels: 1. Are specific and meaningful for the task.2. Capture diverse aspects, including typical and counterintuitive classifications, where applicable.3. Are clearly described to ensure they can guide the generation of features and synthetic text.</p>
<p>Please list the classification labels and provide a brief explanation for each.</p>
<p>Synthetic Dataset Label Generation</p>
<p>System Prompt You are a social scientist tasked with creating a synthetic dataset for the task that will be provided by the user.To do this, you need to define a set of phrase types that can be used as placeholders in generated textual templates.These phrase types should be relevant to the task's expression format and the provided prediction labels.</p>
<p>Example: As a person with [gender], I advocate [advocation] and support [opinion], where "gender", "advocation", and "opinion" are phrase types, acting as placeholders in the generated textual templates.</p>
<p>User Prompt</p>
<p>Here is the description of the classification task and the specified labels: <task_description></p>
<p>Labels: <labels></p>
<p>Your task is to define <num_blanks> phrase types that can be used as placeholders in the generated textual templates following the instructions provided in the system prompt.Please list the phrase types that are relevant to the provided task and labels.</p>
<p>Synthetic Dataset Template Blank Type Generation</p>
<p>System Prompt You are an expert in feature engineering for social science research and synthetic dataset creation.Your task is to generate a diverse set of descriptive phrases for a given classification task with labels and a phrase type as a placeholder given from a text template.The generated phrases must: 1. Be relevant to the task and grounded in social science principles.2. Be diverse and representative of the classification labels and the phrase type provided.</p>
<ol>
<li>Be described in a way that facilitates the generation of synthetic text and classification.</li>
</ol>
<p>Example: if the phrase type is "opinion" and one of the labels is "Democrat", one possible generated phrase could be "supports progressive policies".</p>
<p>User Prompt</p>
<p>Here is the description of the classification task and the specified labels: <task_description></p>
<p>Labels: <labels></p>
<p>Phrase Type: <phrase_type> Generate <num_phrases> descriptive phrases relevant to this task following the instructions provided in the system prompt.</p>
<p>Synthetic Dataset Feature Generation.Here each feature is represented by a phrase.</p>
<p>System Prompt</p>
<p>You are a social scientist tasked with creating a synthetic dataset for the task that will be provided by the user.</p>
<p>To do this, you need to generate a set of textual templates that can be used to create synthetic text data.These templates should be relevant to the task's expression format and the provided prediction labels.Each template should leave blankspaces for filling the set of phrase types provided by the user.</p>
<p>Example: if the user provided phrase type "gender", the prompt template generated must have a blank space available for filling this information.E.g, the generated template could include components like: "As a person with [gender], ...", where "[gender]" is a placeholder for the phrase type.</p>
<p>You must mark all the blankspaces in the template with the phrase type provided by the user, as well as square brackets to indicate the placeholder, like the example shown above.</p>
<p>User Prompt</p>
<p>Here is the description of the classification task and the specified labels: <task_description></p>
<p>Labels: <labels></p>
<p>The data template must leave blankspaces for filling the set of phrase types shown below: <phrase_types></p>
<p>Generate <num_templates> textual templates that can be used to create synthetic text data following the instructions provided in the system prompt.All templates must include placeholders for ALL the provided phrase types.Remember to mark all the blankspaces in the template with the phrase type provided by the user and square brackets to indicate the placeholder.</p>
<p>Synthetic Dataset Templates Generation</p>
<p>System Prompt You are an expert in social science research and synthetic dataset creation.Your task is to enhance the grammar of a given phrase for a specific phrase type in a generated text prompt template.</p>
<p>You will perform the operations following a series of steps.</p>
<p>Step 1: remind yourself the original text template by reiterating it here, along with the explicit instruction "not to modify any single letter of the template".</p>
<p>Step 2: rewrite those phrases in the sentences with enhanced grammar for the specified phrase type, without changing the template.</p>
<p>Step 3: list those modified phrases from the sentences.</p>
<p>User Prompt</p>
<p>You are an expert in social science research and synthetic dataset creation.Your task is to enhance the grammar of a given phrase for a specific phrase type in a generated text prompt template.</p>
<p>Here is the description of the classification task, the text template, and user specified phrase-type and corresponding phrases: <task_description> Text Template: <text_template> Phrase Type: <phrase_type> Phrases: <phrases> Direct plug in sentences: <rewritten_texts> You should output your responses step by step following the instructions below: First, remind yourself with the text template and the instruction to never replace any components of it, including all the square brackets for parsing by explicitly stating it in your response.Then rewrite the above direct plug in sentences with only modifications for the phrases within the blankspace "[<phrase_type>]" to ensure grammatical coherence with the surrounding text.The modified phrases should take the form "[modified phrase]", where the square brackets are explicitly kept.You may consider using clauses.Your rewritten sentences must keep the square brackets of "<phrase_type>", instead of removing it and erroneously changing the surrounding text of the template for this phrase_type.</p>
<p>Synthetic Dataset Feature Grammar Enhancement</p>
<p>Preprint.</p>
<p>A.2 Hypothesis Evaluations</p>
<p>Qualitative evaluations of the generated hypotheses in real datasets.For evaluating the generated hypotheses in terms of novelty, plausibility, and clarity for the real datasets, we follow Liu et al. ( 2025)'s study and adopt their instructions given to human experts.We show the exact prompts below.</p>
<p>System Prompt</p>
<p>You are an expert evaluator analyzing the novelty of scientific hypotheses.Your task is to evaluate how novel the hypothesis is compared to existing knowledge.</p>
<p>Novelty Scale (1-5): 1: Not novel -The hypothesis has already been shown, proven, or is widely known, closely mirroring existing ideas without introducing any new perspectives.</p>
<p>2: Minimally novel -The hypothesis shows slight novelty, introducing minor variations or nuances that build upon known ideas but do not offer significant new insights.</p>
<p>3: Moderately novel -The hypothesis demonstrates moderate novelty, presenting some new perspectives or angles that provide meaningful, but not groundbreaking, avenues for exploration.</p>
<p>4: Notably novel -The hypothesis is notably novel, offering unique nuances or perspectives that are welldifferentiated from existing ideas, representing valuable and fresh contributions to the field.</p>
<p>5: Highly novel -The hypothesis is highly novel, introducing a pioneering perspective or idea that has not been previously explored, opening entirely new directions for future research.</p>
<p>User Prompt</p>
<p>Evaluate the novelty of this hypothesis compared to existing knowledge: Existing Knowledge: <known_hypotheses> Hypothesis: <hypothesis> Format your response as: Score: [1-5] Reasoning: [explanation] Scientific Hypothesis Evaluation: Novelty System Prompt You are an expert evaluator analyzing the plausibility of scientific hypotheses.Your task is to evaluate if the hypothesis makes logical sense and aligns with scientific reasoning.</p>
<p>Plausibility Scale (1-5): 1: Not plausible -The hypothesis does not make sense at all, lacking logical or empirical grounding and failing to align with established knowledge or principles.</p>
<p>2: Minimally plausible -The hypothesis has significant plausibility challenges, making sense in limited contexts but contradicting existing evidence or lacking coherence with established theories.</p>
<p>3: Moderately plausible -The hypothesis makes sense overall and aligns with general principles or existing knowledge but has notable gaps or uncertainties that raise questions about its validity.</p>
<p>4: Mostly plausible -The hypothesis is mostly plausible, grounded in logical reasoning and existing evidence, with only minor uncertainties or assumptions that could reasonably be addressed.</p>
<p>5: Highly plausible -The hypothesis is highly plausible, fully aligning with established knowledge and logical reasoning, will likely be supported in experiments or theoretical consistency, and highly likely to be true.</p>
<p>User Prompt</p>
<p>Evaluate the plausibility of this hypothesis:</p>
<p>Existing Knowledge: <known_hypotheses></p>
<p>Hypothesis: <hypothesis></p>
<p>Consider:</p>
<p>-Does it make logical sense?-Are the relationships reasonable and consistent with known patterns?-Does it align with or reasonably extend existing knowledge?-Could this be tested?</p>
<p>Format your response as: Score: [1-5] Reasoning: [explanation]</p>
<p>Preprint.</p>
<p>Scientific Hypothesis Evaluation: Plausibility</p>
<p>System Prompt You are an expert evaluator analyzing the clarity of scientific hypotheses.Your task is to evaluate how clearly and unambiguously the hypothesis is stated.</p>
<p>Clarity Scale (1-5):</p>
<p>1: Highly ambiguous -The hypothesis is presented in a highly ambiguous manner, lacking clear definition and leaving significant room for interpretation or confusion.</p>
<p>2: Somewhat clear but vague -The hypothesis is somewhat defined but suffers from vague terms and insufficient detail, making it challenging to grasp its meaning or how it could be tested.</p>
<p>3: Moderately clear -The hypothesis is stated in a straightforward manner, but lacks the depth or specificity needed to fully convey its nuances, assumptions, or boundaries.</p>
<p>4: Clear and precise -The hypothesis is clearly articulated with precise terminology and sufficient detail, providing a solid understanding of its assumptions and boundaries with minimal ambiguity.</p>
<p>5: Exceptionally clear -The hypothesis is exceptionally clear, concise, and specific, with every term and aspect well-defined, leaving no room for misinterpretation and fully encapsulating its assumptions, scope, and testability.</p>
<p>User Prompt</p>
<p>Evaluate the clarity of this hypothesis in the context of existing knowledge:</p>
<p>Existing Knowledge: <known_hypotheses> Hypothesis: <hypothesis> Consider:</p>
<p>-Are all terms and concepts precisely defined?-Is the relationship between variables explicitly stated?-Is there any ambiguity that could lead to multiple interpretations?</p>
<p>Format your response as: Score: [1-5] Reasoning: [explanation] Scientific Hypothesis Evaluation: Clarity</p>
<p>Hypothesis Discovery Rate Evaluation Prompts.Here we provide the prompts we use to evaluate hypothesis discovery rate (HDR) for the synthetic datasets.We separate the feature discovery rate (FDR) and relationship correctness (RC) in the following:</p>
<p>System Prompt You are an expert evaluator analyzing hypotheses about relationships between input variables (features) and predicted outcomes (labels/classes).</p>
<p>Your task is to identify when two hypotheses discuss the SAME INPUT VARIABLE(S) or FEATURE(S).</p>
<p>Important instructions:</p>
<p>-Match ONLY based on input variables/features discussed.</p>
<p>-DO NOT match based on predicted outcomes, labels, or classes.</p>
<p>-For hypotheses mentioning multiple variables/features, respond 'yes' if ANY input variable matches.</p>
<p>-Ignore the direction, thresholds, or specific values of the relationships.</p>
<p>-Predicted outcomes or labels must be completely ignored when determining matches.</p>
<ul>
<li>Preprint.</li>
</ul>
<p>Responses must be exactly 'yes' or 'no'.</p>
<p>User Prompt Determine if these two hypotheses discuss any of the same INPUT VARIABLES or FEATURES.</p>
<p>True Hypothesis: <hyp_true> Generated Hypothesis: <hyp_gen> Remember:</p>
<p>-DO NOT consider predicted outcomes, labels, or classes.</p>
<p>-Focus ONLY on the input variables/features being discussed.</p>
<p>-Ignore relationship directions, thresholds, or specific values.</p>
<p>-Respond 'yes' if ANY input variable is shared; otherwise, respond 'no'.</p>
<p>-Return 'no' for empty or invalid hypotheses.</p>
<p>Response should be exactly 'yes' or 'no'.</p>
<p>Hypothesis Evaluation: Feature Discovery Rate</p>
<p>System Prompt You are an expert evaluator analyzing hypotheses about relationships between input variables (features) and predicted outcomes (labels/classes).</p>
<p>Your task is to evaluate how correctly a generated hypothesis captures the relationships described by the true hypothesis.</p>
<p>Important guidelines:</p>
<p>-Evaluate BOTH the variables/features AND the direction or nature of their relationships to predicted outcomes.</p>
<p>-Clearly contradictory relationships should always receive a score of 0.0.</p>
<p>-For composite hypotheses (multiple conditions), assign partial scores proportionally.</p>
<p>-Ignore irrelevant additional information if the main relationships and conditions are accurately captured.</p>
<p>-Empty or invalid hypotheses always score 0.0.</p>
<p>Scoring Examples:</p>
<p>True: 'Students with A in math AND 2+ publications are admitted.'Generated: 'Students with A in math are admitted.'Score: 0.5 (captures one of two conditions)</p>
<p>True: 'Students with A in math will be admitted.'Generated: 'Students with F in math will be admitted.'Score: 0.0 (clearly contradictory relationship)</p>
<p>True: 'Users watching health-related shows prefer health-conscious eating.'Generated: 'Users who enjoy hiking prefer health-conscious eating.'Score: 0.5 (captures correct predicted outcome but uses incorrect variable)</p>
<p>True: 'Users mentioning outdoor activities prefer healthy food.'Generated: 'Users mentioning outdoor activities prefer healthy food.'Score: 1.0 (perfect match)</p>
<p>Scoring scale: -1.0: Perfectly matches all variables and relationships -0.75: Captures primary relationship correctly but misses minor details -0.5: Partially correct (correct relationship or correct outcome, but missing important variables or conditions) -0.25: Minimal correct alignment (barely relevant but somewhat aligned in intent) -0.0: Incorrect, contradictory, or invalid/empty hypothesis User Prompt Evaluate how correctly the generated hypothesis captures the relationships described in the true hypothesis.</p>
<p>True Hypothesis: <hyp_true> Generated Hypothesis: <hyp_gen></p>
<p>Provide only the numerical score (0, 0.25, 0.5, 0.75, or 1.0).</p>
<p>Hypothesis Evaluation: Relationship Correctness</p>
<p>Preprint.</p>
<p>Task Description IND Size</p>
<p>OOD Size</p>
<p>Deception Detection</p>
<p>Distinguish genuine and fake hotel reviews.The task requires understanding subtle linguistic cues that indicate deceptive writing.</p>
<p>1,600 640</p>
<p>AI-generated Content Detection</p>
<p>Identify whether a story is written by human or AI given a writing prompt.This tests models' ability to discover distinctive features between human and AI-generated content.800 800</p>
<p>Persuasive Argument Prediction</p>
<p>Predict which text is more persuasive between pairs of arguments.</p>
<p>The task explores linguistic features that contribute to effective persuasion in written communication.</p>
<p>750 500</p>
<p>Mental Stress Detection</p>
<p>Detecting mental stress signals from Reddit posts across different communities.This task investigates linguistic features that are indicative to mental stress in social media content.</p>
<p>1,000 500</p>
<p>News Headline Engagements</p>
<p>Given a pair of headlines of the same news article, predict which one will get more clicks from readers.</p>
<p>700 453</p>
<p>Retweets</p>
<p>Given a pair of tweets, predict which one will be retweeted more.1,000 500</p>
<p>Paper Citations Classify whether an academic paper will get high or low citations.1,182 1,104 Given a person's tweet, determine the personal preferences of the user based on the content, sentiment, and language patterns.76 178,750</p>
<p>College Admission</p>
<p>Predicting whether a student will be admitted or not based on their background information.</p>
<p>7,800</p>
<p>Shoe Sales Given a customer's appearance, predict the shoe they will buy.</p>
<p>3 3300</p>
<p>Marine Ecosystem Given information about a marine ecosystem, predict the daily sunlight hours received per day at the location.</p>
<p>1 500</p>
<p>Table 6: Overview of synthetic datasets used in HYPOBENCH.</p>
<p>B Dataset Creation Details</p>
<p>B.1 Paper Citation</p>
<p>Prediction Target We consider the binary prediction target of highly cited papers.We group papers published in the same venue and year into a cohort.A paper i published in year t is considered highly cited (C i,t,n,α = 1) if, after n years, its citation count is within the top α percent of its cohort, and C i,t,n,α = 0 if it is within the bottom α percent.By focusing on the most and least successful papers, we aim to maximize the potential signal in the differences between these two classes, making the distinguishing patterns more salient for inductive reasoning algorithms.</p>
<p>Input Abstract</p>
<p>There are many possible input features.We can divide them into (1) data: the paper itself, and (2) meta-data: descriptors of the paper such as publication venue, author affiliation, citation networks, etc.We choose to use only the paper text.Our aim is to generate hypothesis to understand how the content itself can influence impact.We only include the abstract instead of the full paper because (1) the full paper has information beyond text, and would make it infeasible for LLMs and (2) current LLMs still have limitations on context length, making abstract more practical for various induction reasoning algorithms.</p>
<p>OpenAlex API We build the citation dataset using the OpenAlex API Priem et al. (2022).We noticed that the OpenAlex database is often unreliable, returning incomplete abstracts and short comentaries that are not full papers.We implemented mechanisms to automatically filter and clean data, which resulted in a total of 5324 data points.</p>
<p>Journal Selection</p>
<p>We select three journals according to the following principles:</p>
<p>• The journals should be among the most influential in their respective fields, as ranked by their impact factors from the Observatory of International Research (OOIR)2 .The impact factor is chosen because it remains one of the most widely recognized indicators of journal influence.By targeting journals with high impact factors, we aim to minimize biases arising from suspicious citation practices, such as citation cartels or citation boosting services commonly found in lower-quality venues Ibrahim et al. (2024).• The selected journals should represent three distinct academic fields, sufficiently distant from each other and with varying degree of distance.This selection strategy allows us to robustly assess the out-of-distribution (OOD) generalization capabilities of various methods.</p>
<p>Difficulty Controls</p>
<p>There are several parameters that can be used to control the difficulty of the dataset.</p>
<p>• n years after publication: The larger the n, longer the time span for the papers to accumulate citations, and thus would make it clearer which papers are truly high impact, increasing the signal to noise ratio.For results in Table 7, we used n = 2 • α percent of top and bottom papers to include: The smaller the α, the greater the gap between the citation counts of the high and low impact papers, increasing the singal to noise ratio.For results in Table 7, we used α = 0.1</p>
<p>B.2 Marine Ecosystem</p>
<p>We choose one dataset from the marine biology domain in DiscoveryBench (Majumder et al., 2024) and convert it to our desired format.Most of the marine biology datasets are constructed based on the assumption that marine ecosystems have a complex combination of environmental factors that affect the target variable of interest, so we manually found one dataset with the groundtruth hypothesis "The average daily sunlight hours at a marine location increase as water clarity improves, particularly when there are fewer clouds."We set daily sunlight hours as our prediction target and keep all features about the marine ecosystem.As a result, there are a lot of noise variables (14 in total) not related to the groundtruth hypotheses.Since the daily sunlight hours is a floating point number and DiscoveryBench mostly consists of regression tasks, we use mean squared error (MSE) to measure the performance of different methods.Results on this dataset can be found in Table 8.</p>
<p>B.3 Dataset Generation for Presidential Election and Personality Prediction</p>
<p>Overview To facilitate rigorous evaluation of machine learning models in a controlled experimental setting, we introduce a synthetic dataset generation pipeline that systematically constructs textual data with precisely defined labels.This dataset is designed to capture Preprint.</p>
<p>structured semantic variations through the integration of template-based text generation and feature-driven label assignment.</p>
<p>Dataset Structure</p>
<p>The dataset consists of textual samples constructed by combining predefined templates with variable placeholders (blanks) and corresponding feature sets.These features introduce semantic differences, with their presence determining the final assigned labels.The overall process ensures a diverse and structured dataset while maintaining grammatical and semantic coherence.The key components of the dataset generation are:</p>
<p>• Templates: Serve as textual frameworks containing blanks to be filled with features.</p>
<p>• Features: Represent semantic variations that fill gaps with templates, influencing the final classification labels.</p>
<p>• Labels: Determined based on the occurrence of features and a randomly initialized multinomial logistic regression model.</p>
<p>Text Generation Process All textual components-including labels, templates, and features-are generated through large language models (LLMs).Labels are produced by prompting the LLM with user-defined task descriptions, ensuring they are semantically meaningful.Feature generation occurs in two stages: first, the LLM identifies the blank types required within a template based on task descriptions and labels; then, the LLM generates feature values for each blank type, ensuring their relevance to the assigned labels.</p>
<p>Templates are then created based on task descriptions, labels, and feature types, with blanks designed to seamlessly integrate different feature variations while maintaining fluency.</p>
<p>As each feature type has multiple corresponding features, and each template contains multiple feature types as blanks, an exhaustive enumeration of all possible combinations is conducted.Each feature type is substituted with all possible feature values in its category, and each template undergoes substitution with all feature value combinations.This results in a comprehensive dataset capturing all possible feature interactions, ensuring diversity for model evaluation.</p>
<p>Label Assignment Mechanism Labels are assigned through a multinomial logistic regression model with randomly initialized numeric weights.The input representation for the model is a boolean vector, where each dimension corresponds to a specific feature.A value of 1 indicates the presence of the feature in the text instance, while a value of 0 denotes its absence.This structured representation allows for systematic label assignment based on predefined logistic regression model.The multinomial logistic regression model is assigned with the randomly generated weight matrix with shape [num_classes, num_features].The weight matrix is applied to input feature vectors to determine the impact of each textual feature on class assignment.</p>
<p>The label preference sorting operation is then applied independently for each feature across all classes in the weight matrix, yielding a ranked preference of classes based on their weight magnitudes.A positive weight increases the probability of assigning a feature-containing text to the corresponding class, while a negative weight decreases this probability.This process ensures that feature-class relationships are systematically captured and interpreted.The class-preference ranking obtained from the logistic regression weight matrix is translated into natural language explanations, providing an interpretable mapping between textual features and label assignments.</p>
<p>Ground Truth Hypothesis Generation Ground truth hypotheses are generated based on the class preference ranking derived from logistic regression model weights.Specifically, the model weights per feature indicate the importance of each feature for class assignment.</p>
<p>For each feature, we extract the maximum and minimum weights across classes to identify the most likely and least likely classes when a feature is present.These relationships are then converted into natural language hypotheses.For instance, a positive weight indicates that texts containing a particular feature are likely to be assigned to a given class, whereas negative weights suggest a reduced likelihood.By systematically translating these Preprint.</p>
<p>relationships into natural language statements, the dataset provides interpretable ground truth hypotheses clearly connecting textual features and their label assignments.</p>
<p>Difficulty Settings</p>
<p>To accommodate various levels of complexity, the dataset includes six predefined difficulty levels:</p>
<p>• Level 0: A single randomly selected feature determines the label; all texts containing this feature are assigned to one class, while others belong to a different class.</p>
<p>• Level 1: A single feature type influences classification, with all features of this type assigned nonzero logistic regression weights, while other feature types have no impact.</p>
<p>• Level 2: Three feature types contribute to label generation, while two additional feature types act as distractors with zero impact.</p>
<p>• Level 3: Introduces 10% label noise on top of Level 1.</p>
<p>• Level 4: Based on Level 2, with 25% of logistic regression weights randomly dropped.</p>
<p>• Level 5: Combines Level 2 conditions with 10% label noise and 25% logistic regression weight dropout.</p>
<p>Each difficulty level is available in two data presentation modes:</p>
<p>• Regular / With Subtlety Features are embedded into natural language templates, resulting in fluent and varied textual inputs.This setting simulates realistic linguistic variation and encourages models to generalize beyond surface patterns.It is well-suited for evaluating language understanding under more naturalistic conditions.</p>
<p>• No Subtlety Inputs consist of explicit enumerations of the features present in each instance, with no templated language.This setting eliminates linguistic variation, offering a controlled environment where the relationship between features and labels is made explicit.It supports fine-grained interpretability, simplifies error analysis, and isolates the impact of feature-based reasoning.Due to the lack of prompt-based augmentation, this variant is smaller in size but more deterministic in structure.</p>
<p>Combining the six predefined difficulty levels with the two presentation modes results in a total of 12 distinct datasets.</p>
<p>Contrastive Difficulty Settings for Controlled Experiments</p>
<p>In addition to the predefined difficulty levels, we also introduce contrastive difficulty settings explicitly designed for controlled experimentation through systematic variation of three hyperparameters:</p>
<ol>
<li>
<p>Number of Features per Template: Varies across the set 5, 10, 15, 20, determining the complexity and semantic richness of the textual instances.</p>
</li>
<li>
<p>Label Noise Ratio: Defined as the proportion of randomly flipped labels, systematically adjusted through the values 0, 0.1, 0.2, 0.3 to evaluate model robustness to labeling errors.</p>
</li>
<li>
<p>Weight Dropout Probability: Applied to randomly eliminate portions of logistic regression weights, varied over the range 0, 0.1, 0.2, 0.3 to assess sensitivity to incomplete or noisy feature-class relationships.</p>
</li>
</ol>
<p>By exhaustively combining these parameters, we construct a comprehensive grid search resulting in a total of 64 distinct dataset configurations.This structured approach enables fine-grained analysis of model performance under varying conditions of semantic complexity, labeling uncertainty, and structural ambiguity.</p>
<p>Preprint.</p>
<p>Dataset Splitting, Formatting, and Conversion</p>
<p>The final dataset undergoes a standard train-validation-test split, where 70% of the data is allocated for training, 10% for validation, and 20% for testing.The dataset is then converted into Hugging Face Dataset format, ensuring compatibility with modern deep learning frameworks for streamlined experimentation.</p>
<p>B.4 Dataset Generation for College Admission and Shoe Sales</p>
<p>For the college admission and shoe sales datasets, we use decision trees as the underlying model.We provide the dataset details below.</p>
<p>College Admission.For the college admission datasets, we include one base difficulty configuration and three different levels on four difficulty controls, including number of features, decision tree depth, noise level in outcome, and number of distractors.For base level, we only include one feature, tree depth one, no noise in outcome, and no distractors.We provide the detailed configurations for the other levels below:</p>
<p>• Number of features: 1, 5, 10, 15 • Tree depth: 1, 2, 3, 4 • Noise in outcome: 0%, 10%, 20%, 30%</p>
<p>• Number of distractors: 0, 3, 6, 10</p>
<p>The inputs to the college admission task will be a list of the candidate student's info, including the required features for each configuration.For example with the 5-feature configuration, we include the student's Math grade, English grade, number of publications, strong extracurricular activities, and recommendation letters.Tree depth, noise in outcome, and number of distractors will affect the underlying decision tree's decision rules accordingly.Furthermore, for each college admission dataset, we construct a counterpart containing counterintuitive hypotheses, e.g., "Students with an F grade in Math will be admitted."These counterintuitive datasets enable additional evaluation of models and hypothesis generation methods in scenarios where prior knowledge is misleading or unhelpful.We will publicly release all datasets in HYPOBENCH.More dataset details will be included in the official release.</p>
<p>B.5 Synthetic Dataset Generation Examples</p>
<p>In this section we show some example dataset generation pipeline for the presidential election task.The personality prediction datasets are generated using the same framework, and we will release the detailed code in the official release of HYPOBENCH.</p>
<p>Preprint.</p>
<ol>
<li>praises Biden's climate change policy 3. condemns government shutdown orchestrated by Republicans 4. expresses frustration over lack of third-party debate presence 5. celebrates passage of bipartisan infrastructure bill</li>
</ol>
<p>Templates Generation Using the LLM-generated feature types, we requested the LLM to produce textual templates with placeholders for feature insertion.Utilizing the Synthetic Dataset Templates Generation template from Appendix A.2, we asked for four templates aligned with the provided task description and labels.The resulting templates are:</p>
<p>• I'm planning to vote for [political_endorsement] because I strongly support their stance on [policy_stance].This is especially important to me following [politi-cal_event_reaction], and I think it's critical that we all use our voices.I know some might disagree, but I can't stand the [partisan_language] being thrown around these days.</p>
<p>•</p>
<p>Grammar Enhancement for Features</p>
<p>Having generated all requisite textual components for the dataset, we requested the LLM to enhance grammatical coherence for each feature, ensuring seamless integration into the textual templates.We employed the Synthetic Dataset Feature Grammar Enhancement template from Appendix A.2 for this purpose.</p>
<p>Ground Truth Hypothesis Generation Ground truth hypotheses are generated systematically by leveraging the weights derived from the multinomial logistic regression model.Specifically, given a weight matrix of shape [num_classes, num_features], we analyze each feature independently across classes to determine its influence on label probabilities.For each textual feature, we compute the maximum and minimum weights across all classes, identifying the most and least likely class assignments respectively.Based on the polarity of these weights (positive, negative, or neutral), the hypotheses explicitly state the likelihood of texts containing specific features being assigned to certain classes.Each hypothesis follows the structured textual format:</p>
<p>If the "<feature_type>" of the given tweet is "<feature_value>", then it is <likelihood_1> to be classified as "<label_1>" and <likelihood_2> to be classified as "<label_2>".</p>
<p>The categorization of likelihood terms follows these criteria:</p>
<p>• A positive weight implies a text with the corresponding feature is likely to belong to the associated class.</p>
<p>• A negative weight implies the feature-containing text is unlikely to belong to that class.</p>
<p>• A zero weight indicates neutrality, meaning the feature has no effect on class assignment probability.</p>
<p>• If both weights associated with a feature are negative or positive, the hypotheses differentiate levels of likelihood with terms such as highly likely/unlikely or a bit likely/unlikely based on the relative magnitude of the weights.</p>
<p>Preprint.</p>
<p>In Table 9, we include the full accuracy and F1 scores of all models and methods on the IND part of the real datasets.We observe similar trends as the results on the OOD part.Here, Qwen is still the best model, outperforming other models by 2.49% on average accuracy.We also see that LITERATURE + DATA is the best hypothesis generation method.On the other hand, finetuned Llama outperforms all other models and methods, outperforming Qwen with LITERATURE + DATA by 8.72%.This may suggest that there is still room for explaining more of the IND datasets.</p>
<p>We also report the cross model inference performance of the IND datasets in Table 10.The results reveal that Qwen, Llama, and DeepSeek are able to use the generated hypotheses from the other models in this subgroup effectively.In contrast, GPT generated hypotheses are not as effective when given to the other models, and GPT is not able to effectively use the other models' generated hypotheses for inference.In Table 12 and Table 11, we report the aggregated performance of all models and methods on the synthetic datasets across all configurations.The results reveal that GPT achieves the best performances in terms of both average HDR score and accuracy.Additionally, we see that HYPOGENIC outperforms all other hypothesis generation methods with a large margin.However, this aggregated results show that recovering the ground-truth hypotheses and fully explaining the synthetic datasets in HYPOBENCH remain challenging for all models, as the best model only achieves 50.02% on average accuracy and 46% on average HDR score.This result further highlight the value of HYPOBENCH as a resource to advance models and hypothesis generation problems.</p>
<p>C.3 Additional Results on Synthetic Datasets</p>
<p>Figure 2 :
2
Figure2: HYPOGENIC hypothesis discovery rate (HDR) results on synthetic datasets with different task difficulty.As task difficulty increases, HDR substantially drops, even to below 30% sometimes.</p>
<p>Figure 3 :
3
Figure 3: HDR scores of Zero-shot Generation and HYPOGENIC on four different synthetic datasets: Presidential Election, Personality Prediction, College Admission, and Shoe Sales.The results show that model priors can affect the quality of the generated hypotheses in different datasets.</p>
<p>Figure 4 :
4
Figure 4: HYPOGENIC HDR scores on the College Admission datasets under different difficulty controlls.Top: normal ground-truth hypotheses; bottom: counterintuitive ground-truth hypotheses.</p>
<p>Baek et al. (2024),Wang et al. (2024), andRadensky et al. (2025) focus on the ideation problem and propose methods for generating novel research ideas from existing literature, whileZhou et al. (2024) andLiu et al.</p>
<p>After [political_event_reaction], I've been re-evaluating my stance on [pol-icy_stance].While I usually align with [political_endorsement], I find the [par-tisan_language] in current discourse off-putting.I'm not sure what my vote will be yet, but these issues are at the forefront of my mind.• Despite the [partisan_language] I've seen, my vote is going to [politi-cal_endorsement] this election.Their position on [policy_stance] resonates with me, especially in light of [political_event_reaction].It's crucial that we look beyond the rhetoric and focus on real issues.• I was quite taken aback by [political_event_reaction], which led me to reconsider my position on [policy_stance].The [partisan_language] makes it difficult to stay neutral, but I'm leaning towards [political_endorsement] as the election approaches.</p>
<p>Figure 5 :
5
Figure 5: HYPOGENIC F1 scores on synthetic datasets with different task difficulty.</p>
<p>Table 1
1
presents example (generated) hypotheses to help familiarize our task setup in addition to Figure 1.See the Appendix for more examples with input data instances.</p>
<p>True hypothesis discovery rate. The</p>
<p>most important dimension we consider in our hypothesis generation problem is if the generated hypotheses Ẑ match the true hypotheses Z.Although this is intractable for real datasets because of the lack of ground-truth hypotheses, we can evaluate this dimension through controlled settings.Inspired by Majumder et al.
(2024), we evaluate the hypothesis discovery rate (HDR) by combining feature discoveryaccuracy with relationship correctness:HDR = FDR • RCwhere FDR (Feature Discovery Rate) measures the proportion of true features discovered:FDR =| Ẑ ∩ Z| |Z|,and RC (Relationship Correctness) evaluates the accuracy of discovered relationships forthe matched features:RC</p>
<p>Table 2 :
2
OOD Accuracy and F1 scores for different methods across models on real-world datasets.We report the average performance across different datasets.
Preprint.MethodGPTQwenLlamaDeepSeekAccuracyF1AccuracyF1AccuracyF1AccuracyF1Zero-shot inference61.8156.0660.0555.5466.9163.6362.8657.96Few-shot inference65.6962.6668.8667.9872.4671.2066.9064.08Zero-shot generation62.3557.5963.4359.1162.8056.4362.9157.77LITERATURE-ONLY61.8657.0762.5257.3462.0355.2959.2953.74IO PROMPTING66.0965.1174.4773.9568.2466.2761.6259.84ITERATIVE REFINEMENT66.0063.8970.4869.5269.8668.8563.6262.70HYPOGENIC71.1770.3177.8177.7572.3370.8670.0068.67LITERATURE + DATA75.3075.0077.9577.8876.2075.8874.8674.46Finetuned LlamaOOD Accuracy: 77.34 / F1: 76.00IND Accuracy: 84.67 / F1: 84.65</p>
<p>Table 3 :
3
Cross-model hypothesis-based inference accuracy for OOD data.The row indicates the model used to generate the hypotheses, while the column indicates the model used to infer the outcome from the generated hypothesis.
Gen \ InfGPT Qwen Llama DeepSeekGPT75.30 68.9564.4867.86Qwen64.67 77.9568.3374.43Llama66.14 74.7676.2072.57DeepSeek 65.67 74.9572.3874.86</p>
<p>Table 4 :
4
Qualitative evaluation results for different methods across models.We report the novelty (N), plausibility (P), and clarity (C) ratings of the generated hypotheses.
.14</p>
<p>Empty or invalid hypotheses should always return 'no'.
Examples:Hypothesis A: 'Students with high math scores and 2+ publications are admitted.'Hypothesis B: 'Students with high math scores are rejected.'Return: 'yes' (matching input variable: math scores)Hypothesis A: 'Users who frequently watch science documentaries tend to be classified as science enthusiasts.'
Hypothesis B: 'Users mentioning climate change tend to be classified as science enthusiasts.'Return: 'no' (the first discusses 'watching documentaries', the second discusses 'mentioning climate change'; the matching label 'science enthusiasts' is irrelevant) Hypothesis A: 'If entertainment preference is watching health-related TV shows, users are classified as Health-Conscious Eater.' Hypothesis B: 'Expressing enthusiasm for outdoor activities indicates health-conscious eating.'Return: 'no' (entertainment preference vs. outdoor activities; shared labels like Health-Conscious Eater should NOT count as matching)</p>
<p>Table 5 :
5
Overview of real-world datasets used in the hypothesis generation benchmark.IND and OOD splits are created based on different data sources or domains.
TaskDescription</p>
<p>Table 7 :
7
• Each chosen journal must have a sufficient number of valid abstracts (≥ 200), as determined after filtering results from the OpenAlex API.The filtering process excludes non-article content such as commentaries, editorials, and incomplete abstracts, which are occasionally retrieved by the OpenAlex API.Number of data points by journal and time range.
Journal2010-2016 2012-2022Health Affairs306238Radiology490328Conference on Neural Information Processing Systems386538</p>
<p>Table 11 :
11
Hypothesis discovery rates for all model and methods.We report feature discovery rate (FDR), relationship correctness (RC), and the final hypothesis discovery rates (HDR).The results are averaged across all difficulty configurations.
MethodGPTQwenLlamaDeepSeekFDR RC HDR FDR RC HDR FDR RC HDR FDR RC HDRZero-shot generation0.71 0.27 0.210.71 0.29 0.240.60 0.23 0.180.49 0.17 0.14IO PROMPTING0.81 0.36 0.300.79 0.37 0.310.82 0.35 0.300.81 0.35 0.29ITERATIVE REFINEMENT 0.70 0.25 0.180.82 0.22 0.190.82 0.24 0.210.68 0.24 0.19HYPOGENIC0.95 0.48 0.460.93 0.43 0.410.98 0.45 0.440.96 0.45 0.44
It is well established that predicting future success is challenging(Salganik et al.,<br />
;Siler et al., 2015).
https://ooir.org/index.php
Preprint.Given a tweet, determine the likely voting preference of the person for the 2024 U.S. presidential election.The classification should consider whether the individual is likely to vote for the Democratic candidate, the Republican candidate, a third-party candidate, or abstain from voting.The analysis should take into account explicit endorsements, political ideology, sentiment toward candidates and policies, use of partisan language, engagement with political topics, and references to past voting behavior.Additionally, indirect indicators such as reactions to major political events, stance on key social and economic issues, and alignment with partyaffiliated hashtags or slogans should be factored into the prediction.The classification should aim to capture both strong political affiliations and nuanced, context-dependent voting tendencies.Election Task DescriptionLabel Generation We applied the prompt Synthetic Dataset Label Generation from Appendix A.2, using the provided task description, and requested the LLM to generate three labels.The resulting labels generated by the LLM are:• Likely Democratic Voter This structured process ensures interpretability and clarity in the feature-to-label mappings, aiding researchers in understanding and evaluating model performance.C Additional ResultsC.1 Marine Ecosystem ResultsDue to limited time and computational resources, we only have results for GPT on the marine ecosystem dataset (see Table8).ITERATIVE REFINEMENT and HYPOGENIC achieves the lowest errors, indicating that updating and refining the hypotheses help predict the sunlight hours.The hypothesis discovery rate, on the other hand, show different trends.All four hypothesis generation methods are able to find all the true features, as indicated by their FDR.So RC is the sole determiner of the HDR.After manually looking at the RC values for hypotheses, we find it hard to quantify for this task.So the HDR value is less accurate than MSE in reflecting the quality of generated hypotheses for the marine ecosystem dataset.Preprint.C.2 IND Results on Real DatasetsZero-shot GenerationD Experiment DetailsD.1 Implementation DetailsIn this section, we report the implementation details of the selected hypothesis generation methods in HYPOBENCH.Zero-shot generation.As a baseline method for hypothesis generation, we prompt the LLMs directly with the task descriptions and instructions to generate relevant hypotheses, without providing any additional information.This method represents the models' existing knowledge and ability to extract useful hypotheses from it.Literature-based generation.Another baseline method we consider is to use existing literature for hypothesis generation.We adopt the method design fromLiu et al. (2025)and curated the necessary data for running this method.We use their default hyperparameters and generation prompts for all the experiments in HYPOBENCH.IO PROMPTING and ITERATIVE REFINEMENT.FollowingQiu et al. (2024), we reimplement the IO PROMPTING and ITERATIVE REFINEMENT method using the exact prompts and the reported best hyperparameters in the original paper.Specifically, we train for 3 epochs with 10 examples, generate 5 hypotheses in each iteration, and update the previous hypotheses with feedbacks.
LitSearch: A retrieval benchmark for scientific literature search. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao, 2024</p>
<p>ResearchAgent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, 2024</p>
<p>ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, 2024</p>
<p>BLADE: Benchmarking language model agents for data-driven science. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, M Tianmai, Lanyi Zhang, Mike A Zhu, Jeffrey Merrill, Tim Heer, Althoff, 2024</p>
<p>IdeaBench: Benchmarking large language models for research idea generation. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Eric Huang, Stefan Xie, Aidong Bekiranov, Zhang, 2024a</p>
<p>DS-Agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, Proceedings of ICML, 2024b. ICML, 2024b</p>
<p>InfiAgent-DABench: Evaluating agents on data analysis tasks. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, Fei Wu, Proceedings of ICML, Proceedings of Machine Learning Research. ICML, Machine Learning ResearchJul 2024</p>
<p>InductionBench: Llms fail in the simplest complexity class. Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang, Wang , 2025</p>
<p>MLAgentBench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Proceedings of ICML. ICML2024</p>
<p>Hazem Ibrahim, Fengyuan Liu, Yasir Zaki, Talal Rahwan, arXiv:2402.04607Google scholar is manipulatable. 2024arXiv preprint</p>
<p>DISCOVERYWORLD: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, Proceedings of NeurIPS. NeurIPS2024</p>
<p>Hao Kang, Chenyan Xiong, ResearchArena: Benchmarking llms' ability to collect and organize information as research agents. 2024</p>
<p>MLR-Copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, 2024</p>
<p>Literature meets data: A synergistic approach to hypothesis generation. Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan, 2025</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 2024</p>
<p>Machine learning as a tool for hypothesis generation*. Jens Ludwig, Sendhil Mullainathan, 10.1093/qje/qjad055The Quarterly Journal of Economics. 0033-5533012024</p>
<p>DiscoveryBench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, 2024</p>
<p>CiteME: Can language models accurately cite scientific claims?. Ori Press, Andreas Hochlehnert, Ameya Prabhu, ; , Matthias Bethge, 2024Vishaal Udandarao, Ofir Press</p>
<p>Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. Jason Priem, Heather Piwowar, Richard Orr, arXiv:2205.018332022arXiv preprint</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren, Proceedings of ICLR. ICLR2024</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, 2025</p>
<p>Experimental study of inequality and unpredictability in an artificial cultural market. science. J Matthew, Peter Sheridan Salganik, Duncan J Dodds, Watts, 2006311</p>
<p>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 2024</p>
<p>Measuring the effectiveness of scientific gatekeeping. Kyle Siler, Kirby Lee, Lisa Bero, Proceedings of the National Academy of Sciences. 11222015</p>
<p>Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Shengyan Li, Di Liu, Yutao Luo, Hao Ma, Kha Tong, Chenyu Trinh, Zihan Tian, Bohao Wang, Yanyu Wu, Shengzhu Xiong, Minhui Yin, Kilian Zhu, Yanxin Lieret, Genglin Lu, Yufeng Liu, Tianhua Du, Tao, SciCode: A research coding benchmark curated by scientists. Jamie CallanEliu Huerta, and Hao Peng2024</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proceedings of ACL. ACL2024</p>
<p>. Wikipedia, Hypothesis, 2025. March 11, 2025</p>
<p>MASSW: A new dataset and benchmark tasks for ai-assisted scientific workflows. Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, Qiaozhu Mei, 2024</p>
<p>Hypothesis generation with large language models. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, Proceedings of EMNLP Workshop of NLP for Science. EMNLP Workshop of NLP for Science2024</p>
<p>We also use the provided prompts and the reported best hyperparameters for all experiments. For both HYPOGENIC and LITERATURE + DATA, we keep a hypothesis bank size of 20, initialize with 10 examples, and train with one epoch of 200 examples. D.2 Costs Costs for running real datasets. For running the complete pipeline of HYPOBENCH on one real dataset, which includes running all hypothesis generation methods, costs approximately $5.5 in total, and 4 hours using 4 NVIDIA A100s for each of Qwen-2.5-72B-Instruct, Llama-3.1-70B-Instruct, and DeepSeek-R1-Distilled-Llama-70B. The cost breaks down to approximately $1.5 for running all the hypothesis generation methods with GPT-4o-mini, and $1 each for running the qualitative ratings using GPT-4o for all four models. Costs for running synthetic datasets. For each of the synthetic datasets, the complete pipeline of HYPOBENCH costs $2 in total, plus 4 hours using 4 NVIDIA A100s for all four selected models. The cost further breaks down to approximately $1. Preprint, Literature + Data ; Hy-Pogenic Hypogenic, Literature + Data ; Zhou, their official code release. 2024. 2025We adopt the exact implementations for. 2 for running the generation methods with GPT-4o-mini, and $0.2 for running the HDR evaluations using GPT-4o for all four models</p>            </div>
        </div>

    </div>
</body>
</html>
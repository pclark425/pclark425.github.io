<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9152 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9152</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9152</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-265609706</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.02143v3.pdf" target="_blank">Competition-Level Problems are Effective LLM Evaluators</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9152.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9152.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4 / gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large language model from OpenAI used in this study to generate C++ solutions to competition-level programming problems; evaluated zero-shot and with prompting/CoT on Codeforces problems with temporal analysis relative to training-data cutoff dates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613) / GPT-4-turbo (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (API variant gpt-4-0613) is used; paper notes its publicized training-data cutoff around September 2021. GPT-4-turbo (separately evaluated) has a later cutoff (up to April 2023). The paper does not report parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Competitive programming / algorithmic problem solving (evaluation of algorithmic reasoning and code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-to-code generation: given a Codeforces problem statement and a natural-language prompt, generate a C++ program implementing an algorithm that must pass the online judge tests (i.e., act as a text-based simulator of a human contest participant solving algorithmic problems).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>OJ outcomes aggregated into metrics: ACC#G (proportion accepted with greedy decoding t=0), ACC#GN (accepted count in sliding window), ACCk#n (proportion with >=k accepted among n samples; top-p sampling t=0.7,p=0.95), pass@k (estimated probability at least one accepted among n samples). Also error-type breakdowns (WA on test1, CE, RE, TLE, MLE).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported major results (Table 1): For problems released Oct 2010–Sep 2021 (Time1) vs Oct 2021–Nov 2023 (Time2): D1 (easy, 800-1100): ACC#G 81.42% -> 11.73% (Δ −69.69%); D2 (1200-1600): 43.72% -> 0.00% (Δ −43.72%); D3 (1700-2400): 11.41% -> 0.00% (Δ −11.41%). pass@1: D1 78.11% -> 10.54%; D2 42.38% -> 0.61%; D3 9.45% -> 0.18%. Additional sampling (ACC1#5) increases success on seen problems but not on unseen: e.g., ACC1#5 for Time1 D1 = 84.03% vs Time2 D1 = 20.09%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Training-data cutoff / data contamination (temporal overlap with training data), problem release date (seen vs unseen relative to cutoff), problem difficulty (strong negative correlation with ACC on seen problems), sampling strategy (greedy vs top-p, multiple samples improves pass@k on seen problems), prompting strategy (Chain-of-Thought gives small gains on easy D1 but none on D3), fine-tuning (supervised fine-tuning on older contest data did not improve performance on new problems), in-context demonstrations (fixed and retrieval-augmented showed little to no benefit), and comprehension of problem statement (simplification did not improve performance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to specialized code models evaluated in the paper: CodeLlama-34B-Instruct and DeepSeek-Coder-33B-Instruct. On problems released prior to model cutoff, GPT-4 outperforms these code LLMs; on unseen problems (post-cutoff), GPT-4's advantage largely disappears and performance is similar to DeepSeek-Coder (termed 'evaluation hallucination'). GPT-4-turbo (later cutoff) outperforms GPT-4 on problems released between GPT-4's and GPT-4-turbo's cutoffs, but both degrade on problems beyond their cutoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Severe performance drop on problems released after model training cutoffs (near-zero acceptance rates on medium/hard unseen problems); prevalent failure mode is 'Wrong Answer on test 1' (~70% of errors), suggesting misunderstanding of examples or problem spec; inability of simple mitigations (fine-tuning on older contest data, Chain-of-Thought, problem simplification, in-context learning) to restore performance on unseen hard problems; limited generalization and apparent reliance on memorized/problem-overlap patterns rather than principled algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use temporally segmented, uncontaminated evaluation datasets (e.g., recent Codeforces contests) to reliably assess genuine reasoning/generalization; focus future work on methods that improve model generalization and intrinsic reasoning rather than memorization; construct more and larger uncontaminated benchmarks; beware of 'evaluation hallucination' where apparent high performance is due to contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Competition-Level Problems are Effective LLM Evaluators', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9152.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9152.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama-34B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open/foundation code-specialized LLM (CodeLlama) evaluated in the study by fine-tuning and zero-shot/in-context experiments on Codeforces problems; used as a baseline to compare generalization to unseen contest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Code llama: Open foundation models for code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-34B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CodeLlama variant with ~34B parameters (as referenced), a model specialized for code tasks; paper cites it as an evaluated open-source code LLM. (No further training-data specifics provided in this paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Competitive programming / algorithmic problem solving (code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate C++ solutions to Codeforces competition problems from natural-language/problem-statement inputs; evaluated via online judge acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same OJ-based metrics as above (ACC#G, ACC#GN, pass@k, ACCk#n).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>On D1 problems CodeLlama underperforms DeepSeek-Coder; specific aggregate numbers: comparison in Table 3 shows CodeLlama ACC#G declines after March 2023 to low levels (<~10%) on recent (unseen) D1 problems (exact percentages in Table 3 of paper). Fine-tuning improved ACC#GN on pre-2017 problems (likely due to recall) but did not improve performance on recent problems.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Temporal data exposure (problems released after model's pretraining/fine-tuning data are unseen), problem difficulty, fine-tuning source distribution (fine-tuning on old contest data led to apparent gains on old but not new problems), prompting strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to DeepSeek-Coder (33B) and GPT-4; tends to underperform DeepSeek-Coder on D1 and is comparable to others on unseen newer problems after respective cutoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor generalization to newly released contest problems even after supervised fine-tuning on old contest data; gains from fine-tuning often reflect memorization of old problems rather than improved reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Fine-tuning on historical contest datasets is insufficient to generalize to novel contest problems; evaluation sets should be temporally disjoint from model training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Competition-Level Problems are Effective LLM Evaluators', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9152.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9152.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-Coder-33B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-Coder-33B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial code-specialized LLM evaluated in the paper that initially outperformed CodeLlama on D1 but showed pronounced performance decline on more recent/unseen problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-Coder-33B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeepSeek-Coder (33B Instruct variant referenced) is a code-focused LLM; the paper treats it as a representative specialized code model (no full training details provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Competitive programming / algorithmic problem solving (code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate C++ solutions for Codeforces problems and be evaluated on an online judge for correctness and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ACC#G, ACC#GN, pass@k, ACCk#n and OJ feedback categories (AC/WA/CE/etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Initially outperforms CodeLlama on D1 problems (before March 2023), but acceptance rates decline sharply for problems released after March 2023, in some intervals dropping below 10% and matching CodeLlama on unseen problems.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Recency of problems relative to model's exposure, problem difficulty, limited benefit from supervised fine-tuning on older dataset, prompting/in-context strategies had limited effect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to CodeLlama-34B and GPT-4; started stronger on older/easier problems but loses advantage on unseen/new problems (evaluation hallucination phenomenon).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Substantial degradation on unseen (post-cutoff) problems; fine-tuning and CoT did not reliably fix poor generalization on new, harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Specialized code LLMs can still suffer from lack of generalization to novel contest problems; evaluations must account for timeline of training data and avoid contaminated test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Competition-Level Problems are Effective LLM Evaluators', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9152.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9152.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics/Chemistry uses (mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Applications of LLMs in scientific fields (physics, chemistry) — related work mentions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper briefly cites other work where LLMs have been applied in scientific research domains such as physics and chemistry, without providing experimental details in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Physics; Chemistry (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Mentioned generally as 'LLMs are widely used in scientific research' (examples cited: physics exams evaluation and chemistry applications), but no simulation tasks, accuracies, or evaluation details are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>The paper only cites these domains as related areas where LLMs are used; it does not evaluate or report on simulations in those scientific subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Competition-Level Problems are Effective LLM Evaluators', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AlphaCode <em>(Rating: 2)</em></li>
                <li>Code llama: Open foundation models for code. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
                <li>Exploring durham university physics exams with large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9152",
    "paper_id": "paper-265609706",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4 / gpt-4-0613)",
            "brief_description": "A closed-source large language model from OpenAI used in this study to generate C++ solutions to competition-level programming problems; evaluated zero-shot and with prompting/CoT on Codeforces problems with temporal analysis relative to training-data cutoff dates.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613) / GPT-4-turbo (comparison)",
            "model_description": "GPT-4 (API variant gpt-4-0613) is used; paper notes its publicized training-data cutoff around September 2021. GPT-4-turbo (separately evaluated) has a later cutoff (up to April 2023). The paper does not report parameter counts.",
            "scientific_subdomain": "Competitive programming / algorithmic problem solving (evaluation of algorithmic reasoning and code generation)",
            "simulation_task": "Text-to-code generation: given a Codeforces problem statement and a natural-language prompt, generate a C++ program implementing an algorithm that must pass the online judge tests (i.e., act as a text-based simulator of a human contest participant solving algorithmic problems).",
            "evaluation_metric": "OJ outcomes aggregated into metrics: ACC#G (proportion accepted with greedy decoding t=0), ACC#GN (accepted count in sliding window), ACCk#n (proportion with &gt;=k accepted among n samples; top-p sampling t=0.7,p=0.95), pass@k (estimated probability at least one accepted among n samples). Also error-type breakdowns (WA on test1, CE, RE, TLE, MLE).",
            "simulation_accuracy": "Reported major results (Table 1): For problems released Oct 2010–Sep 2021 (Time1) vs Oct 2021–Nov 2023 (Time2): D1 (easy, 800-1100): ACC#G 81.42% -&gt; 11.73% (Δ −69.69%); D2 (1200-1600): 43.72% -&gt; 0.00% (Δ −43.72%); D3 (1700-2400): 11.41% -&gt; 0.00% (Δ −11.41%). pass@1: D1 78.11% -&gt; 10.54%; D2 42.38% -&gt; 0.61%; D3 9.45% -&gt; 0.18%. Additional sampling (ACC1#5) increases success on seen problems but not on unseen: e.g., ACC1#5 for Time1 D1 = 84.03% vs Time2 D1 = 20.09%.",
            "factors_affecting_accuracy": "Training-data cutoff / data contamination (temporal overlap with training data), problem release date (seen vs unseen relative to cutoff), problem difficulty (strong negative correlation with ACC on seen problems), sampling strategy (greedy vs top-p, multiple samples improves pass@k on seen problems), prompting strategy (Chain-of-Thought gives small gains on easy D1 but none on D3), fine-tuning (supervised fine-tuning on older contest data did not improve performance on new problems), in-context demonstrations (fixed and retrieval-augmented showed little to no benefit), and comprehension of problem statement (simplification did not improve performance).",
            "comparison_baseline": "Compared to specialized code models evaluated in the paper: CodeLlama-34B-Instruct and DeepSeek-Coder-33B-Instruct. On problems released prior to model cutoff, GPT-4 outperforms these code LLMs; on unseen problems (post-cutoff), GPT-4's advantage largely disappears and performance is similar to DeepSeek-Coder (termed 'evaluation hallucination'). GPT-4-turbo (later cutoff) outperforms GPT-4 on problems released between GPT-4's and GPT-4-turbo's cutoffs, but both degrade on problems beyond their cutoffs.",
            "limitations_or_failure_cases": "Severe performance drop on problems released after model training cutoffs (near-zero acceptance rates on medium/hard unseen problems); prevalent failure mode is 'Wrong Answer on test 1' (~70% of errors), suggesting misunderstanding of examples or problem spec; inability of simple mitigations (fine-tuning on older contest data, Chain-of-Thought, problem simplification, in-context learning) to restore performance on unseen hard problems; limited generalization and apparent reliance on memorized/problem-overlap patterns rather than principled algorithmic reasoning.",
            "author_recommendations_or_insights": "Use temporally segmented, uncontaminated evaluation datasets (e.g., recent Codeforces contests) to reliably assess genuine reasoning/generalization; focus future work on methods that improve model generalization and intrinsic reasoning rather than memorization; construct more and larger uncontaminated benchmarks; beware of 'evaluation hallucination' where apparent high performance is due to contamination.",
            "uuid": "e9152.0",
            "source_info": {
                "paper_title": "Competition-Level Problems are Effective LLM Evaluators",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "CodeLlama-34B",
            "name_full": "CodeLlama-34B-Instruct",
            "brief_description": "An open/foundation code-specialized LLM (CodeLlama) evaluated in the study by fine-tuning and zero-shot/in-context experiments on Codeforces problems; used as a baseline to compare generalization to unseen contest problems.",
            "citation_title": "Code llama: Open foundation models for code.",
            "mention_or_use": "use",
            "model_name": "CodeLlama-34B-Instruct",
            "model_description": "CodeLlama variant with ~34B parameters (as referenced), a model specialized for code tasks; paper cites it as an evaluated open-source code LLM. (No further training-data specifics provided in this paper.)",
            "scientific_subdomain": "Competitive programming / algorithmic problem solving (code generation)",
            "simulation_task": "Generate C++ solutions to Codeforces competition problems from natural-language/problem-statement inputs; evaluated via online judge acceptance.",
            "evaluation_metric": "Same OJ-based metrics as above (ACC#G, ACC#GN, pass@k, ACCk#n).",
            "simulation_accuracy": "On D1 problems CodeLlama underperforms DeepSeek-Coder; specific aggregate numbers: comparison in Table 3 shows CodeLlama ACC#G declines after March 2023 to low levels (&lt;~10%) on recent (unseen) D1 problems (exact percentages in Table 3 of paper). Fine-tuning improved ACC#GN on pre-2017 problems (likely due to recall) but did not improve performance on recent problems.",
            "factors_affecting_accuracy": "Temporal data exposure (problems released after model's pretraining/fine-tuning data are unseen), problem difficulty, fine-tuning source distribution (fine-tuning on old contest data led to apparent gains on old but not new problems), prompting strategy.",
            "comparison_baseline": "Compared to DeepSeek-Coder (33B) and GPT-4; tends to underperform DeepSeek-Coder on D1 and is comparable to others on unseen newer problems after respective cutoffs.",
            "limitations_or_failure_cases": "Poor generalization to newly released contest problems even after supervised fine-tuning on old contest data; gains from fine-tuning often reflect memorization of old problems rather than improved reasoning.",
            "author_recommendations_or_insights": "Fine-tuning on historical contest datasets is insufficient to generalize to novel contest problems; evaluation sets should be temporally disjoint from model training data.",
            "uuid": "e9152.1",
            "source_info": {
                "paper_title": "Competition-Level Problems are Effective LLM Evaluators",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeepSeek-Coder-33B",
            "name_full": "DeepSeek-Coder-33B-Instruct",
            "brief_description": "A commercial code-specialized LLM evaluated in the paper that initially outperformed CodeLlama on D1 but showed pronounced performance decline on more recent/unseen problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-Coder-33B-Instruct",
            "model_description": "DeepSeek-Coder (33B Instruct variant referenced) is a code-focused LLM; the paper treats it as a representative specialized code model (no full training details provided in paper).",
            "scientific_subdomain": "Competitive programming / algorithmic problem solving (code generation)",
            "simulation_task": "Generate C++ solutions for Codeforces problems and be evaluated on an online judge for correctness and efficiency.",
            "evaluation_metric": "ACC#G, ACC#GN, pass@k, ACCk#n and OJ feedback categories (AC/WA/CE/etc.).",
            "simulation_accuracy": "Initially outperforms CodeLlama on D1 problems (before March 2023), but acceptance rates decline sharply for problems released after March 2023, in some intervals dropping below 10% and matching CodeLlama on unseen problems.",
            "factors_affecting_accuracy": "Recency of problems relative to model's exposure, problem difficulty, limited benefit from supervised fine-tuning on older dataset, prompting/in-context strategies had limited effect.",
            "comparison_baseline": "Compared to CodeLlama-34B and GPT-4; started stronger on older/easier problems but loses advantage on unseen/new problems (evaluation hallucination phenomenon).",
            "limitations_or_failure_cases": "Substantial degradation on unseen (post-cutoff) problems; fine-tuning and CoT did not reliably fix poor generalization on new, harder tasks.",
            "author_recommendations_or_insights": "Specialized code LLMs can still suffer from lack of generalization to novel contest problems; evaluations must account for timeline of training data and avoid contaminated test sets.",
            "uuid": "e9152.2",
            "source_info": {
                "paper_title": "Competition-Level Problems are Effective LLM Evaluators",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Physics/Chemistry uses (mentions)",
            "name_full": "Applications of LLMs in scientific fields (physics, chemistry) — related work mentions",
            "brief_description": "The paper briefly cites other work where LLMs have been applied in scientific research domains such as physics and chemistry, without providing experimental details in this study.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Physics; Chemistry (mentioned in related work)",
            "simulation_task": "Mentioned generally as 'LLMs are widely used in scientific research' (examples cited: physics exams evaluation and chemistry applications), but no simulation tasks, accuracies, or evaluation details are reported in this paper.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": "The paper only cites these domains as related areas where LLMs are used; it does not evaluate or report on simulations in those scientific subdomains.",
            "uuid": "e9152.3",
            "source_info": {
                "paper_title": "Competition-Level Problems are Effective LLM Evaluators",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AlphaCode",
            "rating": 2
        },
        {
            "paper_title": "Code llama: Open foundation models for code.",
            "rating": 2,
            "sanitized_title": "code_llama_open_foundation_models_for_code"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Exploring durham university physics exams with large language models.",
            "rating": 1,
            "sanitized_title": "exploring_durham_university_physics_exams_with_large_language_models"
        }
    ],
    "cost": 0.012301749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Competition-Level Problems are Effective LLM Evaluators
4 Jun 2024</p>
<p>Yiming Huang 
Zhenghao Lin 
Xiamen University</p>
<p>Xiao Liu xiaoliu2@microsoft.com 
Yeyun Gong yegong@microsoft.com 
Shuai Lu 
Fangyu Lei 
Yaobo Liang 
Yelong Shen 
Microsoft Azure AI</p>
<p>Chen Lin chenlin@xmu.edu.cn 
Xiamen University</p>
<p>Nan Duan 
Weizhu Chen wzchen@microsoft.com 
Microsoft Azure AI</p>
<p>Microsoft Research 
Sébastien Bubeck 
Varun Chandrasekaran 
Ronen El- Dan 
Johannes Gehrke 
Eric Horvitz 
Ece Kamar 
Nicholas Carlini 
Daphne Ippolito 
Matthew Jagielski 
Katherine Lee 
Florian Tramer 
Chiyuan Zhang 
Federico Cassano 
John Gouwar 
Daniel Nguyen 
Syd- Ney Nguyen 
Luna Phipps-Costin 
Donald Pinckney 
LogeshMing-Ho Yee 
Yangtian Zi 
Carolyn Jane Anderson 
Molly Q Feldman 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Lukasz Kaiser 
Matthias Plappert 
Jacob Hilton 
Reiichiro Nakano 
Peng Di 
Jianguo Li 
Hang Yu 
Wei Jiang 
Wenting Cai 
Yang Cao 
Chaoyu Chen 
Dajun Chen 
Hongwei Chen 
Liang Chen 
Gang Fan 
Jie Gong 
Zi Gong 
Wen Hu 
Tingting Guo 
Zhichao Lei 
Ting Li 
Zheng Li 
Ming Liang 
Cong Liao 
Bingchang Liu 
Jiachen Liu 
Zhiwei Liu 
Shaojun Lu 
Min Shen 
Guang- Pei Wang 
Huan Wang 
Zhiruo Wang 
Zhaogui Xu 
Ji- Awei Yang 
Qing Ye 
Gehao Zhang 
Yu Zhang 
Zelin Zhao 
Xunjin Zheng 
Hailian Zhou 
LoubnaRaymond Li 
Ben Allal 
Niklas Muennighoff 
Denis Kocetkov 
Chenghao Mou 
Marc Marone 
Christopher Akiki 
Jenny Chim 
Qian Liu 
Evgenii Zheltonozhskii 
Terry Yue Zhuo 
Thomas Wang 
Olivier Dehaene 
Mishig Davaadorj 
Joel Lamy-Poirier 
João Monteiro 
Oleh Shliazhko 
Nicolas Gontier 
Nicholas Meade 
Armel Zebaze 
Kumar Umapathi 
Jian Zhu 
Benjamin Lipkin 
Muhtasham Oblokulov 
Rudra Murthy 
Jason Stillerman 
Sankalp Siva 
Dmitry Patel 
Marco Abulkhanov 
Manan Zocca 
Zhihan Dey 
Nour Zhang 
Urvashi Moustafa- Fahmy 
Wenhao Bhattacharyya 
Swayam Yu 
Sasha Singh 
Paulo Luccioni 
Maxim Villegas 
Fedor Ku- Nakov 
Manuel Zhdanov 
Tony Romero 
Nadav Lee 
Jennifer Timor 
Claire Ding 
Hai- Ley Schlesinger 
Jan Schoelkopf 
Tri Ebert 
Mayank Dao 
Alex Mishra 
Jennifer Gu 
Carolyn Jane Robinson 
Brendan Ander- Son 
Danish Dolan-Gavitt 
Siva Contractor 
Daniel Reddy 
Dzmitry Fried 
Yacine Bahdanau 
Jer- Nite 
Yujia Li 
David Choi 
Junyoung Chung 
Nate Kushman 
Julian Schrittwieser 
Rémi Leblond 
Baptiste Rozière 
Jonas Gehring 
Fabian Gloeckle 
Sten Sootla 
XiaoqingItai Gat 
Ellen Tan 
Yossi Adi 
Jingyu Liu 
Tal Remez 
Jérémy Rapin </p>
<p>Andres M Bran
Sam Cox, Andrew D White</p>
<p>Philippe Schwaller
2023</p>
<p>Henrique Ponde de Oliveira Pinto
Jared Kaplan, Harri Edwards, Yuri Burda</p>
<p>Nicholas Joseph
et al. 2021aGreg Brockman</p>
<p>Henrique Ponde de Oliveira Pinto
Jared Kaplan, Harri Edwards, Yuri BurdaNicholas Joseph, Greg</p>
<p>Carlos Muñoz Ferrandis
Sean Hughes</p>
<p>Leandro von Werra, and Harm de Vries
Thomas Wolf
Arjun Guha
2023</p>
<p>James Keeling, Felix GimenoTom Eccles, Agustin Dal Lago</p>
<p>Competition-Level Problems are Effective LLM Evaluators
4 Jun 2024FDC9371407DAFF791A0BAEF0E5D9F31AarXiv:2312.02143v3[cs.CL]Starcoder: may the source be with you! CoRR, abs/2305.06161.
Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently.This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills.We first provide a comprehensive evaluation of GPT-4's perceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered.Surprisingly, the perceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems.We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification.Unfortunately, none of them is able to consistently mitigate the challenges.Through our work, we emphasize the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future.</p>
<p>Introduction</p>
<p>The rise of LLMs has generated significant interest in the artificial intelligence community.These models, notably GPT-4 (OpenAI, 2023), have displayed impressive reasoning capabilities that are being harnessed in various fields (Bubeck et al., 2023).However, questions 1 have been raised about * Equal contribution.† This work was done during their internship at MSRA. ‡ Corresponding authors. 1 https://twitter.com/keirp1/status/1724518513874739618 2 0 1 0 2 0 1 2 2 0 1 4 2 0 1 6 2 0 1 8 2 0 2 0 2 0 2 2 2 0 2 4 Problem Release Date how to accurately evaluate the reasoning abilities of LLMs and the extent of data contamination issues (Mialon et al., 2023;Zhou et al., 2023).</p>
<p>Regarding these issues, our study aims to assess the reasoning capabilities of LLMs through their ability to generate algorithms for solving competition-level programming problems.These questions are meticulously crafted by experts to form rigorous competitions.They possess high quality, are unique, and exhibit excellent discriminative ability.The testing cases are also meticulously prepared.This necessitates that LLMs deduce the solution from the presented scenario, which requires a thorough understanding of algorithms, combined reasoning and coding skills, and strong problem-solving abilities.These problems thus present a significant challenge to both human coders and LLMs.Consequently, competition-level programming problems serve as effective tools for evaluating the two issues previously discussed: they assess the reasoning abilities of LLMs and, due to the strict problem selection process in competitions, reduce the likelihood of data contamina-tion in new problems.</p>
<p>Our research provides an in-depth analysis of the zero-shot performances of GPT-4 and other code LLMs on competition-level programming problems in Codeforces, considering factors such as release time, problem difficulty, and the types of errors encountered.The main insights of our study include: (1) GPT-4 performs significantly worse on programming problems released after September 2021, casting doubt on its actual reasoning abilities.(2) GPT-4 shows limited capability to solve difficult problems, indicating potential weaknesses in complex problem-solving.(3) GPT-4 struggles with the first test case, suggesting errors may stem from its understanding of the problem at hand.(4) The related phenomenon can be also observed in other LLMs, indicating that insufficient reasoning ability may be a common problem.</p>
<p>To explore possible ways to enhance the zeroshot performances of these LLMs on competitionlevel programming problems, we investigate several methods to improve performance on unseen problems.These methods include supervised fine-tuning with code-specific LLMs, Chain-of-Thought prompting (Wei et al., 2022), and problem statement simplification.Specifically, we fine-tuned CodeLlama (Rozière et al., 2023) and DeepSeek-Coder(AI, 2023), which are specialized language models designed to handle programmingrelated tasks.However, none of these methods consistently mitigated the issue or resulted in noticeable performance improvements, particularly for more difficult problems.This finding indicates that difficult and unseen programming problems are effective evaluators of LLMs.</p>
<p>Overall, the primary contributions of this study lie in proposing and validating that recent competition-level programming problems serve as an excellent data source for assessing the genuine reasoning capabilities of LLMs.We aim to foster further research in this field by innovating new approaches to address the challenge of complex reasoning problems in LLMs and by establishing reliable evaluation benchmarks for LLMs that minimize the risk of data contamination.</p>
<p>Problem Setup</p>
<p>Competition-level Programming</p>
<p>Competition-level programming presents a unique arena for testing and developing the reasoning abilities of AI models.In competitive programming, a problem typically consists of a narrative that sets the context, which models need to understand and convert into an algorithmic problem.The challenge lies in comprehending the narrative, identifying the underlying algorithmic issues, and implementing an efficient solution in programming languages such as C++ and Java.Accepted programs must satisfy stringent testing conditions, including producing outputs that exactly match with test cases, executing within memory limits, and terminating within time constraints.In contrast to prior works (Chen et al., 2021a;Austin et al., 2021;Cassano et al., 2023) focusing on basic coding abilities, competition-level programming problems require advanced reasoning and mathematical modeling skills, essential for AI.</p>
<p>Unlike the previous works that focused on Leet-Code2 (Bubeck et al., 2023;Shen et al., 2023;Sakib et al., 2023), we follow AlphaCode (Li et al., 2022) and choose Codeforces3 .Codeforces is universally acknowledged by competitors and enthusiasts in the International Collegiate Programming Competition4 (ICPC) and the International Olympiad in Informatics5 (IOI) as a popular and suitable platform for developing abilities for algorithm contests.The regular contests hosted on this platform are crafted by human experts, and contain plenty of intricate programming problems and contests of high quality.These contests come with comprehensive and robust test cases and exhibit a low degree of problem overlap.The unique nature of these contest problems makes it highly unlikely to find similar content on the internet before the competition concludes.As a result, utilizing specific time-segmented datasets, like those from contests conducted post the introduction of LLMs, serves as an effective strategy to prevent data contamination (Zhou et al., 2023).</p>
<p>Codeforces employs the Elo rating system6 to rank its users and problems, categorizing all problems into 28 distinct difficulties, ranging from 800 to 3500.Compared to commonly utilized metrics such as the ratio of accepted submissions or users, this difficulty rating mechanism is more suitable as it is based on the ranking and performance of the participants during the competition.7</p>
<p>Kefa and Park</p>
<p>The semester is already ending, so Danil made an effort and decided to visit a lesson on harmony analysis to know how does the professor look like, at least.Danil was very bored on this lesson until the teacher gave the group a simple task: find 4 vectors in 4dimensional space, such that every coordinate of every vector is 1 or - 1 and any two vectors are orthogonal.Just as a reminder, two vectors in n-dimensional space are considered to be orthogonal if and only if their scalar product is equal to zero, that is: \n.\nDanil quickly managed to come up with the solution for this problem and the teacher noticed that the problem can be solved in a more general case for 2k vectors in 2k-dimensinoal space.When Danil came home, he quickly came up with the solution for this problem.Can you cope with it?</p>
<p>Input</p>
<p>The only line of the input contains a single integer k (0 ≤ k ≤ 9).</p>
<p>Output</p>
<p>Print 2k lines consisting of 2k characters each.The j-th character of the i-th line must be equal to ' * ' if the j-th coordinate of the i-th vector is equal to - 1, and must be equal to ' + ' if it's equal to + 1.It's guaranteed that the answer always exists.\nIf there are many correct answers, print any.In some cases, additional notes may be provided to assist competitors in understanding these example tests.This information is fed into the LLM, aiming to generate relevant code (right).The generated code is then submitted to an online judge for correctness evaluation.</p>
<p>Hence, it is not subject to inaccuracies stemming from temporal changes, repeated submissions, plagiarism, and other potential distortions.</p>
<p>Problem Definition</p>
<p>Figure 2 presents an example of the problem statement π.The input of LLM is instantiated with the problem statement π and a prompt ρ (like ρ 1 in Table 8).The LLM Γ takes the input to generate the code as α = Γ(ρ(π)).The generated code α is then evaluated by an online judge (OJ).The evaluation process can be summarized in the following equation:
OJ(α) = OJ(Γ(ρ(π))) ∈ {AC, WA, CE, ...}
In this equation, Γ(ρ(π)) denotes the code generated by LLM with the prompt ρ.The OJ platform then rigorously assesses the code for its correctness, computational efficiency, and adherence to specified input/output formats.With an extensive testing mechanism, the platform employs a wide range of test cases and hidden scenarios to ensure the code's robustness across diverse scenarios.The platform provides a spectrum of outcomes, OJ(Γ(ρ(π))), offering a holistic evaluation of the code's performance.This includes results such as Accepted (AC), Wrong Answer (WA), and Compilation Error (CE), among others.</p>
<p>Dataset Collection</p>
<p>The dataset is compiled from the Codeforces website, extracting all publicly available problem statements from completed contests spanning February 2010 through November 2023.For simplicity, problems requiring interaction, featuring non-standard input/output formats, or incompatible with C++ submission are excluded.For detailed explanations, see Appendix B.</p>
<p>The analysis is confined to problems with difficulty levels ranging from 800 to 2400.Based on their difficulty levels, the dataset is divided into three subsets: D1 (800-1100 difficulty, 1683 problems), D2 (1200-1600 difficulty, 1821 problems), and D3 (1700-2400 difficulty, 1453 problems).These problems encompass more than 20 distinct categories of algorithms, as illustrated in Table 7.This diversity in problem types further enhances the comprehensiveness of the dataset and enables a comprehensive assessment of GPT-4's problem-solving abilities across a wide range of competition-level programming problems.</p>
<p>Metric Definition ACC#G</p>
<p>Proportion of accepted solutions using greedy sampling (temperature t = 0).ACC#GN The number of accepted solutions using greedy sampling (temperature t = 0) within the sliding window.ACCk#n Proportion of problems with k or more accepted solution with top-p samplings (t = 0.7, p = 0.95) for n times.pass@k Estimated proportion of problems with at least one accepted solution.</p>
<p>Table 2: Definitions of evaluation metrics.</p>
<p>Experiment Details</p>
<p>In Codeforces, each problem belongs to a contest.</p>
<p>Once the contest concludes, the problems are disclosed and become publicly submittable.Therefore, we submit the solutions to the contests that have concluded for evaluation.</p>
<p>To evaluate the results, we employ ACC#G, ACC#GN, ACCk#n and pass@k as defined in Table 2. Specifically, for ACCk#n metric, we consider two settings: (1) k = n = 1 and (2) k ∈ {1, 2, 3} with n = 5.Following Codex (Chen et al., 2021b), pass@k is computed as
pass@k := E Problems 1 − n−c k n k
where n is defined as the total number of generated samples per problem used for evaluation, and c represents the count of correct samples out of n that have successfully passed the unit tests.Here we use k = 1 and n = 5 for pass@k.</p>
<p>In our experiment, we follow the zero-shot setting.To select an appropriate prompt, we conduct preliminary experiments with three prompts, ρ 1 , ρ 2 , and ρ 3 , as listed in Table 8, using two subsets of D1 problems: one from February to December 2010 and the other from January to October 2023, each comprising approximately 100 problems.The standard deviations are 0.015 and 0.018, respectively, indicating consistent performance.Therefore, we choose ρ 1 as the prompt in the subsequent experiments.Furthermore, we employ a sliding window approach for all temporal analyses to smooth the data, addressing the sporadic release schedule of the problems.This ensures a sufficient number of test problems at each time point, using a window size of 51 (25 before and 25 after the time point).</p>
<p>Insights and Implications</p>
<p>Faltering on Unseen Problems</p>
<p>In this section, we delve into a temporal analysis of GPT-4 (gpt-4-0613)'s performance on programming problems.Figure 1 illustrates GPT-4's performance using the ACC#G metric.On problems released prior to September 2021, GPT-4 exhibits minor fluctuations at different levels across problems of varying difficulty.However, for problems released after September 2021, a significant deviation from the normal fluctuation range is observed.Interestingly, this timing coincides with the cutoff date for the GPT-4 training data as announced by OpenAI8 .We then calculate the average performance on problems before and after September 2021, as shown in Table 1.On D1 problems, GPT-4's ACC#G plummets from 81.42% to 11.73%, marking a stark decrease of 69.69%.Even more strikingly, the ACC#G drops to 0.00% on both D2 and D3 problems, from 43.72% and 11.41%, respectively.To validate the reliability of the conclusion, we also calculate the pass@1 metric, which exhibits a similar trend.This observation raises thought-provoking questions about the severity of the drop and the correlation between the data cutoff date and the performance decline.</p>
<p>To explore the model's potential to generate correct solutions, we perform random sampling multiple times and calculate the pass rate.The average pass rate are shown in Table 1.As observed, multiple samplings can enhance the chances of gen- erating a correct solution.For instance, on the unseen simple D1 problems, ACC1#5 improved by 10.71% compared to ACC1#1.However, across all problems, the performance gap before and after the cut-off date is more pronounced for ACC1#5 than for both ACC1#1 and ACC#G.Figure 3 depicts the performance on D2 problems over time.A notable decline in performance metrics is observed around September 2021.This observation underscores the challenges that LLMs, including the advanced GPT-4, face in addressing unseen programming problems without similar pretraining data.</p>
<p>The observed decline in performance on problems outside the model's training range may stem from limitations in reasoning and generalization.As highlighted by Yadlowsky et al. (2023), when confronted with problems beyond their pretraining data, transformer models exhibit various failure modes and their generalization abilities deteriorate, even for simple problems.Similarly, Lu et al. (2023) suggest that the exceptional abilities of large language models primarily stem from in-context learning, and do not necessarily reflect the emergence of reasoning abilities.</p>
<p>The observed performance drop on unseen problems raises serious questions about GPT-4's intrinsic reasoning and generalization capabilities.This suggests a potential over-reliance on pattern recognition and reproduction from training, as opposed to grasping underlying principles and applying them to novel problems.This observation aligns with recent debates on large models' data memorization tendencies (Carlini et al., 2023;Yang et al., 2023).Therefore, future evaluations should prioritize the minimization of overlap between testing and training data to accurately assess a model's reasoning abilities, rather than simply its capacity for memorization.Furthermore, it's crucial to explore methods that enhance model generalization and reduce reliance on pre-training data.</p>
<p>Limited Ability to Solve Difficult Problems</p>
<p>This section provides an analysis of performance in relation to the problem difficulty.The results of ACC#G for problems with different difficulties are reported for two distinct periods: from October 2010 to September 2021, and from October 2021 to November 2023, as illustrated in Figure 4.</p>
<p>For the results from October 2010 to September 2021, we calculate Pearson correlation coefficient (r = −0.97)and the Kendall rank correlation coefficient (τ = −0.88),which indicate strong linear correlations.Notably, when the difficulty level reaches 2400 (indicating greater challenge than approximately 57% of the problems on Codeforces), the ACC#G drops to zero.However, from October 2021 to November 2023, ACC#G shows a dramatic decrease across all difficulty levels.</p>
<p>These findings reveal a significant limitation in the ability of GPT-4 to handle extremely complex problems.Despite its vast knowledge on code and algorithms, GPT-4 lacks of the competence in solving very challenging problems, particularly those with higher difficulty levels, even in the context of previously encountered problems.This indicates a potential area for further improvement and devel-  opment in future iterations of the model.</p>
<p>Struggling with The First Test Case</p>
<p>In this section, we gather and analyze the errors returned by GPT-4 upon submission to the Codeforces website, as outlined in Table 6.The most common error is "Wrong answer on test 1", which on average accounts for 70% of the observed errors.Test 1 is the first test case, which almost corresponds to or properly includes the example test case provided in the problem statement.This suggests that the model often struggles at the very beginning of problem-solving, possibly due to difficulties in understanding the problem's requirements or generating a correct solution based on the given test case.As depicted in Figure 5, there is a significant increase in the proportion of "Wrong answer on test 1" errors for problems released between 2021 and 2023.This suggests that GPT-4 is more likely to face challenges in understanding and reasoning during at the onset of tackling unseen problems.</p>
<p>Other types of errors account for a smaller proportion, with an average of 10%.They have shown little variation over time.This indicates that GPT-4 demonstrates strong fundamental code-writing capabilities of generating high-quality code.</p>
<p>Similar Phenomenon of Other Code LLMs</p>
<p>We investigate whether the perceived performance degradation on unseen programming problems is observed for other popular code LLMs, such as   We conduct tests on CodeLlama and DeepSeek-Coder using D1 problems, following the settings in §2.3, and the results are shown in Figure 6.The experimental results indicate that CodeLlama consistently underperforms compared to DeepSeek-Coder on D1 problems.Furthermore, the performance of DeepSeek-Coder on D1 problems has been declining with the progression of the problem release date.The ACC#GN of DeepSeek-Coder has declined to a level that is on par with CodeLlama when dealing with newly released problems, as highlighted in the red area of Figure 6.</p>
<p>To precisely and intuitively detect this phenomenon, we calculate the ACC#G of CodeLlama and DeepSeek-Coder on D1 problems, both before and after March 2023, and present the results in Table 3.The results reveal a significant difference in the average accuracy of CodeLlama and DeepSeek-Coder before and after March 2023.Regarding the magnitude of the decrease, DeepSeek-Coder, which previously exhibited superior performance, demonstrates a more pronounced decline, with acceptance rates falling below 10% after March 2023.Considering the release dates of CodeLlama and DeepSeek-Coder, we speculate that most of the programming problems after March 2023 are novel to them, which suggests that they also not be able to perform well on unseen programming problems like GPT4 does.This finding indicates that a fun-  damental limitation of current code LLMs in generalizing effectively to complex reasoning tasks.</p>
<p>Evaluation Hallucination of LLMs</p>
<p>To further analyze the phenomenon, we compare GPT-4 with DeepSeek-Coder on D1 problems as shown in Figure 7 and Table 4.</p>
<p>It is noteworthy that while GPT-4 surpasses DeepSeek-Coder in terms of performance on problems that were released prior to September 2021, an unexpected observation is that DeepSeek-Coder exhibits a performance that is on par with GPT-4 when it comes to tackling problems that were released after September 2021.Considering the previous work (Yang et al., 2023;Zhou et al., 2023), although GPT-4 may perform particularly well on some previously seen problems due to its powerful capacity, it cannot be well generalized on unseen programming problems, and its performance is not significantly different from DeepSeek-Coder, which is specifically trained for code.This phenomenon merits attention, which is termed as "evaluation hallucination".</p>
<p>Hence, a more equitable evaluation strategy would be to select evaluation sets that all the models have not previously encountered.However, finding such data adhering to stringent conditions is challenging, as LLMs are typically pre-trained on extensive corpora containing diverse content, leading to the potential issue of data contamination.Therefore, if we could devote more attention to the data source and timeline of the evaluation sets, such  as the problems in Codeforces, it could potentially mitigate the effects of evaluation hallucination.</p>
<p>One Step Forward</p>
<p>In this section, we explore some approaches to mitigate the poor performance on unseen problems.</p>
<p>Fine-tuning</p>
<p>Fine-tuning is a commonly used method to improve performance on specific downstream tasks.In this study, we employ the Description2Code dataset (OpenAI and Sutskever, 2016) for fine-tuning.This dataset is compiled from three competitive programming websites: Codeforces, CodeChef, and HackerEarth, and contains problems published before 2017.CodeChef and HackerEarth, similar to Codeforces, host online coding competitions, and their problem sets are consistent in style and difficulty.</p>
<p>The dataset includes 2128 problems from Codeforces, 2435 problems from HackerEarth, and 3201 problems from CodeChef, totaling 7764 problems.However, due to some problems lacking corresponding C++ solutions, we retained a total of 7000 problems for our study.Each problem has approximately 10 C++ solutions, resulting in 70,000 pairs of input-output sequences.These sequences are used for fine-tuning both CodeLlama and DeepSeek-Coder in a supervised manner.</p>
<p>As shown in Figure 8 and Figure 9, we compare the performances of the models before and after fine-tuning on D1 problems.We observe that, even after fine-tuning with the same type of data, CodeLlama and DeepSeek-Coder do not exhibit improved performance on recent problems, particularly those post-2022.The significant improvement in ACC#GN before 2017 may result from the models recalling relevant or identical programming problems, rather than mastering the underlying reasoning logic, leading to their inability to adapt well to new programming challenges.Therefore, simple fine-tuning does not effectively enhance the models' performance on new programming problems.</p>
<p>Chain-of-Thought Prompting</p>
<p>In this section, we explore the application of Chainof-Thought (CoT) prompting (Wei et al., 2022) to competition-level programming problems.CoT involves prompting GPT-4 to generate an explanation of the algorithm before coding, denoted as ρ cot in Table 8.We conduct experiments on both the D1 and D3 problems released after October 2021.For D1 problems, employing CoT increases the ACC#G from 11.54% to 16.21%, demonstrating a noticeable improvement.However, for D3 problems, using CoT fails to yield any improvement, leaving the ACC#G at 0.00%.This suggests that while CoT facilitates some improvement for simple D1 problems, it is ineffective for the complex reasoning challenges presented by D3 problems.</p>
<p>In-Context Learning</p>
<p>In this section, we enhance our experimental exploration into in-context learning by integrating both fixed demonstrations and retrieval-augmented demonstrations.</p>
<p>We use D1 problems released before September 2021 as source dataset and those released after as test data.First, we apply the same method as delineated in §4.2 to tackle the source data and validate them on Codeforces.We then retain accepted solutions, resulting in a collection of 1048 problem and CoT response pairs.A demonstration example is presented in Table 12.In the fixed demonstration experiment, we randomly select three problems to create 3-shot prompts.In the retrieval-augmented demonstration experiment, we first generate embeddings for the statements of the source and test data problems utilizing the OpenAI text-embedding-ada-002 model.We then identify the top three problems in the source data based on cosine similarity for each test problem, incorporating them as example demonstrations within the prompts.</p>
<p>The experimental results, summarized in Table 5, show that the retrieval-augmented 3-shot method's accuracy is nearly identical to the 0-shot CoT, while the fixed 3-shot approach is even less effective.This may be due to the highly specialized nature of competitive programming problems, which makes finding valuable references challenging.Furthermore, the model may struggle to acquire problemsolving skills through context learning alone, and inappropriate demonstrations might lead to adverse effects.</p>
<p>Problem Statement Simplification</p>
<p>Intuitively, even experienced programming competition competitors require time to understand problem statements.Therefore, we conduct a simple experiment to assess whether comprehension of problem statements hinders LLMs' ability to excel at programming problems.We first instruct GPT-4 to simplify the problem statement with ρ sip and then generate the code with ρ sipgen as shown in Table 8.The results are also evaluated on both the D1 and D3 problems released after October 2021.However, for D1 problems, using the simplified problem statement even brings a slight decline in ACC#G from 11.54% to 11.14%.And the ACC#G for D3 problems still remains at 0.00%.Consequently, the challenge of genuinely improving the model's reasoning ability and enhancing its performance on unseen problems represents a significant direction for future research.</p>
<p>Code LLMs.Code intelligence is an important topic in AI research.Recently, code LLMs (Zhang et al., 2023b) have received widespread attention.Commercial LLMs (OpenAI, 2023) have achieved tremendous success.Meanwhile, research on open-source code LLMs is also thriving, such as CodeLlama (Rozière et al., 2023), StarCoder (Li et al., 2023), CodeGeeX (Zheng et al., 2023), CodeFuse (Di et al., 2023), WizardCoder (Luo et al., 2023) and Lemur (Xu et al., 2023).</p>
<p>Reasoning on Code.Programming competition is a specialized domain within the broader landscape of programming problems.Unlike simpler tasks on code, such as HumanEval (Chen et al., 2021a), MBPP (Austin et al., 2021), MultiPL-E (Cassano et al., 2023), competition-level programming problems necessitate an advanced understanding of data structures, algorithms, and problem-solving techniques.Enabling models to solve human-designed algorithmic competition problems represents a meaningful research direction, as it reflects the models' integrated capabilities in reasoning, coding, and problem-solving.AlphaCode (Li et al., 2022) simulate evaluations on 10 programming competitions on the Codeforces platform, which is the first work in this topic.ALGO (Zhang et al., 2023a) can integrate with any existing code LLMs in a model-agnostic manner, enhancing its code generation performance.</p>
<p>Reasoning on Other Subjects.Researchers have proposed many benchmarks requiring various reasoning skills, including commonsense reasoning (Talmor et al., 2018;Geva et al., 2021), numerical reasoning (Dua et al., 2019), multi-hop reasoning (Yang et al., 2018), arithmetic reasoning (Patel et al., 2021;Cobbe et al., 2021), structured reasoning (Yu et al., 2018;Lei et al., 2023), inductive reasoning (Sinha et al., 2019) and logical reasoning (Yu et al., 2020).LLMs are also widely used in scientific research in other fields (Wang et al., 2023), such as physics (Yeadon and Halliday, 2023), chemistry (Castro Nascimento and Pimentel, 2023;Bran et al., 2023), etc.</p>
<p>Conclusion</p>
<p>In this study, we utilize competition-level programming problems from Codeforces to analyze the reasoning capabilities of LLMs.We find a significant decrease in perceived performance of GPT-4 on unseen problems, consistent across a range of difficulties, problem types, and experimental settings.This decrease highlights concerns of data contamination in benchmarks and the need for unseen tasks to properly assess LLMs' reasoning ability with complex challenges.Our research also extends these insights to other open-source LLMs, revealing the common difficulties these models face with complex, previously unencountered reasoning tasks.This is indicative of the LLMs' intrinsic limitations in reasoning.As a primary probe, we explore several straightforward strategies, but none of them consistently mitigated the issues.Through our work, we hope to emphasize the critical need for robust datasets to accurately evaluate LLMs' reasoning abilities and to inspire advancements in LLMs that demonstrate improved reasoning abilities.</p>
<p>Limitations</p>
<p>This study identifies expertly-designed, highquality competition-level programming problems as effective evaluation data for evaluating LLMs.However, comparing to the existing benchmarks, the quantity of such problems is limited.Constructing uncontaminated, high-quality evaluation datasets and extending them to other tasks such as mathematics still poses a challenge to researchers.The identification and creation of such datasets are crucial for enhancing our understanding of the LLMs in complex reasoning tasks.We will endeavor to achieve this goal in our future work.</p>
<p>A More Results with Different Versions of GPT-4 APIs</p>
<p>In this study, we conduct an evaluation of two distinct API versions: GPT-4 and GPT-4-turbo, to assess their performance on D1 problems.The training data for GPT-4 extends up to September 2021, while that for GPT-4-turbo reaches up to April 2023.These evaluations are visually represented in Figure 10.Upon analysis of the results, it is observed that on problems prior to September 2021, the GPT-4-turbo exhibits marginally inferior performance compared to GPT-4.Between September 2021 and April 2023, GPT-4-turbo outperforms GPT-4 on D1 problems, reflecting the benefits of its more recent training data.Nonetheless, a decline in GPT-4's performance is observed for newer problems within this period, likely due to the scarcity of such recent data in its training set.</p>
<p>Nevertheless, when faced with problems emerging after April 2023-thus unencountered during their respective training periods-both APIs demonstrate a decline in performance, albeit GPT-4-turbo marginally outperforms GPT-4.Despite this relative improvement, the performance of GPT-4-turbo on problems post-April 2023 noticeably regresses when compared to its performance on problems covered by its training data.This finding is consistent with the conclusions drawn in the §3.1 "Faltering on Unseen Problems", which elucidates the challenges faced by these models when confronted with novel questions that extend beyond their training corpus.</p>
<p>B Dataset Details</p>
<p>In the context of competitive programming challenges, a "non-standard input/output format" typically refers to a situation where the program's input and output are not provided through standard methods such as reading from standard input (stdin) or writing to standard output (stdout), which are the conventional ways for programs to receive and provide data during competitions.Instead, they might involve interacting with files, graphical user interfaces, or network connections, which are not commonly used in standard programming contests (like https://codeforces.com/problemset/problem/120/A).</p>
<p>To filter out problems with non-standard input/output formats, we utilize metadata from the problem descriptions on Codeforces, which indicate whether a problem requires non-standard methods for input and output.By checking this information, we can automatically exclude such problems from our dataset to ensure the consistency of the test data.</p>
<p>Statistics of the types of problems in D1, D2, and D3 are shown in Table 7.</p>
<p>C Prompt Details</p>
<p>Prompts used in this study are shown in Table 8.</p>
<p>D Case Study</p>
<p>Some examples generated by GPT4 are shown in Tables 9-13.</p>
<p>Figure 1 :
1
Figure 1: The perceived zero-shot performance of GPT-4 sees a sharp decline on problems of varying difficulties (D1, D2 and D3 means easy, medium and difficult, respectively) in Codeforces after September 2021.</p>
<p>Figure 3 :
3
Figure3: Random sampling enhances the probability of generating correct solutions on previously encountered problems, but offers no assistance for unseen problems.</p>
<p>Figure 4 :
4
Figure4: For problems released before September 2021, GPT-4's ACC#G showed a negative linear correlation with difficulty, followed by consistently poor performance afterwards.</p>
<p>Figure 5 :
5
Figure 5: Error categories in GPT-4's solutions on problems released from 2010 to 2023.</p>
<p>Figure 6 :
6
Figure 6: ACC#GN of CodeLlama and DeepSeek-Coder on D1 problems.</p>
<p>Figure 7 :
7
Figure 7: Comparison of ACC#GN for GPT-4 and DeepSeek-Coder on D1 problems after 2020.Model 2020.1-2021.92021.9-2023.10GPT-4 73.19% (+50.52%)11.53% (-0.97%)DeepSeek-Coder 22.67% 12.50%</p>
<p>Figure 8 :
8
Figure 8: Comparison of ACC#GN for CodeLlama on D1 problems before and after fine-tuning.</p>
<p>Figure 9 :
9
Figure 9: Comparison of ACC#GN on D1 problems before and after fine-tuning DeepSeek-Coder.</p>
<p>Figure 10 :
10
Figure 10: Comparison of ACC#G for GPT-4 and GPT-4-turbo on D1 problems.</p>
<p>( i n t i = 0 ; i <n ; i ++) c i n >&gt; b [ i ] ;9 v e c t o r &lt; i n t &gt; p r e f i x ( n ) , s u f f i x ( n f i x [ n − 1 ] = b [ n − 1 ] ; 12 f o r ( i n t i = 1 ; i <n ; i ++) p r e f i x [ i ] = max ( p r e f i x [ i − 1 ] , b [ i ] ) ; 13 f o r ( i n t i =n − 2 ; i >=0; i − −) s u f f i x [ i ] = max ( s u f f i x [ i + 1 ] , b [ i ] ) ;14 b o o l ok = t r u e ; 15 f o r ( i n t i = 0 ; i <n ; i ++) { 16 i f ( b [ i ] > max ( ( i &gt;0 ?p r e f i x [ i − 1 ] : 0 ) , ( i &lt;n −1 ?s u f f i x [ i + 1 ] : 0</p>
<p>Time limit per test 3 seconds Memory limit per test 256 megabytes
// generated by GPT4#include<bits/stdc++.h>using namespace std;vector<vector\<int>> v(1, vector<int>(1, 1));void solve(int k) {if(k == 0) return;int sz = 1 &lt;&lt; (k -1);for(int i = 0; i &lt; sz; i++) {for(int j = 0; j &lt; sz; j++) {v[i].push_back(v[i][j]);v.push_back(v[i]);}}for(int i = 0; i &lt; sz; i++) {for(int j = 0; j &lt; sz; j++) {v[i + sz][j] <em>= -1;}}solve(k -1);}int main() {int k;cin &gt;&gt; k;solve(k);Exampleint sz = 1 &lt;&lt; k; for(int i = 0; i &lt; sz; i++) {input 2\nfor(int j = 0; j &lt; sz; j++) { cout &lt;&lt; (v[i][j] == 1 ? '+' : '</em>'); }output ++<strong>\n+<em>+</em>\n++++\n+</strong>+\ncout &lt;&lt; "\n"; return 0; }}Notenone</p>
<p>Online Jugde AC/WA/CE/…</p>
<p>Figure 2: The figure depicts the problem statement (left), comprising a problem set in a narrative context for participants to decipher, detailed input and output format specifications, and one or more example input-output pairs.</p>
<p>Table 1 :
1
Performance of GPT-4 on different groups of problems: Time1 is the problems released from October 2010 to September 2021, and Time2 is the problems released from October 2021 to November 2023.
MetricTime1D1 Time2∆Time1D2 Time2∆Time1D3 Time2∆ACC#G81.42%11.73%-69.69%43.72%0.00%-43.72%11.41%0.00%-11.41%pass@178.11%10.54%-67.57%42.38%0.61%-41.77%9.45%0.18%-9.27%ACC1#178.05%9.38%-68.68%43.37%0.00%-43.37%8.48%0.00%-8.48%ACC1#594.03%20.09%-73.94%69.02%3.06%-65.96%21.24%0.88%-20.36%ACC2#588.34%11.83%-76.51%54.41%0.00%-54.41%12.36%0.00%-12.36%ACC3#581.82%9.38%-72.44%42.42%0.00%-42.42%7.51%0.00%-7.51%</p>
<p>Table 3 :
3
Comparison of ACC#G between CodeLlama and DeepSeek-Coder on D1 problems before and after March 2023.</p>
<p>CodeLlama-34B-Instruct (Rozière et al., 2023) and DeepSeek-Coder-33B-Instruct (AI, 2023).</p>
<p>Table 4 :
4
Comparison of ACC#G between GPT-4 and DeepSeek-Coder over time intervals, on D1 problems.</p>
<p>Table 5 :
5
Accuracy of GPT-4 on D1 problems released after September 2021 using different experimental setups</p>
<p>Table 6 :
6
Error category of GPT-4 from 2010 to 2023.The abbreviations stand for: WA1, WA2, WA3, and WA4+ (Wrong Answers on Test 1, 2, 3, and 4 or above), RE (Runtime Error), CE (Compilation Error), TLE (Time Limit Exceeded), and MLE (Memory Limit Exceeded).
YearWA1 WA2 WA3 WA4+RECETLE MLE20100.490.150.040.190.04 0.03 0.010.0420110.530.130.100.060.07 0.06 0.040.0120120.550.140.050.120.04 0.04 0.030.0220130.630.130.060.070.03 0.03 0.040.0120140.530.180.050.090.04 0.05 0.040.0120150.620.100.020.140.03 0.02 0.040.0220160.680.090.020.100.02 0.05 0.040.0020170.660.130.050.080.03 0.03 0.030.0220180.580.120.060.130.01 0.03 0.050.0120190.670.110.040.060.02 0.05 0.020.0220200.770.060.010.010.05 0.06 0.030.0120210.820.070.020.010.02 0.04 0.020.0020220.910.050.000.000.02 0.00 0.020.0020230.900.030.000.000.03 0.01 0.020.01Average0.700.100.030.060.03 0.03 0.030.01Tag#ProblemsTag#Problemsimplementation1746greedy1441math1382brute force825constructive algorithms783dp577sortings514data structures391strings381binary search342number theory309graphs263dfs and similar244two pointers197combinatorics179bitmasks154geometry142trees137games87dsu84shortest paths66*special58probabilities52hashing48divide and conquer35flows24graph matchings22ternary search22matrices22expression parsing19string suffix structures102-sat7chinese remainder theorem5schedules4meet-in-the-middle4fft4</p>
<p>Table 7 :
7
Statistics of the types of problems in D1, D2, D3.Read the problem, write a C++ solution and explain the algorithm.{promblem_name}:{promblem_description} Input specification is{input_f ormat}.Output specification is {output_f ormat}.Note that {note}.Memory limit is{memory_limit}.Time limit is {time_limit}.Example i input is{inputi}.Example i output is{outputi}.Please provide a C++ code in <code>cpp\n...\n``ρ Qingshan has a string s, while Daniel has a string t.Both strings only contain 0 and 1.A string a of length k is good if and only if a i ̸ = a i+1 for all i = 1, 2, . . ., k − 1.For example, 1, 101, 0101 are good, while 11, 1001, 001100 are not good.Qingshan wants to make s good.To do this, she can do the following operation any number of times (possibly, zero): insert t to any position of s (getting a new s).Please tell Qingshan if it is possible to make s good.Input Format The input consists of multiple test cases.The first line contains a single integer T (1 ≤ T ≤ 2000) -the number of test cases.The description of the test cases follows.The first line of each test case contains two integers n and m (1 ≤ n, m ≤ 50) -the length of the strings s and t, respectively.The second line of each test case contains a string s of length n.The third line of each test case contains a string t of length m.It is guaranteed that s and t only contain 0 and 1.Output Format For each test case, print "YES" (without quotes), if it is possible to make s good, and "NO" (without quotes) otherwise.You can print letters in any case (upper or lower).
ρ1You are given a problem, you need to write a C++ solution and explain the algorithm.{promblem_name}{promblem_description}Input specification: {input_f ormat}Output specification: {output_f ormat}Note: {note}Memory limit: {memory_limit}Time limit: {time_limit}Example:Input:{inputi}Output:{outputi}Please provide a C++ code in</code>cpp\n...\n<code>ρ23Finish the solution of this programming problem.{promblem_name}{promblem_description}Input specification: {input_f ormat}Output specification: {output_f ormat}Note: {note}Memory limit: {memory_limit}Time limit: {time_limit}Example:Input:{inputi}Output:{outputi}C++ code solution:```cppρcotYou are given an algorithm problem. First, provide a detailed explanation of the algorithm solution,including the logic behind it, the time and space complexity, and any important considerations or edgecases. Then, implement the solution in C++ code, ensuring it is clean, efficient, and well-commented.{promblem_name}{promblem_description}Input specification: {input_f ormat}Output specification: {output_f ormat}Note: {note}Memory limit: {memory_limit}Time limit: {time_limit}Example:Input:{inputi}Output:{outputi}Please provide a C++ code in ```cpp\n...\n</code>ρsipPlease extract the essential components from this algorithm problem for a C++ solution, removing anysuperfluous narrative or context.{promblem_name}{promblem_description}Input specification: {input_f ormat}Output specification: {output_f ormat}Note: {note}ρsipgen You are given a problem, you need to write a C++ solution and explain the algorithm.{promblem_simlif ied}Memory limit: {memory_limit}Time limit: {time_limit}Example:Input:{inputi}Output:
{outputi} Please provide a C++ code in <code>`cpp\n...\n</code>T able 8: Prompts used in this study.1 # i n c l u d e &lt; b i t s / s t d c + + .h&gt; 2 u s i n g namespace s t d ; 3 4 b o o l i s G o o d ( s t r i n g s ) { 5 f o r ( i n t i = 0 ; i &lt; s .s i z e ( ) − 1 ; i ++) { 6 i f ( s [ i ] == s [ i + 1 ] ) { 7 r e t u r n f a l s e ; 19 } 20 f o r ( i n t i = 0 ; i &lt; s .s i z e ( ) − 1 ; i ++) { 21 i f ( s [ i ] == s [ i + 1 ] &amp;&amp; t [ 0 ] != s [ i ] &amp;&amp; t [ t .s i z e ( ) −1] != s [ i + 1 ] ) {</p>
<p>Table 9 :
9
An accepted example generated byProblem Statement Problem You are given an array b 1 , b 2 , . .., b n .An anonymous informant has told you that the array b was obtained as follows: initially, there existed an array a 1 , a 2 , . .., a n , after which the following two-component operation was performed k times: A fixed point † x of the array a was chosen.Then, the array a was cyclically shifted to the left ‡ exactly x times.As a result of k such operations, the array b 1 , b 2 , . . ., b n was obtained.You want to check if the words of the anonymous informant can be true or if they are guaranteed to be false.† A number x is called a fixed point of the array a 1 , a 2 , . . ., a n if 1 ≤ x ≤ n and a x = x.‡ A cyclic left shift of the array a 1 , a 2 , . . ., a n is the array a 2 , . . ., a n , a 1 .Input Format Each test contains multiple test cases.The first line contains an integer t (1 ≤ t ≤ 10 4 ) -the number of test cases.The description of the test cases follows.The first line of each test case contains two integers n, k (1 ≤ n ≤ 2 • 10 5 , 1 ≤ k ≤ 10 9 ) -the length of the array b and the number of operations performed.The second line of each test case contains n integers b 1 , b 2 , . . ., b n (1 ≤ b i ≤ 10 9 ) -the elements of the array b.It is guaranteed that the sum of the values of n for all test cases does not exceed 2 • 10 5 .Output Format For each test case, output "Yes" if the words of the anonymous informant can be true, and "No" if they are guaranteed to be false.
Problem Name1893 A: Anonymous InformantWrong answer on test 1
1 # i n c l u d e &lt; b i t s / s t d c + + .h&gt; 2 u s i n g namespace s t d ;</p>
<p>Table 10 :
10
An wrong example generated by</p>
<p>https://leetcode.com/
https://codeforces.com/
https://icpc.global/
https://ioinformatics.org/
https://codeforces.com/blog/entry/102
https://codeforces.com/blog/entry/62865
https://platform.openai.com/docs/models/ gpt-4-and-gpt-4-turbo
Output Format Print a single line containing "Akshat" or "Malvika" (without the quotes), depending on the winner of the game.ResponseThe problem can be solved using a simple observation.The game is essentially about removing rows and columns from a grid.Each move removes one row and one column.Therefore, the game ends when there are no rows or columns left, i.e., when the minimum of n and m reaches zero.Since Akshat starts the game, if the minimum of n and m is even, Malvika will make the last move and win the game.Otherwise, Akshat will make the last move and win the game.The time complexity of this solution is O(1) because it involves only a constant amount of computation.The space complexity is also O(1) because it uses only a constant amount of space.The code first reads the values of n and m.It then checks if the minimum of n and m is even.If it is, it prints "Malvika".Otherwise, it prints "Akshat".Here is the C++ code for the problem:Simplified StatementYou are given a problem, you need to write a C++ solution and explain the algorithm.-You are given two strings s and t, both containing only 0s and 1s.-A string a of length k is considered good if no two consecutive characters are the same.-You can insert string t at any position in string s any number of times.-You need to determine if it is possible to make string s good.-The input consists of multiple test cases.The first line contains an integer T (1 ≤ T ≤ 2000) -the number of test cases.Each test case starts with two integers n and m (1 ≤ n, m ≤ 50) -the lengths of the strings s and t, respectively.The next two lines contain the strings s and t.-For each test case, print "YES" if it is possible to make s good, and "NO" otherwise.The output is case-insensitive.
A I Deepseek, Deepseek coder: Let the code write itself. 2023</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.07732Program synthesis with large language models. 2021arXiv preprint</p>
<p>Code llama: Open foundation models for code. Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Bhatt, Aaron Canton-Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, 10.48550/ARXIV.2308.12950CoRR, abs/2308.12950Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023</p>
<p>Extending the frontier of chatgpt: Code generation and debugging. Saadat Fardin Ahsan Sakib, Hasan Khan, Karim, arXiv:2307.082602023arXiv preprint</p>
<p>Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, arXiv:2307.14936Pangu-coder2: Boosting large language models for code with ranking feedback. 2023arXiv preprint</p>
<p>Clutrr: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937Commonsenseqa: A question answering challenge targeting commonsense knowledge. 2018arXiv preprint</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, arXiv:2310.06830Lemur: Harmonizing natural language and code for language agents. 2023arXiv preprint</p>
<p>Pretraining data mixtures enable narrow model selection capabilities in transformer models. Steve Yadlowsky, Lyric Doshi, Nilesh Tripuraneni, arXiv:2311.008712023arXiv preprint</p>
<p>Rethinking benchmark and contamination for language models with rephrased samples. Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, Ion Stoica, arXiv:2311.048502023arXiv preprint</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Exploring durham university physics exams with large language models. Will Yeadon, Douglas P Halliday, arXiv:2306.156092023arXiv preprint</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, arXiv:2002.04326Reclor: A reading comprehension dataset requiring logical reasoning. 2020arXiv preprint</p>
<p>Algo: Synthesizing algorithmic programs with generated oracle verifiers. Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang, Wang , Lei Li, arXiv:2305.145912023aarXiv preprint</p>
<p>. Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, Rui Wang, 2023b. A survey on language models for code</p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, arXiv:2303.175682023arXiv preprint</p>
<p>Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, arXiv:2311.01964Don't make your llm an evaluation benchmark cheater. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
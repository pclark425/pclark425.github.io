<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7203 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7203</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7203</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-96c56de54b07c296f7b6c86bf9f075d04c0d644e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/96c56de54b07c296f7b6c86bf9f075d04c0d644e" target="_blank">GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs</a></p>
                <p><strong>Paper Venue:</strong> The Web Conference</p>
                <p><strong>Paper TL;DR:</strong> This work generates and curates large-scale graph-summary pair data with the assistance of LLMs, and introduces a novel graph-summary pretraining method, combined with invariant learning, to enhance graph foundation models with strong cross-domain zero-shot transferability.</p>
                <p><strong>Paper Abstract:</strong> Recently, research on Text-Attributed Graphs (TAGs) has gained significant attention due to the prevalence of free-text node features in real-world applications and the advancements in Large Language Models (LLMs) that bolster TAG methodologies. However, current TAG approaches face two primary challenges: (i) Heavy reliance on label information and (ii) Limited cross-domain zero/few-shot transferability. These issues constrain the scaling of both data and model size, owing to high labor costs and scaling laws, complicating the development of graph foundation models with strong transferability. In this work, we propose the GraphCLIP framework to address these challenges by learning graph foundation models with strong cross-domain zero/few-shot transferability through a self-supervised contrastive graph-summary pretraining method. Specifically, we generate and curate large-scale graph-summary pair data with the assistance of LLMs, and introduce a novel graph-summary pretraining method, combined with invariant learning, to enhance graph foundation models with strong cross-domain zero-shot transferability. For few-shot learning, we propose a novel graph prompt tuning technique aligned with our pretraining objective to mitigate catastrophic forgetting and minimize learning costs. Extensive experiments show the superiority of GraphCLIP in both zero-shot and few-shot settings, while evaluations across various downstream tasks confirm the versatility of GraphCLIP. Our code is available at: https://github.com/ZhuYun97/GraphCLIP.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7203.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7203.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphML Prompt (XML-like)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphML-based Graph Prompt Template (XML-like serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An XML/GraphML-style markup template used to serialize sampled subgraphs (nodes with title/abstract attributes and typed edges) into a text markup format that LLMs can ingest as input prompts for graph-to-text summarization; introduced and used in this paper to translate graph structure into text for summary generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GraphML markup serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>An XML-like GraphML template that lists nodes and their textual attributes (e.g., title, abstract) and edges with an explicit edge-type attribute; the template is populated with a sampled subgraph and fed directly to an LLM as a prompt so the LLM can produce a graph-level textual summary.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical token-based markup</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Subgraph sampling (random-walk-with-restart) → serialize nodes and edges into GraphML/XML tag sequences with node-level data fields (title, abstract) and edge-type tags; node and edge entries are listed sequentially within the GraphML structure.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ogbn-ArXiv, ArXiv_2023, PubMed, ogbn-Products (training set), Reddit (source datasets used to generate summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Pretraining data generation (graph → text summary pairs) for self-supervised contrastive pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-72B (used as the LLM to consume GraphML prompts and output summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2-72B, a large open LLM (72B parameters) used as a graph-summary generator; in GraphCLIP the produced summaries are then encoded by a frozen SBERT text encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream zero-shot node classification accuracy and link prediction AUC (used to measure the effectiveness of the overall pipeline where GraphML→summary enabled pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GraphCLIP (using generated summaries via GraphML prompts) achieved zero-shot node classification e.g. WikiCS 70.19% (vs w/o generated summaries 59.96%); link prediction AUC examples: WikiCS AUC 92.67% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enabling LLMs to generate graph-level summaries produced a large-scale graph-summary corpus (~0.2B tokens) that, when used for contrastive pretraining, substantially improved cross-domain zero-shot transferability compared to using raw node text; aligning graph and text modalities via these summaries yielded higher downstream zero-shot accuracy and AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generation requires invoking a large LLM (token and compute cost; reported corpus size ≈0.2B tokens), no canonical ordering guaranteed in template (not reported), and summary quality depends on LLM prompting; also GraphML serialization and long subgraph prompts may be token-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to using raw node text (node title/abstract) as the text modality, GraphML-driven LLM summaries produced more informative graph-level text and substantially better transferability (ablation: w/o Summary degrades performance by >10 percentage points on several datasets). Compared to prior works that directly feed node text or tokenized graph tokens (e.g., G2P2, GraphGPT, LLaGA), this method explicitly creates graph-level textual descriptions enabling stronger graph↔text alignment for contrastive pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7203.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7203.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-generated Graph Summaries</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-generated Graph-level Summaries (graph → text summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Textual summaries produced by a large language model (Qwen2-72B) that describe a sampled subgraph's focal node and its neighborhood, used as the text modality in contrastive pretraining to align graph and language embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM-generated graph summaries</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each sampled subgraph serialized in GraphML, an LLM generates a concise summary (paper/product/user summary plus contextual analysis and optionally a class description) that captures both node content and neighborhood-level structural context; summaries are used as the text view in graph–text contrastive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossy sequential document-level text</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph serialized via GraphML prompt → LLM (Qwen2-72B) generates a free-text summary per subgraph; summaries are later encoded by a sentence encoder (SBERT) into fixed-length vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Synthesized across source TAGs: ogbn-ArXiv, ArXiv_2023, PubMed, ogbn-Products (subset), Reddit; overall generated corpus ≈0.2B tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Pretraining data generation for self-supervised contrastive graph-summary alignment</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-72B (generator) → SBERT (sentence encoder used downstream)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2-72B used to generate graph summaries; SBERT (all-MiniLM-L6-v2 or similar) is used (frozen) to encode the produced summaries into embedding vectors for contrastive alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream zero-shot node classification accuracy and link prediction AUC; ablation contrasts with using raw node text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using generated summaries in GraphCLIP (vs raw node text): e.g., WikiCS accuracy 70.19% (GraphCLIP) vs 59.96% (w/o Summary); on Books-History improvement from 44.87% to 53.88% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provided rich graph-level textual signals that significantly improved modality alignment and cross-domain zero-shot performance; allowed freezing of text encoder during graph-model optimization to mitigate catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Summaries are lossy (they compress structural information and may omit details); generation is compute- and token-expensive; summary quality hinges on LLM prompting and the LLM's capabilities; no per-instance token counts or guaranteed canonicalization reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms approaches that use raw node text (w/o generated summaries) and improves over self-supervised graph-only methods and some graph↔text methods that rely on raw text (e.g., G2P2), by producing graph-aware textual descriptions enabling stronger contrastive alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7203.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7203.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label Sentence Templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Label-specific Sentence Templates for Zero-shot Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small set of deterministic natural-language templates that convert class labels into descriptive sentences (optionally augmented by LLM-generated class descriptions) and are used as text probes for zero-shot classification by cosine similarity to graph embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Label sentence templates (class descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each class label is turned into one or more natural-language sentences (e.g., "this paper belongs to [class] [class_desc]") which are encoded by the (frozen) text encoder; predictions are made by finding the label sentence embedding with maximum cosine similarity to a graph embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based templated sequential text</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Hand-designed sentence templates (dataset-specific) with optional LLM-generated class descriptions → encode with SBERT; compute cosine similarity between graph representation and label sentence encodings for zero-shot prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Evaluated across target datasets: Cora, CiteSeer, WikiCS, Instagram, Ele-Photo, Ele-Computers, Books-History</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot node classification (inference using text-label probes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SBERT (frozen sentence encoder) + GraphGPS graph encoder (GraphCLIP pretrained model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SBERT used to encode label sentences; GraphGPS-based Graph Transformer (12 layers, 1024 hidden in Base model) used to encode subgraphs; a projector aligns graph embedding dimension to SBERT's dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Node classification accuracy (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GraphCLIP zero-shot accuracies reported in Table 2 (e.g., WikiCS 70.19%, Cora 67.31%, CiteSeer 63.13%). These are obtained using the label-sentence probe approach.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Permits direct zero-shot deployment without training a classification head; leverages the shared embedding space learned in pretraining to map labels and graphs into comparable vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Effectiveness depends on the quality and expressiveness of the label descriptions; template design matters (dataset-specific prompts are provided); cannot handle classes not well described by short sentences without additional class descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Similar to CLIP-style label-probe approaches in multimodal models; more effective in this work than methods requiring a trained classifier on target labels (which preclude zero-shot use).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7203.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7203.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Syntax Tree (GraphText)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Syntax Tree (used in GraphText: Graph Reasoning in Text Space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text conversion used by GraphText to transform structured graph information into ordered textual sequences (G-Syntax Trees) so that language models can perform reasoning over graph-structured data; mentioned in the related work of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphText: Graph Reasoning in Text Space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>G-Syntax Tree linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A tree-structured serialization (G-Syntax Tree) that linearizes graph information into a sequence following a syntax designed for graph reasoning in text space; exact formatting and special tokens are defined in GraphText (cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical sequential (syntax-tree linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph → construct a G-Syntax Tree representation (tree/structured traversal) → linearize tree nodes/tokens into a sequence consumed by an LLM (details in GraphText paper).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-to-text conversion for LLM-based graph prediction/reasoning (as reported by GraphText)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Mentioned as an approach that maps graph structure into text sequences for LLMs; specifics and empirical impacts are in the original GraphText paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not discussed in detail in this paper; potential limitations include possible lossiness and sensitivity to linearization choice (typical for tree-to-sequence encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Cited alongside other graph→text approaches (LLaGA, GraphGPT); GraphCLIP authors note prior methods that convert graphs to text/token sequences often require labeled data or show poor cross-domain zero-shot transfer, motivating their summary-based contrastive approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7203.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7203.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaGA Node-level Templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node-level Template Linearization (used in LLaGA: Large Language and Graph Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node-level templating approach that converts graph structures into structured text sequences (node-level templates) mapped into the LLM token embedding space, mentioned in the paper's related work as an example of graph→text conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLaGA: Large Language and Graph Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node-level template serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Structured templates at the node level that serialize node attributes and local structure into sequences (e.g., <graph> with node templates) designed to be consumed by LLMs or mapped into token embeddings via a projector.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based sequential template</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Apply fixed node-centered templates that list node content and neighborhood tokens in a prescribed order; templates are then fed into or mapped to LLM token embeddings (exact method in LLaGA).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph→text conversion for instruction tuning of LLMs on graph tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaGA (LLM + specialized projector in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaGA uses LLM instruction tuning with graph-structured prompts; referred to in this paper as an approach that maps graph data into LLM-understandable sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>The paper notes LLaGA-like instruction-tuned approaches can overfit source data and demonstrate poor cross-domain zero-shot generalization in some evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned weaknesses include reliance on labeled data for tuning and poor cross-domain zero-shot transferability as observed by GraphCLIP authors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared in discussion to GraphCLIP: LLaGA-style node-template conversions map graphs into token sequences but often require supervised instruction tuning and show limited OOD transfer compared to GraphCLIP's self-supervised graph-summary contrastive pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7203.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7203.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT graph tokens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph tokenization via GNN + projector (used in GraphGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that encodes graphs using a GNN to produce graph tokens which are then projected into text-token embedding space for LLM consumption; discussed in related work and evaluated as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN-derived graph tokens (projected into text space)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>GNN encodes graph into discrete or continuous 'graph tokens'; an additional projector maps these tokens into the LLM's text embedding space; tokens are used to instruct or condition the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based embedding projection</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Encode graph structure with a GNN → produce token vectors corresponding to graph elements → learn a projector to map token vectors into LLM token embedding space for instruction-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction tuning / graph-conditioned LLM prediction</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraphGPT (GNN + projector + LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GraphGPT uses a GNN to produce graph representations and trains a projector to align them with an LLM for instruction-tuning; evaluated by GraphCLIP authors as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Zero-shot node classification accuracy (as reported in GraphCLIP baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GraphGPT reported low zero-shot accuracy on WikiCS (6.30%) under the GraphCLIP experimental setup (Table 2), indicating poor cross-domain transfer in that setting.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>GraphGPT-style token projection enables LLM conditioning but, in GraphCLIP's experiments, tended to overfit source data and yielded poor zero-shot cross-domain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires supervised instruction tuning and label information; observed to have poor cross-domain zero-shot generalization in GraphCLIP evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Contrasted with GraphCLIP's self-supervised graph-summary contrastive approach which delivered substantially better cross-domain zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7203.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7203.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OFA human-readable text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-readable Node/Edge Textualization (used in OFA / One For All)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that represents nodes and edges directly as human-readable text (natural-language descriptions) and encodes them via an LLM; cited in related work and used by OFA and ZeroG families of methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>One For All: Towards Training One Graph Model For All Classification Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Human-readable node/edge textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Nodes and edges are converted into plain natural-language descriptions (phrases or sentences describing node content and edge relations) and concatenated into prompts for LLM encoding and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based sequential natural-language serialization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Concatenate human-readable textual strings for nodes and edges into a sequence (often with task-specific prompting substructures); feed sequence to an LLM for encoding/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-to-text conversion for LLM-based encoding and downstream graph tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OFA (uses LLMs as node feature encoders and prompt structures)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OFA uses LLMs to encode human-readable textualizations of graph elements and introduces graph prompting substructures for task adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>OFA-style textualization provides an intuitive translation of graph structure into natural language but depends on labeled data for pretraining and can have limited cross-domain transferability as noted by GraphCLIP authors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on label information for pretraining in many instantiations; natural-language serializations of raw node/edge text alone may lack graph-level context and thus underperform versus graph-level summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>GraphCLIP authors argue that simple human-readable textualization (without generating graph-level summaries) is inferior for cross-domain zero-shot transfer compared to their LLM-generated summary approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GraphText: Graph Reasoning in Text Space <em>(Rating: 2)</em></li>
                <li>LLaGA: Large Language and Graph Assistant <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>One For All: Towards Training One Graph Model For All Classification Tasks <em>(Rating: 2)</em></li>
                <li>ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs <em>(Rating: 2)</em></li>
                <li>Augmenting low-resource text classification with graph-grounded pre-training and prompting <em>(Rating: 2)</em></li>
                <li>ConGrat: Self-supervised contrastive pretraining for joint graph and text embeddings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7203",
    "paper_id": "paper-96c56de54b07c296f7b6c86bf9f075d04c0d644e",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "GraphML Prompt (XML-like)",
            "name_full": "GraphML-based Graph Prompt Template (XML-like serialization)",
            "brief_description": "An XML/GraphML-style markup template used to serialize sampled subgraphs (nodes with title/abstract attributes and typed edges) into a text markup format that LLMs can ingest as input prompts for graph-to-text summarization; introduced and used in this paper to translate graph structure into text for summary generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "GraphML markup serialization",
            "representation_description": "An XML-like GraphML template that lists nodes and their textual attributes (e.g., title, abstract) and edges with an explicit edge-type attribute; the template is populated with a sampled subgraph and fed directly to an LLM as a prompt so the LLM can produce a graph-level textual summary.",
            "representation_type": "hierarchical token-based markup",
            "encoding_method": "Subgraph sampling (random-walk-with-restart) → serialize nodes and edges into GraphML/XML tag sequences with node-level data fields (title, abstract) and edge-type tags; node and edge entries are listed sequentially within the GraphML structure.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ogbn-ArXiv, ArXiv_2023, PubMed, ogbn-Products (training set), Reddit (source datasets used to generate summaries)",
            "task_name": "Pretraining data generation (graph → text summary pairs) for self-supervised contrastive pretraining",
            "model_name": "Qwen2-72B (used as the LLM to consume GraphML prompts and output summaries)",
            "model_description": "Qwen2-72B, a large open LLM (72B parameters) used as a graph-summary generator; in GraphCLIP the produced summaries are then encoded by a frozen SBERT text encoder.",
            "performance_metric": "Downstream zero-shot node classification accuracy and link prediction AUC (used to measure the effectiveness of the overall pipeline where GraphML→summary enabled pretraining)",
            "performance_value": "GraphCLIP (using generated summaries via GraphML prompts) achieved zero-shot node classification e.g. WikiCS 70.19% (vs w/o generated summaries 59.96%); link prediction AUC examples: WikiCS AUC 92.67% (Table 3).",
            "impact_on_training": "Enabling LLMs to generate graph-level summaries produced a large-scale graph-summary corpus (~0.2B tokens) that, when used for contrastive pretraining, substantially improved cross-domain zero-shot transferability compared to using raw node text; aligning graph and text modalities via these summaries yielded higher downstream zero-shot accuracy and AUC.",
            "limitations": "Generation requires invoking a large LLM (token and compute cost; reported corpus size ≈0.2B tokens), no canonical ordering guaranteed in template (not reported), and summary quality depends on LLM prompting; also GraphML serialization and long subgraph prompts may be token-intensive.",
            "comparison_with_other": "Compared to using raw node text (node title/abstract) as the text modality, GraphML-driven LLM summaries produced more informative graph-level text and substantially better transferability (ablation: w/o Summary degrades performance by &gt;10 percentage points on several datasets). Compared to prior works that directly feed node text or tokenized graph tokens (e.g., G2P2, GraphGPT, LLaGA), this method explicitly creates graph-level textual descriptions enabling stronger graph↔text alignment for contrastive pretraining.",
            "uuid": "e7203.0",
            "source_info": {
                "paper_title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-generated Graph Summaries",
            "name_full": "LLM-generated Graph-level Summaries (graph → text summaries)",
            "brief_description": "Textual summaries produced by a large language model (Qwen2-72B) that describe a sampled subgraph's focal node and its neighborhood, used as the text modality in contrastive pretraining to align graph and language embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "LLM-generated graph summaries",
            "representation_description": "For each sampled subgraph serialized in GraphML, an LLM generates a concise summary (paper/product/user summary plus contextual analysis and optionally a class description) that captures both node content and neighborhood-level structural context; summaries are used as the text view in graph–text contrastive learning.",
            "representation_type": "lossy sequential document-level text",
            "encoding_method": "Graph serialized via GraphML prompt → LLM (Qwen2-72B) generates a free-text summary per subgraph; summaries are later encoded by a sentence encoder (SBERT) into fixed-length vectors.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Synthesized across source TAGs: ogbn-ArXiv, ArXiv_2023, PubMed, ogbn-Products (subset), Reddit; overall generated corpus ≈0.2B tokens.",
            "task_name": "Pretraining data generation for self-supervised contrastive graph-summary alignment",
            "model_name": "Qwen2-72B (generator) → SBERT (sentence encoder used downstream)",
            "model_description": "Qwen2-72B used to generate graph summaries; SBERT (all-MiniLM-L6-v2 or similar) is used (frozen) to encode the produced summaries into embedding vectors for contrastive alignment.",
            "performance_metric": "Downstream zero-shot node classification accuracy and link prediction AUC; ablation contrasts with using raw node text.",
            "performance_value": "Using generated summaries in GraphCLIP (vs raw node text): e.g., WikiCS accuracy 70.19% (GraphCLIP) vs 59.96% (w/o Summary); on Books-History improvement from 44.87% to 53.88% (Table 4).",
            "impact_on_training": "Provided rich graph-level textual signals that significantly improved modality alignment and cross-domain zero-shot performance; allowed freezing of text encoder during graph-model optimization to mitigate catastrophic forgetting.",
            "limitations": "Summaries are lossy (they compress structural information and may omit details); generation is compute- and token-expensive; summary quality hinges on LLM prompting and the LLM's capabilities; no per-instance token counts or guaranteed canonicalization reported.",
            "comparison_with_other": "Outperforms approaches that use raw node text (w/o generated summaries) and improves over self-supervised graph-only methods and some graph↔text methods that rely on raw text (e.g., G2P2), by producing graph-aware textual descriptions enabling stronger contrastive alignment.",
            "uuid": "e7203.1",
            "source_info": {
                "paper_title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Label Sentence Templates",
            "name_full": "Label-specific Sentence Templates for Zero-shot Alignment",
            "brief_description": "A small set of deterministic natural-language templates that convert class labels into descriptive sentences (optionally augmented by LLM-generated class descriptions) and are used as text probes for zero-shot classification by cosine similarity to graph embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Label sentence templates (class descriptions)",
            "representation_description": "Each class label is turned into one or more natural-language sentences (e.g., \"this paper belongs to [class] [class_desc]\") which are encoded by the (frozen) text encoder; predictions are made by finding the label sentence embedding with maximum cosine similarity to a graph embedding.",
            "representation_type": "token-based templated sequential text",
            "encoding_method": "Hand-designed sentence templates (dataset-specific) with optional LLM-generated class descriptions → encode with SBERT; compute cosine similarity between graph representation and label sentence encodings for zero-shot prediction.",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "Evaluated across target datasets: Cora, CiteSeer, WikiCS, Instagram, Ele-Photo, Ele-Computers, Books-History",
            "task_name": "Zero-shot node classification (inference using text-label probes)",
            "model_name": "SBERT (frozen sentence encoder) + GraphGPS graph encoder (GraphCLIP pretrained model)",
            "model_description": "SBERT used to encode label sentences; GraphGPS-based Graph Transformer (12 layers, 1024 hidden in Base model) used to encode subgraphs; a projector aligns graph embedding dimension to SBERT's dimension.",
            "performance_metric": "Node classification accuracy (zero-shot)",
            "performance_value": "GraphCLIP zero-shot accuracies reported in Table 2 (e.g., WikiCS 70.19%, Cora 67.31%, CiteSeer 63.13%). These are obtained using the label-sentence probe approach.",
            "impact_on_training": "Permits direct zero-shot deployment without training a classification head; leverages the shared embedding space learned in pretraining to map labels and graphs into comparable vectors.",
            "limitations": "Effectiveness depends on the quality and expressiveness of the label descriptions; template design matters (dataset-specific prompts are provided); cannot handle classes not well described by short sentences without additional class descriptions.",
            "comparison_with_other": "Similar to CLIP-style label-probe approaches in multimodal models; more effective in this work than methods requiring a trained classifier on target labels (which preclude zero-shot use).",
            "uuid": "e7203.2",
            "source_info": {
                "paper_title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "G-Syntax Tree (GraphText)",
            "name_full": "G-Syntax Tree (used in GraphText: Graph Reasoning in Text Space)",
            "brief_description": "A graph-to-text conversion used by GraphText to transform structured graph information into ordered textual sequences (G-Syntax Trees) so that language models can perform reasoning over graph-structured data; mentioned in the related work of this paper.",
            "citation_title": "GraphText: Graph Reasoning in Text Space",
            "mention_or_use": "mention",
            "representation_name": "G-Syntax Tree linearization",
            "representation_description": "A tree-structured serialization (G-Syntax Tree) that linearizes graph information into a sequence following a syntax designed for graph reasoning in text space; exact formatting and special tokens are defined in GraphText (cited work).",
            "representation_type": "hierarchical sequential (syntax-tree linearization)",
            "encoding_method": "Graph → construct a G-Syntax Tree representation (tree/structured traversal) → linearize tree nodes/tokens into a sequence consumed by an LLM (details in GraphText paper).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph-to-text conversion for LLM-based graph prediction/reasoning (as reported by GraphText)",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Mentioned as an approach that maps graph structure into text sequences for LLMs; specifics and empirical impacts are in the original GraphText paper.",
            "limitations": "Not discussed in detail in this paper; potential limitations include possible lossiness and sensitivity to linearization choice (typical for tree-to-sequence encodings).",
            "comparison_with_other": "Cited alongside other graph→text approaches (LLaGA, GraphGPT); GraphCLIP authors note prior methods that convert graphs to text/token sequences often require labeled data or show poor cross-domain zero-shot transfer, motivating their summary-based contrastive approach.",
            "uuid": "e7203.3",
            "source_info": {
                "paper_title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaGA Node-level Templates",
            "name_full": "Node-level Template Linearization (used in LLaGA: Large Language and Graph Assistant)",
            "brief_description": "A node-level templating approach that converts graph structures into structured text sequences (node-level templates) mapped into the LLM token embedding space, mentioned in the paper's related work as an example of graph→text conversion.",
            "citation_title": "LLaGA: Large Language and Graph Assistant",
            "mention_or_use": "mention",
            "representation_name": "Node-level template serialization",
            "representation_description": "Structured templates at the node level that serialize node attributes and local structure into sequences (e.g., &lt;graph&gt; with node templates) designed to be consumed by LLMs or mapped into token embeddings via a projector.",
            "representation_type": "token-based sequential template",
            "encoding_method": "Apply fixed node-centered templates that list node content and neighborhood tokens in a prescribed order; templates are then fed into or mapped to LLM token embeddings (exact method in LLaGA).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph→text conversion for instruction tuning of LLMs on graph tasks",
            "model_name": "LLaGA (LLM + specialized projector in original work)",
            "model_description": "LLaGA uses LLM instruction tuning with graph-structured prompts; referred to in this paper as an approach that maps graph data into LLM-understandable sequences.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "The paper notes LLaGA-like instruction-tuned approaches can overfit source data and demonstrate poor cross-domain zero-shot generalization in some evaluations.",
            "limitations": "Mentioned weaknesses include reliance on labeled data for tuning and poor cross-domain zero-shot transferability as observed by GraphCLIP authors.",
            "comparison_with_other": "Compared in discussion to GraphCLIP: LLaGA-style node-template conversions map graphs into token sequences but often require supervised instruction tuning and show limited OOD transfer compared to GraphCLIP's self-supervised graph-summary contrastive pretraining.",
            "uuid": "e7203.4",
            "source_info": {
                "paper_title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GraphGPT graph tokens",
            "name_full": "Graph tokenization via GNN + projector (used in GraphGPT)",
            "brief_description": "A pipeline that encodes graphs using a GNN to produce graph tokens which are then projected into text-token embedding space for LLM consumption; discussed in related work and evaluated as a baseline in this paper.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models",
            "mention_or_use": "mention",
            "representation_name": "GNN-derived graph tokens (projected into text space)",
            "representation_description": "GNN encodes graph into discrete or continuous 'graph tokens'; an additional projector maps these tokens into the LLM's text embedding space; tokens are used to instruct or condition the LLM.",
            "representation_type": "token-based embedding projection",
            "encoding_method": "Encode graph structure with a GNN → produce token vectors corresponding to graph elements → learn a projector to map token vectors into LLM token embedding space for instruction-tuning.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Instruction tuning / graph-conditioned LLM prediction",
            "model_name": "GraphGPT (GNN + projector + LLM)",
            "model_description": "GraphGPT uses a GNN to produce graph representations and trains a projector to align them with an LLM for instruction-tuning; evaluated by GraphCLIP authors as a baseline.",
            "performance_metric": "Zero-shot node classification accuracy (as reported in GraphCLIP baseline comparisons)",
            "performance_value": "GraphGPT reported low zero-shot accuracy on WikiCS (6.30%) under the GraphCLIP experimental setup (Table 2), indicating poor cross-domain transfer in that setting.",
            "impact_on_training": "GraphGPT-style token projection enables LLM conditioning but, in GraphCLIP's experiments, tended to overfit source data and yielded poor zero-shot cross-domain performance.",
            "limitations": "Requires supervised instruction tuning and label information; observed to have poor cross-domain zero-shot generalization in GraphCLIP evaluations.",
            "comparison_with_other": "Contrasted with GraphCLIP's self-supervised graph-summary contrastive approach which delivered substantially better cross-domain zero-shot performance.",
            "uuid": "e7203.5",
            "source_info": {
                "paper_title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "OFA human-readable text",
            "name_full": "Human-readable Node/Edge Textualization (used in OFA / One For All)",
            "brief_description": "An approach that represents nodes and edges directly as human-readable text (natural-language descriptions) and encodes them via an LLM; cited in related work and used by OFA and ZeroG families of methods.",
            "citation_title": "One For All: Towards Training One Graph Model For All Classification Tasks",
            "mention_or_use": "mention",
            "representation_name": "Human-readable node/edge textualization",
            "representation_description": "Nodes and edges are converted into plain natural-language descriptions (phrases or sentences describing node content and edge relations) and concatenated into prompts for LLM encoding and downstream tasks.",
            "representation_type": "token-based sequential natural-language serialization",
            "encoding_method": "Concatenate human-readable textual strings for nodes and edges into a sequence (often with task-specific prompting substructures); feed sequence to an LLM for encoding/prediction.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph-to-text conversion for LLM-based encoding and downstream graph tasks",
            "model_name": "OFA (uses LLMs as node feature encoders and prompt structures)",
            "model_description": "OFA uses LLMs to encode human-readable textualizations of graph elements and introduces graph prompting substructures for task adaptation.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "OFA-style textualization provides an intuitive translation of graph structure into natural language but depends on labeled data for pretraining and can have limited cross-domain transferability as noted by GraphCLIP authors.",
            "limitations": "Relies on label information for pretraining in many instantiations; natural-language serializations of raw node/edge text alone may lack graph-level context and thus underperform versus graph-level summaries.",
            "comparison_with_other": "GraphCLIP authors argue that simple human-readable textualization (without generating graph-level summaries) is inferior for cross-domain zero-shot transfer compared to their LLM-generated summary approach.",
            "uuid": "e7203.6",
            "source_info": {
                "paper_title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GraphText: Graph Reasoning in Text Space",
            "rating": 2
        },
        {
            "paper_title": "LLaGA: Large Language and Graph Assistant",
            "rating": 2
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models",
            "rating": 2
        },
        {
            "paper_title": "One For All: Towards Training One Graph Model For All Classification Tasks",
            "rating": 2
        },
        {
            "paper_title": "ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs",
            "rating": 2
        },
        {
            "paper_title": "Augmenting low-resource text classification with graph-grounded pre-training and prompting",
            "rating": 2
        },
        {
            "paper_title": "ConGrat: Self-supervised contrastive pretraining for joint graph and text embeddings",
            "rating": 1
        }
    ],
    "cost": 0.01885375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs</h1>
<p>Yun Zhu<br>Zhejiang University<br>Hangzhou, China<br>zhuyun_dcd@zju.edu.cn</p>
<p>Yongchao Liu*<br>Ant Group<br>Hangzhou, China<br>yongchao.ly@antgroup.com</p>
<p>Haizhou Shi
Rutgers University
New Brunswick, New Jersey, US
haizhou.shi@rutgers.edu</p>
<p>Yaoke Wang<br>Zhejiang University<br>Hangzhou, China<br>wangyaoke@zju.edu.cn</p>
<h2>Xiaotang Wang</h2>
<p>Huazhong University of Science and
Technology
Wuhan, China
wangxiaotang0906@foxmail.com</p>
<h2>Boci Peng</h2>
<p>School of Intelligence Science and
Technology, Peking University
Beijing, China
bcpeng@stu.pku.edu.cn</p>
<h2>Chuntao Hong</h2>
<p>Ant Group
Beijing, China
chuntao.hct@antgroup.com</p>
<h2>Abstract</h2>
<p>Recently, research on Text-Attributed Graphs (TAGs) has gained significant attention due to the prevalence of free-text node features in real-world applications and the advancements in Large Language Models (LLMs) that bolster TAG methodologies. However, current TAG approaches face two primary challenges: (i) Heavy reliance on label information and (ii) Limited cross-domain zero/few-shot transferability. These issues constrain the scaling of both data and model size, owing to high labor costs and scaling laws, complicating the development of graph foundation models with strong transferability. In this work, we propose the GraphCLIP framework to address these challenges by learning graph foundation models with strong cross-domain zero/few-shot transferability through a self-supervised contrastive graph-summary pretraining method. Specifically, we generate and curate large-scale graph-summary pair data with the assistance of LLMs, and introduce a novel graphsummary pretraining method, combined with invariant learning, to enhance graph foundation models with strong cross-domain zero-shot transferability. For few-shot learning, we propose a novel graph prompt tuning technique aligned with our pretraining objective to mitigate catastrophic forgetting and minimize learning costs. Extensive experiments show the superiority of GraphCLIP in both zero-shot and few-shot settings, while evaluations across various</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Siliang Tang*<br>Zhejiang University<br>Hangzhou, China<br>siliang@zju.edu.cn</p>
<p>downstream tasks confirm the versatility of GraphCLIP. Our code is available at: https://github.com/ZhuYun97/GraphCLIP.</p>
<h2>CCS Concepts</h2>
<ul>
<li>Information systems $\rightarrow$ Data mining.</li>
</ul>
<h2>Keywords</h2>
<p>Graph Foundation Model, Graph Transformer, Graph Representation Learning, Self-supervised Learning</p>
<h2>ACM Reference Format:</h2>
<p>Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, and Siliang Tang. 2025. GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs. In Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 15 pages. https: //doi.org/10.1145/3696410.3714801</p>
<h2>1 Introduction</h2>
<p>Text-Attributed Graphs (TAGs) have gained significant attention recently $[5,9,14,30,52,74,79,95]$ due to the free-text node feature space prevalent in various domains such as social, e-commerce, and citation networks $[6,26,68]$. TAGs offer two natural advantages for graph learning research: (i) all node features can be aligned into within the same text space, enabling the model to transfer effectively across different graphs, and (ii) powerful off-the-shelf tools can be readily leveraged to address challenges within TAGs, e.g., Large Language Models (LLMs) can be used for enriching the textual representations of TAGs.
Existing TAG methods with LLMs. Significant efforts are underway to combine TAGs with LLMs, aiming to develop Graph Foundation Models (GFMs) [4, 21, 30, 31, 33, 62], a promising approach that enables the transfer of knowledge from source data to tackle various downstream tasks on target data through a unified backbone. Existing methods can be categorized into three main</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Three main categories of TAG methods.
types [21, 30]: LLM as Enhancer, LLM as Predictor, and LLM as Aligner, as illustrated in Figure 1.</p>
<p>LLM as Enhancer involves leveraging language models to augment raw text, yielding refined, high-quality outputs, or to encode node features, surpassing previous shallow embedding methods like Bag of Words (BoW). For instance, OFA [33] and ZeroG [31] utilize language models as node feature extractors, employing labeled source data to pretrain a graph model, which is then applied to target data using complicated graph prompt techniques. These methods depend heavily on high-quality labeled data, potentially limiting the full potential of graph foundation models due to scaling laws [15, 22]. The challenge arises from the difficulty of scaling up the pretraining corpus, primarily due to the associated high labor costs. In the paradigm of LLM as Predictor, the most essential task is to map graph data to a format that LLMs can comprehend. For example, GraphGPT [62] and LLaGA [4] utilize GNNs to encode graph data into graph tokens, training an additional projector to map these tokens into the text space through instruction tuning. However, these methods also require high-quality labels for target data, and recent studies [5, 32] have shown they exhibit poor crossdomain zero-shot performance. LLM as Aligner involves mapping graph and text modalities into a shared embedding space. For example, ConGrat [3] and G2P2 [75] apply self-supervised graph-text contrastive pretraining and focus on transferring pretrained models within the same graph, neglecting the cross-domain or cross-graph transferability of these models. More details of current TAG methods can be found in Appendix A.
Challenges for current TAG methods. As summarized, current methodologies encounter two primary challenges:
(i) Heavy reliance on label information: Most current approaches such as ZeroG [31], OFA [33], LLaGA [4], and others [14, 62, 73, 74, 95] require label information from source data as training signals, leading to significant labor costs. This constraint prevents GFMs from leveraging extensive unlabeled training corpora, thereby restricting the scaling of both data and model sizes in accordance with scaling laws [15].
(ii) Limited cross-domain zero/few-shot transferability: A wellpretrained GFM should be applicable directly to target data, achieving satisfactory performance without any parameter adjustments, i.e., strong cross-domain/dataset zero-shot capability, like CLIP [53] in multi-modal domain [29, 34, 48, 50, 85] and LLMs in NLP domain [47, 52, 64, 89]. However, most existing methods struggle with direct deployment in zero-shot settings on target data. They either cannot perform zero-shot learning because they require the training of a classification head using labeled target data to generate predictions [14, 87, 95], or they demonstrate inadequate zero-shot performance due to insufficient transferable knowledge acquired during pretraining [4, 33]. Additionally, in low-resource scenarios like few-shot learning, effectively leveraging limited training samples from target data while circumventing catastrophic forgetting poses a significant challenge.
Our proposed GraphCLIP framework. We develop GraphCLIP to address the challenges above, which learns graph foundation models with robust cross-domain zero-shot transferability from a novel self-supervised contrastive graph-summary pretraining technique. Specifically, we leverage the open LLM, Q̄wen2-72B [80], to generate graph-related summary texts of subgraphs, capitalizing on their powerful summarization abilities. Utilizing generated large-scale graph-summary pairs, we train a cross-domain graph foundation model through designed self-supervised contrastive graph-summary pretraining (addressing Challenge (i)). Considering the necessity of out-of-domain generalization across graphs, we introduce invariant learning during pretraining to capture invariant features, thereby enhancing out-of-domain generalization. After pretraining, GraphCLIP can be applied directly to target data without fine-tuning, demonstrating strong zero-shot capability on both in-domain and cross-domain graph data. For few-shot scenarios, we propose a novel graph prompt tuning approach aligned with our pretraining objective, reducing catastrophic forgetting [58] and minimizing learning costs [37, 59], ensuring excellent performance in few-shot settings (addressing Challenge (ii)). Extensive experiments show that GraphCLIP exhibits strong zero-shot performance across various in-domain and cross-domain target datasets. In few-shot settings, GraphCLIP with our designed prompt tuning outperforms previous state-of-the-art methods. Additionally, the universality of our method is demonstrated through evaluation across various downstream tasks.</p>
<p>Our contributions can be concluded as:</p>
<ul>
<li>We generate and curate a large-scale graph-summary pair dataset which contains over 0.2B tokens with the assistance of LLMs, establishing a valuable training corpus for the TAGs domain and advancing the field's development.</li>
<li>We propose GraphCLIP, a novel language-graph pretraining method combined with invariant learning that empowers cross-domain graph foundation models with strong zeroshot capability.</li>
<li>We introduce a novel graph prompt tuning technique aligned with our pretraining objectives for few-shot settings, mitigating catastrophic forgetting and minimizing learning costs.</li>
<li>Through extensive experiments, GraphCLIP demonstrates impressive performance in both zero-shot and few-shot scenarios. Furthermore, various downstream tasks are evaluated to validate the universality of GraphCLIP.</li>
</ul>
<h2>2 Preliminaries</h2>
<h3>2.1 Notations</h3>
<p>In this work, we focus on Text-Attributed Graphs, which incorporate raw text information for each node. Formally, given a textattributed graph $G=\left{\mathcal{V},\left{T_{n}\right}<em n="n">{n=1}^{N}, A\right}$, where $\mathcal{V}$ denotes the node set with $|\mathcal{V}|=N$ instances, $A \in \mathbb{R}^{N \times N}$ denotes the adjacency matrix, $T</em>$ represents the raw text for node $n \in[1,2 \ldots, N]$,} \in T^{L_{n}</p>
<p>$\mathcal{T}$ is the token dictionary, and $L_{n}$ is the sequence length. Node attributes $X$ are derived by applying SBERT [55] to the raw text information. To enhance scalability, a sampling function $\Gamma(\cdot)$ is applied to a large graph to derive a set of small ego-graphs $\mathcal{I}=\left{G_{n}\right}<em n="n">{n=1}^{N}$, where $G</em>$ represents the subgraph centered on node $n$.</p>
<h3>2.2 Problem Definition</h3>
<p>To develop a graph foundation model, extensive source graph data $\mathcal{G}^{\mathrm{s}}$ can be utilized to train a general graph model endowed with transferable knowledge:</p>
<p>$$
f_{\partial^{\star}}=\arg \min \underset{G_{i}^{\mathrm{s}} \in \mathcal{I}^{\mathrm{s}}}{\mathbb{E}} \mathcal{L}<em _partial="\partial">{\text {pretext }}\left(f</em>\right)
$$} ; G_{i}^{\mathrm{s}</p>
<p>where $\mathcal{I}^{\mathrm{s}}$ represents the set of sampled subgraphs derived from the source data, $f_{\partial}$ means graph neural networks like GCN [27], GAT [66] and Graph Transformer [54, 81], $\mathcal{L}<em _partial_star="\partial^{\star">{\text {pretext }}$ denotes pretext task like instance discrimination [12]. The optimal pretrained model $f</em>$ to perform downstream tasks such as node classification, link prediction, and graph classification.}}$ can then be applied to low-resource target graph data $\mathcal{G}^{\mathrm{t}</p>
<p>In this work, we focus on low-resource settings, including zeroshot and few-shot scenarios, which are critical capabilities for GFMs. For the zero-shot setting, the pretrained GFM can be directly deployed on target data without any adjustment:</p>
<p>$$
p_{n}=\underset{\hat{y}<em _partial_star="\partial^{\star">{n}}{\arg \max } P</em>}}\left(\hat{y<em n="n">{n} \mid G</em>
$$}^{\mathrm{t}}\right), \quad \forall G_{n}^{\mathrm{t}} \in \mathcal{I}^{\mathrm{t}</p>
<p>where $P_{\partial^{\star}}$ is the pretrained GFM, the prediction for the $n$-th instance is the class with the highest probability. $\mathcal{I}^{\mathrm{t}}$ represents the set of sampled subgraphs derived from the target data.</p>
<p>For the few-shot setting, a limited number of training samples for each class are used for fine-tuning:</p>
<p>$$
f_{\partial^{\star}} \in \underset{\partial}{\arg \max } \mathbb{E}<em n="n">{G</em>}^{\mathrm{t}} \in \mathcal{I}^{\mathrm{t} \mid \mathrm{tr}}} P_{\partial}\left(\hat{y<em n="n">{n}=y</em>\right)
$$} \mid G_{n}^{\mathrm{t}</p>
<p>where $y_{n}$ is the ground truth of $n$-th training sample, $\mathcal{I}^{\mathrm{t} \mid \text { tr }}$ denotes the training set of sampled subgraphs from the target data, and $f_{\partial^{\star}}$ is the fine-tuned model evaluated on the target test data.</p>
<p>Most existing TAG methods heavily rely on label information from source data in Equation 6, and struggle with low-resource target data, particularly in zero-shot setting. We will present solutions to address these challenges in Sec. 3.</p>
<h2>3 Method</h2>
<p>In this section, we present our approach to addressing the aforementioned challenges. First, we introduce the technique for generating and curating source data in Sec. 3.1. Based on this pretraining corpus, we then design a novel contrastive language-graph pretraining method to develop a graph foundation model in Sec. 3.2. Lastly, we outline the implementation of zero-shot learning on target data and propose a novel graph prompt tuning method for few-shot settings to fully leverage our model's potential in Sec. 3.3.2. A detailed complexity analysis of GraphCLIP is provided in Appendix G.</p>
<h3>3.1 Graph-Summary Pair Generation</h3>
<p>In TAG domain, there is abundant text describing each node, most current TAG methods leverage this textual information alongside structural data to pretrain graph models; however, these approaches
heavily depend on label information [14, 31, 33, 74, 87, 95], restricting scalability due to high labeling costs. Alternatively, some approaches [3, 75] design self-supervised training signals from original text data. However, a significant gap remains between raw text and graph-level information, resulting in suboptimal performance and limited transferability. These limitations impede the development of graph foundation models akin to CLIP [53] and BLIP [29] in the multimodal domain, which effectively leverage robust self-supervised signals.</p>
<p>To resolve this challenge, we exploit the remarkable summarization capabilities of LLMs to generate pertinent summaries for graphs to construct graph-summary pair data. Specifically, we employ a graph XML-like markup language, such as GraphML [2], and meticulously design prompt templates to enhance the LLMs' comprehension of input graphs, leveraging their adeptness with markup languages [78]. The proposed prompt template for transcribing citation network into markup language is in Template 3.1. In this template, we design two attributes for node content, title and abstract. Additionally, we establish one attribute for describing edge type. The blue text denotes placeholders that will be replaced with actual data. Using this template, we can seamlessly transform TAG into a format that LLMs can easily comprehend [78].</p>
<p>Considering scalability, we employ a sampling function, random walk with restart sampler, to sample subgraphs $\left{G_{n}\right}<em n="n">{n=1}^{N}$ from a large graph. These subgraphs will be incorporated into our prompt template and designed instructions to enable LLMs to generate the corresponding graph summaries $\left{S</em>$. In this study, we utilize Qwen2-72B [80] as the graph summary generator. Combining realistic unlabelled graph data, we generate and curate large-scale graph-summary pair data which contains over 0.2 B tokens across academic [14, 68, 84], e-commerce [18], and social domains [19]. For detailed instructions for graph summary generation, please refer to the Appendix D.}\right}_{n=1}^{N</p>
<div class="codehilite"><pre><span></span><code>Graph<span class="w"> </span>Prompt<span class="w"> </span>Template<span class="w"> </span>(Template<span class="w"> </span>3.1)
<span class="cp">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class="nt">&lt;graphml&gt;</span>
<span class="nt">&lt;key</span><span class="w"> </span><span class="na">id=</span><span class="s">&quot;d0&quot;</span><span class="w"> </span><span class="na">for=</span><span class="s">&quot;node&quot;</span><span class="w"> </span><span class="na">attr.name=</span><span class="s">&quot;title&quot;</span><span class="w"> </span><span class="na">attr.type=</span><span class="s">&quot;string&quot;</span><span class="nt">/&gt;</span>
<span class="nt">&lt;key</span><span class="w"> </span><span class="na">id=</span><span class="s">&quot;d1&quot;</span><span class="w"> </span><span class="na">for=</span><span class="s">&quot;node&quot;</span><span class="w"> </span><span class="na">attr.name=</span><span class="s">&quot;abstract&quot;</span><span class="w"> </span><span class="na">attr.type=</span><span class="s">&quot;string&quot;</span><span class="nt">/&gt;</span>
<span class="nt">&lt;key</span><span class="w"> </span><span class="na">id=</span><span class="s">&quot;d2&quot;</span><span class="w"> </span><span class="na">for=</span><span class="s">&quot;edge&quot;</span><span class="w"> </span><span class="na">attr.name=</span><span class="s">&quot;type&quot;</span><span class="w"> </span><span class="na">attr.type=</span><span class="s">&quot;string&quot;</span><span class="nt">/&gt;</span>
<span class="nt">&lt;graph</span><span class="w"> </span><span class="na">id=</span><span class="s">&quot;G&quot;</span><span class="w"> </span><span class="na">edgedefault=</span><span class="s">&quot;undirected&quot;</span><span class="nt">&gt;</span>
<span class="w">    </span><span class="nt">&lt;node</span><span class="w"> </span><span class="na">id=</span><span class="s">&quot;n0&quot;</span><span class="nt">&gt;</span>
<span class="w">        </span><span class="nt">&lt;data</span><span class="w"> </span><span class="na">key=</span><span class="s">&quot;d0&quot;</span><span class="nt">&gt;</span>{title_0}<span class="nt">&lt;/data&gt;</span>
<span class="w">        </span><span class="nt">&lt;data</span><span class="w"> </span><span class="na">key=</span><span class="s">&quot;d1&quot;</span><span class="nt">&gt;</span>{abstract_0}<span class="nt">&lt;/data&gt;</span>
<span class="w">    </span><span class="nt">&lt;/node&gt;</span>
<span class="w">    </span>...
<span class="w">    </span><span class="nt">&lt;node</span><span class="w"> </span><span class="na">id=</span><span class="s">&quot;nj&quot;</span><span class="nt">&gt;</span>
<span class="w">        </span><span class="nt">&lt;data</span><span class="w"> </span><span class="na">key=</span><span class="s">&quot;d0&quot;</span><span class="nt">&gt;</span>{title_j}<span class="nt">&lt;/data&gt;</span>
<span class="w">        </span><span class="nt">&lt;data</span><span class="w"> </span><span class="na">key=</span><span class="s">&quot;d1&quot;</span><span class="nt">&gt;</span>{abstract_j}<span class="nt">&lt;/data&gt;</span>
<span class="w">    </span><span class="nt">&lt;/node&gt;</span>
<span class="w">    </span><span class="nt">&lt;edge</span><span class="w"> </span><span class="na">id=</span><span class="s">&quot;e0&quot;</span><span class="w"> </span><span class="na">source=</span><span class="s">&quot;n0&quot;</span><span class="w"> </span><span class="na">target=</span><span class="s">&quot;nj&quot;</span><span class="nt">&gt;</span>
<span class="w">        </span><span class="nt">&lt;data</span><span class="w"> </span><span class="na">key=</span><span class="s">&quot;d2&quot;</span><span class="w"> </span><span class="nt">&gt;</span>{relation_0}<span class="nt">&lt;/data&gt;</span>
<span class="w">    </span><span class="nt">&lt;/edge&gt;</span>
<span class="w">    </span>...
<span class="w">    </span><span class="nt">&lt;edge</span><span class="w"> </span><span class="na">id=</span><span class="s">&quot;ek&quot;</span><span class="w"> </span><span class="na">source=</span><span class="s">&quot;ni&quot;</span><span class="w"> </span><span class="na">target=</span><span class="s">&quot;nj&quot;</span><span class="nt">&gt;</span>
<span class="w">        </span><span class="nt">&lt;data</span><span class="w"> </span><span class="na">key=</span><span class="s">&quot;d2&quot;</span><span class="w"> </span><span class="nt">&gt;</span>{relation_k}<span class="nt">&lt;/data&gt;</span>
<span class="w">    </span><span class="nt">&lt;/edge&gt;</span>
<span class="nt">&lt;/graph&gt;</span>
<span class="nt">&lt;/graphml&gt;</span>
</code></pre></div>

<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our proposed GraphCLIP Framework: (a) represents the self-supervised pretraining method we designed, (b) denotes zero-shot learning of GraphCLIP, and (c) refers to our graph prompt tuning method on target data.</p>
<h3>3.2 Self-Supervised Graph-Summary Contrastive Pretraining</h3>
<p>For graph-summary pair data, we employ different encoders to process their information according to their respective modalities. Then, a novel contrastive loss combined with invariant learning is deployed to align these modalities into the same subsapce.</p>
<h4>3.2.1 Graph Encoding</h4>
<p>Considering that model scale is crucial for the emergence and homogenization [35] of graph foundation models, we utilize Graph Transformers (GTs) [54, 72, 77, 81] instead of small GNN models like GCN [27] and GAT [66] to encode graph information. Given a subgraph $G_i = (P_i, X_i, A_i)$, we encode the graph information as follows:</p>
<p>$$h_i = \mathcal{P}(g_{\theta}(P_i, X_i, A_i)),\tag{4}$$</p>
<p>where $g_{\theta}$ denotes the Graph Transformer, such as GraphGPS [54], $P_i$ represents the positional embeddings for the subgraph, e.g., RWPE [7], and $\mathcal{P}$ is the mean pooling function that yields the graph-level encoding $h_i \in \mathbb{R}^{1 \times d}$ for the subgraph.</p>
<h4>3.2.2 Summary Encoding</h4>
<p>To encode sentence or document-level information into a compact and semantically rich vector, we utilize sentence-level text encoders like SBERT [55]:</p>
<p>$$u_i = \mathcal{P}(f_{\phi}(S_i)) = \mathcal{P}(\text{LM}([CLS], s_1, s_2, \dots, s_L)) \tag{5}$$</p>
<p>where $S_i$ is the summary text for subgraph $G_i$, and $s_1, s_2, \dots, s_L$ are the tokens of the summary text. where $S_i = [CLS, s_1, s_2, \dots, s_L]$ is the summary text of subgraph $G_i$, which is pre-generated by LLMs as described in Sec. 3.1. We use a mean pooling function $\mathcal{P}$ to average token representations as the summary encoding $u_i$, which captures high-level document-level information. In subsequent sections, $\mathcal{P}$ will be omitted for clarity.</p>
<h4>3.2.3 Contrastive Graph-Summary Pretraining</h4>
<p>After obtaining the graph and summary encodings $H, U \in \mathbb{R}^{N \times d}$, we employ contrastive loss [46] to align the two modalities. Unlike previous multimodal pretraining methods such as CLIP, different graphs can vary significantly across domains, making it essential to capture transferable or causal features in the graph domain. To achieve this goal, we introduce invariant learning [1, 91] efficiently to extract causal features rather than spurious ones. Below we first revisit the concepts of contrastive loss [46] and invariant learning [1]. Then we formulate how they can be combined to solve the challenges of graph foundation models.</p>
<p><strong>Definition 1 (Contrastive Loss [46]).</strong> The contrastive loss function is applied to representations, pulling together the positive pairs while pushing apart negative pairs:</p>
<p>$$\mathcal{L}<em _theta="\theta">{CL}(g</em>}, f_{\phi}, \mathbb{P}, \mathbb{Q<em G_S="G,S" _mathbb_P="\mathbb{P" _sim="\sim">G, \mathbb{Q}_S, \pi) = \mathbb{E}</em>}}[\mathbb{E<em _alpha="\alpha">{r</em>(S)) ||^2]$$},r_{\beta} \sim \pi^{2}} ||g_{\theta} (\tau_{\alpha} (G)) - f_{\phi} (\tau_{\beta</p>
<p>$$\mathbb{E}<em G_="G'" _mathbb_Q="\mathbb{Q" _sim="\sim">{S \sim \mathbb{Q}_S} \log \sum</em><em _pi="\pi" _sim="\sim" r_="r'">G} \mathbb{E}</em> \right],$$} \left[ e^{\left| f_{\phi} (\tau_{\alpha}(S)) - g_{\theta} (\tau' (G') ) \right|^2</p>
<p>where $G, S \sim \mathbb{P}$ represent positive pairs sampled from the joint distribution of graphs and summaries, while $\mathbb{Q}_G$ and $\mathbb{Q}_S$ denote the marginal distributions of graphs and summaries, respectively. $\tau$ refers to the set of data transformations (augmentations) used to generate augmented views. The second line in Equation 6 is termed the alignment loss, and the third line is termed the uniformity loss [70].</p>
<p>However, this loss is not robust to distribution shifts [90, 94] because the expectation operator over different data transformation in the alignment loss can not guarantee the invariant features, resulting poor transferability. Refer to Appendix B for detailed derivation. In order to solve this dilemma, we will combine invariant learning into our method.</p>
<p>Definition 2 (Invariant learning [1]). If a classifier $c_{c t^{<em>}}$ is considered simultaneously optimal for all domains in $\mathcal{H}$, then a data representation $g_{\theta}$ can elicit an invariant predictor $c_{c t^{</em>}} \circ g_{\theta}$ across the domain set $\mathcal{H}$ :</p>
<p>$$
c_{c t^{*}} \in \arg \min <em c="c" t="t">{c</em>
$$}} \mathcal{R}\left(c_{c t} \circ g_{\theta} ; \mathcal{G}\right) \text { for all } \mathcal{G} \in \mathcal{H</p>
<p>where $\mathcal{R}$ denotes the risk associated with the predictor $c_{c t} \circ g_{\theta}$ evaluated on the domain $\mathcal{G}$.</p>
<p>However, this method heavily relies on environment and downstream labels [1], which is not compatible with our self-supervised contrastive loss. To address this issue, we combine the merits of invariant learning and vanilla contrastive loss to obtain a shift-robust contrastive loss, thereby enhancing transferability and generalization across diverse graphs. The core component of our shift-robust contrastive loss is the invariant alignment loss:</p>
<p>Definition 3 (Invariant Alignment Loss [94]). The invariant alignment loss $\mathcal{L}<em _theta="\theta">{\text {IAL }}$ of the encoders $g</em>$ of graphs and summaries is defined as follows:}$ and $f_{\phi}$ over the joint distribution $\mathbb{P</p>
<p>$$
\mathcal{L}<em _theta="\theta">{I A L}\left(g</em> \sup } ; \mathcal{G}\right):=\underset{G, S \in \mathbb{P}}{\mathbb{E}<em _theta="\theta">{\tau, \tau^{\prime} \sim \pi}\left|g</em>
$$}(\tau(G))-f_{\phi}\left(\tau^{\prime}(S)\right)\right|^{2</p>
<p>The supreme operator quantifies the disparity between two representations under the most "challenging" augmentations, as opposed to the trivial expectation delineated in Equation 6. This methodology enables the invariant alignment loss to generate consistent linear optimal predictors across disparate domains, thus facilitating enhanced out-of-distribution (OOD) generalization and transferability that are deficient in the original alignment loss. Further analysis and theoretical justifications are provided in the Appendix C.</p>
<p>A significant concern regarding the substitution of alignment loss $\mathcal{L}<em _IAL="{IAL" _text="\text">{A L}$ with $\mathcal{L}</em>$, as this requires iterating through all augmentation spaces. To efficiently identify the worst-case scenario in the continuous space, we employ adversarial training $[25,28,57,61,93,94]$ to approximate the supremum operator:}}$ lies in the impracticality of estimating $\sup _{\tau, \tau^{\prime} \sim \tau}|g(\tau(G))-f\left(\tau^{\prime}(S)\right)|^{2</p>
<p>$$
\min <em S_="S)" _G_="(G," _mathbb_P="\mathbb{P" _sim="\sim">{\theta} \mathbb{E}</em>\left{\max }<em _theta="\theta">{|\delta|</em>} \leq \epsilon} \mathcal{L<em _theta="\theta">{\mathrm{CL}}\left(g</em>(S)\right)\right}
$$}(X+\delta, A, P), f_{\phi</p>
<p>where the inner loop optimizes the loss to approximate the most challenging perturbation $\delta$, whose magnitude $|\delta| \leq \epsilon$ is meticulously regulated to ensure that it does not alter the semantic labels of the original view, e.g., $\epsilon=1 \times 10^{-2}$. Here, we only add perturbation on graph encoding, because we freeze the text encoder to mitigate catastrophic forgetting and avoid overfitting [45, 58, 96].</p>
<h3>3.3 Model Adaptation on Target Data</h3>
<p>In this section, we introduce the techniques employed to adapt models to target datasets. First, we illustrate the adaptation of GraphCLIP on target data for zero-shot learning. Then, we propose a novel graph prompt tuning method for few-shot learning.
3.3.1 Zero-shot Learning. Upon pretraining with Equation 9, our model can be directly deployed on target datasets without any additional training, i.e., enabling zero-shot inference as depicted in Figure 2b. We meticulously craft the prompt to incorporate target
label information. For example, in the context of a citation network, the sentence associated with label information is formulated as "This paper belongs to [class]". The specific templates are detailed in the Appendix D. Formally, zero-shot learning is defined as follows:</p>
<p>$$
\hat{y}<em u__k="u_{k">{i}=\underset{k}{\arg \max } \mathbb{E}</em>\right)
$$}} \operatorname{sim}\left(h_{i}, u_{k</p>
<p>where sim denotes the cosine similarity function, we identify the most similar label sentence as the predicted label for node $i$.
3.3.2 Graph Prompt Tuning under Few-shot Setting. In lowresource scenarios, where only a few samples exist for each class of target data, effectively utilizing this data while preventing overfitting and catastrophic forgetting [39, 45, 49, 58, 96] becomes crucial. In this work, we introduce a novel graph prompt tuning approach to address this challenge.</p>
<p>Specifically, during the graph prompt tuning process, both the text and graph models remain frozen, allowing only a limited set of parameters to be learnable. To align with our pretraining objective, we incorporate a learnable prompt feature that resemble perturbations used during pretraining, and we employ supervised contrastive loss [24]. The total loss of our designed graph prompt tuning is as follows:</p>
<p>$$
\min <em Z_="Z)" _G_="(G," _mathbb_P="\mathbb{P" _sim="\sim">{\sigma} \mathbb{E}</em>}^{\text {tar }}}\left{\mathcal{L<em>{\mathrm{SCL}}\left(g</em>{\theta^{<em>}}(X+\sigma, A, P), f_{\phi^{</em>}}(Z)\right)\right}
$$</p>
<p>where $g_{\theta^{<em>}}$ and $f_{\phi^{</em>}}$ denote the frozen graph and text models, respectively, $Z$ represents the label-related sentence, and $\mathbb{P}^{\text {tar }}$ signifies the distribution of labeled target data. $\mathcal{L}_{\mathrm{SCL}}$ is the supervised contrastive loss [76], which considers pairs with the same labels as positive and those with different labels as negative. After the graph prompt tuning, the evaluation on the target testing data proceeds in the same manner as outlined in Equation 10.</p>
<h2>4 Experiments</h2>
<p>In this section, we first introduce the datasets used in Section 4.1 and the baselines in Section 4.2. We then aim to address the following research questions through our experiments: RQ1: How good is the GraphCLIP's in-domain and cross-domain zero-shot transferability? RQ2: How effective is our proposed graph tuning in few-shot scenarios? RQ3: What impact does the source data have on cross-domain transferability? RQ4: What is the effect of hyperparameters on performance? RQ5: How do the main components of our model influence performance?</p>
<h3>4.1 Datasets</h3>
<p>In this work, we utilize 12 open text-attributed graphs across four diverse domains, comprising 5 large-scale TAGs for source data during pretraining and 7 small-scale TAGs for target data evaluation. The statistics of these datasets are detailed in Table 1. To balance the ratio of different source data, we employ the training set of ogbn-Products as pretraining corpus, which consists of around 200K products. More details of these datasets can be found in Appendix E.</p>
<p>Table 1: Statistics of Text-Attributed Graph datasets. $\mathcal{G}<em _target="{target" _text="\text">{\text {source }}$ denotes source datasets, and $\mathcal{G}</em>$ indicates target datasets.}</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">4Nodes</th>
<th style="text-align: center;">4Edges</th>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">4 C</th>
<th style="text-align: center;">Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ogbn-ArXiv [68]</td>
<td style="text-align: center;">169,343</td>
<td style="text-align: center;">1,166,243</td>
<td style="text-align: center;">Academic</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$\mathcal{G}_{\text {source }}$</td>
</tr>
<tr>
<td style="text-align: center;">ArXiv_2023 [14]</td>
<td style="text-align: center;">46,198</td>
<td style="text-align: center;">78,543</td>
<td style="text-align: center;">Academic</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$\mathcal{G}_{\text {source }}$</td>
</tr>
<tr>
<td style="text-align: center;">PubMed [84]</td>
<td style="text-align: center;">19,717</td>
<td style="text-align: center;">44,338</td>
<td style="text-align: center;">Academic</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\mathcal{G}_{\text {source }}$</td>
</tr>
<tr>
<td style="text-align: center;">ogbn-Products [18]</td>
<td style="text-align: center;">2,449,029</td>
<td style="text-align: center;">61,859,140</td>
<td style="text-align: center;">E-commerce</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">$\mathcal{G}_{\text {source }}$</td>
</tr>
<tr>
<td style="text-align: center;">Reddit [19]</td>
<td style="text-align: center;">33,434</td>
<td style="text-align: center;">198,448</td>
<td style="text-align: center;">Social</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\mathcal{G}_{\text {source }}$</td>
</tr>
<tr>
<td style="text-align: center;">Cora [56]</td>
<td style="text-align: center;">2,708</td>
<td style="text-align: center;">5,429</td>
<td style="text-align: center;">Academic</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$\mathcal{G}_{\text {target }}$</td>
</tr>
<tr>
<td style="text-align: center;">CiteSeer [10]</td>
<td style="text-align: center;">3,186</td>
<td style="text-align: center;">4,277</td>
<td style="text-align: center;">Academic</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$\mathcal{G}_{\text {target }}$</td>
</tr>
<tr>
<td style="text-align: center;">Ele-Photo [79]</td>
<td style="text-align: center;">48,362</td>
<td style="text-align: center;">500,928</td>
<td style="text-align: center;">E-commerce</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$\mathcal{G}_{\text {target }}$</td>
</tr>
<tr>
<td style="text-align: center;">Ele-Computers [79]</td>
<td style="text-align: center;">87,229</td>
<td style="text-align: center;">721,081</td>
<td style="text-align: center;">E-commerce</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$\mathcal{G}_{\text {target }}$</td>
</tr>
<tr>
<td style="text-align: center;">Books-History [79]</td>
<td style="text-align: center;">41,551</td>
<td style="text-align: center;">358,574</td>
<td style="text-align: center;">E-commerce</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$\mathcal{G}_{\text {target }}$</td>
</tr>
<tr>
<td style="text-align: center;">WikiCS [43]</td>
<td style="text-align: center;">11,701</td>
<td style="text-align: center;">215,863</td>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$\mathcal{G}_{\text {target }}$</td>
</tr>
<tr>
<td style="text-align: center;">Instagram [19]</td>
<td style="text-align: center;">11,339</td>
<td style="text-align: center;">144,010</td>
<td style="text-align: center;">Social</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\mathcal{G}_{\text {target }}$</td>
</tr>
</tbody>
</table>
<h3>4.2 Baselines</h3>
<p>To evaluate the effectiveness of GraphCLIP, we compare it against 17 baselines, which include: (i) eight LLM-only methods (without modeling structural information of the graphs), i.e., BERT [23], SBERT [55], DeBERTa [13], E5 [69], Qwen2-7BInsturct [80], Qwen2-72B-Insturct [80], LLaMA3.1-8B-Instruct [65], and LLaMA3.1-Insturct-70B [65], (ii) 4 state-of-the-art TAG methods, i.e., GraphGPT [62], LLaGA [4], OFA [33], and ZeroG [31], and (iii) 5 self-supervised graph algorithms applied to TAGs, i.e., DGI [67], GRACE [97], BGRL [63], GraphMAE [16] and G2P2 [75]. Specifically, to assess the zero-shot performance of discriminative LLMs and self-supervised graph algorithms on target data, we will use the cosine similarity between node embeddings and label embeddings for predictions. For generative LLM methods, we leverage their generative capabilities to estimate their zero-shot performance. Details of these baselines can be found in the Appendix F.</p>
<h3>4.3 Zero-Shot Inference on Target Data (RQ1)</h3>
<p>In order to evaluate the zero-shot transferability of our pretrained model, we conduct experiments of node-level and link-level tasks.
4.3.1 Node Classification. In this subsection, we will perform zeroshot node classification by directly applying pretrained models to target datasets.</p>
<p>Experimental Setup. For the LLM baselines, we directly apply them to the source data, as they have already been pretrained on extensive corpora, and we find that continuing to pretrain them on our source data deteriorates performance on the target data. For GraphGPT, we utilize their released checkpoint ${ }^{1}$ and conduct experiments under our settings. For other methods, we use their official codes and pretrain models using the source datasets specified in Table 1 before applying them to the target datasets. For data splitting, we use the public split for the WikiCS dataset, while for other datasets, we randomly select $20 \%$ as testing samples. We report the mean accuracy along with the standard deviation after five runs with different random seeds.</p>
<p>Analysis. From Table 2, we can draw several conclusions. First, generative LLMs demonstrate decent zero-shot performance, particularly LLaMA3.1-70B and Qwen2-72B, attributed to their extensive parameters and training on vast source data. However, these models</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>struggle to leverage structural information, resulting in subpar performance on certain target datasets; for instance, LLaMA3.1-70B only achieves $62.82 \%$ and $43.68 \%$ on the WikiCS and Instagram datasets, respectively.</p>
<p>Second, LLaGA and GraphGPT employ graph instruction tuning to bridge this gap, but they tend to overfit to the source data, leading to poor generalization. This is evident as GraphGPT and LLaGA achieve only $6.3 \%$ and $2.65 \%$ accuracy on the WikiCS dataset, significantly trailing behind other methods.</p>
<p>Third, OFA and ZeroG require label information for pretraining on source data, which results in suboptimal cross-domain transferability due to mismatched label spaces and a failure to capture causal features. For example, ZeroG only achieves $54.93 \%$ accuracy on the WikiCS dataset.</p>
<p>On the contrary, our approach utilizes a self-supervised training task combined with invariant learning to enhance both crossdomain and in-domain transferability. Notably, on the cross-domain dataset WikiCS, our method achieves $70.19 \%$ zero-shot accuracy, surpassing ZeroG by over $15 \%$ in absolute improvement. Similarly, on in-domain datasets such as Books-History, GraphCLIP outperforms the previous SoTA method, ZeroG, by over $10 \%$ in absolute improvement.</p>
<p>Lastly, we observe that four self-supervised graph methods, i.e., DGI, GRACE, BGRL, and GraphMAE, consistently underperform across most target datasets due to their lack of effective alignment between graph and text modalities, thereby failing to leverage powerful off-the-shelf text encoders. Furthermore, G2P2 uses a text encoder as an aligner; however, it relies on raw text as input for the text modality, which lacks comprehensive graph-level descriptions, resulting in limited transferability.
4.3.2 Link Prediction. In this subsection, we perform zero-shot link prediction by directly applying the pretrained models to the target datasets.</p>
<p>Experimental Setup. For link prediction, we report the mean AUC score along with the standard deviation after five runs with different random seeds. Since generative LLM methods produce text outputs, which complicates the extraction of logits or probabilities, we exclude them from this evaluation. For the data splitting, we randomly select $50 \%$ as testing samples and employ the same pretrained models as discussed in the previous section.</p>
<p>Analysis. From Table 2, we observe that SBERT achieves decent performance. ZeroG, which relies on SBERT and integrates structural information to finetune SBERT through LoRA [17], achieves subpar performance. Notably, our approach demonstrates the best zero-shot performance on link prediction across the evaluated target datasets, highlighting the effectiveness of our designed selfsupervised pretraining task and the versatility of our framework.</p>
<h3>4.4 Graph Prompt Tuning (RQ2)</h3>
<p>In low-resource scenarios with few training samples, effectively utilizing these samples while preventing overfitting and negative transfer is crucial. This section evaluates our graph prompt tuning approach to address this challenge.</p>
<p>Table 2: Zero-shot inference results for node classification across various target datasets. Boldface indicates the best performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\mathcal{G}_{\text {source }}$</th>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">CiteSeer</th>
<th style="text-align: center;">WikiCS</th>
<th style="text-align: center;">Instagram</th>
<th style="text-align: center;">Ele-Photo</th>
<th style="text-align: center;">Ele-Computers</th>
<th style="text-align: center;">Books-History</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">BERT [23]</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">$19.56 \pm 0.98$</td>
<td style="text-align: center;">$33.26 \pm 2.35$</td>
<td style="text-align: center;">$29.37 \pm 0.00$</td>
<td style="text-align: center;">$57.02 \pm 0.57$</td>
<td style="text-align: center;">$21.80 \pm 0.14$</td>
<td style="text-align: center;">$13.88 \pm 0.29$</td>
<td style="text-align: center;">$9.95 \pm 0.42$</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">SBERT [55]</td>
<td style="text-align: center;">66 M</td>
<td style="text-align: center;">$54.35 \pm 1.26$</td>
<td style="text-align: center;">$50.47 \pm 0.90$</td>
<td style="text-align: center;">$48.16 \pm 0.00$</td>
<td style="text-align: center;">$48.34 \pm 1.23$</td>
<td style="text-align: center;">$35.96 \pm 0.44$</td>
<td style="text-align: center;">$41.82 \pm 0.22$</td>
<td style="text-align: center;">$30.45 \pm 0.19$</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">DeBERTa [13]</td>
<td style="text-align: center;">184 M</td>
<td style="text-align: center;">$16.42 \pm 1.26$</td>
<td style="text-align: center;">$16.42 \pm 1.26$</td>
<td style="text-align: center;">$15.29 \pm 0.00$</td>
<td style="text-align: center;">$39.81 \pm 0.58$</td>
<td style="text-align: center;">$12.38 \pm 0.26$</td>
<td style="text-align: center;">$10.62 \pm 0.15$</td>
<td style="text-align: center;">$8.70 \pm 0.26$</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">E5 [69]</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">$44.65 \pm 0.82$</td>
<td style="text-align: center;">$42.57 \pm 0.54$</td>
<td style="text-align: center;">$31.49 \pm 0.00$</td>
<td style="text-align: center;">$61.28 \pm 0.97$</td>
<td style="text-align: center;">$35.14 \pm 0.28$</td>
<td style="text-align: center;">$16.54 \pm 0.14$</td>
<td style="text-align: center;">$12.92 \pm 0.48$</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Qwen2 [80]</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$61.44 \pm 1.29$</td>
<td style="text-align: center;">$53.57 \pm 0.86$</td>
<td style="text-align: center;">$58.72 \pm 0.25$</td>
<td style="text-align: center;">$39.13 \pm 0.78$</td>
<td style="text-align: center;">$45.55 \pm 0.12$</td>
<td style="text-align: center;">$59.18 \pm 0.20$</td>
<td style="text-align: center;">$23.79 \pm 0.34$</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Qwen2 [80]</td>
<td style="text-align: center;">72B</td>
<td style="text-align: center;">$62.18 \pm 0.98$</td>
<td style="text-align: center;">$60.97 \pm 0.87$</td>
<td style="text-align: center;">$60.91 \pm 0.08$</td>
<td style="text-align: center;">$47.70 \pm 0.31$</td>
<td style="text-align: center;">$52.41 \pm 0.39$</td>
<td style="text-align: center;">$60.88 \pm 0.30$</td>
<td style="text-align: center;">$53.56 \pm 0.64$</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LLaMA3.1 [65]</td>
<td style="text-align: center;">8B</td>
<td style="text-align: center;">$57.75 \pm 1.21$</td>
<td style="text-align: center;">$53.54 \pm 1.71$</td>
<td style="text-align: center;">$58.32 \pm 0.21$</td>
<td style="text-align: center;">$39.37 \pm 1.14$</td>
<td style="text-align: center;">$34.38 \pm 0.25$</td>
<td style="text-align: center;">$46.98 \pm 0.21$</td>
<td style="text-align: center;">$22.28 \pm 0.18$</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">LLaMA3.1 [65]</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$65.72 \pm 1.24$</td>
<td style="text-align: center;">$62.79 \pm 1.24$</td>
<td style="text-align: center;">$62.82 \pm 0.04$</td>
<td style="text-align: center;">$43.68 \pm 0.52$</td>
<td style="text-align: center;">$51.26 \pm 0.53$</td>
<td style="text-align: center;">$61.62 \pm 0.42$</td>
<td style="text-align: center;">$53.33 \pm 0.55$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A, Y$</td>
<td style="text-align: center;">GraphGPT [62]</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$23.25 \pm 1.45$</td>
<td style="text-align: center;">$18.04 \pm 1.45$</td>
<td style="text-align: center;">$6.30 \pm 0.26$</td>
<td style="text-align: center;">$45.12 \pm 1.16$</td>
<td style="text-align: center;">$7.62 \pm 0.22$</td>
<td style="text-align: center;">$29.71 \pm 0.83$</td>
<td style="text-align: center;">$15.92 \pm 0.14$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A, Y$</td>
<td style="text-align: center;">LLaGA [4]</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$21.44 \pm 0.65$</td>
<td style="text-align: center;">$16.07 \pm 1.15$</td>
<td style="text-align: center;">$2.65 \pm 0.72$</td>
<td style="text-align: center;">$41.12 \pm 0.94$</td>
<td style="text-align: center;">$6.50 \pm 0.53$</td>
<td style="text-align: center;">$23.10 \pm 0.33$</td>
<td style="text-align: center;">$11.17 \pm 0.58$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A, Y$</td>
<td style="text-align: center;">OFA [33]</td>
<td style="text-align: center;">30 M</td>
<td style="text-align: center;">$37.25 \pm 1.38$</td>
<td style="text-align: center;">$29.64 \pm 0.19$</td>
<td style="text-align: center;">$45.52 \pm 1.06$</td>
<td style="text-align: center;">$32.71 \pm 0.16$</td>
<td style="text-align: center;">$33.03 \pm 0.64$</td>
<td style="text-align: center;">$22.09 \pm 0.39$</td>
<td style="text-align: center;">$16.87 \pm 0.93$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A, Y$</td>
<td style="text-align: center;">ZeroG [31]</td>
<td style="text-align: center;">66 M</td>
<td style="text-align: center;">$62.32 \pm 1.91$</td>
<td style="text-align: center;">$52.55 \pm 1.23$</td>
<td style="text-align: center;">$54.93 \pm 0.06$</td>
<td style="text-align: center;">$48.97 \pm 0.78$</td>
<td style="text-align: center;">$45.12 \pm 0.65$</td>
<td style="text-align: center;">$56.20 \pm 0.35$</td>
<td style="text-align: center;">$40.74 \pm 0.65$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A$</td>
<td style="text-align: center;">DGI [67]</td>
<td style="text-align: center;">128 M</td>
<td style="text-align: center;">$24.03 \pm 1.40$</td>
<td style="text-align: center;">$18.71 \pm 1.22$</td>
<td style="text-align: center;">$18.86 \pm 0.25$</td>
<td style="text-align: center;">$61.42 \pm 1.12$</td>
<td style="text-align: center;">$13.96 \pm 0.17$</td>
<td style="text-align: center;">$27.12 \pm 0.03$</td>
<td style="text-align: center;">$15.77 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A$</td>
<td style="text-align: center;">GRACE [97]</td>
<td style="text-align: center;">128 M</td>
<td style="text-align: center;">$13.69 \pm 1.27$</td>
<td style="text-align: center;">$22.88 \pm 1.49$</td>
<td style="text-align: center;">$16.07 \pm 0.32$</td>
<td style="text-align: center;">$62.23 \pm 0.93$</td>
<td style="text-align: center;">$10.16 \pm 0.13$</td>
<td style="text-align: center;">$10.94 \pm 0.12$</td>
<td style="text-align: center;">$32.39 \pm 0.11$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A$</td>
<td style="text-align: center;">BGRL [63]</td>
<td style="text-align: center;">128 M</td>
<td style="text-align: center;">$20.80 \pm 1.06$</td>
<td style="text-align: center;">$26.50 \pm 1.22$</td>
<td style="text-align: center;">$18.35 \pm 0.22$</td>
<td style="text-align: center;">$61.45 \pm 0.82$</td>
<td style="text-align: center;">$5.21 \pm 0.22$</td>
<td style="text-align: center;">$24.12 \pm 0.22$</td>
<td style="text-align: center;">$16.28 \pm 0.35$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A$</td>
<td style="text-align: center;">GraphMAE [16]</td>
<td style="text-align: center;">128 M</td>
<td style="text-align: center;">$23.25 \pm 1.07$</td>
<td style="text-align: center;">$20.75 \pm 0.88$</td>
<td style="text-align: center;">$12.14 \pm 0.20$</td>
<td style="text-align: center;">$62.39 \pm 0.84$</td>
<td style="text-align: center;">$12.53 \pm 0.08$</td>
<td style="text-align: center;">$8.36 \pm 0.06$</td>
<td style="text-align: center;">$21.76 \pm 0.17$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A$</td>
<td style="text-align: center;">G2P2 [75]</td>
<td style="text-align: center;">63 M</td>
<td style="text-align: center;">$41.51 \pm 0.78$</td>
<td style="text-align: center;">$51.02 \pm 0.62$</td>
<td style="text-align: center;">$31.92 \pm 0.15$</td>
<td style="text-align: center;">$52.87 \pm 0.78$</td>
<td style="text-align: center;">$22.21 \pm 0.12$</td>
<td style="text-align: center;">$32.52 \pm 0.13$</td>
<td style="text-align: center;">$26.18 \pm 0.25$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A, S$</td>
<td style="text-align: center;">GraphCLIP</td>
<td style="text-align: center;">150 M</td>
<td style="text-align: center;">$\mathbf{6 7 . 3 1 \pm 1 . 7 6}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 1 3 \pm 1 . 1 3}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 1 9 \pm 0 . 1 0}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 0 5 \pm 0 . 3 4}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 4 0 \pm 0 . 6 4}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 0 4 \pm 0 . 2 1}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 8 8 \pm 0 . 3 5}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Zero-shot inference for link prediction of different methods across target datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">WikiCS</th>
<th style="text-align: center;">History</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SBERT [55]</td>
<td style="text-align: center;">$77.76 \pm 0.54$</td>
<td style="text-align: center;">$81.55 \pm 0.09$</td>
<td style="text-align: center;">$87.02 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;">E5 [69]</td>
<td style="text-align: center;">$69.57 \pm 0.60$</td>
<td style="text-align: center;">$80.69 \pm 0.06$</td>
<td style="text-align: center;">$69.93 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;">GRACE [97]</td>
<td style="text-align: center;">$67.19 \pm 0.56$</td>
<td style="text-align: center;">$69.33 \pm 0.06$</td>
<td style="text-align: center;">$76.90 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">GraphMAE [16]</td>
<td style="text-align: center;">$70.17 \pm 0.77$</td>
<td style="text-align: center;">$73.39 \pm 0.04$</td>
<td style="text-align: center;">$83.26 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;">OFA [33]</td>
<td style="text-align: center;">$75.36 \pm 0.89$</td>
<td style="text-align: center;">$89.03 \pm 0.09$</td>
<td style="text-align: center;">$86.47 \pm 0.10$</td>
</tr>
<tr>
<td style="text-align: center;">ZeroG [31]</td>
<td style="text-align: center;">$81.20 \pm 0.82$</td>
<td style="text-align: center;">$88.82 \pm 0.07$</td>
<td style="text-align: center;">$92.85 \pm 0.12$</td>
</tr>
<tr>
<td style="text-align: center;">GraphCLIP</td>
<td style="text-align: center;">$\mathbf{8 3 . 1 5 \pm 0 . 7 6}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 6 7 \pm 0 . 1 0}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 0 4 \pm 0 . 0 5}$</td>
</tr>
</tbody>
</table>
<p>Experimental Setup. We compare our method with classical graph prompt tuning methods for node classification, i.e., GPPT [59], GraphPrompt [37], GPF [8], and All-in-One [60]. We apply each method to our pretrained model and evaluate their performance using $1,3,5$, and 10 shots per class, reporting the mean accuracy.</p>
<p>Analysis. From Figure 3, we observe that GPPT and GraphPrompt fall behind significantly in the 1-shot and 3-shot settings. This underperformance can be attributed to the need for initializing an additional linear head for prediction, preventing direct use of the aligned text model for predictions. However, as the number of shots per class increases, these methods close the performance gap, becoming comparable with others. In contrast, GPF and All-in-One operate directly within the input graph space and can leverage the aligned text model. However, discrepancies between their training objectives and our pretraining objective sometimes result in negative transfer. For instance, in the 1-shot setting, GPF and All-in-One attain accuracy scores of $69.82 \%$ and $69.42 \%$, respectively, which are lower than 0 -shot performance, i.e., $70.19 \%$.</p>
<p>Our proposed prompt tuning method outperforms the other approaches due to its unified training objectives, effectively mitigating
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Node classification of different graph prompt tuning techniques under few-shot setting.
catastrophic forgetting while minimizing the learning cost, leading to superior results.</p>
<h3>4.5 Explorations on Source Datasets (RQ3)</h3>
<p>In this section, we explore the selection of source data for both crossdomain and in-domain transferability. We mask different source domains and evaluate performance on the CiteSeer (Academic), WikiCS (Wikipedia), History (E-commerce), and Instagram (Social) datasets, as depicted in Figure 4. The term 'Full' denotes utilizing all source data as described in Table 1. 'w/o Academia' means excluding academic source datasets, i.e., ogbn-ArXiv, ArXiv_2023, and PubMed. 'w/o E-commerce' is the exclusion of the e-commerce source dataset, while 'w/o Social' means omitting the Reddit dataset.</p>
<p>From Figure 4, we draw several conclusions. First, a greater amount of source data enhances cross-domain transferability; for instance, 'Full' achieves the best performance on the WikiCS dataset. Second, in-domain source data is critical for in-domain transferability, as demonstrated by 'w/o Academia', which significantly lags behind 'Full' on the CiteSeer dataset, and 'w/o E-commerce', which</p>
<p>Table 4: Ablation study of masking different components in GraphCLIP.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\mathcal{G}_{\text {source }}$</th>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">CiteSeer</th>
<th style="text-align: center;">WikiCS</th>
<th style="text-align: center;">Instagram</th>
<th style="text-align: center;">Ele-Photo</th>
<th style="text-align: center;">Ele-Computers</th>
<th style="text-align: center;">Books-History</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$X, A, S$</td>
<td style="text-align: center;">GraphCLIP</td>
<td style="text-align: center;">$67.31 \pm 1.76$</td>
<td style="text-align: center;">$63.13 \pm 1.13$</td>
<td style="text-align: center;">$70.19 \pm 0.10$</td>
<td style="text-align: center;">$64.05 \pm 0.34$</td>
<td style="text-align: center;">$53.40 \pm 0.64$</td>
<td style="text-align: center;">$62.04 \pm 0.21$</td>
<td style="text-align: center;">$53.88 \pm 0.35$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A, T$</td>
<td style="text-align: center;">w/o Summary</td>
<td style="text-align: center;">$58.89 \pm 1.77$</td>
<td style="text-align: center;">$60.67 \pm 1.31$</td>
<td style="text-align: center;">$59.96 \pm 0.12$</td>
<td style="text-align: center;">$59.46 \pm 0.62$</td>
<td style="text-align: center;">$41.65 \pm 0.62$</td>
<td style="text-align: center;">$46.26 \pm 0.50$</td>
<td style="text-align: center;">$44.87 \pm 0.35$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A, S$</td>
<td style="text-align: center;">w/o Freeze</td>
<td style="text-align: center;">$61.29 \pm 1.37$</td>
<td style="text-align: center;">$61.66 \pm 1.01$</td>
<td style="text-align: center;">$58.92 \pm 0.10$</td>
<td style="text-align: center;">$60.15 \pm 0.75$</td>
<td style="text-align: center;">$45.90 \pm 0.65$</td>
<td style="text-align: center;">$53.24 \pm 0.26$</td>
<td style="text-align: center;">$29.40 \pm 0.31$</td>
</tr>
<tr>
<td style="text-align: center;">$X, A, S$</td>
<td style="text-align: center;">w/o IL</td>
<td style="text-align: center;">$65.02 \pm 1.94$</td>
<td style="text-align: center;">$60.09 \pm 1.09$</td>
<td style="text-align: center;">$68.64 \pm 0.09$</td>
<td style="text-align: center;">$61.16 \pm 0.76$</td>
<td style="text-align: center;">$51.51 \pm 0.60$</td>
<td style="text-align: center;">$63.21 \pm 0.38$</td>
<td style="text-align: center;">$45.68 \pm 0.12$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Analyzing the impact of source data on the performance of target datasets.
is inferior to 'Full' on the History dataset. Third, while combining all domains may slightly hurt in-domain transferability, it generally improves overall performance. For example, the performance of 'Full' on the History dataset is slightly lower than 'w/o Academia' but substantially better on the CiteSeer dataset. For simplicity, we use all source data throughout this paper. More complex combinations will be addressed in future work, as balancing the ratio of different source domains remains essential yet challenging.</p>
<p>Table 5: Performance of various model scales on target data. #L, #H denote the number of layers, the hidden size.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Scale</th>
<th style="text-align: center;">#L</th>
<th style="text-align: center;">#H</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">WikiCS</th>
<th style="text-align: center;">Photo</th>
<th style="text-align: center;">Computer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">33 M</td>
<td style="text-align: center;">67.85</td>
<td style="text-align: center;">50.68</td>
<td style="text-align: center;">57.39</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">71 M</td>
<td style="text-align: center;">69.42</td>
<td style="text-align: center;">51.16</td>
<td style="text-align: center;">58.06</td>
</tr>
<tr>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">150 M</td>
<td style="text-align: center;">70.19</td>
<td style="text-align: center;">53.40</td>
<td style="text-align: center;">62.04</td>
</tr>
<tr>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">192 M</td>
<td style="text-align: center;">70.14</td>
<td style="text-align: center;">54.50</td>
<td style="text-align: center;">60.24</td>
</tr>
</tbody>
</table>
<h3>4.6 Analysis on Model Scale (RQ4)</h3>
<p>We explore the impact of model scale in this section. Since the text model remains frozen, our focus is primarily on the scale of the graph model, specifically the graph transformer [54]. We construct four different model scales: Small, Medium, Base and Large. The Small model comprises 4 layers with the hidden size of 512, the Medium model consists of 8 layers with the hidden size of 768 , and the Base model, which is used as our primary model throughout this paper, consists of 12 layers with the hidden size of 1024. The Large model has 16 layers with the hidden size of 1024.</p>
<p>From Table 5, we observe that the Base model consistently outperforms the smaller models, likely due to the increased number of parameters [15]. However, while increasing the number of layers to 16 marginally improves performance, it introduces significant
computational overhead and, in some cases, even hinders performance on certain target datasets. As a result, we adopt the Base model as our primary model throughout this work.</p>
<h3>4.7 Ablation Study (RQ5)</h3>
<p>In this section, we investigate the impact of different components in GraphCLIP by masking them individually. The term 'w/o Summary' indicates using the original text for each node instead of the generated summaries introduced in Sec. 3.1 as text modality input. 'w/o Freeze' means the non-freezing of the text model, and 'w/o IL' is the removal of invariant learning from our pretraining loss.</p>
<p>From Table 4, it is evident that the generated summaries are crucial for achieving zero-shot transferability. The original text, which contains only individual node content, lacks structural information, resulting in a gap between the text and graph modalities. Additionally, original text may include noisy information, leading to suboptimal performance. For example, 'w/o Summary' achieves $41.65 \%$ and $46.26 \%$ on the Photo and Computers datasets, respectively, falling over 10 absolute percentage points behind GraphCLIP. 'w/o Freeze' shows the poorest performance on the WikiCS and History datasets, with scores of $58.92 \%$ and $29.40 \%$, respectively. This suggests that fully tuning the text model may lead to overfitting on the source data, impairing transferability. Last, 'w/o IL' performs worse than GraphCLIP on most datasets, indicating that incorporating invariant learning into the pretraining loss significantly enhances both cross-domain and in-domain transferability.</p>
<h2>5 Conclusion</h2>
<p>In this work, we propose GraphCLIP, a framework for enhancing the transferability of Graph Foundation Models (GFMs) in lowresource scenarios. Our approach addresses this challenge on two levels. On the data level, we generate and curate a novel, largescale dataset of graph-summary pairs. On the algorithmic level, we introduce an innovative contrastive graph-summary pretraining method integrated with invariant learning to boost model transferability. Moreover, we develop a novel graph prompt tuning method that aligns with our pretraining task to mitigate catastrophic forgetting. GraphCLIP consistently outperforms existing approaches in both zero-shot and few-shot learning contexts. These results demonstrate the potential for a highly generalizable GFM that can be efficiently and effectively adapted to real-world scenarios.</p>
<h2>Acknowledgment</h2>
<p>This work was supported by the National Key Research and Development Plan of China (2023YFB4502305), and Ant Group through Ant Research Intern Program. We thank Yinchao Zou and Dr. Shoumeng Yan from Ant Group for providing computational resources for this project.</p>
<h2>References</h2>
<p>[1] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019. Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[2] Ulrik Brandes, Markus Eiglsperger, Ivan Herman, Michael Himsoft, and M Scott Marshall. 2002. GraphML progress report structural layer proposal: Structural layer proposal. In Graph Drawing: 9th International Symposium, GD 2001 Vienna, Austria, September 23-26, 2001 Revised Papers 9. Springer, 501-512.
[3] William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, Jad Kabbara, and Deb Roy. 2023. Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings. arXiv preprint arXiv:2305.14321 (2023).
[4] Runjin Chen, Tong Zhao, AJAY KUMAR JAISWAL, Neil Shah, and Zhangyang Wang. 2024. LLaGA: Large Language and Graph Assistant. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id= 1b0Pxc4oKj
[5] Zhikui Chen, Haitao Mao, Jingzhe Liu, Yu Song, Bingheng Li, Wei Jin, Bahare Fatemi, Anton Tsitsulin, Bryan Perozzi, Hui Liu, et al. 2024. Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights. arXiv preprint arXiv:2406.10727 (2024).
[6] Wei-Lin Chiang, Xuanging Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. 2019. Cluster-grn: An efficient algorithm for training deep and large graph convolutional networks. In Proc. of KDD.
[7] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. [n.d.]. Graph Neural Networks with Learnable Structural and Positional Representations. In International Conference on Learning Representations.
[8] Taoran Fang, Yunchao Zhang, Yang Yang, and Chunping Wang. 2022. Prompt tuning for graph neural networks. arXiv preprint arXiv:2209.15240 (2022).
[9] Jianxi Feng, Hao Liu, Lecheng Kong, Yixin Chen, and Muhan Zhang. 2024. TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models. arXiv preprint arXiv:2406.14683 (2024).
[10] C Lee Giles, Kurt D Bollacker, and Steve Lawrence. 1998. CiteSeer: An automatic citation indexing system. In Proceedings of the third ACM conference on Digital libraries.
[11] Jean-Bastien Grill, Florian Struh, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a new approach to self-supervised learning. Proc. of NeurIPS (2020).
[12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proc. of CVPR.
[13] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION. In Proc. of ICLR.
[14] Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. 2023. Explanations as Features: LLM-Based Features for Text-Attributed Graphs. arXiv preprint arXiv:2305.19523 (2023).
[15] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. Scaling laws for transfer. arXiv preprint arXiv:2102.01293 (2021).
[16] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. 2022. Graphmae: Self-supervised masked graph autoencoders. In Proc. of KDD.
[17] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In Proc. of ICLR.
[18] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for machine learning on graphs. Proc. of NeurIPS (2020).
[19] Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, and Qi Zhu. 2024. Can GNN be Good Adapter for LLMs? (WWW '24). Association for Computing Machinery, New York, NY, USA, 893-904. https: $/ /$ doi.org/10.1145/3589334.3645627
[20] Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou. 2021. Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models. Journal of cheminformatics 13 (2021), 1-25.
[21] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023. Large Language Models on Graphs: A Comprehensive Survey. arXiv preprint arXiv:2312.02783 (2023).
[22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).
[23] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proc. of AACL.
[24] Pramuy Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive
learning. Advances in neural information processing systems 33 (2020), 1866118673.
[25] Minseon Kim, Jihoon Tack, and Sung Ju Hwang. 2020. Adversarial self-supervised contrastive learning. Proc. of NeurIPS (2020).
[26] Seunghse Kim, Jyun-Yu Jiang, Masaki Nakada, Jinyoung Han, and Wei Wang. 2020. Multimodal Post Attentive Profiling for Influencer Marketing. In Proceedings of The 96th Conference 2020 (Taipei, Taiwan) (WWW '20). Association for Computing Machinery, New York, NY, USA, 2878-2884. https://doi.org/10.1145/3366423. 3380052
[27] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In Proc. of ICLR.
[28] Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. 2022. Robust optimization as data augmentation for large-scale graphs. In Proc. of CVPR.
[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning. PMLR, 12888-12900.
[30] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. 2024. A Survey of Graph Meets Large Language Model: Progress and Future Directions. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24, Kate Larson (Ed.). International Joint Conferences on Artificial Intelligence Organization, 8123-8131. https: //doi.org/10.24963/ijcai.2024/898 Survey Track.
[31] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. 2024. ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Barcelona, Spain) (KDD '24). Association for Computing Machinery, New York, NY, USA, 1725-1735. https://doi.org/10.1145/3637328.3671982
[32] Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, and Jia Li. 2024. GLBench: A Comprehensive Benchmark for Graph with Large Language Models. arXiv preprint arXiv:2407.07457 (2024).
[33] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang. 2024. One For All: Towards Training One Graph Model For All Classification Tasks. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=4IT2pgc9v6
[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems 36 (2024).
[35] Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S Yu, et al. 2023. Towards graph foundation models: A survey and beyond. arXiv preprint arXiv:2310.11829 (2023).
[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2022. Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Comput. Surv. (sep 2022). https://doi.org/10.1145/3560815 Just Accepted.
[37] Zemin Liu, Xingfeng Yu, Yuan Fang, and Xinming Zhang. 2023. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In Proceedings of the ACM Web Conference 2023.
[38] Ilya Loshchilov and Frank Hutter. [n.d.]. Decoupled Weight Decay Regularization. In International Conference on Learning Representations.
[39] Zheqi Lv, Feng Wang, Shengyu Zhang, Wenqiao Zhang, Kun Kuang, and Fei Wu. 2023. Parameters Efficient Fine-Tuning for Long-Tailed Sequential Recommendation. In CAAI International Conference on Artificial Intelligence. Springer, $442-459$.
[40] Zheqi Lv, Keming Ye, Zishu Wei, Qi Tian, Shengyu Zhang, Wenqiao Zhang, Wenjie Wang, Kun Kuang, Tat-Seng Chua, and Fei Wu. 2023. Optimize Incompatible Parameters through Compatibility-aware Knowledge Integration. arXiv preprint arXiv:2501.07596 (2025).
[41] Zheqi Lv, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao Zhang, Jiexi Li, Kun Kuang, and Fei Wu. 2025. Collaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation. arXiv preprint arXiv:2501.05647 (2025).
[42] Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, et al. 2023. DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization. In Proceedings of the ACM Web Conference 2023. 3077-3085.
[43] Peter Meroyei and Cătălina Cangea. 2020. Wiki-cx: A wikipedia-based benchmark for graph neural networks. arXiv preprint arXiv:2007.02901 (2020).
[44] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proc. of EMNLP.
[45] Zixuan Ni, Haizhou Shi, Siliang Tang, Longhui Wei, Qi Tian, and Yueting Zhuang. 2021. Revisiting catastrophic forgetting in class incremental learning. arXiv preprint arXiv:2107.12308 (2021).
[46] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[47] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[48] Kaihang Pan, Zhaoyu Fan, Juncheng Li, Qifan Yu, Hao Fei, Siliang Tang, Richang Hong, Hanwang Zhang, and Qianru Sun. 2024. Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration. arXiv preprint arXiv:2409.19872</p>
<p>(2024).
[49] Kailung Pan, Juncheng Li, Hongye Song, Jun Lin, Xiaozhong Liu, and Siliang Tang. 2023. Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. arXiv preprint arXiv:2303.12314 (2023).
[50] Kailung Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tai-Seng Chua, Yueting Zhuang, and Hanwang Zhang. 2024. Auto-Encoding Morph-Tokens for Multimodal LLM. arXiv preprint arXiv:2405.01926 (2024).
[51] Boci Peng, Yongchao Liu, Xiaohr Bo, Sheng Tian, Baokun Wang, Chuntao Hong, and Yan Zhang. 2024. Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense Question Answering. In Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference, ECML-PKDD 2024, Vilnius, Lithuania, September 9-13, 2024, Proceedings, Part VI (Lecture Notes in Computer Science, Vol. 14946). Springer, 39-56.
[52] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohr Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. 2024. Graph Retrieval-Augmented Generation: A Survey. arXiv:2408.08921 [cs.AI] https://arxiv.org/abs/2408.08921
[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748-8763.
[54] Ladislav Rampášek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beami. 2022. Recipe for a General, Powerful, Scalable Graph Transformer. Advances in Neural Information Processing Systems 35 (2022).
[55] Nils Reimers and Iryna Guevrych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT Networks. In Proc. of EMNET.
[56] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine (2008).
[57] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. 2019. Adversarial training for free! Proc. of NeurIPS (2019).
[58] Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Elvahimi, and Hao Wang. 2024. Continual Learning of Large Language Models: A Comprehensive Survey. arXiv preprint arXiv:2404.16789 (2024).
[59] Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. 2022. GPPT: Graph Pre-Training and Prompt Tuning to Generalize Graph Neural Networks. In Proc. of KDD.
[60] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One: Multi-Task Prompting for Graph Neural Networks. In Proc. of KDD.
[61] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. 2021. Adversarial graph augmentation to improve graph contrastive learning. Proc. of NeurIPS (2021).
[62] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. 2024. Graphgpt: Graph instruction tuning for large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 491-500.
[63] Shantanu Thakoor, Cocentin Tufiec, Mohammad Gheshlaghi Azar, Rémi Munos, Petar Veličković, and Michal Valko. 2021. Bootstrapped representation learning on graphs. In ICLR 2021 Workshop on Geometrical and Topological Representation Learning.
[64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).
[65] Raja Vavekanand and Kira Sum. 2024. Llama 3.1: An In-Depth Analysis of the Next-Generation Large Language Model.
[66] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lió, and Yoshua Bengio. 2018. Graph Attention Networks. In Proc. of ICLR.
[67] Petar Veličković, William Fodus, William L. Hamilton, Pietro Lió, Yoshua Bengio, and R. Devon Hjelm. 2019. Deep Graph Infomax. In Proc. of ICLR.
[68] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. 2020. Microsoft academic graph: When experts are not enough. Quantitative Science Studies (2020).
[69] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).
[70] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In Proc. of ICML.
[71] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems 33 (2020), 5776-5788.
[72] Xiaotang Wang, Yun Zhu, Haizhou Shi, Yongchao Liu, and Chuntao Hong. 2024. Graph Triple Attention Network: A Decoupled Perspective. arXiv:2408.07654 [cs.LG] https://arxiv.org/abs/2408.07654
[73] Xiaotang Wang, Yun Zhu, Haizhou Shi, Yongchao Liu, and Chuntao Hong. 2024. UniGAP: A Universal and Adaptive Graph Upsampling Approach to Mitigate</p>
<p>Over-Smoothing in Node Classification Tasks. arXiv:2407.19420 [cs.LG] https: //arxiv.org/abs/2407.19420
[74] Yaoke Wang, Yun Zhu, Wenqiao Zhang, Yueting Zhuang, Liyunfei Liyunfei, and Siliang Tang. 2024. Bridging Local Details and Global Context in Text-Attributed Graphs. In Proc. of EMNET. 14830-14841.
[75] Zhihao Wen and Yuan Fang. 2023. Augmenting low-resource text classification with graph-grounded pre-training and prompting. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 506-516.
[76] Man Wu, Shirui Pan, Chuan Zhou, Xiaojun Chang, and Xingquan Zhu. 2020. Unsupervised domain adaptive graph convolutional networks. In Proc. of WWW.
[77] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Nodeformer: A scalable graph structure learning transformer for node classification. Proc. of NeurIPS (2022).
[78] Ryutaro Yamauchi, Sho Sonoda, Akiyoshi Sannai, and Wataru Kumagai. 2023. Lpml: Lim-prompting markup language for mathematical reasoning. arXiv preprint arXiv:2309.13078 (2023).
[79] Hao Yan, Chaozhuo Li, Ruosong Long, Chao Yan, Jianan Zhao, Wenwen Zhuang, Jun Yin, Peiyan Zhang, Weihao Han, Hao Suo, et al. 2023. A Comprehensive Study on Text-attributed Graphs: Benchmarking and Rethinking. In Proc. of NeurIPS.
[80] An Yang, Ruosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayzheng Liu, Fei Huang, et al. 2024. Qwcn2 technical report. arXiv preprint arXiv:2407.10671 (2024).
[81] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, and Xing Xie. 2021. GraphFormers: GNN-nested transformers for representation learning on textual graph. Proc. of NeurIPS (2021).
[82] Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, and Yi Zhou. 2024. Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement. Knowledge-Based Systems 300 (2024), 112155.
[83] Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, and Yi Zhou. 2024. Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains. arXiv preprint arXiv:2408.10819 (2024).
[84] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting semisupervised learning with graph embeddings. In Proc. of ICML.
[85] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2024. Vision-language models for vision tasks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).
[86] Wenqiao Zhang and Zheqi Lv. 2024. Revisiting the domain shift and sample uncertainty in multi-source active domain transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16751-16761.
[87] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. 2022. Learning on Large-scale Text-attributed Graphs via Variational Inference. In Proc. of ICLR.
[88] Jianan Zhao, Le Zhou, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. 2023. GraphText: Graph Reasoning in Text Space. arXiv:2310.01089 [cs.CL]
[89] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).
[90] Xuyang Zhao, Tianqi Du, Yisen Wang, Jun Yao, and Weiran Huang. 2022. ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations. In Proc. of ICLR.
[91] Xiao Zhou, Yong Lin, Weizhong Zhang, and Tong Zhang. 2022. Sparse invariant risk minimization. In Proc. of ICML.
[92] Yun Zhu, Junhao Guo, and Siliang Tang. 2023. SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. arXiv preprint arXiv:2302.12449 (2023).
[93] Yun Zhu, Junhao Guo, Fei Wu, and Siliang Tang. 2022. RoSA: A Robust SelfAligned Framework for Node-Node Graph Contrastive Learning. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, Lud De Raedt (Ed.). International Joint Conferences on Artificial Intelligence Organization, 3795-3801. https://doi.org/10.24963/ijcai.2022/527 Main Track.
[94] Yun Zhu, Haizhou Shi, Zhenshuo Zhang, and Siliang Tang. 2024. MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. In Proc. of TheWebConf (Singapore, Singapore) (WWW '24). Association for Computing Machinery, 300-311. https://doi.org/10.1145/3589534.3645322
[95] Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. 2024. Efficient Tuning and Inference for Large Language Models on Textual Graphs. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI24, Kate Larson (Ed.). International Joint Conferences on Artificial Intelligence Organization, 5734-5742. https://doi.org/10.24963/ijcai.2024/634 Main Track.
[96] Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Dian Jiao, and Siliang Tang. 2024. GraphControl: Adding Conditional Control to Universal Graph Pretrained Models for Graph Domain Transfer Learning. In Proc. of TheWebConf. $539-550$.
[97] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020. Deep Graph Contrastive Representation Learning. In ICML Workshop on Graph Representation Learning and Beyond.</p>
<h2>A Related Work</h2>
<h2>A. 1 Text-Attributed Graph Methods with LLMs</h2>
<p>Research on TAGs has gained great attention with rapid development of LLMs, classified into three categories [30]: LLM as Enhancer, LLM as Predictor, and LLM as Aligner, as depicted in Figure 1.</p>
<p>LLM as Enhancer [14, 33, 73, 95] involves augmenting raw text or encoding node features, surpassing traditional methods like Bag of Words (BoW). For example, TAPE [14] uses ChatGPT to enhance node attributes, while OFA [33] and ZeroG [31] utilize language models to unify node features and introduce innovative graph prompting techniques to standardize various tasks. LLM as Predictor [4, 41, 62, 82, 83, 88] uses LLMs to predict graph data by converting it into a comprehensible format. GraphText [88] employs a G-Syntax Tree to transform graph data into text sequences, while GraphGPT and LLaGA utilize GNNs to encode graph data into tokens, requiring labeled data for training projector but showing limited transferability [5, 32]. LLM as Aligner [3, 51, 75, 87] maps graph and text modalities into a shared embedding space. GLEM [87] optimizes GNNs and LLMs through iterative training, while ConGrat [3] and G2P2 [75] focus on node-text contrastive pretraining, lacking of graph summary text information.</p>
<p>While TAG methods have achieved significant success, they face two primary challenges: (1) heavy reliance on label information, and (2) poor zero/few-shot transferability. In this work, we propose GraphCLIP framework to address these challenges.</p>
<h2>A. 2 Graph Prompt Tuning</h2>
<p>In low-resource scenarios, the "pretraining, prompt tuning" paradigm [36] has become a standard approach to address overfitting and catastrophic forgetting. In the graph domain, graph prompt tuning has seen notable success. GPPT [59] introduces task tokens and structure tokens to unify pretraining and downstream tasks such as link prediction. Similarly, GraphPrompt [37] unifies tasks as link prediction, enhancing performance through learnable readout prompt functions. SGL-PT [92] focuses on unifying tasks as masked node prediction, while GPF [8] introduces a learnable universal prompt feature into the input feature for downstream task adaptation. Additionally, All-In-One [60] reformulates all tasks at the graph level by introducing a learnable universal graph prompt, which is inserted into the original graph.</p>
<p>Different to previous studies, our proposed prompt tuning method aims to further align graph and text modalities by leveraging downstream (target) labeled data. Meanwhile, our prompt tuning is unified with the pretraining task, effectively mitigating catastrophic forgetting [40, 42, 45, 58, 86] and minimizing learning costs [59, 92] for superior performance.</p>
<h2>B The Dilemma of Vanilla Contrastive Loss</h2>
<p>The representation learned through vanilla contrastive loss lacks domain invariance, which is crucial for enabling a model to effectively generalize across diverse target datasets [90, 94]. In this section, we present a specific scenario that highlights the limitations of vanilla contrastive loss. This example, adapted from ArCL [90], demonstrates how encoders trained via vanilla contrastive learning can
exhibit markedly different behaviors across varying graph domains, denoted as $\mathcal{G}_{\tau}$.</p>
<p>Proposition B.1. In a binary classification setting, let $\left(Z_{1}, Z_{2}\right)$ be normally distributed as $\mathcal{N}\left(0, I_{2}\right)$. Assign $Y=1$ if $Z_{1} \geq 0$. For data augmentation, $Z_{2}$ is scaled by a normal distribution:</p>
<p>$$
\begin{gathered}
\tau_{\alpha}(Z)=\left(Z_{1}, \alpha \cdot Z_{2}\right) \
\alpha \sim \mathcal{N}(0,1)
\end{gathered}
$$</p>
<p>This creates a set of transformation-induced domains $\mathcal{B}=\left{\mathcal{G}<em m="m">{m}\right.$ : $\mathcal{G}</em>}=\left(Z_{1}, m \cdot Z_{2}\right) \mid m \in \mathbb{R}}$. For any $\zeta \geq 0$, there is a representation $g$ and two specific domains, $\mathcal{G<em m_prime="m^{\prime">{m}$ and $\mathcal{G}</em>$, such that:}</p>
<p>$$
\mathcal{L}_{A L}(g ; \mathcal{G}, \pi)&lt;\zeta
$$</p>
<p>However, $g$ demonstrates significantly varied performance across $\mathcal{G}<em m_prime="m^{\prime">{m}$ and $\mathcal{G}</em>$ :}</p>
<p>$$
\left|\mathcal{R}\left(g ; \mathcal{G}<em m_prime="m^{\prime">{m}\right)-\mathcal{R}\left(g ; \mathcal{G}</em>
$$}}\right)\right| \geq \frac{1}{4</p>
<p>where $\mathcal{L}<em A="A" L="L">{A L}$ denotes alignment loss in the vanilla contrastive loss [70], $\mathcal{R}$ denotes supervised risk of binary classification. This demonstrates that even with a low alignment loss, the model's performance can vary significantly across different augmented domains. This variability arises because a low $\mathcal{L}</em>$ is achieved through averaged alignments rather than uniform ones, causing representations in certain less frequently selected domains to experience substantial alignment losses.</p>
<p>Proof. For $\zeta \geq 0$, define $t=\sqrt{\zeta} / 2$ and let $g\left(z_{1}, z_{2}\right)=z_{1}+t z_{2}$. The alignment loss becomes:</p>
<p>$$
\mathcal{L}<em 2="2">{\mathrm{AL}}(g ; \mathcal{G}, \pi)=t^{2} \mathbb{E} Z</em>&lt;\zeta
$$}^{2} \underset{\left(\alpha_{1}, \alpha_{2}\right) \sim \mathcal{N}(0,1)^{2}}{\mathbb{E}}\left(\alpha_{1}-\alpha_{2}\right)^{2}=2 t^{2</p>
<p>Setting $m=0$ and $m^{\prime}=1 / t$, we have:</p>
<p>$$
\mathcal{R}\left(g ; \mathcal{G}_{m}\right)=0
$$</p>
<p>but</p>
<p>$$
\begin{aligned}
&amp; \mathcal{R}\left(g ; \mathcal{G}<em 1="1">{m^{\prime}}\right)= \
&amp; P\left(Z</em>
\end{aligned}
$$}&lt;0, Z_{1}+Z_{2} \geq 0\right)+P\left(Z_{1} \geq 0, Z_{1}+Z_{2} \leq 0\right)=\frac{1}{4</p>
<h2>C Theoretical Analysis of Invariant Alignment Loss</h2>
<p>In this section, we will give the theoretical justification of why using the supremum operator in invariant alignment loss can addresses the dilemma of original alignment loss. Because it can lower the upper bound of variations across different domains. Formal illustrations are as follow:</p>
<p>Theorem C. 1 (Upper Bound on Variation Across Different Domains [90]). Given two augmentation functions $\tau$ and $\tau^{\prime}$, along with a linear predictor $c_{c}$, and representation $g$, the variation across distinct domains is constrained by the following expression:</p>
<p>$$
\sup <em _tau="\tau">{\tau, \tau^{\prime} \in T}\left|\mathcal{R}\left(c \circ g ; \mathcal{G}</em>}\right)-\mathcal{R}\left(c \circ g ; \mathcal{G<em _mathrm_IAL="\mathrm{IAL">{\tau^{\prime}}\right)\right| \leq m \cdot|c| \mathcal{L}</em>)
$$}}(g, \mathcal{G</p>
<p>Additionally, fixing $g$ and defining $c_{\tau} \in \arg \min <em _tau="\tau">{c} \mathcal{R}\left(c \circ g, \mathcal{G}</em>\right)$, we find that:</p>
<p>$$
\begin{gathered}
\left|\mathcal{R}\left(c_{\tau} \circ g ; \mathcal{G}<em _tau_prime="\tau^{\prime">{\tau^{\prime}}\right)-\mathcal{R}\left(c</em>}} \circ g ; \mathcal{G<em _tau="\tau">{\tau^{\prime}}\right)\right| \leq \
2 m \cdot\left(\left|c</em>)
\end{gathered}
$$}\right|+\left|c_{\tau^{\prime}}\right|\right) \mathcal{L}_{\mathrm{IAL}}(g, \mathcal{G</p>
<p>When $\mathcal{L}<em _tau="\tau">{\text {IAL }}$ is minimized to a small value, it signifies that $\mathcal{R}(c \circ g ; \mathcal{G} r)$ remains consistent across various augmentation functions $\tau$, suggesting that the optimal representation for $\mathcal{G}</em>}$ closely resembles that of $\mathcal{G<em _IAL="{IAL" _text="\text">{\tau^{\prime}}$. In other words, representations with smaller $\mathcal{L}</em>$ are more likely to yield similar linear optimal predictors across different domains, a characteristic that the original alignment loss lacks.}</p>
<h2>D Prompt Design</h2>
<p>In this section, we present the prompt templates utilized in this study. Initially, we outline the prompt used for generating graph summaries on source data. Then, we provide the prompt template applied for zero-shot learning of baseline models on target data.</p>
<h2>D. 1 Prompts for Graph Summary Generation</h2>
<p>We present the prompts used for generating graph-summary pair data in Table 6. The violet font represents placeholders, with "seed" to be replaced by the index of the target node within the subgraph. "GraphML" refers to the graph markup language utilized for describing the subgraph, as detailed in Table 3.1. This prompt instructs LLMs to produce both a paper summary and a contextual analysis that encapsulates the essence of the subgraph. We have provided prompts for three source datasets here, as ArXiv_2023 and PubMed are analogous to obgn-ArXiv.</p>
<h2>D. 2 Prompts for Baselines</h2>
<p>Regarding the prompts for generative LLMs and TAG methods that are based on generative LLMs, we utilize the prompt templates specified in their respective original papers [4, 62]. For illustration, we present the Cora dataset as an example in Table 7. The violet font denotes placeholders, with "raw_text" to be substituted with the original text of the paper. Additionally, "<graph>" signifies graph tokens processed by GNNs.</p>
<h2>E Datasets</h2>
<p>In this section, we present a comprehensive overview of the datasets utilized in this paper. The specifics of five source datasets are outlined below:</p>
<p>ArXiv-2023 dataset, featured in TAPE [14], is a directed graph illustrating the citation network of computer science arXiv papers published in 2023 or later. Similar to OGBN-ArXiv, it consists of nodes representing arXiv papers and directed edges that indicate citations. The objective is to classify each paper into one of 40 subject areas, including cs.AI, cs.LG, and cs.OS, with classifications provided by the authors and arXiv moderators.</p>
<p>OGBN-ArXiv dataset represents a directed graph showcasing the citation network among computer science arXiv papers indexed by MAG [68]. Each node signifies an arXiv paper, with directed edges denoting citations. The goal is to categorize papers into one of 40 subject areas such as cs.AI, cs.LG, and cs.OS, with labels manually assigned by authors and arXiv moderators.</p>
<p>PubMed [84] dataset comprises three categories: Experimental studies on diabetes mechanisms and therapies, Type 1 Diabetes research focused on autoimmune processes and treatments, and Type 2 Diabetes studies that emphasize insulin resistance and management strategies. Each category addresses distinct facets of diabetes
research, contributing to the understanding and treatment of this multifaceted disease.</p>
<p>OGBN-Products [18] dataset includes 2 million nodes and 61 million edges, where each node represents an Amazon product, and edges reflect co-purchase relationships. The classification task involves categorizing products into one of 47 top-level categories.</p>
<p>Reddit [19] dataset represents a social network where each node corresponds to a user, with node features comprising the content of users' historically published subreddits, and edges indicating whether two users have responded to each other.</p>
<p>We use the full set of OGBN-ArXiv, ArXiv_2023, PubMed, Reddit datasets and training set of OGBN-Products as pretraining data.</p>
<p>The details of seven target datasets are as follows:
Cora [56] dataset consists of 2,708 scientific publications classified into seven categories: case-based, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory. Each paper in the citation network cites or is cited by at least one other paper, resulting in a total of 5,429 edges.</p>
<p>CiteSeer [10] dataset encompasses 3,186 scientific publications categorized into six domains: Agents, Machine Learning, Information Retrieval, Database, Human-Computer Interaction, and Artificial Intelligence, with the objective of classifying each paper based on its title and abstract.</p>
<p>WikiCS [43] dataset is a Wikipedia-based dataset designed for benchmarking Graph Neural Networks, comprising 10 computer science branches as classes characterized by high connectivity. Node features are extracted from the corresponding article texts ${ }^{2}$.</p>
<p>Instagram [19] dataset reflects a social network where edges represent following relationships, nodes signify users, and the prediction task involves classifying users as either commercial or regular.</p>
<p>Ele-Photo [79] dataset, derived from the Amazon Electronics dataset [44], consists of nodes representing electronic products, with edges denoting frequent co-purchases or co-views. Each node is labeled according to a three-level classification of electronics products. The text attribute for each node comprises the user review with the highest votes, or a randomly selected review if no highlyvoted reviews are available. The task is to classify these products into 12 categories.</p>
<p>Ele-Computer [79] dataset, also extracted from the Amazon Electronics dataset [44], consists of nodes representing electronic products, with edges indicating frequent co-purchases or co-views. Each node is similarly labeled according to a three-level classification of electronics products. The text attribute for each node is the user review with the most votes, or a randomly selected review if no highly-voted reviews exist. The classification task involves categorizing these products into 10 categories.</p>
<p>Books-History [79] dataset is derived from the Amazon-Books dataset, focusing on items labeled as "History." Nodes represent books, while edges indicate frequent co-purchases or co-views between two books. Each node is labeled according to a three-level classification of the book. The title and description of the book itself serve as the text attributes for the nodes. The task is to classify these books into 12 categories.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Prompts for generating graph summary on source data</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ogbn-ArXiv</td>
<td style="text-align: center;">I am providing you with a GraphML file depicting a citation network in computer science. Each node in the network represents a scholarly article, and each edge signifies a citation relationship between articles. Please analyze the article represented by node ' $n$ [seed]' using the provided GraphML data in the following two ways: <br> 1. Paper Summary and Context Analysis: <br> - Extract and summarize the key findings or contributions of the paper denoted by ' $n$ [seed]'. Consider the details embedded within node ' $n$ [seed]', including its title, abstract, and keywords (if available). <br> - Provide an overall summary of prevalent themes or concepts shared by the papers that cite or are cited by ' $n$ [seed]' (its direct neighbors in the network). Identify common threads or research topics among these neighbors. <br> 2. Research Area Classification: <br> - Based on the information summarized from ' $n$ [seed]' and its neighboring nodes, determine the specific research area to which ' $n$ [seed]' primarily contributes. <br> - Justify the classification by explaining which aspects of ' $n$ [seed]' align with recognized themes, issues, or methodologies in the identified research area(s). <br> Please ensure your analyses are grounded in the data provided by the GraphML file within 500 tokens, focusing on node ' $n$ [seed]' and its immediate citation neighborhood. The detailed GraphML citation network data is as follows: [GraphML]</td>
</tr>
<tr>
<td style="text-align: center;">ogbn-Products</td>
<td style="text-align: center;">I have a GraphML file representing an Amazon product co-purchasing network. In this network, nodes represent products sold on Amazon, edges indicate that two products are frequently purchased together. I would like you to analyze the product represented by the node ' $n$ [seed]' using the GraphML data in the following two ways: <br> 1. Product Summary and Context Analysis <br> - Extract and summarize the details of the product denoted by ' $n$ [seed]', including its title and description (if available). <br> - Provide an overall summary of the prevalent themes or trends among the products that co-purchased with ' $n$ [seed]'. Identify common threads or topics shared by these neighboring products. <br> 2. Category Classification <br> - Using the information gathered from ' $n$ [seed]' and its neighboring nodes, classify ' $n$ [seed]' into one of product categories. <br> - Justify the classification by explaining which aspects of ' $n$ [seed]' align with recognized prevalent themes, trends or threads in the identified product category. <br> Your analysis should be directly based on the data provided in the GraphML file and should be limited to 500 tokens. Focus exclusively on node ' $n$ [seed]' and its immediate co-purchased neighborhood. The detailed GraphML co-purchased network data is as follows: <br> [GraphML]</td>
</tr>
<tr>
<td style="text-align: center;">Reddit</td>
<td style="text-align: center;">I have a GraphML file representing a social network where each node denotes a user, the node features are the content of users' historically published subreddits, and edges denote whether two users have replied to each other. I would like you to analyze the user represented by the node ' $n$ [seed]' using the GraphML data in the following two ways: <br> 1. Content Summary and Context Analysis <br> - Extract and summarize the details of the the user's historical post content denoted by ' $n$ [seed]'. Identify and analyze the user's interests based on their historical posts. <br> - Provide an overall summary of the prevalent themes or trends among the users that reply with ' $n$ [seed]'. Identify common topics or interests shared by these users. <br> 2. Category Classification <br> - Using the information gathered from ' $n$ [seed]' and its neighboring nodes, classify whether the user denoted as ' $n$ [seed]' is in the top $50 \%$ popular (average score of all subreddits). <br> - Justify the classification by explaining which aspects of ' $n$ [seed]' align with recognized common topics or interests in the identified user category. <br> Your analysis should be directly based on the data provided in the GraphML file and should be limited to 500 tokens. Focus exclusively on node ' $n$ [seed]' and its immediate neighborhoods which have replied to each other. The detailed GraphML social network data is as follows: <br> [GraphML]</td>
</tr>
</tbody>
</table>
<p>Table 6: Prompts for generating graph-summary pair data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Prompts of baselines for zero-shot learning on target data</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLMs</td>
<td style="text-align: center;">Paper: {raw_text} <br> Task: Please classify this paper into one of following categories: Case_Based, Genetic_Algorithms, Neural_Networks, <br> Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory. Output the answer without any explanations. <br> Answer:</td>
</tr>
<tr>
<td style="text-align: center;">LLaGA</td>
<td style="text-align: center;">Given a node-centered graph: <graph>, each node represents a paper, we need to classify the center node into 7 classes: <br> Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, <br> Theory, please tell me which class the center node belongs to?</td>
</tr>
<tr>
<td style="text-align: center;">GraphGPT</td>
<td style="text-align: center;">Given a citation graph: <graph> where the 0th node is the target paper, and other nodes are its one-hop or multi-hop <br> neighbors, with the following information: <br> Title: {raw_text} <br> Abstract: {raw_text} <br> Question: Classify the target node into one of the following categories: Case_Based, Genetic_Algorithms, <br> Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory. <br> Give the most likely one category of this paper directly.</td>
</tr>
</tbody>
</table>
<p>Table 7: Prompts of baselines for zero-shot learning on target datasets.</p>
<h2>F Baselines</h2>
<p>The details of the baselines are outlined below:
GraphGPT [62] aligns the graph encoder with natural language semantics through text-graph grounding, integrating the trained encoder with a LLM via a projector. This two-stage instruction tuning enhances the model's ability to perform graph tasks using natural language, facilitating zero-shot transferability.</p>
<p>LLaGA [4] employs node-level templates to convert graph data into structured sequences mapped into the token embedding space, enhancing LLM versatility, generalizability, and interpretability when processing graph-structured data.</p>
<p>OFA [33] represents all nodes and edges with human-readable texts, encoding them across various domains into a unified space via LLMs. This framework adapts to diverse tasks by incorporating task-specific prompting substructures into the input graph.</p>
<p>ZeroG [31] uses a language model to encode node attributes and class descriptions, addressing cross-dataset zero-shot transferability challenges in graph learning through prompt-based subgraph sampling and lightweight fine-tuning strategies.</p>
<p>DGI [67] employs a node-graph contrastive method, contrasting node representations with graph representations, where corrupted embeddings and the readout graph representation are treated as negative pairs, while original node representations are considered positive pairs.</p>
<p>GRACE [97] focuses on node-node graph contrastive learning, treating representations from the same original node as positive pairs and others as negative pairs.</p>
<p>BGRL [63] follows a similar approach to GRACE but omits negative samples, drawing inspiration from BYOL [11].</p>
<p>GraphMAE [16] is a masked autoencoder that masks portions of input node attributes before the encoder compresses the masked graph into latent space, with the decoder aiming to reconstruct the masked attributes.</p>
<p>G2P2 [75] proposes graph-grounded pre-training and prompting to boost low-resource text classification.</p>
<h2>G Complexity Analysis</h2>
<p>In this section, we present the time and space complexity analysis of GraphCLIP. Considering that GraphCLIP is a self-supervised learning framework, we will compare its complexity with other self-supervised graph learning methods.</p>
<h2>G. 1 Time Complexity</h2>
<p>The primary time overhead arises from three components: the text model, the graph model, optimizing perturbations, and computing the pretraining loss. For simplicity, we assume the layer count and hidden size of the text model are the same as those of the graph model. The time complexity of the graph model is $O\left(L N^{2} D+\right.$ $\left.L N D^{2}\right)$, and similarly, the time complexity of the text model is $O\left(L N^{2} D+L N D^{2}\right)$. The pretraining loss has a time complexity of $O\left(N^{2} D\right)$. For optimizing perturbations, we run the inner loop M times (set to 3 to approximate the max operation in Equation 9) and accumulate gradients for other parameters in the outer loop. Thus, the total time complexity of GraphCLIP is $O\left(L N^{2} D+L N D^{2}\right)$, ignoring smaller terms, which is of the same order as the classical graph self-supervised method GRACE [97], $O\left(L N^{2} D\right)$.</p>
<h2>G. 2 Space Complexity</h2>
<p>Each layer of the Graph Transformer has a space complexity of $O\left(N D+D^{2}\right)$ for computing queries, keys, and values. The attention score calculation then incurs a space complexity of $O\left(N^{2}+N D\right)$, while obtaining the hidden states results in a perlayer space complexity of $O\left(N^{2}+N D+D^{2}\right)$. Performing these operations across all layers leads to a cumulative space complexity of $O\left(L N^{2}+L N D+L D^{2}\right)$. Similarly, the text model has a space complexity of $O\left(L N^{2}+L N D+L D^{2}\right)$. Finally, the contrastive loss adds an additional space complexity of $O\left(N^{2}\right)$. Consequently, the overall space complexity of GraphCLIP amounts to $O\left(L N^{2}+L N D+L D^{2}\right)$, which is of the same order as the classical graph self-supervised method GRACE, $O\left(L N^{2}+L N D+L D^{2}\right)$.</p>
<h2>H Implementation Details of GraphCLIP</h2>
<p>In this section, we detail the implementation of GraphCLIP. First, we describe the experimental setup for GraphCLIP during pretraining. Next, we present the prompts utilized in zero-shot learning. Finally, we explain the experimental configurations for prompt tuning.</p>
<h2>H. 1 Pretraining Phase</h2>
<p>For GraphCLIP, only a few hyperparameters need to be adjusted. In our main experiments, we utilize the AdamW [38] optimizer with both learning rate and weight decay set to $1 \mathrm{e}-5$. The graph model employed is GraphGPS [54], consisting of 12 layers with a hidden size of 1024 . For the text model, we use a fine-tuned version of MiniLM ${ }^{3}$ [71], featuring 6 layers with a hidden size of 384 . To align both models in a unified subspace, a projector is applied to transform the graph model's 1024 dimensions to match the 384 dimensions of the text model. During pretraining, we optimize only the parameters of the graph model and projector, keeping the text model frozen to reduce training costs and mitigate catastrophic forgetting. Pretraining is conducted over 30 epochs with a batch size of 800 per GPU, utilizing eight A100-40G GPUs for pretraining within 7 hours. We will release our pretrained checkpoint ${ }^{4}$ after the anonymous phase.</p>
<h2>H. 2 Zero-shot Learning</h2>
<p>In zero-shot learning, we incorporate label information into labelspecific sentences to align with the pretraining format. Table 8 presents various prompts designed for different datasets. Placeholders are marked in violet font: "[class]" represents the label text for the target node, and "[class_desc]" is a descriptive sentence generated by LLMs to elaborate on the label. For detailed cases, please refer to our anonymous repository ${ }^{5}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Prompts for zero-shot learning of GraphCLIP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Cora</td>
<td style="text-align: center;">"this paper has a topic on {class} {class_desc}"</td>
</tr>
<tr>
<td style="text-align: center;">CiteSeer</td>
<td style="text-align: center;">"good paper of {class} {class_desc}"</td>
</tr>
<tr>
<td style="text-align: center;">WikiCS</td>
<td style="text-align: center;">"it belongs to {class} research area {class_desc}"</td>
</tr>
<tr>
<td style="text-align: center;">Instagram</td>
<td style="text-align: center;">"[class} {class_desc}"</td>
</tr>
<tr>
<td style="text-align: center;">Ele-Photo</td>
<td style="text-align: center;">"this product belongs to {class} {class_desc}"</td>
</tr>
<tr>
<td style="text-align: center;">Computers</td>
<td style="text-align: center;">"is {class} category {class_desc}"</td>
</tr>
<tr>
<td style="text-align: center;">History</td>
<td style="text-align: center;">"this book belongs to {class} {class_desc}"</td>
</tr>
</tbody>
</table>
<p>Table 8: Prompts for zero-shot learning of GraphCLIP</p>
<h2>H. 3 Graph Prompt Tuning</h2>
<p>During prompt tuning, we utilize the AdamW [38] optimizer, with a learning rate of $1 \times 10^{-4}$ and a weight decay of $1 \times 10^{-5}$. The training is conducted over 100 epochs.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>I Limitations</h2>
<p>In this work, we do not incorporate complex edge attributes, which can be critical for certain graph tasks [18, 20], such as molecule property prediction [18], where each edge may possess distinct properties. Addressing this complexity requires encoding various edge attributes within a unified space and extending Graph Transformers to process these attributes. In future work, we will expand our framework to integrate complex edge information.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
${ }^{4}$ https://github.com/ZhuYun97/GraphCLIP/checkpoints/
${ }^{5}$ https://github.com/ZhuYun97/GraphCLIP/data/load.py&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
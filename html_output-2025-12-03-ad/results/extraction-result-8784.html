<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8784 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8784</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8784</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264405943</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.13023v3.pdf" target="_blank">GraphGPT: Graph Instruction Tuning for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Graph Neural Networks (GNNs) have evolved to understand graph structures through recursive exchanges and aggregations among nodes. To enhance robustness, self-supervised learning (SSL) has become a vital tool for data augmentation. Traditional methods often depend on fine-tuning with task-specific labels, limiting their effectiveness when labeled data is scarce. Our research tackles this by advancing graph model generalization in zero-shot learning environments. Inspired by the success of large language models (LLMs), we aim to create a graph-oriented LLM capable of exceptional generalization across various datasets and tasks without relying on downstream graph data. We introduce the GraphGPT framework, which integrates LLMs with graph structural knowledge through graph instruction tuning. This framework includes a text-graph grounding component to link textual and graph structures and a dual-stage instruction tuning approach with a lightweight graph-text alignment projector. These innovations allow LLMs to comprehend complex graph structures and enhance adaptability across diverse datasets and tasks. Our framework demonstrates superior generalization in both supervised and zero-shot graph learning tasks, surpassing existing benchmarks. The open-sourced model implementation of our GraphGPT is available at https://github.com/HKUDS/GraphGPT.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8784.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8784.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphTokens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph token projection (GraphGPT graph-token representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that maps graph-encoder node/subgraph embeddings into a sequence of learned 'graph tokens' (special token embeddings) via a lightweight alignment projector, then feeds those tokens into an LLM in place of a textual <graph> indicator to represent structure compactly to the language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph token projection / tokenized graph embedding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode graph structure with a pretrained graph encoder to obtain node/subgraph embeddings ƒ§, apply a learned lightweight alignment projector P (e.g., a single linear layer) to ƒ§ to produce projected embeddings X_G that serve as graph-specific token embeddings {<graph_begin>, <graph_token>1,...,<graph_token>n,<graph_end>}, then insert these tokens into the LLM input sequence so the LLM receives a compact tokenized representation of the graph structure rather than a long natural-language serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs / citation graphs / general node-attributed graphs (OGB-arxiv, PubMed, Cora were used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph encoder (message-passing GNN or graph transformer) computes H = f_G(X) and normalizes ƒ§; projector P maps ƒ§ to token embeddings X_G = P(ƒ§). The sequence of graph tokens replaces a <graph> indicator token in the LLM instruction. Subgraphs are constructed by h-hop neighbor sampling with a designated central node, yielding ordered tokens for central node and neighbors.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification and link prediction (evaluated in both supervised and zero-shot transfer settings); also used to create graph-matching instructions for self-supervised tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported numeric results for GraphGPT variants (node classification): GraphGPT-7B-v1.5-std: Arxiv (supervised) accuracy 0.6258, macro-F1 0.2622; Arxiv->PubMed (zero-shot) accuracy 0.7011, macro-F1 0.6491; Arxiv->Cora accuracy 0.1256, macro-F1 0.0819. GraphGPT-7B-v1.5-cot: Arxiv->Cora 0.1813 acc, 0.1272 macro-F1. GraphGPT-7B-v1.1-cot: Arxiv->PubMed acc 0.6103, macro-F1 0.5982. The paper states a 2‚Äì10√ó increase in zero-shot accuracy relative to many GNN baselines overall. Token-efficiency: a 103-node subgraph required ~750 tokens with GraphTokens vs ~4649 tokens when serialized as text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly against text-serialization approaches (text-based structural prompts used with ChatGPT) and standard GNN baselines; GraphTokens were substantially more token-efficient (750 vs 4649 tokens on a 103-node subgraph) and led to better accuracy and zero-shot transfer than (i) text-only LLM baselines (Baichuan, Vicuna, ChatGPT prompts) and (ii) many GNN baselines in zero-shot transfer; ablation shows the token projection + instruction tuning outperforms using LLM without structural input ('w/o GS').</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Very token-efficient compared to natural-language serialization of graphs; preserves structural information via graph encoder embeddings; enables LLM reasoning with structural context while keeping LLM parameters frozen (only projector tuned) for computational efficiency; empirically improves supervised performance and especially zero-shot transfer; reduces GPU memory footprint during tuning (orders-of-magnitude fewer tuned parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on a pretrained graph encoder and a learned projector; may require the two-stage instruction tuning (self-supervised graph-matching then task-specific tuning) to avoid overfitting; sometimes lower performance on very complex tasks unless chain-of-thought distillation is applied; representation is less human-interpretable than explicit text serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Omitting the self-supervised first-stage (graph matching) leads to overfitting and degraded zero-shot performance. For complex multi-class tasks (e.g., Cora with 70 classes) standard instruction data without COT distillation yields mediocre results ‚Äî necessitating chain-of-thought distillation. Tuning the LLM parameters (rather than only the projector) caused OOM in the reported environment, indicating scaling limitations if full LLM tuning is attempted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphGPT: Graph Instruction Tuning for Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8784.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8784.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-Graph Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-graph grounding via contrastive alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive multi-modal alignment that brings graph structural embeddings and node text embeddings into a shared space by computing similarity matrices and optimizing a contrastive cross-entropy loss across three pairwise modalities (graph‚Üîtext, graph‚Üîneighbor-augmented text, text‚Üîneighbor-augmented text).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>contrastive text-graph alignment (text-graph grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use a pretrained graph encoder f_G to produce normalized graph embeddings ƒ§ and a text encoder f_T (e.g., transformer/BERT) to produce normalized text embeddings T; compute similarity matrices Œì1 = ƒ§ T^T, Œì2 = ƒ§ T' ^T (neighbor-aggregated text), Œì3 = T T' ^T scaled by exp(œÑ) and minimize symmetric cross-entropy losses (CE(Œì_i, y) + CE(Œì_i^T, y)) summed over i with weights Œª_i, where y is a contrastive one-hot label per node.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (nodes with textual contents such as paper titles/abstracts) ‚Äî citation graphs used in experiments (OGB-arxiv, PubMed, Cora).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No explicit text serialization; instead aligns continuous graph embeddings and text embeddings so that downstream projector + LLM can operate on graph tokens that are in the LLM-aligned space. Neighbor-aggregated text T' is computed by averaging text embeddings of neighbors for each node.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Enables the two-stage instruction tuning and downstream node classification and link prediction by ensuring graph structural embeddings are compatible with LLM token space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Indirect: contributes to the GraphGPT performance reported (see GraphTokens entry). The paper reports that the self-supervised grounding stage (graph matching + projector initialization) substantially improves zero-shot transfer; ablation ('w/o GS') shows worse results: 'w/o GS' entry: Arxiv supervised accuracy 0.4962 (macro-F1 0.1853) vs full model 0.6258 (0.2622).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared conceptually against naive text-only prompts and prior prompt-based approaches; the paper argues contrastive alignment provides a principled way to align modalities rather than relying on text serialization alone. Ablation shows the alignment stage is necessary for strong zero-shot transfer compared to models without it.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides principled multi-modal alignment so that graph tokens can be mapped into the LLM-compatible space; reduces overfitting and improves zero-shot transfer; can be trained self-supervised using unlabeled graph data (contrastive objective), enabling broad pre-training without downstream labels.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires training/availability of both graph and text encoders and careful tuning of contrastive objective hyperparameters (Œª_i, œÑ); adds pretraining complexity and computational cost prior to instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If omitted or poorly trained, downstream projector and instruction tuning degrade (ablation 'w/o GS' shows poorer supervised and zero-shot performance). Alignment by itself is insufficient for complex reasoning tasks without subsequent COT distillation and task-specific tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphGPT: Graph Instruction Tuning for Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8784.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8784.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-Serialization Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based structural prompts / natural-language serialization of graph structure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approach that converts graph topology and neighbor information into natural-language text and appends it to LLM prompts (e.g., listing neighbors, edge relations, or descriptive sentences) so that an off-the-shelf LLM can be prompted to perform graph tasks without structural token injection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>textual serialization / text-based structural prompting</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert nodes, edges, and neighborhood structure into long natural language descriptions or enumerations (e.g., a list of neighbor node texts, edge lists, or sentences describing relations) and include these strings as additional context in the LLM prompt; sometimes inspired by prior prompt designs in related work [2,5].</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs / citation graphs (used in baselines/case studies such as ChatGPT prompts with node content plus textual graph structure).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Create human-readable lists or sentences that enumerate node attributes and neighbor relationships for the subgraph (e.g., 'Here is the list of neighbor paper titles: ...'), often shuffling or ordering nodes in text; length grows linearly with number of nodes and neighbors and thus can become very long.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification case study with ChatGPT; used in previous works for prompting LLMs on graph-structured tasks without model alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Case study qualitative/quantitative comparison: For a 103-node subgraph the text-based method required ~4649 tokens and produced incorrect classification examples with ChatGPT in Figure 1; overall, LLMs relying on text-only graph descriptions (e.g., ChatGPT) underperformed GraphGPT and often made incorrect predictions on node classification, especially for cross-disciplinary or structurally ambiguous nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly compared to GraphGPT's graph-token approach: text-serialization consumed ~6√ó more tokens (4649 vs 750 for 103-node subgraph) and led to worse predictions in the reported case study; also compared against open-source LLM baselines (Baichuan, Vicuna) which similarly only consume text and show lower structured understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Straightforward to implement, no need to modify or align LLM internals or train a projector; works in a tuning-free manner using off-the-shelf LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Very token-inefficient (large subgraphs produce long prompts), often exceeds token limits of LLMs or becomes computationally and memory expensive; provides weak structural understanding leading to incorrect predictions for many graph tasks and poor zero-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Examples in the paper show text-based prompts lead to incorrect node classification (Figure 1); token explosion for large subgraphs (practical infeasibility); poor zero-shot transfer and inability to leverage structure for reasoning without explicit alignment or supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphGPT: Graph Instruction Tuning for Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking <em>(Rating: 2)</em></li>
                <li>Gpt-gnn: Generative pre-training of graph neural networks <em>(Rating: 1)</em></li>
                <li>GraphPrompt: Unifying pre-training and downstream tasks for graph neural networks <em>(Rating: 2)</em></li>
                <li>GPPT: Graph pre-training and prompt tuning to generalize graph neural networks <em>(Rating: 2)</em></li>
                <li>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs <em>(Rating: 1)</em></li>
                <li>Explanations as Features: LLM-Based Features for Text-Attributed Graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8784",
    "paper_id": "paper-264405943",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "GraphTokens",
            "name_full": "Graph token projection (GraphGPT graph-token representation)",
            "brief_description": "A representation that maps graph-encoder node/subgraph embeddings into a sequence of learned 'graph tokens' (special token embeddings) via a lightweight alignment projector, then feeds those tokens into an LLM in place of a textual &lt;graph&gt; indicator to represent structure compactly to the language model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph token projection / tokenized graph embedding",
            "representation_description": "Encode graph structure with a pretrained graph encoder to obtain node/subgraph embeddings ƒ§, apply a learned lightweight alignment projector P (e.g., a single linear layer) to ƒ§ to produce projected embeddings X_G that serve as graph-specific token embeddings {&lt;graph_begin&gt;, &lt;graph_token&gt;1,...,&lt;graph_token&gt;n,&lt;graph_end&gt;}, then insert these tokens into the LLM input sequence so the LLM receives a compact tokenized representation of the graph structure rather than a long natural-language serialization.",
            "graph_type": "Text-attributed graphs / citation graphs / general node-attributed graphs (OGB-arxiv, PubMed, Cora were used in experiments)",
            "conversion_method": "Graph encoder (message-passing GNN or graph transformer) computes H = f_G(X) and normalizes ƒ§; projector P maps ƒ§ to token embeddings X_G = P(ƒ§). The sequence of graph tokens replaces a &lt;graph&gt; indicator token in the LLM instruction. Subgraphs are constructed by h-hop neighbor sampling with a designated central node, yielding ordered tokens for central node and neighbors.",
            "downstream_task": "Node classification and link prediction (evaluated in both supervised and zero-shot transfer settings); also used to create graph-matching instructions for self-supervised tuning.",
            "performance_metrics": "Reported numeric results for GraphGPT variants (node classification): GraphGPT-7B-v1.5-std: Arxiv (supervised) accuracy 0.6258, macro-F1 0.2622; Arxiv-&gt;PubMed (zero-shot) accuracy 0.7011, macro-F1 0.6491; Arxiv-&gt;Cora accuracy 0.1256, macro-F1 0.0819. GraphGPT-7B-v1.5-cot: Arxiv-&gt;Cora 0.1813 acc, 0.1272 macro-F1. GraphGPT-7B-v1.1-cot: Arxiv-&gt;PubMed acc 0.6103, macro-F1 0.5982. The paper states a 2‚Äì10√ó increase in zero-shot accuracy relative to many GNN baselines overall. Token-efficiency: a 103-node subgraph required ~750 tokens with GraphTokens vs ~4649 tokens when serialized as text.",
            "comparison_to_others": "Compared directly against text-serialization approaches (text-based structural prompts used with ChatGPT) and standard GNN baselines; GraphTokens were substantially more token-efficient (750 vs 4649 tokens on a 103-node subgraph) and led to better accuracy and zero-shot transfer than (i) text-only LLM baselines (Baichuan, Vicuna, ChatGPT prompts) and (ii) many GNN baselines in zero-shot transfer; ablation shows the token projection + instruction tuning outperforms using LLM without structural input ('w/o GS').",
            "advantages": "Very token-efficient compared to natural-language serialization of graphs; preserves structural information via graph encoder embeddings; enables LLM reasoning with structural context while keeping LLM parameters frozen (only projector tuned) for computational efficiency; empirically improves supervised performance and especially zero-shot transfer; reduces GPU memory footprint during tuning (orders-of-magnitude fewer tuned parameters).",
            "disadvantages": "Relies on a pretrained graph encoder and a learned projector; may require the two-stage instruction tuning (self-supervised graph-matching then task-specific tuning) to avoid overfitting; sometimes lower performance on very complex tasks unless chain-of-thought distillation is applied; representation is less human-interpretable than explicit text serialization.",
            "failure_cases": "Omitting the self-supervised first-stage (graph matching) leads to overfitting and degraded zero-shot performance. For complex multi-class tasks (e.g., Cora with 70 classes) standard instruction data without COT distillation yields mediocre results ‚Äî necessitating chain-of-thought distillation. Tuning the LLM parameters (rather than only the projector) caused OOM in the reported environment, indicating scaling limitations if full LLM tuning is attempted.",
            "uuid": "e8784.0",
            "source_info": {
                "paper_title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Text-Graph Grounding",
            "name_full": "Text-graph grounding via contrastive alignment",
            "brief_description": "A contrastive multi-modal alignment that brings graph structural embeddings and node text embeddings into a shared space by computing similarity matrices and optimizing a contrastive cross-entropy loss across three pairwise modalities (graph‚Üîtext, graph‚Üîneighbor-augmented text, text‚Üîneighbor-augmented text).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "contrastive text-graph alignment (text-graph grounding)",
            "representation_description": "Use a pretrained graph encoder f_G to produce normalized graph embeddings ƒ§ and a text encoder f_T (e.g., transformer/BERT) to produce normalized text embeddings T; compute similarity matrices Œì1 = ƒ§ T^T, Œì2 = ƒ§ T' ^T (neighbor-aggregated text), Œì3 = T T' ^T scaled by exp(œÑ) and minimize symmetric cross-entropy losses (CE(Œì_i, y) + CE(Œì_i^T, y)) summed over i with weights Œª_i, where y is a contrastive one-hot label per node.",
            "graph_type": "Text-attributed graphs (nodes with textual contents such as paper titles/abstracts) ‚Äî citation graphs used in experiments (OGB-arxiv, PubMed, Cora).",
            "conversion_method": "No explicit text serialization; instead aligns continuous graph embeddings and text embeddings so that downstream projector + LLM can operate on graph tokens that are in the LLM-aligned space. Neighbor-aggregated text T' is computed by averaging text embeddings of neighbors for each node.",
            "downstream_task": "Enables the two-stage instruction tuning and downstream node classification and link prediction by ensuring graph structural embeddings are compatible with LLM token space.",
            "performance_metrics": "Indirect: contributes to the GraphGPT performance reported (see GraphTokens entry). The paper reports that the self-supervised grounding stage (graph matching + projector initialization) substantially improves zero-shot transfer; ablation ('w/o GS') shows worse results: 'w/o GS' entry: Arxiv supervised accuracy 0.4962 (macro-F1 0.1853) vs full model 0.6258 (0.2622).",
            "comparison_to_others": "Compared conceptually against naive text-only prompts and prior prompt-based approaches; the paper argues contrastive alignment provides a principled way to align modalities rather than relying on text serialization alone. Ablation shows the alignment stage is necessary for strong zero-shot transfer compared to models without it.",
            "advantages": "Provides principled multi-modal alignment so that graph tokens can be mapped into the LLM-compatible space; reduces overfitting and improves zero-shot transfer; can be trained self-supervised using unlabeled graph data (contrastive objective), enabling broad pre-training without downstream labels.",
            "disadvantages": "Requires training/availability of both graph and text encoders and careful tuning of contrastive objective hyperparameters (Œª_i, œÑ); adds pretraining complexity and computational cost prior to instruction tuning.",
            "failure_cases": "If omitted or poorly trained, downstream projector and instruction tuning degrade (ablation 'w/o GS' shows poorer supervised and zero-shot performance). Alignment by itself is insufficient for complex reasoning tasks without subsequent COT distillation and task-specific tuning.",
            "uuid": "e8784.1",
            "source_info": {
                "paper_title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Text-Serialization Prompts",
            "name_full": "Text-based structural prompts / natural-language serialization of graph structure",
            "brief_description": "Prior approach that converts graph topology and neighbor information into natural-language text and appends it to LLM prompts (e.g., listing neighbors, edge relations, or descriptive sentences) so that an off-the-shelf LLM can be prompted to perform graph tasks without structural token injection.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "textual serialization / text-based structural prompting",
            "representation_description": "Convert nodes, edges, and neighborhood structure into long natural language descriptions or enumerations (e.g., a list of neighbor node texts, edge lists, or sentences describing relations) and include these strings as additional context in the LLM prompt; sometimes inspired by prior prompt designs in related work [2,5].",
            "graph_type": "Text-attributed graphs / citation graphs (used in baselines/case studies such as ChatGPT prompts with node content plus textual graph structure).",
            "conversion_method": "Create human-readable lists or sentences that enumerate node attributes and neighbor relationships for the subgraph (e.g., 'Here is the list of neighbor paper titles: ...'), often shuffling or ordering nodes in text; length grows linearly with number of nodes and neighbors and thus can become very long.",
            "downstream_task": "Node classification case study with ChatGPT; used in previous works for prompting LLMs on graph-structured tasks without model alignment.",
            "performance_metrics": "Case study qualitative/quantitative comparison: For a 103-node subgraph the text-based method required ~4649 tokens and produced incorrect classification examples with ChatGPT in Figure 1; overall, LLMs relying on text-only graph descriptions (e.g., ChatGPT) underperformed GraphGPT and often made incorrect predictions on node classification, especially for cross-disciplinary or structurally ambiguous nodes.",
            "comparison_to_others": "Directly compared to GraphGPT's graph-token approach: text-serialization consumed ~6√ó more tokens (4649 vs 750 for 103-node subgraph) and led to worse predictions in the reported case study; also compared against open-source LLM baselines (Baichuan, Vicuna) which similarly only consume text and show lower structured understanding.",
            "advantages": "Straightforward to implement, no need to modify or align LLM internals or train a projector; works in a tuning-free manner using off-the-shelf LLMs.",
            "disadvantages": "Very token-inefficient (large subgraphs produce long prompts), often exceeds token limits of LLMs or becomes computationally and memory expensive; provides weak structural understanding leading to incorrect predictions for many graph tasks and poor zero-shot generalization.",
            "failure_cases": "Examples in the paper show text-based prompts lead to incorrect node classification (Figure 1); token explosion for large subgraphs (practical infeasibility); poor zero-shot transfer and inability to leverage structure for reasoning without explicit alignment or supervision.",
            "uuid": "e8784.2",
            "source_info": {
                "paper_title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Gpt-gnn: Generative pre-training of graph neural networks",
            "rating": 1,
            "sanitized_title": "gptgnn_generative_pretraining_of_graph_neural_networks"
        },
        {
            "paper_title": "GraphPrompt: Unifying pre-training and downstream tasks for graph neural networks",
            "rating": 2,
            "sanitized_title": "graphprompt_unifying_pretraining_and_downstream_tasks_for_graph_neural_networks"
        },
        {
            "paper_title": "GPPT: Graph pre-training and prompt tuning to generalize graph neural networks",
            "rating": 2,
            "sanitized_title": "gppt_graph_pretraining_and_prompt_tuning_to_generalize_graph_neural_networks"
        },
        {
            "paper_title": "Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs",
            "rating": 1,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        },
        {
            "paper_title": "Explanations as Features: LLM-Based Features for Text-Attributed Graphs",
            "rating": 1,
            "sanitized_title": "explanations_as_features_llmbased_features_for_textattributed_graphs"
        }
    ],
    "cost": 0.01349675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GraphGPT: Graph Instruction Tuning for Large Language Models</p>
<p>Jiabin Tang jiabintang77@gmail.com 
Yuhao Yang yuhao-yang@outlook.com 
Wei Wei 
Lei Shi 
Lixin Su sulixinict@gmail.com 
Suqi Cheng chengsuqi@gmail.com 
Dawei Yin yindawei@acm.org 
Chao Huang chaohuang75@gmail.com </p>
<p>University of Hong Kong</p>
<p>University of Hong Kong</p>
<p>University of Hong Kong</p>
<p>Baidu Inc</p>
<p>Baidu Inc</p>
<p>Baidu Inc</p>
<p>Baidu Inc</p>
<p>University of Hong Kong</p>
<p>GraphGPT: Graph Instruction Tuning for Large Language Models
08E05BC20D2AED7F95E5B0AC2B636C0510.1145/3626772.3657775CCS CONCEPTSInformation systems ‚Üí Data miningLanguage models‚Ä¢ Mathematics of computing ‚Üí Graph algorithms Large Language Models, Graph Learning, Instruction Tuning
Graph Neural Networks (GNNs) have evolved to understand graph structures through recursive exchanges and aggregations among nodes.To enhance robustness, self-supervised learning (SSL) has become a vital tool for data augmentation.Traditional methods often depend on fine-tuning with task-specific labels, limiting their effectiveness when labeled data is scarce.Our research tackles this by advancing graph model generalization in zero-shot learning environments.Inspired by the success of large language models (LLMs), we aim to create a graph-oriented LLM capable of exceptional generalization across various datasets and tasks without relying on downstream graph data.We introduce the GraphGPT framework, which integrates LLMs with graph structural knowledge through graph instruction tuning.This framework includes a text-graph grounding component to link textual and graph structures and a dual-stage instruction tuning approach with a lightweight graph-text alignment projector.These innovations allow LLMs to comprehend complex graph structures and enhance adaptability across diverse datasets and tasks.Our framework demonstrates superior generalization in both supervised and zero-shot graph learning tasks, surpassing existing benchmarks.The open-sourced model implementation of our GraphGPT is available at https://github.com/HKUDS/GraphGPT.</p>
<p>INTRODUCTION</p>
<p>Graph neural networks (GNNs) have emerged as a powerful framework for analyzing and learning from graph-structured data [4,27], enabling advancements in various domains, such as social network analysis [31,65], recommender systems [9,42], and biological network analysis [6,25].One of the key benefits of GNNs is their ability to capture the inherent structural information and dependencies present in graph data.By leveraging message passing and aggregation mechanisms, GNNs can effectively propagate and combine information across the graph, enabling them to model complex relationships and make accurate predictions.</p>
<p>In recent years, various GNN architectures have introduced innovations in how information is exchanged and aggregated among graph nodes.For example, graph convolutional network (GCNs) [17,22] adapt convolutional operations to the graph domain, enabling effective feature representations.Graph attention networks (GATs) [39,43] leverages attention mechanisms to assign different weights to neighboring nodes, allowing for more fine-grained information aggregation.Graph transformer networks (GTNs) [14,60] incorporate self-attention and positional encoding to capture global dependencies and structural patterns in the graph.However, a notable limitation of many GNN approaches is their heavy reliance on supervised learning, which can lead to inadequate robustness and generalization when confronted with sparse and noisy data.</p>
<p>To enhance the generalization ability of GNNs, self-supervised learning (SSL) has emerged as a promising approach in graph representation learning.It aims to pre-train a robust graph model using auxiliary tasks on unlabeled graph data.The idea is to leverage the inherent structure and patterns within the graph itself to create meaningful self-supervisory signals.SSL-enhanced graph learning methods exhibit two primary paradigms: contrastive SSL and generative SSL.Within contrastive SSL, the emphasis lies on learning representations by contrasting positive and negative samples, with notable advancements of DGI [40] and GCA [67].Conversely, generative SSL focuses on generating synthetic samples that closely resemble the original graph structures with masked autoencoders, exemplified by techniques like GraphMAE [11] and S2GAE [35].</p>
<p>While these methods aim to generate graph embeddings that are generalizable to different downstream tasks, they often require a fine-tuning process using labels specific to the downstream graph learning scenarios.However, this reliance on labeled data from downstream tasks can restrict their generalization in practical situations where obtaining high-quality labels may not always be feasible.This limitation is particularly relevant in learning scenarios like cold-start recommendation systems or traffic flow prediction in new cities where accurate labels may be scarce or unavailable.</p>
<p>As a result, the objective of this research is to advance the generalization capabilities of graph models by addressing challenging and practical zero-shot learning scenarios.Inspired by the remarkable success of large language models (LLMs) in natural language processing (NLP) tasks [48], where they have demonstrated exceptional generalization abilities, this work aims to develop a graph-oriented LLM capable of achieving high generalization across diverse downstream datasets and tasks.However, effectively integrating large language models with graph learning poses non-trivial challenges.</p>
<p>‚Ä¢ C1: Achieving a proper alignment between the structural information of a graph and the language space demands meticulous deliberation and thoughtful consideration.‚Ä¢ C2: Effectively guiding LLMs to comprehend the structural information of graphs remains a considerable challenge.‚Ä¢ C3: Endowing LLMs with the ability to reason step-by-step is important when tackling complex graph learning tasks.</p>
<p>To gain a deeper understanding of the limitations associated with directly prompting LLMs using purely text-based prompts for graph structure modeling, we provide illustrative examples in Figure 1.These examples facilitate a comparative analysis between our GraphGPT framework and the ChatGPT approach.We focus on a representative node classification task, where the objective is to predict the category of a given paper.In Figure 1 (a) and Figure 1 (b), we showcase the prediction results for two scenarios using ChatGPT: (1) utilizing only the input node textual data, and (2) employing text-based graph structure-aware prompts inspired by the prompt designs in recent studies [2,5].These figures highlight the potential limitations that arise when relying solely on textbased prompts for graph structure modeling, as evidenced by the incorrect paper node classification results presented.In contrast, our GraphGPT framework effectively addresses these limitations by preserving and leveraging the graph structural information, as shown in Figure 1 (c).It enables accurate identification of the paper category, in understanding the underlying graph structure.</p>
<p>Additionally, the utilization of text-based structural prompts leads to an increase in token size, which presents challenges in practical scenarios.Longer token sequences incur higher computational and memory costs, making it less feasible for real-world applications.Furthermore, existing LLMs have token limits, which further restrict the applicability of longer text-based prompts for large-scale graph structure modeling.These limitations emphasize the necessity for more efficient and scalable approaches that can effectively incorporate graph structural information into LLMs.</p>
<p>Contributions.</p>
<p>To address these challenges, we propose a novel framework called GraphGPT, which aims to align Large Language Models (LLMs) with Graphs using a carefully designed graph instruction tuning paradigm.(C1) Our framework introduces a textgraph grounding paradigm as the initial step to align the encoding of graph structures with the natural language space.By incorporating textual information in a contrastive manner, we enable effective alignment of graph structure information within language models.‚Ä¢ This work aims to align graph domain-specific structural knowledge with the reasoning ability of Large Language Models (LLMs) to improve the generalization of graph learning.‚Ä¢ Our approach aims to align LLMs with Graphs through a graph instruction tuning paradigm.This paradigm incorporates selfsupervised instruction tuning, enhancing the LLM's comprehension of graph structural knowledge and its reasoning capabilities.Additionally, we introduce task-specific instruction tuning to improve the model's adaptability across diverse graph tasks.‚Ä¢ We evaluate our proposed GraphGPT on supervised and zeroshot graph learning tasks.We conduct thorough analyses of its component-wise effects and generalization ability.By comparing it with state-of-the-art baselines, we demonstrate the superior generalization power of our approach across various settings.</p>
<p>PRELIMINARIES</p>
<dl>
<dt>Graph-structured Data.represents information as entities (nodes) and the relationships (edges) between them.A graph is denoted as G(V, E, A, X), comprising key components.</dt>
<dd>ùë¢ ‚àà N (ùë£)}), ‚Ñé (ùëô ) ùë£ = Aggregate (ùëô ) (‚Ñé (ùëô ‚àí1) ùë£ , ùëö (ùëô ) ùë£ )(1)
In Graph Neural Networks, the feature vector of node  at layer  is denoted as ‚Ñé ( )</dd>
</dl>
<p>.Message passing is performed by the Propagate ( ) function, aggregating information from neighboring nodes of  in layer .The Aggregate ( ) function combines this information with the previous layer's representation of node  to update ‚Ñé ( )  .By incorporating graph structure into learned representations, GNNs can be tailored for tasks like node classification and link prediction.</p>
<p>METHODOLOGY 3.1 Structural Information Encoding with Text-Graph Grounding</p>
<p>To improve the understanding of graph structural information by large language models, our framework focuses on aligning the encoding of graph structures with the natural language space.This alignment enables language models to effectively comprehend the graph's structural elements using their language understanding capabilities.To achieve this, we introduce a text-graph grounding paradigm that generates prompts preserving the graph's structural context for language models.This paradigm acts as a bridge, connecting the semantic understanding of textual information with the inherent structural relationships in the graph.</p>
<p>In our GraphGPT, we design the graph encoder to be highly flexible, allowing it to leverage a wide range of backbone GNN architectures obtained from diverse graph pre-training paradigms.We incorporate a message-passing neural network architecture, which can be a graph transformer [60] or a graph convolutional network [17], as the structure-level pre-trained graph model.In each message-passing step, the graph encoder aggregates information from neighboring nodes, considering their relationships:
H (ùëô ) = ùúé √ÉH (ùëô ‚àí1) W(2)
The self-loop adjacency matrix, denoted as √É, is obtained by adding the identity matrix I to the original adjacency matrix A. W is the parameter matrix.This matrix captures the self-connections and local connectivity of nodes in the graph. (‚Ä¢) is the non-linear activation.H ( ) is the graph representations at the -th layer.</p>
<p>Text-Structure Alignment.To enhance the alignment of graph structure information with Language Models (LLMs), our focus is on exploring effective encoding methods that can collaborate seamlessly with LLMs.Building upon previous works [30,49], we adopt a contrastive approach by incorporating textual information into the graph structure encoding process.We directly integrate a pre-trained graph encoder into our GraphGPT framework, enabling the seamless utilization of its capabilities.Formally, given a graph G(V, E, A, X) with raw textual contents C =   ‚àà R   √ó , 1 ‚â§  ‚â§  for  nodes, we obtain encoded graph representations ƒ§ ‚àà R  √ó and encoded text representations T ‚àà R  √ó as follows:
H = ùëì G (X), T = ùëì T (C), ƒ§ = norm(H), T = norm(T)(3)
We utilize the graph encoder,  G , to generate structure-level graph representations from the input graph G(V, E, A, X).To encode the raw textual contents C associated with the nodes, we employ a text encoder, such as a transformer or BERT, denoted as  T .This step produces encoded text representations of nodes, which are then normalized row-wise using the norm function.The text-structure alignment across modalities is conducted as follows:
Œì 1 = ( ƒ§ T‚ä§ ) ‚Ä¢ exp(ùúè), Œì 2 = ( ƒ§ T‚Ä≤‚ä§ ) ‚Ä¢ exp(ùúè), Œì 3 = ( T‚ä§ T‚Ä≤‚ä§ ) ‚Ä¢ exp(ùúè) L = 3 ‚àëÔ∏Å ùëñ=1 1 2 ùúÜ ùëñ (CE(Œì ùëñ , y) + CE(Œì ‚ä§ ùëñ , y))(4)
where
T‚Ä≤ = { 1 | N ùëñ | ùëó ‚àà N ùëñ Tùëó , 1 ‚â§ ùëñ ‚â§ ùëÅ }
and  is the number of nodes.In our text-graph grounding, we use the label y = (0, 1, ‚Ä¢ ‚Ä¢ ‚Ä¢ ,  ‚àí 1) ‚ä§ for the contrastive alignment objective.We employ a graph transformer [61] as the graph encoder and a vanilla transformer [38] as the text encoder.</p>
<p>Dual-Stage Graph Instruction Tuning</p>
<p>The dual-stage graph instruction tuning paradigm proposed in this work builds upon the concept of instruction tuning, which has been recently introduced to enhance the adaptability of language models for specific domains [45].In this paradigm, we aim to align the language capacity of the model with the nuances of graph learning tasks, enabling the language model to generate more accurate and contextually appropriate responses for graph-structured data.</p>
<p>3.2.1</p>
<p>Self-Supervised Instruction Tuning.In the initial stage of our graph instruction tuning, we introduce self-supervised instruction tuning.This mechanism enhances the language model's reasoning abilities by incorporating graph domain-specific structural knowledge and effectively understanding contextual information within the graph's structure.To achieve this, we utilize self-supervised signals derived from unlabeled graph structures as instructions for model tuning.Specifically, we design a structureaware graph matching task that guides the language model in differentiating between graph tokens using language tokens.This instruction task plays a vital role in accurately associating graph tokens with their corresponding textual descriptions, deepening the model's comprehension of the graph with the provided guidance.</p>
<p>Instruction Design.The instruction for our graph matching task consists of three components: i) graph information, ii) human question, and iii) GraphGPT response.In this task, we treat each node in the graph as a central node and perform h-hops with random neighbor sampling, resulting in a subgraph structure.The natural language input for the LLM is the human question.In the context of the graph matching task, the instruction includes the indicator</p>
<p>Self-Supervised Instruction Tuning Task-Specific Instruction Tuning</p>
<p>[cls]</p>
<p>[eos]</p>
<p>Alignment Projector</p>
<p>Language Tokens</p>
<p>[Instruct]</p>
<p>[NodeText]</p>
<p>[Graph]</p>
<p>[Instruct]</p>
<p>Text Attributes Cardiovascular complications are the primary‚Ä¶ token <graph> and a shuffled list of node text information.For example, in a citation graph, the node text information corresponds to paper titles.The objective of the LLM in the graph matching task is to align each graph token with its corresponding node text information.This requires reordering the node text information list based on the sequence of graph tokens, effectively associating each graph token with its relevant textual description.The detailed designs of graph matching are shown in Figure 4.</p>
<p>Tuning Strategy.To optimize the tuning process efficiently, we propose incorporating a Lightweight Alignment Projector.During training, we focus on optimizing the parameters of the projector  P , while keeping the parameters of both the LLM and the graph encoder fixed.We assume that the projector successfully learns to map the encoded graph representation to graph tokens, while the LLM excels at aligning these tokens with diverse node text information.To align the graph tokens with the language tokens, we employ a projector  P , which can be as simple as a single linear layer.This projector establishes correspondence between the graph tokens and the language tokens.By replacing the indicator token <graph> in the original language token sequence, the aligned graph tokens create a modified token sequence for the LLM.This modified sequence, denoted as {<graph_begin>, <graph_token> 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , <graph_token>  , <graph_end>}, corresponds to the number of nodes  in the graph associated with the given prompt.Given that the graph matching process is unsupervised, we have the opportunity to leverage a vast amount of unlabeled graph data from different domains, to enhance the generalizability of the learned projector.Mathematically, with projected graph tokens X G =  P ( ƒ§) and text embeddings X I = tokenizer(instruction), for a sequence of length , we compute the probability of generating the target output X O as follows:
ùëù (X O |X G , X I ) = ùêø ùëñ=1 ùëù ùúÉ (ùë• ùëñ |X G , X I,&lt;ùëñ , X O,&lt;ùëñ )(5)
where  are the learnable parameters within GraphGPT.</p>
<p>3.2.2</p>
<p>Task-Specific Instruction Tuning.In the second stage, we introduce task-specific instruction tuning to customize the model's reasoning behavior for different graph learning tasks, such as node classification or link prediction.By fine-tuning the LLM using taskspecific graph instructions, we guide the model to generate responses that align with the constraints and requirements of the specific graph learning task.This enhances the model's adaptability and performance in handling diverse graph learning tasks.Instruction Design.We utilize a consistent instruction template comprising three parts.To generate graph information for each node, we employ the same neighbor sampling approach as in the first stage.This approach ensures the inclusion of relevant graph information, with each node serving as the central node.For the node classification task, the human question instruction includes the indicator token <graph> and specific text information about the central node.This instruction guides the language model to predict the category of the central node based on both the graph structure data and the accompanying text information.Figure 4 provides instruction examples for different tasks, visually illustrating the presentation of the instruction to the language model.</p>
<p>GNN Text Attribute
Transformer
Tuning Strategy.In the second stage of training, we utilize the parameters of the structure-aware projector that were trained in the first stage as the initial state.This allows us to conduct instruction tuning specifically for downstream tasks.During this training process, we keep the parameters of the language model (LLM) and graph encoder fixed, focusing solely on optimizing the parameters of the projector from the previous stage.By doing so, we ensure that the LLM further aligns with the requirements of downstream tasks, enhancing its ability to comprehend and interpret graph structures.</p>
<p>After completing the two training stages as described above, we have confidence that our GraphGPT has acquired the capability to comprehend the given graph structure and perform downstream tasks on the provided graph.The training process involving instruction tuning and the freezing of specific model parameters has refined the model's understanding of graph structures, enabling it to effectively tackle various tasks associated with the given graph.</p>
<p>Chain-of-Thought (CoT) Distillation</p>
<p>When faced with diverse graph data, language models may encounter unfamiliar patterns and structures, leading to challenges in generating accurate and coherent responses.This is especially true when the number of node classes varies across different types of graph data, causing distribution shift.To address this challenge and enhance accuracy in the presence of distribution shift, it is crucial to equip our GraphGPT with step-by-step reasoning abilities.Thus, we propose incorporating the Chain-of-Thought (COT) technique [47], which explicitly models the flow of thoughts and reasoning steps.By leveraging COT, our language model improves the coherence and consistency of generated text, enabling it to follow a logical progression of ideas and enhance its understanding and reasoning capabilities for the given graph data.</p>
<p>Incorporating the Chain-of-Thought (COT) technique can be challenging due to the influence of model parameter scale [32].To overcome this, we draw inspiration from previous research [32] and adopt a distillation approach.By extracting valuable knowledge from a closed-source, powerful language model like ChatGPT (with over 200 billion parameters), we can generate high-quality COT instructions and enhance our model's COT reasoning capabilities without increasing the parameter count.</p>
<p>COT Distillation Paradigm.Our approach involves designing tailored Chain-of-Thought (COT) prompts for node-specific tasks.For the node classification task in a citation graph, we provide the abstract, paper title, and a task description as input.Using the GPT-3.5 language model (LLM), we incorporate "Please think about the categorization in a step-by-step manner." to enable step-by-step reasoning.By engaging in sequential thought, the LLM generates output that includes predictions for node classes and detailed explanations for each prediction.This ensures transparent and comprehensible reasoning and decision-making.To further enhance performance, we integrate the generated COT instruction data with previously designed instructions for task-specific instruction tuning.With the integrated instructions, we proceed with the proposed instruction tuning paradigm.</p>
<p>EVALUATION</p>
<p>We conduct experiments to address key research questions:</p>
<p>‚Ä¢ RQ1: How does the proposed GraphGPT framework perform in both supervised and zero-shot graph learning settings?‚Ä¢ RQ2: What is the generalization ability of our model in handling multiple tasks without experiencing catastrophic forgetting?</p>
<p>‚Ä¢ RQ3: What is the contribution of various key components in the proposed GraphGPT framework to its overall performance?‚Ä¢ RQ4: How scalable and efficient is our GraphGPT framework?</p>
<p>Experimental Settings</p>
<p>4.1.1Data Descriptions.We evaluate our GraphGPT using three datasets: OGB-arxiv, PubMed, and Cora.The OGB-arxiv dataset [12] represents a directed graph capturing the citation network among computer science arXiv papers indexed by MAG [41].Each paper is manually labeled with a research category selected from 40 subject areas.The PubMed dataset [8] consists of 19,717 scientific publications on diabetes from the PubMed database, categorized into Experimental induced diabetes, Type 1 diabetes, and Type 2 diabetes.Additionally, it includes a citation network with 44,338 links.The Cora dataset [49] comprises 25,120 research papers connected through citations.We use an expanded version with 70 classes, larger than previous versions [17].4.1.2Evaluation Protocols.To facilitate comparison across different datasets, we map node features into a unified vector space by encoding raw text information with a pre-trained BERT [3].</p>
<p>In our experiments, we partition the Cora and PubMed datasets into training, validation, and testing sets following a 3:1:1 ratio, as described in previous works [8,49].For the OGB-arxiv data, we adhere to the public split setting [12] with a training, validation, and testing ratio of 6:2:3.To evaluate our model's performance, we utilize three commonly used metrics: Accuracy and Macro F1 for node classification, and AUC for link prediction.</p>
<p>Baseline Methods.</p>
<p>In our performance comparison, we consider various state-of-the-art methods for comprehensive evaluation.(i) The first category includes MLP, which employs a Multilayer Perceptron for node representation.(ii) The second category comprises representative graph neural encoders, including Graph-SAGE [7], GCN [17], GAT [39], and RevGNN [21].(iii) The third category focuses on the self-supervised approach DGI [40] for graph learning.(iv) The fourth category explores knowledge distillationenhanced GNNs, with GKD [55] and GLNN [63] as notable methods.(v).The fifth category showcases recently proposed strong graph transformer networks, with NodeFormer [51] and DIFFormer [50] as competitors.(vi) Lastly, we consider open-sourced LLMs, such as Baichuan-7B, vicuna-7B-v1.1,and vicuna-7B-v1.5 as baselines for understanding text-attributed graph data.4.1.4Implementation Details.For our model implementation, we primarily use the PyTorch and Transformers libraries.We utilize Vicuna-7B-v1.1 and Vicuna-7B-v1.5 as the base models.The batch size is set to 2 per GPU, and the learning rate is 2 ‚àí3 .We apply a warmup ratio of 3 ‚àí2 and set the maximum input length of the Large Language Model (LLM) to 2048.The training process runs for 3 epochs.In the task-specific instruction tuning stage, we explore various combinations of instruction data to assess the model's performance under different data mixtures.The hyperparameter settings remain constant, except for the number of training epochs, which is set to 2 in this stage.The alignment projector parameters fine-tuned in the self-supervised instruction tuning stage serve as the initial parameters for the projector in the second tuning stage.</p>
<p>For evaluating most baselines, we use their publicly available code.For more implementation details, please refer to our released code.</p>
<p>Overall Performance Comparison (RQ1)</p>
<p>We conduct experiments on the node classification task, evaluating both supervised and zero-shot scenarios.The overall performance is summarized in Table 1.In the Supervised Task Setting, models are trained on a specific dataset and evaluated on the corresponding test set (e.g., training on Arxiv-Arxiv and testing on the Arxiv test set).In the Zero-Shot Task Setting, models are trained on a specific dataset and tested on other datasets without additional training (e.g., training on Arxiv-PubMed and testing on the PubMed dataset).</p>
<p>To handle variations in the number of classes across datasets, we employ a transfer-trained classifier, such as a linear layer, when testing GNN-based models.In Table 1, "-7B-" indicates the parameter scale, while "-v1.1-"and "-v1.5-"represent different versions of the base Vicuna model."-stage2" indicates the adoption of only the second stage tuning."-std" and "-cot" denote the use of the standard and generated COT instruction datasets, respectively.</p>
<p>Obs.1: Overall Superiority of our GraphGPT.Our graph LLM consistently outperforms various state-of-the-art baselines in both supervised and zero-shot scenarios.Notably, even recently developed strong GNN-based models, such as NodeFormer, DIFFormer, and GKD, exhibit good structural modeling capabilities in the supervised setting.However, when transferred to new datasets without further training, their performance significantly declines.In contrast, our GraphGPT not only surpasses all state-of-the-art methods in supervised tasks but also achieves a remarkable 2-10 times increase in accuracy in the zero-shot graph learning scenario.LLM-based solutions like Baichuan-7B and Vicuna-7B maintain stable performance across different datasets but rely solely on text information for predictions.In contrast, our GraphGPT preserves graph structure, providing a comprehensive solution for graph learning tasks.Two key factors contribute to these improvements: (i) Our dual-stage graph instruction tuning aligns structural information encoded by the graph encoder with natural language tokens, enabling the LLM to understand the graph's inherent characteristics.(ii) Our framework facilitates mutual enhancement between the graph encoder and LLM, filling the LLM's gap in structural understanding and enabling it to reason about the graph's structure.</p>
<p>Obs.2: Benefits with Structure-aware Graph Matching.The presence of the first stage, which involves self-supervised graph matching tasks for instruction tuning, plays a crucial role in enhancing the zero-shot transferability of our GraphGPT.The first stage focuses on aligning the graph tokens, which encode rich structural information, with the language tokens.This alignment enables the model to develop a deeper understanding of the inherent structural characteristics of the graph data.Without the first stage, if we only conduct the second stage of task-specific instruction tuning, the model tends to be more prone to overfitting on the specific dataset.In such cases, the model's performance may be heavily reliant on dataset-specific patterns and characteristics, rather than a genuine understanding of the underlying graph structure.This can limit the model's ability to generalize to new, unseen datasets.</p>
<p>Obs.3: Benefits with COT Distillation.The "-std" and "-cot" variants indicate that the use of COT distillation substantially benefits more complex graph learning tasks.Models tuned with the standard instruction dataset can already achieve prominent results when transferred to simpler tasks, such as the PubMed dataset with 3 classes, with an accuracy of 0.7011 for Arxiv-PubMed.However,  0.6704 0.6087 Arxiv-std + PubMed-std + Link 0.8246 0.8026 Arxiv-mix + PubMed-mix + Link 0.6451 0.5886 their performance tends to be mediocre when applied to complex tasks like the Cora dataset with 70 classes.By leveraging the powerful reasoning capabilities of the closed-source model (GPT-3.5)through COT distillation, our model can integrate this knowledge and significantly enhance its performance on complex graph tasks.</p>
<p>Generalization Ability Investigation (RQ2)</p>
<p>In this subsection, we explore the generalization ability of our model by incorporating more instruction data to fine-tune the LLM for effectively handling various types of tasks.Our main results and experimental observations are presented as follows:</p>
<p>More Data Boost Model Transfer Ability.In our preliminary investigation, we examine the influence of data quantity on the transfer capability of our GraphGPT, as illustrated in the "(Arxiv + PubMed)-Cora" column of Table 1.In this experiment, we train models using a combination of the Arxiv and PubMed datasets and perform zero-shot testing on the Cora dataset.The results reveal that by incorporating a relatively smaller PubMed dataset (with 20,000+ items) alongside Arxiv, our GraphGPT exhibits a significant improvement in transfer performance on Cora.In contrast, the transfer performance of GNN-based models, trained separately on Arxiv and PubMed, actually deteriorates.</p>
<p>More Data Yet No Forgetting.We further validate the performance of the combined Arxiv and PubMed instruction data on the original Arxiv data, as demonstrated in the "(Arxiv + PubMed)-Arxiv" column in Table 1.The results indicate that most traditional GNN-based approaches experience a significant decline in performance on Arxiv after iterative training.In contrast, our model exhibits improved performance.We attribute this phenomenon to the occurrence of catastrophic forgetting in GNN-based models, where the structural modeling competence of the model trained solely on the smaller PubMed dataset is compromised.However, our model effectively mitigates this issue through our unified graph instruction tuning paradigm.This enables our model to maintain and even enhance its performance by retaining the generalized graph structure patterns despite incorporating additional data.</p>
<p>Generalization for Multitasking Graph Learner.Recent studies on instruction tuning suggest that mixing different instruction tuning data can further enhance the performance of Language and Logic Models (LLMs).In this study, we ensure a consistent number of instruction entries and mix different types of instruction data, including standard instruction ("-std"), COT instruction ("-cot"), a blend of standard (50%) and COT (50%) instruction ("-mix"), and link prediction instruction ("Link").The results are presented in Tables 2 and Table 3.We can observe that effective data mixture solutions can significantly improve the performance of our GraphGPT under various settings.The addition of task-specific instruction for link prediction task notably enhances the performance of our model in node classification.Interestingly, after incorporating node classification, the performance of link prediction also exceeds that of the selected best-performing existing models.After mixing the instructions of different tasks, our model demonstrates the ability to effectively handle various graph learning tasks and transfer its knowledge to other unseen datasets.</p>
<p>Module Ablation Study (RQ3)</p>
<p>We conduct an ablation study to investigate the individual contributions of different sub-modules of our proposed framework, and the results are reported in Table 5.The observations are as follows:</p>
<p>Effect of Graph Instruction Tuning.In our study, we investigate the benefit of incorporating graph structural information into LLM using the variant "w/o GS."In this variant, we directly adopt the base LLM (specifically, Vicuna-7B-v1.5)to perform node classification on three datasets, without incorporating graph structural information.The results of our study demonstrate that our model significantly outperforms the base model that lacks structural information.This indicates that our graph instruction tuning paradigm enables the LLM to understand the graph structural information more effectively.Importantly, this improvement in performance was achieved without altering the original parameters of the LLM.Instead, it was solely accomplished through our lightweight alignment projector, which aligns graph tokens and natural language tokens through the 1-linear projection operation.Effect of LLM-enhanced Semantic Reasoning.We conduct further investigations to assess the influence of the LLM's reasoning ability in our GraphGPT by performing supervised and zero-shot predictions using only the default graph encoders.This variant is denoted as "w/o LR".The results of our study indicate that our GraphGPT, which integrates the LLM, significantly enhances the performance of graph encoder, especially in the zero-shot setting.This suggests that the rich semantic information injected by the LLM provides a substantial gain in performance.</p>
<p>Model Efficiency Study (RQ4)</p>
<p>The study aims to assess the computational efficiency of our model during both the model training and inference stages.</p>
<p>Training Efficiency with Graph Instruction Tuning.Our instruction tuning framework follows a two-stage process where the parameters of both the LLM and the graph encoder are frozen, and only the graph-text alignment projector is tuned.We conduct a comparison between freezing and tuning the LLM parameters in a 4-card 40G Nvidia A100 environment, denoted by "-freeze" and "-tune" respectively.The study analyze the time and space efficiency in terms of training time, the number of tuned parameters, and GPU occupancy (MiB per GPU).Under the same experimental conditions, when tuning LLM parameters, we encounter Out of Memory (OOM) errors even with a batch size of 1.However, by utilizing our tuning strategy, the training process remains stable even with a batch size of 2.Moreover, the number of tuned parameters decreases by more than 50 times compared to the freezing stage.</p>
<p>Model Inference Efficiency.In our exploration, we assess the inference speed and accuracy of our GraphGPT by comparing it with baichuan-7B, vicuna-7B-v1.1,and vicuna-7B-v1.5LLMs.Using a single 40G Nvidia A100, we measure inference time (seconds per response) on the Arxiv and Cora COT instruction datasets, as shown in Figure 5.Our graph LLM demonstrates superior efficiency and accuracy.Lower inference time doesn't necessarily mean better performance: baichuan-7B yields quick but often incorrect or irrelevant answers, while vicuna-7B-v1.1 and vicuna-7B-v1.5require longer, complex reasoning steps for better answers.In contrast, our model achieves accurate predictions through a brief reasoning process, enhancing inference efficiency.</p>
<p>Model Case Study (RQ5)</p>
<p>We conduct a detailed analysis of our model's performance in downstream graph learning tasks compared to traditional LLMs using different types of instructions.We evaluate ChatGPT and our GraphGPT using Arxiv data, with prompts based on node content, node content with text-based graph structure, and our designed graph instruction.The results, shown in Table 6, clearly demonstrate that despite its massive parameter count (over 200B), Chat-GPT struggles to make accurate predictions solely based on node text information or node content with text-based graph structure.This challenge is particularly evident when dealing with papers that have cross-disciplinary characteristics, as seen in the example of research domains in machine learning and hardware architecture.</p>
<p>In contrast, our GraphGPT consistently provides accurate predictions and reasonable explanations.This is because our GraphGPT incorporates a subgraph structure with 103 nodes, allowing it to extract rich structural information from neighboring nodes' citation relationships, leading to accurate predictions.Furthermore, we believe that our approach of using graph tokens to represent the graph structure as input to the LLM is more efficient than the natural language solution.In the case of a 103-node subgraph, our GraphGPT only requires 750 tokens to be fed into the LLM, while the text-based method requires 4649 tokens.This significant reduction in token consumption translates to a substantial decrease in training and inference resource requirements.</p>
<p>RELATED WORK</p>
<p>Self-supervised Learning and Pre-training on Graphs.To enhance the robustness of graph models, self-supervised learning (SSL) has been introduced as a powerful technique [13,16,24].It allows GNNs to learn meaningful graph representations without heavily relying on labeled data.The core idea behind self-supervised learning in graph models is to design pretext tasks that leverage the graph's intrinsic properties to generate additional supervision signals [52].SSL-enhanced graph learning methods can be broadly classified into two main paradigms: contrastive SSL and generative SSL.In particular, i) Contrastive SSL focuses on learning representations by contrasting positive and negative samples.Notable methods in this domain include GraphCL [59] and GCA [67].Recent advancements in contrastive SSL include automated contrastive augmentation (i.e., JOAO [58], AdaGCL [15]), Hyperbolic-Euclidean dual space contrasting (e.g., DSGC [56]), or community-aware contrastive learning (e.g., gCooL [20]).ii) Generative SSL, on the other hand, focuses on generating realistic samples that resemble the original graph structures.Recent advancements in this line include GraphMAE [10,11] for feature masking, and S2GAE [35], AutoCF [53] for reconstructing masked edges as SSL tasks.</p>
<p>Prompt-Tuning for Graph Neural Networks.Recent efforts in enhancing the generalization capability of graph neural networks (GNNs) have focused on training GNN models in a self-supervised manner, followed by fine-tuning on specific downstream tasks using prompt-tuning techniques [64].For example, GPPT [33] is a transfer learning paradigm, where GNNs are first pre-trained on masked edge prediction and then prompted with token pairs for downstream node classification.GraphPrompt [26] aims to handle downstream tasks by integrating pre-training and downstream tasks ChatGPT (Node Content Only): cs.AR, cs.AI, cs.SY, cs.ET, cs.NE.The paper presents a hardware accelerator for deep neural networks (DNNs) that is specifically designed to execute ternary DNNs, which represent weights and activations by signed ternary values.Therefore, the most likely category for this paper is cs.AR (Hardware Architecture), which focuses on the design and analysis of hardware and systems for AI and machine learning applications.The paper also involves the optimization of compute and storage requirements of DNNs, which falls under the category of cs.AI (Artificial Intelligence).Additionally, the paper proposes a programmable, in-memory accelerator, which is a type of embedded system, and therefore, it could be categorized under cs.SY (Systems and Control).Furthermore, the paper discusses the use of ternary values for DNNs, which is an emerging technique to optimize the compute and storage requirements of DNNs, and therefore, it could be categorized under cs.ET (Emerging Technologies).Lastly, the paper proposes a new bit-cell called Ternary Processing Cells (TPCs), which could be categorized under cs.NE (Neural and Evolutionary Computing).The TiM-DNN accelerator is a system-level solution for accelerating deep neural networks, so it could be categorized under systems and control research.5. cs.DC (Distributed, Parallel, and Cluster Computing): While the paper does not explicitly mention distributed or parallel computing, the TiM-DNN accelerator is designed to optimize the compute and storage requirements of deep neural networks, which are often executed on distributed or parallel computing systems.</p>
<p>GraphGPT: Based on the title and abstract of the paper, we can identify the following arXiv CS sub-categories that are most likely to be relevant: ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ 1. cs.LG (Machine Learning): This sub-category is relevant because the paper discusses the use of ternary DNNs for language modeling and image classification, which are both tasks that involve learning.2. cs.AI (Artificial Intelligence): This sub-category is relevant because the paper proposes a new accelerator for DNNs, which is designed to optimize the compute and storage requirements of complex DNNs.3. cs.NE (Numerical Analysis): This sub-category is relevant because the paper discusses the use of ternary DNNs, which involve signed ternary values, and the use of TiM tiles, which are specialized memory arrays that perform massively parallel signed vector-matrix multiplications on ternary values.4. cs.SY (Systems and Control Theory): This sub-category is relevant because the paper discusses the implementation of TiM-DNN in 32nm technology using an architectural simulator calibrated with SPICE simulations and RTL synthesis.5. cs.AR (Hardware Architecture): This sub-category is relevant because the paper proposes a new accelerator for DNNs, which is designed to be programmable and in-memory.into a unified task template.Additionally, Sun et al. [34] presents a unified prompt format, reformulates tasks to the graph level, and incorporates meta-learning techniques to improve multi-task performance in graph prompting.Despite these advances, these methods still require further fine-tuning that relies on supervision labels from downstream tasks to ensure accurate learning.In contrast, this work addresses this limitation by introducing a foundational graph model that tackles the more challenging task of zero-shot graph learning.By eliminating the need for label inputs from downstream tasks, this approach allows for a more general and flexible graph learning paradigm in real-world scenarios.large Language Models.In recent years, LLMs (e.g., ChatGPT [29] and Claude [1]) have gained widespread attention for their remarkable capabilities in various NLP tasks [18,46].Based on these unique capabilities of LLMs, many tuning-free prompting techniques have been explored to enhance their generative abilities, such as incontext learning [28] and Chain-of-Thought [47,57].With the rise of open-source LLMs, such as Llama [36,37], ChatGLM [62], and Baichuan [54], technologies for aligning pre-trained LLMs to different specific tasks and human feedback have been proposed, making private LLMs in specific domains possible [19,44,45].</p>
<p>While there have been successful attempts to align LLMs with visual information, such as multimodal LLMs [23,66], the alignment of LLMs with graph structures remains largely unexplored.This research addresses this gap by introducing a dual-stage graph instruction tuning paradigm that effectively aligns the language capacity of LLMs with graph learning.Previous studies [2,5] have attempted to incorporate graph information into LLMs using natural language, but they have faced challenges in handling complex graph structures and achieving a deep understanding of graphs due to the limitations of relying solely on text-based prompts.</p>
<p>CONCLUSION</p>
<p>This work presents an effective and scalable graph large language model, aims at improving the generalization capabilities of graph models.The proposed framework, GraphGPT, injects graph domainspecific structural knowledge into the LLM through a dual-stage graph instruction tuning paradigm.By leveraging a simple yet effective graph-text alignment projector, we enable LLMs to comprehend and interpret the structural components of graphs.Extensive evaluations across different settings demonstrate the effectiveness of our method in both supervised and zero-shot graph learning scenarios.Furthermore, the model exhibits strong generalization abilities, allowing it to handle diverse downstream datasets and tasks without suffering from catastrophic forgetting.A potential avenue for future investigation is exploring pruning techniques to compress redundant or less important parameters of LLM, thereby reducing the overall model size while preserving its performance.</p>
<p>Figure 1 :
1
Figure 1: Limitation of LLMs in understanding graph structural contexts with heavy reliance on textual data.(C2)In our proposed dual-stage graph instruction tuning paradigm, we leverage self-supervised signals through the graph matching task, which is derived from unlabeled graph structures, to serve as instructions for guiding model tuning of LLMs.By incorporating this self-supervised instruction tuning, the language model acquires domain-specific structural knowledge related to graphs, thereby enhancing its understanding of graph structures.To further customize the LLM's reasoning behavior for diverse downstream graph learning tasks, the second stage of our graph instruction tuning paradigm involves fine-tuning the LLM with task-specific graph instructions, to improve the model's adaptability.(C3) By incorporating the Chain-of-Thought (COT) distillation into our framework, GraphGPT enhances its step-by-step reasoning abilities and improves its performance in the face of distribution shift.In summary, our work makes the following contributions:</p>
<p>Given a sequence of graph tokens <Graph>‚Ä¶ Here is a list of node text: <NodeTexts> Please reorder the list of texts according to the order of graph tokens.Human InstructBased on the information, we obtain the matching as follows: Graph token 1 corresponds to... Graph token 2 corresponds to‚Ä¶ Graph token 3 corresponds to‚Ä¶ LLM Response Given a sequence of graph tokens <Graph>.The first token represents the central node of the subgraph.The remaining represent the first and second order neighbors... <NodeTexts> Which category does this node belong to?Please think in a step-by-step manner and provide your reasoning.Human InstructLLM ResponseTo determine the categorization, we consider the specific topics in the text.First, it involves‚Ä¶ Second, there is evidence that‚Ä¶ Finally, this node is about‚Ä¶, which can be categorized into‚Ä¶</p>
<p>Figure 2 :
2
Figure 2: The overall architecture of our proposed GraphGPT with graph instruction tuning paradigm.</p>
<p>Figure 3 :
3
Figure 3: Workflow of text-structure alignment.</p>
<p>Figure 4 :
4
Figure 4: Our instruction designs for graph matching task (upper), node classification (middle) and link prediction (lower).</p>
<p>Figure 5 :
5
Figure 5: Inference efficiency study of our GraphGPT.</p>
<p>ChatGPT (Node Content and Text-based Graph Structure): Based on the title "TiM-DNN: Ternary In-Memory Accelerator for Deep Neural Networks", the paper is likely to belong to the following arXiv CS sub-categories: 1. cs.AR (Hardware Architecture): The paper proposes a hardware accelerator, TiM-DNN, specifically designed for executing ternary deep neural networks.This falls under the domain of hardware architecture research.2. cs.NE (Neural and Evolutionary Computing): The paper focuses on accelerating deep neural networks, which is a topic within the field of neural and evolutionary computing.3. cs.CV (Computer Vision and Pattern Recognition): Deep neural networks are widely used in computer vision tasks, and the TiM-DNN accelerator could potentially be applied to accelerate computer vision algorithms.4. cs.SY (Systems and Control):</p>
<p>The node set V represents the collection of nodes, with |V | =  indicating the total number of nodes.The edge set E characterizes the relationships between nodes.The adjacency matrix A ‚àà R  √ó encodes the graph's topology, with each element  , indicating the presence or absence of an edge between nodes  and .The feature matrix X ‚àà R  √ó contains attribute or feature information associated with each node, where  represents the feature dimensionality.
Graph Neural Networks. have become a powerful frameworkfor learning representations from graph-structured data. Unliketraditional neural networks that process grid-like data, GNNs excelin capturing the intricate relationships and dependencies withingraphs. They utilize the graph's structure-comprising nodes andedges-to derive expressive node representations through repeatedmessage propagation and aggregation operations.ùëö(ùëô ) ùë£ = Propagate (ùëô ) ({‚Ñé ùë¢ (ùëô ‚àí1)</p>
<p>Table 1 :
1
Performance comparison of various methods on node classification under both supervised and zero-shot settings.
DatasetArxiv-ArxivArxiv-PubMedArxiv-Cora(Arxiv+PubMed)-Cora (Arxiv+PubMed)-ArxivModelAccuracy Macro-F1accMacro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1MLP0.51790.25360.39400.18850.02580.00370.02200.00060.21270.0145GraphSAGE0.54800.32900.39500.19390.03280.01320.01320.00290.12810.0129GCN0.52670.32020.39400.18840.02140.00880.01870.00320.01220.0008GAT0.53320.31180.39400.18840.01670.01100.01610.00570.17070.0285RevGNN0.54740.32400.44400.30460.02720.01010.02170.00160.13090.0126DGI0.50590.27870.39910.19050.02050.00110.02050.00110.50590.2787GKD0.55700.15950.36450.25610.04700.00930.04060.00370.20890.0179GLNN0.60880.37570.42980.31820.02670.01150.01820.00920.33730.1115NodeFormer0.59220.33280.20640.16780.01520.00650.01440.00530.27130.0855DIFFormer0.59860.33550.29590.25030.01610.00940.01000.00070.16370.0234baichuan-7B0.09460.03630.46420.38760.04050.04690.04050.04690.09460.0363vicuna-7B-v1.10.26570.13750.52510.48310.10900.09700.10900.09700.26570.1375vicuna-7B-v1.50.49620.18530.63510.52310.14890.12130.14890.12130.49620.1853GraphGPT-7B-v1.1-cot0.49130.17280.61030.59820.11450.10160.12500.09620.48530.2102GraphGPT-7B-v1.5-stage20.75110.56000.64840.56340.08130.07130.09340.09780.62780.2538GraphGPT-7B-v1.5-std0.62580.26220.70110.64910.12560.08190.15010.09360.63900.2652GraphGPT-7B-v1.5-cot0.57590.22760.52130.48160.18130.12720.16470.13260.64760.2854p-val2.26ùëí ‚àí91.56ùëí ‚àí10 2.22ùëí ‚àí71.55ùëí ‚àí91.04ùëí ‚àí99.96ùëí ‚àí67.62ùëí ‚àí81.97ùëí ‚àí71.5e ‚àí134.63ùëí ‚àí6</p>
<p>Table 2 :
2
Performance comparison of various instruction mixtures in supervised learning on the Arxiv dataset and the zero-shot setting on the Cora dataset for node classification.
DatasetSupervision. on Arxiv Zero Shot on CoraModelAccMacro-F1AccMacro-F1MLP0.51790.25360.02200.0006GraphSAGE0.54800.32900.01320.0029GCN0.52670.32020.01870.0032GAT0.53320.31180.01610.0057RvGNN0.54740.32400.02170.0016DGI0.50590.27870.02050.0011GKD0.55700.15950.04060.0037GLNN0.60880.37570.01820.0092NodeFormer0.59220.33280.01440.0053DIFFormer0.59860.33550.01000.0007baichuan-7b0.09460.03630.04050.0469vicuna-7B-v1.10.26570.13750.10900.0970vicuna-7B-v1.50.49620.18530.14890.1213Arxiv-std + PubMed-std0.63900.26520.15010.0936Arxiv-cot + PubMed-cot0.64760.28540.16470.1326Arxiv-mix + PubMed-mix0.61390.27720.15440.1048Arxiv-std + PubMed-std + Link0.59310.22380.18470.1579Arxiv-mix + Pubmed-mix + Link 0.68740.37610.18360.1494</p>
<p>Table 3 :
3
Performance comparison of various instruction mixtures for link prediction on PubMed.
DatasetPubMedModelAUCAPMLP0.5583 0.5833GAT0.5606 0.6373GraphSAGE0.5041 0.5813RevGNN0.4538 0.5083Node2Vec0.6535 0.6885w/o Link0.5010 0.5005only Link</p>
<p>Table 4 :
4
Module ablation study under both supervised and zero-shot settings to analyze the individual contributions.
DatasetArxiv-ArxivArxiv-PubMedArxiv-CoraVariantAccMac-F1AccMac-F1AccMac-F1w/o GS0.49620.18530.63510.52310.14890.1213w/o LR0.58070.24620.25230.19250.00500.0016ours0.62580.26220.70110.64910.18130.1272</p>
<p>Table 5 :
5
Study on the time and space efficiency of our GraphGPT during both the training and inference stages.
VariantsTraining Time Tuned Parameters GPU OccupyStage-1-tuneOOM6,607,884,288OOMStage-1-freeze 22:53:33131,612,67239517.75improvement-‚Üì √ó 50.21-Stage-2-tuneOOM6,607,884,288OOMStage-2-freeze 03:44:35131,612,67238961.75improvement-‚Üì √ó 50.21-</p>
<p>Table 6 :
6
Comparison of prediction results between our GraphGPT and ChatGPT.TiM-DNN: Ternary in-Memory accelerator for Deep Neural Networks.(with 102 first-order and second-order neighbors) Ground-Truth Category: cs.LG, Machine Learning</p>
<p>Title:</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, CoRR abs/2212.08073Constitutional AI: Harmlessness from AI Feedback. 2022. 2022</p>
<p>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. Zhikai Chen, Haitao Mao, Hang Li, abs/2307.033932023. 2023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT (1). Association for Computational Linguistics2019</p>
<p>EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks. Yushun Dong, Ninghao Liu, Brian Jalaian, WWW. ACM. 2022</p>
<p>GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, CoRR abs/2305.150662023. 2023</p>
<p>Graph-based Molecular Representation Learning. Zhichun Guo, Kehan Guo, Bozhao Nan, Yijun Tian, G Roshni, Iyer, IJCAI. 2023</p>
<p>Inductive Representation Learning on Large Graphs. William L Hamilton, Zhitao Ying, Jure Leskovec, NeurIPS. 2017</p>
<p>Explanations as Features: LLM-Based Features for Text-Attributed Graphs. Xiaoxin He, Xavier Bresson, CoRR abs/2305.195232023. 2023</p>
<p>LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, SIGIR. ACM2020</p>
<p>GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner. Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, 2023</p>
<p>Graphmae: Self-supervised masked graph autoencoders. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Jie Tang, KDD. 2022</p>
<p>Open Graph Benchmark: Datasets for Machine Learning on Graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, NeurIPS. 2020</p>
<p>Gpt-gnn: Generative pre-training of graph neural networks. Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun, KDD. 2020</p>
<p>Heterogeneous Graph Transformer. Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun, WWW. ACM / IW3C22020</p>
<p>Adaptive graph contrastive learning for recommendation. Yangqin Jiang, Chao Huang, Lianghao Huang, KDD. 2023</p>
<p>Hdmi: High-order deep multiplex infomax. Baoyu Jing, Chanyoung Park, Hanghang Tong, WWW. 2021</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, ICLR (Poster). 2017OpenReview.net</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS. 2022</p>
<p>RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, CoRR abs/2309.002672023. 2023</p>
<p>Graph communal contrastive learning. Bolian Li, Baoyu Jing, Hanghang Tong, WWW. 1203-12132022</p>
<p>Training Graph Neural Networks with 1000 Layers. Guohao Li, Matthias M√ºller, Bernard Ghanem, Vladlen Koltun, ICML. 2021</p>
<p>Resource-Efficient Training for Large Graph Convolutional Networks with Label-Centric Cumulative Sampling. Mingkai Lin, Wenzhong Li, Ding Li, Yizhou Chen, Sanglu Lu, WWW. ACM2022</p>
<p>Haotian Liu, Chunyuan Li, Visual Instruction Tuning. 2023</p>
<p>Graph self-supervised learning: A survey. Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, Philip Yu, TKDE. 352022. 2022</p>
<p>Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery. Yunchao Liu, Yu Wang, Oanh Vu, Rocco Moretti, AAAI. 2023</p>
<p>Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Zemin Liu, Xingtong Yu, 2023In WWW</p>
<p>Meta-Weight Graph Neural Network: Push the Limits Beyond Global Homophily. Xiaojun Ma, Qin Chen, WWW. ACM. 2022</p>
<p>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, EMNLP. 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, NeurIPS. 2022</p>
<p>Learning Transferable Visual Models From Natural Language Supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, International Conference on Machine Learning (ICML). PMLR2021</p>
<p>Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting. Zezhi Shao, KDD. ACM2022</p>
<p>Distilling Reasoning Capabilities into Smaller Language Models. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, ACL. 2023</p>
<p>Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. Mingchen Sun, Kaixiong Zhou, KDD. 2022</p>
<p>All in One: Multi-Task Prompting for Graph Neural Networks. Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan, KDD. 2023</p>
<p>S2GAE: Self-Supervised Graph Autoencoders are Generalizable Learners with Graph Masking. Qiaoyu Tan, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, Xia Hu, WSDM. 2023</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth√©e Lachaux, Lacroix, Baptiste Rozi√®re, CoRR abs/2302.139712023. 2023</p>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, CoRR abs/2307.092882023. 2023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, In NeurIPS. 302017</p>
<p>Graph Attention Networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, ICLR (Poster). 2018OpenReview.net</p>
<p>Deep Graph Infomax. Petar Velickovic, William Fedus, William L Hamilton, Pietro Li√≤, ICLR (Poster). 2019OpenReview.net</p>
<p>Microsoft Academic Graph: When experts are not enough. Kuansan Wang, Zhihong Shen, Quant. Sci. Stud. 12020. 2020</p>
<p>Learning Intents behind Interactions with Knowledge Graph for Recommendation. Xiang Wang, Tinglin Huang, Dingxian Wang, WWW. 2021</p>
<p>Heterogeneous Graph Attention Network. Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, WWW. ACM. 2019</p>
<p>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, CoRR abs/2306.047512023. 2023</p>
<p>Self-Instruct: Aligning Language Models with Self-Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, ACL. 2023</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 20222022. 2022</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS</p>
<p>LLMRec: Large Language Models with Graph Augmentation for Recommendation. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang, abs/2311.004232023. 2023</p>
<p>Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting. Zhihao Wen, Yuan Fang, SIGIR. 2023</p>
<p>DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion. Qitian Wu, Chenxiao Yang, 2023In ICLR</p>
<p>NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification. Qitian Wu, Wentao Zhao, CoRR abs/2306.083852023. 2023</p>
<p>Simgrace: A simple framework for graph contrastive learning without data augmentation. Jun Xia, Lirong Wu, Jintao Chen, 2022</p>
<p>Automated Self-Supervised Learning for Recommendation. Lianghao Xia, Chao Huang, Tao Yu, Ben Kao, WWW. 2023</p>
<p>Baichuan 2: Open Large-scale Language Models. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, CoRR abs/2309.103052023. 2023</p>
<p>Geometric Knowledge Distillation: Topology Compression for Graph Neural Networks. Chenxiao Yang, Qitian Wu, Junchi Yan, 2022In NeurIPS</p>
<p>Dual space graph contrastive learning. Haoran Yang, Hongxu Chen, Shirui Pan, Lin Li, Philip S Yu, Guandong Xu, WWW. 1238-12472022</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, CoRR abs/2305.106012023. 2023</p>
<p>Graph contrastive learning automated. Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang, ICML. PMLR. 2021</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, In NeurIPS. 332020</p>
<p>Graph Transformer Networks. Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, Hyunwoo J Kim, NeurIPS. 11960-119702019</p>
<p>Graph transformer networks. Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, Hyunwoo J Kim, In NeurIPS. 322019</p>
<p>GLM-130B: An Open Bilingual Pre-trained Model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, ICLR. 2023</p>
<p>Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation. Shichang Zhang, Yozen Liu, Yizhou Sun, Neil Shah, ICLR. 2022</p>
<p>Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer. Wen Zhang, Yushan Zhu, Mingyang Chen, WWW. 2581-25902023</p>
<p>Robust Self-Supervised Structural Graph Neural Network for Social Network Prediction. Yanfu Zhang, WWW. ACM. 2022</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. 2023. 2023arXiv preprint</p>
<p>Graph contrastive learning with adaptive augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, WWW. 2069-20802021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4408 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4408</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4408</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-281886200</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.05138v1.pdf" target="_blank">LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of scientific publications has made it increasingly difficult to keep literature reviews comprehensive and up-to-date. Though prior work has focused on automating retrieval and screening, the writing phase of systematic reviews remains largely under-explored, especially with regard to readability and factual accuracy. To address this, we present LiRA (Literature Review Agents), a multi-agent collaborative workflow which emulates the human literature review process. LiRA utilizes specialized agents for content outlining, subsection writing, editing, and reviewing, producing cohesive and comprehensive review articles. Evaluated on SciReviewGen and a proprietary ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey and MASS-Survey in writing and citation quality, while maintaining competitive similarity to human-written reviews. We further evaluate LiRA in real-world scenarios using document retrieval and assess its robustness to reviewer model variation. Our findings highlight the potential of agentic LLM workflows, even without domain-specific tuning, to improve the reliability and usability of automated scientific writing.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4408.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4408.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literature Review Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based multi-agent pipeline that decomposes literature-review writing into specialized agents (outline drafter, subsection writer, editor, reviewer, optional researcher) to produce readable, citation-grounded systematic reviews without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LiRA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LiRA is a modular, agentic workflow implemented with LangGraph that emulates human SLR writing: (1) Outline Drafter constructs candidate outlines from up to 50 references; (2) Subsection Writer generates each subsection in parallel conditioned on a section description and a retrieved subset of references (FAISS index used for retrieval; per-subsection cap = 25% of pool, min 3, max 150); (3) Editor refines style and cohesion without changing factual content and handles continuation for very long outputs; (4) Reviewer evaluates components against systematic-review criteria and triggers up to three regeneration loops; (optional) Researcher agent can analyze abstracts into findings for downstream agents. Agents return structured documents, use agent memory, and employ Zero-Shot Chain-of-Thought in prompts for most agents. Citation grounding uses full article titles as semantic anchors and post-processes to numbered references.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o-mini (used as the primary LLM across experiments); gemma3:4b evaluated as an alternative reviewer model in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>embedding-based retrieval (FAISS) for section-level retrieval of titles/abstracts/full-text; per-reference abstract analysis by optional researcher agent; structured-agent prompting to extract key points</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>multi-agent hierarchical synthesis: outline-driven decomposition, parallel subsection synthesis, editor-level global refinement, reviewer-guided iterative regeneration; citation-grounded claim-to-reference alignment</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed to process large reference pools; outline uses up to 50 references, subsection retrieval caps at 25% of the total pool (min 3, max 150); evaluated on reviews with ~70 references on average</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science (SciReviewGen) and multiple domains via ScienceDirect (23 subject areas including business, microbiology, materials science)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured long-form literature reviews (title, abstract, sections/subsections, conclusion) with in-text citations and reference lists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-L, heading soft recall (hsr), heading/article entity recall (her/aer), Citation Quality F1 (CQF1), Prometheus LLM-based writing quality scores (coverage, structure, relevance), SME human annotation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On SciReviewGen: ROUGE-L 0.13, hsr 0.82, her 0.10, aer 0.27, CQF1 0.76; Writing quality average (Prometheus) ≈ 4.11 (coverage/structure/relevance components reported). On ScienceDirect: ROUGE-L 0.13, CQF1 0.73; article token output ≈ 22,000 tokens on average.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct prompting (single-pass LLM), MASS-Survey (MASS), AutoSurvey (AS)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LiRA outperforms baselines on most metrics: e.g., ROUGE-L 0.13 vs AutoSurvey 0.09 and MASS 0.09 (SciReviewGen); CQF1 0.76 vs AutoSurvey ≤0.63 and MASS 0.13 (SciReviewGen); writing-quality averages higher (LiRA 4.11 vs AutoSurvey 3.78). LiRA produces more concise, coherent reviews (22k tokens vs AutoSurvey 50k tokens on average).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposing SLR writing into specialized LLM agents with explicit outline-first planning, citation-grounded generation (title anchors), parallel subsection drafting, and reviewer-driven refinement yields better structural coherence and citation reliability while remaining concise; citation grounding and reviewer loops reduce hallucination; system is robust to swapping reviewer LLMs and to retrieval-based references.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Remaining hallucination risk (mitigated but not eliminated), dependence on proprietary LLM (gpt-4o-mini) causing reproducibility concerns, lack of seedable model runs, limited open-source datasets for end-to-end evaluation across domains, computational cost for long-form generation and multi-agent orchestration; optional researcher and editor agents had marginal or sometimes negative impacts in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Handles large contexts (tested with 128,000-token context window); outline limited to 50 refs but subsection retrieval allows up to 150 per subsection; AutoSurvey's verbosity demonstrated that longer outputs bias recall metrics; LiRA remained robust when using retrieved references (retrieval setting) with only minor metric differences; swapping reviewer to gemma3:4b had little impact on metrics, suggesting moderate robustness to model choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4408.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4408.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage LLM framework for automatically generating literature surveys that retrieves relevant publications, constructs an outline from titles/abstracts, drafts subsections with refinement, and partially uses retrieved article content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoSurvey: Large Language Models Can Automatically Write Surveys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoSurvey is a staged pipeline: given a query it retrieves relevant publications, uses titles and abstracts to construct an outline, then drafts subsections with refinement steps. The released code supports partial use of retrieved article content (up to ~1,500 tokens) during drafting. In this paper, AutoSurvey was adapted to limit retrieval to the target review's references and to generalize prompts for multi-domain use.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented with gpt-4o-mini for fair comparison in this study (original paper may use different LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>retrieval over titles/abstracts (embedding similarity), partial content usage (up to 1,500 tokens) in drafting</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>outline construction from clusters of retrieved documents followed by subsection drafting and refinement; largely sequential multi-stage summarization</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Varies by retrieval; in original design can retrieve many candidates per subsection; in this paper modified to 2–25% of references per subsection or fallback to 60 documents when fraction exceeded threshold</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Originally computer science; used here across multiple domains matching evaluation datasets</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form literature surveys (outlines and subsections); tends toward very long outputs (reported ~50,000 tokens per article on average in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics as LiRA: ROUGE-L, hsr/her/aer, CQF1, Prometheus writing scores, SME evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In these experiments (with gpt-4o-mini): SciReviewGen ROUGE-L ≈ 0.09, CQF1 ≤ 0.63, produced ~50,000 tokens per article on average; writing quality average ≈ 3.78.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to LiRA, MASS, and direct prompting baselines in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AutoSurvey achieved higher heading/entity recall in some metrics (attributable to verbosity) but lower ROUGE and lower citation-quality than LiRA; AutoSurvey had higher coverage scores but worse structure/readability compared to LiRA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sequential multi-stage retrieval-and-draft pipelines can cover more documents (higher recall) by producing much longer outputs, but verbosity can harm structure and readability and inflate recall-based metrics; retrieval grounding in code was limited/unclear in the released implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Very long outputs bias recall metrics; partial content usage (1,500 tokens) limits grounding; in released code retrieval-grounded drafting functionality may be incomplete; higher hallucination and lower citation reliability compared to LiRA in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Produces much longer outputs as it scales retrieval (observed average ~50k tokens), which increases coverage but reduces conciseness and harms structure; recall metrics benefit from longer outputs without length normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4408.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4408.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MASS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MASS-Survey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic framework (seeded by a survey-writing challenge) that follows a strictly sequential pipeline: cluster references by topical similarity to build an outline, generate section contents directly from clusters, then append a conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Overview of the NLPCC2024 Shared Task 6: Scientific Literature Survey Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MASS-Survey (MASS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MASS first clusters references by topical similarity to construct an outline, then generates section contents and a title directly from those clusters, and finally appends a conclusion. Long reference lists are passed as attachments in the workflow. The pipeline is sequential rather than decomposed into specialized roles with iterative feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented with gpt-4o-mini in this study for fair comparison (original challenge implementations may vary)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>clustering of references by topical similarity (likely embedding-based) to group documents used to build outline and inform generation; attachment of long reference lists for model access</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>cluster-driven section generation (sequential aggregation of cluster content into sections) without specialized editor/reviewer loops</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Depends on the review; in experiments used the full reference lists (on average ~70 references in sampled reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Presented as a survey-writing system for scientific literature; evaluated here on SciReviewGen and ScienceDirect datasets</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature survey articles (outline, sections, conclusion)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-L, hsr/her/aer, CQF1, Prometheus writing scores, SME annotations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In experiments with gpt-4o-mini: SciReviewGen ROUGE-L ≈ 0.09, CQF1 ≈ 0.13, writing-quality average ≈ 3.83 (per Table 2), variable hsr/her/aer values lower than AutoSurvey on some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a baseline vs LiRA, AutoSurvey, and direct prompting in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MASS generally underperformed LiRA on ROUGE and CQF1; tended to be weaker in structure and citation reliability in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A strictly sequential cluster-then-generate pipeline can construct outlines from topical clusters but lacks the iterative refinement and specialized editing/review that improve citation fidelity and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Sequential design lacks specialized reviewer/editor loops, lower citation quality (CQF1), and less emphasis on readability; passing long lists as attachments can exceed context limits and harm grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scales with the number of references via clustering, but no evidence of refinement mechanisms to maintain coherence or citation fidelity as size grows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4408.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4408.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct prompting (DP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that uses a single prompt to an LLM with task instructions and the full set of reference titles/metadata to generate a review in one pass, possibly using file attachments for long reference lists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Direct Prompting (DP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DP directly prompts an LLM with instructions plus the references (titles/metadata) asking it to produce a review with specified sections and length. If reference lists exceed context window, they're provided as attached files requiring model file-reading capability. No decomposition, iterative refinement, or specialized agents are used.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o-mini in this study's baseline experiments</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>single-pass full-context prompting with attached reference lists when necessary; no structured retrieval or per-section selection</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>single-step generation (monolithic synthesis) without intermediate outline/review cycles</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Uses the provided full set of references for a target review (on average ~70 references in sampled reviews); attachments used when beyond context limits</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied to same datasets as other systems (SciReviewGen, ScienceDirect) in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Full literature review generated in one pass</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-L, hsr/her/aer, CQF1, Prometheus writing scores, SME annotation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Lower performance overall in this study: SciReviewGen ROUGE-L ≈ 0.06, CQF1 ≈ 0.14, lower writing-quality scores relative to agentic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as the simplest baseline for comparison to agentic frameworks (LiRA, AutoSurvey, MASS)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Direct prompting underperformed agentic approaches (LiRA, AutoSurvey) on structural coherence, citation quality, and ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A single-pass LLM prompt is insufficient to reliably produce coherent, citation-grounded long-form literature reviews compared to decomposed, iterative agentic pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prone to hallucination, limited ability to control structure and citation grounding, sensitive to context-window constraints when many references are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance degrades when the number of references or length exceeds context limits; reliance on attachments requires file-reading capability and may limit grounding fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4408.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4408.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open multi-agent programming framework for coordinating specialized agents (originally applied to software development) which LiRA adapts for inter-agent communication and structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT (adapted design aspects)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MetaGPT is a multi-agent collaborative framework designed to coordinate specialized agent roles and their communications; LiRA adapts aspects of MetaGPT's design to require agents to return structured documents and to use shared message pools for efficient inter-agent communication.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper for MetaGPT (MetaGPT is cited as prior art); LiRA implements agents with gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>MetaGPT itself is an agent orchestration framework rather than a specific extraction method; used as inspiration for structured agent outputs and shared messaging</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agent-coordination patterns enabling modular task decomposition and structured result aggregation (conceptual inspiration used in LiRA)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General multi-agent coordination framework (original work applied to programming tasks); cited here for architectural adaptations</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Framework for orchestrating agent outputs (not a specific literature-synthesis output in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MetaGPT's multi-agent design patterns are useful for enforcing structured outputs and efficient inter-agent communication in LLM agent systems like LiRA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper only borrows design aspects; MetaGPT's original evaluation context differs from long-form literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not evaluated here; MetaGPT cited as a scalable agent coordination inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4408.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4408.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where agents store feedback in persistent memory and adapt behaviors based on that feedback; LiRA compares its agent memory practice to Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reflexion (conceptual inspiration)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reflexion introduces persistent memory and verbal reinforcement learning for LLM agents to iteratively adapt. LiRA uses per-agent memory for storing feedback and adjusting behavior, which the paper notes is comparable to Reflexion's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified here for Reflexion; cited for memory/feedback design patterns</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Memory-backed iterative refinement (conceptual); not directly used for extraction in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative behavior adaptation via stored feedback leading to improved subsequent outputs (cited as design parallel)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM agent behavior and learning; cited for architectural practice in LiRA</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Persistent agent memory is a common design pattern for multi-agent LLM workflows to incorporate feedback; LiRA implements per-agent memories similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reflexion focuses on reinforcement learning-style adaptation; LiRA uses memory for feedback but does not implement full RL.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not directly reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4408.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4408.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qi2025 MAS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generation of Scientific Literature Surveys Based on Large Language Models (LLM) and Multi-Agent Systems (MAS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work proposing use of LLMs with multi-agent systems to generate scientific literature surveys; cited as related work on automating survey writing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation of Scientific Literature Surveys Based on Large Language Models (LLM) and Multi-Agent Systems (MAS)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM + MAS survey approach (Qi et al. 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The referenced work proposes combining large language models with multi-agent system architectures to automatically generate literature surveys; LiRA positions itself as building upon and integrating concepts from related multi-agent survey-writing works including this one.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified within this paper for that work (citation only); treated as prior LLM+MAS literature</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Likely retrieval and agent-directed extraction (paper cites it as related agentic literature-review work), but this paper only references it without implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-agent synthesis (as suggested by the title); LiRA claims to build upon concepts from such works</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated literature survey generation (general scientific literature)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature surveys (as per title)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as one of the few prior works addressing LLM-based literature-review writing and multi-agent approaches; LiRA claims to extend these ideas with explicit readability and factuality mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>This paper does not provide internal details; Qi et al. referenced as prior art only.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AutoSurvey: Large Language Models Can Automatically Write Surveys <em>(Rating: 2)</em></li>
                <li>Generation of Scientific Literature Surveys Based on Large Language Models (LLM) and Multi-Agent Systems (MAS) <em>(Rating: 2)</em></li>
                <li>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework <em>(Rating: 2)</em></li>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation <em>(Rating: 2)</em></li>
                <li>Overview of the NLPCC2024 Shared Task 6: Scientific Literature Survey Generation <em>(Rating: 2)</em></li>
                <li>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4408",
    "paper_id": "paper-281886200",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "LiRA",
            "name_full": "Literature Review Agents",
            "brief_description": "An LLM-based multi-agent pipeline that decomposes literature-review writing into specialized agents (outline drafter, subsection writer, editor, reviewer, optional researcher) to produce readable, citation-grounded systematic reviews without task-specific fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LiRA",
            "system_description": "LiRA is a modular, agentic workflow implemented with LangGraph that emulates human SLR writing: (1) Outline Drafter constructs candidate outlines from up to 50 references; (2) Subsection Writer generates each subsection in parallel conditioned on a section description and a retrieved subset of references (FAISS index used for retrieval; per-subsection cap = 25% of pool, min 3, max 150); (3) Editor refines style and cohesion without changing factual content and handles continuation for very long outputs; (4) Reviewer evaluates components against systematic-review criteria and triggers up to three regeneration loops; (optional) Researcher agent can analyze abstracts into findings for downstream agents. Agents return structured documents, use agent memory, and employ Zero-Shot Chain-of-Thought in prompts for most agents. Citation grounding uses full article titles as semantic anchors and post-processes to numbered references.",
            "llm_model_used": "gpt-4o-mini (used as the primary LLM across experiments); gemma3:4b evaluated as an alternative reviewer model in ablations",
            "extraction_technique": "embedding-based retrieval (FAISS) for section-level retrieval of titles/abstracts/full-text; per-reference abstract analysis by optional researcher agent; structured-agent prompting to extract key points",
            "synthesis_technique": "multi-agent hierarchical synthesis: outline-driven decomposition, parallel subsection synthesis, editor-level global refinement, reviewer-guided iterative regeneration; citation-grounded claim-to-reference alignment",
            "number_of_papers": "Designed to process large reference pools; outline uses up to 50 references, subsection retrieval caps at 25% of the total pool (min 3, max 150); evaluated on reviews with ~70 references on average",
            "domain_or_topic": "Computer science (SciReviewGen) and multiple domains via ScienceDirect (23 subject areas including business, microbiology, materials science)",
            "output_type": "Structured long-form literature reviews (title, abstract, sections/subsections, conclusion) with in-text citations and reference lists",
            "evaluation_metrics": "ROUGE-L, heading soft recall (hsr), heading/article entity recall (her/aer), Citation Quality F1 (CQF1), Prometheus LLM-based writing quality scores (coverage, structure, relevance), SME human annotation",
            "performance_results": "On SciReviewGen: ROUGE-L 0.13, hsr 0.82, her 0.10, aer 0.27, CQF1 0.76; Writing quality average (Prometheus) ≈ 4.11 (coverage/structure/relevance components reported). On ScienceDirect: ROUGE-L 0.13, CQF1 0.73; article token output ≈ 22,000 tokens on average.",
            "comparison_baseline": "Direct prompting (single-pass LLM), MASS-Survey (MASS), AutoSurvey (AS)",
            "performance_vs_baseline": "LiRA outperforms baselines on most metrics: e.g., ROUGE-L 0.13 vs AutoSurvey 0.09 and MASS 0.09 (SciReviewGen); CQF1 0.76 vs AutoSurvey ≤0.63 and MASS 0.13 (SciReviewGen); writing-quality averages higher (LiRA 4.11 vs AutoSurvey 3.78). LiRA produces more concise, coherent reviews (22k tokens vs AutoSurvey 50k tokens on average).",
            "key_findings": "Decomposing SLR writing into specialized LLM agents with explicit outline-first planning, citation-grounded generation (title anchors), parallel subsection drafting, and reviewer-driven refinement yields better structural coherence and citation reliability while remaining concise; citation grounding and reviewer loops reduce hallucination; system is robust to swapping reviewer LLMs and to retrieval-based references.",
            "limitations_challenges": "Remaining hallucination risk (mitigated but not eliminated), dependence on proprietary LLM (gpt-4o-mini) causing reproducibility concerns, lack of seedable model runs, limited open-source datasets for end-to-end evaluation across domains, computational cost for long-form generation and multi-agent orchestration; optional researcher and editor agents had marginal or sometimes negative impacts in ablations.",
            "scaling_behavior": "Handles large contexts (tested with 128,000-token context window); outline limited to 50 refs but subsection retrieval allows up to 150 per subsection; AutoSurvey's verbosity demonstrated that longer outputs bias recall metrics; LiRA remained robust when using retrieved references (retrieval setting) with only minor metric differences; swapping reviewer to gemma3:4b had little impact on metrics, suggesting moderate robustness to model choice.",
            "uuid": "e4408.0",
            "source_info": {
                "paper_title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey",
            "brief_description": "A multi-stage LLM framework for automatically generating literature surveys that retrieves relevant publications, constructs an outline from titles/abstracts, drafts subsections with refinement, and partially uses retrieved article content.",
            "citation_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
            "mention_or_use": "use",
            "system_name": "AutoSurvey",
            "system_description": "AutoSurvey is a staged pipeline: given a query it retrieves relevant publications, uses titles and abstracts to construct an outline, then drafts subsections with refinement steps. The released code supports partial use of retrieved article content (up to ~1,500 tokens) during drafting. In this paper, AutoSurvey was adapted to limit retrieval to the target review's references and to generalize prompts for multi-domain use.",
            "llm_model_used": "Implemented with gpt-4o-mini for fair comparison in this study (original paper may use different LLMs)",
            "extraction_technique": "retrieval over titles/abstracts (embedding similarity), partial content usage (up to 1,500 tokens) in drafting",
            "synthesis_technique": "outline construction from clusters of retrieved documents followed by subsection drafting and refinement; largely sequential multi-stage summarization",
            "number_of_papers": "Varies by retrieval; in original design can retrieve many candidates per subsection; in this paper modified to 2–25% of references per subsection or fallback to 60 documents when fraction exceeded threshold",
            "domain_or_topic": "Originally computer science; used here across multiple domains matching evaluation datasets",
            "output_type": "Long-form literature surveys (outlines and subsections); tends toward very long outputs (reported ~50,000 tokens per article on average in experiments)",
            "evaluation_metrics": "Same metrics as LiRA: ROUGE-L, hsr/her/aer, CQF1, Prometheus writing scores, SME evaluations",
            "performance_results": "In these experiments (with gpt-4o-mini): SciReviewGen ROUGE-L ≈ 0.09, CQF1 ≤ 0.63, produced ~50,000 tokens per article on average; writing quality average ≈ 3.78.",
            "comparison_baseline": "Compared to LiRA, MASS, and direct prompting baselines in this paper",
            "performance_vs_baseline": "AutoSurvey achieved higher heading/entity recall in some metrics (attributable to verbosity) but lower ROUGE and lower citation-quality than LiRA; AutoSurvey had higher coverage scores but worse structure/readability compared to LiRA.",
            "key_findings": "Sequential multi-stage retrieval-and-draft pipelines can cover more documents (higher recall) by producing much longer outputs, but verbosity can harm structure and readability and inflate recall-based metrics; retrieval grounding in code was limited/unclear in the released implementation.",
            "limitations_challenges": "Very long outputs bias recall metrics; partial content usage (1,500 tokens) limits grounding; in released code retrieval-grounded drafting functionality may be incomplete; higher hallucination and lower citation reliability compared to LiRA in experiments.",
            "scaling_behavior": "Produces much longer outputs as it scales retrieval (observed average ~50k tokens), which increases coverage but reduces conciseness and harms structure; recall metrics benefit from longer outputs without length normalization.",
            "uuid": "e4408.1",
            "source_info": {
                "paper_title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "MASS",
            "name_full": "MASS-Survey",
            "brief_description": "An agentic framework (seeded by a survey-writing challenge) that follows a strictly sequential pipeline: cluster references by topical similarity to build an outline, generate section contents directly from clusters, then append a conclusion.",
            "citation_title": "Overview of the NLPCC2024 Shared Task 6: Scientific Literature Survey Generation",
            "mention_or_use": "use",
            "system_name": "MASS-Survey (MASS)",
            "system_description": "MASS first clusters references by topical similarity to construct an outline, then generates section contents and a title directly from those clusters, and finally appends a conclusion. Long reference lists are passed as attachments in the workflow. The pipeline is sequential rather than decomposed into specialized roles with iterative feedback.",
            "llm_model_used": "Implemented with gpt-4o-mini in this study for fair comparison (original challenge implementations may vary)",
            "extraction_technique": "clustering of references by topical similarity (likely embedding-based) to group documents used to build outline and inform generation; attachment of long reference lists for model access",
            "synthesis_technique": "cluster-driven section generation (sequential aggregation of cluster content into sections) without specialized editor/reviewer loops",
            "number_of_papers": "Depends on the review; in experiments used the full reference lists (on average ~70 references in sampled reviews)",
            "domain_or_topic": "Presented as a survey-writing system for scientific literature; evaluated here on SciReviewGen and ScienceDirect datasets",
            "output_type": "Literature survey articles (outline, sections, conclusion)",
            "evaluation_metrics": "ROUGE-L, hsr/her/aer, CQF1, Prometheus writing scores, SME annotations",
            "performance_results": "In experiments with gpt-4o-mini: SciReviewGen ROUGE-L ≈ 0.09, CQF1 ≈ 0.13, writing-quality average ≈ 3.83 (per Table 2), variable hsr/her/aer values lower than AutoSurvey on some metrics.",
            "comparison_baseline": "Used as a baseline vs LiRA, AutoSurvey, and direct prompting in this paper",
            "performance_vs_baseline": "MASS generally underperformed LiRA on ROUGE and CQF1; tended to be weaker in structure and citation reliability in these experiments.",
            "key_findings": "A strictly sequential cluster-then-generate pipeline can construct outlines from topical clusters but lacks the iterative refinement and specialized editing/review that improve citation fidelity and coherence.",
            "limitations_challenges": "Sequential design lacks specialized reviewer/editor loops, lower citation quality (CQF1), and less emphasis on readability; passing long lists as attachments can exceed context limits and harm grounding.",
            "scaling_behavior": "Scales with the number of references via clustering, but no evidence of refinement mechanisms to maintain coherence or citation fidelity as size grows.",
            "uuid": "e4408.2",
            "source_info": {
                "paper_title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Direct Prompting",
            "name_full": "Direct prompting (DP)",
            "brief_description": "A baseline approach that uses a single prompt to an LLM with task instructions and the full set of reference titles/metadata to generate a review in one pass, possibly using file attachments for long reference lists.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Direct Prompting (DP)",
            "system_description": "DP directly prompts an LLM with instructions plus the references (titles/metadata) asking it to produce a review with specified sections and length. If reference lists exceed context window, they're provided as attached files requiring model file-reading capability. No decomposition, iterative refinement, or specialized agents are used.",
            "llm_model_used": "gpt-4o-mini in this study's baseline experiments",
            "extraction_technique": "single-pass full-context prompting with attached reference lists when necessary; no structured retrieval or per-section selection",
            "synthesis_technique": "single-step generation (monolithic synthesis) without intermediate outline/review cycles",
            "number_of_papers": "Uses the provided full set of references for a target review (on average ~70 references in sampled reviews); attachments used when beyond context limits",
            "domain_or_topic": "Applied to same datasets as other systems (SciReviewGen, ScienceDirect) in experiments",
            "output_type": "Full literature review generated in one pass",
            "evaluation_metrics": "ROUGE-L, hsr/her/aer, CQF1, Prometheus writing scores, SME annotation",
            "performance_results": "Lower performance overall in this study: SciReviewGen ROUGE-L ≈ 0.06, CQF1 ≈ 0.14, lower writing-quality scores relative to agentic systems.",
            "comparison_baseline": "Used as the simplest baseline for comparison to agentic frameworks (LiRA, AutoSurvey, MASS)",
            "performance_vs_baseline": "Direct prompting underperformed agentic approaches (LiRA, AutoSurvey) on structural coherence, citation quality, and ROUGE.",
            "key_findings": "A single-pass LLM prompt is insufficient to reliably produce coherent, citation-grounded long-form literature reviews compared to decomposed, iterative agentic pipelines.",
            "limitations_challenges": "Prone to hallucination, limited ability to control structure and citation grounding, sensitive to context-window constraints when many references are provided.",
            "scaling_behavior": "Performance degrades when the number of references or length exceeds context limits; reliance on attachments requires file-reading capability and may limit grounding fidelity.",
            "uuid": "e4408.3",
            "source_info": {
                "paper_title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "brief_description": "An open multi-agent programming framework for coordinating specialized agents (originally applied to software development) which LiRA adapts for inter-agent communication and structured outputs.",
            "citation_title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "mention_or_use": "mention",
            "system_name": "MetaGPT (adapted design aspects)",
            "system_description": "MetaGPT is a multi-agent collaborative framework designed to coordinate specialized agent roles and their communications; LiRA adapts aspects of MetaGPT's design to require agents to return structured documents and to use shared message pools for efficient inter-agent communication.",
            "llm_model_used": "Not specified in this paper for MetaGPT (MetaGPT is cited as prior art); LiRA implements agents with gpt-4o-mini",
            "extraction_technique": "MetaGPT itself is an agent orchestration framework rather than a specific extraction method; used as inspiration for structured agent outputs and shared messaging",
            "synthesis_technique": "Agent-coordination patterns enabling modular task decomposition and structured result aggregation (conceptual inspiration used in LiRA)",
            "number_of_papers": "",
            "domain_or_topic": "General multi-agent coordination framework (original work applied to programming tasks); cited here for architectural adaptations",
            "output_type": "Framework for orchestrating agent outputs (not a specific literature-synthesis output in this paper)",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "MetaGPT's multi-agent design patterns are useful for enforcing structured outputs and efficient inter-agent communication in LLM agent systems like LiRA.",
            "limitations_challenges": "Paper only borrows design aspects; MetaGPT's original evaluation context differs from long-form literature synthesis.",
            "scaling_behavior": "Not evaluated here; MetaGPT cited as a scalable agent coordination inspiration.",
            "uuid": "e4408.4",
            "source_info": {
                "paper_title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "brief_description": "An approach where agents store feedback in persistent memory and adapt behaviors based on that feedback; LiRA compares its agent memory practice to Reflexion.",
            "citation_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "mention_or_use": "mention",
            "system_name": "Reflexion (conceptual inspiration)",
            "system_description": "Reflexion introduces persistent memory and verbal reinforcement learning for LLM agents to iteratively adapt. LiRA uses per-agent memory for storing feedback and adjusting behavior, which the paper notes is comparable to Reflexion's approach.",
            "llm_model_used": "Not specified here for Reflexion; cited for memory/feedback design patterns",
            "extraction_technique": "Memory-backed iterative refinement (conceptual); not directly used for extraction in this paper",
            "synthesis_technique": "Iterative behavior adaptation via stored feedback leading to improved subsequent outputs (cited as design parallel)",
            "number_of_papers": "",
            "domain_or_topic": "General LLM agent behavior and learning; cited for architectural practice in LiRA",
            "output_type": "",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Persistent agent memory is a common design pattern for multi-agent LLM workflows to incorporate feedback; LiRA implements per-agent memories similarly.",
            "limitations_challenges": "Reflexion focuses on reinforcement learning-style adaptation; LiRA uses memory for feedback but does not implement full RL.",
            "scaling_behavior": "Not directly reported here.",
            "uuid": "e4408.5",
            "source_info": {
                "paper_title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Qi2025 MAS",
            "name_full": "Generation of Scientific Literature Surveys Based on Large Language Models (LLM) and Multi-Agent Systems (MAS)",
            "brief_description": "A referenced prior work proposing use of LLMs with multi-agent systems to generate scientific literature surveys; cited as related work on automating survey writing.",
            "citation_title": "Generation of Scientific Literature Surveys Based on Large Language Models (LLM) and Multi-Agent Systems (MAS)",
            "mention_or_use": "mention",
            "system_name": "LLM + MAS survey approach (Qi et al. 2025)",
            "system_description": "The referenced work proposes combining large language models with multi-agent system architectures to automatically generate literature surveys; LiRA positions itself as building upon and integrating concepts from related multi-agent survey-writing works including this one.",
            "llm_model_used": "Not specified within this paper for that work (citation only); treated as prior LLM+MAS literature",
            "extraction_technique": "Likely retrieval and agent-directed extraction (paper cites it as related agentic literature-review work), but this paper only references it without implementation details",
            "synthesis_technique": "Multi-agent synthesis (as suggested by the title); LiRA claims to build upon concepts from such works",
            "number_of_papers": "",
            "domain_or_topic": "Automated literature survey generation (general scientific literature)",
            "output_type": "Literature surveys (as per title)",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Cited as one of the few prior works addressing LLM-based literature-review writing and multi-agent approaches; LiRA claims to extend these ideas with explicit readability and factuality mechanisms.",
            "limitations_challenges": "This paper does not provide internal details; Qi et al. referenced as prior art only.",
            "scaling_behavior": "",
            "uuid": "e4408.6",
            "source_info": {
                "paper_title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation",
                "publication_date_yy_mm": "2025-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Generation of Scientific Literature Surveys Based on Large Language Models (LLM) and Multi-Agent Systems (MAS)",
            "rating": 2,
            "sanitized_title": "generation_of_scientific_literature_surveys_based_on_large_language_models_llm_and_multiagent_systems_mas"
        },
        {
            "paper_title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "rating": 2,
            "sanitized_title": "metagpt_meta_programming_for_a_multiagent_collaborative_framework"
        },
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
            "rating": 2,
            "sanitized_title": "scireviewgen_a_largescale_dataset_for_automatic_literature_review_generation"
        },
        {
            "paper_title": "Overview of the NLPCC2024 Shared Task 6: Scientific Literature Survey Generation",
            "rating": 2,
            "sanitized_title": "overview_of_the_nlpcc2024_shared_task_6_scientific_literature_survey_generation"
        },
        {
            "paper_title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
            "rating": 1,
            "sanitized_title": "prometheus_2_an_open_source_language_model_specialized_in_evaluating_other_language_models"
        }
    ],
    "cost": 0.01909525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation
1 Oct 2025</p>
<p>Gregory Hok 
Tjoan Go 
University of Amsterdam</p>
<p>Elsevier B.V</p>
<p>Khang Ly 
Elsevier B.V</p>
<p>Anders Søgaard soegaard@di.ku.dk 
University of Copenhagen</p>
<p>Amin Tabatabaei 
Elsevier B.V</p>
<p>Maarten De Rijke m.derijke@uva.nl 
University of Amsterdam</p>
<p>Xinyi Chen x.chen2@uva.nl 
University of Amsterdam</p>
<p>LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation
1 Oct 2025ADD478DECA514052B578F09DA061106EarXiv:2510.05138v1[cs.CL]
The rapid growth of scientific publications has made it increasingly difficult to keep literature reviews comprehensive and up-to-date.Though prior work has focused on automating retrieval and screening, the writing phase of systematic reviews remains largely under-explored, especially with regard to readability and factual accuracy.To address this, we present LiRA (Literature Review Agents), a multi-agent collaborative workflow which emulates the human literature review process.LiRA utilizes specialized agents for content outlining, subsection writing, editing, and reviewing, producing cohesive and comprehensive review articles.Evaluated on SciReviewGen and a proprietary ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey and MASS-Survey in writing and citation quality, while maintaining competitive similarity to human-written reviews.We further evaluate LiRA in real-world scenarios using document retrieval and assess its robustness to reviewer model variation.Our findings highlight the potential of agentic LLM workflows, even without domain-specific tuning, to improve the reliability and usability of automated scientific writing.</p>
<p>Introduction</p>
<p>Since their inception, literature reviews have been consistently used to streamline the advancement of various scientific fields (Snyder et al. 2016).Of these reviews, one of the most important types is the systematic literature review (SLR), which reproducibly synthesizes a significant portion of existing research relating to a specific research question being addressed (Kitchenham and Charters 2007;Bangdiwala 2024).This role has become increasingly more crucial, evidenced by how quite a few researchers consider them to be original research or potentially even a mandatory step in the scientific process itself (Kraus, Mahto, and Walsh 2023;Palmatier, Houston, and Hulland 2018).</p>
<p>Due to the large amount of new findings and research being disseminated through publications, it has become very difficult to release SLRs in a timely fashion (Qi, Li, and Lyu 2025;Ofori-Boateng et al. 2024;Tian et al. 2025).For example, the estimated time required to complete an SLR has increased significantly in the past few decades in the medical domain (Allen and Olkin 1999;Borah et al. 2017), which is further compounded by the necessity of using expert labor (Atkinson 2025).In tackling this, the vast majority of research relating to SLR automation focuses on the retrieval and screening of scientific papers (Orel et al. 2023;Marshall and Wallace 2019;Chen et al. 2025), as these are the most time-consuming steps (Chai et al. 2021).However, there remains the task of compiling the findings into a comprehensive review paper.Only a small number of works have been published (Kasanishi et al. 2023;Qi, Li, and Lyu 2025;Wang et al. 2024), let alone those which focus on the readability and hallucination mitigation aspects.</p>
<p>In this work, we present LiRA (Literature Review Agents), an agentic solution aimed at addressing the minimal research related to automated literature review writing.It is a large language model (LLM)-based agentic workflow building upon existing relevant works and integrates some of the most recent SLR-writing guidelines to generate accurate and high-quality reviews, with an additional emphasis on readability.Moreover, it is entirely out-of-the-box, requiring no task-specific pre-training or fine-tuning.We also reduce hallucination in the outputs, which is one of the main barriers in trustworthy automated writing and a key factor preventing the widespread use of similar artificial intelligence (AI)-powered systems (Alkaissi and McFarlane 2023;O'Connor et al. 2024;Xu et al. 2023).</p>
<p>To demonstrate LiRA's capabilities, we propose the below research questions:</p>
<p>RQ1 To what extent are the qualitative systematic literature reviews created by LiRA similar to human-written ones compared to existing literature writing methods when all are given the same set of references?RQ2 How well are the generated articles written in terms compared to existing literature writing methods when all are given the same set of references?RQ3 How well can LiRA properly use citations from the provided sources to generate qualitative systematic literature reviews compared to other methods?RQ4 How well can LiRA be used in real-world settings when using references returned by a scientific document retriever?</p>
<p>We summarize our main contributions as follows:</p>
<p>• To the best of our knowledge, we propose the first agentic LLM-based automated literature review writing workflow which emulates the human writing process and integrates the findings of other relevant agentic workflows.• We explore the usage of formally defined guidelines and techniques from relevant similar fields in the agentic workflow, such as the idea of thoroughly analyzing the existing papers before beginning the writing process, establishing a crucial link between theory and application.• We establish several state-of-the-art baseline results for the automated literature review writing task across multiple settings, comparing between existing systems when using the same LLM type throughout.</p>
<p>Related Work</p>
<p>Agentic workflows Comparisons have been made between agentic LLM systems and human cognition, as the idea of breaking down a task into smaller steps, which is often done by these workflows, is commonly used to describe how humans solve more intricate problems (Flower and Hayes 1981;Correa et al. 2023).Using this concept, several works show promise in implementing agentic workflows in various fields, such as the medical sciences (Tang et al. 2024) and law (Watson et al. 2025).In some cases, this results in an improvement of more than 90% compared to a simple baseline (Watson et al. 2025).However, for the task of automated writing specifically, only few works have been published thus far (Qi, Li, and Lyu 2025;Wang et al. 2024;Shao et al. 2024;Tian et al. 2025).Of these works, only two of them address literature reviews or a similar document type and include open-source code (Qi, Li, and Lyu 2025;Wang et al. 2024).Neither paper takes output lengths and how they relate to the readability and evaluation of each proposed system using the listed metrics into account.As a result, no works seem to exist which tackle the issues of readability, and only minimal work exists in addressing the factuality of the resulting articles.</p>
<p>Literature review For centuries (Lind 2014), the process of writing literature reviews has been considered crucial for the development of science (Meerpohl et al. 2012;Higgins et al. 2011;Chalmers, Hedges, and Cooper 2002), as it provides researchers valuable insight on which research questions to answer via an analysis of earlier works (Chalmers and Glasziou 2009;Eagly and Wood 1994).Moreover, improvements have been made to eliminate personal biases (Egger, Smith, and O'Rourke 2001) through the introduction of the SLR, which uses systematic methods of review for the collation and synthesis of findings (Randles and Finnegan 2023;Snyder 2019;Page et al. 2021).</p>
<p>Given how time-consuming (Borah et al. 2017) and costly (Michelson and Reuter 2019) this process is, a need for a viable alternative has emerged.Despite this, there is not much relevant innovation in natural language processing that tackles this issue (Mohammad et al. 2009;Kasanishi et al. 2023;Agarwal et al. 2011), let alone results indicating real-world usability.Therefore, we address all aforementioned problems by introducing an agentic workflow which uses LLMs to generate literature reviews automatically, demonstrating its potential by focusing on both readability and factuality.</p>
<p>The LiRA Framework</p>
<p>LiRA emulates the human literature review process by decomposing it into specialized, interacting LLM-based agents.Each agent tackles a key sub-task: either structural planning, fine-grained writing, consistency refinement, or factual verification, which results in a modular and scalable pipeline.This section introduces the core agents and their design motivations.</p>
<p>Outline Drafter Agent</p>
<p>A key challenge in literature review writing is constructing a coherent structure from a large and unorganized set of references.Instead of relying on the model to implicitly determine the structure during generation, LiRA explicitly drafts an outline to guide the writing process.This agent takes the topic and abstracts (or alternatively the full texts) of the provided references to produce a set of candidate outlines consisting of main sections and subsections.These are then combined into a unified draft structure that includes descriptions for each section and suggested supporting papers.</p>
<p>To manage the context size and maintain focus on the most relevant content, the outline is constructed from up to 50 references.We use existing heuristics (Wang et al. 2024) that recommend generating approximately 8 sections with around 4 subsections, but the agent can adapt this amount as needed.We also include default sections such as an introduction and conclusion to ensure consistency and completeness.</p>
<p>Subsection Writer Agent</p>
<p>Generating a full review in a single pass risks superficial coverage and poor section coherence.We tackle this by having a subsection writer agent, which writes each (sub)section individually in parallel, conditioned on the description and a selected subset of relevant references.This design encourages fine-grained generation and makes the outputs more modular and easier to revise.</p>
<p>Relevant references are retrieved using the FAISS index (Douze et al. 2025) based on section-level descriptions, capped at 25% of the total pool per subsection (min 3, max 150).This balances citation diversity with input tractability.Moreover, the reference titles, abstracts, or full texts can be retrieved depending on the availability.The agent outputs 1,000 words per subsection, enabling long-form synthesis while staying within model context limits.Also, the article title, abstract, and conclusion are written after the body is generated, mimicking human writing practices and avoiding the premature commitment of top-down generation.</p>
<p>Editor Agent</p>
<p>Even with structural planning and localized generation, the assembled review may exhibit issues such as redundancy, inconsistency, or stylistic mismatches across sections.These are addressed by an editor agent, which refines the entire draft with a focus on presentation and style.It performs standard editing operations, including improving transitions, enhancing vocabulary, and ensuring overall fluency and cohesion.Importantly, this agent does not alter the factual content, preserving the integrity of the generated information.</p>
<p>If the generated output exceeds the model's context window, the agent detects whether the text ends abruptly-typically when the maximum output length (16,384 tokens) is reached.In such cases, the model is prompted to continue the output using the original input and previously edited content as context.This mechanism effectively doubles the generation capacity, though at the cost of additional input overhead.</p>
<p>Reviewer Agent</p>
<p>To improve factual accuracy and review quality, we introduce an LLM-based reviewer agent inspired by human editorial workflows.This agent evaluates intermediate outputs (e.g., outlines) based on adapted criteria from systematic review guidelines (Snyder 2019), including content completeness, transparency, clarity, and contribution.</p>
<p>If a component (e.g., an outline or section) fails to meet quality thresholds, the reviewer provides structured feedback and triggers regeneration.The review loop continues up to 3 rounds before fallback progression, balancing refinement with computational efficiency.</p>
<p>Citation Behavior</p>
<p>Citation hallucination remains a major concern in automated scientific writing, as models may generate plausiblesounding but non-existent references.To address this, LiRA incorporates citation grounding directly into the generation process.Rather than relying on abstract placeholders (e.g., numbered citations), agents cite sources using full article titles, which act as semantic anchors and help the model maintain alignment with the provided references.</p>
<p>After generation, these in-text citations are post-processed into standard numbered references, and any hallucinated ti-tles are redacted during evaluation to ensure fair comparison.This approach improves factual consistency and ensure fair comparison during evaluation.</p>
<p>Additional Implementation Details</p>
<p>All agents in LiRA were implemented using LangGraph.Moreover, each agent has its own memory, a standard practice for LLM-based agentic workflows to better act upon feedback (Sumers et al. 2023;Qian et al. 2024).This system of storing feedback in memory is comparable to Reflexion (Shinn et al. 2023), where the agent has to adjust its behavior based on the feedback provided.Parallelization is also included to increase the processing speed of certain steps, namely the researcher when analyzing papers and the content writer for generating the article (sub)sections.</p>
<p>Aspects from the design of MetaGPT (Hong et al. 2023) were adapted for more efficient inter-agent communication.Specifically, all agents are required to return their outputs as structured documents to avoid potential inefficiencies relating to information presentation, which are then sent to unique shared message pools for quick information retrieval.Moreover, the input contents are filtered based on the model's maximum context window length (128,000 tokens in our case) to prevent information overload.As a method of improving the model's output quality with minimal intervention, Zero-Shot Chain-of-Thought (Kojima et al. 2022) is included in the prompts for all agents except the researcher and editor, as they do not perform refinement.</p>
<p>Experiments 4.1 Baselines</p>
<p>To evaluate LiRA, we compare it against direct prompting and, to the best of our knowledge, the only two publicly available agentic frameworks for survey writing.For fair comparison, all systems (including LiRA) are implemented with gpt-4o-mini as the underlying LLM.</p>
<p>Direct prompting (DP)</p>
<p>As the simplest baseline, we directly prompt the LLM with task instructions and the full set of reference titles and metadata, asking it to generate a review with the specified sections and length.When the reference list exceeds the context window, it is passed as an attached file, requiring the model's file-reading capability.This baseline tests whether a single prompt can produce a coherent review without decomposition or refinement.</p>
<p>MASS-Survey (MASS)</p>
<p>Introduced in an automated survey-writing challenge (Tian et al. 2025), MASS is the only agentic framework with publicly available code.Its workflow differs fundamentally from LiRA: Instead of decomposing the writing process into specialized roles with iterative feedback, MASS follows a strictly sequential pipeline.The system first clusters references by topical similarity to construct an outline, then generates section contents and a title directly from these clusters, and finally appends a conclusion.Long reference lists are handled by passing them as attachments.</p>
<p>AutoSurvey (AS) AutoSurvey (Wang et al. 2024) is a multi-stage framework for automatically generating litera-ture surveys in computer science.Given a query (in our case, the original review title), it retrieves relevant publications and uses their titles and abstracts to construct an outline, followed by subsection drafting with refinement steps and partial use of retrieved article content (up to 1,500 tokens).While the paper claims retrieval-grounded drafting, we did not find corresponding functionality in the released code.</p>
<p>For fair comparison, we modified AutoSurvey to restrict retrieval to the references cited in the target human-written review.This required reducing the number of candidate documents per subsection to between 2 and 25% of the references, or falling back to 60 when the fraction exceeded this threshold (as in the original design).We also generalized system prompts from "You are an expert in Artificial Intelligence" to "You are an expert in a relevant field" to make the system better able to generate articles for multiple domains.</p>
<p>Metrics</p>
<p>To comprehensively evaluate the generated literature reviews, we consider three complementary dimensions: content similarity to human-written reviews, writing quality, and citation reliability.</p>
<p>Similarity to the human-written review We measure how similar the generated literature reviews are to humanwritten ones.Metrics include ROUGE-L, heading soft recall (hsr), heading entity recall (her), and article entity recall (aer).Together, these capture lexical overlap, structural alignment, and coverage of key cited works.Full definitions are provided in Appendix A.</p>
<p>Writing quality We evaluate writing quality using both automatic and human assessments.For automatic evaluation, we use the Prometheus 2 LLM (Kim et al. 2024).It is an open-source LLM evaluator that uses the appropriate reference materials (the instruction, reference answer, and score rubric) to provide assessments which mostly align with those of human annotators.Three aspects are evaluated for all generated articles, namely the coverage, structure, and relevance.Here, coverage represents how broad the subject matter of the review is, while structure measures the organization and flow of the review, and relevance indicates how well the review is able to stay on-topic overall.Additional specifications for the model and how it measures the aforementioned aspects can be found in Appendix B.</p>
<p>For human evaluation, we employed subject matter experts (SMEs) who helped evaluate the outputs of the system on the same aspects as mentioned above.For feasibility, the annotation was performed differently for each dataset, though the title, outline, and a section snippet were utilized in both cases.For SciReviewGen, we employed a group of 3 SMEs from Straive to select their preferences between human-and LiRA-written articles for 30 sample snippets, using a rubric as guidance for determining their choice.It must also be noted that the order in which these samples were presented was randomized for each row.</p>
<p>Meanwhile, a dedicated team from within the company provided scores ranging from 1 to 5 for the AutoSurvey and LiRA articles while using the human-written ones as a baseline.This grading was done using the same rubric as men-tioned above.Furthermore, due to this type of annotation being more labor-intensive, only 15 article snippets were used.</p>
<p>Citation quality</p>
<p>We evaluate how well generated claims are grounded in appropriate references.Our metric, Citation Quality F1-Score (CQF1), balances precision (penalizing irrelevant or hallucinated citations) and recall (capturing missing but necessary citations), serving as a proxy for hallucination in scientific writing.A full overview of the details is given in Appendix D.</p>
<p>Datasets</p>
<p>We evaluate the effectiveness of the LiRA framework using two datasets.The primary dataset is SciReviewGen (Kasanishi et al. 2023), a publicly available benchmark built on the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al. 2020).It contains 10,000 review articles in computer science, referencing approximately 690,000 papers.Each review is annotated with structured metadata, including titles, section headers, full texts, and references.Following the setup in Shao et al. (2024), we randomly sample 125 reviews, ensuring each selected article contains a sufficient number of references-averaging around 70 per paper.</p>
<p>To assess the generalizability beyond computer science, we additionally evaluate on an internal dataset of 125 expertwritten reviews from ScienceDirect, covering 23 subject areas including business, microbiology, and materials science.The dataset is matched in size to the SciReviewGen subset to support direct cross-domain comparison.</p>
<p>Results</p>
<p>Across both datasets, LiRA consistently outperforms baseline systems on the majority of evaluation metrics, demonstrating stronger alignment with human-written reviews, higher writing quality, and more reliable citation use.</p>
<p>Similarity to human-written review LiRA achieves the highest ROUGE scores, indicating stronger lexical alignment with human-written reviews.AutoSurvey attains slightly higher heading/entity recall, but largely due to verbosity: on average, AutoSurvey produces 50,000 tokens per article compared to only 22,000 for LiRA.Since recallbased metrics do not normalize for length, longer outputs are naturally favored.Crucially, this shows that LiRA generates concise yet information-dense reviews, rather than inflating scores by producing excessive text.</p>
<p>Writing quality LiRA achieves the best overall writing quality, with a clear advantage in structural coherence.AutoSurvey performs marginally better in coverage, again reflecting its longer outputs, but at the cost of organization and readability.SME evaluations align with these trends: on ScienceDirect, experts strongly preferred LiRA for structure, while AutoSurvey received marginally higher scores for coverage and relevance, suggesting a trade-off between breadth and coherence.On SciReviewGen, SMEs favored LiRA over the human-written reviews, noting its broader and more balanced coverage given the outlines.These results highlight LiRA's strength in generating wellstructured, concise, and expert-aligned reviews, in some cases even being favored over human-written baselines.</p>
<p>Citation quality LiRA demonstrates the largest gains in citation reliability, achieving the highest Citation Quality F1 (CQF1) scores across both datasets (0.76 on SciReviewGen, 0.73 on ScienceDirect) and substantially outperforming Au-toSurvey (≤0.63) and all other baselines.This indicates that LiRA is more effective at grounding claims in appropriate references, avoiding both omissions (recall errors) and hallucinations (precision errors), which stem from LiRA's citation-grounded generation design that explicitly enforces reference anchoring during drafting and refinement.</p>
<p>From this, it can be seen that LiRA overall produces literature reviews that are concise, structurally coherent, and citation-faithful, while maintaining competitive coverage.This balance between quality and reliability highlights LiRA as a more trustworthy and practically useful framework for automated survey writing.achieves the highest ROUGE scores, indicating stronger lexical alignment with human-written reviews.AutoSurvey attains slightly higher heading/entity recall, but largely due to verbosity: on average, AutoSurvey produces 50,000 tokens per article compared to only 22,000 for LiRA.Since recall-based metrics do not normalize for length, longer outputs are naturally favored.Crucially, this shows that LiRA generates concise yet information-dense reviews, rather than inflating scores by producing excessive text.</p>
<p>Metric</p>
<p>DP MASS AS LiRA SciReviewGen ROUGE 0.06 ± 0.0 0.09 ± 0.0 0.09 ± 0.0 0.13 ± 0.0 hsr 0.69 ± 0.1 0.66 ± 0.1 0.92 ± 0.4 0.82 ± 0.1 her 0.06 ± 0.0 0.05 ± 0.0 0.15 ± 0.0 0.10 ± 0.0 aer 0.06 ± 0.0 0.09 ± 0.0 0.34 ± 0.0 0.27 ± 0.0 ScienceDirect ROUGE 0.02 ± 0.0 0.04 ± 0.0 0.13 ± 0.0 0.13 ± 0.0 hsr 0.24 ± 0.2 0.22 ± 0.2 0.24 ± 0.4 0.25 ± 0.1 her 0.03 ± 0.0 0.03 ± 0.0 0.13 ± 0.0 0.05 ± 0.0 aer 0.03 ± 0.0 0.05 ± 0.0 0.25 ± 0.0 0.17 ± 0.0 Table 1: Results for similarity to the human-written reviews with the baseline settings.Note that if both numbers in a row are bolded, it means they returned the exact same value.</p>
<p>Potential Modifications on LiRA</p>
<p>In this section, we discuss the adjustments tested on LiRA to evaluate its performance more robustly.This involves the usage of a different LLM type for the reviewer agent to potentially mitigate self-bias amplification in the refinement process, and document retriever usage to evaluate if LiRA can be deployed in real-world settings.</p>
<p>Using a different reviewer model</p>
<p>Method Based on concerns stemming from self-bias amplification (Xu et al. 2024)  during generation for each component, which is based on existing suggestions.To this end, the gemma3:4b model was used through ollama (Team et al. 2025).This model was chosen because it is open-source, has a context window similar to gpt-4o-mini, and has lower hardware requirements compared to most other models.In addition, ollama was the selected model provider because of its accessibility and ease of use, with LangGraph already being compatible with it.The parameter values used by gemma3:4b were the default ollama ones aside from the context window size (128,000) and seed (42).</p>
<p>Results</p>
<p>The results for this setting can be found in figure 4. Though the extent of bias mitigation itself cannot be measured properly with the current metrics, we can remark how using gemma3:4b has little impact on the scores overall across all metrics.This suggests that alternative model configurations may not significantly alter the article output quality, therefore indicating potentially that the pipeline can work well even when using different LLMs in the process.
SciReviewGen ScienceDirect Metric LiRA LiRA +gemma3
LiRA LiRA +gemma3 ROUGE 0.13 ± 0.0 0.13 ± 0.0 0.13 ± 0.0 0.13 ± 0.0 hsr 0.82 ± 0.1 0.83 ± 0.1 0.25 ± 0.1 0.25 ± 0.1 her 0.10 ± 0.0 0.10 ± 0.0 0.05 ± 0.0 0.05 ± 0.0 aer 0.27 ± 0.0 0.26 ± 0.0 0.17 ± 0.0 0.17 ± 0.</p>
<p>Retrieval Usage</p>
<p>Method All prior experiments assumed the availability of gold references from a reference article to generate a review.This is not the case for real-world settings, however, as novel literature reviews are required to keep up with current developments.Therefore, we evaluate if the system can generate reviews similar enough to the human-written ones when provided with retrieved references instead, hence the inclusion of RQ4.Specifically, an internal API for embeddingsimilarity search was used, which can be called to retrieve as many references as listed in the human-written review.</p>
<p>Results</p>
<p>We examined if the results when using retrieval differed significantly compared to the baseline researcher setting, which was tested using the appropriate statistical tests.From the results (shown in 5), we note that only two results were significantly different compared to the baseline, indicating that LiRA is capable of performing similarly despite the different references used.More details on the statistical tests can be found in Appendix F.</p>
<p>Deployment</p>
<p>The deployment of LiRA will use the following steps.First, it will be developed using the Python version of Lang-Graph, which is an open-source and production ready agentic framework.Furthermore, the gpt-4o-mini LLM from AzureOpenAI will be used, with the possibility of using other models given LangGraph's extensive support for various other endpoints such as form ollama.</p>
<p>As the use case of LiRA requires it to generate novel literature reviews not based on existing reviews, a document retrieval system will be added on to the system.It would function by asking the user for a review topic as input, which would then be enriched using an LLM (i.e., gpt-4o-mini) and afterwards used for embedding-based retrieval using an internal API.This API by default has access to a large collection of scientific articles, and can be replaced depending on the specific circumstances.</p>
<p>Conclusion, Limitations, and Future Work</p>
<p>This work introduces LiRA, an agentic workflow designed for the automatic writing of literature reviews, which integrates the concepts of research before writing and refinement in its core pipeline.The results obtained show that LiRA is capable of performing the task of automated literature review quite well, outperforming all tested open-source methods when accounting for the varying output lengths, indicating a positive result for essentially every research question proposed.Moreover, it reduces hallucination through improved citation behavior and can demonstrably be used in real-world settings.Several improvements could be made, mainly regarding the irreproducibility of results due to the usage of gpt-4o-mini for all experiments.This can be solved by using seedable models instead, which should be feasible given cunrrent LLM availability.In addition, there is a lack of open-source datasets for this task specifically, which hinders the generalizability of all results to other scientific fields.Therefore, we encourage authors to create additional datasets, ideally in a similar format to SciReviewGen, to facilitate the evaluation of similar systems in the future.</p>
<p>Furthermore, opportunities exist to create more end-toend pipelines, as the current project does not take into account factors such as primary studies and risk of bias in randomized trials (i.e., the implementation of automated tools based on (Cochrane 2024)).Doing this would allow for the integration of more steps within the literature review writing process, namely the screening and search criteria definition steps, which would allow for better paper reproducibility.</p>
<p>A Metrics Measuring the Similarity with Human-Written Articles</p>
<p>This section includes an in-depth explanation of how the textual similarity metrics were implemented and how they function.</p>
<p>• ROUGE-L: This metric is frequently used for text evaluation and functions by counting the number of recalled units in a text from the reference (Lin 2004).For this project, we used the ROUGE-L metric from the Hug-gingFace evaluate package.• Heading Soft Recall: Inspired by the idea of soft recall (Fränti and Mariescu-Istodor 2023), this metric quantitatively measures the heading coverage of a review without relying on exact matches, where a higher score indicates a more comprehensive outline by virtue of being similar to the human-written one.Here, we use the original implementation provided by Shao et al. (2024).Let S be a set, and let both P and G be sets of predicted and gold headers, respectively.For each item in S, the soft count is defined as the inverse sum of its similarity to all other items within S, denoted as follows:
count(S i ) = 1 K j=1 Sim(S i , S j ) , Sim(S i , S j ) = cos (embed(S i ), embed(S j )),
where embed(•) is parametrized by the parahprase-MiniLM-L6-v2 model provided by Sentence-Transformers.This is then used to measure the cardinality of the set, defined as the sum of its individual items:
card(S) = K i=1 count(S i ).
The heading soft recall (hsr) is then defined as:
hsr = card(G ∩ P ) card(G) , card(G ∩ P ) = card(G) + card(P ) − card(G ∪ P ).
• Heading and Article Entity Recall: These metrics indicate the proportion of named entities in the baseline gold review mentioned in the generated text, between headings or the full review texts, respectively.The implementation for this is also taken from Shao et al. (2024).However, due to the presence of more scientific terms, we instead use the SciSpacy named entity tagger for all settings, specifically using the en_core_sci_lg model, as it has the largest available vocabulary.</p>
<p>B LLM Evaluation</p>
<p>Here, we outline the details for LLM evaluation.For the specific LLM model, we used prometheus-7b-v2.0using (Kim et al. 2024) with the same configurations outlined in Shao et al. (2024), namely a temperature of 0.01, cumulative probability of top tokens of 0.95, maximum number of new tokens of 512, and repetition penalty of 1.03.Also, because of hardware limitations, the model had to be run with fp8-quantization and a memory usage value of 0.8.Note that no seeding was used here, with the justification being that it would encourage more diverse outputs from the model, thus better mimicking reviews made by separate experts.Moreover, to simulate the process of evaluation by Subject Matter Experts (SMEs) and account for randomness, the evaluation was performed three times for all articles, with the final score being the average between all runs.Furthermore, due to the model's context size, the evaluation for the criteria had to be done in the following ways:</p>
<p>• For coverage and relevance, each generated review and its original article were split into their sections, with each section being evaluated separately using the same criteria.If any section is still too long for any of the two, then said section is split further into two parts each.The criteria for determining this is if the combined length of the generated and original section exceeds the context window (32,768 tokens) minus roughly three thousand tokens to account for the rubric and other components.</p>
<p>• For structure, evaluation was performed using only the outlines of both articles.This is because they are reasonably short and serve as a proper approximation for how the respective contents are arranged.</p>
<p>The rubric provided (shown in table 7) to the LLM contains scores ranging from 1 to 5, with the coverage, structure, and relevance evaluated for all generated literature reviews.Furthermore, the description for each score bracket is specified to accurately reflect the proper gradings.These were adapted from the rubric used by Wang et al. (2024), with minor adjustments to accommodate for the output structure.Also, because three runs were performed per setting, the results reported are the means of the average scores and the averaged resulting standard deviations.</p>
<p>C Sample SME Evaluation</p>
<p>We include a sample annotation for the SciReviewGen dataset in figure 3. It can be seen that the SMEs were provided with the title, article snippet, and outline, which were then used to determine which article was qualitatively better.</p>
<p>D Citation Quality</p>
<p>This section provides an overview of how the citation quality F1-score functions.It uses an LLM (gpt-4o-mini) to evaluate how well the article cites the given reference abstracts.More precisely, for a given article containing a set of claims |C|, an LLM performing Natural Language Inference denoted as m determines the entailment of a claim c i given the references listed Ref i = {r i1 , r i2 , • • • }, returning 1 if the references support the claim and 0 otherwise.This is then used to calculate the citation recall (R c ) and precision (P c ), which are defined as the following:
R c = |C| i=1 m(c i , Ref i ) |C| , P c = |C| i=1 |Refi| j=1 m(c i , Ref i ) ∩ n(c i , r ij ) |C| j=1 |Ref i | , where n(c i , r ij ) = (m(c i , {r ij }) = 1) ∪ (m(c i , Ref i \ {r ij }) = 0), which indicates if the paper r ij is related to the claim c i .
Given the consideration that longer articles are more difficult to cite properly for, a scaling factor was included for the recall, as it is reliant on the number of claims present.We argue that for a given article, the amount of effort required to properly cite references for the claims is roughly exponential due to the amount of information which has to be remembered from each paper and the relations between all papers.For example, if given references Ref = {r 1 , r 2 , r 3 }, not only must one consider the relations between individual pairs ((r 1 , r 2 ), (r 2 , r 3 ), (r 1 , r 3 )), but also the potential relations between sets of papers.Thus, if a generated article is too short, its performance is penalized accordingly.</p>
<p>The scaling factor is defined as the following:
Scaling = 1 − e (−k×nclaims) ,
where k is a scaling factor (set as 0.01 based on the assumption that a paper has roughly 100 citations on average) and n claims is the average number of claims per article across the generated articles.By definition, this value asymptotically approaches 1 due to the exponential term when n is large enough.This is applied to the aggregate recall score to smooth out the weighting, which is then used to calculate a final F1-score.</p>
<p>For the references, only the abstracts were used, as doing so would replicate the process of how SMEs generally perform the same task.Furthermore, the implementation was taken from Wang et al. (2024) with some adjustments to better handle longer inputs.Specifically, assuming a claimreference pair is too long, the reference is split into a few sections, and then the metric checks if any split supports the claim.This assumes the rest of the references are also supported by the sources as a result.</p>
<p>E Ablation and Modification</p>
<p>Figures 4 and 5 show the results for additional studies relating to system design modification.More precisely, this involves two experiments, with the first being an ablation study of the editor model.This was performed because this component can be considered less impactful towards the output compared to most other agents in the system design.</p>
<p>The other experiment performed was the addition of a researcher agent.As a method of better imitating the process of literature review writing by humans, this agent takes each reference abstract and analyzes the contents based on several pre-determined questions based on existing literature (Snyder 2019).This results in a list of findings (i.e., key points, limitations) which are then utilized by the other agents in place of the original abstract.</p>
<p>The results indicate that the editor model has a generally small but negative impact towards the final results, though it does seem to improve the article coverage and relevance slightly in both cases.Meanwhile, the addition of a researcher agent also tends to decrease overall performance, again with marginal differences except in some metrics.As such, these results suggest that the editor agent in LiRA provides a trade-off in terms of article quality and similarity, while the researcher agent as it currently stands tends to reduce performance, likely because the abstracts are already full of information and do not require further processing.</p>
<p>F Retrieval Setting Statistical Tests</p>
<p>We include the statistical significance results for all results related to the retriever setting comparison in table 6.For all ROUGE-and recall-based metrics, the two-tailed Mann-Whitney U-test was used, as almost all of the results were not normally distributed (determined using the Shapiro-Wilk test on each set of results).Additionally, the results for the Prometheus gradings use the same test, though the results across all runs were averaged by sample first to combine the results first by group.</p>
<p>All tests and normality checks were performed using the corresponding functions available in the scipy.statsThe literature review section comprehensively covers all key and peripheral topics, providing detailed discussions and extensive information.</p>
<p>Metric</p>
<p>4</p>
<p>The literature review section covers most key areas of the topic comprehensively, with only very minor topics left out.</p>
<p>3</p>
<p>The literature review section is generally comprehensive in coverage but still misses a few key points that are not fully discussed.</p>
<p>2</p>
<p>The literature review section covers some parts of the topic but has noticeable omissions, with significant areas either underrepresented or missing.</p>
<p>1</p>
<p>The literature review section has very limited coverage, only touching on a small portion of the topic and lacking discussion on key areas.</p>
<p>Structure 5</p>
<p>The literature review outline is tightly structured and logically clear, with all sections and content arranged most reasonably, and transitions between adjacent sections smooth without redundancy.</p>
<p>4</p>
<p>The literature review outline has good logical consistency, with content well arranged and natural transitions, only slightly rigid in a few parts.</p>
<p>3</p>
<p>The literature review outline has a generally reasonable logical structure, with most content arranged orderly, though some links and transitions could be improved such as repeated subsections.</p>
<p>2</p>
<p>The literature review outline has weak logical flow with some content arranged in a disordered or unreasonable manner.</p>
<p>1</p>
<p>The literature review outline lacks logic, with no clear connections between sections, making it difficult to understand the overall framework.</p>
<p>Relevance 5</p>
<p>The literature review section is exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic.</p>
<p>4</p>
<p>The literature review section is mostly on topic and focused; the narrative has a consistent relevance to the core subject with infrequent digressions.3</p>
<p>The literature review section is generally on topic, despite a few unrelated details.</p>
<p>2</p>
<p>The literature review section is somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to.</p>
<p>1</p>
<p>The literature review section is outdated or unrelated to the field it purports to review, offering no alignment with the topic.</p>
<p>Table 7: The rubric criteria alongside their descriptions, adapted from (Wang et al. 2024).</p>
<p>Figure 1 :
1
Figure 1: An overview of the LiRA architecture.The narrow dotted arrows represent document input/output, the wide dotted arrows indicate the refinement process, and the filledin arrows signify the system's main flow.Each agent is explained in the below sections.</p>
<p>Figure 2 :
2
Figure 2: SME evaluation results.</p>
<p>Figure 3 :
3
Figure 3: Annotation sample for the SciReviewGen dataset.</p>
<p>Figure 4 :
4
Figure 4: Textual similarity and citation quality results for the different reviewer model setting.</p>
<p>Figure 5 :
5
Figure 5: Prometheus evaluation results for the different reviewer model setting.</p>
<p>Table 2 :
2
Writing quality results for the baseline settings.
MetricDPMASSASLiRASciReviewGenCoverage 3.53 ± 0.4 4.30 ± 0.3 4.50 ± 0.1 4.40 ± 0.1Structure 3.15 ± 0.9 2.47 ± 1.2 2.30 ± 1.3 3.38 ± 0.9Relevance 4.49 ± 0.2 4.74 ± 0.2 4.55 ± 0.2 4.57 ± 0.2Average3.723.833.784.11ScienceDirectCoverage 3.08 ± 1.0 3.44 ± 1.0 4.10 ± 0.1 3.90 ± 0.3Structure 3.23 ± 1.0 2.59 ± 1.1 2.21 ± 1.3 3.42 ± 0.9Relevance 3.98 ± 0.7 4.15 ± 0.7 4.29 ± 0.3 4.33 ± 0.3Average3.433.393.533.88DatasetDP MASS AS LiRASciReviewGen 0.14 0.13 0.63 0.76ScienceDirect 0.06 0.33 0.55 0.73
, experimentation was performed using a reviewer model type different from the one used</p>
<p>Table 3 :
3
Citation quality results for the baseline settings.</p>
<p>Table 4 :
4
The gemma3 results for the ScienceDirect dataset.
0</p>
<p>Table 5 :
5
The ScienceDirect retrieval results.The stars indicate results significantly lower than the baseline.
MetricLiRALiRA +retrieverROUGE 0.130 ± 0.021 0.128 ± 0.019hsr0.257 ± 0.130 0.251 ± 0.116her0.056 ± 0.043 0.054 ± 0.044aer0.170 ± 0.057 0.152 ± 0.054<em>Coverage 3.892 ± 0.407 3.839 ± 0.415</em>Structure 3.264 ± 1.049 3.411 ± 1.023Relevance 4.296 ± 0.389 4.270 ± 0.372</p>
<p>Table 6 :
6
The statistical significance test results for the retriever experiment setting.
U Statistic valuep-valueROUGE-L 8503.5.226hsr8041.0.690her8414.5.292aer10097.5 &lt;.001Coverage8714.5.114Structure7909.5.864Relevance8970.5.042
Acknowledgments We would like to thank Marcela Haldan and Alexandra Noti for their help in providing annotations for the ScienceDirect articles.
Towards Multi-Document Summarization of Scientific Articles:Making Interesting Comparisons with SciSumm. N Agarwal, R S Reddy, K Gvr, C P Rosé, Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages. A Nenkova, J Hirschberg, Y Liu, the Workshop on Automatic Summarization for Different Genres, Media, and LanguagesPortland, OregonAssociation for Computational Linguistics2011</p>
<p>Estimating Time to Conduct a Meta-analysis From Number of Citations Retrieved. H Alkaissi, S I Mcfarlane, Cureus, I E Allen, I Olkin, JAMA. 28272023. 1999Artificial Hallucinations in ChatGPT: Implications in Scientific Writing</p>
<p>AI-pocalypse now: Automating the Systematic Literature Review with SPARK (Systematic Processing and Automated Review Kit) -Gathering, Organising, Filtering, and Scaffolding. C F Atkinson, 2025MethodsX14103129</p>
<p>Analysis of the Time and Workers Needed to Conduct Systematic Reviews of Medical Interventions Using Data from the PROSPERO Registry. S I Bangdiwala, R Borah, A W Brown, P L Capers, K A Kaiser, 10.1080/17457300.2024.2388484International Journal of Injury Control and Safety Promotion. Taylor &amp; Francis eprint313e0125452024. 2017BMJ Open. Publisher: British Medical Journal Publishing Group Section: Health informatics</p>
<p>Research Screener: A Machine Learning Tool to Semi-automate Abstract Screening for Systematic Reviews. K E K Chai, R L J Lines, D F Gucciardi, L Ng, Systematic Reviews. 101932021</p>
<p>Avoidable Waste in the Production and Reporting of Research Evidence. I Chalmers, P Glasziou, The Lancet. 37496832009Elsevier</p>
<p>Can Large Language Models Fully Automate or Partially Assist Paper Selection in Systematic Reviews?. I Chalmers, L V Hedges, H Cooper, H Chen, Z Jiang, X Liu, C C Xue, S M E Yew, B Sheng, Y.-F Zheng, X Wang, Y Wu, S Sivaprasad, T Y Wong, V Chaudhary, Y C Tham, Evaluation &amp; the Health Professions. 2512002. 2025EpidemiologyBritish Journal of Ophthalmology. Publisher: BMJ Publishing Group Ltd Section</p>
<p>Cochrane Handbook for Systematic Reviews of Interventions. Cochrane, 2024</p>
<p>Humans Decompose Tasks by Trading Off Utility and Computational Cost. C G Correa, M K Ho, F Callaway, N D Daw, T L Griffiths, ): e1011087. Publisher: Public Library of Science. 202319</p>
<p>M Douze, A Guzhva, C Deng, J Johnson, G Szilvasy, P.-E Mazaré, M Lomeli, L Hosseini, H Jégou, ArXiv:2401.08281The Faiss Library. 2025</p>
<p>Using Research Syntheses to Plan Future Research. A H Eagly, W Wood, The Handbook of Research Synthesis. New York, NY, USRussell Sage Foundation1994</p>
<p>Introduction: Rationale, Potentials, and Promise of Systematic Reviews. M Egger, G D Smith, K O'rourke, Systematic Reviews in Health Care, 1-19. John Wiley &amp; Sons2001</p>
<p>A Cognitive Process Theory of Writing. L Flower, J R Hayes, 1981College Composition and Communication32365</p>
<p>Soft Precision and Recall. P Fränti, R Mariescu-Istodor, Pattern Recognition Letters. 1672023</p>
<p>The Cochrane Collaboration's Tool for Assessing Risk of Bias in Randomised Trials. J P T Higgins, D G Altman, P C Gøtzsche, P Jüni, D Moher, A D Oxman, J Savović, K F Schulz, L Weeks, J A C Sterne, Publisher: British Medical Journal Publishing Group Section: Research Methods. 34359282011BMJ</p>
<p>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. S Hong, M Zhuge, J Chen, X Zheng, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, J Schmidhuber, Proceedings of the Twelfth International Conference on Learning Representations. the Twelfth International Conference on Learning Representations2023</p>
<p>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation. T Kasanishi, M Isonuma, J Mori, I Sakata, Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. S Kim, J Suk, S Longpre, B Y Lin, J Shin, S Welleck, G Neubig, M Lee, K Lee, M Seo, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Guidelines for Performing Systematic Literature Reviews in Software Engineering. B Kitchenham, S Charters, T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Proceedings of the 2nd international workshop on Evidential assessment of software technologies. the 2nd international workshop on Evidential assessment of software technologies2007. 202235Advances in Neural Information Processing Systems</p>
<p>The Importance of Literature Reviews in Small Business and Entrepreneurship Research. S Kraus, R V Mahto, S T Walsh, 10.1080/00472778.2021.1955128Publisher: Routledge eprint. 202361</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. C.-Y Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Containing an Inquiry into the Nature, Causes, and Cure, of that Disease. Cambridge Library Collection -History of Medicine. J Lind, Three Parts. CambridgeCambridge University Press2014A Treatise of the Scurvy</p>
<p>S2ORC: The Semantic Scholar Open Research Corpus. K Lo, L L Wang, M Neumann, R Kinney, D Weld, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Toward Systematic Review Automation: A Practical Guide to Using Machine Learning Tools in Research Synthesis. I J Marshall, B C Wallace, Systematic Reviews. 811632019</p>
<p>Scientific Value of Systematic Reviews: Survey of Editors of Core Clinical Journals. J J Meerpohl, F Herrle, G Antes, E V Elm, PLOS ONE. 75e357322012Publisher: Public Library of Science</p>
<p>The Significant Cost of Systematic Reviews and Meta-analyses: A Call for Greater Involvement of Machine Learning to Assess the Promise of Clinical Trials. M Michelson, K Reuter, Contemporary Clinical Trials Communications. 161004432019</p>
<p>Using Citations to Generate Surveys of Scientific Paradigms. S Mohammad, B Dorr, M Egan, A Hassan, P Muthukrishan, V Qazvinian, D Radev, D Zajic, Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. M Ostendorf, M Collins, S Narayanan, D W Oard, L Vanderwende, Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational LinguisticsBoulder, ColoradoAssociation for Computational Linguistics2009</p>
<p>Towards the Automation of Systematic Reviews Using Natural Language Processing, Machine Learning, and Deep Learning: A Comprehensive Review. R Ofori-Boateng, M Aceves-Martins, N Wiratunga, C F Moreno-Garcia, Artificial Intelligence Review. 5782002024</p>
<p>An Automated Literature Review Tool (LiteRev) for Streamlining and Accelerating Research Using Natural Language Processing and Machine Learning: Descriptive Performance Evaluation Study. E Orel, I Ciglenecki, A Thiabaud, A Temerev, A Calmy, O Keiser, A Merzouki, Journal of Medical Internet Research. 25e397362023</p>
<p>Large Language Models, Updates, and Evaluation of Automation Tools for Systematic Reviews: A Summary of Significant Discussions at the Eighth Meeting of the International Collaboration for the Automation of Systematic Reviews (ICASR). A M O'connor, J Clark, J Thomas, R Spijker, W Kusa, V R Walker, M Bond, Systematic Reviews. 1312902024</p>
<p>M J Page, J E Mckenzie, P M Bossuyt, I Boutron, T C Hoffmann, C D Mulrow, L Shamseer, J M Tetzlaff, E A Akl, S E Brennan, R Chou, J Glanville, J M Grimshaw, A Hróbjartsson, M M Lalu, T Li, E W Loder, E Mayo-Wilson, S Mcdonald, L A Mcguinness, L A Stewart, J Thomas, A C Tricco, V A Welch, P Whiting, D Moher, The PRISMA 2020 Statement: An Updated Guideline for Reporting Systematic Reviews. 202137271</p>
<p>Review Articles: Purpose, Process, and Structure. R W Palmatier, M B Houston, J Hulland, Journal of the Academy of Marketing Science. 4612018</p>
<p>Generation of Scientific Literature Surveys Based on Large Language Models (LLM) and Multi-Agent Systems (MAS). R Qi, W Li, H Lyu, Natural Language Processing and Chinese Computing. D F Wong, Z Wei, M Yang, SingaporeSpringer Nature2025</p>
<p>ChatDev: Communicative Agents for Software Development. C Qian, W Liu, H Liu, N Chen, Y Dang, J Li, C Yang, W Chen, Y Su, X Cong, J Xu, D Li, Z Liu, M Sun, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Guidelines for Writing a Systematic Review. R Randles, A Finnegan, Nurse Education Today. 1251058032023</p>
<p>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. Y Shao, Y Jiang, T Kanell, P Xu, O Khattab, M Lam, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American ChapterMexico City; MexicoAssociation for Computational Linguistics20241</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Literature Review as a Research Methodology: An Overview and Guidelines. H Snyder, Journal of Business Research. 1042019</p>
<p>Identifying Categories of Service Innovation: A Review and Synthesis of the Literature. H Snyder, L Witell, A Gustafsson, P Fombelle, P Kristensson, Journal of Business Research. 6972016</p>
<p>Cognitive Architectures for Language Agents. T Sumers, S Yao, K Narasimhan, T Griffiths, Transactions on Machine Learning Research. 2023</p>
<p>MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning. X Tang, A Zou, Z Zhang, Z Li, Y Zhao, X Zhang, A Cohan, M Gerstein, Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>. G Team, A Kamath, J Ferret, S Pathak, N Vieillard, R Merhej, S Perrin, T Matejovicova, A Ramé, M Rivière, L Rouillard, T Mesnard, G Cideron, J Grill, S -B.; Ramos, E Yvinec, M Casbon, E Pot, I Penchev, G Liu, F Visin, K Kenealy, L Beyer, X Zhai, A Tsitsulin, R Busa-Fekete, A Feng, N Sachdeva, B Coleman, Y Gao, B Mustafa, I Barr, E Parisotto, D Tian, M Eyal, C Cherry, J.-T Peter, D Sinopalnikov, S Bhupatiraju, R Agarwal, M Kazemi, D Malkin, R Kumar, D Vilar, I Brusilovsky, J Luo, A Steiner, A Friesen, A Sharma, A Sharma, A M Gilady, A Goedeckemeyer, A Saade, A Feng, A Kolesnikov, A Bendebury, A Abdagic, A Vadi, A György, A S Pinto, A Das, A Bapna, A Miech, A Yang, A Paterson, A Shenoy, A Chakrabarti, B Piot, B Wu, B Shahriari, B Petrini, C Chen, C L Lan, C A Choquette-Choo, C J Carey, C Brick, D Deutsch, D Eisenbud, D Cattle, D Cheng, D Paparas, D S Sreepathihalli, D Reid, D Tran, D Zelle, E Noland, E Huizenga, E Kharitonov, F Liu, G Amirkhanyan, G Cameron, H Hashemi, H Klimczak-Plucińska, H Singh, H Mehta, H T Lehri, H Hazimeh, I Ballantyne, I Szpektor, I Nardini, J Pouget-Abadie, J Chan, J Stanton, J Wieting, J Lai, J Orbay, J Fernandez, J Newlan, J.-Y Ji, J Singh, K Black, K Yu, K Hui, K Vodrahalli, K Greff, L Qiu, M Valentine, M Coelho, M Ritter, M Hoffman, M Watson, M Chaturvedi, M Moynihan, M Ma, N Babar, N Noy, N Byrd, N Roy, N Momchev, N Chauhan, N Sachdeva, O Bunyan, P Botarda, P Caron, P K Rubenstein, P Culliton, P Schmid, P G Sessa, P Xu, P Stanczyk, P Tafti, R Shivanna, R Wu, R Pan, R Rokni, R Willoughby, R Vallu, R Mullins, S Jerome, S Smoot, S Girgin, S Iqbal, S Reddy, S Sheth, S Põder, S Bhatnagar, S R Panyam, S Eiger, S Zhang, T Liu, T Yacovone, T Liechty, U Kalra, U Evci, V Misra, V Roseberry, V Feinberg, V Kolesnikov, W Han, W Kwon, X Chen, Y Chow, Y Zhu, Z Wei, Z Egyed, V Cotruta, M Giang, P Kirk, A Rao, K Black, N Babar, J Lo, E Moreira, L G Martins, O Sanseviero, L Gonzalez, Z Gleicher, T Warkentin, Mirrokni, Senter, E.; Collins, E.; Barral, J.; Ghahramani, Z.; Hadsell, R.; Matias, Y.; Sculley, D.; Petrov, S.; Fiedel, N.; Shazeer, N.; Vinyals, O.; Dean, J.; Hassabis, D.; Kavukcuoglu, K.; Farabet, C.; Buchatskaya, E.; Alayrac, J.-B.; Anil, R.; Dmitry; Lepikhin</p>
<p>. S Borgeaud, O Bachem, A Joulin, A Andreev, C Hardin, R Dadashi, L Hussenot, ArXiv:2503.19786Gemma. 32025Technical Report</p>
<p>Overview of the NLPCC2024 Shared Task 6: Scientific Literature Survey Generation. Y Tian, X Gu, A Li, H Zhang, R Xu, Y Li, M Liu, Natural Language Processing and Chinese Computing. D F Wong, Z Wei, M Yang, SingaporeSpringer Nature2025</p>
<p>AutoSurvey: Large Language Models Can Automatically Write Surveys. Y Wang, Q Guo, W Yao, H Zhang, X Zhang, Z Wu, M Zhang, X Dai, M Zhang, Q Wen, W Ye, S Zhang, Y Zhang, Proceedings of the 38th Annual Conference on Neural Information Processing Systems. the 38th Annual Conference on Neural Information Processing Systems2024</p>
<p>LAW: Legal Agentic Workflows for Custody and Fund Services Contracts. W Watson, N Cho, N Srishankar, Z Zeng, L Cecchi, D Scott, S Siddagangappa, R Kaur, T Balch, M Veloso, Proceedings of the 31st International Conference on Computational Linguistics: Industry Track. O Rambow, L Wanner, M Apidianaki, H Al-Khalifa, B D Eugenio, S Schockaert, K Darwish, A Agarwal, the 31st International Conference on Computational Linguistics: Industry TrackAbu Dhabi, UAEAssociation for Computational Linguistics2025</p>
<p>A Critical Evaluation of Evaluations for Long-form Question Answering. F Xu, Y Song, M Iyyer, E Choi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsPapers; Toronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement. W Xu, G Zhu, X Zhao, L Pan, L Li, W Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>            </div>
        </div>

    </div>
</body>
</html>
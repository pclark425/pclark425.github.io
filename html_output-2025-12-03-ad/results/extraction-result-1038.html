<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1038 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1038</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1038</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-227254851</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.02096v1.pdf" target="_blank">Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</a></p>
                <p><strong>Paper Abstract:</strong> A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1038.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1038.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAIRED_Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Protagonist-Antagonist Induced Regret Environment Design - Navigation agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pair of simulated embodied navigation agents (protagonist and antagonist) trained with PPO in procedurally generated partially-observable grid navigation tasks where an adversary constructs environments; PAIRED uses regret (antagonist reward minus protagonist reward) to drive environment generation, producing a curriculum of increasingly complex but solvable navigation tasks and improving zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Protagonist (learner) and Antagonist (competitor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents are recurrent-policy (LSTM) Reinforcement Learning agents trained with PPO. Agents receive a partial 5x5x3 visual observation and orientation input, use convolution + LSTM networks, and produce actions to navigate to a goal; the antagonist is trained simultaneously to maximize environment reward (and thus produce an upper-bound used to compute regret).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual, grid-world embodied agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Partially-observable tile-based navigation (15x15 grid, Minigrid-like)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>15x15 tile grid with border walls and up to hundreds of placement steps (adversary places agent, goal, and up to 50 obstacles). The environment is partially observable (agent has a limited local field-of-view), and environment complexity arises from obstacle configurations that create long, structured paths (e.g., mazes, labyrinths, multi-room layouts) and varying start/goal placements.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of obstacles/blocks placed; Euclidean distance start→goal; shortest path length between start and goal (measured in grid steps); 'solved path length' (shortest path length of a maze successfully solved by the agent during training). Specifics: adversary places up to 50 obstacles; map free tiles = 13x13 = 169; solved path length measured in tiles.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies from low → high during training under PAIRED; complexity quantified up to map-scale shortest paths (~O(10) tiles, map 13 free tiles per side; experiments report solved path lengths reaching the scale of the map versus low baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural parameter variation via adversary (positions of agent, goal, obstacle placement sequence), domain randomization baseline (uniform sampling of (x,y) for agent/goal/blocks), out-of-distribution tests by varying number-of-blocks; variation quantified by number of different generated environment instances and parameter distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (PAIRED and domain randomization generate large variation across placements); however DR variation is unstructured while PAIRED variation is structured and curriculum-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Zero-shot transfer success rate (percent successful trials), per-episode cumulative reward during training, and solved shortest-path length (measure of complexity of tasks agent can solve).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot transfer: on challenging human-designed tests PAIRED achieved substantially higher success rates (Labyrinth: PAIRED ≈ 40% success, Minimax ≈ 10%, Domain Randomization ≈ 0%; Maze: PAIRED ≈ 18%, Minimax 0–10%, DR 0%). Exact solved-path-length time series reported qualitatively (PAIRED agents achieved substantially longer solved path lengths than baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly argues and demonstrates that adversarial minimax (worst-case) produces environments that are often unsolvable (high complexity but infeasible), and domain randomization produces unstructured variation that rarely yields highly structured complex tasks; PAIRED (minimax regret) produces structured, solvable high-complexity environments by tailoring environment variation to the agents' current capabilities, thereby producing an automatic curriculum: as protagonist improves on simpler environments the adversary is incentivized to propose harder but solvable configurations, increasing complexity while maintaining feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Simple OOD tests (varying number of blocks) — all methods transfer well to modest OOD numbers-of-blocks; quantitative values not fully enumerated in text except that minimax performed significantly worse on the 50-block OOD in one case.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Tested (16 Rooms, Labyrinth, Maze): PAIRED outperformed baselines (Labyrinth ≈ 40% success for PAIRED vs ≈ 10% minimax and 0% DR; Maze ≈ 18% PAIRED vs 0% DR; minimax near 0% on Maze).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Four Rooms (within-distribution unlikely simple configuration): most methods performed well (nearly straight-line paths); exact success rates not numerically specified but described as high.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Unsupervised Environment Design with adversarial environment generator (PAIRED) producing curriculum; compared against domain randomization, minimax adversarial training, and population-based minimax.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PAIRED-trained protagonists exhibited better zero-shot transfer to novel, more structured and more difficult navigation tasks (16 Rooms, Labyrinth, Maze), achieving substantially higher success rates than domain randomization and plain minimax adversary; DR and minimax failed on the hardest, structured maze-like tests.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported as a single scalar; training details: agents trained with PPO, 30 environment workers in parallel collecting batches; hyperparameter sweeps and many training iterations (figures show long-running training). Exact numbers of environment interactions per final policy are not provided in aggregate form.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PAIRED generates a natural curriculum of increasingly complex navigation environments tailored to agent ability; this curriculum produces agents capable of solving more complex mazes and achieving higher zero-shot transfer on structured, challenging test environments than domain randomization or minimax adversarial generation, which either fail to produce structure (DR) or produce unsolvable tasks (minimax). Regret objective incentivizes generating the easiest tasks on which the protagonist fails but antagonist succeeds (zone of proximal development), yielding progressively harder but solvable tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1038.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1038.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAIRED_Hopper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Protagonist-Antagonist Induced Regret Environment Design - MuJoCo Hopper</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated MuJoCo hopper agents (protagonist and antagonist) trained with PPO while an adversary applies additional torques or environmental perturbations; PAIRED uses regret to constrain adversary to produce challenging but solvable dynamics, improving robustness to mass/friction and transfer compared to unconstrained minimax adversary and domain randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Protagonist (learner) and Antagonist (competitor) in MuJoCo Hopper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Feed-forward neural network Reinforcement Learning agents trained with PPO; adversary applies per-joint torque disturbances θ (scaled proportion α of agent strength). Protagonist and antagonist receive the same exogenous torques during episodes and are trained in parallel to produce a regret signal for the adversary.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual physics-based robot simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MuJoCo Hopper with external adversarial torques; transfer tests varying mass and friction coefficients</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous control locomotion task where environment variation is produced by adversary-applied torques at each timestep (bounded by a fraction α of agent torque strength), and transfer evaluation uses changes to physical parameters (mass, friction). Complexity arises from stronger adversarial forces and parameter settings that make the task harder or unsolvable.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Adversary strength α (proportion of agent torque) scaled from 0.1 → 1.0 over 300 iterations; environment difficulty measured by cumulative episode reward and whether agent can maintain locomotion; unsolvability indicated by agent reward driven to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies; experiments ramp adversary strength from low (0.1) to high (1.0); at full strength unconstrained minimax produced effectively unsolvable conditions (maximal complexity), while PAIRED maintained feasible but challenging settings.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Adversary-generated continuous torque perturbations per joint (procedural adversary); transfer variation includes mass and friction coefficient changes at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High — adversary can apply torques up to full agent strength after ramp; transfer tests include multiple unseen mass/friction parameter settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Per-episode cumulative reward during training and transfer; robustness measured by reward under varied mass/friction at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: After adversary strength constraints are removed, unconstrained minimax adversary drove agent reward to zero for remainder of training (policy failed to learn), while PAIRED agents maintained non-zero reward and generalized better to unseen mass/friction parameters. Exact numeric reward traces are reported in plots but not given as single numeric summaries in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper emphasizes that unconstrained worst-case (minimax) adversaries can create arbitrarily hard/unsolvable dynamics (high complexity + high variation) which prevent learning, whereas regret-based adversary (PAIRED) selects perturbations that are challenging but solvable, balancing complexity and feasibility; thus there is a trade-off where excessive variation in adversarial strength reduces learnability.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>When adversary strength is high and unconstrained, minimax training causes agent reward to collapse to zero (failed learning); PAIRED avoids collapse and yields policies more robust under high variation at test time. Exact numeric reward comparisons are plotted but not enumerated as single numbers in text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>With low adversary strength (early in ramp) both approaches can learn; agents were pre-trained for 100 iterations without adversary.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>PAIRED adversarial environment generation optimizing regret (adversary reward = antagonist reward − protagonist reward), compared to minimax adversary (adversary reward = −protagonist reward) and domain-randomization-style baselines; PPO optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PAIRED-trained hopper policies generalized better to unseen physical parameter variations (mass and friction) than policies trained against unconstrained minimax adversaries, which failed when adversary was allowed to make tasks unsolvable; PAIRED maintained performance whereas minimax policies had poor transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training schedule: pre-train agents 100 iterations without adversary; adversary strength ramped from 0.1 to 1.0 over 300 iterations; PPO hyperparameters and learning rates swept. Total environment-interaction counts not summarized as a single scalar.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In continuous control, unconstrained minimax adversaries can destroy learnability by producing unsolvable dynamics; PAIRED's regret objective constrains the adversary to propose perturbations that the antagonist can solve but the protagonist cannot, preserving solvability while increasing difficulty and producing policies that generalize more robustly to unseen dynamics (mass/friction). This demonstrates a practical trade-off between environment variation strength and learnability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Active domain randomization <em>(Rating: 1)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 1)</em></li>
                <li>Adversarial policies: Attacking deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1038",
    "paper_id": "paper-227254851",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PAIRED_Navigation",
            "name_full": "Protagonist-Antagonist Induced Regret Environment Design - Navigation agents",
            "brief_description": "A pair of simulated embodied navigation agents (protagonist and antagonist) trained with PPO in procedurally generated partially-observable grid navigation tasks where an adversary constructs environments; PAIRED uses regret (antagonist reward minus protagonist reward) to drive environment generation, producing a curriculum of increasingly complex but solvable navigation tasks and improving zero-shot transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Protagonist (learner) and Antagonist (competitor)",
            "agent_description": "Agents are recurrent-policy (LSTM) Reinforcement Learning agents trained with PPO. Agents receive a partial 5x5x3 visual observation and orientation input, use convolution + LSTM networks, and produce actions to navigate to a goal; the antagonist is trained simultaneously to maximize environment reward (and thus produce an upper-bound used to compute regret).",
            "agent_type": "simulated agent (virtual, grid-world embodied agent)",
            "environment_name": "Partially-observable tile-based navigation (15x15 grid, Minigrid-like)",
            "environment_description": "15x15 tile grid with border walls and up to hundreds of placement steps (adversary places agent, goal, and up to 50 obstacles). The environment is partially observable (agent has a limited local field-of-view), and environment complexity arises from obstacle configurations that create long, structured paths (e.g., mazes, labyrinths, multi-room layouts) and varying start/goal placements.",
            "complexity_measure": "Number of obstacles/blocks placed; Euclidean distance start→goal; shortest path length between start and goal (measured in grid steps); 'solved path length' (shortest path length of a maze successfully solved by the agent during training). Specifics: adversary places up to 50 obstacles; map free tiles = 13x13 = 169; solved path length measured in tiles.",
            "complexity_level": "Varies from low → high during training under PAIRED; complexity quantified up to map-scale shortest paths (~O(10) tiles, map 13 free tiles per side; experiments report solved path lengths reaching the scale of the map versus low baselines).",
            "variation_measure": "Procedural parameter variation via adversary (positions of agent, goal, obstacle placement sequence), domain randomization baseline (uniform sampling of (x,y) for agent/goal/blocks), out-of-distribution tests by varying number-of-blocks; variation quantified by number of different generated environment instances and parameter distributions.",
            "variation_level": "High (PAIRED and domain randomization generate large variation across placements); however DR variation is unstructured while PAIRED variation is structured and curriculum-driven.",
            "performance_metric": "Zero-shot transfer success rate (percent successful trials), per-episode cumulative reward during training, and solved shortest-path length (measure of complexity of tasks agent can solve).",
            "performance_value": "Zero-shot transfer: on challenging human-designed tests PAIRED achieved substantially higher success rates (Labyrinth: PAIRED ≈ 40% success, Minimax ≈ 10%, Domain Randomization ≈ 0%; Maze: PAIRED ≈ 18%, Minimax 0–10%, DR 0%). Exact solved-path-length time series reported qualitatively (PAIRED agents achieved substantially longer solved path lengths than baselines).",
            "complexity_variation_relationship": "Yes — the paper explicitly argues and demonstrates that adversarial minimax (worst-case) produces environments that are often unsolvable (high complexity but infeasible), and domain randomization produces unstructured variation that rarely yields highly structured complex tasks; PAIRED (minimax regret) produces structured, solvable high-complexity environments by tailoring environment variation to the agents' current capabilities, thereby producing an automatic curriculum: as protagonist improves on simpler environments the adversary is incentivized to propose harder but solvable configurations, increasing complexity while maintaining feasibility.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": "Simple OOD tests (varying number of blocks) — all methods transfer well to modest OOD numbers-of-blocks; quantitative values not fully enumerated in text except that minimax performed significantly worse on the 50-block OOD in one case.",
            "high_complexity_high_variation_performance": "Tested (16 Rooms, Labyrinth, Maze): PAIRED outperformed baselines (Labyrinth ≈ 40% success for PAIRED vs ≈ 10% minimax and 0% DR; Maze ≈ 18% PAIRED vs 0% DR; minimax near 0% on Maze).",
            "low_complexity_low_variation_performance": "Four Rooms (within-distribution unlikely simple configuration): most methods performed well (nearly straight-line paths); exact success rates not numerically specified but described as high.",
            "training_strategy": "Unsupervised Environment Design with adversarial environment generator (PAIRED) producing curriculum; compared against domain randomization, minimax adversarial training, and population-based minimax.",
            "generalization_tested": true,
            "generalization_results": "PAIRED-trained protagonists exhibited better zero-shot transfer to novel, more structured and more difficult navigation tasks (16 Rooms, Labyrinth, Maze), achieving substantially higher success rates than domain randomization and plain minimax adversary; DR and minimax failed on the hardest, structured maze-like tests.",
            "sample_efficiency": "Not reported as a single scalar; training details: agents trained with PPO, 30 environment workers in parallel collecting batches; hyperparameter sweeps and many training iterations (figures show long-running training). Exact numbers of environment interactions per final policy are not provided in aggregate form.",
            "key_findings": "PAIRED generates a natural curriculum of increasingly complex navigation environments tailored to agent ability; this curriculum produces agents capable of solving more complex mazes and achieving higher zero-shot transfer on structured, challenging test environments than domain randomization or minimax adversarial generation, which either fail to produce structure (DR) or produce unsolvable tasks (minimax). Regret objective incentivizes generating the easiest tasks on which the protagonist fails but antagonist succeeds (zone of proximal development), yielding progressively harder but solvable tasks.",
            "uuid": "e1038.0",
            "source_info": {
                "paper_title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "PAIRED_Hopper",
            "name_full": "Protagonist-Antagonist Induced Regret Environment Design - MuJoCo Hopper",
            "brief_description": "Simulated MuJoCo hopper agents (protagonist and antagonist) trained with PPO while an adversary applies additional torques or environmental perturbations; PAIRED uses regret to constrain adversary to produce challenging but solvable dynamics, improving robustness to mass/friction and transfer compared to unconstrained minimax adversary and domain randomization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Protagonist (learner) and Antagonist (competitor) in MuJoCo Hopper",
            "agent_description": "Feed-forward neural network Reinforcement Learning agents trained with PPO; adversary applies per-joint torque disturbances θ (scaled proportion α of agent strength). Protagonist and antagonist receive the same exogenous torques during episodes and are trained in parallel to produce a regret signal for the adversary.",
            "agent_type": "simulated agent (virtual physics-based robot simulation)",
            "environment_name": "MuJoCo Hopper with external adversarial torques; transfer tests varying mass and friction coefficients",
            "environment_description": "Continuous control locomotion task where environment variation is produced by adversary-applied torques at each timestep (bounded by a fraction α of agent torque strength), and transfer evaluation uses changes to physical parameters (mass, friction). Complexity arises from stronger adversarial forces and parameter settings that make the task harder or unsolvable.",
            "complexity_measure": "Adversary strength α (proportion of agent torque) scaled from 0.1 → 1.0 over 300 iterations; environment difficulty measured by cumulative episode reward and whether agent can maintain locomotion; unsolvability indicated by agent reward driven to zero.",
            "complexity_level": "Varies; experiments ramp adversary strength from low (0.1) to high (1.0); at full strength unconstrained minimax produced effectively unsolvable conditions (maximal complexity), while PAIRED maintained feasible but challenging settings.",
            "variation_measure": "Adversary-generated continuous torque perturbations per joint (procedural adversary); transfer variation includes mass and friction coefficient changes at test time.",
            "variation_level": "High — adversary can apply torques up to full agent strength after ramp; transfer tests include multiple unseen mass/friction parameter settings.",
            "performance_metric": "Per-episode cumulative reward during training and transfer; robustness measured by reward under varied mass/friction at test time.",
            "performance_value": "Qualitative: After adversary strength constraints are removed, unconstrained minimax adversary drove agent reward to zero for remainder of training (policy failed to learn), while PAIRED agents maintained non-zero reward and generalized better to unseen mass/friction parameters. Exact numeric reward traces are reported in plots but not given as single numeric summaries in main text.",
            "complexity_variation_relationship": "Yes — the paper emphasizes that unconstrained worst-case (minimax) adversaries can create arbitrarily hard/unsolvable dynamics (high complexity + high variation) which prevent learning, whereas regret-based adversary (PAIRED) selects perturbations that are challenging but solvable, balancing complexity and feasibility; thus there is a trade-off where excessive variation in adversarial strength reduces learnability.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "When adversary strength is high and unconstrained, minimax training causes agent reward to collapse to zero (failed learning); PAIRED avoids collapse and yields policies more robust under high variation at test time. Exact numeric reward comparisons are plotted but not enumerated as single numbers in text.",
            "low_complexity_low_variation_performance": "With low adversary strength (early in ramp) both approaches can learn; agents were pre-trained for 100 iterations without adversary.",
            "training_strategy": "PAIRED adversarial environment generation optimizing regret (adversary reward = antagonist reward − protagonist reward), compared to minimax adversary (adversary reward = −protagonist reward) and domain-randomization-style baselines; PPO optimization.",
            "generalization_tested": true,
            "generalization_results": "PAIRED-trained hopper policies generalized better to unseen physical parameter variations (mass and friction) than policies trained against unconstrained minimax adversaries, which failed when adversary was allowed to make tasks unsolvable; PAIRED maintained performance whereas minimax policies had poor transfer.",
            "sample_efficiency": "Training schedule: pre-train agents 100 iterations without adversary; adversary strength ramped from 0.1 to 1.0 over 300 iterations; PPO hyperparameters and learning rates swept. Total environment-interaction counts not summarized as a single scalar.",
            "key_findings": "In continuous control, unconstrained minimax adversaries can destroy learnability by producing unsolvable dynamics; PAIRED's regret objective constrains the adversary to propose perturbations that the antagonist can solve but the protagonist cannot, preserving solvability while increasing difficulty and producing policies that generalize more robustly to unseen dynamics (mass/friction). This demonstrates a practical trade-off between environment variation strength and learnability.",
            "uuid": "e1038.1",
            "source_info": {
                "paper_title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions",
            "rating": 2,
            "sanitized_title": "paired_openended_trailblazer_poet_endlessly_generating_increasingly_complex_and_diverse_learning_environments_and_their_solutions"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Active domain randomization",
            "rating": 1,
            "sanitized_title": "active_domain_randomization"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 1,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Adversarial policies: Attacking deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "adversarial_policies_attacking_deep_reinforcement_learning"
        }
    ],
    "cost": 0.01415025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</p>
<p>Michael Dennis michael_dennis@berkeley.edu 
Natasha Jaques natashajaques@google.com 
Google Research
Brain team94043Mountain ViewCA</p>
<p>Eugene Vinitsky 
Alexandre Bayen 
Stuart Russell 
Andrew Critch 
Sergey Levine svlevine&gt;@berkeley.edu </p>
<p>University of California Berkeley AI Research (BAIR)
94704BerkeleyCA</p>
<p>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</p>
<p>A wide range of reinforcement learning (RL) problems -including robustness, transfer learning, unsupervised RL, and emergent complexity -require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments. * Equal contribution Preprint. Under review.</p>
<p>Introduction</p>
<p>Many reinforcement learning problems require designing a distribution of tasks and environments that can be used to evaluate and train effective policies. This is true for a diverse array of methods including transfer learning (e.g., [2,48,32,41]), robust RL (e.g., [4,16,28]), unsupervised RL (e.g., [15]), and emergent complexity (e.g., [38,45,46]). For example, suppose we wish to train a robot in simulation to pick up objects from a bin in a real-world warehouse. There are many possible configurations of objects, including objects being stacked on top of each other. We may not know a priori the typical arrangement of the objects, but can naturally describe the simulated environment as having a distribution over the object positions.</p>
<p>However, designing an appropriate distribution of environments is challenging. The real world is complicated, and correctly enumerating all of the edge cases relevant to an application could be impractical or impossible. Even if the developer of the RL method knew every edge case, specifying this distribution could take a significant amount of time and effort. We want to automate this process.  Figure 1: An agent learns to navigate an environment where the position of the goal and obstacles is an underspecified parameter. If trained using domain randomization to randomly choose the obstacle locations (a), the agent will fail to generalize to a specific or complex configuration of obstacles, such as a maze (d). Minimax adversarial training encourages the adversary to create impossible environments, as shown in (b). In contrast, Protagonist Antagonist Induced Regret Environment Design (PAIRED), trains the adversary based on the difference between the reward of the agent (protagonist) and the reward of a second, antagonist agent. Because the two agents are learning together, the adversary is motivated to create a curriculum of difficult but achievable environments tailored to the agents' current level of performance (c). PAIRED facilitates learning complex behaviors and policies that perform well under zero-shot transfer to challenging new environments at test time.</p>
<p>In order for this automation to be useful, there must be a way of specifying the domain of environments in which the policy should be trained, without needing to fully specify the distribution. In our approach, developers only need to supply an underspecified environment: an environment which has free parameters which control its features and behavior. For instance, the developer could give a navigation environment in which the agent's objective is to navigate to the goal and the free parameters are the positions of obstacles. Our method will then construct distributions of environments by providing a distribution of settings of these free parameters; in this case, positions for those blocks. We call this problem of taking the underspecified environment and a policy, and producing an interesting distribution of fully specified environments in which that policy can be further trained Unsupervised Environment Design (UED). We formalize unsupervised environment design in Section 3, providing a characterization of the space of possible approaches which subsumes prior work. After training a policy in a distribution of environments generated by UED, we arrive at an updated policy and can use UED to generate more environments in which the updated policy can be trained. In this way, an approach for UED naturally gives rise to an approach for Unsupervised Curriculum Design (UCD). This method can be used to generate capable policies through increasingly complex environments targeted at the policy's current abilities.</p>
<p>Two prior approaches to UED are domain randomization, which generates fully specified environments uniformly randomly regardless of the current policy (e.g., [17,32,41]), and adversarially generating environments to minimize the reward of the current policy; i.e. minimax training (e.g., [28,25,43,18]). While each of these approaches have their place, they can each fail to generate any interesting environments. In Figure 1 we show examples of maze navigation environments generated by each of these techniques. Uniformly random environments will often fail to generate interesting structures; in the maze example, it will be unlikely to generate walls (Figure 1a). On the other extreme, a minimax adversary is incentivized to make the environments completely unsolvable, generating mazes with unreachable goals (Figure 1b). In many environments, both of these methods fail to generate structured and solvable environments. We present a middle ground, generating environments which maximize regret, which produces difficult but solvable tasks (Figure 1c). Our results show that optimizing regret results in agents that are able to perform difficult transfer task (Figure 1d), which are not possible using the other two techniques.</p>
<p>We propose a novel adversarial training technique which naturally solves the problem of the adversary generating unsolvable environments by introducing an antagonist which is allied with the environmentgenerating adversary. For the sake of clarity, we refer to the primary agent we are trying to train as the protagonist. The environment adversary's goal is to design environments in which the antagonist achieves high reward and the protagonist receives low reward. If the adversary generates unsolvable environments, the antagonist and protagonist would perform the same and the adversary would get a score of zero, but if the adversary finds environments the antagonist solves and the protagonist does not solve, the adversary achieves a positive score. Thus, the environment adversary is incentivized to create challenging but feasible environments, in which the antagonist can outperform the protagonist. Moreover, as the protagonist learns to solves the simple environments, the antagonist must generate more complex environments to make the protagonist fail, increasing the complexity of the generated tasks and leading to automatic curriculum generation.</p>
<p>We show that when both teams reach a Nash equilibrium, the generated environments have the highest regret, in which the performance of the protagonist's policy most differs from that of the optimal policy. Thus, we call our approach Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our results demonstrate that compared to domain randomization, minimax adversarial training, and population-based minimax training (as in Wang et al. [45]), PAIRED agents learn more complex behaviors, and have higher zero-shot transfer performance in challenging, novel environments.</p>
<p>In summary, our main contributions are: a) formalizing the problem of Unsupervised Environment Design (UED) (Section 3), b) introducing the PAIRED Algorithm (Section 4), c) demonstrating PAIRED leads to more complex behavior and more effective zero-shot transfer to new environments than existing approaches to environment design (Section 5), and d) characterizing solutions to UED and connecting the framework to classical decision theory (Section 3).</p>
<p>Related Work</p>
<p>When proposing an approach to UED we are essentially making a decision about which environments to prioritize during training, based on an underspecified environment set provided by the designer.</p>
<p>Making decisions in situations of extreme uncertainty has been studied in decision theory under the name "decisions under ignorance" [27] and is in fact deeply connected to our work, as we detail in Section 3. Milnor [24] characterize several of these approaches; using Milnor's terminology, we see domain randomization [17] as "the principle of insufficient reason" proposed by Laplace, minimax adversarial training [18] as the "maximin principle" proposed by Wald [44], and our approach, PAIRED, as "minimax regret" proposed by Savage [33]. Savage's approach was built into a theory of making sequential decisions which minimize worst case regret [31,19], which eventually developed into modern literature on minimizing worst case regret in bandit settings [6].</p>
<p>Multi-agent training has been proposed as a way to automatically generate curricula in RL [20,21,14]. Competition can drive the emergence of complex skills, even some that the developer did not anticipate [5,13]. Asymmetric Self Play (ASP) [38] trains a demonstrator agent to complete the simplest possible task that a learner cannot yet complete, ensuring that the task is, in principle, solvable. In contrast, PAIRED is significantly more general because it generates complex new environments, rather than trajectories within an existing, limited environment. Campero et al. [7] study curriculum learning in a similar environment to the one under investigation in this paper. They use a teacher to propose goal locations to which the agent must navigate, and the teacher's reward is computed based on whether the agent takes more or less steps than a threshold value. To create a curriculum, the threshold is linearly increased over the course of training. POET [45,46] uses a population of minimax (rather than minimax regret) adversaries to generate the terrain for a 2D walker. However, POET [45] requires generating many new environments, testing all agents within each one, and discarding environments based on a manually chosen reward threshold, which wastes a significant amount of computation. In contrast, our minimax regret approach automatically learns to tune the difficulty of the environment by comparing the protagonist and antagonist's scores. In addition, these works do not investigate whether adversarial environment generation can provide enhanced generalization when agents are transferred to new environments, as we do in this paper.</p>
<p>We evaluated PAIRED in terms of its ability to learn policies that transfer to unseen, hand-designed test environments. One approach to produce policies which can transfer between environments is unsupervised RL (e.g., [34,36]). Gupta et al. [15] propose an unsupervised meta-RL technique that computes minimax regret against a diverse task distribution learned with the DIAYN algorithm [10]. However, this algorithm does not learn to modify the environment and does not adapt the task distribution based on the learning progress of the agent. PAIRED provides a curriculum of increasingly difficult environments and allows us to provide a theoretical characterization of when minimax regret should be preferred over other approaches.</p>
<p>The robust control literature has made use of both minimax and regret objectives. Minimax training has been used in the controls literature [37,3,49], the Robust MDP literature [4,16,26,39], and more recently, through the use of adversarial RL training [28,43,25]. Unlike our algorithm, minimax adversaries have no incentive to guide the learning progress of the agent and can make environments arbitrarily hard or unsolvable. Ghavamzadeh et al. [12] minimize the regret of a model-based policy against a safe baseline to ensure safe policy improvement. Regan and Boutilier [29,30] study Markov Decision Processes (MDPs) in which the reward function is not fully specified, and use minimax regret as an objective to guide the elicitation of user preferences. While these approaches use similar theoretical techniques to ours, they use analytical solution methods that do not scale to the type of deep learning approach used in this paper, do not attempt to learn to generate environments, and do not consider automatic curriculum generation.</p>
<p>Domain randomization (DR) [17] is an alternative approach in which a designer specifies a set of parameters to which the policy should be robust [32,41,40]. These parameters are then randomly sampled for each episode, and a policy is trained that performs well on average across parameter values. However, this does not guarantee good performance on a specific, challenging configuration of parameters. While DR has had empirical success [1], it requires careful parameterization and does not automatically tailor generated environments to the current proficiency of the learning agent. Mehta et al. [23] propose to enhance DR by learning which parameter values lead to the biggest decrease in performance compared to a reference environment.</p>
<p>Unsupervised Environment Design</p>
<p>The goal of this work is to construct a policy that performs well across a large set of environments. We train policies by starting with an initially random policy, generating environments based on that policy to best suit its continued learning, train that policy in the generated environments, and repeat until the policy converges or we run out of resources. We then test the trained policies in a set of challenging transfer tasks not provided during training. In this section, we will focus on the environment generation step, which we call unsupervised environment design (UED). We will formally define UED as the problem of using an underspecified environment to produce a distribution over fully specified environments, which supports the continued learning of a particular policy. To do this, we must formally define fully specified and underspecified environments, and describe how to create a distribution of environments using UED. We will end this section by proposing minimax regret as a novel approach to UED.</p>
<p>We will model our fully specified environments with a Partially Observable Markov Decision Process (POMDP), which is a tuple A, O, S , T , I , R, γ where A is a set of actions, O is a set of observations, S is a set of states, T : S × A → ∆(S) is a transition function, I : S → O is an observation (or inspection) function, R : S → R, and γ is a discount factor. We will define the utility as U (π) = T i=0 r t γ t , where T is a horizon. To model an underspecified environment, we propose the Underspecified Partially Observable Markov Decision Process (UPOMDP) as a tuple M = A, O, Θ, S M , T M , I M , R M , γ . The only difference between a POMDP and a UPOMDP is that a UPOMDP has a set Θ representing the free parameters of the environment, which can be chosen to be distinct at each time step and are incorporated into the transition function as T M : S × A × Θ → ∆(S). Thus a possible setting of the environment is given by some trajectory of environment parameters θ. As an example UPOMDP, consider a simulation of a robot in which θ are additional forces which can be applied at each time step. A setting of the environment θ can be naturally combined with the underspecified environment M to give a specific POMDP, which we will denote M θ .</p>
<p>In UED, we want to generate a distribution of environments in which to continue training a policy. We would like to curate this distribution of environments to best support the continued learning of a particular agent policy. As such, we can propose a solution to UED by specifying some environment policy, Λ : Π → ∆(Θ T ) where Π is the set of possible policies and Θ T is the set of possible sequences of environment parameters. In Table 2, we see a few examples of possible choices for environment policies, as well as how they correspond to previous literature. Each of these choices also have a corresponding decision rule used for making decisions under ignorance [27]. Each decision rule can be understood as a method for choosing a policy given an underspecified environment. This connection between UED and decisions under ignorance extends further than these few decision rules. In the Appendix we will make this connection concrete and show that, under reasonable assumptions, UED and decisions under ignorance are solving the same problem.  [28,43,25] Λ M (π) = argmin θ∈Θ T {U θ (π)} Maximin PAIRED (ours) Λ M R (π) = {θ π : cπ vπ ,D π : otherwise} Minimax Regret Table 2: The environment policies corresponding to the three techniques for UED which we study, along with their corresponding decision rule. Where U(X) is a uniform distribution over X,D π is a baseline distribution, θ π is the trajectory which maximizes regret of π, v π is the value above the baseline distribution that π achieves on that trajectory, and c π is the negative of the worst-case regret of π, normalized so that cπ vπ is between 0 and 1. This is described in full in the appendix.</p>
<p>PAIRED can be seen as an approach for approximating the environment policy, Λ M R , which corresponds to the decision rule minimax regret. One useful property of minimax regret, which is not true of domain randomization or minimax adversarial training, is that whenever a task has a sufficiently well-defined notion of success and failure it chooses policies which succeed. By minimizing the worst case difference between what is achievable and what it achieves, whenever there is a policy which ensures success, it will not fail where others could succeed. Theorem 1. Suppose that all achievable rewards fall into one of two class of outcomes labeled SUCCESS giving rewards in [S min , S max ] and FAILURE giving rewards in
[F min , F max ], such that F min ≤ F max &lt; S min ≤ S max .
In addition assume that the range of possible rewards in either class is smaller than the difference between the classes so we have S max − S min &lt; S min − F max and F max − F min &lt; S min − F max . Further suppose that there is a policy π which succeeds on any θ whenever success is possible. Then minimax regret will choose a policy which has that property.</p>
<p>The proof of this property, and examples showing how minimax and domain randomization fail to have this property, are in the Appendix. In the next section we will formally introduce PAIRED as a method for approximating the minimax regret environment policy, Λ M R .</p>
<p>Protagonist Antagonist Induced Regret Environment Design (PAIRED)</p>
<p>Here, we describe how to approximate minimax regret, and introduce our proposed algorithm, Protagonist Antagonist Induced Regret Environment Design (PAIRED). Regret is defined as the difference between the payoff obtained for a decision, and the optimal payoff that could have been obtained in the same setting with a different decision. In order to approximate regret, we use the difference between the payoffs of two agents acting under the same environment conditions. Assume we are given a fixed environment with parameters θ, a fixed policy for the protagonist agent, π P , and we then train a second antagonist agent, π A , to optimality in this environment. Then, the difference between the reward obtained by the antagonist, U θ (π A ), and the protagonist, U θ (π P ), is the regret:
REGRET θ π P , π A = U θ π A − U θ π P(1)
In PAIRED, we introduce an environment adversary that learns to control the parameters of the environment, θ, to maximize the regret of the protagonist against the antagonist. For each training batch, the adversary generates the parameters of the environment, θ ∼Λ, which both agents will play. The adversary and antagonist are trained to maximize the regret as computed in Eq. 1, while the protagonist learns to minimize regret. This procedure is shown in Algorithm 1. The code for PAIRED and our experiments is available in open source at https://github.com/google-research/ google-research/tree/master/social_rl/. We note that our approach is agnostic to the choice of RL technique used to optimize regret.</p>
<p>To improve the learning signal for the adversary, once the adversary creates an environment, both the protagonist and antagonist generate several trajectories within that same environment. This allows for a more accurate approximation the minimax regret as the difference between the maximum reward of the antagonist and the average reward of the protagonist over all trajectories: REGRET ≈
max τ A U θ (τ A ) − E τ P [U θ (τ P )].
We have found this reduces noise in the reward and more accurately rewards the adversary for building difficult but solvable environments.</p>
<p>Algorithm 1: PAIRED.</p>
<p>Randomly initialize Protagonist π P , Antagonist π A , and AdversaryΛ; while not converged do Use adversary to generate environment parameters: θ ∼Λ. Use to create POMDP M θ .
Collect Protagonist trajectory τ P in M θ . Compute: U θ (π P ) = T i=0 r t γ t Collect Antagonist trajectory τ A in M θ . Compute: U θ (π A ) = T i=0 r t γ t Compute: REGRET θ (π P , π A ) = U θ (π A ) − U θ (π P )
Train Protagonist policy π P with RL update and reward R(τ P ) = −REGRET Train Antagonist policy π A with RL update and reward R(τ A ) = REGRET Train Adversary policyΛ with RL update and reward R(τΛ) = REGRET end At each training step, the environment adversary can be seen as solving a UED problem for the current protagonist, generating a curated set of environments in which it can learn. While the adversary is motivated to generate tasks beyond the protagonist's abilities, the regret can actually incentivize creating the easiest task on which the protagonist fails but the antagonist succeeds. This is because if the reward function contains any bonus for solving the task more efficiently (e.g. in fewer timesteps), the antagonist will get more reward if the task is easier. Thus, the adversary gets the most regret for proposing the easiest task which is outside the protagonist's skill range. Thus, regret incentivizes the adversary to propose tasks within the agent's "zone of proximal development" [8]. As the protagonist learns to solve the simple tasks, the adversary is forced to find harder tasks to achieve positive reward, increasing the complexity of the generated tasks and leading to automatic curriculum generation.</p>
<p>Though multi-agent learning may not always converge [22], we can show that, if each team in this game finds an optimal solution, the protagonist would be playing a minimax regret policy.</p>
<p>Theorem 2. Let (π P , π A , θ) be in Nash equilibrium and the pair (π A , θ) be jointly a best response to π P . Then π P ∈ argmin
π P ∈Π P { argmax π A , θ∈Π A ×Θ T {REGRET θ (π P , π A )}}.
Proof. Let (π P , π A , θ) be in Nash equilibria and the pair (π A , θ) is jointly a best response to π P . Then we can consider (π A , θ) as one player with policy which we will write as π A+ θ ∈ Π × Θ T . Then π A+ θ is in a zero sum game with π P , and the condition that the pair (π A , θ) is jointly a best response to π P is equivalent to saying that π A+ θ is a best response to π P . Thus π P and π A+ θ form a Nash equilibria of a zero sum game, and the minimax theorem applies. Since the reward in this game is defined by REGRET we have:
π P ∈ argmin π P ∈Π P { argmax π A+ θ ∈Π×Θ T {REGRET θ (π P , π A )}}
By the definition of π A+ θ this proves the protagonist learns the minimax regret policy.</p>
<p>The proof gives us reason to believe that the iterative PAIRED training process in Algorithm 1 can produce minimax regret policies. In Appendix D we show that if the agents are in Nash equilibrium, without the coordination assumption, the protagonist will perform at least as well as the antagonist in every parameterization, and Appendix E.1 provides empirical results for alternative methods for approximating regret which break the coordination assumption. In the following sections, we will show that empirically, policies trained with Algorithm 1 exhibit good performance and transfer, both in generating emergent complexity and in training robust policies.</p>
<p>Experiments</p>
<p>The experiments in this section focus on assessing whether training with PAIRED can increase the complexity of agents' learned behavior, and whether PAIRED agents can achieve better or more , and the shortest path length between the start and the goal, which is zero if there is no possible path (c). The final plot shows agent learning in terms of the shortest path length of a maze successfully solved by the agents. Each plot is measured over five random seeds; error bars are a 95% CI. Domain randomization (DR) cannot tailor environment design to the agent's progress, so metrics remain fixed or vary randomly. Minimax training (even with populations of adversaries and agents) has no incentive to improve agent performance, so the length of mazes that agents are able to solve remains similar to DR (d). In contrast, PAIRED is the only method that continues to increase the passable path length to create more challenging mazes (c), producing agents that solve more complex mazes than the other techniques (d).</p>
<p>robust performance when transferred to novel environments. To study these questions, we first focus on the navigation tasks shown in Figure 1. Section 5.2 presents results in continuous domains.</p>
<p>Partially Observable Navigation Tasks</p>
<p>Here we investigate navigation tasks (based on [9]), in which an agent must explore to find a goal (green square in Figure 1) while navigating around obstacles. The environments are partially observable; the agent's field of view is shown as a blue shaded area in the figure. To deal with the partial observability, we parameterize the protagonist and antagonist's policies using recurrent neural networks (RNNs). All agents are trained with PPO [35]. Further details about network architecture and hyperparameters are given in Appendix F.</p>
<p>We train adversaries that learn to build these environments by choosing the location of the obstacles, the goal, and the starting location of the agent. The adversary's observations consist of a fully observed view of the environment state, the current timestep t, and a random vector z ∼ N (0, I), z ∈ R D sampled for each episode. At each timestep, the adversary outputs the location where the next object will be placed; at timestep 0 it places the agent, 1 the goal, and every step afterwards an obstacle. Videos of environments being constructed and transfer performance are available at https://www.youtube.com/channel/UCI6dkF8eNrCz6XiBJlV9fmw/videos.</p>
<p>Comparison to Prior Methods</p>
<p>To compare to prior work that uses pure minimax training [28,43,25] (rather than minimax regret), we use the same parameterization of the environment adversary and protagonist, but simply remove the antagonist agent. The adversary's reward is
R(Λ) = −E τ P [U (τ P )]
. While a direct comparison to POET [45] is challenging since many elements of the POET algorithm are specific to a 2D walker, the main algorithmic distinction between POET and minimax environment design is maintaining a population of environment adversaries and agents that are periodically swapped with each other. Therefore, we also employ a Population Based Training (PBT) minimax technique in which the agent and adversary are sampled from respective populations for each episode. This baseline is our closest approximation of POET [45]. To apply domain randomization, we simply sample the (x, y) positions of the agent, goal, and blocks uniformly at random. We sweep shared hyperparameters for all methods equally. Parameters for the emergent complexity task are selected to maximize the solved path length, and parameters for the transfer task are selected using a set of validation environments. Details are given in the appendix.</p>
<p>Emergent Complexity</p>
<p>Prior work [45,46] focused on demonstrating emergent complexity as the primary goal, arguing that automatically learning complex behaviors is key to improving the sophistication of AI agents. Here,  Figure 3: Percent successful trials in environments used to test zero-shot transfer, out of 10 trials each for 5 random seeds. The first two (a, b) simply test out-of-distribution generalization to a setting of the number of blocks parameter. Four Rooms (c) tests within-distribution generalization to a specific configuration that is unlikely to be generated through random sampling. The 16 Rooms (d), Labyrinth environment (e) and Maze (f) environments were designed by a human to be challenging navigation tasks. The bar charts show the zero-shot transfer performance of models trained with domain randomization (DR), minimax, or PAIRED in each of the environments. Error bars show a 95% confidence interval. As task difficulty increases, only PAIRED retains its ability to generalize to the transfer tasks.</p>
<p>we track the complexity of the generated environments and learned behaviors throughout training. Figure 2 shows the number of blocks (a), distance to the goal (b), and the length of the shortest path to the goal (c) in the generated environments. The solved path length (d) tracks the shortest path length of a maze that the agent has completed successfully, and can be considered a measure of the complexity of the agent's learned behavior.</p>
<p>Domain randomization (DR) simply maintains environment parameters within a fixed range, and cannot continue to propose increasingly difficult tasks. Techniques based on minimax training are purely motivated to decrease the protagonist's score, and as such do not enable the agent to continue learning; both minimax and PBT obtain similar performance to DR. In contrast, PAIRED creates a curriculum of environments that begin with shorter paths and fewer blocks, but gradually increase in complexity based on both agents' current level of performance. As shown in Figure 2 (d), this allows PAIRED protagonists to learn to solve more complex environments than the other two techniques.</p>
<p>Zero-Shot Transfer</p>
<p>In order for RL algorithms to be useful in the real world, they will need to generalize to novel environment conditions that their developer was not able to foresee. Here, we test the zero-shot transfer performance on a series of novel navigation tasks shown in Figure 3. The first two transfer tasks simply use a parameter setting for the number of blocks that is out of the distribution (OOD) of the training environments experienced by the agents (analogous to the evaluation of [28]). We expect these transfer scenarios to be easy for all methods. We also include a more structured but still very simple Four Rooms environment, where the number of blocks is within distribution, but in an unlikely (though simple) configuration. As seen in the figure, this can usually be completed with nearly straight-line paths to the goal. To evaluate difficult transfer settings, we include the 16 rooms, Labyrinth, and Maze environments, which require traversing a much longer and more challenging path to the goal. This presents a much more challenging transfer scenario, since the agent must learn meaningful navigation skills to succeed in these tasks. Figure 3 shows zero-shot transfer performance. As expected, all methods transfer well to the first two environments, although the minimax adversary is significantly worse in 50 Blocks. Varying the number of blocks may be a relatively easy transfer task, since partial observability has been shown to improve generalization performance in grid-worlds [47]. As the environments increase in difficulty, so does the performance gap between PAIRED and the prior methods. In 16 Rooms, Labyrinth, and Maze, PAIRED shows a large advantage over the other two methods. In the Labyrinth (Maze) environment, PAIRED agents are able to solve the task in 40% (18%) of trials. In contrast, minimax agents solve 10% (0.0%) of trials, and DR agents solve 0.0%. The performance of PAIRED on these tasks can be explained by the complexity of the generated environments; as shown in Figure 1c, the field of view of the agent (shaded blue) looks similar to what it might encounter in a maze, although the idea of a maze was never explicitly specified to the agent, and this configuration blocks is very unlikely to be generated randomly. These results suggest that PAIRED training may be better able to prepare agents for challenging, unknown test settings.</p>
<p>Continuous Control Tasks</p>
<p>To compare more closely with prior work on minimax adversarial RL [28,43], we construct an additional experiment in a modified version of the MuJoCo hopper domain [42]. Here, the adversary outputs additional torques to be applied to each joint at each time step, θ. We benchmark PAIRED against unconstrained minimax training. To make minimax training feasible, the torque the adversary can apply is limited to be a proportion of the agent's strength, and is scaled from 0.1 to 1.0 over the course of 300 iterations. After this point, we continue training with no additional mechanisms for constraining the adversary. We pre-train the agents for 100 iterations, then test transfer performance to a set of mass and friction coefficients. All agents are trained with PPO [35] and feed-forward policies. Figure 4a shows the rewards throughout training. We observe that after the constraints on the adversaries are removed after 300 iterations, the full-strength minimax adversary has the ability to make the task unsolvable, so the agent's reward is driven to zero for the rest of training. In contrast, PAIRED is able to automatically adjust the difficulty of the adversary to ensure the task remains solvable. As expected, Figure 4 shows that when constraints on the adversary are not carefully tuned, policies trained with minimax fail to learn and generalize to unseen environment parameters. In contrast, PAIRED agents are more robust to unseen environment parameters, even without careful tuning of the forces that the adversary is able to apply.</p>
<p>Conclusions</p>
<p>We develop the framework of Unsupervised Environment Design (UED), and show its relevance to a range of RL tasks ranging from learning increasingly complex behavior, more robust policies, and improving generalization to novel environments. In environments like games, for which the designer has an accurate model of the test environment, or can easily enumerate all of the important cases, using UED may be unnecessary. However, UED could provide an approach to building AI systems in many of the ambitious uses of AI in real-world settings which are difficult to accurately model.</p>
<p>We have shown that tools from the decision theory literature can be used to characterize existing approaches to environment design, and motivate a novel approach based on minimax regret. Our algorithm, which we call Protagonist Antagonist Induced Regret Environment Design (PAIRED), avoids common failure modes of existing UED approaches, and is able to generate a curriculum of increasingly complex environments. Our results demonstrate that PAIRED agents learn more complex behaviors, and achieve higher zero-shot transfer performance in challenging, novel environments.</p>
<p>Broader Impact</p>
<p>Unsupervised environment design is a technique with a wide range of applications, including unsupervised RL, transfer learning, and Robust RL. Each of these applications has the chance of dramatically improving the viability of real-world AI systems. Real-world AI systems could have a variety of positive impacts, reducing the possibility for costly human error, increasing efficiency, and doing tasks that are dangerous or difficult for humans. However, there are a number of possible negative impacts: increasing unemployment by automating jobs [11] and improving the capabilities of automated weapons. These positives and negatives are potentially exacerbated by the emergent complexity we described in Section 5.1.2, which could lead to improved efficiency and generality allowing the robotics systems to be applied more broadly.</p>
<p>However, if we are to receive any of the benefits of AI powered systems being deployed into real world settings, it is critical that we know how to make these systems robust and that they know how to make good decisions in uncertain environments. In Section 3 we discussed the deep connections between UED and decisions under ignorance, which shows that reasonable techniques for making decisions in uncertain settings correspond to techniques for UED, showing that the study of UED techniques can be thought of as another angle of attack at the problem of understanding how to make robust systems. Moreover, we showed in Section 5.2 that these connections are not just theoretical, but can lead to more effective ways of building robust systems. Continued work in this area can help ensure that the predominant impact of our AI systems is the impact we intended.</p>
<p>Moreover, many of the risks of AI systems come from the system acting unexpectedly in a situation the designer had not considered. Approaches for Unsupervised Environment Design could work towards a solution to this problem, by automatically generating interesting and challenging environments, hopefully detecting troublesome cases before they appear in deployment settings.</p>
<p>[47] Chang Ye, Ahmed Khalifa, Philip Bontrager, and Julian Togelius. Rotation, translation, and cropping for zero-shot generalization. arXiv preprint arXiv:2001.09908, 2020.</p>
<p>[48] Wenhao Yu, Jie Tan, C Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal policy with online system identification. arXiv preprint arXiv:1702.02453, 2017.</p>
<p>[49] Kemin Zhou, John Comstock Doyle, Keith Glover, et al. Robust and optimal control, volume 40. Prentice hall New Jersey, 1996.</p>
<p>A Generality of UED</p>
<p>In Section 3 we defined UPODMPS and UED and showed a selection of natural approaches to UED and corrisponding decision rules. However, the connection between UED and decisions under ignorance is much broader than these few decision rules. In fact, they can be seen as nearly identical problems.</p>
<p>To see the connection to decisions under ignorance, it is important to notice that any decision rule can be thought of as an ordering over policies, ranking the policies that it chooses higher than other policies. We will want to define a condition on this ordering, to do this we will first define what it means for one policy to totally dominate another. Definition 3. A policy, π A , is totally dominated by some policy, π B if for every pair of parameteriza-
tions θ A , θ B U M θ A (π A ) &lt; U M θ B (π B ).
Thus if π A totally dominates π B , it is reasonable to assume that π A is better, since the best outcome we could hope for from policy π B is still worse than the worst outcome we fear from policy π A . Thus we would hope that our decision rule would not prefer π B to π A . If a decision rule respects this property we will say that it respects total domination or formally: Definition 4. We will say that an ordering ≺ respects total domination iff π A ≺ π B whenever π B totally dominates π A .</p>
<p>This is a very weak condition, but it is already enough to allow us to provide a characterization of all such orderings over policies in terms of policy-conditioned distributions over parameters θ, which we will notate Λ as in Section 3. Specifically, any ordering which respects total domination can be written as maximizing the expected value with respect to a policy-conditioned value function, thus every reasonable way of ranking policies can be described in the UED framework. For example, this implies that there is an environment policy which represents the strategy of minimax regret. We explicitly construct one such environment policy in Appendix B.</p>
<p>To make this formal, the policy-conditioned value function, V MΛ (π), is defined to be the expected value a policy will receive in the policy-conditioned distribution of environments M Λ(π) , or formally:
V MΛ (π) = E θ∼Λ(π) <a href="2">U θ (π)</a>
.</p>
<p>The policy-conditioned value function is like the normal value function, but computed over the distribution of environments defined by the UPOMDP and environment policy. This of course implies an ordering over policies, defined in the natural way. Definition 5. We will say that Λ prefers
π B to π A notated π A ≺ Λ π B if V MΛ (π A ) &lt; V MΛ (π B )
Finally, this allows us to state the main theorem of this Section. Theorem 6. Given an order over deterministic policies in a finite UPOMDP, ≺, there exits an environment policy Λ : Π → ∆(Θ T ) such that ≺ is equivalent to ≺ Λ iff it respects total domination and it ranks policies with an equal and a deterministic outcome as equal.</p>
<p>Proof. Suppose you have some order of policies in a finite UPOMDP, ≺, which respects total domination and ranks policies equal if they have an equal and deterministic outcome. Our goal is to construct a function Λ such that π A ≺ Λ π B iff π A ≺ π B .</p>
<p>When constructing Λ notice that we can chose Λ(π) independently for each policy π and that we can choose the resulting value for V MΛ (π) to lie anywhere within the range [ min
θ∈Θ T {U θ (π)}, max θ∈Θ T {U θ (π)}].
Since the number of deterministic policies in a finite POMDP is finite, we can build Λ inductively by taking the lowest ranked policy π in terms of ≺ for which Λ has not yet been defined and choosing the value for V MΛ (π) appropriately.</p>
<p>For the lowest ranked policy π B for which Λ(π B ) has not yet been defined we set Λ(π B ) such that V MΛ (π B ) greater than Λ(π A ) for all π A ≺ π B and is lower than the minimum possible value of any π C such that π B ≺ π C , if such a setting is possible. That is, we choose Λ(π B ) to satisfy for all π A ≺ π B ≺ π C :
Λ(π A ) &lt; V MΛ (π B ) &lt; max θ∈Θ T {U θ (π C )}(3)
Intuitively this ensures that V MΛ (π B ) is high enough to be above all π A lower than π B and low enough such that all future π C can still be assigned an appropriate value.</p>
<p>Finally, we will show that it is possible to set Λ(π B ) to satisfy these conditions in Equation 3. By our inductive hypothesis, we know that Λ(π A ) &lt; max
θ∈Θ T {U θ (π B )} and Λ(π A ) &lt; max θ∈Θ T {U θ (π C )}. Since
≺ respects total domination, we know that min
θ∈Θ T {U θ (π B )} ≤ max θ∈Θ T {U θ (π C )} for all π B ≤ π C .
Since there are a finite number of π C we can set V MΛ (π B ) to be the average of the smallest value for max θ∈Θ T {U θ (π C )} and the largest value for Λ(π A ) for any π C and π A satisfying π A ≺ π B ≺ π C .</p>
<p>The other direction, can be checked directly. If π A is totally dominated by π B , then:
V MΛ (π A ) = E θ∼Λ(π A ) [U θ (π A )] &lt; E θ∼Λ(π B ) [U θ (π B )] = V MΛ (π B )
Thus if ≺ can be represented with an appropriate choice of Λ then it respects total domination and it ranks policies with an equal and a deterministic outcome equal.</p>
<p>Thus, the set of decision rules which respect total domination are exactly those which can be written by some concrete environment policy, and thus any approach to making decisions under uncertainty which has this property can be thought of as a problem of unsupervised environment design and vice-versa. To the best of our knowledge there is no seriously considered approach to making decisions under uncertainty which does not satisfy this property.</p>
<p>B Defining the Environment Policy Corresponding to Minimax Regret</p>
<p>In this section we will be deriving a function Λ(π) which corresponds to regret minimization. Explicitly, we will be deriving a Λ such that minimax regret is the decision rule which maximizes the corresponding policy-conditioned value function defined in Appendix A.</p>
<p>In this context, we will define regret to be:
REGRET(π, θ) = max π B ∈Π {U M θ (π B ) − U M θ (π)}
Thus the set of MINIMAXREGRET strategies can be defined naturally as:
MINIMAXREGRET = argmin π∈Π { max θ∈Θ T</p>
<p>{REGRET(π, θ)}}</p>
<p>We want to find a function Λ for which the set of optimal policies which maximize value with respect to Λ is the same as the set of MINIMAXREGRET strategies. That is, we want to find Λ such that:
MINIMAXREGRET = argmax π∈Π {V MΛ (π)}
To do this we must define a normalization function D : Π → ∆(Θ T ) which has the property that if there are two policies π A , π B such that neither is totally dominated by the other, then they evaluate the same value on the distribution of environments. In general there are many such normalization functions, but we will simply show that one of these exists. Lemma 7. There exists a function D : Π → ∆(Θ T ) such that Λ(π) has support on all of Θ T for all π and such that for all π A , π B such that neither is totally dominated by any other policy,
V M D (π A ) = V M D (π B ) and V M D (π A ) ≤ V M D (π B )
when π B is totally dominated and π A is not.</p>
<p>Proof. We will first define D for the set of policies which are not totally dominated, which we will call X. Note that by the definition not being totally dominated, there is a constant C which is between the worst-case and best-case values of all of the policies in X. Thus for each π ∈ X we can find a distribution θ such that U θ (π) = C. We will chose D(π) = θ to satisfy that condition for all π ∈ X. For other π, we can let D(π) = argmin θ∈Θ T {U θ (π)}. Thus, by construction D satisfies the desired conditions.</p>
<p>Given such a function D, we will construct a Λ which works by shifting probability towards or away from the environment parameters that maximize the regret for the given agent, in the right proportions to achieve the desired result. We claim that the following definition works:
Λ(π) = {θ π : c π v π ,D π : otherwise}
WhereD π = D(π) is a baseline distribution, θ π is the trajectory which maximizes regret of π, and v π is the value above the baseline distribution that π achieves on that trajectory, and c π is the negative of the worst-case regret of π, normalized so that cπ vπ is between 0 and 1. Theorem 8. For Λ as defined above:
argmax π∈Π { E θ∼Λ(π) [U θ (π)]} = MINIMAXREGRET Proof. argmax π∈Π { E θ∼Λ(π) [U θ (π)] = argmax π∈Π { U θπ (π)c π v π +D π (1 − c π v π )} (4) = argmax π∈Π { v π c π +D π c π v π + ( v πDπ −D π c π v π )} (5) = argmax π∈Π { v π c π + v πDπ v π } (6) = argmax π∈Π {c π −D π } (7) = argmax π∈Π {c π } (8) = argmax π∈Π { min θ∈Θ T {−REGRET(π, θ)}} (9) = argmin π∈Π { max θ∈Θ T {REGRET(π, θ)}}(10)</p>
<p>C Minimax Regret Always Succeeds when There is a Clear Notion of Success</p>
<p>In this section we will show that when there is a sufficiently strong notion of success and failure, and there is a policy which can ensure success, minimax regret will choose a successful strategy. Moreover, we will show that neither pure randomization, nor maximin has this property.</p>
<p>Theorem 1. Suppose that all achievable rewards fall into one of two class of outcomes labeled SUCCESS giving rewards in [S min , S max ] and FAILURE giving rewards in [F min , F max ], such that F min ≤ F max &lt; S min ≤ S max . In addition assume that the range of possible rewards in either class is smaller than the difference between the classes so we have S max − S min &lt; S min − F max and F max − F min &lt; S min − F max . Further suppose that there is a policy π which succeeds on any θ whenever success is possible. Then minimax regret will choose a policy which has that property.</p>
<p>Further suppose that there is a policy π which succeeds on any θ whenever any policy succeeds on θ.</p>
<p>Then minimax regret will choose a policy which has that property.</p>
<p>Proof. Let C = S min − F max and let π * be one of the policies that succeeds on any θ whenever success is possible. By assumption S max − S min &lt; C and F max − F min &lt; C. Since π * succeeds whenever success is possible we have:
max θ∈Θ T {REGRET(π * , θ)} &lt; C
Thus any strategy which achieves minimax regret must have regret less than C, since it must do at least as well as π * . Suppose π ∈ MINIMAXREGRET, then if π does not solve some solvable θ then REGRET(π * , θ) &gt; C. Thus every minimax regret policy succeeds whenever that is possible.</p>
<p>Though minimax regret has this property, the other decision rules considered do not. For example, consider the task described by Table 3a, where an entry in position (i, j) shows the payoff U θj (π i ) obtained for the policy π i in row i, in the environment parameterized by θ j in column j. If you choose π B you can ensure success whenever any policy can ensure success, but maximin will choose π A , as it only cares about the worst case outcome.
θ 1 θ 2 π A 0 0 π B 100 -1 (a) θ 1 θ 2 θ 3 θ 4 θ 5 π A 75 75 75 75 75 π B
0 100 100 100 100 π C 100 0 100 100 100 π D 100 100 0 100 100 π E 100 100 100 0 100 π F 100 100 100 100 0 Choosing the policy that maximizes the expected return fails in the game described by Table 3b. Under a uniform distribution all of π B , π C , π D , π E , π F give an expected value of 80, while π A gives an expected value of 75. Here maximizing expected value on a uniform distribution will choose one of π B , π C , π D , π E , π F while π A guarantees success. Moreover, this holds for every possible distribution over parameters, since π A will always give an expected value of 75 and on average π B , π C , π D , π E , π F will give an expected value of 80, so at least one of them gives above 80.
(b)</p>
<p>D Nash solutions to PAIRED</p>
<p>In this section we show how PAIRED works when coordination is not achieved. The following proof shows that if the antagonist, protagonist, and adversary find a Nash Equilibrium, then the protagonist performs better than or equal to the antagonist in every parameterization.</p>
<p>Theorem 9. Let (π P , π A , θ) be in Nash equilibria. Then for all θ : U θ (π P ) ≥ U θ (π A ).</p>
<p>Proof. Let (π P , π A , θ) be in Nash equilibria. Then regardless of π A , θ, the protagonist always has the choice to play π P = π A , so REGRET(π P , π A , θ) ≤ 0. Since, the adversary chooses θ over any other θ we know that REGRET(π P , π A , θ ) ≤ 0 must hold for all θ . Thus by the definition of Regret
U θ (π P ) ≥ U θ (π A ).
This shows that with a capable antagonist the protagonist would learn the minimax regret policy, even without the adversary and the antagonist coordinating, and suggests that methods which strengthen the antagonist could serve to improve the protagonist.</p>
<p>E Additional experiments E.1 Alternative methods for computing regret</p>
<p>Given the arguments in Appendix D, we experimented with additional methods for approximating the regret, which attempt to improve the antagonist. We hypothesized that using a fixed agent as the antagonist could potentially lead to optimization problems in practice; if the antagonist became stuck in a local minimum and stopped learning, it could limit the complexity of the environments the adversary could generate. Therefore, we developed an alternative approach using population-based training (PBT), where for each environment designed by the adversary, each agent in the population collects a single trajectory in the environment. Then, the regret is computed using the difference between the maximum performing agent in the population, and the mean of the population. Assume there is a population of K agents, and i and j index agents within the population. Then:
REGRET pop = max i U (τ i ) − 1 K K j=1 <a href="11">U (τ j )</a>
We also used a population of adversaries, where for each episode, we randomly select an adversary to generate the environment, then test all agents within that environment. We call this approach PAIRED Combined PBT.</p>
<p>Since using PBT can be expensive, we also investigated using a similar approach in the case of a population of K = 2 agents, and a single adversary. This is analogous to a version of PAIRED where there is no fixed antagonist, but the antagonist is flexibly chosen to be the currently best-performing agent. We call this approach Flexible PAIRED. Figure 5 plots the complexity results of these two approaches. We find that they both perform worse than the proposed version of PAIRED. Figure 6 shows the transfer performance of both methods against the original PAIRED. Both methods achieve reasonable transfer performance, retaining the ability to solve complex test tasks like the labyrinth and maze (when domain randomization and minimax training cannot). However, we find that the original method for computing the regret outperforms both approaches. We note that both the combined population and flexible PAIRED approaches do not enable the adversary and antagonist to coordinate (since the antagonist does not remain fixed). Coordination between the antagonist and adversary is an assumption in Theorem 2 of Section 4, which shows that if the agents reach a Nash equilibrium, the protagonist will be playing the minimum regret policy. These empirical results appear to indicate that when coordination between the adversary and antagonist is no longer possible, performance degrades. Future work should investigate whether these results hold across more domains.</p>
<p>E.2 Should agents themselves optimize regret?</p>
<p>We experimented with whether the protagonist and antagonist should optimize for regret, or for the normal reward supplied by the environment (note that the environment-generating adversary always optimizes for the regret). For both the protagonist and antagonist, the regret is based on the difference between their reward for the current trajectory, and the max reward of the other agent received over several trajectories played in the current environment. Let the current agent be A, and the other agent be O. Then agent A should receive reward
R A = U (τ A ) − max τ O U (τ O ) for episode τ A .
However, this reward is likely to be negative. If given at the end of the trajectory, it would essentially punish the agent for reaching the goal, making learning difficult. Therefore we instead compute a per-timestep penalty of
max τ O U (τ O ) T
, where T is the maximum length of an episode. This is subtracted post-hoc While both alternatives still retain the ability to solve complex tasks, achieving superior performance to minimax and domain randomization, their performance is not as good as the proposed method for computing the regret.</p>
<p>from each agent's reward at every step during each episode, after the episode has been collected but before the agent is trained on it. When the agent reaches the goal, it receives the normal Minigrid reward for successfully navigating to the goal, which is R = 1 − 0.9 * (M/T ), where M is the timestep on which the agent found the goal. Figure 7 shows the complexity results for the best performing hyperparameters in which the protagonist and antagonist optimized the regret according to the formula above, or whether they simply learned according to the environment reward. Note that in both cases, the environment-generating adversary optimizes the regret of the protagonist with respect to the antagonist. As is evident in Figure 7, training agents on the environment reward itself, rather than the regret, appears to be more effective for learning complex behavior. We hypothesize this is because the regret is very noisy. The performance of the other agent is stochastic, and variations in the other agent's reward are outside of the agent's control. Further, agents do not receive observations about the other agent, and cannot use them to determine what is causing the reward to vary. However, we note that optimizing for the regret can provide good transfer performance. The transfer plots in Figure 3 were created with an agent that optimized for regret, as we describe below. It is possible that as the other agent converges, the regret provides a more reliable signal indicating when the agent's performance is sub-optimal.  -c), and the length of mazes that agents are able to solve. Neither the combined population or flexible approach show improved complexity. Statistics of generated and solved environments measured over five random seeds; error bars are a 95% CI.</p>
<p>the direction the agent is currently facing. The agents use a network architecture consisting of a single convolutional layer which connects to an LSTM, and then to two fully connected layers which connect to the policy outputs. A second network with identical parameters is used to estimate the value function. The agents use convolutional kernels of size 3 with 16 filters to input the view of the environment, an extra fully-connected layer of size 5 to process the direction and input it to the LSTM, an LSTM of size 256, and two fully connected layers of size 32 which connect to either the policy outputs or the value estimate. The best entropy regularization coefficient was found to be 0.0. All agents (including environment adversaries) are trained with PPO with a discount factor of 0.995, a learning rate of 0.0001, and 30 environment workers operating in parallel to collect a batch of episodes, which is used to complete one training update.</p>
<p>F.1.2 Adversary parameterization</p>
<p>The environments explored in this paper are a 15 × 15 tile discrete grid, with a border of walls around the edge. This means there are 13 × 13 = 169 free tiles for the adversary to use to place obstacles. We parameterized the adversary by giving it an action space of dimensionality 169, and each discrete action indicates the location of the next object to be placed. It plays a sequence of actions, such that on the first step it places the agent, on the second it places the goal, and for 50 steps afterwards it places a wall (obstacle). If the adversary places an object on top of a previously existing object, its action does nothing; this allows it to place fewer than 50 obstacles. If it tries to place the goal on top of the agent, the goal will be placed randomly.</p>
<p>We also explored an alternative parameterization, in which the adversary had only 4 actions, which corresponded to placing the agent, goal, an obstacle, or nothing. It then took a sequence of 169 steps to place all objects, moving through the grid from top to bottom and left to right. If it chose to place the agent or goal when they had already been placed elsewhere in the map, they would be moved to the current location. This parameterization allows the adversary to place as many blocks as there are squares in the map. However, we found that adversaries trained with this parameterization drastically underperformed the alternative version used in the paper, scoring an average solved path length of ≈ 2, as opposed to ≈ 15. We hypothesize this is because when the adversary is randomly initialized, sampling from its random policy is more likely to produce impossible environments. This makes it impossible for the agents to learn, and cannot provide a regret signal to the adversary to allow it to improve. We suggest that when designing an adversary parameterization, it may be important to ensure that sampling from a random adversary policy can produce feasible environments.</p>
<p>The environment-constructing adversary's observations consist of a 15 × 15 × 3 image of the state of the environment, an integer t representing the current timestep, and a random vector z ∼ N (0, I), z ∈ R 50 to allow it to generate random mazes. Because the sequencing of actions is important, we experiment with using an RNN to parameterize the adversary as well, although we find it is not always necessary. The adversary architecture is similar to that of the agents; it consists of a single convolutional layer which connects to an LSTM, and then to two fully connected layers which connect to the policy outputs. Additional inputs such as t and z are connected directly to the LSTM layer. We use a second, identical network to estimate the value function. To find the best hyperparameters for PAIRED and the baselines, we perform a grid search over the number of convolution filters used by the adversary, the degree of entropy regularization, which of the two parameterizations to use, and whether or not to use an RNN, and finally, the number of steps the protagonist is given to find the goal (we reasoned that lowering the protagonist episode length relative to the antagonist length would make the regret a less noisy signal). These parameters are shared between PAIRED and the baseline Minimax adversary, and we sweep all of these parameters equally for both. For PAIRED, we also experiment with whether the protagonist and antagonist optimize regret, and with using a non-negative regret signal, i.e. REGRET = max(0, REGRET), reasoning this could also lead to a less noisy reward signal.</p>
<p>For the complexity experiments, we chose the parameters that resulted in the highest solved path length in the last 20% of training steps. The best parameters for PAIRED were an adversary with 128 convolution filters, entropy regularization coefficient of 0.0, protagonist episode length of 250, non-negative regret, and the agents did not optimize regret. The best parameters for the minimax adversary were 256 convolution filters, entropy regularization of 0.0, and episode length of 250. For the Population-based-training (PBT) Minimax experiment, the best parameters were an adversary population of size 3, and an agent population of size 3. All adversaries used a convolution kernel size of 3, an LSTM size of 256, two fully connected layers of size 32 each, and a fully connected layer of size 10 that inputs the timestep and connects to the LSTM.</p>
<p>For the transfer experiments, we first limited the hyperparameters we searched based on those which produced the highest adversary reward, reasoning that these were experiments in which the optimization objective was achieved most effectively. We tested a limited number of hyperparameter settings on a set of validation environments, consisting of a different maze and labyrinth, mazes with 5 blocks and 40 blocks, and a nine rooms environment. We then tested these parameters on novel test environments to produce the scores in the paper. The best parameters for the PAIRED adversary were found to be 64 convolution filters, entropy regularization of 0.1, no RNN, non-negative regret, and having the agents themselves optimize regret. The best parameters for the minimax adversary in this experiment were found to be the same: 64 convolution filters, entropy regularization of 0.1, and no RNN.</p>
<p>F.2 Hopper Experiments</p>
<p>The hopper experiments in this paper are in the standard MuJoCo [42] simulator. The adversary is allowed to apply additional torques to the joints of the agent at a some proportion of the original agent's strength α. The torques that are applied at each time step are chosen by the adversary independent of the state of the environment, and in the PAIRED experiments, both the protagonist and the antagonist are given the same torques.</p>
<p>The adversaries observation consists of only of the the time step. Each policy is a DNN with two hidden layers each with width 32, tanh activation internally and a linear activation on the output layer. They were trained simultaneously using PPO [35] and an schedule in the adversary strength parameter α is scaled from 0.1 to 1.0 over the course of 300 iterations, after which training continues at full strength. We pre-train agents without any adversary for 100 iterations.</p>
<p>Hyperparameter tuning was conducted over the learning rate values [5e-3, 5e-4, 5e-5] and the GAE lambda values [0.5, 0.9]. The minimax adversary worked best with a leaning rate of 5e-4 and a lambda value of 0.5; PAIRED worked best with a leaning rate of 5e-3 and a lambda value of 0.09. Otherwise we use the standard hyperparameters in Ray 0.8.</p>
<p>Figure 2 :
2Statistics of generated environments in terms of the number of blocks (a), distance from the start position to the goal (b)</p>
<p>Figure 4 :
4Training curves for PAIRED and minimax adversaries on the MuJoCo hopper, where adversary strength is scaled up over the first 300 iterations (a). While both adversaries reduce the agent's reward by making the task more difficult, the minimax adversary is incentivized to drive the agent's reward as low as possible. In contrast, the PAIRED adversary must maintain feasible parameters. When varying force and mass are applied at test time, the minimax policy performs poorly in cumulative reward (b), while PAIRED is more robust (c).</p>
<p>Figure 5 :Figure 6 :
56Comparison of alternative methods for computing the regret in terms of the statistics of generated environments (a-c), and the length of mazes that agents are able to solve (d). Neither the combined population or flexible approach (which both break the coordination assumption in Theorem 2) show improved complexity. Statistics of generated and solved environments measured over five random seeds; error bars are a 95% CI. Comparison of transfer performance for alternative methods for computing the regret.</p>
<p>FFigure 7 :
7.1.1 Agent parameterizationThe protagonist and antagonist agents for both PAIRED and the baselines received a partially observed, 5 × 5 × 3 view of the environment, as well as an integer ranging from 0 − 3 representing Comparison of alternative methods for computing the regret in terms of the statistics of generated environments (a</p>
<p>Table 1 :
1Example Environment Policies 
UED Technique 
Environment Policy 
Decision Rule </p>
<p>Domain Randomization [17, 32, 41] Λ DR (π) = U(Θ T ) 
Insufficient Reason 
Minimax Adversary </p>
<p>Table 3 :
3In these setting, SUCCESS is scoring in the range [75, 100] and FAILURE is scoring in the range [−1, 0]
Acknowledgments and Disclosure of FundingWe would like to thank Michael Chang, Marvin Zhang, Dale Schuurmans, Aleksandra Faust, Chase Kew, Jie Tan, Dennis Lee, Kelvin Xu, Abhishek Gupta, Adam Gleave, Rohin Shah, Daniel Filan, Lawrence Chan, Sam Toyer, Tyler Westenbroek, Igor Mordatch, Shane Gu, DJ Strouse, and Max Kleiman-Weiner for discussions that contributed to this work. We are grateful for funding of this work as a gift from the Berkeley Existential Risk Intuitive. We are also grateful to Google Research for funding computation expenses associated with this work.
Learning dexterous in-hand manipulation. Bowen Openai: Marcin Andrychowicz, Maciek Baker, Rafal Chociej, Bob Jozefowicz, Jakub Mc-Grew, Arthur Pachocki, Matthias Petron, Glenn Plappert, Alex Powell, Ray, The International Journal of Robotics Research. 391OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob Mc- Grew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.</p>
<p>Reinforcement learning for pivoting task. Rika Antonova, Silvia Cruciani, Christian Smith, Danica Kragic, arXiv:1703.00472arXiv preprintRika Antonova, Silvia Cruciani, Christian Smith, and Danica Kragic. Reinforcement learning for pivoting task. arXiv preprint arXiv:1703.00472, 2017.</p>
<p>Theory and applications of adaptive control-a survey. Karl Johan Åström, Automatica. 195Karl Johan Åström. Theory and applications of adaptive control-a survey. Automatica, 19(5): 471-486, 1983.</p>
<p>Solving uncertain markov decision processes. Andrew Bagnell, Y Andrew, Jeff G Ng, Schneider, J Andrew Bagnell, Andrew Y Ng, and Jeff G Schneider. Solving uncertain markov decision processes. 2001.</p>
<p>Emergent tool use from multi-agent autocurricula. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob Mcgrew, Igor Mordatch, arXiv:1909.07528arXiv preprintBowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528, 2019.</p>
<p>Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Sébastien Bubeck, Nicolo Cesa-Bianchi, arXiv:1204.5721arXiv preprintSébastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.</p>
<p>Learning with amigo: Adversarially motivated intrinsic goals. Andres Campero, Roberta Raileanu, Heinrich Küttler, Joshua B Tenenbaum, Tim Rocktäschel, Edward Grefenstette, arXiv:2006.12122arXiv preprintAndres Campero, Roberta Raileanu, Heinrich Küttler, Joshua B Tenenbaum, Tim Rocktäschel, and Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint arXiv:2006.12122, 2020.</p>
<p>The zone of proximal development in vygotsky's analysis of learning and instruction. Vygotsky's educational theory in cultural context. Seth Chaiklin, 1Seth Chaiklin et al. The zone of proximal development in vygotsky's analysis of learning and instruction. Vygotsky's educational theory in cultural context, 1(2):39-64, 2003.</p>
<p>Minimalistic gridworld environment for openai gym. Maxime Chevalier, - Boisvert, Lucas Willems, Suman Pal, Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environ- ment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.</p>
<p>Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, arXiv:1802.06070Diversity is all you need: Learning skills without a reward function. arXiv preprintBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.</p>
<p>The future of employment: How susceptible are jobs to computerisation? Technological forecasting and social change. Benedikt Carl, Michael A Frey, Osborne, 114Carl Benedikt Frey and Michael A Osborne. The future of employment: How susceptible are jobs to computerisation? Technological forecasting and social change, 114:254-280, 2017.</p>
<p>Safe policy improvement by minimizing robust baseline regret. Mohammad Ghavamzadeh, Marek Petrik, Yinlam Chow, Advances in Neural Information Processing Systems. Mohammad Ghavamzadeh, Marek Petrik, and Yinlam Chow. Safe policy improvement by minimizing robust baseline regret. In Advances in Neural Information Processing Systems, pages 2298-2306, 2016.</p>
<p>Adversarial policies: Attacking deep reinforcement learning. Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, Stuart Russell, International Conference on Learning Representations. Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversarial policies: Attacking deep reinforcement learning. In International Conference on Learning Representations, 2020.</p>
<p>Automated curriculum learning for neural networks. Alex Graves, G Marc, Jacob Bellemare, Remi Menick, Koray Munos, Kavukcuoglu, arXiv:1704.03003arXiv preprintAlex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Auto- mated curriculum learning for neural networks. arXiv preprint arXiv:1704.03003, 2017.</p>
<p>Unsupervised meta-learning for reinforcement learning. Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine, arXiv:1806.04640arXiv preprintAbhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised meta-learning for reinforcement learning. arXiv preprint arXiv:1806.04640, 2018.</p>
<p>Robust dynamic programming. N Garud, Iyengar, Mathematics of Operations Research. 302Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2): 257-280, 2005.</p>
<p>Evolutionary robotics and the radical envelope-of-noise hypothesis. Nick Jakobi, Adaptive behavior. 62Nick Jakobi. Evolutionary robotics and the radical envelope-of-noise hypothesis. Adaptive behavior, 6(2):325-368, 1997.</p>
<p>Rawal Khirodkar, Donghyun Yoo, Kris M Kitani, Vadra, arXiv:1812.00491Visual adversarial domain randomization and augmentation. arXiv preprintRawal Khirodkar, Donghyun Yoo, and Kris M Kitani. Vadra: Visual adversarial domain randomization and augmentation. arXiv preprint arXiv:1812.00491, 2018.</p>
<p>Asymptotically efficient adaptive allocation rules. Advances in applied mathematics. Tze Leung, Lai , Herbert Robbins, 6Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Ad- vances in applied mathematics, 6(1):4-22, 1985.</p>
<p>Autocurricula and the emergence of innovation from social interaction: A manifesto for multi-agent intelligence research. Z Joel, Edward Leibo, Marc Hughes, Thore Lanctot, Graepel, arXiv:1903.00742arXiv preprintJoel Z Leibo, Edward Hughes, Marc Lanctot, and Thore Graepel. Autocurricula and the emergence of innovation from social interaction: A manifesto for multi-agent intelligence research. arXiv preprint arXiv:1903.00742, 2019.</p>
<p>Teacher-student curriculum learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, IEEE transactions on neural networks and learning systems. Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning. IEEE transactions on neural networks and learning systems, 2019.</p>
<p>On gradient-based learning in continuous games. Eric Mazumdar, Lillian J Ratliff, S Shankar Sastry, SIAM Journal on Mathematics of Data Science. 21Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. On gradient-based learning in continuous games. SIAM Journal on Mathematics of Data Science, 2(1):103-131, 2020.</p>
<p>Active domain randomization. Bhairav Mehta, Manfred Diaz, Florian Golemo, J Christopher, Liam Pal, Paull, Conference on Robot Learning. PMLRBhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active domain randomization. In Conference on Robot Learning, pages 1162-1176. PMLR, 2020.</p>
<p>John Milnor, Games against nature. Decision Processes. John Milnor. Games against nature. Decision Processes, pages 49-59, 1954.</p>
<p>Robust reinforcement learning. Jun Morimoto, Kenji Doya, Advances in neural information processing systems. Jun Morimoto and Kenji Doya. Robust reinforcement learning. In Advances in neural informa- tion processing systems, pages 1061-1067, 2001.</p>
<p>Robust control of markov decision processes with uncertain transition matrices. Arnab Nilim, Laurent El Ghaoui, Operations Research. 535Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain transition matrices. Operations Research, 53(5):780-798, 2005.</p>
<p>An introduction to decision theory. Martin Peterson, Cambridge University PressMartin Peterson. An introduction to decision theory. Cambridge University Press, 2017.</p>
<p>Supervision via competition: Robot adversaries for learning tasks. Lerrel Pinto, James Davidson, Abhinav Gupta, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEELerrel Pinto, James Davidson, and Abhinav Gupta. Supervision via competition: Robot adver- saries for learning tasks. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 1601-1608. IEEE, 2017.</p>
<p>Robust online optimization of reward-uncertain mdps. Kevin Regan, Craig Boutilier, Twenty-Second International Joint Conference on Artificial Intelligence. Citeseer. Kevin Regan and Craig Boutilier. Robust online optimization of reward-uncertain mdps. In Twenty-Second International Joint Conference on Artificial Intelligence. Citeseer, 2011.</p>
<p>Regret-based reward elicitation for markov decision processes. Kevin Regan, Craig Boutilier, arXiv:1205.2619arXiv preprintKevin Regan and Craig Boutilier. Regret-based reward elicitation for markov decision processes. arXiv preprint arXiv:1205.2619, 2012.</p>
<p>Some aspects of the sequential design of experiments. Herbert Robbins, Bulletin of the American Mathematical Society. 585Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527-535, 1952.</p>
<p>Cad2rl: Real single-image flight without a single real image. Fereshteh Sadeghi, Sergey Levine, arXiv:1611.04201arXiv preprintFereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image flight without a single real image. arXiv preprint arXiv:1611.04201, 2016.</p>
<p>The theory of statistical decision. J Leonard, Savage, Journal of the American Statistical association. 46253Leonard J Savage. The theory of statistical decision. Journal of the American Statistical association, 46(253):55-67, 1951.</p>
<p>Formal theory of creativity, fun, and intrinsic motivation (1990-2010). Jürgen Schmidhuber, IEEE Transactions on Autonomous Mental Development. 23Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE Transactions on Autonomous Mental Development, 2(3):230-247, 2010.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Time-contrastive networks: Self-supervised learning from video. Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, Google Brain, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEPierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1134-1141. IEEE, 2018.</p>
<p>On the adaptive control of robot manipulators. Jean-Jacques E Slotine, Weiping Li, The international journal of robotics research. 63Jean-Jacques E Slotine and Weiping Li. On the adaptive control of robot manipulators. The international journal of robotics research, 6(3):49-59, 1987.</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, Rob Fergus, arXiv:1703.05407arXiv preprintSainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017.</p>
<p>Scaling up robust mdps using function approximation. Aviv Tamar, Shie Mannor, Huan Xu, International Conference on Machine Learning. Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation. In International Conference on Machine Learning, pages 181-189, 2014.</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, Vincent Vanhoucke, arXiv:1804.10332arXiv preprintJie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEEJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23-30. IEEE, 2017.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.</p>
<p>D-malt: Diverse multi-adversarial learning for transfer 003. Eugene Vinitsky, Kanaad Parvate, Yuqing Du, Pieter Abbeel, Alexandre Bayen, Submission to ICML 2020). Eugene Vinitsky, Kanaad Parvate, Yuqing Du, Pieter Abbeel, and Alexandre Bayen. D-malt: Diverse multi-adversarial learning for transfer 003. In Submission to ICML 2020).</p>
<p>Statistical decision functions. Abraham Wald, Abraham Wald. Statistical decision functions. 1950.</p>
<p>Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. Rui Wang, Joel Lehman, Jeff Clune, Kenneth O Stanley, arXiv:1901.01753arXiv preprintRui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019.</p>
<p>Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeff Clune, Kenneth O Stanley, arXiv:2003.08536arXiv preprintRui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeff Clune, and Kenneth O Stanley. Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. arXiv preprint arXiv:2003.08536, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
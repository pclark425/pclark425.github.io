<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9952 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9952</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9952</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-b5069352383579c6464d8e5ec34eab693c45f59a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b5069352383579c6464d8e5ec34eab693c45f59a" target="_blank">FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse- level scoring to a skill set-level scoring for each instruction and observes that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation.</p>
                <p><strong>Paper Abstract:</strong> Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations. We publicly release the evaluation data and code implementation at https://github.com/kaistAI/FLASK.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9952.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9952.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLASK human vs model correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correlation between human-based evaluation and model-based (EVAL LM) evaluation under FLASK</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative comparison showing that fine-grained, skill-specific evaluation (FLASK) increases the agreement between human annotators and LLM-based evaluators, with GPT-4 (EVAL LM) achieving the highest reported correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Task-agnostic instruction-following / general LLM alignment evaluation (FLASK dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (gpt-4-0613) used as EVAL LM; comparisons also run with GPT-3.5 and Claude</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>EVAL LM given: (instruction, reference answer, model response, skill-specific score rubrics). Prompted to generate a rationale (CoT-style) then assign a score 1–5 per annotated skill; also compared with skill-agnostic rubric and ROUGE-L baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>10 human labelers with diverse academic backgrounds; human evaluation used 3 labelers per instance for agreement measurements (human-based scoring 1–5 per skill using same rubrics and reference answers).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman, Kendall-Tau, Pearson correlations between aggregated human scores and EVAL LM scores; FLASK (GPT-4) reported: Spearman 0.680, Kendall-Tau 0.541, Pearson 0.732. ROUGE-L: Spearman 0.333. Skill-agnostic (GPT-4): Spearman 0.641. (Table 1, Section 4)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges loses some human judgment characteristics: while correlations are high, LLM judges show systematic biases (style/verbosity preference) that differ from humans and can misalign absolute scores; reliance on LLM-evaluators without careful rubric/ref-answer/rationale reduces agreement with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>ROUGE-L baseline had much lower correlation (Spearman 0.333). Skill-agnostic model evaluation yielded lower correlation than FLASK's skill-specific rubric. Removing the reference answer in model-based evaluation reduced Spearman correlation from 0.680 to 0.516 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Despite losses, model-based evaluation (especially GPT-4 with FLASK rubrics and rationale) attains strong correlation with humans, indicating LLM judges can closely approximate human judgments when given skill-specific rubrics, reference answers, and instructed to generate rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4 (Reliability of FLASK), Table 1, Figure 2</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9952.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9952.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human central tendency vs model verbosity/style bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differences in systematic biases: human central tendency bias versus LLM evaluator style/verbosity bias</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Describes contrasting evaluation biases: human annotators tend to give middle Likert scores (central tendency) and suffer fatigue, while model evaluators prefer certain writing styles and longer/verbose responses, producing different scoring patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General instruction-following evaluation (FLASK)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 as primary EVAL LM; observations also mention tendencies of GPT-3.5 and Claude when used as evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Model-based evaluation conducted with skill-specific rubrics, reference answers, and enforced rationale generation; comparisons also performed under skill-agnostic rubric and adversarial verbosity changes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labelers scored with same skill rubrics and reference answers; annotator pool described in Section 4; annotators experienced fatigue and central tendency bias.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Qualitative descriptions plus quantitative evidence in score distributions (Figure 2) and robustness to stylistic change experiments (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When using LLMs as judges, subjective nuances reflected in human central tendency and fatigue patterns are lost or changed; LLMs instead introduce style/verbosity bias (favoring responses that match the evaluator model's own generation style and longer responses), which can distort scores relative to human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper notes EVAL LM tends to prefer GPT-3.5 response styles over BARD, unlike human evaluators (Section 4, Figure 2). Model-based evaluation prefers longer, more verbose outputs; adversarially making responses verbose changed model scores while humans were less affected.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Fine-grained rubrics (FLASK) and requiring rationales mitigate some of the style/verbosity biases; instance-specific rubrics are most robust to stylistic changes (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4 (paragraphs on biases), Figure 2, Figure 3, Appendix A.1</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9952.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9952.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inter-labeler agreement contrast</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Krippendorff's alpha and human-model agreement differences for skills</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compares inter-annotator agreement for human annotators versus repeated runs of model evaluators and reports human-model correlations per skill, revealing that model-based evaluation is more internally consistent but can diverge from human judgments on subjective skills.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Skill-level scoring across FLASK skills (12 skills)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 used as EVAL LM for model-model agreement; model-based runs used nondeterministic generation (temperature 1.0) to measure M-M agreement</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Three model-based evaluation runs per instance (for M-M agreement); scoring 1–5 per skill with rubrics and rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three human labelers per instance (for H-H agreement); same scoring protocol and rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha reported: Overall Human-Human (H-H) = 0.488; Model-Model (M-M) = 0.835. Per-skill human-model Pearson correlations reported (Table 2) — example: Logical Correctness H-M = 0.896, Factuality H-M = 0.747, Readability H-M = 0.223.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-as-a-judge reduces inter-annotator variability (higher internal consistency) but this can mask human disagreement and subjective nuance—especially on User Alignment skills (Readability, Conciseness) where human agreement is low and model agreement is high, indicating potential misalignment with human subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Readability: H-H alpha 0.089 (very low), M-M 0.329, H-M correlation 0.223 — humans highly subjective while model shows more consistent but not strongly human-aligned judgments (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>For objective skills (e.g., Logical Correctness), human-model correlation is very high (H-M = 0.896) and model evaluators agree with humans, suggesting LLM judges are reliable for less subjective dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section C.1 (Inter-labeler Agreement), Table 2</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9952.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9952.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation effects: reference/rationale/rubric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of removing reference answer, rationale generation, or score rubric from model-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation analysis showing that omitting the reference answer, rationale requirement, or the skill-specific score rubric substantially reduces the correlation between model-based and human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based evaluation protocol design for FLASK</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (EVAL LM) (ablation rows in Table 1 reflect GPT-4 without certain components)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Standard setup uses reference answer + rationale generation + score rubric; ablations remove one factor at a time to measure effect on agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluations with full information (reference answer and rubrics) serve as the ground truth for correlation measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman / Kendall-Tau / Pearson correlations reported in Table 1. Full FLASK (GPT-4): Spearman 0.680. Without reference answer: Spearman drops to 0.516. Without rationale: 0.634. Without score rubric: 0.646.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Dropping these supporting elements degrades alignment of LLM judges with humans — especially loss of the reference answer causes the largest drop, indicating model judges rely strongly on having the reference to match human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Removing the reference answer reduced Spearman correlation from 0.680 to 0.516 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Even without one component the correlations remain non-trivial; including all components (reference answer, rationale, rubric) yields the best alignment, showing that methodological choices can mitigate LLM judge shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4, Table 1</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9952.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9952.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robustness to stylistic changes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM judge robustness vs human under stylistic (verbosity) perturbations and the mitigating effect of fine-grained rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiment showing model-based evaluators are sensitive to stylistic changes (verbosity) but that increasing evaluation fine-graininess (skill-specific → instance-specific rubrics) improves robustness of LLM scoring to such changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>FLASK-HARD subset and stylistic robustness experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>EVAL LM (GPT-4) primary for FLASK; adversarial responses created by GPT-3.5 to be more verbose; model-based scoring compared across rubric granularities</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Original GPT-3.5 responses on FLASK-HARD and adversarial verbose variants; evaluators (EVAL LM) scored both versions under three rubric granularities: skill-agnostic, skill-specific (FLASK), instance-specific (FLASK-HARD). Robustness measured as fraction of identical scores assigned despite stylistic change.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Not explicitly quantified here for stylistic perturbation experiment (focus is on model-based robustness), but human evaluations are discussed as having central tendency and fatigue.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Robustness ratio (same-score ratio under stylistic perturbation) visualized in Figure 3; instance-specific rubric shows highest robustness, skill-agnostic lowest.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using coarse, skill-agnostic LLM judging is especially vulnerable to stylistic manipulations (verbosity) and can change scores where human judgments would be more stable; much of this vulnerability is mitigated by moving to fine-grained, instance-specific rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Adversarial verbosity (making GPT-3.5 responses more verbose) led to larger score inconsistencies under skill-agnostic rubrics; instance-specific rubrics produced the most stable scores (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Instance-specific rubrics are more robust but costly (require extra annotation and manual validation); FLASK skill-specific rubrics provide a practical middle ground that improves robustness over skill-agnostic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 4 (Robustness to stylistic changes), Figure 3, Section 3.4 (FLASK-HARD)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9952.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9952.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Practical tradeoffs and limitations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Practical tradeoffs when replacing humans with LLMs as evaluators (efficiency vs subjectivity and coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summarizes the paper's stated tradeoffs: LLM-based evaluation is faster, cheaper, and internally more consistent, but loses human subjectivity, is biased by evaluator style/verbosity, and cannot fully substitute human judgment especially on subjective skills and difficult instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General LLM evaluation practice (FLASK recommendations)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 as recommended EVAL LM for model-based evaluation in the paper, with comparisons to GPT-3.5 and Claude</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Model-based evaluation with rationale generation, score rubrics, and reference answers; used to run large-scale automatic analysis across the 1,740-instance FLASK dataset with emphasis on scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labeling used for validation and a 200-instance subset comparison; full human evaluation found to be costly and limited in scale.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Multiple: correlation metrics (Table 1), Krippendorff's alpha for inter-annotator agreement (Table 2), qualitative assessments of bias and fatigue (Section 4, Appendix A.1).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Losses include: reduced capture of subjective human preferences (user-alignment skills), risk of evaluator-model style/verbosity bias, and potential misalignment on hard, expert-level instances; model-based evaluation may mask human disagreement by being overly consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>User Alignment skills (Readability, Conciseness) show low human-human agreement and low human-model correlation, suggesting LLM judges cannot reliably emulate human subjective judgments (Table 2). FLASK-HARD evaluations show instance-specific rubrics produce lower scores, highlighting stricter human-like evaluation requirements that models struggle with (Section 5).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Model-based evaluation is efficient and, when carefully designed (FLASK rubrics + reference answers + rationale), highly correlated with human labels for many skills (especially objective ones); paper recommends combining both human and model evaluators rather than replacing humans entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 4 and 5, Appendix A.1 (Limitations of Evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An automatic evaluator of instruction-following models <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9952",
    "paper_id": "paper-b5069352383579c6464d8e5ec34eab693c45f59a",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "FLASK human vs model correlation",
            "name_full": "Correlation between human-based evaluation and model-based (EVAL LM) evaluation under FLASK",
            "brief_description": "Quantitative comparison showing that fine-grained, skill-specific evaluation (FLASK) increases the agreement between human annotators and LLM-based evaluators, with GPT-4 (EVAL LM) achieving the highest reported correlations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Task-agnostic instruction-following / general LLM alignment evaluation (FLASK dataset)",
            "llm_judge_model": "GPT-4 (gpt-4-0613) used as EVAL LM; comparisons also run with GPT-3.5 and Claude",
            "llm_judge_setup": "EVAL LM given: (instruction, reference answer, model response, skill-specific score rubrics). Prompted to generate a rationale (CoT-style) then assign a score 1–5 per annotated skill; also compared with skill-agnostic rubric and ROUGE-L baseline.",
            "human_evaluation_setup": "10 human labelers with diverse academic backgrounds; human evaluation used 3 labelers per instance for agreement measurements (human-based scoring 1–5 per skill using same rubrics and reference answers).",
            "agreement_metric": "Spearman, Kendall-Tau, Pearson correlations between aggregated human scores and EVAL LM scores; FLASK (GPT-4) reported: Spearman 0.680, Kendall-Tau 0.541, Pearson 0.732. ROUGE-L: Spearman 0.333. Skill-agnostic (GPT-4): Spearman 0.641. (Table 1, Section 4)",
            "losses_identified": "Using LLMs as judges loses some human judgment characteristics: while correlations are high, LLM judges show systematic biases (style/verbosity preference) that differ from humans and can misalign absolute scores; reliance on LLM-evaluators without careful rubric/ref-answer/rationale reduces agreement with humans.",
            "examples_of_loss": "ROUGE-L baseline had much lower correlation (Spearman 0.333). Skill-agnostic model evaluation yielded lower correlation than FLASK's skill-specific rubric. Removing the reference answer in model-based evaluation reduced Spearman correlation from 0.680 to 0.516 (Table 1).",
            "counterexamples_or_caveats": "Despite losses, model-based evaluation (especially GPT-4 with FLASK rubrics and rationale) attains strong correlation with humans, indicating LLM judges can closely approximate human judgments when given skill-specific rubrics, reference answers, and instructed to generate rationales.",
            "paper_reference": "Section 4 (Reliability of FLASK), Table 1, Figure 2",
            "uuid": "e9952.0",
            "source_info": {
                "paper_title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Human central tendency vs model verbosity/style bias",
            "name_full": "Differences in systematic biases: human central tendency bias versus LLM evaluator style/verbosity bias",
            "brief_description": "Describes contrasting evaluation biases: human annotators tend to give middle Likert scores (central tendency) and suffer fatigue, while model evaluators prefer certain writing styles and longer/verbose responses, producing different scoring patterns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "General instruction-following evaluation (FLASK)",
            "llm_judge_model": "GPT-4 as primary EVAL LM; observations also mention tendencies of GPT-3.5 and Claude when used as evaluators",
            "llm_judge_setup": "Model-based evaluation conducted with skill-specific rubrics, reference answers, and enforced rationale generation; comparisons also performed under skill-agnostic rubric and adversarial verbosity changes.",
            "human_evaluation_setup": "Human labelers scored with same skill rubrics and reference answers; annotator pool described in Section 4; annotators experienced fatigue and central tendency bias.",
            "agreement_metric": "Qualitative descriptions plus quantitative evidence in score distributions (Figure 2) and robustness to stylistic change experiments (Figure 3).",
            "losses_identified": "When using LLMs as judges, subjective nuances reflected in human central tendency and fatigue patterns are lost or changed; LLMs instead introduce style/verbosity bias (favoring responses that match the evaluator model's own generation style and longer responses), which can distort scores relative to human judgment.",
            "examples_of_loss": "Paper notes EVAL LM tends to prefer GPT-3.5 response styles over BARD, unlike human evaluators (Section 4, Figure 2). Model-based evaluation prefers longer, more verbose outputs; adversarially making responses verbose changed model scores while humans were less affected.",
            "counterexamples_or_caveats": "Fine-grained rubrics (FLASK) and requiring rationales mitigate some of the style/verbosity biases; instance-specific rubrics are most robust to stylistic changes (Figure 3).",
            "paper_reference": "Section 4 (paragraphs on biases), Figure 2, Figure 3, Appendix A.1",
            "uuid": "e9952.1",
            "source_info": {
                "paper_title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Inter-labeler agreement contrast",
            "name_full": "Krippendorff's alpha and human-model agreement differences for skills",
            "brief_description": "Compares inter-annotator agreement for human annotators versus repeated runs of model evaluators and reports human-model correlations per skill, revealing that model-based evaluation is more internally consistent but can diverge from human judgments on subjective skills.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Skill-level scoring across FLASK skills (12 skills)",
            "llm_judge_model": "GPT-4 used as EVAL LM for model-model agreement; model-based runs used nondeterministic generation (temperature 1.0) to measure M-M agreement",
            "llm_judge_setup": "Three model-based evaluation runs per instance (for M-M agreement); scoring 1–5 per skill with rubrics and rationale.",
            "human_evaluation_setup": "Three human labelers per instance (for H-H agreement); same scoring protocol and rubrics.",
            "agreement_metric": "Krippendorff's alpha reported: Overall Human-Human (H-H) = 0.488; Model-Model (M-M) = 0.835. Per-skill human-model Pearson correlations reported (Table 2) — example: Logical Correctness H-M = 0.896, Factuality H-M = 0.747, Readability H-M = 0.223.",
            "losses_identified": "LLM-as-a-judge reduces inter-annotator variability (higher internal consistency) but this can mask human disagreement and subjective nuance—especially on User Alignment skills (Readability, Conciseness) where human agreement is low and model agreement is high, indicating potential misalignment with human subjectivity.",
            "examples_of_loss": "Readability: H-H alpha 0.089 (very low), M-M 0.329, H-M correlation 0.223 — humans highly subjective while model shows more consistent but not strongly human-aligned judgments (Table 2).",
            "counterexamples_or_caveats": "For objective skills (e.g., Logical Correctness), human-model correlation is very high (H-M = 0.896) and model evaluators agree with humans, suggesting LLM judges are reliable for less subjective dimensions.",
            "paper_reference": "Section C.1 (Inter-labeler Agreement), Table 2",
            "uuid": "e9952.2",
            "source_info": {
                "paper_title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Ablation effects: reference/rationale/rubric",
            "name_full": "Impact of removing reference answer, rationale generation, or score rubric from model-based evaluation",
            "brief_description": "Ablation analysis showing that omitting the reference answer, rationale requirement, or the skill-specific score rubric substantially reduces the correlation between model-based and human evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Model-based evaluation protocol design for FLASK",
            "llm_judge_model": "GPT-4 (EVAL LM) (ablation rows in Table 1 reflect GPT-4 without certain components)",
            "llm_judge_setup": "Standard setup uses reference answer + rationale generation + score rubric; ablations remove one factor at a time to measure effect on agreement.",
            "human_evaluation_setup": "Human evaluations with full information (reference answer and rubrics) serve as the ground truth for correlation measurement.",
            "agreement_metric": "Spearman / Kendall-Tau / Pearson correlations reported in Table 1. Full FLASK (GPT-4): Spearman 0.680. Without reference answer: Spearman drops to 0.516. Without rationale: 0.634. Without score rubric: 0.646.",
            "losses_identified": "Dropping these supporting elements degrades alignment of LLM judges with humans — especially loss of the reference answer causes the largest drop, indicating model judges rely strongly on having the reference to match human judgments.",
            "examples_of_loss": "Removing the reference answer reduced Spearman correlation from 0.680 to 0.516 (Table 1).",
            "counterexamples_or_caveats": "Even without one component the correlations remain non-trivial; including all components (reference answer, rationale, rubric) yields the best alignment, showing that methodological choices can mitigate LLM judge shortcomings.",
            "paper_reference": "Section 4, Table 1",
            "uuid": "e9952.3",
            "source_info": {
                "paper_title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Robustness to stylistic changes",
            "name_full": "LLM judge robustness vs human under stylistic (verbosity) perturbations and the mitigating effect of fine-grained rubrics",
            "brief_description": "Experiment showing model-based evaluators are sensitive to stylistic changes (verbosity) but that increasing evaluation fine-graininess (skill-specific → instance-specific rubrics) improves robustness of LLM scoring to such changes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "FLASK-HARD subset and stylistic robustness experiments",
            "llm_judge_model": "EVAL LM (GPT-4) primary for FLASK; adversarial responses created by GPT-3.5 to be more verbose; model-based scoring compared across rubric granularities",
            "llm_judge_setup": "Original GPT-3.5 responses on FLASK-HARD and adversarial verbose variants; evaluators (EVAL LM) scored both versions under three rubric granularities: skill-agnostic, skill-specific (FLASK), instance-specific (FLASK-HARD). Robustness measured as fraction of identical scores assigned despite stylistic change.",
            "human_evaluation_setup": "Not explicitly quantified here for stylistic perturbation experiment (focus is on model-based robustness), but human evaluations are discussed as having central tendency and fatigue.",
            "agreement_metric": "Robustness ratio (same-score ratio under stylistic perturbation) visualized in Figure 3; instance-specific rubric shows highest robustness, skill-agnostic lowest.",
            "losses_identified": "Using coarse, skill-agnostic LLM judging is especially vulnerable to stylistic manipulations (verbosity) and can change scores where human judgments would be more stable; much of this vulnerability is mitigated by moving to fine-grained, instance-specific rubrics.",
            "examples_of_loss": "Adversarial verbosity (making GPT-3.5 responses more verbose) led to larger score inconsistencies under skill-agnostic rubrics; instance-specific rubrics produced the most stable scores (Figure 3).",
            "counterexamples_or_caveats": "Instance-specific rubrics are more robust but costly (require extra annotation and manual validation); FLASK skill-specific rubrics provide a practical middle ground that improves robustness over skill-agnostic evaluation.",
            "paper_reference": "Section 4 (Robustness to stylistic changes), Figure 3, Section 3.4 (FLASK-HARD)",
            "uuid": "e9952.4",
            "source_info": {
                "paper_title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Practical tradeoffs and limitations",
            "name_full": "Practical tradeoffs when replacing humans with LLMs as evaluators (efficiency vs subjectivity and coverage)",
            "brief_description": "Summarizes the paper's stated tradeoffs: LLM-based evaluation is faster, cheaper, and internally more consistent, but loses human subjectivity, is biased by evaluator style/verbosity, and cannot fully substitute human judgment especially on subjective skills and difficult instances.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "General LLM evaluation practice (FLASK recommendations)",
            "llm_judge_model": "GPT-4 as recommended EVAL LM for model-based evaluation in the paper, with comparisons to GPT-3.5 and Claude",
            "llm_judge_setup": "Model-based evaluation with rationale generation, score rubrics, and reference answers; used to run large-scale automatic analysis across the 1,740-instance FLASK dataset with emphasis on scalability.",
            "human_evaluation_setup": "Human labeling used for validation and a 200-instance subset comparison; full human evaluation found to be costly and limited in scale.",
            "agreement_metric": "Multiple: correlation metrics (Table 1), Krippendorff's alpha for inter-annotator agreement (Table 2), qualitative assessments of bias and fatigue (Section 4, Appendix A.1).",
            "losses_identified": "Losses include: reduced capture of subjective human preferences (user-alignment skills), risk of evaluator-model style/verbosity bias, and potential misalignment on hard, expert-level instances; model-based evaluation may mask human disagreement by being overly consistent.",
            "examples_of_loss": "User Alignment skills (Readability, Conciseness) show low human-human agreement and low human-model correlation, suggesting LLM judges cannot reliably emulate human subjective judgments (Table 2). FLASK-HARD evaluations show instance-specific rubrics produce lower scores, highlighting stricter human-like evaluation requirements that models struggle with (Section 5).",
            "counterexamples_or_caveats": "Model-based evaluation is efficient and, when carefully designed (FLASK rubrics + reference answers + rationale), highly correlated with human labels for many skills (especially objective ones); paper recommends combining both human and model evaluators rather than replacing humans entirely.",
            "paper_reference": "Sections 4 and 5, Appendix A.1 (Limitations of Evaluators)",
            "uuid": "e9952.5",
            "source_info": {
                "paper_title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "AlpacaEval: An automatic evaluator of instruction-following models",
            "rating": 2
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 1
        }
    ],
    "cost": 0.01624525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FLASK: Fine-Grained Language Model Evaluation based on Alignment SKill Sets</h1>
<p>Seonghyeon Ye<em> Doyoung Kim ${ }^{</em>}$ Sungdong Kim Hyeonbin Hwang<br>Seungone Kim Yongrae Jo James Thorne Juho Kim Minjoon Seo<br>KAIST</p>
<h4>Abstract</h4>
<p>Evaluation of Large Language Models (LLMs) is challenging because instructionfollowing necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations ${ }^{1}$.</p>
<h2>1 INTRODUCTION</h2>
<p>Large Language Models (LLMs) have shown an impressive capability of following user instructions by aligning to human values, such as responding in a helpful, honest, and harmless manner (Ouyang et al., 2022; Bai et al., 2022a;b; Kim et al., 2023c; Korbak et al., 2023; Askell et al., 2021). In particular, techniques such as instruction tuning or reinforcement learning from human feedback (RLHF) have significantly improved this ability by fine-tuning a pretrained LLM on diverse tasks or user preferences (Ouyang et al., 2022; Chung et al., 2022; Wang et al., 2022b). However, evaluating the alignment of LLMs to human values is challenging for two reasons. First, open-ended user instructions usually require a composition of multiple abilities, which makes measurement with a single metric insufficient. Second, since these instructions are task-agnostic, the required abilities often vary from one instance to another, making it impractical to use a fixed set of metrics.</p>
<p>Currently, the evaluation of LLMs primarily relies on multiple independent benchmarks using automatic metrics (accuracy, ROUGE, etc.) or overall scoring to the model response based on human or model-based preference (Longpre et al., 2023a; Wang et al., 2023b; Ouyang et al., 2022; Zheng et al., 2023). However, both evaluation settings are insufficient. Benchmarks that adopt multiple metrics are not scalable since each of them targets different skills, domains, and difficulties such as GSM8K (Cobbe et al., 2021) for logical correctness, and TruthfulQA (Lin et al., 2022) for truthfulness. Also, relying on these automatic metrics limits interpretability and reliability because only task-wise analysis is possible and automatic metrics are sensitive to surface forms (Krishna et al., 2021). Moreover, merely assigning a single score based on preferences does not tell the whole story because there could be multiple axes to evaluate the response, such as completeness, factuality, etc. Instead, we need to evaluate the model's performance using fine-grained criteria to comprehend the model from various perspectives. Although many recent works have studied multi-metric or finegrained evaluation of LLMs, they mainly focus on a fixed metric set across instances for specific tasks, which is not applicable to the task-agnostic evaluation setting for LLM alignment (Liu et al., 2023; Liang et al., 2022; Lee et al., 2022; Min et al., 2023; Krishna et al., 2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Skill-agnostic evaluation gives a single overall score for the model response, which limits interpretability. (b) Fine-grained evaluation of FLASK first annotates fine-grained metadata for each instruction and conducts evaluation by assigning a score to each skill based on skill-specific or instance-specific score rubrics.
To address the limitations of current evaluation settings, we propose FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets), a novel evaluation protocol that adopts a fine-grained scoring setup, enabling task-agnostic skill evaluation aligned with the provided instructions. We define 4 primary abilities which are divided into 12 fine-grained skills for comprehensive language model evaluation: Logical Thinking (Logical Correctness, Logical Robustness, Logical Efficiency), Background Knowledge (Factuality, Commonsense Understanding), Problem Handling (Comprehension, Insightfulness, Completeness, Metacognition), and User Alignment (Conciseness, Readability, Harmlessness). First, we collect a total of 1,740 evaluation instances from various NLP datasets and annotate the relevant set of skills (a skill set), domains, and the difficulty level for each instance. Then, evaluators assign scores ranging from 1 to 5 for each annotated skill based on the reference answer and skill-specific scoring rubrics, where the evaluators could be human evaluators or state-of-the-art LLMs ${ }^{2}$. For the 89 instances that are labeled to be most difficult (FLASK-HARD), we additionally introduce adopting even a more fine-grained evaluation by using instance-specific rubrics. The overall illustration is shown in Figure 1.
By applying FLASK, we compare and analyze various open-source and proprietary LLMs depending on the skill set, target domain, and difficulty. We conduct both human-based and model-based evaluations and observe that their results are highly correlated. We experimentally observe that applying fine-grained evaluation not only leads to better interpretability but also better reliability, increasing the correlation between human and model evaluation and mitigating the bias of modelbased evaluation. Also, by conducting extensive analysis based on automatic model-based evaluation, we present several findings:</p>
<ul>
<li>We observe that current open-source LLMs significantly underperform proprietary LLMs for Logical Thinking and Background Knowledge abilities.</li>
<li>We observe that some skills such as Logical Correctness and Logical Efficiency require larger model sizes to effectively acquire them compared to other skills.</li>
<li>We show that even state-of-the-art proprietary LLMs struggle on FLASK-HARD set, up to 50\% performance degradation for some skills compared to the whole FLASK evaluation set.</li>
</ul>
<p>We suggest that comprehensive analysis of LLMs through fine-grained evaluation is important and practical for both the developers and practitioners. For model developers, FLASK facilitates accurate interpretation of the model's current state, providing clear guidance for improving model alignment. For practitioners, FLASK's fine-grained comparison of different LLMs helps recommend suitable models for specific situations.</p>
<h1>2 Related Works</h1>
<p>Holistic Evaluation of LLMs Holistic evaluation of LLMs is crucial for assessing model strengths, weaknesses, and potential risks (Shevlane et al., 2023; Liang et al., 2022; Gehrmann et al., 2022; Chia et al., 2023; Laskar et al., 2023). To comprehensively evaluate the performance of LLMs,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>many works have assessed models on multiple independent benchmarks using automated metrics, such as accuracy for knowledge/reasoning tasks or ROUGE for long-form text generation (Chung et al., 2022; Hendrycks et al., 2020; Suzgun et al., 2022; Wang et al., 2022c; Gao et al., 2021; Zhong et al., 2023). To assess multiple aspects of the model response, multi-metric evaluation settings have been proposed, providing a more comprehensive perspective of the model performance beyond accuracy (Liang et al., 2022; Thoppilan et al., 2022; Fu et al., 2023; Jain et al., 2023; Lee et al., 2022). Furthermore, to faithfully evaluate LLMs on tasks such as fact verification or summarization, recent works have proposed fine-grained atomic evaluation settings (Min et al., 2023; Krishna et al., 2023). Especially, Wu et al. (2023a); Lightman et al. (2023) show that fine-grained evaluation of model responses could be utilized for better rewards. In FLASK, we adopt an instance-wise fine-grained multi-metric setting, which distinguishes it from previous works and is more applicable to evaluate the general capabilities of LLMs.</p>
<p>Alignment of LLMs Aligning pre-trained LLMs to human values can be achieved through different fine-tuning techniques such as supervised instruction tuning or reinforcement learning from human feedback (RLHF). For instruction tuning, various techniques have shown effectiveness such as task and model scaling (Mishra et al., 2022; Wei et al., 2021; Wang et al., 2022c; Chung et al., 2022), dataset distillation (Chiang et al., 2023; Taori et al., 2023; Xu et al., 2023; Dettmers et al., 2023; Geng et al., 2023; Gao et al., 2023; Zhang et al., 2023), instruction generation (Ye et al., 2022b; Honovich et al., 2022b), data augmentation through model-generated response (Wang et al., 2022b; Honovich et al., 2022a; Kim et al., 2023b), multilingual instruction tuning (Muennghoff et al., 2022) or in-context instruction learning (Ye et al., 2023a). For RLHF, techniques such as training on synthetic feedback (Bai et al., 2022b; Kim et al., 2023c) or applying reinforcement learning during pretraining (Korbak et al., 2023) have shown to better control the model's response to make LLMs aligned to human values. However, a comprehensive comparison between various user-aligned models trained with different techniques is yet to be studied in sufficient detail.</p>
<h1>3 FLASK: Fine-Grained Language Model Evaluation Protocol</h1>
<p>We introduce FLASK, a fine-grained skill set-based evaluation protocol for assessing the alignment of language models. We define 4 primary abilities, divided into 12 skills, that are necessary to follow user instructions in a desirable manner (Section 3.1). We specify the process of the evaluation dataset construction (Section 3.2) and the evaluation process (Section 3.3). Additionally, for a challenging scenario, we introduce FLASK-HARD (Section 3.4). The illustration of the overall process is shown in Figure 21 in the Appendix. We emphasize that applying instance-wise multi-metric evaluation is what mainly distinguishes our work from previous evaluation settings, enabling task-agnostic evaluation. In this work, we consider two types of evaluators: human evaluators and EVAL LM, one of the state-of-the-art LLMs used for evaluation.</p>
<h3>3.1 SKILL SET CATEGORIZATION</h3>
<p>Building on previous research in language model evaluation, (Sugawara \&amp; Aizawa, 2016; Sugawara et al., 2017; Radziwill \&amp; Benton, 2017; Schlegel et al., 2020; Rogers et al., 2021), we aim to develop a comprehensive taxonomy for assessing the performance of LLMs. This taxonomy is designed as a systematic framework to categorize the essential skills for understanding and responding to a wide range of single-turn English instructions. Based on the skill categorization of Rogers et al. (2021) which was specifically proposed for question answering and reading comprehension, we recategorize skills suitable for LLM alignment. Our proposed categorization includes four primary abilities, each of which is further divided into 2-4 skills, resulting in a total of 12 skills:</p>
<ul>
<li>Logical Thinking refers to the ability to apply reasoning, critical thinking, and deductive skills when processing and responding to instructions. In order to do so, models should generate a logically correct final answer (LOGICAL CORRECTNESS) while preserving generalizability during the step-by-step logical process without any contradiction (LOGICAL ROBUSTNESS). Also, the logical process should be efficient and not contain any unnecessary steps (LOGICAL EFFICIENCY).</li>
<li>Background Knowledge comprises the capacity to generate responses by accessing a broad repository of general and domain-specific information. This ability requires the model to provide</li>
</ul>
<p>accurate and contextually relevant responses to instructions requiring factual (FACTUALITY) or commonsense knowledge (COMMONSENSE UNDERSTANDING).</p>
<ul>
<li>Problem Handling pertains to the proficiency in addressing challenges that emerge while processing and responding to user instructions. This category encompasses the capacity to understand the implicit and explicit purpose and requirements of the instruction (COMPREHENSION), develop creative perspectives or interpretations of the instruction (INSIGHTFULNESS), handle the instruction by providing in-depth and in-breadth information (COMPLETENESS), and be aware of its own capability to answer the instruction (METACOGNITION).</li>
<li>User Alignment represents the ability to empathize with the user and align its responses to the user's intentions, preferences, and expectations. This category encompasses the model's ability to structure the answer to promote the users' readability (READABILITY), presenting a concise response for the reader without unnecessary information (CONCISENESS), and considering potential risks to user safety (HARMLESSNESS).</li>
</ul>
<p>We ensure that each skill offers a wide range of criteria for a holistic evaluation of various LLMs. We provide the specific definition for each skill in Table 11 in the Appendix.</p>
<h1>3.2 Evaluation Data Construction</h1>
<p>The process of constructing the evaluation data involves several steps, 1) collecting input-output pairs from various datasets, 2) modifying the collected instances, and 3) filtering based on length criteria, resulting in a total of 1,740 instances sourced from 122 datasets. We first collect input (instruction) and output (reference answer) pairs from various English NLP datasets, both multitask datasets (e.g. MMLU (Hendrycks et al., 2020)) and single-task datasets (e.g. GSM8K (Cobbe et al., 2021)). For single-task datasets, we restrict them to account for at most 20 instances per dataset for diversity. After collection, we modify the instances by manually writing instructions for datasets that do not include instructions. Lastly, we remove instances where the input length exceeds 2048. More details including the list of source datasets are provided in Appendix J.</p>
<p>For each evaluation instance, we annotate the metadata which consists of 1) the essential skills to follow the instruction, 2) target domains, and 3) the difficulty level of the instructions. We first validate that human labelers and EVAL LM have a high correlation for the metadata annotation on a subset of 200 instances. We have observed a $95.22 \%$ acceptance rate for skill annotation, an $81.32 \%$ acceptance rate for domain annotation, and a Pearson correlation coefficient of 0.774 for difficulty annotation. Since the model-based annotation has acceptable noise and high correlation to human labelers, we utilize the EVAL LM for metadata annotation to reduce the burden of human annotations. We provide more details on validating the annotation of EVAL LM in Appendix G.2.</p>
<p>For the selection of necessary skills, the EVAL LM selects the top-3 essential skills required to follow the instructions for each instance, from the 12 skills defined in Section 3.1. We achieve this by providing the EVAL LM with the instruction, reference answer, and descriptions of all 12 skills. For domain annotation, we identify 10 domains: Humanities, Language, Culture, Health, History, Natural Science, Math, Social Science, Technology, and Coding by modifying the Wikipedia categorization of Reid et al. (2022). Lastly, for difficulty level annotation, we divide the difficulty level into 5 levels based on the extent of required domain knowledge by referencing Webb's depth of knowledge (Webb, 1997; 1999) and NIH proficiency scale ${ }^{3}$ : simple lifestyle knowledge, advanced lifestyle knowledge, formal education knowledge, major-level knowledge, and expert-level knowledge where we map each level into a level from 1 to 5 . Details of the metadata annotation process are provided in Appendix E and the statistics of the evaluation dataset are provided in Appendix F.</p>
<h3>3.3 Evaluation Process</h3>
<p>Utilizing the annotated metadata for each instance, we evaluate and analyze the target model response in a fine-grained manner. Evaluators, either human annotators or EVAL LM, are given the evaluation instruction, reference answer, response of the target model, and pre-defined score rubric for each selected skill from Section 3.2. The evaluators assess the target model's response by assigning scores ranging from 1 to 5 , following skill-specific scoring rubrics, which include detailed</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>descriptions for each score. For model-based evaluation, we enforce the EVAL LM to generate a rationale before assigning a score, inspired by the effectiveness of CoT prompting (Wei et al., 2022b) for the evaluation of LLMs (Liu et al., 2023). Once the evaluators have scored each skill of the instance, we aggregate the scores based on the skill, domain, and difficulty level for fine-grained analysis. This analysis allows for an in-depth understanding of how the target model performs across various metadata compositions. The illustration of the evaluation process and the score rubric for each skill is provided in Figure 1 and Appendix K.1.</p>
<h1>3.4 FLASK-HARD</h1>
<p>To assess state-of-the-art LLMs in challenging scenarios, we additionally introduce FLASK-HARD subset. This subset comprises 89 instances that are annotated as expert-level knowledge difficulty (Level 5), including tasks such as predicting chess checkmates and solving advanced mathematics problems. Due to the intricate nature of FLASK-HARD tasks which may prevent reliable evaluation, we explore a more fine-grained evaluation setting for FLASK-HARD. Instead of using a fixed score rubric for each skill, we introduce an instance-specific score rubric for each skill. Specifically, EVAL LM first generates at most 5 subquestions (checklists) that correspond to one of the related skills annotated in Section 3.2 for each instance. Then, we manually remove duplicates or subquestions unrelated to the annotated skillset. After we annotate subquestions for each instance, evaluators give a score ranging from 1 to 5 based on the judgment of whether the model response fulfilled the specific criteria of the subquestions. We specify the illustration in Figure 1 and the prompt in Figure 35 (Appendix) for the instance-specific score rubric, respectively.</p>
<h2>4 Reliability of FLASK</h2>
<p>In this section, we investigate the reliability of FLASK by 1) measuring the correlation between human-based and model-based evaluation and 2) the robustness to stylistic changes of model-based evaluation. For correlation measurement, we conduct both human-based and model-based evaluations on 200 instances randomly sampled from the whole FLASK evaluation set. We recruited 10 human labelers who have majored in various fields including computer science, mathematics, economics, business, chemistry, etc. We evaluate 4 models: 1) GPT-3.5, 2) BARD, 3) Vicuna-13B, and 4) AlPACA-13B ${ }^{4}$. For model-based evaluation, we use GPT-4 (OpenAI, 2023) as the default EVAL LM since it is known to show the highest correlation with human labelers (Liu et al., 2023; Dubois et al., 2023) ${ }^{5}$. Details of the human evaluation process are provided in Appendix G. 1 and the analysis of inter-labeler agreement between skills is provided in Appendix C.1. To measure the robustness to stylistic changes, we use the response of GPT-3.5 of FLASK-HARD and generate an adversarial set to make the response more verbose. We measure the consistency of the scores given by the EVAL LM between the original and the adversarial response.</p>
<p>Fine-graininess leads to a high correlation between human-based and model-based evaluation. We compare the result of human-based and model-based evaluation of FLASK in Figure 2. Overall, the tendency is similar between the two evaluation settings: AlPACA model results in the worst performance for most of the skills, and both Vicuna and AlPACA have a significant performance gap between GPT-3.5 and BARD on Logical Thinking (Logical Robustness, Logical Correctness, Logical Efficiency) and Background Knowledge abilities (Factuality, Commonsense Understanding skills) compared to other skills. However, it's worth noting that both evaluation settings are necessary, as neither is perfect and they complement each other. In human-based evaluation, we observe central tendency bias (Goldfarb-Tarrant et al., 2020), where labelers tend to assign middle scores more often on the Likert scale, resulting in a more uniform score distribution. Also, human labelers are prone to fatigue since the annotation task requires knowledge-intensive evaluation, such as code implementation tasks (Casper et al., 2023; Bowman et al., 2022). On the other hand, modelbased evaluation is known to possess style and verbosity bias (Wang et al., 2023b; Dubois et al., 2023; Zheng et al., 2023), where the evaluation model tends to prefer responses similar to its own</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Human-based Evaluation
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Model-based Evaluation</p>
<p>Figure 2: (a) The skill comparison between different models (GPT-3.5, Vicuna, Bard, Alpaca) through human-based evaluation on the subset of FLASK evaluation set. (b) The skill comparison between different models through model-based evaluation of FLASK. Both settings are highly correlated with each other.
generation styles and responses with longer lengths. For example, for some skills, the EVAL LM tends to prefer the response styles of GPT-3.5 compared to BARD, unlike human evaluators.</p>
<p>To quantitatively analyze the correlation between human-based and model-based evaluation, we measure the Spearman, Kendall-Tau, and Pearson correlation. We first observe that using an automatic metric (ROUGE-L) results in the lowest correlation. Next, we compare the skillspecific rubric setting of FLASK with the reference answer-guided, skill-agnostic evaluation setting introduced in Zheng et al. (2023) and illustrated in Figure 1a, which provides an overall single score without considering the skill set ${ }^{6}$. As shown in Table 1, applying a skill-specific finegrained evaluation leads to a stronger correlation between human-based and model-based evaluation consistently across various EVAL LMS. Also, by comparing different EVAL LMS, we observe that GPT-4 shows the highest correlation compared to GPT-3.5 and Claude. Additionally, we analyze the effect of including a reference answer, generating a rationale before assigning a score, and including a score rubric for each skill during the model-based evaluation of FLASK, respectively. As shown in Table 1, we notice that removing any of the factors leads to a significant drop in the correlation, especially for the reference answer.</p>
<p>Fine-grained evaluation mitigates the bias of model-based evaluation. As mentioned previously, model-based evaluation is known to be prone to biases (Wang et al., 2023b; Zheng et al., 2023). Among various biases, we investigate the effect of finegrained evaluation on verbosity bias which is quantitatively measurable in a controllable setup. We take the original response of GPT-3.5 on FLASK-HARD and prompt GPT-3.5 to make the response more verbose while retaining the contents. We measure the robustness of the evaluation method by calculating the ratio that the EVAL LM assigns the same score regardless of the stylistic changes. We compare the skill-agnostic evaluation, the skill-specific rubric of FLASK, and the instance-specific rubric of FLASK introduced</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$\rho$</th>
<th style="text-align: center;">$\tau$</th>
<th style="text-align: center;">$r$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.289</td>
</tr>
<tr>
<td style="text-align: left;">Skill-agnostic (GPT-3.5)</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.450</td>
</tr>
<tr>
<td style="text-align: left;">FLASK (GPT-3.5)</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.449</td>
</tr>
<tr>
<td style="text-align: left;">Skill-agnostic (Claude)</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.391</td>
</tr>
<tr>
<td style="text-align: left;">FLASK (Claude)</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.458</td>
</tr>
<tr>
<td style="text-align: left;">Skill-agnostic (GPT-4)</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.673</td>
</tr>
<tr>
<td style="text-align: left;">FLASK (GPT-4)</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 2}$</td>
</tr>
<tr>
<td style="text-align: left;">- Reference Answer</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.566</td>
</tr>
<tr>
<td style="text-align: left;">- Rationale</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.683</td>
</tr>
<tr>
<td style="text-align: left;">- Score Rubric</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.696</td>
</tr>
</tbody>
</table>
<p>Table 1: Correlation between model-based evaluation and human labelers for Skill-agnostic (skillagnostic rubric) and FLASK (skill-specific rubric) across different EVAL LMS (GPT-3.5, Claude, GPT-4). We report Spearman $(\rho)$, Kendall-Tau $(\tau)$, and Pearson $(r)$ correlation. We also measure the effect of including a reference answer, rationale generation, and score rubric.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Comparison of skill-agnostic, skill-specific, and instance-specific score rubrics in terms of their robustness to stylistic changes.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>in Section 3.4 and illustrated in Figure 1. As shown in Figure 3, we observe that the robustness increases as the fine-graininess of the evaluation setting increases. This indicates that increasing the fine-graininess could mitigate the biases and enhance the reliability of the model-based evaluation to some extent. We provide the correlation between response length and the performance score for each skill of various models on the whole FLASK evaluation set in Figure 22 and Table 5 in the Appendix. Although the instance-specific rubric is the most robust to stylistic changes, it is more costly as it requires an additional stage for annotating subquestions and manual validation. We therefore utilize the instance-specific rubric in FLASK-HARD only. We leave extending it to the whole evaluation set and the investigation of other biases as future work.</p>
<h1>5 ANALYSIS BASED ON AUTOMATIC EVALUATION OF FLASK</h1>
<p>Although conducting both human-based and model-based evaluation is reliable for comprehensive analysis, human-based evaluation is time-consuming and expensive. Therefore, considering the high correlation with human-based evaluation shown in Table 1, for the evaluation on the whole FLASK evaluation set, we focus on automatic model-based evaluation for an extensive analysis of LLMs.</p>
<p>Current open-source models significantly underperform proprietary models on particular skills. First, to compare open-sourced models with proprietary models on the entire set, we compare GPT-3.5, Vicuna-13B, and Wiz-ARDLM-13B where the latter two models are trained with GPT-3.5 responses during instruction tuning. As shown in Figure 4, Vicuna and WIZARDLM show similar performance across all skills. In contrast to the claim of Xu et al. (2023), this implies that the effect of complex instructions is not significant when using the same base model, teacher model, and training configuration. By comparing GPT-3.5 and the other two open-source models (Vicuna and WIZARDLM), we observe that Problem Handling and User Alignment abilities can be almost fully imitated, including Metacognition, Readability, and Conciseness. However, a large gap is especially noticeable in Logical Thinking and Background Knowledge abilities. This result aligns with Gudibande et al. (2023)
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: The performance comparison between GPT-3.5, Vicuna, and WIZARDLM for each skill on the FLASK evaluation set.
which demonstrates that the open-source models only imitate the style of the proprietary models rather than the factuality. We also observe a similar tendency for larger open-source models such as TÜLU-65B as shown in Table 9. By analyzing the performance in terms of each domain, we find that both open-source models significantly underperform GPT-3.5 in Math, and Coding domains, as shown in Figure 29a in the Appendix. Moreover, by analyzing the performance by difficulty level in Figure 30 in the Appendix, open-source models consistently exhibit poor performance across difficulties, especially on Logical Thinking and Background Knowledge abilities.</p>
<p>Some skills require larger model sizes. We analyze the effect of the model scale for each skill by comparing TÜLU 7B, 13B, 30B, and 65B shown in Figure 5. Overall, we can observe that larger models lead to better performance, which aligns with the result of Chung et al. (2022); Wei et al. (2022a). However, the range of improvement varies across different skills. For example, skills such as Readability, Harmlessness, and Metacognition show slow improvement as the model scales up. On the other hand, skills such as Logical Robustness, Logical Correctness, and Logical Efficiency show rapid improvements. Using FLASK, we confirm the findings of Gudibande et al. (2023) that skills requiring logical reasoning or fact retrieval benefit significantly from model scaling. Interestingly, we observe that for some skills, the performance nearly saturates after a particular scale; Logical Efficiency and Conciseness after 30B, Insightfulness after 13B and Metacognition after 7B. This suggests that some skills necessitate larger model sizes, while others can be achieved with smaller models. By analyzing the effect of model scaling for different levels of difficulty for each</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: The performance of TÜLU shown for each skill depending on the model scale (7B, 13B, 30B, 65B). While skills such as Logical Robustness and Logical Correctness largely benefit from model scaling, smaller models also perform well in skills such as Readability and Metacognition.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: The performance comparison among GPT-3.5, TÜLU-7B, 13B, 30B, and 65B for Logical Robustness, Logical Correctness, Factuality, and Completeness, depending on the difficulty of the instructions. Larger models show effectiveness on easier instructions especially. The full results are shown in Figure 31 (Appendix).
skill, we find that scaling the model size is more effective for easier instructions, as shown in Figure 6. Larger models of TÜLU reduce the performance gap with GPT-3.5, especially for the simple lifestyle knowledge (Level 1), whereas the gap increases for higher difficulties. We provide the results for each domain in Figure 32 and additionally observe that different skills require different training steps in Appendix C.6.</p>
<p>Proprietary models also struggle on the FLASK-HARD set. We also compare the performance of various proprietary models (GPT-3.5, BARD, CLAUDE, INSTRUCTGPT, GPT-4) on the FLASK evaluation set as shown in Figure 7a. For all skills of Problem Handling, CLAUDE shows the best performance while for Logical Thinking and Background Knowledge, GPT-3.5 shows the best performance. INSTRUCTGPT shows the worst performance across most skills because it often provides short responses while not fully addressing the intention of given instruction. We provide the comparison between proprietary models for each domain in Figure 33. Furthermore, we compare the performance of different proprietary models on the FLASK-HARD set, as shown in Figure 7b and 7c, which adopts skill-specific and instance-specific score rubrics, respectively. First, we observe that on FLASK-HARD, the performance significantly degrades for Logical Thinking and Background Knowledge abilities compared to Figure 7a. Also, by comparing other models with GPT-4, we observe that there is a large gap for Logical Correctness, Insightfulness, and Commonsense Understanding. Interestingly, even the state-of-the-art GPT-4 model also performs poorly for Logical Correctness and Factuality skills on the FLASK-HARD set. This suggests there is significant room for improvement in those abilities even for the proprietary models. By</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: (a) Performance comparison of various proprietary models (GPT-3.5, BARD, InStruCtGPT, Claude) on the FLASK evaluation set. (b) Performance comparison of various proprietary models on the FLASK-HARD evaluation set using skill-specific score rubrics. (c) Performance comparison of various proprietary models on the FLASK-HARD evaluation set using instance-specific score rubrics. Exact numbers including those for open-source models are reported in Table 9 and Table 10 (Appendix).
comparing Figure 7b and Figure 7c, we can observe that adopting an instance-specific score rubric leads to a lower score overall. This indicates that instance-specific score rubric is a more strict setting since it necessitates accomplishing a more specific requirement as shown in the example of Figure 1. Although an in-depth analysis of the model scales or training techniques is infeasible for proprietary models, FLASK-HARD could provide action items for companies developing proprietary models.</p>
<h1>6 APPLICATION OF FLASK</h1>
<p>FLASK for Developers FLASK enables model developers to more accurately analyze the performance of their own models and suggests detailed action items for intermediate model checkpoints. Specifically, developers working on open-source LLMs can compare the performance with proprietary LLMs and try to close the gap between them, especially for Logical Thinking and Background Knowledge abilities. On the other hand, developers working on proprietary LLMs can devise methods to enhance the performance of their own models on the FLASK-HARD set. Similar to the role of Wang et al. (2022a); Longpre et al. (2023a) for instruction-tuned LLMs and Longpre et al. (2023b); Xie et al. (2023) for pre-trained LLMs, FLASK can be utilized for making better base models, making better training datasets, and making better training techniques.</p>
<p>FLASK for Practitioners FLASK enables practitioners to select appropriate LLMs for different situations, similar to the role of Jiang et al. (2023). Because the evaluation setting of FLASK is dynamic, practitioners can perform metadata annotation on their own test sets and approximate which models would be suitable. For example, if the end-use case is a chatbot for chit-chat, using 7B fine-tuned open-source models might be enough. In contrast, it might be worthwhile to pay for API calls of proprietary LLMs for complex reasoning tasks. Potentially, the result of FLASK can be used to automatically route and recommend suitable LLMs depending on the instruction.</p>
<h2>7 CONCLUSION</h2>
<p>In this paper, we introduce FLASK, a fine-grained language skill set evaluation setting for the alignment of language models. We categorize 12 fine-grained skills to evaluate LLMs and annotate necessary skills, the target domain, and the difficulty level for each instance. FLASK provides a comprehensive and interpretable analysis of the capabilities of LLMs by allowing the analysis of the performance depending on different skills, domains, and difficulty levels. Also, we observe that applying fine-grained evaluation results in better reliability in terms of correlation between humanbased and model-based evaluation and the robustness of model-based evaluation to stylistic changes. We analyze various open-source and proprietary LLMs and suggest that FLASK could be utilized for making better language models and providing meaningful insights of various LLMs for both developers and practitioners. We hope that FLASK could serve as an initial guideline for fine-grained evaluation towards a comprehensive and reliable evaluation setting.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>This work was partly supported by KAIST-NAVER Hypercreative AI Center and Institute of Information \&amp; communications Technology Planning \&amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00264, Comprehensive Video Understanding and Generation with Knowledge-based Deep Logic Neural Network, 40\%; No.2021-0-02068, Artificial Intelligence Innovation Hub, 20\%). We thank Hyunji Lee, Yizhong Wang, Eric Wallace, and Swaroop Mishra for helpful discussions and constructive feedback. We also thank members of KAIST for participating in human evaluation for FLASK.</p>
<h2>REFERENCES</h2>
<p>Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. Topiocqa: Open-domain conversational question answering with topic switching, 2022.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022.</p>
<p>Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https://github.com/nomic-ai/gpt4all, 2023.</p>
<p>Anthropic. Claude. https://www.anthropic.com/index/introducing-claude, 2023.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment, 2021.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen tau Yih, and Yejin Choi. Abductive commonsense reasoning, 2020.</p>
<p>Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.</p>
<p>Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilė Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli TranJohnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado,</p>
<p>Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and Jared Kaplan. Measuring progress on scalable oversight for large language models, 2022.</p>
<p>Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback, 2023.</p>
<p>Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https: //github.com/sahil280114/codealpaca, 2023.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, and Pan Lu. Theoremqa: A theorem-driven question answering dataset. arXiv preprint arXiv:2305.12524, 2023.</p>
<p>Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. Finqa: A dataset of numerical reasoning over financial data, 2022.</p>
<p>Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic evaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757, 2023.</p>
<p>Cheng-Han Chiang and Hung yi Lee. Can large language models be an alternative to human evaluations?, 2023.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. Decontextualization: Making sentences stand-alone, 2021.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Peter Clark, Bhavana Dalvi Mishra, and Oyvind Tafjor. Barda: A belief and reasoning datasetthat separates factual accuracy and reasoning ability, 2023.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Jeremy R. Cole, Palak Jain, Julian Martin Eisenschlos, Michael J. Q. Zhang, Eunsol Choi, and Bhuwan Dhingra. Diffqg: Generating questions to summarize factual changes, 2023.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.</p>
<p>Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin. e-care: a new dataset for exploring explainable causal reasoning, 2022.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.</p>
<p>Ahmed Elgohary, Denis Peskov, and Jordan Boyd-Graber. Can you unpack that? learning to rewrite questions-in-context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5918-5924, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1605. URL https://aclanthology. org/D19-1605.</p>
<p>Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\mathcal{V}$-usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5988-6008. PMLR, 17-23 Jul 2022.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018.
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire, 2023.
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo. 5371628 .</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models, 2020.</p>
<p>Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text, 2022.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https: //bair.berkeley.edu/blog/2023/04/03/koala/.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies, 2021.</p>
<p>Deepanway Ghosal, Siqi Shen, Navonil Majumder, Rada Mihalcea, and Soujanya Poria. Cicero: A dataset for contextualized commonsense inference in dialogues, 2022.</p>
<p>Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, and Nanyun Peng. Content planning for neural story generation with aristotelian rescoring. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4319-4338, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.351. URL https://aclanthology.org/2020.emnlp-main.351.</p>
<p>Google. Bard. https://blog.google/technology/ai/ bard-google-ai-search-updates/, 2023.</p>
<p>Mitchell L Gordon, Kaitlyn Zhou, Kayur Patel, Tatsunori Hashimoto, and Michael S Bernstein. The disagreement deconvolution: Bringing machine learning performance metrics in line with reality. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-14, 2021.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023.</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. Folio: Natural language reasoning with first-order logic, 2022.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values, 2023.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022a.</p>
<p>Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022b.</p>
<p>Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, and Wen mei Hwu. Open relation modeling: Learning to define relations between entities, 2022.</p>
<p>Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan. Cosqa: 20,000+ web queries for code search and question answering, 2021.</p>
<p>John Hughes. krippendorffsalpha: An r package for measuring agreement using krippendorff's alpha coefficient. arXiv preprint arXiv:2103.12170, 2021.</p>
<p>Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in programmatic context, 2018.</p>
<p>Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Bring your own data! self-supervised evaluation for large language models. arXiv preprint arXiv:2306.13651, 2023.</p>
<p>Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Exploring the benefits of training expert language models over instruction tuning. arXiv preprint arXiv:2302.03202, 2023.</p>
<p>Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. Realtime qa: What's the answer right now?, 2022.</p>
<p>Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5376-5384, 2017. doi: 10.1109/CVPR.2017.571.</p>
<p>Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and Maarten Sap. Prosocialdialog: A prosocial backbone for conversational agents, 2022.</p>
<p>Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. Soda: Million-scale dialogue distillation with social commonsense contextualization, 2023a.</p>
<p>Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning. arXiv preprint arXiv:2305.14045, 2023b.</p>
<p>Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Minjoon Seo. Aligning large language models through synthetic feedback. arXiv preprint arXiv:2305.13735, 2023c.</p>
<p>Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. arXiv preprint arXiv:2302.08582, 2023.</p>
<p>Yuta Koreeda and Christopher D. Manning. Contractnli: A dataset for document-level natural language inference for contracts, 2021.</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering. arXiv preprint arXiv:2103.06332, 2021.</p>
<p>Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 1650-1669, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main. 121.</p>
<p>Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. Openassistant conversations - democratizing large language model alignment, 2023.</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. arXiv preprint arXiv:2305.18486, 2023.</p>
<p>Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, et al. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746, 2022.</p>
<p>Noah Lee, Na Min An, and James Thorne. Can large language models infer and disagree like humans?, 2023.</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023a.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023b.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Commongen: A constrained text generation challenge for generative commonsense reasoning, 2020.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022.</p>
<p>Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and Bill Dolan. A token-level reference-free hallucination detection benchmark for free-form text generation, 2022.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023a.</p>
<p>Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \&amp; toxicity. arXiv preprint arXiv:2305.13169, 2023b.</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories, 2023.</p>
<p>Ian McKenzie, Alexander Lyzhov, Alicia Parrish, Ameya Prabhu, Aaron Mueller, Najoung Kim, Sam Bowman, and Ethan Perez. The inverse scaling prize, 2022. URL https://github. com/inverse-scaling/prize.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023.</p>
<p>Roshanak Mirzaee and Parisa Kordjamshidi. Transfer learning with synthetic corpora for spatial role labeling and reasoning, 2022.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2022.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.</p>
<p>Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryściński, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, and Dragomir Radev. Fetaqa: Free-form table question answering, 2021.</p>
<p>Jekaterina Novikova, Ondřej Dušek, and Verena Rieser. The e2e dataset: New challenges for end-to-end generation, 2017.</p>
<p>OpenAI. Chatgpt: Optimizing language models for dialogue. 2022. URL https://openai. com/blog/chatgpt/.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Jungsoo Park, Sewon Min, Jaewoo Kang, Luke Zettlemoyer, and Hannaneh Hajishirzi. Faviq: Fact verification from information-seeking questions, 2022.</p>
<p>Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 2086-2105, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. findings-acl.165. URL https://aclanthology.org/2022.findings-acl.165.</p>
<p>Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1470-1480, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1142. URL https://aclanthology.org/P15-1142.</p>
<p>Abhilash Potluri, Fangyuan Xu, and Eunsol Choi. Concise answers to complex questions: Summarization of long-form answers, 2023.</p>
<p>Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, and Yejin Choi. Counterfactual story reasoning and generation, 2019.</p>
<p>Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui. Timedial: Temporal commonsense reasoning in dialog, 2021.</p>
<p>Nicole M. Radziwill and Morgan C. Benton. Evaluating quality of chatbots and intelligent conversational agents, 2017.</p>
<p>Rachel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer. M2d2: A massively multidomain language modeling dataset. arXiv preprint arXiv:2210.07370, 2022.</p>
<p>Anna Rogers, Matt Gardner, and Isabelle Augenstein. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. July 2021.</p>
<p>Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution, 2018.</p>
<p>Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inference in natural language. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4661-4675, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.418. URL https://aclanthology.org/2020. findings-emnlp. 418 .</p>
<p>Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, and Yejin Choi. proscript: Partially ordered scripts generation via pre-trained language models, 2021.</p>
<p>Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large language models meet personalization, 2023.</p>
<p>Viktor Schlegel, Marco Valentino, André Freitas, Goran Nenadic, and Riza Batista-Navarro. A framework for evaluation of machine reading comprehension gold standards, 2020.</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. Get your vitamin c! robust fact verification with contrastive evidence, 2021.</p>
<p>Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Fine-tuned language models are continual learners, 2022.</p>
<p>Jingyuan Selena She, Christopher Potts, Samuel R. Bowman, and Atticus Geiger. Scone: Benchmarking negation reasoning in language models with fine-tuning and in-context learning, 2023.</p>
<p>Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et al. Flan-moe: Scaling instruction-finetuned language models with sparse mixture of experts. arXiv preprint arXiv:2305.14705, 2023a.</p>
<p>Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023b.</p>
<p>Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324, 2023.</p>
<p>Roman Sitelew, Jascha Sohl-Dickstein, and Josh Rule. self_awareness: a benchmark task to measure self-awareness of language models. In: The Beyond the Imitation Game Benchmark (BIG-bench). GitHub repository: https://github.com/google/BIG-bench , 2021. URL https://github.com/google/BIG-bench/tree/main/bigbench/ benchmark_tasks/self_awareness. (a GitHub repository).</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form answers, 2023.</p>
<p>Saku Sugawara and Akiko Aizawa. An analysis of prerequisite skills for reading comprehension. In Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods, pp. 1-5, Austin, TX, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-6001. URL https://aclanthology.org/ W16-6001.</p>
<p>Saku Sugawara, Yusuke Kido, Hikaru Yokono, and Akiko Aizawa. Evaluation metrics for machine reading comprehension: Prerequisite skills and readability. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 806-817, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/ P17-1075. URL https://aclanthology.org/P17-1075.</p>
<p>Haitian Sun, William W. Cohen, and Ruslan Salakhutdinov. Conditionalqa: A complex reading comprehension dataset with conditional answers, 2021.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them, 2022.</p>
<p>Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. Commonsenseqa 2.0: Exposing the limits of ai through gamification, 2022.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 809-819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://aclanthology.org/N18-1074.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023a.</p>
<p>Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective work best for zero-shot generalization? arXiv preprint arXiv:2204.05832, 2022a.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022b.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022c.</p>
<p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023b.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments, 2019.</p>
<p>Norman Lott Webb. Criteria for alignment of expectations and assessments in mathematics and science education. research monograph no. 6. 1997.</p>
<p>Norman Lott Webb. Alignment of science and mathematics standards and assessments in four states. research monograph no. 18. 1999.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023a.</p>
<p>Zeqiu Wu, Ryu Parish, Hao Cheng, Sewon Min, Prithviraj Ammanabrolu, Mari Ostendorf, and Hannaneh Hajishirzi. Inscit: Information-seeking conversations with mixed-initiative interactions, 2023b.</p>
<p>Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. arXiv preprint arXiv:2305.10429, 2023.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.</p>
<p>Fangyuan Xu, Junyi Jessy Li, and Eunsol Choi. How do we answer complex questions: Discourse structure of long-form answers, 2022a.</p>
<p>Ying Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bingsheng Yao, Tongshuang Wu, Zheng Zhang, Toby Jia-Jun Li, Nora Bradford, Branda Sun, Tran Bao Hoang, Yisi Sang, Yufang Hou, Xiaojuan Ma, Diyi Yang, Nanyun Peng, Zhou Yu, and Mark Warschauer. Fantastic questions and where to find them: Fairytaleqa - an authentic dataset for narrative comprehension, 2022b.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.</p>
<p>Seonghyeon Ye, Joel Jang, Doyoung Kim, Yongrae Jo, and Minjoon Seo. Retrieval of soft prompt enhances zero-shot task generalization. arXiv preprint arXiv:2210.03029, 2022a.</p>
<p>Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo Shin, and Minjoon Seo. Guess the instruction! making language models stronger zero-shot learners. arXiv preprint arXiv:2210.02969, 2022b.</p>
<p>Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, and Minjoon Seo. In-context instruction learning. arXiv preprint arXiv:2302.14691, 2023a.</p>
<p>Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May 2023b. URL https://kaistai.github.io/SelFee/.</p>
<p>Xinyan Velocity Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi. Crepe: Open-domain question answering with false presuppositions, 2022.</p>
<p>Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Victor Zhong, Weijia Shi, Wen tau Yih, and Luke Zettlemoyer. Romqa: A benchmark for robust, multi-evidence, multi-answer question answering, 2022.</p>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.</p>
<h1>A Limitation and Future Work</h1>
<h2>A. 1 Limitation of Evaluators</h2>
<p>As discussed in Section 4, both human and model evaluators possess limitations during evaluation. Human labelers tend to show central tendency bias and are prone to annotation fatigue due to the difficulty and wide scope of knowledge needed to evaluate each instance. These factors might have caused the moderate inter-agreement between human labelers. We expect that using advanced features such as document retrieval for fact verification (Min et al., 2023) or highlight hints (Krishna et al., 2023) could mitigate this issue. On the other hand, the model-based evaluation shows bias in preferring longer responses and in writing styles that are similar to the evaluation's model writing style. While model-based evaluation is more efficient in terms of time and cost as discussed in Appendix G.3, we emphasize that evaluation in both settings is crucial to reliably figure out the true capability of a language model. We leave mitigating the limitations for respective evaluation settings as future work. Also, we did not extensively conduct human-based evaluations due to cost and time constraints. For a more reliable setting, a larger number of labelers from diverse demographics could be recruited and the human-based evaluation could be conducted on a larger set. Also, while we evaluated only 4 models for human-based evaluation, a larger number of models could be evaluated for future work.</p>
<h2>A. 2 Scope of the Evaluation</h2>
<p>We restrict the scope of the current evaluation instance to be monolingual (including only English user instructions), single-turn, language-focused, and zero-shot. We leave extension to multilingual instructions, multi-turn, multi-modal, and few-shot in-context learning evaluation to future work. Also, the FLASK-HARD subset only contains 89 instances, making the effect of outliers unavoidable when analyzing by each skill, domain, or difficulty. However, expansion to these axes could be easily implemented once the instances are collected using the process described in Section 3.2, because the metadata annotation is automatic and dynamic. Also, we only apply instance-specific scoring rubrics on FLASK-HARD. Although we have shown that adopting a more fine-grained evaluation setting leads to increased robustness for model-based evaluation, we have not conducted human evaluations for the instance-specific scoring rubrics on the FLASK whole set due to time and cost constraints. Additionally, new abilities of LLMs are newly discovered (Wei et al., 2022a), indicating that recategorization of the primary abilities and skills might be needed for future models possessing potentially much more powerful abilities and skills.</p>
<h2>B Model Details</h2>
<p>We evaluate LLMs with varying model sizes, training techniques, and training datasets. We evaluate several proprietary LLMs where the model responses are provided through private APIs with model details hidden from the end users. These include 1) OpenAI's GPT-3.5 (OpenAI, 2022), 2) OpenAI's InStrUCTGPT (text-davinci-003) (Ouyang et al., 2022), 3) Google’s BARD (Google, 2023), and 4) Anthropic’s Claude 1.0 (Anthropic, 2023) ${ }^{8}$. For open-source models which are fine-tuned based on human-curated datasets or responses from proprietary models, we compare 1) AlPACA 13B (Taori et al., 2023) which is a fine-tuned LLAMA model (Touvron et al., 2023a) on 52,000 instructions and responses generated by text-davinci-0039, 2) Vicuna 13B(Chiang et al., 2023) which is a LLAMA model fine-tuned on 70K responses of GPT-3.5 available through ShareGPT, 3) WIZARdLM 13B (Xu et al., 2023), a LLAMA model fine-tuned on 250K instructions and responses augmented by GPT-3.5 through instruction evolving, 4) TÜLU 13B (Wang et al., 2023b), a LLAMA model fine-tuned on 490K training instances which are a mixture of human and machinegenerated instructions and responses, 5) LLAMA2 Chat 70B(Touvron et al., 2023b), a chat-variant of LLAMA2 model fine-tuned with instruction tuning and RLHF. To evaluate LLMs with various model sizes, we also compare TÜLU 7B, 13B, 30B, and 65B models. Also, to compare the effect of different fine-tuning datasets, we compare models finetuned on SHAREGPT ${ }^{10}$, CODE-ALPACA</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(Chaudhary, 2023), Alpaca, FLAN V2 (Longpre et al., 2023a), and Evol-Instruct (Xu et al., 2023) respectively using the model checkpoints provided by Wang et al. (2023b). For the response generation of each target model, we set the temperature to 0.7 and set the max generation sequences as 1024 .</p>
<h1>C Additional Analysis</h1>
<h2>C. 1 Inter-Labeler Agreement between Skills</h2>
<p>We analyze the inter-labeler agreement of both humanbased evaluation and model-based evaluation using Krippendorff's alpha (Hughes, 2021). For human-based evaluation, because we assign 3 labelers for each instance, we measure the agreement between 3 labelers. For modelbased evaluation, we set the decoding temperature as 1.0 for nondeterministic generations while keeping the EVAL LM (GPT-4) fixed and measure the agreement between 3 runs. First, the overall agreement of inter-labeler agreement for human-based evaluation is 0.488 , indicating a moderate correlation while the agreement is 0.835 for model-based evaluation. Second, we analyze the humanhuman agreement, model-model agreement, and humanmodel correlation for each skill as shown in Table 2. While skills such as Logical Correctness and Commonsense Understanding have a high agreement or correlation for all settings, skills such as Readability and Conciseness do not. This implies that more subjectivity tends to exist in User Alignment ability than Logical Thinking and Background Knowledge abilities consistent for all settings. We expect that disagreement between labelers for User Alignment ability could be utilized for additional training signals or personalization for subjective tasks (Gordon et al., 2021; Salemi et al., 2023). We explore agreement between different EVAL LMs in Appendix C.8.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">H-H</th>
<th style="text-align: center;">M-M</th>
<th style="text-align: center;">H-M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Robustness</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.780</td>
</tr>
<tr>
<td style="text-align: left;">Correctness</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Efficiency</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.776</td>
<td style="text-align: center;">0.640</td>
</tr>
<tr>
<td style="text-align: left;">Factuality</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.747</td>
</tr>
<tr>
<td style="text-align: left;">Commonsense</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.816</td>
</tr>
<tr>
<td style="text-align: left;">Comprehension</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.575</td>
</tr>
<tr>
<td style="text-align: left;">Insightfulness</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.587</td>
</tr>
<tr>
<td style="text-align: left;">Completeness</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.656</td>
</tr>
<tr>
<td style="text-align: left;">Metacognition</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.827</td>
</tr>
<tr>
<td style="text-align: left;">Readability</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.223</td>
</tr>
<tr>
<td style="text-align: left;">Conciseness</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.507</td>
</tr>
<tr>
<td style="text-align: left;">Harmlessness</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.755</td>
</tr>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.732</td>
</tr>
</tbody>
</table>
<p>Table 2: Inter-labeler agreement for human-based and model-based evaluation and the correlation between human labelers and EVAL LM shown for each skill. We report Krippendorf's alpha for interlabeler agreement and Pearson correlation for human-model correlation. We observe that the Human-Human (H-H), ModelModel agreement (M-M), and HumanModel correlation (H-M) all show similar tendencies depending on the skill.</p>
<h2>C. 2 Analysis of Different Finetuning Data</h2>
<p>Through the metadata annotation process of FLASK, we can analyze not only the evaluation data but also the instructions of fine-tuning data. To compare different fine-tuning datasets, we compare ShareGPT, FLAN V2, Alpaca, Code-Alpaca, and Evol-Instruct data by randomly sampling 200 instances. We first compare the primary ability and skill proportion for each training data as shown in Figure 8 and Figure 9. While ShareGPT and FLAN V2 show similar proportions, Evol-InStruCt focuses more on Logical Thinking and Problem Handling. Also, Alpaca focuses on Problem Handling and User Alignment while Code-Alpaca mainly focuses on Logical Thinking. By comparing the domain proportion shown in Figure 10, we observe that ShareGPT, Code-Alpaca and Evol-InstrucThave a large proportion of the Coding and Technology domain while FLAN-v2 and Alpaca have a large proportion of Language domain. Lastly, we compare the difficulty level of each instruction of training data shown in Figure 11. Overall, Alpaca and FLAN V2 show relatively low difficulty while Code-Alpaca and ShareGPT show moderate difficulty and Evol-InStruCt shows the highest difficulty.</p>
<p>We also report the performance of different fine-tuning datasets on a subset of FLASK where only the instances that have short reference answers (less than 5 words) are selected in Figure 12. Different from the result of Figure 14, the performance gap between different training instructions reduces especially for Logical Thinking and User Alignment. This indicates that the low performance of FLAN V2 in Figure 14 is due to the failure to generate long-form responses rather than the lack of ability. We leave exploring the effect of replacing the responses of FLAN V2 instruction to longer responses as future work.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ For proprietary models, we use the most recent model versions at the period of May 2023 - June 2023.
${ }^{9}$ Because the official ALPACA 13B checkpoint is not released at the point of conducting evaluation, we use the open-instruct-stanford-alpaca-13b model weights provided by Wang et al. (2023b).
${ }^{10}$ https://sharegpt.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
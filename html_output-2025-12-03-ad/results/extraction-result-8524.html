<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8524 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8524</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8524</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-32112b798f70faab00e14806f51d46058cf5e597</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/32112b798f70faab00e14806f51d46058cf5e597" target="_blank">Chameleon: Mixed-Modal Early-Fusion Foundation Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence, is presented, a significant step forward in a unified modeling of full multimodal documents.</p>
                <p><strong>Paper Abstract:</strong> We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8524",
    "paper_id": "paper-32112b798f70faab00e14806f51d46058cf5e597",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0053915,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chameleon: Mixed-Modal Early-Fusion Foundation Models</h1>
<p>Chameleon Team ${ }^{1, <em>}$<br>${ }^{1}$ FAIR at Meta<br></em>See Contributions section for full author list.</p>
<p>We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.</p>
<p>Date: May 17, 2024</p>
<h2>1 Introduction</h2>
<p>Recent multimodal foundation models are very widely adopted but still model different modalities separately, often using modality specific encoders or decoders. This can limit their ability to integrate information across modalities and generate multimodal documents that can contain arbitrary sequences of images and text. In this paper, we present Chameleon, a family of mixed-modal foundation models capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and image content (Figures 2-4). This allows for full multimodal document modeling, which is a direct generalization of standard multimodal tasks such as image generation, understanding and reasoning over images, and text-only LLMs. Chameleon is instead designed to be mixed-modal from inception and uses a uniform architecture trained from scratch in an end-to-end fashion on an interleaved mixture of all modalities, i.e., images, text, and code.</p>
<p>Our unified approach uses fully token-based representations for both image and textual modalities (Figure 1). By quantizing images into discrete tokens, analogous to words in text, we can apply the same transformer architecture to sequences of both image and text tokens, without the need for separate image/text encoders (Alayrac et al., 2022; Liu et al., 2023b; Laurençon et al., 2023) or domain-specific decoders (Ramesh et al., 2022; Jin et al., 2023; Betker et al., 2023). This early-fusion approach, where all modalities are projected into a shared representational space from the start, allows for seamless reasoning and generation across modalities. However, it also presents significant technical challenges, particularly in terms of optimization stability and scaling.</p>
<p>We address these challenges through a combination of architectural innovations and training techniques. We introduce novel modifications to the transformer architecture, such as query-key normalization and revised placement of layer norms, which we find to be crucial for stable training in the mixed-modal setting (Section 2.3). We further show how to adapt the supervised finetuning approaches used for text-only LLMs to the mixed-modal setting, enabling strong alignment at scale (Section 3). Using these techniques, we successfully train Chameleon-34B on $5 x$ the number of tokens as Llama-2 - enabling new mixed-modal applications while still matching or even outperforming existing LLMs on unimodal benchmarks.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 Chameleon represents all modalities - images, text, and code, as discrete tokens and uses a uniform transformer-based architecture that is trained from scratch in an end-to-end fashion on $\sim 10 \mathrm{~T}$ tokens of interleaved mixed-modal data. As a result, Chameleon can both reason over, as well as generate, arbitrary mixed-modal documents. Text tokens are represented in green and image tokens are represented in blue.</p>
<p>Extensive evaluations demonstrate that Chameleon is a broadly capable model on a diverse set of tasks. On visual question answering and image captioning benchmarks, Chameleon-34B achieves state-of-the-art performance, outperforming models like Flamingo, IDEFICS and Llava-1.5 (Section 5.2). At the same time, it maintains competitive performance on text-only benchmarks, matching models like Mixtral 8x7B and Gemini-Pro on commonsense reasoning and reading comprehension tasks (Section 5.1). But perhaps most impressively, Chameleon unlocks entirely new capabilities in terms of mixed-modal reasoning and generation.</p>
<p>As using only static, public benchmarks to evaluate model performance could be limited (Schaeffer, 2023), we also conduct a carefully designed human evaluation experiment by measuring the quality of mixed-modal long form responses to open-ended prompts. Chameleon-34B substantially outperforms strong baselines like Gemini-Pro and GPT-4V (Section 4), achieving a $60.4 \%$ preference rate against Gemini-Pro and a $51.6 \%$ preference rate against GPT-4V in pairwise comparisons.</p>
<p>In summary, we present the following contributions:</p>
<ul>
<li>We present Chameleon, a family of early-fusion token-based mixed-modal models capable of reasoning over and generating interleaved image-text documents, setting a new bar for open multimodal foundation models.</li>
<li>We introduce architectural innovations and training techniques that enable the stable and scalable training of early-fusion token-based models, addressing key challenges in mixed-modal learning.</li>
<li>Through extensive evaluations, we demonstrate state-of-the-art performance across a diverse set of vision-language benchmarks, while maintaining competitive performance on text-only tasks, and high quality image generation, all in the same model.</li>
<li>We conduct the first large-scale human evaluation on open-ended mixed-modal reasoning and generation, demonstrating the unique capabilities of Chameleon in this new setting.</li>
</ul>
<p>Chameleon represents a significant step towards realizing the vision of unified foundation models capable of flexibly reasoning over and generating multimodal content.</p>
<p>Prompt: I'm bored. Could you show me some cool, quirky-looking birds? Also tell me about them in 2-3 sentences.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Response: Absolutely! Here are three birds with some unique features that you might find interesting:</p>
<p>Keel-Billed Toucan The Keel-Billed Toucan is a brightly colored bird native to South American rainforests and woodlands. Its most distinctive feature is its colorful beak, which can grow up to seven inches long and has a distinctive, curved shape. The beak is used for plucking fruit from trees and for social displays. Despite its large size, the Keel-Billed Toucan is a skilled flyer and can often be seen flitting through the treetops. Here is an image of this vibrant bird: <img></p>
<p>Puffin Puffins are small, aquatic birds that are found in the northern hemisphere. They have distinctive black and white feathers, as well as a brightly colored beak that is used for attracting mates and defending their territory. Puffins are known for their distinctive "puffin" call, which sounds like a loud, guttural chuckle. They feed on fish, which they catch by diving into the water from the air. Puffins are colonial birds, and they breed in large colonies on rocky cliffs and islands. Here is an image of this adorable bird: <img>
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Golden Pheasant The Golden Pheasant is a brightly colored bird native to the forests of China. It is a member of the pheasant family and is known for its striking plumage, which features a golden-yellow body, red face, and green tail feathers. The Golden Pheasant is a ground-dwelling bird, and it feeds on a diet of insects, seeds, and fruits. It is a popular game bird, and is often hunted for its meat and feathers. In captivity, the Golden Pheasant is sometimes kept as an ornamental bird, due to its striking appearance. Here is an image of this beautiful bird: <img></p>
<p>I hope these images and descriptions have helped to brighten your day! If you have any other questions or requests, please don't hesitate to ask.</p>
<p>Figure 2 Sample interleaved image and text generation from Chameleon. The corresponding images are generated in locations marked by <img>.</p>
<h1>2 Pre-Training</h1>
<p>Chameleon represents images, in addition to text, as a series of discrete tokens and takes advantage of the scaling properties of auto-regressive Transformers (Ramesh et al., 2021; Aghajanyan et al., 2022, 2023; Yu et al., 2023). We present any ordering of images and text during training ranging from text-only, to single text/image pairs to full interleaved text-image documents.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Prompt: <img> While chameleons are masters of camouflage in the natural world, let's see how they might fare in a man-made setting.</p>
<ol>
<li>Analyze the colors, patterns, and textures in this image and identify the most challenging spot for a chameleon to blend in.</li>
<li>Then generate an image of a chameleon with the optimal pattern to camouflage itself.</li>
<li>Then, rate and explain how successful each attempt is on a scale from 1-10.</li>
</ol>
<p>Response: The image is of a stop sign in a forested area, with some sunlight filtering through the trees. The sign has a red octagonal shape with a white border and the word "STOP" written in white letters. In the background, there are green leaves and trees.</p>
<p>Challenging Spot for a Chameleon: The stop sign would be the most challenging spot for a chameleon to blend in due to its bright red color and bold white letters, which would stand out in the forested environment.</p>
<p>Optimal Pattern for Camouflage: To camouflage itself on the stop sign, a chameleon would need to have a pattern that would break up its outline and make it less conspicuous. A pattern with shades
<img alt="img-4.jpeg" src="img-4.jpeg" />
of brown and red would help the chameleon blend in more effectively. Additionally, a mottled or dappled pattern would help to scatter light and further reduce the chameleon's visibility.
<img>
Rating and Explanation: A rating of $2 / 10$ would be given for the chameleon's ability to blend in on the stop sign. While the chameleon's green and brown pattern helps to break up its outline, the bright red color of the stop sign will still make it stand out in the forested environment.</p>
<p>Figure 3 Sample generation from Chameleon.</p>
<h3>2.1 Tokenization</h3>
<p>Image Tokenization We train a new image tokenizer based on Gafni et al. (2022), which encodes a $512 \times 512$ image into 1024 discrete tokens from a codebook of size 8192. For training this tokenizer, we use only licensed images. Given the importance of generating human faces, we up-sample the percentage of images with faces during pre-training by 2 times. A core weakness of our tokenizer is in reconstructing images with a large amount of text, therefore upper bounding the capability of our models, when it comes to heavy OCR-related tasks.</p>
<p>Tokenizer We train a new BPE tokenizer (Sennrich et al., 2016) over a subset of the training data outlined below with a vocabulary size of 65,536 , which includes the 8192 image codebook tokens, using the sentencepiece library (Kudo and Richardson, 2018).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Prompt: What can I bake with this? Give me the full recipe and show me an image with the final dish. <img> Response: Bananas are a versatile fruit that can be used in various baked goods, such as muffins, bread, cakes, and cookies. They add natural sweetness, moisture, and a subtle banana flavor to the finished product. Additionally, bananas are rich in potassium, fiber, and other nutrients, making them a healthy addition to your baking repertoire.</p>
<p>Here is a simple recipe for banana bread that uses 3 ripe bananas:
Ingredients:</p>
<ul>
<li>3 ripe bananas, mashed</li>
<li>$11 / 2$ cups all-purpose flour</li>
<li>1 tsp baking powder</li>
<li>$1 / 2$ tsp baking soda</li>
<li>$1 / 2$ tsp salt</li>
<li>$1 / 4$ cup unsalted butter, softened</li>
<li>1 cup granulated sugar</li>
<li>2 large eggs</li>
<li>1 tsp vanilla extract</li>
<li>$1 / 2$ cup chopped walnuts (optional)</li>
</ul>
<p>Instructions:</p>
<ul>
<li>Preheat your oven to $350^{\circ} \mathrm{F}\left(180^{\circ} \mathrm{C}\right)$. Grease a 9 x 5 -inch loaf pan and set it aside.</li>
<li>In a medium bowl, whisk together the flour, baking powder, baking soda, and salt. Set aside.</li>
<li>In a large mixing bowl, use an electric mixer to cream together the butter and sugar until light and fluffy, about 2-3 minutes.</li>
<li>Beat in the eggs one at a time, followed by the mashed bananas and vanilla extract.
<img alt="img-6.jpeg" src="img-6.jpeg" /></li>
<li>Gradually mix in the dry ingredients until just combined, being careful not to overmix.</li>
<li>If using walnuts, fold them into the batter.</li>
<li>Pour the batter into the prepared loaf pan and smooth the top.</li>
<li>Bake for 50-60 minutes, or until a toothpick inserted into the center of the bread comes out clean.</li>
<li>Remove the bread from the oven and let it cool in the pan for 10 minutes before transferring it to a wire rack to cool completely. <img></li>
</ul>
<p>Figure 4 Sample Chameleon outputs.</p>
<h1>2.2 Pre-Training Data</h1>
<p>We delineate the pre-training stage into two separate stages. The first stage takes up the first $80 \%$ of training while the second stage takes the last $20 \%$. For all Text-To-Image pairs we rotate so that $50 \%$ of the time the image comes before the text (i.e., captioning).</p>
<h3>2.2.1 First Stage</h3>
<p>In the first stage we use a data mixture consisting of the following very large scale completely unsupervised datasets.</p>
<p>Text-Only: We use a variety of textual datasets, including a combination of the pre-training data used to train LLaMa-2 (Touvron et al., 2023) and CodeLLaMa (Roziere et al., 2023) for a total of 2.9 trillion text-only tokens.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
(a) Uncontrolled growth of output norms is a strong indicator of future training divergence.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(b) An ablation with Chameleon-7B with and without QK-Norm.
<img alt="img-9.jpeg" src="img-9.jpeg" />
(c) An ablation with Chameleon-7B with and without dropout.</p>
<p>Figure 5 Output norm and training loss curves for Chameleon models under various settings.</p>
<p>Text-Image: The text-image data for pre-training is a combination of publicly available data sources and licensed data. The images are then resized and center cropped into $512 \times 512$ images for tokenization. In total, we include 1.4 billion text-image pairs, which produces $\mathbf{1 . 5}$ trillion text-image tokens.</p>
<p>Text/Image Interleaved: We procure data from publicly available web sources, not including data from Meta's products or services, for a total of $\mathbf{4 0 0}$ billion tokens of interleaved text and image data similar to Laurençon et al. (2023). We apply the same filtering for images, as was applied in Text-To-Image.</p>
<h1>2.2.2 Second Stage</h1>
<p>In the second stage, we lower the weight of the first stage data by $50 \%$ and mix in higher quality datasets while maintaining a similar proportion of image text tokens.</p>
<p>We additionally include a filtered subset of the train sets from a large collection of instruction tuning sets.</p>
<h3>2.3 Stability</h3>
<p>It was challenging to maintain stable training when scaling the Chameleon models above 8B parameters and 1 T tokens, with instabilities often only arising very late in training. We adopted to following recipe for architecture and optimization to achieve stability.</p>
<p>Architecture Our architecture largely follows LLaMa-2 (Touvron et al., 2023). For normalization, we continue to use RMSNorm (Zhang and Sennrich, 2019); we use the SwiGLU (Shazeer, 2020) activation function and rotary positional embeddings (RoPE) (Su et al., 2021).</p>
<p>We found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the mid-to-late stages of training. We narrowed down the cause of the divergence to the softmax operation being problematic when training with multiple modalities of significantly varying entropy due to the translation invariant property of softmax (i.e., $\operatorname{softmax}(z)=\operatorname{softmax}(z+c)$ ). Because we share all weights of the model across modalities, each modality will try to "compete" with the other by increasing its norms slightly; while not problematic at the beginning of training, it manifests in divergences once we get outside the effective representation range of bf16 (In Figure 6b, we show that ablations without image generation did not diverge). In a unimodal setting, this problem has also been named the logit drift problem (Wortsman et al., 2023). In Figure 5a, we plot the norms of the output of the last transformer layer as training progresses and we find that although training divergences can manifest after as much as even $20-30 \%$ of training progress, monitoring uncontrolled growth of output norms is strongly correlated with predicting future loss divergence.</p>
<p>The softmax operation appears in two places in transformers: the core attention mechanism and the softmax over the logits. As inspired by Dehghani et al. (2023) and Wortsman et al. (2023), we first deviate from the Llama architecture by using query-key normalization (QK-Norm). QK-Norm directly controls the norm growth of input to the softmax by applying layer norm to the query and key vectors within the attention.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" />
(a) Training Curves for 600k steps for Chameleon-7B and Chameleon-34B over Mixed-Modal Data.
<img alt="img-11.jpeg" src="img-11.jpeg" />
(b) Training loss curve with image generation disabled does not suffer from instability issues.
<img alt="img-12.jpeg" src="img-12.jpeg" />
(c) For Chameleon-34B, using dropout does not fix divergences, both with and without normreordering.</p>
<p>Figure 6 Training loss curves for Chameleon models under various settings.</p>
<p>In Figure 5b, we show training loss curves for Chameleon-7B with and without QK-Norm, and the latter diverges after approximately $20 \%$ of a training epoch.</p>
<p>We found that to stabilize Chameleon-7B by controlling norm growth, it was necessary to introduce dropout after the attention and feed-forward layers, in addition to QK-norm (see Figure 5c). However, this recipe was not enough to stabilitize, Chameleon-34B, which required an additional re-ordering of the norms. Specifically, we use the strategy of normalization proposed in Liu et al. (2021, 2022), within the transformer block. The benefit of the Swin transformer normalization strategy is that it bounds the norm growth of the feedforward block, which can become additionally problematic given the multiplicate nature of the SwiGLU activation function. If $h$ represents the hidden vector at time-step $t$ after self-attention is applied to input $x$,</p>
<p>$$
\begin{aligned}
\text { Chameleon-34B: } &amp; h=x+\text { attention_norm }(\text { attention }(x)) \
&amp; \text { output }=h+\text { ffn_norm }(\text { feed_forward }(h)) \
\text { Llama2: } &amp; h=x+\text { attention }(\text { attention_norm }(x)) \
&amp; \text { output }=h+\text { feed_forward }(\text { ffn_norm }(h))
\end{aligned}
$$</p>
<p>There was no difference in perplexity when training a model from scratch with and without the normalization re-ordering until the divergence of the LLaMa-2 parameterization. Additionally, we found that this type of normalization did not work well in combination with dropout and therefore, we train Chameleon-34B without dropout (Figure 6c). Furthermore, we retroactively found that Chameleon-7B can also be stably trained without dropout, when using norm-reordering, but QK-norm is essential in both cases. We plot training curves for the first 600 k steps for both Chameleon-7B and Chameleon-34B in Figure 6a. We note that similar attempts at stabilizing multi-modal trainings were concurrently discovered in (Lu et al., 2023).</p>
<p>Optimization Our training process uses the AdamW optimizer (Loshchilov and Hutter, 2017), with $\beta_{1}$ set to 0.9 and $\beta_{2}$ to 0.95 , with an $\epsilon=10^{-5}$. We use a linear warm-up of 4000 steps with an exponential decay schedule of the learning rate to 0 . Additionally, we apply a weight decay of 0.1 and global gradient clipping at a threshold of 1.0. We use a dropout of 0.1 (Srivastava et al., 2014) for Chameleon-7B for training stability, but not for Chameleon-34B (see Figure 5c and 6c).</p>
<p>The application of QK-Norm while helping the inner softmaxes within the Transformer does not solve the problem of logit shift in the final softmax. Following Chowdhery et al. (2022); Wortsman et al. (2023), we apply z-loss regularization. Specifically, we regularize the partition function $Z$ of the softmax function $\sigma(x)<em i="i">{i}=\frac{e^{x</em> Z$ to our loss function.}}}{Z}$ where $Z=\sum_{i} e^{x_{i}}$ by adding $10^{-5} \log ^{2</p>
<p>For Chameleon-7B it was important to use both dropout and z-loss to achieve stability, while Chameleon-34B only required z-loss (Figure 6c).</p>
<p>Chameleon-7B was trained with a global batch size of $2^{23}(\sim 8 \mathrm{M})$ tokens and Chameleon-34B was trained with a global batch size of $3 \times 2^{22}(\sim 12 \mathrm{M})$ tokens. We do 2.1 epochs over our full training dataset for a total</p>
<p>of 9.2 trillion tokens seen during training. We show the first 600 k steps of training ( $55 \%$ for Chameleon-7B and $80 \%$ for Chameleon-34B) in Figure 6a.</p>
<p>Table 1 Summary of core architecture and optimization decisions made in Chameleon in contrast to LLaMa-1 and LLaMa-2.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>Context Length</th>
<th>GQA</th>
<th>Tokens</th>
<th>LR</th>
<th>Epochs</th>
<th>Dropout</th>
<th>Zloss</th>
<th>Qknorm</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMa-1</td>
<td>7B</td>
<td>2 k</td>
<td>$\times$</td>
<td>1.0 T</td>
<td>$3.0 \times 10^{-4}$</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>$\times$</td>
</tr>
<tr>
<td></td>
<td>33B</td>
<td>2 k</td>
<td>$\times$</td>
<td>1.4 T</td>
<td>$1.5 \times 10^{-4}$</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>$\times$</td>
</tr>
<tr>
<td>LLaMa-2</td>
<td>7B</td>
<td>4 k</td>
<td>$\times$</td>
<td>2.0 T</td>
<td>$3.0 \times 10^{-4}$</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>$\times$</td>
</tr>
<tr>
<td></td>
<td>34B</td>
<td>4 k</td>
<td>$\checkmark$</td>
<td>2.0 T</td>
<td>$1.5 \times 10^{-4}$</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
<td>$\times$</td>
</tr>
<tr>
<td>Chameleon</td>
<td>7B</td>
<td>4 k</td>
<td>$\times$</td>
<td>4.4 T</td>
<td>$1.0 \times 10^{-4}$</td>
<td>2.1</td>
<td>0.1</td>
<td>$10^{-5}$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td></td>
<td>34B</td>
<td>4 k</td>
<td>$\checkmark$</td>
<td>4.4 T</td>
<td>$1.0 \times 10^{-4}$</td>
<td>2.1</td>
<td>0.0</td>
<td>$10^{-5}$</td>
<td>$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Pre-Training Hardware Our model pretraining was conducted on Meta's Research Super Cluster (RSC) (Lee and Sengupta, 2022), and our alignment was done on other internal research clusters. NVIDIA A100 80 GB GPUs power both environments. The primary distinction is the interconnect technology: RSC employs NVIDIA Quantum InfiniBand, whereas our research cluster utilizes Elastic Fabric. We report our GPU usage for pre-training in Table 2.</p>
<p>Table 2 Chameleon Model Pre-Training Resource Usage</p>
<table>
<thead>
<tr>
<th>Chameleon</th>
<th>Concurrent GPUs</th>
<th>GPU Hours</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B</td>
<td>1024</td>
<td>856481</td>
</tr>
<tr>
<td>34B</td>
<td>3072</td>
<td>4282407</td>
</tr>
</tbody>
</table>
<h1>2.4 Inference</h1>
<p>To support alignment and evaluation, both automated and human, and to demonstrate the applicationreadiness of our approach, we augment the inference strategy with respect to interleaved generation to improve throughput and reduce latency.</p>
<p>Autoregressive, mixed-modal generation introduces unique performance-related challenges at inference time. These include:</p>
<ul>
<li>Data-dependencies per-step - given that our decoding formulation changes depending on whether the model is generating images or text at a particular step, tokens must be inspected at each step (i.e. copied from the GPU to the CPU in a blocking fashion) to guide control flow.</li>
<li>Masking for modality-constrained generation - to facilitate exclusive generation for a particular modality (e.g. image-only generation), tokens that do not fall in a particular modality space must be masked and ignored when de-tokenizing.</li>
<li>Fixed-sized text units - unlike text-only generation, which is inherently variable-length, token-based image generation produces fixed-size blocks of tokens corresponding to an image.</li>
</ul>
<p>Given these unique challenges, we built a standalone inference pipeline based on PyTorch (Paszke et al., 2019) supported with GPU kernels from xformers (Lefaudeux et al., 2022).</p>
<p>Our inference implementation supports streaming for both text and images. When generating in a streaming fashion, token-dependent conditional logic is needed at each generation step. Without streaming, however, blocks of image tokens can be generated in a fused fashion without conditional computation. In all cases, token masking removes branching on the GPU. Even in the non-streaming setting, however, while generating text, each output token must be inspected for image-start tokens to condition image-specific decoding augmentations.</p>
<p>Table 3: Supervised Fine-Tuning Dataset Statistics</p>
<table>
<thead>
<tr>
<th></th>
<th>Category</th>
<th># of Samples</th>
<th># of Tokens</th>
<th># of Images</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Text</td>
<td>1.6M</td>
<td>940.0M</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Code</td>
<td>14.1K</td>
<td>1.1M</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Prompt: Can you write a poem using rhyming couplets?
Response: Sure, here's a short poem using rhyming couplets:</p>
<p>The sun sets in the sky, A beautiful sight to the eye. The stars come out to play, A magical sight to behold.</p>
<p>The wind whispers through the trees,
A gentle breeze that brings ease. The birds sing their sweet melodies, A soothing sound that calms the soul.</p>
<p>The world is full of wonder, A place of beauty and thunder. So let us cherish every moment, And bask in the beauty all around us.
(a) Text</p>
<p>Prompt: Given a list of integers, find the first number that appears at least twice.
Response:
def find_duplicate(my_list):
seen $=$ set()
for num in my_list:
if num in seen:
return num
seen.add(num)
return None
(b) Code</p>
<p>Prompt: Renaissance square of Stepanakert
Response:
<img alt="img-13.jpeg" src="img-13.jpeg" />
(c) Image Generation</p>
<p>Prompt: Which elements contribute to this flower arrangement's distinctiveness, and what sets its surroundings apart?
Response: The image features an autumnal-themed floral arrangement in a classically designed vase. The arrangement stands out with its striking combination of red maple leaves, purple flowers, and pink-red roses. The vase's rosettes give it an antique design. In the background, blurry green vegetation is visible, creating a contrast to the autumnal theme of the bouquet. The red maple leaves, both in the arrangement and scattered around the vase, evoke autumn, while the green vegetation suggests a more summer-like setting. The outdoor setting is bright and sunny, adding to the contrast between the arrangement and its settings.
(d) Visual Chat</p>
<p>Gyumri is an urban municipal community and the second-largest city in Armenia, serving as the administrative center of Shirak Province in the northwestern part of the country.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Archaeological excavations conducted throughout the Soviet period have shown that the area of modern-day Gyumri has been populated since at least the third millennium BC.
<img alt="img-15.jpeg" src="img-15.jpeg" />
(e) Interleaved Text/Image Generation</p>
<p>Figure 7 Example alignment data for different categories.
sequence, inserting a distinct token to delineate the end of a prompt and the beginning of an answer. We use an autoregressive training objective, selectively masking the loss for the prompt tokens. This targeted approach allows us to optimize the model exclusively based on the answer tokens, which provides slight gains overall. We also apply a dropout of 0.05 . Additionally, we maintain the same zloss that was used during</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 8 Task categories and examples of prompts. Image attributions: Seguin (2010); Agriflanders (2009); Tuszyński (2015); Sokolov (2022).
pre-training. During supervised fine-tuning, images in the prompt are resized with border padding to ensure that all the information is available in the image, whereas images in the answer are center-cropped to ensure visually good image generation quality.</p>
<h1>4 Human Evaluations and Safety Testing</h1>
<p>Chameleon has significant new mixed modal understanding and generation abilities that cannot be measured with existing benchmarks. In this section, we detail how we conduct human evaluations on large multi-modal language models' responses to a set of diverse prompts that regular users may ask daily. We first introduce how we collect the prompts and then describe our baselines and evaluation methods, along with the evaluation results and analysis. A safety study is also included in this section.</p>
<h3>4.1 Prompts for Evaluation</h3>
<p>We work with a third-party crowdsourcing vendor to collect a set of diverse and natural prompts from human annotators. Specifically, we ask annotators to creatively think about what they want a multi-modal model to generate for different real-life scenarios. For example, for the scenario of "imagine you are in a kitchen", annotators may come up with prompts like "How to cook pasta?" or "How should I design the layout of my island? Show me some examples." The prompts can be text-only or text with some images, and the expected responses should be mixed-modal, containing both text and images.</p>
<p>After collecting an initial set of prompts, we ask three random annotators to evaluate whether the prompts are clear and whether they expect the responses to contain images. We use a majority vote to filter unclear prompts and prompts that don't expect mixed-modal responses. In the end, our final evaluation set contains 1,048 prompts: $441(42.1 \%)$ are mixed-modal (i.e., containing both text and images), and the remaining 607 $(57.9 \%)$ are text-only.</p>
<p>To better understand the tasks users would like a multi-modal AI system to fulfill, we manually examine</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 9 Performance of Chameleon vs baselines, on mixed-modal understanding and generation on a set of diverse and natural prompts from human annotators.
the prompts and classify them into 12 categories. The description of these task categories ${ }^{1}$, as well as their example prompts, can be found in Figure 8.</p>
<h1>4.2 Baselines and Evaluations</h1>
<p>We compare Chameleon 34B with OpenAI GPT-4V and Google Gemini Pro by calling their APIs. While these models can take mixed-modal prompts as input, their responses are text-only. We create additional baselines by augmenting GPT-4V and Gemini responses with images to have even stronger baselines. Specifically, we instruct these models to generate image captions by adding the following sentence at the end of each original input prompt: "If the question requires an image to be generated, then generate an image caption instead and enclose the caption in a pair of (caption) (/caption) tags." We then use OpenAI DALL-E 3 to generate images conditioned on these captions and replace the captions in the original responses with those generated images. We denote the enhanced responses as GPT-4V+ and Gemini+ in this section. Working with the same third-party crowdsourcing vendor, we conduct two types of evaluations to measure the model performance: absolute and relative.</p>
<h3>4.2.1 Absolute Evaluation</h3>
<p>For absolute evaluations, the output of each model is judged separately by asking three different annotators a set of questions regarding the relevance and quality of the responses. Below, we give detailed results and analysis on the most critical question, whether the response fulfills the task described in the prompt.</p>
<p>On task fulfillment, we ask annotators whether the response fulfills, partially fulfills, or does not fulfill the task described in the prompt. As shown in Figure 9a, much more of Chameleon's responses are considered to have completely fulfilled the tasks: $55.2 \%$ for Chameleon vs. $37.6 \%$ of Gemini+ and $44.7 \%$ of GPT-4V+. When judging the original responses of Gemini and GPT-4V, the annotators consider much fewer prompts to be fully fulfilled: Gemini completely fulfills $17.6 \%$ of the tasks and GPT-4V $23.1 \%$. We suspect that because all the prompts expect mixed-modal output, the text-only responses from Gemini and GPT-4V might be viewed as only partially completing the tasks by the annotators.</p>
<p>The task fulfillment rates in each category and in each input modality can be found in Appendix B. The task categories that Chameleon performs well include Brainstorming, Comparison, and Hypothetical, and the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>categories Chameleon needs to improve include Identification and Reasoning. On the other hand, we don't see that the model performance differs a lot when comparing mixed-modality and text-only prompts, although Chameleon seems to perform slightly better on text-only prompts, while Gemini+ and GPT-4V+ are slightly better on mixed-modal ones. Figure 2 shows an example of Chameleon's response to a brainstorming prompt.</p>
<h1>4.2.2 Relative Evaluation</h1>
<p>For relative evaluations, we directly compare Chameleon with each baseline model by presenting their responses to the same prompt in random order and asking human annotators which response they prefer. The options include the first response, the second response, and about the same. Figure 9b shows Chameleon's win rates ${ }^{2}$ over the baselines. Compared with Gemini+, Chameleon's responses are better in $41.5 \%$ of the cases, $34.5 \%$ are tie, and $24.0 \%$ are inferior. Annotators also think that Chameleon's responses are slightly more often better than GPT-4V+, with $35.8 \%$ win, $31.6 \%$ tie, and $32.6 \%$ loss. Overall, Chameleon has win rates of $60.4 \%$ and $51.6 \%$ over Gemini+ and GPT-4V+, respectively. When compared with the original responses from Gemini without the augmented images, Chameleon's responses are considered better in $53.5 \%$ of the cases, $31.2 \%$ are tied, and $15.3 \%$ are inferior. Chameleon's responses are also considered better than GPT-4V more frequently, with $46.0 \%$ win, $31.4 \%$ tie, and $22.6 \%$ loss. Chameleon's win rates over Gemini and GPT-4V are $69.1 \%$ and $61.7 \%$, respectively.</p>
<h3>4.3 Inter-annotator Agreement</h3>
<p>Every question in our evaluation is answered by three different human annotators, and we take the majority votes as the final answer. To understand the quality of the human annotators and whether the questions we asked are reasonably designed, we examine the level of agreement between different annotators.</p>
<p>The levels of agreement on each question in the absolute evaluation are shown in Figure 10.
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 10 The inter-annotator agreement on the questions in the absolute evaluation.</p>
<p>For questions about simple, objective properties of the responses, we very rarely see three annotators disagree with each other. For example, annotators have unanimous judgments on whether the model responses contain objectionable content (e.g., hate speech); in this case, all models produce safe responses. For some questions, such as whether the response fulfills the task or whether the model interprets the prompt correctly, when one</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>annotator's judgment differs from the other two's, the decision is usually still close (e.g., fulfills vs. partially fulfills) rather than opposite (e.g., fulfills vs. does not fulfill). ${ }^{3}$</p>
<p>Table 4 The inter-annotator agreement on relative evaluations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">All 3 annotators agree</th>
<th style="text-align: left;">2 of 3 annotators agree</th>
<th style="text-align: left;">No Agreement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chameleon vs. Gemini+</td>
<td style="text-align: left;">$331(31.5 \%)$</td>
<td style="text-align: left;">$609(58.1 \%)$</td>
<td style="text-align: left;">$108(10.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Chameleon vs. GPT-4V+</td>
<td style="text-align: left;">$371(35.4 \%)$</td>
<td style="text-align: left;">$579(55.2 \%)$</td>
<td style="text-align: left;">$98(9.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Chameleon vs. Gemini</td>
<td style="text-align: left;">$317(30.2 \%)$</td>
<td style="text-align: left;">$621(59.3 \%)$</td>
<td style="text-align: left;">$110(10.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Chameleon vs. GPT-4V</td>
<td style="text-align: left;">$300(28.6 \%)$</td>
<td style="text-align: left;">$611(58.3 \%)$</td>
<td style="text-align: left;">$137(13.1 \%)$</td>
</tr>
</tbody>
</table>
<p>For the relative evaluation, Table 4 shows the numbers of cases where all three annotators agree, two annotators agree, and there is no agreement. For each model pair, we have a bit higher than $10 \%$ of the cases where there is no agreement among the three annotators (considered as a tie in our evaluation.) On about $28 \%$ to $35 \%$ of the pairs, all annotators have unanimous judgments, and in about $55 \%$ to $60 \%$ of the pairs, one annotator differs from other two. This may be interpreted as Chameleon performing similarly to other baselines in many cases, making the relative evaluation challenging. ${ }^{4}$</p>
<h1>4.4 Safety Testing</h1>
<p>We crowdsource prompts that provoke the model to create unsafe content in predefined categories such as self-harm, violence and hate, and criminal planning. These prompts cover both text and mixed-modal inputs, as well as intents to produce unsafe text, images, or mixed-modal outputs. We generate the model's response to each prompt, and ask annotators to label whether the response is safe or unsafe with respect to each category's definition of safety; an unsure option is also provided for borderline responses. Table 5 shows that an overwhelming majority of Chameleon's responses are considered safe, with only $78(0.39 \%)$ unsafe responses for the 7B model and $19(0.095 \%)$ for the 30B model.</p>
<p>We also evaluate the model's ability to withstand adversarial prompting in an interactive session. For that purpose, an internal red team probed the 30B model over 445 prompt-response interactions, including multi-turn interactions. Table 5 shows that of those responses, $7(1.6 \%)$ were considered unsafe and $20(4.5 \%)$ were labeled as unsure. While further safety tuning using RLHF/RLAIF has been shown to further harden the model against jailbreaking and intentional malicious attacks, these results demonstrate that our current safety tuning approach provides significant protection for reasonable, benign usage of this research artifact.</p>
<h3>4.5 Discussion</h3>
<p>Compared to Gemini and GPT-4V, Chameleon is very competitive when handling prompts that expect interleaving, mixed-modal responses. The images generated by Chameleon are usually relevant to the context, making the documents with interleaving text and images very appealing to users. However, readers should be aware of the limitations of human evaluation. First, the prompts used in the evaluation came from crowdsourcing instead of real users who interact with a model. While we certainly have a diverse set of prompts, the coverage may still be limited, given the size of the dataset. Second, partially because our prompts focus on the mixed-modal output, certain visual understanding tasks, such as OCR or Infographics (i.e., interpreting a given chart or plot), are naturally excluded from our evaluation. Finally, at this moment, the APIs of existing multi-modal LLMs provide only textual responses. While we strengthen the baselines by augmenting their output with separately generated images, it is still preferred if we can compare Chameleon to other native mixed-modal models.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5 Safety testing on 20,000 crowdsourced prompts and 445 red team interactions provoking the model to produce unsafe content.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Safe</th>
<th style="text-align: center;">Unsafe</th>
<th style="text-align: center;">Unsure</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Crowdsourced</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$99.2 \%$</td>
<td style="text-align: center;">$0.4 \%$</td>
<td style="text-align: center;">$0.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$99.7 \%$</td>
<td style="text-align: center;">$0.1 \%$</td>
<td style="text-align: center;">$0.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Red Team</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$93.9 \%$</td>
<td style="text-align: center;">$1.6 \%$</td>
<td style="text-align: center;">$4.5 \%$</td>
</tr>
</tbody>
</table>
<h1>5 Benchmark Evaluations</h1>
<p>Given the general capabilities of Chameleon, there is not a single model that we can directly evaluate against; therefore, we evaluate against the best models in every category within our capabilities.</p>
<h3>5.1 Text</h3>
<p>We evaluate the general text-only capabilities of our pre-trained (not SFT'd) model against other state-of-theart text-only large language models. We follow the evaluation protocol outlined by Touvron et al. (2023). Specifically we evaluate all models, using an in-house evaluation platform on the areas of commonsense reasoning, reading comprehension, math problems, and world knowledge. We report our results in Table 6.</p>
<p>Table 6 Comparison of overall performance on collective academic benchmarks against open-source foundational models. * Evaluated using our framework/using API. For GSM8k/MATH, we report maj@1 unless mentioned otherwise.
${ }^{<em> </em>}$ From Gemini et al. (2023).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Chameleon</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Llama-2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mistral</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Gemini <br> Pro</th>
<th style="text-align: center;">GPT- <br> 4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">8x7B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Commonsense Reasoning and Reading Comprehension</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PIQA</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SIQA</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">95.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10-short</td>
<td style="text-align: center;">10-short</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10-short</td>
<td style="text-align: center;">10-short</td>
<td style="text-align: center;">10-short</td>
<td style="text-align: center;">10-short</td>
<td style="text-align: center;">10-short</td>
</tr>
<tr>
<td style="text-align: center;">WinoGrande</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Arc-E</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Arc-C</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">$84.7^{*}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Math and World Knowledge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GSM8k</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">92.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">75.1*</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">maj@8</td>
<td style="text-align: center;">maj@32</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">maj@32</td>
<td style="text-align: center;">CcT</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">6.24</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">$52.9^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">maj@1</td>
<td style="text-align: center;">maj@1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">maj@4</td>
<td style="text-align: center;">maj@4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">maj@4</td>
<td style="text-align: center;">maj@4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">86.4</td>
</tr>
</tbody>
</table>
<ul>
<li>Commonsense Reasoning and Reading Comprehension: We report 0 -shot performance on the following benchmarks that measure commonsense reasoning and reading comprehension capabilities: PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and BoolQ (Clark et al., 2019). We score the prompt with each candidate answer and compute accuracy using the candidate with the highest score. All baseline model performances except a few are taken directly from the reported sources. We observe that Chameleon-7B and Chameleon-34B</li>
</ul>
<p>are competitive with the corresponding Llama-2 models, with Chameleon-34B even outperforming Llama-2 70B on $5 / 8$ tasks and performing on par with Mixtral 8x7B.</p>
<ul>
<li>MATH and World Knowledge We report 8-shot performance on GSM8K (Cobbe et al., 2021) i.e., grade school math word problems and 4-shot performance on the MATH (Hendrycks et al., 2021) benchmark. We report maj@N exact match accuracy for both benchmarks by sampling N generations from the model (greedy sampling for $\mathrm{N}=1$ ) and choosing the answer via majority voting. Despite training for additional modalities, both Chameleon models demonstrate strong math capabilities. On GSM8k, Chameleon-7B outperforms the corresponding Llama-2 models, with performance comparable to Mistral 7B (50.9 vs 52.1 maj@8). Furthermore, Chameleon-34B can outperform Llama2-70B on maj@1 (61.4 vs 56.8) and Mixtral 8x7B on maj@32 (77.0 vs 75.1). Similarly, on MATH, Chameleon-7B outperforms Llama-2 and matches Mistral 7B on maj@4, while Chameleon-34B outperforms Llama2-70B, approaching the performance of Mixtral 8x7B on maj@4 (24.7 vs 28.4).</li>
</ul>
<p>We also report performance on MMLU (Hendrycks et al., 2020), which measures world/in-domain knowledge and problem-solving abilities using 57 subjects, including elementary mathematics, US history, computer science, and law. Both Chameleon models outperform their Llama-2 counterparts with Chameleon-34B approaching the performance of Mixtral 8x7B/Gemini-Pro (65.8 vs 70.6/71.8).</p>
<p>Overall, Chameleon outperforms LLaMa-2 across the board, with performance approaching Mistral 7B/Mixtral 8x7B (Jiang et al., 2023, 2024) on some tasks. These gains are likely due to multiple factors. First, we do two epochs over the LLaMa-2 pre-training data, and in general use more compute for pretraining. Second, including code data significantly improves performance on text-only reasoning tasks. Lastly, having higher quality data in the last $20 \%$ of pre-training significantly improves performance.</p>
<h1>5.2 Image-To-Text</h1>
<p>We next evaluate Chameleon on the segment of tasks that requires text generation conditioned on an image, specifically on image captioning and visual question-answering tasks, and present results of Chameleon-34B in Table 7. Together with our pre-trained model, we also present results with a model fine-tuned on all tasks together (Chameleon-34B-MultiTask), as well as models exclusively fine-tuned for the specific evaluation tasks (Chameleon-34B-SFT).</p>
<p>We evaluate against available open-source late-fusion models: specifically Flamingo 80B (Alayrac et al., 2022), IDEFICS 80B (Laurençon et al., 2023), and Llava-1.5 (Liu et al., 2023a), as well as recent closed-source models, such as Gemini (Gemini et al., 2023) and GPT4-V (OpenAI, 2023). We note that we did not take any special care when formatting the pre-training data to ensure that 0 -shot inference can be effectively done. Therefore, we augment the input images or questions with the published prompts used by other models. This was purposefully done to maintain the fidelity of the pre-training data.</p>
<ul>
<li>Image Captioning: For image captioning evaluations we report CiDER (Vedantam et al., 2015) scores on the Karpathy test split of MS-COCO (Lin et al., 2014), and the Karpathy test split of Flickr30k (Plummer et al., 2015) using the pycocoevalcap (Chen et al., 2020) package. For Chameleon models, we restrict captions to 30 tokens. We evaluated GPT-4V and Gemini models using several prompts and generation lengths via their APIs and report the best performance that we were able to achieve.</li>
</ul>
<p>In the open-source pre-trained category, Chameleon-34B (2-shot) outperforms the larger 80B models of both Flamingo and IDEFICS on COCO with 32-shots, while matching their performance on Flickr30k. With respect to fine-tuned/closed-source models, both multi-task and SFT variants of Chameleon-34B outperform all other models on COCO, while for Flickr30k, the SFT model outperforms other models with the multitask model being a close competitor.</p>
<ul>
<li>Visual Question Answering: For visual question answering (VQA) we report performance on the testdev split of VQA-v2 (Goyal et al., 2017). For VQA-v2, the pre-trained Chameleon-34B model with 2-shots matches the 32-shot performance of the larger Flamingo and IDEFICS models, while for finetuned/closed models, Chameleon-34B-Multitask approaches the performance of IDEFICS-80B-Instruct and Gemini Pro, but trails larger models such as Flamingo-80B-FT, GPT-4V, and Gemini Ultra. Llava-1.5 outperforms Chameleon-34B on VQAv2 potentially owing to its additional fine-tuning on</li>
</ul>
<p>Table 7 Model Performances on Image-to-Text Capabilities. * Evaluated using API.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Model Size</th>
<th style="text-align: center;">COCO</th>
<th style="text-align: center;">Flickr30k</th>
<th style="text-align: center;">VQAv2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Pre-trained</td>
<td style="text-align: center;">Flamingo-80B</td>
<td style="text-align: center;">80B</td>
<td style="text-align: center;">$\begin{aligned} &amp; 113.8 \ &amp; 32 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 75.1 \ &amp; 4 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 67.6 \ &amp; 32 \text {-chot } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IDEFICS-80B</td>
<td style="text-align: center;">80B</td>
<td style="text-align: center;">$\begin{aligned} &amp; 116.6 \ &amp; 32 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 73.7 \ &amp; 4 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 65.9 \ &amp; 32 \text {-chot } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Chameleon</td>
<td style="text-align: center;">Chameleon</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\begin{aligned} &amp; 120.2 \ &amp; 2 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 74.7 \ &amp; 2 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 66.0 \ &amp; 2 \text {-chot } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chameleon-SFT</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\begin{aligned} &amp; 140.8 \ &amp; 0 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 82.3 \ &amp; 2 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chameleon-MultiTask</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\begin{aligned} &amp; 139.1 \ &amp; 2 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 76.2 \ &amp; 2 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">69.6</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">Flamingo-80B-FT</td>
<td style="text-align: center;">80B</td>
<td style="text-align: center;">138.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IDEFICS-80B-Instruct</td>
<td style="text-align: center;">80B</td>
<td style="text-align: center;">$\begin{aligned} &amp; 123.2 \ &amp; 32 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 78.4 \ &amp; 32 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 68.8 \ &amp; 32 \text {-chot } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Closed Source <br> (finetuning status unknown)</td>
<td style="text-align: center;">GPT-4V</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$\begin{aligned} &amp; 78.5^{*} \ &amp; 8 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 55.3^{*} \ &amp; 8 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">77.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini Nano 2</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">67.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini Pro</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$\begin{aligned} &amp; 99.8^{*} \ &amp; 2 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 82.2^{*} \ &amp; 4 \text {-chot } \end{aligned}$</td>
<td style="text-align: center;">71.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini Ultra</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">77.8</td>
</tr>
</tbody>
</table>
<p>conversations from GPT-4, ShareGPT (ShareGPT, 2023), GQA (Hudson and Manning, 2019), and region-level VQA datasets, but significantly trails behind on the other tasks.</p>
<p>In general, we find Chameleon is fairly competitive on both image captioning and VQA tasks. It rivals other models by using much fewer in-context training examples and with smaller model sizes, in both pre-trained and fine-tuned model evaluations.</p>
<h1>6 Related Work</h1>
<p>Chameleon builds upon the lineage of works exploring token-based approaches for multimodal learning. The idea of using discrete tokens to represent continuous modalities like images was first explored in works like BEiT (Bao et al., 2021), which proposed a self-supervised vision representation learning method based on tokenized image patches. Aghajanyan et al. (2022) extended this idea to learning from mixed-modal documents through interleaved image and text tokens, allowing for joint reasoning over both modalities within a unified architecture. CM3Leon (Yu et al., 2023) further scaled up this approach to autoregressive text-to-image generation, building on the initial proposal of token-based image generation in DALL-E (Ramesh et al., 2021).</p>
<p>As a fully token-based early-fusion model, Chameleon differs from late-fusion approaches like Flamingo (Alayrac et al., 2022) which encode images and text separately before combining them at a later stage. Other models like LLaVA (Liu et al., 2023a), IDEFICS (Laurençon et al., 2023), and VisualGPT (Chen et al., 2022) also maintain separate image and text encoders. In contrast, Chameleon's unified token space allows it to seamlessly reason over and generate interleaved image and text sequences, without the need for modality-specific components. This early-fusion approach, however, comes with significant challenges in terms of representation learning and alignment, as discussed in Baltrušaitis et al. (2018).</p>
<p>The most similar model to Chameleon is Gemini (Gemini et al., 2023), which also uses an early-fusion token-based approach. However, a key difference is that Gemini uses separate image decoders, whereas Chameleon is an end-to-end dense model without any routing components. This makes Chameleon a more general-purpose model for both multimodal understanding and generation tasks, similar in spirit to the Perceiver (Jaegle et al., 2021) architecture which also aims for a unified model across modalities and tasks.</p>
<p>In summary, Chameleon builds on a rich history of work in multimodal learning and token-based architectures, while pushing the boundaries in terms of model scale and architecture design. By demonstrating strong performance across a wide range of vision-language tasks and enabling new capabilities in mixed-modal reasoning and generation, Chameleon represents a significant step towards realizing the vision of general-</p>
<p>purpose multimodal foundation models.</p>
<h1>7 Conclusion</h1>
<p>In this paper, we introduced Chameleon, a new family of early-fusion token-based foundation models that set a new bar for multimodal machine learning. By learning a unified representation space over interleaved image and text tokens, Chameleon is a single model that achieves strong performance across a wide range of vision-language benchmarks while enabling new mixed-modal reasoning and generation capabilities.</p>
<p>The key to Chameleon's success is its fully token-based architecture, which allows for seamless information integration across modalities. By quantizing images into discrete tokens and training on mixed-modal data from scratch, Chameleon learns to jointly reason over image and text in a way that is impossible with late-fusion architectures or models that maintain separate encoders for each modality. At the same time, Chameleon introduces novel techniques for stable and scalable training of early-fusion models, addressing key optimization and architectural design challenges that have previously limited the scale of such approaches. On tasks such as image captioning and visual question answering, Chameleon-34B outperforms models such as Flamingo and IDEFICS, while maintaining competitive performance on text-only benchmarks. Chameleon also unlocks entirely new possibilities for multimodal interaction, as demonstrated by its strong performance on our new benchmark for mixed-modal open-ended QA.</p>
<h2>Acknowledgements</h2>
<p>We thank Naren Briar for her invaluable contribution to manually curating safety prompts, which were crucial for our safety tuning efforts. We also thank Pierre Fernandez for his indispensable support with the Chameleon release, Shelly Sheynin for her work on the Chameleon image tokenizer, Puxin Xu and David for helping us with datasets. Additionally, we thank Mitchell Wortsman for engaging in insightful discussions about stability in large-scale language models and Mike Lewis for general discussions and advice throughout the project. We thank Aaron Grattafiori, Firat Ozgenel, Divya Shah, Danny Livshits, Cristian Canton Ferrer, Saghar Hosseini, Ramon Calderer, Joshua Saxe, Daniel Song and Manish Bhatt for their help with the safety and red teaming efforts.</p>
<h2>Contributors</h2>
<p>We attribute credit separated by bucket of work. Additionally, * indicates joint first authors, ${ }^{\dagger}$ indicates key contributors, ${ }^{\ddagger}$ indicates workstream leads, and ${ }^{\ddagger}$ indicates project leads.</p>
<p>Pre-Training: Srinivasan Iyer<em>, Bernie Huang</em>, Lili Yu ${ }^{\dagger}$, Arun Babu ${ }^{\dagger}$, Chunting Zhou ${ }^{\dagger}$, Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy ${ }^{\ddagger}$, Armen Aghajanyan ${ }^{\ddagger}$</p>
<p>Alignment and Safety: Ram Pasunuru*, Andrew Cohen ${ }^{\dagger}$, Aram H. Markosyan ${ }^{\dagger}$, Koustuv Sinha ${ }^{\dagger}$, Xiaoqing Ellen Tan ${ }^{\dagger}$, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz ${ }^{\ddagger}$</p>
<p>Inference and Evaluation: Pedro Rodriguez ${ }^{\dagger}$, Leonid Shamis ${ }^{\dagger}$, Vasu Sharma ${ }^{\dagger}$, Christine Jou, Karthik Padthe ${ }^{\dagger}$, Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn ${ }^{\ddagger}$, Daniel Li ${ }^{\ddagger}$, Scott Yih ${ }^{\ddagger}$</p>
<p>Overall Project: Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Raghavendra, Barbara Usher, Emanuele Aiello, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath (Lead PM), Ammar Rizvi (Lead TPM), Gargi Ghosh ${ }^{\ddagger}$, Luke Zettlemoyer ${ }^{\ddagger}$</p>
<h1>References</h1>
<p>Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. arXiv preprint arXiv:2301.03728, 2023.
Agriflanders. Miniatuurpaardjes prijskamp - Agriflanders 2009, 2009. https://en.wikipedia.org/wiki/File: Miniatuurpaardje.jpg. CC-BY-SA 2.0, https://creativecommons.org/licenses/by/2.0/deed.en.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.
Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence, 41(2):423-443, 2018.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 7432-7439, 2020.
Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18030-18040, 2022.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, and Ramakrishna Vedantam. https://github.com/salaniz/pycocoevalcap, 2020.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480-7512. PMLR, 2023.
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022.
Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor, 2022.</p>
<p>Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages $6700-6709,2019$.</p>
<p>Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured inputs \&amp; outputs. arXiv preprint arXiv:2107.14795, 2021.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.</p>
<p>Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669, 2023.</p>
<p>Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023.</p>
<p>Klaus Krippendorff. Content analysis: An introduction to its methodology. Sage publications, 2018.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.</p>
<p>Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv preprint arXiv:2306.16527, 2023.</p>
<p>Kevin Lee and Shubho Sengupta. Introducing the ai research supercluster - meta's cutting-edge ai supercomputer for ai research. https://ai.facebook.com/blog/ai-rsc/, 2022.</p>
<p>Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hackable transformer modelling library. https://github.com/facebookresearch/ xformers, 2022.</p>
<p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023a.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023b.</p>
<p>Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021.</p>
<p>Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin transformer v2: Scaling up capacity and resolution, 2022. https://arxiv.org/abs/2111. 09883 .</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.</p>
<p>Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action, 2023. https://arxiv.org/abs/2312.17172.</p>
<p>Giacomo Marzi, Marco Balzano, and Davide Marchiori. K-alpha calculator-krippendorff's alpha calculator: A userfriendly tool for computing krippendorff's alpha inter-rater reliability coefficient. MethodsX, 12:102545, 2024. ISSN 2215-0161. doi: https://doi.org/10.1016/j.mex.2023.102545. https://www.sciencedirect.com/science/article/pii/ S2215016123005411.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.</p>
<p>OpenAI. GPTV System Card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.</p>
<p>Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641-2649, 2015.</p>
<p>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.</p>
<p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.</p>
<p>Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.</p>
<p>Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, and Roberta Raileanu. Rainbow teaming: Open-ended generation of diverse adversarial prompts, 2024.</p>
<p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.</p>
<p>Rylan Schaeffer. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632, 2023.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.</p>
<p>Georges Seguin. Mille-feuille, 2010. https://en.wikipedia.org/wiki/File:Mille-feuille_20100916.jpg. CC-BY-SA 3.0, https://creativecommons.org/licenses/by-sa/3.0/deed.en.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In ACL, Berlin, Germany, 2016. https://aclanthology.org/P16-1162.</p>
<p>ShareGPT. GPTV System Card. https://sharegpt.com/, 2023.
Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
Maksim Sokolov. Sagrada Familia July 2022, 2022. https://en.wikipedia.org/wiki/File:Sagrada_Familia_\%28July_ 2022\%29_08.jpg. CC-BY-SA-4.0, https://creativecommons.org/licenses/by-sa/4.0/deed.en.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arxiv e-prints, art. arXiv preprint arXiv:2104.09864, 2021.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ For the question of task fulfillment, the inter-rater reliability derived by Krippendorff's Alpha (Krippendorff, 2018; Marzi et al., 2024) is 0.338 ; the $95 \%$ confidence interval is $[0.319,0.356]$, based on bootstrap sampling of 1,000 iterations.
${ }^{4}$ When comparing Chameleon with Gemini+ and GPT-4V+, the Krippendorff's Alpha values are 0.337 [0.293, 0.378] and $0.396[0.353,0.435]$, respectively.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
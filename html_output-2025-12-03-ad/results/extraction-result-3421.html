<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3421 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3421</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3421</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-220483148</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.15875v1.pdf" target="_blank">Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning</a></p>
                <p><strong>Paper Abstract:</strong> A compelling approach to complex question answering is to convert the question to a sequence of actions, which can then be executed on the knowledge base to yield the answer, aka the programmer-interpreter approach. Use similar training questions to the test question, meta-learning enables the programmer to adapt to unseen questions to tackle potential distributional biases quickly. However, this comes at the cost of manually labeling similar questions to learn a retrieval model, which is tedious and expensive. In this paper, we present a novel method that automatically learns a retrieval model alternately with the programmer from weak supervision, i.e., the system's performance with respect to the produced answers. To the best of our knowledge, this is the first attempt to train the retrieval model with the programmer jointly. Our system leads to state-of-the-art performance on a large-scale task for complex question answering over knowledge bases. We have released our code at https://github.com/DevinJake/MARL.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3421.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3421.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetA Retrieval Learning (MARL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Joint meta-learning system that alternately trains a retriever and an encoder-decoder programmer to produce executable programs for complex KB question answering; uses weak supervision (answer reward) to train the retriever and Reptile/MAML-style meta-RL to adapt the programmer from few similar support questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MARL (MetA Retrieval Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Joint system: (1) encoder-decoder BiLSTM programmer treated as a policy π(τ|q;θ) producing action sequences/programs; (2) retriever policy π(q_c|q_pri;φ) that samples N support questions using a filtered softmax (type-based mask) and DSSM similarity; training alternates meta-RL updates to θ (Reptile approximation of MAML) and REINFORCE updates to φ; uses beam search at inference and weak supervision from denotation rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CQA — Logical Reasoning category (complex KB question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Logical Reasoning questions from the CQA dataset: single-turn KB questions requiring discrete logical operations (e.g., select, intersection, count, min/max) and multiple-step program induction; Logical Reasoning category requires about three actions (higher than Simple category).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Alternate joint meta-learning of retriever and programmer: meta-RL adaptation of programmer using top-N retrieved similar questions (support set); retriever trained weakly via REINFORCE using the difference in reward between adapted and generic programmer; filter-softmax to constrain candidate pool by question type; Reptile used to approximate MAML; DSSM for similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical Reasoning F1 = 79.43% (Table 1). Overall micro F1 = 77.71%, overall macro F1 = 66.96%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla (BiLSTM RL) Logical Reasoning F1 = 76.58%; KVmem = 37.56%; CIPITR-All = 21.31%; CIPITR-Sep = 85.33%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>MARL improves Logical Reasoning F1 by +2.85 percentage points over Vanilla (79.43 - 76.58). It modestly improves over MAML variants with non-learning or random retrievers (Jaccard 78.89, Random 78.87) by +0.54 to +0.56 points, but performs worse than CIPITR-Sep (−5.90 points).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Authors report limitations: coarse-grained adaptation still insufficient for the three hardest categories (Comparative Reasoning, Quantitative (Count), Comparative (Count)), where MARL performs poorly compared to models trained per-category (CIPITR-Sep). MARL also relies on a small set of pseudo-annotated programs (≈1% via BFS) and a separately trained entity/class/relation linker (entity linking handled outside model).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation (Table 2): adding random retriever to MAML yields +0.97 pp micro-F1 over Vanilla; Jaccard retriever +1.59 pp; MARL full joint training +2.99 pp. Analysis indicates MAML-based variants outperform Vanilla, and joint training of retriever+programmer yields further gains over fixed retrievers. Per-category results show largest gains in many categories but highlight remaining weakness on the hardest categories where coarse-grained adaptation is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3421.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla BiLSTM programmer (RL fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single one-size-fits-all BiLSTM encoder-decoder programmer trained on a small pseudo-annotated set and further optimized with reinforcement learning; used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vanilla (BiLSTM-based programmer + RL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BiLSTM encoder-decoder programmer trained on ≈1% pseudo-gold BFS-annotated data (Q_pre) then optimized with reinforcement learning on additional unannotated samples; beam search used for program prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CQA — Logical Reasoning category</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above: logical multi-step KB queries requiring discrete operators and program induction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised pretraining on small pseudo-annotated set (BFS-generated action sequences), followed by RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical Reasoning F1 = 76.58%. Overall micro F1 = 74.72%, macro F1 = 64.01%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>One-size-fits-all model struggles when training data is diverse — less common knowledge is not well captured, leading to lower adaptability to different question types compared to meta-learning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Serves as primary baseline for ablation: MAML-based variants (Random, Jaccard, MARL) outperform Vanilla, indicating meta-learning and retrieval provide task-specific adaptation advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3421.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CIPITR-Sep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CIPITR (separate per-category models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural Program Induction approach that trains separate models per question category, using high-level constraints to guide program generation without gold programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Complex program induction for querying knowledge bases in the absence of gold programs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CIPITR-Sep</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CIPITR trains independent NPI-style models per CQA question category and employs high-level constraints to produce semantically plausible programs under weak supervision (terminal rewards) without annotated programs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CQA — Logical Reasoning category</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Neural Program Induction with category-specific training and high-level constraints to guide search in absence of gold programs (per-category one-size-fits-all).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical Reasoning F1 = 85.33% (Table 1). CIPITR-Sep achieves very high per-category performance because it trains a separate model per question type.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires training separate models per category (CIPITR-Sep), which is costly and not a single adaptive model; may not generalize across categories. The paper notes CIPITR-All (single model) performs much worse than CIPITR-Sep, illustrating that per-category specialization helps but lacks transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comparison shows CIPITR-Sep outperforms MARL on some categories (including Logical Reasoning) because it optimizes per-type models; this highlights tradeoff between per-category specialization and single-model adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3421.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CIPITR-All</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CIPITR (single model across all categories)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CIPITR variant where a single model is trained over all question types (one-size-fits-all), used as baseline for breadth-generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Complex program induction for querying knowledge bases in the absence of gold programs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CIPITR-All</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same neural program induction approach as CIPITR but trained as a single model across all categories rather than per-category specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CQA — Logical Reasoning category</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Neural Program Induction across mixed-category training data; uses high-level constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical Reasoning F1 = 21.31% (Table 1) — substantially worse than CIPITR-Sep and MARL in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Per authors, one-size-fits-all CIPITR-All struggles when data varies widely across categories; difficulty finding weights that fit diverse examples and class imbalance harms performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Contrast between CIPITR-All and CIPITR-Sep underscores the benefit of per-category specialization; motivates meta-learning approaches that aim to adapt per-task without training separate models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3421.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KVmem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KVmem (Key-Value memory model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline CQA architecture combining Hierarchical Recurrent Encoder-Decoder (HRED) with a Key-Value memory network that attends to memory entries to predict answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KVmem</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Memory-augmented sequence model (HRED + Key-Value memory) that retrieves related memory entries and attends to encoded vectors to predict answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CQA — Logical Reasoning category</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Memory network retrieval and attention over key-value pairs (no specialist meta-learning or program induction in this baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical Reasoning F1 = 37.56% (Table 1). Overall micro F1 = 31.18%, macro F1 = 19.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Per-table results, KVmem performs poorly on multi-step logical/programmatic questions compared to program-induction or meta-learning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Serves as a non-program-induction baseline demonstrating the need for programmatic solutions or meta-adaptation for complex KB queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3421.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random retriever (MAML variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAML variant with random retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta-learning (MAML/Reptile-style) programmer that uses a retriever which randomly selects support questions within the same category; included as an ablation/variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Random-retriever MAML variant</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MAML-based meta-learning of programmer where the retriever samples N support questions uniformly at random from same-type pool (filter applied), used to measure impact of learned retriever vs random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CQA — Logical Reasoning category</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>MAML-style meta-learning with random support set selection (same type filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical Reasoning F1 = 78.87% (Table 1). Overall micro F1 = 75.69%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla Logical Reasoning F1 = 76.58%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Random retriever improves Logical Reasoning F1 by +2.29 points over Vanilla (78.87 - 76.58).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Random support selection may include non-analogous examples; gains show that meta-learning itself helps but learned retriever yields further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation: Random retriever yields +0.97 pp micro-F1 over Vanilla; demonstrates part of the gain comes from meta-learning independent of retriever quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3421.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jaccard retriever (MAML variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAML variant with Jaccard-similarity retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MAML-based programmer that uses a fixed non-learning retriever based on Jaccard similarity of question words as an ablation variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Jaccard-retriever MAML variant</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MAML/Reptile meta-learning of programmer where support sets are selected by fixed Jaccard-similarity over question word overlap (with type filter).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CQA — Logical Reasoning category</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fixed, non-learned retriever based on Jaccard word overlap used to form support sets for meta-learning adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Logical Reasoning F1 = 78.89% (Table 1). Overall micro F1 = 76.31%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla Logical Reasoning F1 = 76.58%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Jaccard retriever improves Logical Reasoning F1 by +2.31 points over Vanilla (78.89 - 76.58).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fixed similarity heuristics may miss deeper semantic analogies; joint learned retriever (MARL) further improves over Jaccard (+0.54 pp on Logical Reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation: Jaccard retriever yields +1.59 pp micro-F1 improvement over Vanilla; comparing Jaccard to MARL indicates benefit of jointly training retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3421.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D2A</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dialog-to-Action (D2A)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Behavior-cloning approach to complex question answering that learns to map dialog/question context to action sequences using BFS-annotated pseudo-gold programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dialog-to-action: conversational question answering over a large-scale knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dialog-to-Action (D2A)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Behavior cloning trained from BFS-generated annotations (pseudo-gold) to learn programs for complex QA; primarily for conversational multi-turn KBQA in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Related: KBQA / semantic parsing tasks (not evaluated on CQA logical reasoning in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used for mapping dialog or questions to action sequences/programs; requires logical composition of operations when present.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised imitation learning from BFS-annotated programs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Mentioned as prior work; requires annotated programs (BFS pseudo-gold) and may not address weak supervision/meta-retrieval joint training considered in MARL.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Cited as related work; no ablation results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3421.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-Symbolic Machines (NSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural program induction framework that learns semantic parsers with weak supervision by anchoring high-reward programs and combining neural generation with symbolic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural-Symbolic Machines (NSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural program generator combined with symbolic execution over a KB; uses pseudo-gold programs and deterministic probabilities to anchor learning under weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Related: semantic parsing / KBQA tasks involving multi-step logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Addresses multi-hop and compositional logical queries by generating programs executed on KBs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Neural program induction with pseudo-gold annotations and deterministic anchoring of high-reward programs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Mentioned as related work; not experimentally compared in this paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Cited for context; no ablation data here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3421.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3421.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stable Sparse Reward based Programmer (SSRP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An NPI-style approach for KBQA that alleviates sparse-reward problems by guiding program generation toward semantically plausible programs under weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural program induction for kbqa without gold programs or query annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SSRP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural program induction approach that introduces techniques to handle sparse rewards and produce plausible programs when gold programs are not available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Related: complex KBQA / logical program induction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Targets program induction for complex KB queries under weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>High-level constraints and reward-shaping techniques to stabilize learning under sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Mentioned in related work; not evaluated directly in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Cited to contextualize MARL's contributions; no analysis here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. <em>(Rating: 2)</em></li>
                <li>Complex program induction for querying knowledge bases in the absence of gold programs. <em>(Rating: 2)</em></li>
                <li>Coupling retrieval and metalearning for context-dependent semantic parsing. <em>(Rating: 2)</em></li>
                <li>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. <em>(Rating: 2)</em></li>
                <li>Dialog-to-action: conversational question answering over a large-scale knowledge base. <em>(Rating: 1)</em></li>
                <li>Neural program induction for kbqa without gold programs or query annotations. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3421",
    "paper_id": "paper-220483148",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "MARL",
            "name_full": "MetA Retrieval Learning (MARL)",
            "brief_description": "Joint meta-learning system that alternately trains a retriever and an encoder-decoder programmer to produce executable programs for complex KB question answering; uses weak supervision (answer reward) to train the retriever and Reptile/MAML-style meta-RL to adapt the programmer from few similar support questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MARL (MetA Retrieval Learning)",
            "model_description": "Joint system: (1) encoder-decoder BiLSTM programmer treated as a policy π(τ|q;θ) producing action sequences/programs; (2) retriever policy π(q_c|q_pri;φ) that samples N support questions using a filtered softmax (type-based mask) and DSSM similarity; training alternates meta-RL updates to θ (Reptile approximation of MAML) and REINFORCE updates to φ; uses beam search at inference and weak supervision from denotation rewards.",
            "model_size": null,
            "reasoning_task_name": "CQA — Logical Reasoning category (complex KB question answering)",
            "reasoning_task_description": "Logical Reasoning questions from the CQA dataset: single-turn KB questions requiring discrete logical operations (e.g., select, intersection, count, min/max) and multiple-step program induction; Logical Reasoning category requires about three actions (higher than Simple category).",
            "method_or_intervention": "Alternate joint meta-learning of retriever and programmer: meta-RL adaptation of programmer using top-N retrieved similar questions (support set); retriever trained weakly via REINFORCE using the difference in reward between adapted and generic programmer; filter-softmax to constrain candidate pool by question type; Reptile used to approximate MAML; DSSM for similarity.",
            "performance": "Logical Reasoning F1 = 79.43% (Table 1). Overall micro F1 = 77.71%, overall macro F1 = 66.96%.",
            "baseline_performance": "Vanilla (BiLSTM RL) Logical Reasoning F1 = 76.58%; KVmem = 37.56%; CIPITR-All = 21.31%; CIPITR-Sep = 85.33%.",
            "improvement_over_baseline": "MARL improves Logical Reasoning F1 by +2.85 percentage points over Vanilla (79.43 - 76.58). It modestly improves over MAML variants with non-learning or random retrievers (Jaccard 78.89, Random 78.87) by +0.54 to +0.56 points, but performs worse than CIPITR-Sep (−5.90 points).",
            "limitations_or_failures": "Authors report limitations: coarse-grained adaptation still insufficient for the three hardest categories (Comparative Reasoning, Quantitative (Count), Comparative (Count)), where MARL performs poorly compared to models trained per-category (CIPITR-Sep). MARL also relies on a small set of pseudo-annotated programs (≈1% via BFS) and a separately trained entity/class/relation linker (entity linking handled outside model).",
            "ablation_or_analysis": "Ablation (Table 2): adding random retriever to MAML yields +0.97 pp micro-F1 over Vanilla; Jaccard retriever +1.59 pp; MARL full joint training +2.99 pp. Analysis indicates MAML-based variants outperform Vanilla, and joint training of retriever+programmer yields further gains over fixed retrievers. Per-category results show largest gains in many categories but highlight remaining weakness on the hardest categories where coarse-grained adaptation is insufficient.",
            "uuid": "e3421.0",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Vanilla",
            "name_full": "Vanilla BiLSTM programmer (RL fine-tuned)",
            "brief_description": "A single one-size-fits-all BiLSTM encoder-decoder programmer trained on a small pseudo-annotated set and further optimized with reinforcement learning; used as a baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vanilla (BiLSTM-based programmer + RL)",
            "model_description": "BiLSTM encoder-decoder programmer trained on ≈1% pseudo-gold BFS-annotated data (Q_pre) then optimized with reinforcement learning on additional unannotated samples; beam search used for program prediction.",
            "model_size": null,
            "reasoning_task_name": "CQA — Logical Reasoning category",
            "reasoning_task_description": "Same as above: logical multi-step KB queries requiring discrete operators and program induction.",
            "method_or_intervention": "Supervised pretraining on small pseudo-annotated set (BFS-generated action sequences), followed by RL fine-tuning.",
            "performance": "Logical Reasoning F1 = 76.58%. Overall micro F1 = 74.72%, macro F1 = 64.01%.",
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "One-size-fits-all model struggles when training data is diverse — less common knowledge is not well captured, leading to lower adaptability to different question types compared to meta-learning approaches.",
            "ablation_or_analysis": "Serves as primary baseline for ablation: MAML-based variants (Random, Jaccard, MARL) outperform Vanilla, indicating meta-learning and retrieval provide task-specific adaptation advantages.",
            "uuid": "e3421.1",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "CIPITR-Sep",
            "name_full": "CIPITR (separate per-category models)",
            "brief_description": "Neural Program Induction approach that trains separate models per question category, using high-level constraints to guide program generation without gold programs.",
            "citation_title": "Complex program induction for querying knowledge bases in the absence of gold programs.",
            "mention_or_use": "use",
            "model_name": "CIPITR-Sep",
            "model_description": "CIPITR trains independent NPI-style models per CQA question category and employs high-level constraints to produce semantically plausible programs under weak supervision (terminal rewards) without annotated programs.",
            "model_size": null,
            "reasoning_task_name": "CQA — Logical Reasoning category",
            "reasoning_task_description": "As above.",
            "method_or_intervention": "Neural Program Induction with category-specific training and high-level constraints to guide search in absence of gold programs (per-category one-size-fits-all).",
            "performance": "Logical Reasoning F1 = 85.33% (Table 1). CIPITR-Sep achieves very high per-category performance because it trains a separate model per question type.",
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "Requires training separate models per category (CIPITR-Sep), which is costly and not a single adaptive model; may not generalize across categories. The paper notes CIPITR-All (single model) performs much worse than CIPITR-Sep, illustrating that per-category specialization helps but lacks transfer.",
            "ablation_or_analysis": "Comparison shows CIPITR-Sep outperforms MARL on some categories (including Logical Reasoning) because it optimizes per-type models; this highlights tradeoff between per-category specialization and single-model adaptability.",
            "uuid": "e3421.2",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "CIPITR-All",
            "name_full": "CIPITR (single model across all categories)",
            "brief_description": "CIPITR variant where a single model is trained over all question types (one-size-fits-all), used as baseline for breadth-generalization.",
            "citation_title": "Complex program induction for querying knowledge bases in the absence of gold programs.",
            "mention_or_use": "use",
            "model_name": "CIPITR-All",
            "model_description": "Same neural program induction approach as CIPITR but trained as a single model across all categories rather than per-category specialization.",
            "model_size": null,
            "reasoning_task_name": "CQA — Logical Reasoning category",
            "reasoning_task_description": "As above.",
            "method_or_intervention": "Neural Program Induction across mixed-category training data; uses high-level constraints.",
            "performance": "Logical Reasoning F1 = 21.31% (Table 1) — substantially worse than CIPITR-Sep and MARL in this paper's experiments.",
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "Per authors, one-size-fits-all CIPITR-All struggles when data varies widely across categories; difficulty finding weights that fit diverse examples and class imbalance harms performance.",
            "ablation_or_analysis": "Contrast between CIPITR-All and CIPITR-Sep underscores the benefit of per-category specialization; motivates meta-learning approaches that aim to adapt per-task without training separate models.",
            "uuid": "e3421.3",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KVmem",
            "name_full": "KVmem (Key-Value memory model)",
            "brief_description": "A baseline CQA architecture combining Hierarchical Recurrent Encoder-Decoder (HRED) with a Key-Value memory network that attends to memory entries to predict answers.",
            "citation_title": "Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph.",
            "mention_or_use": "use",
            "model_name": "KVmem",
            "model_description": "Memory-augmented sequence model (HRED + Key-Value memory) that retrieves related memory entries and attends to encoded vectors to predict answers.",
            "model_size": null,
            "reasoning_task_name": "CQA — Logical Reasoning category",
            "reasoning_task_description": "As above.",
            "method_or_intervention": "Memory network retrieval and attention over key-value pairs (no specialist meta-learning or program induction in this baseline).",
            "performance": "Logical Reasoning F1 = 37.56% (Table 1). Overall micro F1 = 31.18%, macro F1 = 19.45%.",
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "Per-table results, KVmem performs poorly on multi-step logical/programmatic questions compared to program-induction or meta-learning approaches.",
            "ablation_or_analysis": "Serves as a non-program-induction baseline demonstrating the need for programmatic solutions or meta-adaptation for complex KB queries.",
            "uuid": "e3421.4",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Random retriever (MAML variant)",
            "name_full": "MAML variant with random retriever",
            "brief_description": "Meta-learning (MAML/Reptile-style) programmer that uses a retriever which randomly selects support questions within the same category; included as an ablation/variant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Random-retriever MAML variant",
            "model_description": "MAML-based meta-learning of programmer where the retriever samples N support questions uniformly at random from same-type pool (filter applied), used to measure impact of learned retriever vs random selection.",
            "model_size": null,
            "reasoning_task_name": "CQA — Logical Reasoning category",
            "reasoning_task_description": "As above.",
            "method_or_intervention": "MAML-style meta-learning with random support set selection (same type filtering).",
            "performance": "Logical Reasoning F1 = 78.87% (Table 1). Overall micro F1 = 75.69%.",
            "baseline_performance": "Vanilla Logical Reasoning F1 = 76.58%.",
            "improvement_over_baseline": "Random retriever improves Logical Reasoning F1 by +2.29 points over Vanilla (78.87 - 76.58).",
            "limitations_or_failures": "Random support selection may include non-analogous examples; gains show that meta-learning itself helps but learned retriever yields further improvements.",
            "ablation_or_analysis": "Ablation: Random retriever yields +0.97 pp micro-F1 over Vanilla; demonstrates part of the gain comes from meta-learning independent of retriever quality.",
            "uuid": "e3421.5",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Jaccard retriever (MAML variant)",
            "name_full": "MAML variant with Jaccard-similarity retriever",
            "brief_description": "MAML-based programmer that uses a fixed non-learning retriever based on Jaccard similarity of question words as an ablation variant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Jaccard-retriever MAML variant",
            "model_description": "MAML/Reptile meta-learning of programmer where support sets are selected by fixed Jaccard-similarity over question word overlap (with type filter).",
            "model_size": null,
            "reasoning_task_name": "CQA — Logical Reasoning category",
            "reasoning_task_description": "As above.",
            "method_or_intervention": "Fixed, non-learned retriever based on Jaccard word overlap used to form support sets for meta-learning adaptation.",
            "performance": "Logical Reasoning F1 = 78.89% (Table 1). Overall micro F1 = 76.31%.",
            "baseline_performance": "Vanilla Logical Reasoning F1 = 76.58%.",
            "improvement_over_baseline": "Jaccard retriever improves Logical Reasoning F1 by +2.31 points over Vanilla (78.89 - 76.58).",
            "limitations_or_failures": "Fixed similarity heuristics may miss deeper semantic analogies; joint learned retriever (MARL) further improves over Jaccard (+0.54 pp on Logical Reasoning).",
            "ablation_or_analysis": "Ablation: Jaccard retriever yields +1.59 pp micro-F1 improvement over Vanilla; comparing Jaccard to MARL indicates benefit of jointly training retriever.",
            "uuid": "e3421.6",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "D2A",
            "name_full": "Dialog-to-Action (D2A)",
            "brief_description": "Behavior-cloning approach to complex question answering that learns to map dialog/question context to action sequences using BFS-annotated pseudo-gold programs.",
            "citation_title": "Dialog-to-action: conversational question answering over a large-scale knowledge base.",
            "mention_or_use": "mention",
            "model_name": "Dialog-to-Action (D2A)",
            "model_description": "Behavior cloning trained from BFS-generated annotations (pseudo-gold) to learn programs for complex QA; primarily for conversational multi-turn KBQA in prior work.",
            "model_size": null,
            "reasoning_task_name": "Related: KBQA / semantic parsing tasks (not evaluated on CQA logical reasoning in this paper)",
            "reasoning_task_description": "Used for mapping dialog or questions to action sequences/programs; requires logical composition of operations when present.",
            "method_or_intervention": "Supervised imitation learning from BFS-annotated programs.",
            "performance": null,
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "Mentioned as prior work; requires annotated programs (BFS pseudo-gold) and may not address weak supervision/meta-retrieval joint training considered in MARL.",
            "ablation_or_analysis": "Cited as related work; no ablation results in this paper.",
            "uuid": "e3421.7",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "NSM",
            "name_full": "Neural-Symbolic Machines (NSM)",
            "brief_description": "Neural program induction framework that learns semantic parsers with weak supervision by anchoring high-reward programs and combining neural generation with symbolic execution.",
            "citation_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision.",
            "mention_or_use": "mention",
            "model_name": "Neural-Symbolic Machines (NSM)",
            "model_description": "Neural program generator combined with symbolic execution over a KB; uses pseudo-gold programs and deterministic probabilities to anchor learning under weak supervision.",
            "model_size": null,
            "reasoning_task_name": "Related: semantic parsing / KBQA tasks involving multi-step logical reasoning",
            "reasoning_task_description": "Addresses multi-hop and compositional logical queries by generating programs executed on KBs.",
            "method_or_intervention": "Neural program induction with pseudo-gold annotations and deterministic anchoring of high-reward programs.",
            "performance": null,
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "Mentioned as related work; not experimentally compared in this paper's tables.",
            "ablation_or_analysis": "Cited for context; no ablation data here.",
            "uuid": "e3421.8",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "SSRP",
            "name_full": "Stable Sparse Reward based Programmer (SSRP)",
            "brief_description": "An NPI-style approach for KBQA that alleviates sparse-reward problems by guiding program generation toward semantically plausible programs under weak supervision.",
            "citation_title": "Neural program induction for kbqa without gold programs or query annotations.",
            "mention_or_use": "mention",
            "model_name": "SSRP",
            "model_description": "Neural program induction approach that introduces techniques to handle sparse rewards and produce plausible programs when gold programs are not available.",
            "model_size": null,
            "reasoning_task_name": "Related: complex KBQA / logical program induction",
            "reasoning_task_description": "Targets program induction for complex KB queries under weak supervision.",
            "method_or_intervention": "High-level constraints and reward-shaping techniques to stabilize learning under sparse rewards.",
            "performance": null,
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "Mentioned in related work; not evaluated directly in this paper.",
            "ablation_or_analysis": "Cited to contextualize MARL's contributions; no analysis here.",
            "uuid": "e3421.9",
            "source_info": {
                "paper_title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph.",
            "rating": 2,
            "sanitized_title": "complex_sequential_question_answering_towards_learning_to_converse_over_linked_question_answer_pairs_with_a_knowledge_graph"
        },
        {
            "paper_title": "Complex program induction for querying knowledge bases in the absence of gold programs.",
            "rating": 2,
            "sanitized_title": "complex_program_induction_for_querying_knowledge_bases_in_the_absence_of_gold_programs"
        },
        {
            "paper_title": "Coupling retrieval and metalearning for context-dependent semantic parsing.",
            "rating": 2,
            "sanitized_title": "coupling_retrieval_and_metalearning_for_contextdependent_semantic_parsing"
        },
        {
            "paper_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision.",
            "rating": 2,
            "sanitized_title": "neural_symbolic_machines_learning_semantic_parsers_on_freebase_with_weak_supervision"
        },
        {
            "paper_title": "Dialog-to-action: conversational question answering over a large-scale knowledge base.",
            "rating": 1,
            "sanitized_title": "dialogtoaction_conversational_question_answering_over_a_largescale_knowledge_base"
        },
        {
            "paper_title": "Neural program induction for kbqa without gold programs or query annotations.",
            "rating": 1,
            "sanitized_title": "neural_program_induction_for_kbqa_without_gold_programs_or_query_annotations"
        }
    ],
    "cost": 0.0152295,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning</p>
<p>Yuncheng Hua 
School of Computer Science and Engineering
Southeast University
NanjingChina</p>
<p>Southeast University-Monash University Joint Research Institute
SuzhouChina</p>
<p>Yuan-Fang Li yuanfang.li@monash.edu 
Faculty of Information Technology
Monash University
MelbourneAustralia</p>
<p>Gholamreza Haffari gholamreza.haffari@monash.edu 
Faculty of Information Technology
Monash University
MelbourneAustralia</p>
<p>Guilin Qi 
School of Computer Science and Engineering
Southeast University
NanjingChina</p>
<p>Key Laboratory of Computer Network and Information Integration
Southeast University
China</p>
<p>Wei Wu wuwei@seu.edu.cn 
School of Computer Science and Engineering
Southeast University
NanjingChina</p>
<p>Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning
557DD3245C5D370119C6C736D74EE44E
A compelling approach to complex question answering is to convert the question to a sequence of actions, which can then be executed on the knowledge base to yield the answer, aka the programmerinterpreter approach.Use similar training questions to the test question, meta-learning enables the programmer to adapt to unseen questions to tackle potential distributional biases quickly.However, this comes at the cost of manually labeling similar questions to learn a retrieval model, which is tedious and expensive.In this paper, we present a novel method that automatically learns a retrieval model alternately with the programmer from weak supervision, i.e., the system's performance with respect to the produced answers.To the best of our knowledge, this is the first attempt to train the retrieval model with the programmer jointly.Our system leads to state-of-the-art performance on a large-scale task for complex question answering over knowledge bases.We have released our code at https://github.com/DevinJake/MARL.</p>
<p>Introduction</p>
<p>Complex question answering (CQA) over knowledge base (KB) aims to map natural-language questions to logical forms (annotations), i.e., programs or action sequences, which can be directly executed on the KB to yield answers (denotations) [Berant et al., 2013;Shen et al., 2019].Different from other forms of KBQA, such as multi-hop question answering, CQA requires discrete aggregation actions, such as set intersection/union, counting, and min/max, to yield answers, which can be entities in the KB as well as numbers.</p>
<p>Taking one dataset CQA [Saha et al., 2018] as an example, the questions are organized into seven categories, and the questions in the different categories vary substantially in length and complexity.For instance, as shown in Figure 1, the question in the 'Simple' category only requires one 'select' action to answer, while the question in the 'Quantitative Count' category requires the execution of the sequence of actions 'select ', 'intersection' and 'count' to obtain the answer.* Contact Author Standard KBQA approaches [Berant et al., 2013;Yih et al., 2014;Bordes et al., 2015;Yu et al., 2017;Guo et al., 2018;Saha et al., 2018;Ansari et al., 2019] adopt imitation learning, memory network, or RL, and they typically train one model that fits the entire training dataset and use it to answer all questions.Such a one-size-fits-all model aims to learn the generic knowledge across the questions in the training phase and use the knowledge to predict action sequences for each test question in the inference phase.However, when the training questions vary widely, less common knowledge will be shared across them.Thus the increase in the diversity of the questions is associated with a reduction in the generic knowledge.It would be challenging for the one-size-fits-all model to produce optimal logical forms for each instance.</p>
<p>Several methods have recently been proposed to address this challenge.Complex Imperative Program Induction from Terminal Rewards (CIPITR) [Saha et al., 2019] takes a neural program induction (NPI) approach and proposes to train different independent models, one for each category of CQA questions.Essentially, CIPITR aims to learn a one-size-fitsall model for each question type instead of granting the model arXiv:2010.15875v1[cs.AI] 29 Oct 2020 the ability to transfer from one question type to another.</p>
<p>S2A [Guo et al., 2019] attempts to address this challenge with a method based on a meta-learning method [Schmidhuber, 1992], employing Model Agnostic Meta-Learning (MAML) [Finn et al., 2017] specifically.S2A puts forward a retriever and a meta-learner.It first collects annotations for each question in the training set and trains a retriever to find questions with similar annotations.Subsequently, in training the meta-learner, when faced with a new question, the already-trained retriever finds samples from the training set that are similar to the new question and regards these samples as a support set.The meta-learner views the new question along with the support set as a new task and then learns a specific model adaptive to the task.</p>
<p>However, S2A is faced with two difficulties.Firstly, as it employs a teacher forcing approach in the process of training the retriever, it places a burden on collecting annotations for each question.Secondly, the retriever is trained independently of the meta-learner.Thus, the result of the metalearner answering the questions is irrelevant to the retriever.Therefore it is hard to evaluate the quality of the support set that the retriever establishes for each new question and consequently tricky to measure the impact of the retriever on answering the questions.If the samples found by the retriever are not similar to the new question as expected, the metalearner will be misguided by the deviated examples and thus learns a model that is not suitable for the current task.</p>
<p>Furthermore, though the approach taken by S2A is the most similar to ours, the tasks differ significantly.S2A aims to answer context-dependent questions, where each question is part of a multiple-round conversation.Hence, the contextaware retriever proposed in S2A considers the relevant conversation.On the contrary, however, in this paper, we consider the setup where the questions are single-round and directly answerable from the KB.Thus, a novel challenge arises in retrieving accurate support sets without conversation-based context information.</p>
<p>In this work, to address the above problems, we propose MetA Retrieval Learning (MARL), a new learning algorithm that jointly optimizes retriever and programmer in two stages.</p>
<p>In the first stage, we fix the parameter of the retriever and employ it to select the top-N similar questions (secondary questions) to a given target question (primary question).The trial trajectories, along with the corresponding rewards for answering each secondary question, are used to adapt the programmer to the current primary question.The feedback on how correctly the adaptive programmer answers the primary question is used to update the weights of the programmer.</p>
<p>In the second stage, we fix the parameter of the programmer and optimize the retriever.The general programmer first outputs an answer and gain a reward to the primary question without using any secondary questions.Then we sample M different sets of secondary questions following the retriever policy and employ the question sets to learn M different programmers.Each specific programmer will generate an answer to the primary question and gain some reward.We consider the difference between the reward yielded by the general programmer and the reward gained by each adapted programmer as the contribution of employing the corresponding sup-port set.Thus, the reward difference provides the training signal to optimize the policy of the retriever: positive difference increases the probability that a set of secondary questions is chosen, and a negative difference lowers the probability.</p>
<p>We train the programmer and the retriever alternatively until the models converge.Note that in our method, the training of the retriever is done in a weakly-supervised manner.The retriever is optimized to find better support sets according to the programmer's performance of answering primary questions, rather than employing teacher forcing.Since one support set generates one adaptive programmer, we employ the feedback obtained by evaluating the adaptive programmer as a weak supervision signal to optimize the retriever.Therefore we encourage the retriever to find the support set that leads to a superior programmer that gains more reward.At the same time, the programmer is optimized alongside the retriever.</p>
<p>We evaluate our method on CQA [Saha et al., 2018], a large-scale complex question answering dataset.MARL outperforms standard imitation learning or RL-based methods and achieves new state-of-the-art results with overall micro and macro F1 score.Notably, MARL uses only 1% of the training data to form the tasks for meta-learning and achieve competitive results.</p>
<p>2 Alternate Meta-learning for Complex Question Answering over Knowledge Bases</p>
<p>We now introduce our method for combining weaklysupervised retriever and meta-learning to solve the CQA task, which we call MARL.</p>
<p>In this paper, we consider each new complex question in the training dataset as one primary question and view answering one primary question as an individual task.To solve an unseen task effectively, we aim to build a unique programmer for each task.Thus we harness the retriever to find the top-N most similar questions as the secondary questions, and propose a meta reinforcement learning approach to rapidly adapt the programmer to the primary question with the support of the secondary questions.</p>
<p>Method Overview</p>
<p>The goal of MARL is to train the retriever to find the optimal secondary questions for the programmer, which, when faced with a new primary question, can quickly adapt the programmer to the unseen question and effectively improve QA performance.To accomplish this goal, we train two networks jointly: (1) an encoder-decoder network, which is viewed as the programmer, that transforms questions into programs; and (2) a retriever network, which finds the secondary questions, i.e., the N questions that are the most analogous to the primary question.</p>
<p>We denote the encoder-decoder network as a policy π(τ |q i ; θ) with parameter θ, and the retriever network as a policy π(q ci |q pri ; φ) with parameter φ.Both networks take questions as the input, while the output of the programmer network is the programs that can be directly executed on the KB to generate answers, and the production of the retriever network is the probability distribution over all candidate training samples.For the retriever network, the N sam- ples that have the highest probabilities are selected as secondary questions for a given primary question.Both the parameters θ and φ are optimized by the rewards, which are the comparison between the generated answers and the groundtruth answers.</p>
<p>In the training process, when the primary question and the corresponding secondary questions are similar (i.e., with analogical structures and semantic meanings), the programmer will be guided more effectively in finding the optimal direction of parameter update.On the contrary, if the secondary questions are randomly extracted from the training set, they might share a little commonality with the primary question and consequently yield disparate action sequences.Since only a small amount of samples are used to finetune the adaptive model, such dissimilar instances will lead the programmer on a noisier (more random) path for gradient update.</p>
<p>In the retriever network, we thus cluster the training samples into several groups based on the question type and use a vector ψ to record group membership of each question.For each primary question, based on ψ, a unique filter F will be generated to determine the possible questions as candidate secondary questions by filtering out irrelevant questions.Given the primary question q pri , the retriever network computes the probability of choosing a candidate question q ci as a secondary question by applying a filter softmax function, which could be denoted as
π(q ci |q pri ; ψ, φ)(1)
Following the probability, we sample a set of secondary questions and denote them as s qpri , which is used to support adapting the programmer for the primary question q pri .</p>
<p>Model Objectives</p>
<p>We consider two types of knowledge that can contribute to the answering of a new question: the task-specific knowledge, which is the particular features shared with a small number of similar questions that inform what the current task is; and the task-agnostic knowledge, which is the generic features shared across different tasks.If similar questions are accurately selected, the model could effectively exploit the particular features among these questions and acquire task-specific knowledge to recognize the new task.If the generic features are suitable for many tasks, the model could rapidly adapt itself to a new task and produce satisfactory results for the task by conducting only a small number of gradient updates.We employ a retriever to find similar questions for acquiring the task-specific knowledge and train a generic policy to accumulate task-agnostic knowledge.</p>
<p>In our method, the performance of answering a new question is represented by the reward feedback on whether the question is correctly answered.It is hard to disentangle the contribution of the two types of knowledge.In other words, it is difficult to determine which factor is the reason for generating a correct answer, the task-specific knowledge gained from similar questions, or the task-agnostic knowledge acquired from the well-learned generic policy.</p>
<p>As shown in Figure 2(a), we split the training process in each epoch into two independent stages and train the programmer and the retriever alternately.In the first stage, we fix the parameter φ of the retriever network and employ it to select secondary questions to optimize the parameter θ of the programmer.The target of the first stage is to encourage the model to learn the generic knowledge that is broadly applicable to all tasks by updating θ.In the second stage, we fix the parameter θ of the programmer and update the retriever network by computing its gradients with respect to the programmer's output.We compare the results of answering the same primary question with or without using the retriever and consider the difference between the results as the indication of how the retriever contributes to solving the primary question.Such differences are used to train the retriever to find the optimal set of secondary questions (the support set).Both networks are trained iteratively until convergence.</p>
<p>The training approach is depicted in Algorithm 1.</p>
<p>In the first stage (lines 4-12 in Algorithm 1) of each epoch, we propose a meta reinforcement learning (meta-RL) approach to update θ, i.e. the programmer.We use the gradientbased meta-learning method to solve the meta-RL problem such that we can obtain the optimal policy for a given task after performing a few steps of vanilla policy gradient (VPG) with the task.We employ Monte Carlo integration (MC) as the approximation strategy in the Policy Gradient.</p>
<p>The meta-learning process is divided into two steps to solve a task, namely the meta-training step and the meta-test step.Suppose we are trying to answer the primary question q pri , N secondary questions s qpri will be first found by the retriever network, and we consider q pri together with s qpri as a pseudo-task T pse .During meta-training, the meta-RL model first generates K trajectories for each question in s qpri based on parameter θ.The reward of each trajectory is given by the environment and subsequently used to adapt θ to task T pse as follows:
θ ← θ + η 1 ∇ θ qi∈s q pri E τ ∼π(τ |qi;θ) [R(τ )]
(2)</p>
<p>In the meta-test step, another K trajectories corresponding to the primary question q pri are further produced by θ .The reward produced by the new trajectories is considered as the evaluation of the adapted policy θ for the given task T pse ; thus, we have the following objective:
J qpri (θ ) def = E τ ∼π(τ |qpri;θ ) [R(τ )]
(3)</p>
<p>The parameter of the generic policy θ are then trained by maximizing the objective J qpri (θ ),
θ ← θ + η 2 ∇ θ J qpri (θ )(4)
In each VPG step, since we have N samples in s qpri , we use N policy gradient adaptation steps to update θ .Meanwhile, we use one policy gradient step to optimize θ based on the evaluation of θ .We denote the optimized parameter of the programmer θ * .</p>
<p>In the second stage (lines 13-25 in Algorithm 1) of each epoch, we propose a reinforcement learning approach to update φ, i.e. the retriever.The primary questions that we want to solve are the same as the data used in the first stage.When answering the primary question q pri , we first generate a trajectory based on parameter θ * that has been optimized in the first stage as:
τ * ← decode(π(τ |q pri ; θ * ))(5)
and further, obtain the reward R(τ * ) by executing the trajectory.</p>
<p>We then employ the retriever to compute the probability of selecting one candidate question as a secondary question.From this distribution, we sample N secondary questions to form a support set, rather than directly choosing the N questions with the highest probability.The probability of sampling a set of questions s qpri is:
P (s qpri ) = qc i ∈s q pri π(q ci |q pri , ψ; φ)(6)
We could repeat sampling several times and acquire different support sets.Employ one set s qpri m from these support sets, as what we have done in the first stage, we get the adapted θ * m :
θ * m ← θ * + η 1 ∇ θ * qi∈s q pri m E τ ∼π(τ |qi;θ * ) [R(τ )] (7)
Then we generate a new trajectory under θ * m for the primary question:
τ * m ← decode(π(τ |q pri ; θ * m ))(8)
and also compute the reward for this trajectory as R(τ * m ).We regard the difference between R(τ * ) and R(τ * m ) as the contribution of the support set s qpri m , which is used to learn the task-specific knowledge from the questions in s qpri m .The retriever network is then updated by encouraging the particular support sets to be chosen such that, if the policy θ is adapted to the current task by using these support sets, the reward of answering the primary question would be maximized.Therefore, we harness the difference as the reward and have the objective: Sample a batch of primary questions
J qpri (φ) def = E s q pri m ∼P (s q pri ) [R(τ * m ) − R(τ * )](9Q pri ∼ Q train 6 foreach q pri ∈ Q pri do 7
Retrieve s qpri with φ and ψ
8 L = qi∈s q pri E τ ∼π(τ |qi;θ) [R(τ )] 9 Get adapted parameters: θ ← θ + η 1 ∇ θ L 10 J qpri (θ ) def = E τ ∼π(τ |qpri;θ ) [R(τ )] 11 Update θ ← θ + η 2 ∇ θ qpri∈Qpri J qpri (θ ) 12 θ * ← θ 13 foreach training iteration do 14
Sample a batch of primary questions
Q pri ∼ Q train 15 foreach q pri ∈ Q pri do 16 τ * ← decode(π(τ |q pri ; θ * )) 17 Compute reward R(τ * ) 18 Sample support sets M with φ and ψ 19 foreach s qpri m ∈ M do 20 θ * m ← θ * + η 1 ∇ θ * qi∈s q pri m E τ ∼π(τ |qi;θ * ) R(τ ) 21 τ * m ← decode(π(τ |q pri ; θ * m )) 22 Compute reward R(τ * m ) 23 J qpri (φ) def = E s q pri m ∼P (s q pri ) [R(τ * m ) − R(τ * )] 24 Update φ ← φ + η 3 ∇ φ qpri∈Qpri J qpri (φ) 25 φ * ← φ 26 Return
The learned θ * and φ *</p>
<p>The parameter of the retriever network φ are then updated by maximizing the objective J(φ) as:
φ ← φ + η 3 ∇ φ J(φ)(10)
However, it is often infeasible to compute the gradient in Equation ( 9) because it involves taking an expectation over all possible sampled support sets.Hence, in practice, we employ Monte Carlo integration to approximate the expectation, which is:
∆ M C = 1 M s q pri m ∈M [R(τ * m ) − R(τ * ) − C]∇log(P (s qpri m ))
(11) where M is a collection of M sets of secondary questions retrieved to support the primary question q pri , and C is a baseline with a constant value to reduce the variance of the estimate without altering its expectation.</p>
<p>Filter Softmax</p>
<p>As mentioned before, we categorize the instances in the training set based on the question type and employ a vector ψ to record the type information of the questions.For each question type, based on ψ, a filter F will be created to make the retriever only consider questions of the same type when searching for secondary questions for a primary question of that type.The filter value is set to zero for all instances that do not have the same type of a given question [Liu et al., 2019].For example, as shown in Figure 2(b), assume we have a dataset with two question types y = [A, B], and we arrange the dataset to group the same type questions together.Thus we have a ψ = [2, 3] to indicate that two questions belong to type A and three questions from type B. In this case, the filter is set as F = [1, 1, 0, 0, 0] for the primary question of type A and F = [0, 0, 1, 1, 1] for type B. Applying the filter to the softmax function would mask the irrelevant questions, which have types different from the primary question.This allows the retriever to reduce the search space and increase the probability of finding expected questions.The filter softmax function produces the following probability:
p(q ci |q pri ) = e sim(qpri,qc i ) F i i (e sim(qpri,q c i ) F i ) ,(12)
where p(q ci ) represents the probability that the candidate question q ci is selected as one of the secondary questions for the primary question q pri , and sim(q pri , q ci ) is the semantic similarity between them.Besides, F i is the binary value in the filter F that recognizes whether question q ci has the same type as q pri or not, and represents element-wise multiplication.In our work, Deep Structured Semantic Model (DSSM) method [Huang et al., 2013] is employed to compute the similarity between two questions.</p>
<p>Evaluation</p>
<p>We evaluated our MARL model on the CQA dataset [Saha et al., 2018].CQA is a large-scale complex question answering dataset containing 944K/100K/156K question-answer pairs for training/validation/test.CQA divides itself into seven categories based on the nature of answers, e.g., entities as answers in the 'Simple Question' category and numbers in the category 'Quantitative (Count)'.We used 'accuracy' as the evaluation metric for questions whose type is 'Verification', 'Quantitative (Count)', and 'Comparative (Count)'; and 'F1 measure' to other kinds of questions.However, to simplify the presentation and stay consistent with literature [Saha et al., 2019;Ansari et al., 2019], we denote 'accuracy' as 'F1 measure' in Table 1.Hence, the model performance is evaluated on the F1 measure in this paper.Furthermore, we compute the micro F1 and macro F1 scores for all the models based on the F1-scores of the seven question types.Also, in our analysis of the CQA dataset, we found that the seven types of questions vary substantially in complexity.We discovered that 'Simple' is the simplest that only requires two actions to answer a question, whereas 'Logical Reasoning' is more difficult that requires three actions.Categories 'Verification' and 'Quantitative Reasoning' are the next in the order of difficulty, which need 3-4 actions to answer.The most difficult categories are 'Comparative Reasoning', 'Quantitative (Count)', and 'Comparative (Count)', needing 4-5 actions to yield an answer.Saha et al. [2019] drew a similar conclusion in the manual inspection of these seven question categories.</p>
<p>Implementation Details</p>
<p>In the CQA dataset, we randomly sampled approximately 1% of the training set (10,353 out of 944K training samples) and annotated them with pseudo-gold action sequences with a breadth-first-search (BFS) algorithm [Guo et al., 2018].We denote this dataset as Q pre .We trained a BiLSTM-based programmer with Q pre , and further optimized it through RL with another 1% unannotated questions from the training set.We note this RL-based model is a one-size-fits-all model and denote it as Vanilla.We randomly selected another 2,072 samples from the 944K training questions to establish pseudotasks for meta-learning, which represented only approx.0.2% of the training set.This model that jointly learns the programmer along with the retriever by employing MAML is our full model and is denoted MARL.</p>
<p>We implemented the MARL model in PyTorch with all the weights initialized randomly.We randomly initialized the word embeddings to represent the tokens in questions and output action sequences.We updated the word embeddings within the process of training the Vanilla model and fixed them when training our MARL model.Therefore the primary and the candidate questions in the training dataset were represented as the sum of the vector for each token.The DSSM model took such representation of the questions as the input to compute the semantic similarity between the questions.</p>
<p>In the programmer learning stage, we employed Reptile [Nichol and Schulman, 2018] for fast and simple implementation of MAML while avoiding the significant expense of computing second-order derivatives.We set η 1 = 1e-4 when adapting the model to each new task, and set η 2 = 0.1 to optimize θ with the gradient update derived from the metatest data.The reward that the adaptive programmer gained was used to update the retriever parameter φ through the Ad-aBound optimizer [Luo et al., 2019] in which the learning rate η 3 was initially set to 1e-3 and the final (SGD) learning rate was set to 0.1.</p>
<p>When finding the top-N support set, we set N = 5.For each question, we generated five action sequences to output the answers and rewards.Adam optimizer was applied in RL to maximize the expected reward.</p>
<p>In the retriever learning stage, we employed the REIN-FORCE model to optimize the non-differentiable objective directly.The baseline C used in REINFORCE was set with a constant value of 1e-3 to reduce the variance.We sampled M = 5 different sets of the secondary questions and got one unique adaptive programmer for each set with the same metalearning configuration in the programmer learning stage.</p>
<p>As solving the entity linking problem is beyond the scope of this work, we separately trained an entity/class/relation linker, achieving an accuracy of 99.97%, 91.93%, and 94.29%, respectively.When training the MARL model, the predicted entity/class/relation annotations, along with natural language questions, were used as the input sequence.We trained the MARL model with the batch size of 1 and stopped training when the accuracy on the validation set converged (at around 30 epochs).We release the source code at https://github.com/DevinJake/MARL to facilitate replication.</p>
<p>Performance Evaluation</p>
<p>We compared our model with two baseline methods on the CQA task: KVmem [Saha et al., 2018] and CIPITR [Saha et al., 2019].Saha et al. [2018] propose KVmem, a baseline CQA model that combines Hierarchical Recurrent Encoder-Decoder (HRED) with a Key-Value memory (KVmem) network.The KVmem model retrieves the most related memory to predict the answer from candidate words by attending on the encoded vectors stored in the memory.CIPITR [Saha et al., 2019] takes a further step that employs the NPI approach to solving the CQA task without annotations.CIPITR designs high-level constraints to guide the programmer to produce semantically plausible programs for a question.It is worth noting that CIPITR separately trains a separate model for each of the seven question categories, and selects the corresponding model to answer questions of the relevant type.We denote the model learned in this way as CIPITR-Sep.Besides, CIPITR also trains one single model over all types of training examples and uses this single model to answer all questions.We denote this single model as CIPITR-All.</p>
<p>Also, we compared our full model, MARL, with several model variants to understand the effect of our retriever and meta-learner.Specifically, Vanilla is a BiLSTM-based model further optimized with reinforcement learning.Both Random and Jaccard are MAML-based models with different retriev-ers.Random denotes the model with a retriever that randomly selects questions within the same category.Jaccard means the model with a non-learning retriever that makes use of Jaccard similarity on question words.</p>
<p>We ran the open-source code of KVmem and CIPITR to train the model and presented the best result we got.KVmem does not have any beam search, and both CIPITR and our model employ beam search for predicting the action sequences.When inferring the testing samples, we used the top beam [Saha et al., 2019], i.e., the predicted program with the highest probability in the beam to yield the answer.</p>
<p>Table 1 below summarizes the results.We can make a number of important observations.1.Our full model MARL achieves the best overall performance of 66.96% and 77.71% for macro and micro F1, respectively, outperforming all the baseline models KVmem, CIPITR-All, and CIPITR-Sep.The performance advantage on macro F1 over the three baselines is especially pronounced (47.51, 47.14, and  For the three hard categories, it performs poorly compared to other models.This further demonstrates the limitation of coarse-grained adaptation.</p>
<ol>
<li>CIPITR-All, the model that trains over all types of the questions, performs much worse in all the categories than CIPITR-Sep, which learns a different model separately for each question category.For CIPITR-Sep, the results reported for each category are obtained from the models explicitly tuned for that category.A possible reason for CIPITR-All's significant performance degradation is that it is hard for such a one-size-fits-all model to find the weights that fit the training data when the examples vary widely.Besides, the imbalanced classes of questions also deteriorate the performance of the model.Different from CIPITR, our model is designed to adapt appropriately to various categories of questions with one model, thus only needs to be trained once. 5.In general, our MAML-based model variants, Random, Jaccard, and MARL, outperform their non-MAML counterpart, Vanilla, which demonstrates the advantage of our method in learning task-specific knowledge.</li>
</ol>
<p>We also conducted an ablation study to examine the effectiveness of the retriever.Table 2 presents the result of the ablation study.As can be seen, the random retriever slightly improves performance over the base model Vanilla by 0.97 percentage points, and the fixed retriever Jaccard improves upon Vanilla by 1.59 percentage points.Our full model achieves a 2.99 percentage point improvement over Vanilla.We can observe that the full model improves the overall micro F1 score by 1.40 percentage points compared with the fixed retriever Jaccard.The performance disparity between the full model and the Jaccard retriever can be attributed to our joint training strategy, in which our full model alternately optimizes the programmer and the retriever.On the contrary, less optimally, the Jaccard retriever is separately trained before training the programmer.These results demonstrate the effectiveness of our meta-learning approach as well as the power of our model that jointly optimizes the retriever with the programmer.</p>
<p>Related Work</p>
<p>CQA.A behavior cloning based method, Dialog-to-Action (D2A) [Guo et al., 2018], is proposed to answer complex questions by learning from the annotated programs.D2A employs a BFS algorithm to annotate the questions with the corresponding action sequences, and use the annotations to train the programmer.On the other hand, the NPI based methods, i.e., Neural-Symbolic Machines (NSM) [Liang et al., 2017], CIPITR [Saha et al., 2019], and Stable Sparse Reward based Programmer (SSRP) [Ansari et al., 2019], employ the yielded answers as the distant-supervision to learn a programmer.NSM is proposed to answer the multi-hop questions.It first annotates the questions with the pseudo-gold programs and then assigns the annotated programs with a deterministic probability, therefore, to anchor the model to the high-reward programs.CIPITR and SSRP both aim to alleviate the sparse reward problem that appears in conventional NPI approaches and employ high-level constraints to guide the programmer to produce semantically plausible programs.Except for CIP-ITR, the NPI approaches learn a one-size-fits-all model for the entire dataset.However, CIPITR learns a one-size-fitsall model for each question type instead of empowering the model to learn to transfer from one question type to another.Different from learning a one-size-fits-all model, we aim to learn a model that quickly discovers the knowledge specific to a new task, and employ the acquired knowledge to adapt the programmer to the new task.</p>
<p>Meta-Learning.The Meta-learning approaches utilize the inductive biases, which are meta-learned in learning similar tasks to make the model learn the new task quickly.To make the model sensitive to the new task, one popular direction of meta-learning is to train a meta-learner to learn how to update the parameters of the underlying model [Li and Malik, 2017;Ha et al., 2017], which has been investigated in MAML [Finn et al., 2017].In semantic parsing tasks, Huang et al [2018] propose a relevance function to find similar samples to form a pseudo-task for each WikiSQL question.Subsequently, they reduce a supervised learning problem into a meta-learning problem and employ MAML to adapt the programmer to each pseudo-task.Likewise, S2A [Guo et al., 2019] separately trains the retriever and the programmer by using the pseudogold annotations.Based on the similar samples found by the retriever, S2A establishes a meta-learning task for each question and thus employs MAML to finetune the programmer.Unlike them, we propose a MAML-based approach that trains the retriever and the programmer jointly.It is worth noting that S2A aims to answer conversational questions while we consider answering single-turn questions.Therefore we do not include S2A as a baseline method in the evaluation as it is not directly comparable to our problem setup.</p>
<p>Conclusion</p>
<p>In this paper, we presented a novel method for complex question answering over knowledge bases.In a meta-learning framework, our model jointly and alternately optimized a retriever, which learned to select questions, and a programmer, which learned to adapt to the selected secondary questions to produce an answer to a given primary question.Our model was capable of quickly adapting to new questions as it could learn from similar questions.Moreover, it did so from weak supervision signals, the model's performance on question answering.Thus, our model addressed several essential challenges facing existing methods, namely the significant distributional biases present in the dataset and the high cost associated with manual labeling of similar questions.Our evaluation against a number of state-of-the-art models showed the superiority of our model on the large-scale complex question answering dataset CQA.In the future, we plan to extend our model to other domains and tasks that require the manual construction of support sets.</p>
<p>Figure 1 :
1
Figure 1: Two questions of different types in the CQA dataset.</p>
<p>Figure 2 :
2
Figure 2: (a).Illustration of the two stages in the training process of MARL.(b) Illustration of the filter softmax.</p>
<p>)</p>
<p>Algorithm 1: The MARL algorithm Input: Training dataset Q train , step size η 1 , η 2 , η 3 Output: The learned θ * and φ * 1 Initialize groups vector ψ 2 Randomly initialize θ and φ 3 while not converged do
4foreach training iteration do5</p>
<p>Table 1 :
1
Performance comparison (measured in F1) on the CQA test set.For each category, best result is bolded and second-best result is underlined.The number of instances in each category in the training set is also given next to the category name.
Question categoryKVmem CIPITR-All CIPITR-SepVanilla Random Jaccard MARLSimple Question (462K)41.40%41.62%94.89%84.67%85.22% 86.07% 88.06%Logical Reasoning (93K)37.56%21.31%85.33%76.58%78.87% 78.89% 79.43%Quantitative Reasoning (99K)0.89%5.65%33.27%47.20%48.11% 48.21% 49.93%Verification (Boolean) (43K)27.28%30.86%61.39%81.94%85.04% 85.24% 85.83%Comparative Reasoning (41K)1.63%1.67%9.60%58.05%61.96% 63.07% 64.10%Quantitative (Count) (122K)17.80%37.23%48.40%60.36%60.04% 60.47% 60.89%Comparative (Count) (42K)9.60%0.36%0.99%39.25%38.50% 39.50% 40.50%Overall macro F119.45%19.82%47.70%64.01%65.39% 65.92% 66.96%Overall micro F131.18%31.52%73.31%74.72%75.69% 76.31% 77.71%FeatureOverall micro F1Vanilla74.72%MARL (random retriever)+0.97%MARL (Jaccard retriever)+1.59%MARL (full model)+2.99%</p>
<p>Table 2 :
2
Ablation study on the test set on macro F1 score change with the addition of meta-learning and different retrievers.Full model (MARL) has micro F1 of 77.71% as shown in Table1.</p>
<p>AcknowledgmentsResearch presented in this paper was partially supported by the National Key Research and Development Program of China under grants (2017YFB1002801, 2018YFC0830200), the Natural Science Foundation of China grants (U1736204, 61602259), Australian Research Council (DP190100006), the Judicial Big Data Research Centre, School of Law at Southeast University, and the project no.31511120201 and 31510040201.
Neural program induction for kbqa without gold programs or query annotations. Ansari, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceAAAI Press2019. 2019</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Berant, arXiv:1506.02075Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Chelsea Finn, Pieter Abbeel, Sergey Levine, the 2013 Conference on Empirical Methods in Natural Language Processing2013. 2013. 2015. 2015. 2017. 201770arXiv preprintProceedings of the 34th International Conference on Machine Learning</p>
<p>Dialog-to-action: conversational question answering over a large-scale knowledge base. Guo, Advances in Neural Information Processing Systems. 2018. 2018</p>
<p>Coupling retrieval and metalearning for context-dependent semantic parsing. Guo, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Ha, the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, Italy; Toulon, France2019. July 28-August 2, 2019. 2019. 2017. April 24-26, 2017. 20171Conference Track Proceedings</p>
<p>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. Huang, CoRR, abs/1703.00441Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2013. 2013. 2018. 2018. Li and Malik, 2017. 2017. 2017. 20172Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</p>
<p>Self-supervised generalisation with meta auxiliary learning. Liu, Advances in Neural Information Processing Systems. 2019. 2019</p>
<p>Adaptive gradient methods with dynamic bound of learning rate. Luo, arXiv:1803.02999Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. New Orleans, LA, USA2019. May 6-9, 2019. OpenReview.net, 2019. 2018. 20182arXiv preprint7th International Conference on Learning Representations</p>
<p>Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. Saha, Thirty-Second AAAI Conference on Artificial Intelligence. 2018. 2018</p>
<p>Complex program induction for querying knowledge bases in the absence of gold programs. Saha, 2019. 2019Transactions of the Association for Computational Linguistics7</p>
<p>Wen-tau Yih, Xiaodong He, and Christopher Meek. Semantic parsing for single-relation question answering. Jürgen Schmidhuber, ; Schmidhuber, Shen, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaShort Papers1992. 1992. 2019. November 2019. 2014. 20144Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</p>
<p>Improved neural relation detection for knowledge base question answering. Yu , Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics2017. 20171</p>            </div>
        </div>

    </div>
</body>
</html>
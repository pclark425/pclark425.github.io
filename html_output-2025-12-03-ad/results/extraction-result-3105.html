<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3105 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3105</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3105</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267500136</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03877v3.pdf" target="_blank">Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3105.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3105.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI ChatGPT (gpt-3.5-turbo-16k, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned, conversational GPT family model (gpt-3.5-turbo variant) used via API in the paper for single-agent and multi-agent experiments, evaluated on constructive geometry (Euclidea) and math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-16k (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following transformer from OpenAI, accessible via API; used as the primary language-only LLM for multi-round dialogues, few-shot prompting, and role-based (solver/validator) agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Zero-shot prompting', 'Few-shot prompting (static examples)', 'Adaptive-Shot (ST and Self) adaptive few-shot', 'Chain-of-Thought style natural-language planning (S_NL)', 'Tool-step translation (S_GT)', 'Multi-agent simulacra (role specialization: solver + validator)', 'Visual Relations Prompt (VRP) augmentation', 'Variable renaming (target -> X)', 'Feedback mode (+FB)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zero-shot/few-shot: standard instruction/ in-context prompting. Adaptive-Shot (ST) uses a Sentence Transformer to select relevant prior examples; Adaptive-Shot (Self) prompts the model to choose the most useful examples after a prefiltering stage. Chain-of-Thought (S_NL) agents produce natural-language rationales; S_GT agents translate those rationales into geometric-tool-step sequences. Validators (SV_*) critique and request revisions in up to 5 rounds. VRP is an auxiliary scene-description produced by a VLLM (GPT4-V) and inserted into agent prompts to improve spatial understanding. Variable renaming substitutes target labels with 'X' to reduce alphabetical naming bias. Feedback mode gives validators access to ground-truth correctness signals only.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse — the paper experiments with and combines multiple distinct reasoning styles (natural-language planning COT-like, program/tool-step translation, adaptive example selection, visual relation extraction, validators and feedback), comparing single-agent (similar-style) operation to multi-agent heterogeneous-role pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Euclidea (constructive geometry), plus GSM8K / SVAMP / MATH-geometry splits</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Euclidea: constructive geometry problems requiring sequences of geometric-tool operations (ruler/compass-style tools) verified by an API. GSM8K/SVAMP/MATH-geometry: standard arithmetic and mathematical word-problem benchmarks used to test generalization of the multi-agent framework.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Euclidea (pass@1 / pass@50): Few-shot (baseline few-shot with static examples) = 11.7% (±1.1) / 18.6% (±1.5). Single-agent geometric-tool solver (S_GT) = 13.6% / 21.5% (ChatGPT on Alpha/Beta packs). Two-agent S_NL−S_GT = 22.2% / 46.7%. With validators (SV_NL−SV_GT) = 28.1% (±1.1) / 53.5% (±2.4). Adding VRP (visual relations prompt) and running multi-agent setup (starred configs) increased ChatGPT from 28.1% to 34.5% pass@1 and to ~59.2% pass@50 in VRP-ChatGPT ⋆ experiments. Adaptive-Shot improved few-shot performance from ~11–12% to ~13.6% pass@1 (ChatGPT experiments in ablation). Feedback mode (+FB) gave only small gains. On non-geometric math benchmarks, combined SV_COT→SV_PAL pipeline achieved results reported as 'close to SOTA' in Table 6 (numerical table in paper not included in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct experimental comparisons show: static few-shot < adaptive few-shot; single-agent S_GT < two-agent S_NL−S_GT (22.2% vs 13.6% pass@1) < multi-agent with validators SV_NL−SV_GT (28.1% pass@1). VRP further improved multi-agent performance substantially for language-only models. Feedback (+FB: validators see ground truth correctness only) produced only slight improvements, indicating limited benefit from ground-truth hints alone. Chaining agents (diverse roles) yields larger gains than repeating similar-style prompting in a single agent.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For ChatGPT, moving from a single-tool-style agent to a heterogeneous multi-agent system (natural-language planner + tool-step translator + validators) plus adaptive example selection and VRP approximately doubled pass@1 performance on Euclidea; VRP particularly helps models lacking vision by supplying scene relations. Diversity of reasoning styles (COT-like planning + strict tool-step execution + validators) substantially outperforms reliance on a single, similar prompting style.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Validators and chained agents can propagate errors when the initial natural-language rationale is flawed. Feedback mode with ground-truth correctness signals gave only marginal improvement. Zero-shot and naive few-shot often lead to hallucinated tool usage or style-mimicking irrelevant step sequences. Multimodal image input to GPT4-V sometimes validated incorrect steps and produced only marginal improvement over language+VRP approaches for later-stage geometric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3105.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3105.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / GPT4-V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 and GPT-4 Vision (GPT4-V)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (capable of long-context reasoning) and its multimodal variant GPT4-V (vision-enabled) evaluated as high-performing baselines on Euclidea and in VRP/image experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (and GPT4-V for visual extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal-capable transformer by OpenAI; used for direct image+prompt reasoning (GPT4-V) and as language-only agent in multi-agent setups; used also to produce Visual Relations Prompt (VRP) via gpt4-vision-preview endpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Few-shot prompting', 'Adaptive-Shot', 'Chain-of-Thought style natural-language planning', 'Tool-step (geometric) execution', 'Multimodal image+prompt (GPT4-V)', 'VRP (visual relations prompt produced by GPT4-V)', 'Multi-agent simulacra with role specialization']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same general methods as applied to ChatGPT. Additionally tested as a VLLM that outputs explicit scene descriptions (points, lines, relations) from images which are then used as VRP in agent prompts or used directly (GPT4-V + image).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse — experiments compare language-only, multimodal direct image prompting, and VRP-augmented pipelines; also compare single-agent vs multi-agent configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Euclidea (constructive geometry) and other geometry/math splits</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Constructive geometry tasks requiring sequences of abstract geometric tool steps; verification via Euclidea API.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Euclidea (pass@1 / pass@50): Few-shot GPT-4 baseline ~21.2% (±1.3) / 38.3% (±1.4) in earlier table. On Alpha/Beta packs: language-only GPT4 ~23.9% / 44.8%; multi-agent GPT4 ⋆ ~33.7% / 62.0%. GPT4-V + raw Image: 24.2% / 45.1% (marginal improvement). VRP-GPT4 (VRP inserted into prompts) improved to 25.3% / 48.6%; VRP-GPT4 ⋆ (multi-agent starred config) further to 38.8% / 64.6%. Under 'Ours' full multi-agent framework: GPT-4 reached 38.9% pass@1 and 67.7% pass@50.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparisons: GPT4-V raw-image pipeline provided only modest gains over language-only GPT-4; using GPT4-V to extract a structured VRP and feeding that to agents produced a larger benefit (VRP-GPT4 > GPT4-V + image). Multi-agent role-specialization further boosted GPT-4 performance significantly. Overall, diverse-method pipelines (VRP + multi-agent + adaptive-shot + renaming) outperform similar single-method prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 benefits from multi-agent role specialization and VRP; however direct image prompting (GPT4-V) does not solve late-stage geometric reasoning errors and occasionally endorses invalid steps. Structured visual relation extraction (VRP) is more useful than raw image prompting for downstream geometric-tool planning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>GPT4-V validated some provably incorrect tool steps in experiments, indicating that vision alone without structured relation prompting can mislead geometric reasoning. Feedback mode with ground-truth validators yields only small gains even for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3105.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3105.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaMath-Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Math fine-tuned Mistral (MetaMath-Mistral, 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B model (Mistral base) finetuned on mathematical datasets (MetaMath) used to test transfer of arithmetic/algebraic reasoning to constructive geometry and its responsiveness to the multi-agent framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaMath-Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B Mistral checkpoint finetuned on mathematical reasoning corpora (MetaMath) to specialize in arithmetic and algebraic problem solving; evaluated here for geometric transfer and within the simulacra multi-agent system.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Few-shot prompting (static)', "Fine-tuning on Euclid's Elements (QLoRA, 4-bit)", 'Adaptive-Shot', 'Simulacra multi-agent (S_NL, S_GT, validators)', 'VRP, variable renaming, feedback mode']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Tested standard few-shot and fine-tuned variants (fine-tuned on Euclid's Elements chapters to provide inductive geometric knowledge). Then evaluated under Adaptive-Shot and the full multi-agent framework with VRP and renaming.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse — the paper explores both single-method baselines (few-shot / finetuned) and diverse multi-agent combinations including visual relations and validators, showing how methods combine to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Euclidea (constructive geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Constructive geometry tasks from PyEuclidea verified programmatically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Euclidea (pass@1 / pass@50): Few-shot MetaMath-Mistral = 8.9% (±0.7) / 13.4% (±1.2). Finetuned on Euclid's Elements = 9.4% (±0.9) / 16.0% (approx; excerpt truncated). Under full multi-agent 'Ours' configuration: MetaMath-Mistral (7B) = 14.9% (±1.3) / 21.1% (±1.7). With Feedback (+FB) slight increase to 15.1% / 21.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Fine-tuning on Euclid's Elements gave only modest gains over few-shot baselines. The multi-agent framework (diverse methods) produced larger, nontrivial improvements (≈+5–6 percentage points pass@1) compared to finetuning alone, indicating that diverse prompting/agent interactions add capabilities beyond static finetuning on classical geometry texts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mathematics-specialized pretraining/fine-tuning (MetaMath) does not transfer strongly to constructive geometry; combining mathematical models with diverse reasoning components (Adaptive-Shot, simulated agents, VRP, renaming) yields larger improvements than fine-tuning alone.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Fine-tuning on Euclid's Elements was insufficient to reach the performance of larger closed models; gains from feedback (+FB) were small, suggesting limitations in how much hints or correctness signals help solvers to explore alternate correct reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3105.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3105.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COT_vs_PAL (SV_COT→SV_PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (COT) vs Program-Aided Language (PAL) combined multi-agent pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrasted pair of reasoning styles: COT (free-form natural-language chain-of-thought planning) and PAL (program-aided, code-like step synthesis); the paper composes them via multi-agent pairs (SV_COT producing rationales then SV_PAL translating to code) to leverage creativity and computational precision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM multi-agent pipeline (SV_COT then SV_PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SV_COT: solver-validator pair operating in a chain-of-thought (natural language) domain to generate creative/rationale-rich plans; SV_PAL: solver-validator pair specialized in program/code generation to convert rationales into executable computations (e.g., Python). Both stages are LLM-driven in the paper's experiments (no external interpreter used for V_PAL).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (COT)', 'Program-Aided Language (PAL)', 'Adaptive few-shot (Adaptive-Shot)', 'Role-specialized multi-agent chaining (SV_COT → SV_PAL)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>COT agents generate natural-language explanatory reasoning (creative, flexible); PAL agents translate reasoning into programmatic steps (accurate for numeric computation). The paper chains a COT-style solver-validator pair to a PAL-style pair so that creative plans get transformed into precise code or tool steps. The PAL stage in the paper is another LLM agent (not a Python interpreter).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Explicitly diverse — the experiment purposely composes two qualitatively different reasoning styles (COT and PAL) to exploit complementary strengths (creativity vs computational accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, SVAMP, MATH (geometry split)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Arithmetic and math word-problem datasets where both creative decomposition and precise computation are needed; used to test generalisation of the multi-agent framework beyond constructive geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Paper reports that the SV_COT→SV_PAL multi-agent setup performs 'very close to current state-of-the-art methods' on these math datasets (exact numeric values referenced in Table 6 of the paper; excerpt did not include full table numbers). MetaMath-Mistral also enjoyed a significant boost from this method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The authors reference prior work (Zhao et al., 2023) that found COT yields creative solutions while PAL yields higher numeric accuracy; their experiments confirm that chaining COT-like planning with PAL-style translation yields a system that leverages both properties and attains near-SOTA performance without using an external interpreter for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining diverse reasoning styles (COT for planning, PAL for programmatic precision) in a chained multi-agent architecture yields better empirical performance on math benchmarks than either homogeneous approach alone, supporting the claim that diverse reasoning methods are complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper chose not to use an external Python interpreter as a checker for PAL stage; using only LLMs for PAL still performed well, but lacking a real executor could limit final numeric accuracy versus approaches that use actual code execution/verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3105.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3105.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulacra Multi-Agent Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulacra-based multi-agent role-specialized framework (solvers + validators + VRP + Adaptive-Shot + renaming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper that composes multiple LLM agents with distinct roles (natural-language solver, geometric-tool solver, validators), an adaptive example-selection mechanism, a visual relations prompt, and variable-renaming to improve constructive-geometry reasoning without weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Simulacra Multi-Agent Framework (applied to various LLM backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A role-assigned multi-agent system inspired by simulacra literature: S_NL (natural-language solver) produces plans; S_GT (geometric-tool solver) maps plans to tool steps; validators (SV_*) critique and request fixes. Augmentations include Adaptive-Shot (example retrieval/selection), Visual Relations Prompt (VRP) produced by a VLLM, and target variable renaming to 'X' to mitigate naming biases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Role-specialized multi-agent reasoning (dividing planning and acting across agents)', 'Adaptive few-shot retrieval (Adaptive-Shot ST / Self)', 'Validator feedback loops (multi-round up to 5)', 'Visual Relations Prompt (VRP) as auxiliary scene representation', 'Variable renaming to neutralize naming biases', 'Optional Feedback mode (+FB) where validators see ground-truth correctness']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Agents are prompted with domain-specific roles and few-shot examples (adaptive). Validators receive Euclid's Elements examples and incorrect/rectified pairs. VRP gives explicit scene element lists/relations extracted once by GPT4-V and inserted into all agent prompts. Adaptive-Shot uses a Sentence Transformer (ST) for filtering and/or the model to self-select useful examples. Variable renaming sets targets to 'X' to avoid alphabetical-naming heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Highly diverse — integrates multiple distinct reasoning styles and auxiliary modules (language planning, strict tool-language execution, visual-relation extraction, retrieval, role-based critique) into a single pipeline; the paper shows their combination yields compounding improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Euclidea (primary), plus GSM8K/SVAMP/MATH geometry for generalization tests</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Constructive geometry tasks requiring multi-step tool sequences; multi-agent framework also tested on math word-problem datasets to assess cross-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Aggregate reported effects: On Euclidea, single LLM few-shot baselines are low (single-digit to low-teens pass@1); applying the full multi-agent pipeline + Adaptive-Shot + VRP + renaming produced large improvements. Example numeric snapshots: MetaMath-Mistral: few-shot 8.9% → 'Ours' multi-agent 14.9% pass@1; ChatGPT: few-shot ~11.7% → multi-agent (SV_NL−SV_GT + VRP + renaming + Adaptive-Shot) 32.3% pass@1 (Ours entry); GPT-4: few-shot ~21.2% → 'Ours' 38.9% pass@1. Agent configuration ablation: S_GT only = 13.6% pass@1; S_NL−S_GT = 22.2%; SV_NL−SV_GT = 28.1%. VRP-augmented starred configs push these higher (e.g., VRP-ChatGPT ⋆ 34.5% pass@1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Ablations directly compare: static few-shot vs Adaptive-Shot (Adaptive-Shot(Self) slightly better than ST), single-agent tool solver vs dual-agent separation of planning and execution, and adding validators vs not. Results consistently show diverse-role multi-agent systems outperform similar-style single-agent prompting. VRP gives extra gains especially for language-only models. Variable renaming reduces naming-induced failure modes and improves validator acceptance rates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposing reasoning into specialized agent roles and combining diverse supporting methods (retrieval-based adaptive examples, structured visual-relations input, naming-bias mitigation, and validators) yields much larger gains on constructive-geometry tasks than incremental refinements of a single prompting style or finetuning on geometry texts alone.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Chaining agents can propagate flawed rationales; validators mitigate but do not eliminate this. Ground-truth-only feedback (+FB) provided to validators resulted in only slight gains, indicating limited utility of correctness hints without changes in solver exploration strategies. The framework is more costly (API rounds, context) and may be slower than single-agent few-shot in low-cost settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3105.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3105.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive-Shot (ST / Self)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive-Shot adaptive few-shot example selection (Sentence-Transformer / Self-filtered)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive few-shot selection mechanism introduced in the paper which retrieves and filters prior solved examples similar to the current task; two variants: ST (Sentence Transformer selection) and Self (model filters examples itself after a prefilter).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive-Shot retrieval + selection pipeline (applies to prompts for various LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stage 1: maintain a knowledge base of solved problems; Stage 2: use a Sentence Transformer (cosine similarity) to prefilter examples (keep those > 0.5 similarity or top-15); Stage 3: either use the Sentence Transformer to pick the top-5 examples (Adaptive-Shot ST) or present prefiltered candidates to the solver LLM and have it choose the top-5 (Adaptive-Shot Self).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Retrieval-augmented few-shot selection (Adaptive few-shot)', 'Model-in-the-loop example selection (Self variant)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Adaptive-Shot ST: purely retrieval-based using sentence embeddings to pick the most relevant prior solved levels. Adaptive-Shot Self: the model is asked to select the most useful examples from a prefiltered candidate set, potentially leveraging its internal reasoning about relevance at the expense of more API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>This is a retrieval/selection technique that increases diversity of in-context exemplars by selecting relevant, potentially more complex examples rather than repeating static similar examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Euclidea and the math datasets used in generalisation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Selecting few-shot exemplars that better match current problem requirements to improve downstream geometric/tool-step generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Adaptive-Shot (ST) ChatGPT on Alpha/Beta: pass@1 = 13.3% (±1.8) / pass@50 = 21.2% (±3.0). Adaptive-Shot (Self): pass@1 = 13.6% (±1.8) / pass@50 = 21.5% (±2.7). These outperform static few-shot prompts (e.g., Few-shot Alpha: pass@1 11.4% / pass@50 18.6%). Zero-shot: 5.9% / 9.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Adaptive-Shot variants outperform static few-shot and zero-shot baselines. Adaptive-Shot Self slightly beats the ST variant but costs more API calls (because the model must filter candidates itself).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptive selection of few-shot exemplars that are semantically similar to the task (especially when the model self-selects) helps reduce style-mimicking and hallucination and improves pass@k metrics for geometric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Adaptive-Shot gains are modest and limited by the model's capacity to recognize which examples are useful; Adaptive-Shot Self requires more API calls and context, which can be costly for closed-source usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3105.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3105.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual Relations Prompt (VRP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Relations Prompt (structured scene description extracted by a VLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured scene-description produced by an auxiliary VLLM (GPT4-V) that lists points, lines, objects, and spatial relations; injected into agent prompts to improve spatial reasoning without requiring every agent to be multimodal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Visual Relations Prompt (produced by GPT4-V, consumed by agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT4-V receives the image/problem pair and returns a bullet-point list of detected points, lines, objects, and explicit relations/orientations. This single extraction is cached and inserted into all agent prompts between few-shot examples and the current problem description.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Scene-structure extraction via a VLLM (vision→language)', 'Augmenting language-only agents with structured visual relations']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>VRP disentangles scene recognition from symbolic geometric reasoning: GPT4-V extracts relations once per problem; the resulting structured text is appended to agent prompts so language-only LLMs can use spatial relations without native vision.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>VRP introduces a cross-modality step (vision→structured language) distinct from both native multimodal prompting and pure-language methods, increasing the diversity of inputs and reasoning support.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Euclidea (constructive geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Provide explicit spatial relations (point positions, collinearities, right angles, intersections) to assist LLM agents in constructing valid geometric tool sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Comparisons: GPT4-V + Image = 24.2% / 45.1% (pass@1 / pass@50). VRP-GPT4 = 25.3% / 48.6%. VRP-GPT4 ⋆ (multi-agent starred config) = 38.8% / 64.6%. VRP-ChatGPT improved ChatGPT from 28.1% pass@1 (multi-agent w/o VRP) to 34.5% (VRP-ChatGPT ⋆). VRP benefits non-visual models strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Structured VRP outperforms raw image prompting (GPT4-V + Image) especially when combined with multi-agent role specialization; VRP is cost-efficient (one extraction per problem) and portable to models without vision.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Supplying explicit, structured visual relations extracted by a VLLM to language-only agents improves geometric reasoning more than direct image prompting; VRP is particularly effective when combined with multi-agent role separation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>A raw multimodal pipeline (GPT4-V directly used for reasoning) sometimes validated incorrect geometric steps and gave only marginal improvement; VRP's benefits require the VLLM to produce accurate relation lists—mis-extraction would degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3105.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3105.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variable Renaming (target -> X)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Target Variable Renaming to 'X' to Mitigate Naming Bias</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple intervention that replaces problem target variable names (e.g., 'C', 'D', 'E') with 'X' to reduce alphabetical-order and naming biases in LLM-produced geometric constructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Variable-renaming intervention (applied in prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Renames the target variable in the problem statement to 'X' (a neutral unknown symbol) so that agents avoid spurious heuristics tied to alphabetical ordering (e.g., creating an intermediate point 'D' because the target is 'E').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Prompt rewriting / variable normalization']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Substitute the original target label with 'X' in prompts and the few-shot examples to encourage the model to generate minimal direct constructions rather than follow alphabetical naming patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>This is a single-style prompt intervention (not a reasoning style) aimed at reducing a linguistic bias; it is combined with other diverse methods in the multi-agent pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Euclidea (constructive geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Mitigate naming heuristics that cause agents to produce unnecessarily long or incorrect construction sequences due to alphabetical ordering artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>In a 20-problem experiment (3–5 step solutions), shifting the target by +1 or +2 letters worsened performance; substituting with 'X' reduced this effect. In multi-agent starred configurations, using 'X' contributed to higher pass@50 scores: e.g., VRP-ChatGPT ⋆ and VRP-GPT4 ⋆ show notably higher pass@50 than non-renamed variants (see Table 5; VRP-GPT4 ⋆ pass@50 ~64.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Renaming to 'X' reduces naming-bias failure modes compared to keeping original alphabetic targets or shifting them; it is complementary to other methods (validators, VRP).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A straightforward renaming to 'X' helps avoid unintended alphabetical heuristics in LLM reasoning and improves downstream validator acceptance and final solution correctness in multi-agent pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The effect is strongest when alphabetical proximity creates ambiguous intermediate labels; renaming alone does not solve deeper geometric reasoning failures and must be combined with other methods for large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Pal: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Diverse reasoning patterns when LLMs use Chain of Thought versus Program-Aid (Zhao et al., 2023 reference) <em>(Rating: 2)</em></li>
                <li>Simulacra / Generative agents: Interactive simulacra of human behavior (Park et al., 2023 / 2022 references) <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Solving geometry problems: Combining text and diagram interpretation (Seo et al., 2015) <em>(Rating: 1)</em></li>
                <li>Solving geometry from images: Learning to solve geometric construction problems from images (Macke et al., 2021) <em>(Rating: 2)</em></li>
                <li>AlphaGeometry / Solving olympiad geometry without human demonstrations (Trinh et al., 2024) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3105",
    "paper_id": "paper-267500136",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "ChatGPT (gpt-3.5)",
            "name_full": "OpenAI ChatGPT (gpt-3.5-turbo-16k, instruction-tuned)",
            "brief_description": "An instruction-tuned, conversational GPT family model (gpt-3.5-turbo variant) used via API in the paper for single-agent and multi-agent experiments, evaluated on constructive geometry (Euclidea) and math benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-16k (ChatGPT)",
            "model_description": "Instruction-following transformer from OpenAI, accessible via API; used as the primary language-only LLM for multi-round dialogues, few-shot prompting, and role-based (solver/validator) agents.",
            "model_size": null,
            "reasoning_methods": [
                "Zero-shot prompting",
                "Few-shot prompting (static examples)",
                "Adaptive-Shot (ST and Self) adaptive few-shot",
                "Chain-of-Thought style natural-language planning (S_NL)",
                "Tool-step translation (S_GT)",
                "Multi-agent simulacra (role specialization: solver + validator)",
                "Visual Relations Prompt (VRP) augmentation",
                "Variable renaming (target -&gt; X)",
                "Feedback mode (+FB)"
            ],
            "reasoning_methods_description": "Zero-shot/few-shot: standard instruction/ in-context prompting. Adaptive-Shot (ST) uses a Sentence Transformer to select relevant prior examples; Adaptive-Shot (Self) prompts the model to choose the most useful examples after a prefiltering stage. Chain-of-Thought (S_NL) agents produce natural-language rationales; S_GT agents translate those rationales into geometric-tool-step sequences. Validators (SV_*) critique and request revisions in up to 5 rounds. VRP is an auxiliary scene-description produced by a VLLM (GPT4-V) and inserted into agent prompts to improve spatial understanding. Variable renaming substitutes target labels with 'X' to reduce alphabetical naming bias. Feedback mode gives validators access to ground-truth correctness signals only.",
            "diversity_of_methods": "Diverse — the paper experiments with and combines multiple distinct reasoning styles (natural-language planning COT-like, program/tool-step translation, adaptive example selection, visual relation extraction, validators and feedback), comparing single-agent (similar-style) operation to multi-agent heterogeneous-role pipelines.",
            "reasoning_task_name": "Euclidea (constructive geometry), plus GSM8K / SVAMP / MATH-geometry splits",
            "reasoning_task_description": "Euclidea: constructive geometry problems requiring sequences of geometric-tool operations (ruler/compass-style tools) verified by an API. GSM8K/SVAMP/MATH-geometry: standard arithmetic and mathematical word-problem benchmarks used to test generalization of the multi-agent framework.",
            "performance_by_method": "Euclidea (pass@1 / pass@50): Few-shot (baseline few-shot with static examples) = 11.7% (±1.1) / 18.6% (±1.5). Single-agent geometric-tool solver (S_GT) = 13.6% / 21.5% (ChatGPT on Alpha/Beta packs). Two-agent S_NL−S_GT = 22.2% / 46.7%. With validators (SV_NL−SV_GT) = 28.1% (±1.1) / 53.5% (±2.4). Adding VRP (visual relations prompt) and running multi-agent setup (starred configs) increased ChatGPT from 28.1% to 34.5% pass@1 and to ~59.2% pass@50 in VRP-ChatGPT ⋆ experiments. Adaptive-Shot improved few-shot performance from ~11–12% to ~13.6% pass@1 (ChatGPT experiments in ablation). Feedback mode (+FB) gave only small gains. On non-geometric math benchmarks, combined SV_COT→SV_PAL pipeline achieved results reported as 'close to SOTA' in Table 6 (numerical table in paper not included in excerpt).",
            "comparison_of_methods": "Direct experimental comparisons show: static few-shot &lt; adaptive few-shot; single-agent S_GT &lt; two-agent S_NL−S_GT (22.2% vs 13.6% pass@1) &lt; multi-agent with validators SV_NL−SV_GT (28.1% pass@1). VRP further improved multi-agent performance substantially for language-only models. Feedback (+FB: validators see ground truth correctness only) produced only slight improvements, indicating limited benefit from ground-truth hints alone. Chaining agents (diverse roles) yields larger gains than repeating similar-style prompting in a single agent.",
            "key_findings": "For ChatGPT, moving from a single-tool-style agent to a heterogeneous multi-agent system (natural-language planner + tool-step translator + validators) plus adaptive example selection and VRP approximately doubled pass@1 performance on Euclidea; VRP particularly helps models lacking vision by supplying scene relations. Diversity of reasoning styles (COT-like planning + strict tool-step execution + validators) substantially outperforms reliance on a single, similar prompting style.",
            "counter_examples_or_negative_results": "Validators and chained agents can propagate errors when the initial natural-language rationale is flawed. Feedback mode with ground-truth correctness signals gave only marginal improvement. Zero-shot and naive few-shot often lead to hallucinated tool usage or style-mimicking irrelevant step sequences. Multimodal image input to GPT4-V sometimes validated incorrect steps and produced only marginal improvement over language+VRP approaches for later-stage geometric reasoning.",
            "uuid": "e3105.0",
            "source_info": {
                "paper_title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 / GPT4-V",
            "name_full": "OpenAI GPT-4 and GPT-4 Vision (GPT4-V)",
            "brief_description": "GPT-4 (capable of long-context reasoning) and its multimodal variant GPT4-V (vision-enabled) evaluated as high-performing baselines on Euclidea and in VRP/image experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (and GPT4-V for visual extraction)",
            "model_description": "Closed-source multimodal-capable transformer by OpenAI; used for direct image+prompt reasoning (GPT4-V) and as language-only agent in multi-agent setups; used also to produce Visual Relations Prompt (VRP) via gpt4-vision-preview endpoint.",
            "model_size": null,
            "reasoning_methods": [
                "Few-shot prompting",
                "Adaptive-Shot",
                "Chain-of-Thought style natural-language planning",
                "Tool-step (geometric) execution",
                "Multimodal image+prompt (GPT4-V)",
                "VRP (visual relations prompt produced by GPT4-V)",
                "Multi-agent simulacra with role specialization"
            ],
            "reasoning_methods_description": "Same general methods as applied to ChatGPT. Additionally tested as a VLLM that outputs explicit scene descriptions (points, lines, relations) from images which are then used as VRP in agent prompts or used directly (GPT4-V + image).",
            "diversity_of_methods": "Diverse — experiments compare language-only, multimodal direct image prompting, and VRP-augmented pipelines; also compare single-agent vs multi-agent configurations.",
            "reasoning_task_name": "Euclidea (constructive geometry) and other geometry/math splits",
            "reasoning_task_description": "Constructive geometry tasks requiring sequences of abstract geometric tool steps; verification via Euclidea API.",
            "performance_by_method": "Euclidea (pass@1 / pass@50): Few-shot GPT-4 baseline ~21.2% (±1.3) / 38.3% (±1.4) in earlier table. On Alpha/Beta packs: language-only GPT4 ~23.9% / 44.8%; multi-agent GPT4 ⋆ ~33.7% / 62.0%. GPT4-V + raw Image: 24.2% / 45.1% (marginal improvement). VRP-GPT4 (VRP inserted into prompts) improved to 25.3% / 48.6%; VRP-GPT4 ⋆ (multi-agent starred config) further to 38.8% / 64.6%. Under 'Ours' full multi-agent framework: GPT-4 reached 38.9% pass@1 and 67.7% pass@50.",
            "comparison_of_methods": "Direct comparisons: GPT4-V raw-image pipeline provided only modest gains over language-only GPT-4; using GPT4-V to extract a structured VRP and feeding that to agents produced a larger benefit (VRP-GPT4 &gt; GPT4-V + image). Multi-agent role-specialization further boosted GPT-4 performance significantly. Overall, diverse-method pipelines (VRP + multi-agent + adaptive-shot + renaming) outperform similar single-method prompting.",
            "key_findings": "GPT-4 benefits from multi-agent role specialization and VRP; however direct image prompting (GPT4-V) does not solve late-stage geometric reasoning errors and occasionally endorses invalid steps. Structured visual relation extraction (VRP) is more useful than raw image prompting for downstream geometric-tool planning.",
            "counter_examples_or_negative_results": "GPT4-V validated some provably incorrect tool steps in experiments, indicating that vision alone without structured relation prompting can mislead geometric reasoning. Feedback mode with ground-truth validators yields only small gains even for GPT-4.",
            "uuid": "e3105.1",
            "source_info": {
                "paper_title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MetaMath-Mistral-7B",
            "name_full": "Meta-Math fine-tuned Mistral (MetaMath-Mistral, 7B)",
            "brief_description": "An open-source 7B model (Mistral base) finetuned on mathematical datasets (MetaMath) used to test transfer of arithmetic/algebraic reasoning to constructive geometry and its responsiveness to the multi-agent framework.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MetaMath-Mistral-7B",
            "model_description": "A 7B Mistral checkpoint finetuned on mathematical reasoning corpora (MetaMath) to specialize in arithmetic and algebraic problem solving; evaluated here for geometric transfer and within the simulacra multi-agent system.",
            "model_size": "7B",
            "reasoning_methods": [
                "Few-shot prompting (static)",
                "Fine-tuning on Euclid's Elements (QLoRA, 4-bit)",
                "Adaptive-Shot",
                "Simulacra multi-agent (S_NL, S_GT, validators)",
                "VRP, variable renaming, feedback mode"
            ],
            "reasoning_methods_description": "Tested standard few-shot and fine-tuned variants (fine-tuned on Euclid's Elements chapters to provide inductive geometric knowledge). Then evaluated under Adaptive-Shot and the full multi-agent framework with VRP and renaming.",
            "diversity_of_methods": "Diverse — the paper explores both single-method baselines (few-shot / finetuned) and diverse multi-agent combinations including visual relations and validators, showing how methods combine to improve performance.",
            "reasoning_task_name": "Euclidea (constructive geometry)",
            "reasoning_task_description": "Constructive geometry tasks from PyEuclidea verified programmatically.",
            "performance_by_method": "Euclidea (pass@1 / pass@50): Few-shot MetaMath-Mistral = 8.9% (±0.7) / 13.4% (±1.2). Finetuned on Euclid's Elements = 9.4% (±0.9) / 16.0% (approx; excerpt truncated). Under full multi-agent 'Ours' configuration: MetaMath-Mistral (7B) = 14.9% (±1.3) / 21.1% (±1.7). With Feedback (+FB) slight increase to 15.1% / 21.4%.",
            "comparison_of_methods": "Fine-tuning on Euclid's Elements gave only modest gains over few-shot baselines. The multi-agent framework (diverse methods) produced larger, nontrivial improvements (≈+5–6 percentage points pass@1) compared to finetuning alone, indicating that diverse prompting/agent interactions add capabilities beyond static finetuning on classical geometry texts.",
            "key_findings": "Mathematics-specialized pretraining/fine-tuning (MetaMath) does not transfer strongly to constructive geometry; combining mathematical models with diverse reasoning components (Adaptive-Shot, simulated agents, VRP, renaming) yields larger improvements than fine-tuning alone.",
            "counter_examples_or_negative_results": "Fine-tuning on Euclid's Elements was insufficient to reach the performance of larger closed models; gains from feedback (+FB) were small, suggesting limitations in how much hints or correctness signals help solvers to explore alternate correct reasoning paths.",
            "uuid": "e3105.2",
            "source_info": {
                "paper_title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "COT_vs_PAL (SV_COT→SV_PAL)",
            "name_full": "Chain-of-Thought (COT) vs Program-Aided Language (PAL) combined multi-agent pipeline",
            "brief_description": "A contrasted pair of reasoning styles: COT (free-form natural-language chain-of-thought planning) and PAL (program-aided, code-like step synthesis); the paper composes them via multi-agent pairs (SV_COT producing rationales then SV_PAL translating to code) to leverage creativity and computational precision.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLM multi-agent pipeline (SV_COT then SV_PAL)",
            "model_description": "SV_COT: solver-validator pair operating in a chain-of-thought (natural language) domain to generate creative/rationale-rich plans; SV_PAL: solver-validator pair specialized in program/code generation to convert rationales into executable computations (e.g., Python). Both stages are LLM-driven in the paper's experiments (no external interpreter used for V_PAL).",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (COT)",
                "Program-Aided Language (PAL)",
                "Adaptive few-shot (Adaptive-Shot)",
                "Role-specialized multi-agent chaining (SV_COT → SV_PAL)"
            ],
            "reasoning_methods_description": "COT agents generate natural-language explanatory reasoning (creative, flexible); PAL agents translate reasoning into programmatic steps (accurate for numeric computation). The paper chains a COT-style solver-validator pair to a PAL-style pair so that creative plans get transformed into precise code or tool steps. The PAL stage in the paper is another LLM agent (not a Python interpreter).",
            "diversity_of_methods": "Explicitly diverse — the experiment purposely composes two qualitatively different reasoning styles (COT and PAL) to exploit complementary strengths (creativity vs computational accuracy).",
            "reasoning_task_name": "GSM8K, SVAMP, MATH (geometry split)",
            "reasoning_task_description": "Arithmetic and math word-problem datasets where both creative decomposition and precise computation are needed; used to test generalisation of the multi-agent framework beyond constructive geometry.",
            "performance_by_method": "Paper reports that the SV_COT→SV_PAL multi-agent setup performs 'very close to current state-of-the-art methods' on these math datasets (exact numeric values referenced in Table 6 of the paper; excerpt did not include full table numbers). MetaMath-Mistral also enjoyed a significant boost from this method.",
            "comparison_of_methods": "The authors reference prior work (Zhao et al., 2023) that found COT yields creative solutions while PAL yields higher numeric accuracy; their experiments confirm that chaining COT-like planning with PAL-style translation yields a system that leverages both properties and attains near-SOTA performance without using an external interpreter for verification.",
            "key_findings": "Combining diverse reasoning styles (COT for planning, PAL for programmatic precision) in a chained multi-agent architecture yields better empirical performance on math benchmarks than either homogeneous approach alone, supporting the claim that diverse reasoning methods are complementary.",
            "counter_examples_or_negative_results": "The paper chose not to use an external Python interpreter as a checker for PAL stage; using only LLMs for PAL still performed well, but lacking a real executor could limit final numeric accuracy versus approaches that use actual code execution/verifiers.",
            "uuid": "e3105.3",
            "source_info": {
                "paper_title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Simulacra Multi-Agent Framework",
            "name_full": "Simulacra-based multi-agent role-specialized framework (solvers + validators + VRP + Adaptive-Shot + renaming)",
            "brief_description": "A framework introduced in this paper that composes multiple LLM agents with distinct roles (natural-language solver, geometric-tool solver, validators), an adaptive example-selection mechanism, a visual relations prompt, and variable-renaming to improve constructive-geometry reasoning without weight updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Simulacra Multi-Agent Framework (applied to various LLM backbones)",
            "model_description": "A role-assigned multi-agent system inspired by simulacra literature: S_NL (natural-language solver) produces plans; S_GT (geometric-tool solver) maps plans to tool steps; validators (SV_*) critique and request fixes. Augmentations include Adaptive-Shot (example retrieval/selection), Visual Relations Prompt (VRP) produced by a VLLM, and target variable renaming to 'X' to mitigate naming biases.",
            "model_size": null,
            "reasoning_methods": [
                "Role-specialized multi-agent reasoning (dividing planning and acting across agents)",
                "Adaptive few-shot retrieval (Adaptive-Shot ST / Self)",
                "Validator feedback loops (multi-round up to 5)",
                "Visual Relations Prompt (VRP) as auxiliary scene representation",
                "Variable renaming to neutralize naming biases",
                "Optional Feedback mode (+FB) where validators see ground-truth correctness"
            ],
            "reasoning_methods_description": "Agents are prompted with domain-specific roles and few-shot examples (adaptive). Validators receive Euclid's Elements examples and incorrect/rectified pairs. VRP gives explicit scene element lists/relations extracted once by GPT4-V and inserted into all agent prompts. Adaptive-Shot uses a Sentence Transformer (ST) for filtering and/or the model to self-select useful examples. Variable renaming sets targets to 'X' to avoid alphabetical-naming heuristics.",
            "diversity_of_methods": "Highly diverse — integrates multiple distinct reasoning styles and auxiliary modules (language planning, strict tool-language execution, visual-relation extraction, retrieval, role-based critique) into a single pipeline; the paper shows their combination yields compounding improvements.",
            "reasoning_task_name": "Euclidea (primary), plus GSM8K/SVAMP/MATH geometry for generalization tests",
            "reasoning_task_description": "Constructive geometry tasks requiring multi-step tool sequences; multi-agent framework also tested on math word-problem datasets to assess cross-domain generalization.",
            "performance_by_method": "Aggregate reported effects: On Euclidea, single LLM few-shot baselines are low (single-digit to low-teens pass@1); applying the full multi-agent pipeline + Adaptive-Shot + VRP + renaming produced large improvements. Example numeric snapshots: MetaMath-Mistral: few-shot 8.9% → 'Ours' multi-agent 14.9% pass@1; ChatGPT: few-shot ~11.7% → multi-agent (SV_NL−SV_GT + VRP + renaming + Adaptive-Shot) 32.3% pass@1 (Ours entry); GPT-4: few-shot ~21.2% → 'Ours' 38.9% pass@1. Agent configuration ablation: S_GT only = 13.6% pass@1; S_NL−S_GT = 22.2%; SV_NL−SV_GT = 28.1%. VRP-augmented starred configs push these higher (e.g., VRP-ChatGPT ⋆ 34.5% pass@1).",
            "comparison_of_methods": "Ablations directly compare: static few-shot vs Adaptive-Shot (Adaptive-Shot(Self) slightly better than ST), single-agent tool solver vs dual-agent separation of planning and execution, and adding validators vs not. Results consistently show diverse-role multi-agent systems outperform similar-style single-agent prompting. VRP gives extra gains especially for language-only models. Variable renaming reduces naming-induced failure modes and improves validator acceptance rates.",
            "key_findings": "Decomposing reasoning into specialized agent roles and combining diverse supporting methods (retrieval-based adaptive examples, structured visual-relations input, naming-bias mitigation, and validators) yields much larger gains on constructive-geometry tasks than incremental refinements of a single prompting style or finetuning on geometry texts alone.",
            "counter_examples_or_negative_results": "Chaining agents can propagate flawed rationales; validators mitigate but do not eliminate this. Ground-truth-only feedback (+FB) provided to validators resulted in only slight gains, indicating limited utility of correctness hints without changes in solver exploration strategies. The framework is more costly (API rounds, context) and may be slower than single-agent few-shot in low-cost settings.",
            "uuid": "e3105.4",
            "source_info": {
                "paper_title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Adaptive-Shot (ST / Self)",
            "name_full": "Adaptive-Shot adaptive few-shot example selection (Sentence-Transformer / Self-filtered)",
            "brief_description": "An adaptive few-shot selection mechanism introduced in the paper which retrieves and filters prior solved examples similar to the current task; two variants: ST (Sentence Transformer selection) and Self (model filters examples itself after a prefilter).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adaptive-Shot retrieval + selection pipeline (applies to prompts for various LLMs)",
            "model_description": "Stage 1: maintain a knowledge base of solved problems; Stage 2: use a Sentence Transformer (cosine similarity) to prefilter examples (keep those &gt; 0.5 similarity or top-15); Stage 3: either use the Sentence Transformer to pick the top-5 examples (Adaptive-Shot ST) or present prefiltered candidates to the solver LLM and have it choose the top-5 (Adaptive-Shot Self).",
            "model_size": null,
            "reasoning_methods": [
                "Retrieval-augmented few-shot selection (Adaptive few-shot)",
                "Model-in-the-loop example selection (Self variant)"
            ],
            "reasoning_methods_description": "Adaptive-Shot ST: purely retrieval-based using sentence embeddings to pick the most relevant prior solved levels. Adaptive-Shot Self: the model is asked to select the most useful examples from a prefiltered candidate set, potentially leveraging its internal reasoning about relevance at the expense of more API calls.",
            "diversity_of_methods": "This is a retrieval/selection technique that increases diversity of in-context exemplars by selecting relevant, potentially more complex examples rather than repeating static similar examples.",
            "reasoning_task_name": "Euclidea and the math datasets used in generalisation experiments",
            "reasoning_task_description": "Selecting few-shot exemplars that better match current problem requirements to improve downstream geometric/tool-step generation.",
            "performance_by_method": "Adaptive-Shot (ST) ChatGPT on Alpha/Beta: pass@1 = 13.3% (±1.8) / pass@50 = 21.2% (±3.0). Adaptive-Shot (Self): pass@1 = 13.6% (±1.8) / pass@50 = 21.5% (±2.7). These outperform static few-shot prompts (e.g., Few-shot Alpha: pass@1 11.4% / pass@50 18.6%). Zero-shot: 5.9% / 9.6%.",
            "comparison_of_methods": "Adaptive-Shot variants outperform static few-shot and zero-shot baselines. Adaptive-Shot Self slightly beats the ST variant but costs more API calls (because the model must filter candidates itself).",
            "key_findings": "Adaptive selection of few-shot exemplars that are semantically similar to the task (especially when the model self-selects) helps reduce style-mimicking and hallucination and improves pass@k metrics for geometric reasoning.",
            "counter_examples_or_negative_results": "Adaptive-Shot gains are modest and limited by the model's capacity to recognize which examples are useful; Adaptive-Shot Self requires more API calls and context, which can be costly for closed-source usage.",
            "uuid": "e3105.5",
            "source_info": {
                "paper_title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Visual Relations Prompt (VRP)",
            "name_full": "Visual Relations Prompt (structured scene description extracted by a VLLM)",
            "brief_description": "A structured scene-description produced by an auxiliary VLLM (GPT4-V) that lists points, lines, objects, and spatial relations; injected into agent prompts to improve spatial reasoning without requiring every agent to be multimodal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Visual Relations Prompt (produced by GPT4-V, consumed by agents)",
            "model_description": "GPT4-V receives the image/problem pair and returns a bullet-point list of detected points, lines, objects, and explicit relations/orientations. This single extraction is cached and inserted into all agent prompts between few-shot examples and the current problem description.",
            "model_size": null,
            "reasoning_methods": [
                "Scene-structure extraction via a VLLM (vision→language)",
                "Augmenting language-only agents with structured visual relations"
            ],
            "reasoning_methods_description": "VRP disentangles scene recognition from symbolic geometric reasoning: GPT4-V extracts relations once per problem; the resulting structured text is appended to agent prompts so language-only LLMs can use spatial relations without native vision.",
            "diversity_of_methods": "VRP introduces a cross-modality step (vision→structured language) distinct from both native multimodal prompting and pure-language methods, increasing the diversity of inputs and reasoning support.",
            "reasoning_task_name": "Euclidea (constructive geometry)",
            "reasoning_task_description": "Provide explicit spatial relations (point positions, collinearities, right angles, intersections) to assist LLM agents in constructing valid geometric tool sequences.",
            "performance_by_method": "Comparisons: GPT4-V + Image = 24.2% / 45.1% (pass@1 / pass@50). VRP-GPT4 = 25.3% / 48.6%. VRP-GPT4 ⋆ (multi-agent starred config) = 38.8% / 64.6%. VRP-ChatGPT improved ChatGPT from 28.1% pass@1 (multi-agent w/o VRP) to 34.5% (VRP-ChatGPT ⋆). VRP benefits non-visual models strongly.",
            "comparison_of_methods": "Structured VRP outperforms raw image prompting (GPT4-V + Image) especially when combined with multi-agent role specialization; VRP is cost-efficient (one extraction per problem) and portable to models without vision.",
            "key_findings": "Supplying explicit, structured visual relations extracted by a VLLM to language-only agents improves geometric reasoning more than direct image prompting; VRP is particularly effective when combined with multi-agent role separation.",
            "counter_examples_or_negative_results": "A raw multimodal pipeline (GPT4-V directly used for reasoning) sometimes validated incorrect geometric steps and gave only marginal improvement; VRP's benefits require the VLLM to produce accurate relation lists—mis-extraction would degrade performance.",
            "uuid": "e3105.6",
            "source_info": {
                "paper_title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Variable Renaming (target -&gt; X)",
            "name_full": "Target Variable Renaming to 'X' to Mitigate Naming Bias",
            "brief_description": "A simple intervention that replaces problem target variable names (e.g., 'C', 'D', 'E') with 'X' to reduce alphabetical-order and naming biases in LLM-produced geometric constructions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Variable-renaming intervention (applied in prompts)",
            "model_description": "Renames the target variable in the problem statement to 'X' (a neutral unknown symbol) so that agents avoid spurious heuristics tied to alphabetical ordering (e.g., creating an intermediate point 'D' because the target is 'E').",
            "model_size": null,
            "reasoning_methods": [
                "Prompt rewriting / variable normalization"
            ],
            "reasoning_methods_description": "Substitute the original target label with 'X' in prompts and the few-shot examples to encourage the model to generate minimal direct constructions rather than follow alphabetical naming patterns.",
            "diversity_of_methods": "This is a single-style prompt intervention (not a reasoning style) aimed at reducing a linguistic bias; it is combined with other diverse methods in the multi-agent pipeline.",
            "reasoning_task_name": "Euclidea (constructive geometry)",
            "reasoning_task_description": "Mitigate naming heuristics that cause agents to produce unnecessarily long or incorrect construction sequences due to alphabetical ordering artifacts.",
            "performance_by_method": "In a 20-problem experiment (3–5 step solutions), shifting the target by +1 or +2 letters worsened performance; substituting with 'X' reduced this effect. In multi-agent starred configurations, using 'X' contributed to higher pass@50 scores: e.g., VRP-ChatGPT ⋆ and VRP-GPT4 ⋆ show notably higher pass@50 than non-renamed variants (see Table 5; VRP-GPT4 ⋆ pass@50 ~64.6%).",
            "comparison_of_methods": "Renaming to 'X' reduces naming-bias failure modes compared to keeping original alphabetic targets or shifting them; it is complementary to other methods (validators, VRP).",
            "key_findings": "A straightforward renaming to 'X' helps avoid unintended alphabetical heuristics in LLM reasoning and improves downstream validator acceptance and final solution correctness in multi-agent pipelines.",
            "counter_examples_or_negative_results": "The effect is strongest when alphabetical proximity creates ambiguous intermediate labels; renaming alone does not solve deeper geometric reasoning failures and must be combined with other methods for large gains.",
            "uuid": "e3105.7",
            "source_info": {
                "paper_title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Pal: Program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Diverse reasoning patterns when LLMs use Chain of Thought versus Program-Aid (Zhao et al., 2023 reference)",
            "rating": 2,
            "sanitized_title": "diverse_reasoning_patterns_when_llms_use_chain_of_thought_versus_programaid_zhao_et_al_2023_reference"
        },
        {
            "paper_title": "Simulacra / Generative agents: Interactive simulacra of human behavior (Park et al., 2023 / 2022 references)",
            "rating": 2,
            "sanitized_title": "simulacra_generative_agents_interactive_simulacra_of_human_behavior_park_et_al_2023_2022_references"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Solving geometry problems: Combining text and diagram interpretation (Seo et al., 2015)",
            "rating": 1,
            "sanitized_title": "solving_geometry_problems_combining_text_and_diagram_interpretation_seo_et_al_2015"
        },
        {
            "paper_title": "Solving geometry from images: Learning to solve geometric construction problems from images (Macke et al., 2021)",
            "rating": 2,
            "sanitized_title": "solving_geometry_from_images_learning_to_solve_geometric_construction_problems_from_images_macke_et_al_2021"
        },
        {
            "paper_title": "AlphaGeometry / Solving olympiad geometry without human demonstrations (Trinh et al., 2024)",
            "rating": 2,
            "sanitized_title": "alphageometry_solving_olympiad_geometry_without_human_demonstrations_trinh_et_al_2024"
        }
    ],
    "cost": 0.022377249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models
20 Sep 2024</p>
<p>Spyridon Mouselinos <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#115;&#46;&#109;&#111;&#117;&#115;&#101;&#108;&#105;&#110;&#111;&#115;&#64;&#117;&#119;&#46;&#101;&#100;&#117;&#46;&#112;&#108;">&#115;&#46;&#109;&#111;&#117;&#115;&#101;&#108;&#105;&#110;&#111;&#115;&#64;&#117;&#119;&#46;&#101;&#100;&#117;&#46;&#112;&#108;</a>. 
University of Warsaw</p>
<p>Henryk Michalewski 
Google DeepMind</p>
<p>Mateusz Malinowski 
Google DeepMind</p>
<p>Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models
20 Sep 20249047830D90A739B375A1161EBC5F984EarXiv:2402.03877v3[cs.CL]
Figure 1.Drawing inspirations from the Ancient Greek Academy, we divide the reasoning pipeline into three stages.From left to right:The current geometric construction task is broken down into the image, its task description, and available tools.Our framework employs four LLM-based agents, each prompted with a specific role and task.A collaborative multi-round discussion is conducted where the geometric construction is effectively solved, reflecting the Academy's collective approach towards problem-solving and reasoning.</p>
<p>Introduction</p>
<p>Recent advancements in Large Language Models (LLMs) are groundbreaking.Models' capacity to handle complex mathematical and algorithmic tasks, interpreting data from graphs and tables, shows an ever-increasing proficiency in various forms of reasoning.Despite this, constructive geometry is one domain where LLMs still face significant challenges.This area of study, fundamental to human mathematical reasoning, lies at the intersection of tool usage, planning, and spatial reasoning.</p>
<p>Our investigation into LLMs' capabilities in this domain reveals several intriguing aspects.We find that in instruction following, LLMs often exhibit a bias towards the style of the examples rather than focusing on the reasoning and proper abstraction necessary for solving these geometrical problems.Furthermore, LLMs that are specialized in mathematical domains do not necessarily show proficiency in constructive geometry.This suggests that the skills required for numerical or algebraic reasoning do not directly translate to the spatial and tool-based problem-solving required in this domain.Another interesting observation is the impact of variable naming on problem-solving.Our findings show that the choice of names for variables in geometric constructions can significantly affect both the length and quality of the solutions, pointing to a potential bias in LLMs where variable names carry unintended semantic weight.Moreover, despite being provided with visual aids, multimodal LLMs such as GPT4-V demonstrate difficulties in interpreting 2D spatial relationships.They can identify objects in a scene but struggle to integrate them into a coherent plan involving tools or steps.There's also a tendency to 'hallucinate' objects or points, which complicates their spatial reasoning process.</p>
<p>We propose a framework that aims to overcome these challenges.Our solution includes renaming variables to more context-appropriate names, aiming to mitigate the influence of naming conventions on the LLMs' problem-solving process.We also introduce an adaptive selection mechanism on the prompt examples, focusing the LLM on relevant information and avoiding information overload.The model uses previous instances of geometric tasks as a basis for subsequent ones, allowing it to build on past experiences and enhance its adaptability and context awareness.A key factor in our approach is the use of simulacra-based conversational agents (Park et al., 2023).In this setup, agents take on specialized roles, some acting as reasoners while others acting as solvers or tool users.This cross-domain dialogue leverages the strengths of each agent type and fosters a more effective problem-solving approach than traditional role-playing methods.Here, we aim to bridge the gap in geometric reasoning for LLMs, enhancing their capabilities in this complex and fundamental domain.</p>
<p>Contributions.The main contributions of our work can be summarized in three points: First, we are the first to provide an extensive analysis of the state-of-the-art leading LLMs' surprising difficulties in solving fundamental constructive geometric problems, highlighting a critical gap in their reasoning capabilities.Second, we introduce three methods that assist LLMs in overcoming current limitations in the domain of constructive geometry.Our dynamic prompting mechanism enables LLMs to build upon their previous interactions instead of uninformative static prompts, our variable renaming technique neutralizes biases from variable name conventions that complicate reasoning, and our scene description prompt enhances LLMs' abilities to understand and manipulate spatial relationships in a geometric context.Third, we present a novel simulacra-based system that effectively combines tool usage, instruction following, and geometric problem-solving.Our suggestions show a substantial improvement over non-collaborative methods, leading to effective solutions in geometric reasoning.</p>
<p>Related Work</p>
<p>Our approach is inspired by works of various research directions, which we briefly describe here.Prompt Engineering The emergent reasoning and incontext abilities of multi-billion-parameter LLMs (OpenAI, 2023;Brown et al., 2020;Google, 2023;Mitchell et al., 2022;Chowdhery et al., 2022) transformed prompt engineering from simple commands to sophisticated interactions for eliciting detailed responses.Works like (Wei et al., 2022;Kojima et al., 2022;Zhang et al., 2023b) use intermediate reasoning steps in prompts, significantly improving performance in arithmetic and symbolic reasoning tasks.As an alternative to hand-crafted prompts, a series of techniques (Reynolds &amp; McDonell, 2021;Zhou et al., 2023d;Shin et al., 2020) are proposed for automated prompt generation, exhibiting better results in reasoning tasks.In multi-agent scenarios, (Li et al., 2023) introduce Inception Prompting, a method that enables LLM-based agents to prompt each other in collaborative environments under the assignment of roles.Recently, (Hao et al., 2023;Xie et al., 2023;Yao et al., 2023a) use tree-search and self-evaluation for tasks requiring exploration and strategic lookahead.(Yao et al., 2023b) unifies planning and acting in LLMs, prompting the models to generate both reasoning traces and task-specific actions.As a direct expansion, (Zhou et al., 2023b) advances these concepts with a Monte Carlo tree search over possible reasoning steps and actions, achieving state-of-the-art results in coding tasks.</p>
<p>Simulacra -Conversational Agents</p>
<p>The concept of 'Agents' as entities exhibiting emergent intelligence through collective interaction was introduced by (Minsky, 1986).This idea has been extensively applied in machine learning, particularly within reinforcement learning (Sukhbaatar et al., 2016;Lowe et al., 2017;Havrylov &amp; Titov, 2017;Mordatch &amp; Abbeel, 2018;Dafoe et al., 2020;Bard et al., 2020;Sheng et al., 2020;Hosseini-Asl et al., 2020;Du et al., 2021).Given their global knowledge and conversational skills, Large Language Models (LLMs) have been considered potential agents (Huang et al., 2022;Andreas, 2022;Lo et al., 2023).(Park et al., 2022), has shown that LLMs can effectively function in complex social scenarios.Recently, (Li et al., 2023;Hong et al., 2023) systematize the concept of a simulacrum, providing a less error-prone and message-effective framework for agents.Specialized agents are also suggested, with LLMs adopting different roles through prompting in various domains.Building on this work, (Qian et al., 2023) exhibit impressive results through collaborative personas, imitating the entire workflow of a software company.Novel frameworks (Wu et al., 2023;Chen et al., 2023;Lin et al., 2023;Zhou et al., 2023c) add further functionalities such as visualization capabilities, the addition of custom roles and objectives, dynamic generation of agents, and distributed action execution.Finally, (Wang et al., 2023) introduce the first benchmark for finegrained role-playing and suggest training on role-specific contexts and instructions, allowing models to better adapt to different characters or personas during inference.</p>
<p>Geometric problems While mathematics and algorithms remain predominant in reasoning challenges, the exploration of geometry has been limited.Key datasets such as (Seo et al., 2015;Chen et al., 2022;Zhang et al., 2023a;Lu et al., 2021) typically adopt a multiple-choice format featuring annotated diagrams for interpretation and questions presented in either natural or symbolic language.Common approaches involve converting such problems into relational sets in a domain-specific language (DSL) or as formal structural clauses.The final reasoning stage is often executed through a symbolic solver or a model trained on DSL.The most recent and parallel work, AlphaGeometry (Trinh et al., 2024), achieved impressive results on IMO-level geometry problems.The system uses an LLM trained on synthetic DSL data as an interface to a theorem-proof engine where mathematical reasoning is delegated.Through rounds of interaction and parallel search, the LLM translates the final answer back to human-readable steps, achieving the interpretability of the proposed solution.That level of competence is possible as geometry is complete and decidable (Tarski, 1959).Contrary to that, we define all the necessary modules that generates and verify hypotheses using open-ended LLMs; showing how to improve their geometric abilities without changing their weights.In the field of constructive geometry, which encompasses tasks requiring planning, reasoning, and the use of tools.We draw inspiration from (Macke et al., 2021;Wong et al., 2022), which focus on Euclidea (Euclidea; PyEuclidea), a dataset that contains progressively challenging geometric problems.In our work, we propose an alternative to symbolic solvers and tree-based search algorithms, enhancing the reasoning capabilities of LLMs in this under-explored domain.</p>
<p>Preliminaries</p>
<p>In this section, we present the datasets, models, metrics, and definitions central to our experimental framework.Euclidea Our primary benchmark is the geometry game Euclidea, an online construction challenge with a range of geometric problems.The game offers eight geometric tools, the availability of which varies by level, to solve progressively complex challenges.While initial levels are straightforward, their difficulty quickly escalates, presenting significant challenges.In our experiments, we use the Python version of Euclidea (PyEuclidea), designed for academic use, encompassing ninety-eight challenges across ten difficulty levels.A custom API for programmatic construction and verification of solutions is also available.We also compile a natural language version of the Euclidea dataset, including challenge definitions, solutions, and explanations from https://euclidea.fandom.com/wiki/Euclidea_Wiki, which we will make accessible for future research.</p>
<p>Euclid's Elements We explore the efficacy of training opensource LLMs on Euclid's Elements, the seminal work on constructive geometry.In Elements, a set of fundamental axioms and tools -the ruler and compass -are initially presented.Then, progressively more complex tools are synthesized through the solution of constructions using knowledge acquired exclusively from previous chapters.Similarly, various corollaries are suggested during the Elements, offering reasoning shortcuts for more advanced problems in forthcoming chapters.We posit that exposure to Elements would better align LLMs with constructive geometry tasks, as this work theoretically contains the required knowledge to solve the challenges.We utilize the publicly available English translation of Euclid's Elements (Fitzpatrick, 2007(Fitzpatrick, -2008) ) found here.</p>
<p>Models In our experimental setup, we examine the performance of seven LLMs (five open-source and two closedsource).LLamaV2 (Touvron et al., 2023) is one the most popular instruction-following LLMs, exhibiting impressive performance across various tasks involving commonsense reasoning, natural language understanding, dialogue, maths, and coding.We test two available checkpoints, namely its 7B and 13B variants.We also choose to include the recently introduced Mistral (Jiang et al., 2023) and its finetuned variant named Zephyr (Tunstall et al., 2023), two 7B LLMs that, at their scale exhibit performance comparative to larger checkpoints of other open-source LLMs.Meta-Math (Yu et al., 2023) specializes in mathematical and algebraic reasoning, achieving state-of-the-art results on the Math (Hendrycks et al., 2021) and GSM8k (Cobbe et al., 2021) challenges among all open-source LLMs.We test two variants: the MetaMath-LlamaV2-13B and the MetaMath-Mistral-7B.Incorporating these models will enable us to test whether the knowledge acquired by exposure to standard mathematical datasets is transferable to the domain of constructive geometrical reasoning.Regarding closed-source models, we select OpenAI's ChatGPT and GPT-4 models due to their superior performance in reasoning and problemsolving tasks.GPT-4, in particular, is the leading model across various datasets.Additionally, these models offer the capability to maintain a large context, which is especially beneficial for storing multiple rounds of dialogue between LLMs in our multi-agent setup.We hypothesize that fostering longer dialogues can be beneficial in domains such as constructive geometry, where various paths can yield correct solutions, and minor deviations can cause significant errors.We also aimed to examine how well these models' sophisticated instruction-following abilities perform with abstract yet precise geometric tools introduced at the prompt level.Finally, the recent integration of a vision module in GPT-4 further expands our investigation, allowing us to assess the role of visual inputs in solving geometric challenges.</p>
<p>Performance metrics In constructive geometry, multiple reasoning paths can lead to a correct result.In some cases, even reordering the steps of a proposed solution may be possible without harming its correctness.Thus, instead of employing the accuracy metric and requiring the model to perform an exact match of the ground truth answer, we evaluate generated constructions with the pass@k metric (Kulal et al., 2019).Pass@k measures the existence of a correct completion among k independent generations of the model.In (Chen et al., 2021), authors propose an updated unbiased version that we adopt throughout the rest of this work.We validate the functional correctness of the generated solutions using the Euclidea Python API.The average of ten runs with different seeds is presented for all experiments.We choose sampling temperatures of 0.2 / 0.6 for pass@1 / pass@50 during generation.</p>
<p>Method</p>
<p>This section introduces the various components of our proposed framework, each specifically designed to address the limitations of LLMs in solving constructive geometrical problems.</p>
<p>Prompt Engineering for Geometric Reasoning</p>
<p>For each tested geometric challenge, we prompt our LLMs with a description outlining the available geometric tools, their expected operation, and the task requirements.We employ a few-shot learning setup to enhance our models' accuracy and reduce erroneous interpretations of tool functionalities.Specifically, we maintain a memory bank of all previously encountered problems.For each new problem, we select a subset of these problems that are most relevant to the current task.This approach ensures that our model is consistently exposed to more intricate problems and diverse tools, cultivating a more nuanced and context-aware reasoning process.We refer to this mechanism as Adaptive-Shot.</p>
<p>Our proposed Adaptive-Shot mechanism employs a Sentence Transformer to compare the similarity between the current level's description and available tools and those of all other levels.After this initial filtering stage, the remaining examples are presented back to the model.Subsequently, the model is tasked to identify the top five most useful examples for the current challenge, integrating them into the final few-shot prompt.This method not only refines the model's understanding of geometric concepts but also enhances its ability to apply this knowledge effectively to new and more complex problems.An illustrated schematic of our mechanism can be found in the Appendix, Section D.</p>
<p>From Single Models To Simulacra</p>
<p>In the following stage, inspired by multi-agent collaborative environments of LLM-powered agents -simulacra (Li et al., 2023;Park et al., 2022), we utilize role-assigning prompts and employ two sets of agents.In the first agent set, which we will refer to as solvers, the natural language solver S N L is tasked to generate rationales for approaching the problem in natural language, and the geometric tool solver S GT interprets these rationales and converts them to a series of steps using exclusively the available geometric tools.Note here that S GT can also be used directly without the existence of S N L , which is the default approach of a single LLM, prompted with the available geometric tool examples of the Euclidea dataset.The second set of agents, which we call validators, is responsible for assessing the proposed rationales and geometric tool steps, thus introducing a new layer of roles.Like solvers, validators receive domain-specific prompts, distinguishing them as natural language or geometric tool agents.However, unlike solvers who use the adaptive-shot mechanism for their fewshot examples, validators are prompted with propositions from Euclid's Elements and a static collection of incorrect examples alongside their rectifications.Depending on their domain, these are expressed in natural language or geometric tool steps.Each validator is tasked to provide feedback to its assigned solver through rounds of dialogue.We limit the number of interactions to up to five rounds for our experiments.During each round, the validator either accepts the solver's steps or suggests changes, prompting the solver to incorporate them into their solution.</p>
<p>Enhancing Spatial Awareness with the Visual Relations Prompt</p>
<p>Building upon the collaborative dynamic between solvers and validators, we recognize a critical limitation in their capability to conceptualize spatial relationships in geometric problems.This limitation manifests in several ways, notably through geometrically implausible actions such as attempting to connect non-aligned points with a straight line or assuming unproven relationships between objects.</p>
<p>To address this, we introduce a method incorporating an auxiliary Vision-Language Large Model (VLLM) to augment scene comprehension while operating within the language domain.Our approach uses the VLLM not as the primary reasoning tool but as a scene analysis instrument.We opt for GPT4-V, given its superior performance and ease of use.</p>
<p>The process begins by feeding the VLLM with an imageproblem pair, prompting it to comprehensively describe the scene's geometric elements, their interrelations, and spatial orientations.This description, which we will refer to as the Visual Relations Prompt (VRP), is added to each agent's prompt.Through this integration, we disentangle the process of spatial recognition from the challenging geometric problem-solving that the agents must perform concurrently.The proposed VRP is also cost-efficient, as the extraction is required only once per problem and can be shared across all agents concurrently, significantly reducing the need for recurrent interactions with visual extractors.Furthermore, it is also quite flexible, enabling models that lack innate visual capabilities to utilize the VRP, enhancing their decisionmaking abilities.For a comprehensive visual representation of how the VRP functions within our framework, please refer to the Appendix, Section G.</p>
<p>Mitigating Naming Biases</p>
<p>LLMs have been studied for adopting social biases found among humans (Wallace et al., 2019;Liang et al., 2021), or being negatively affected by language bias in their reasoning process (Lin et al., 2020;Mouselinos et al., 2023).We observe a similar bias arising from the terminology used for geometric entities.For instance, when tasked with constructing a target named 'E' in contexts with existing objects' A,' 'B,' and 'C,' models will create an intermediate object 'D' before proceeding to 'E.'This tendency extends to choosing longer reasoning paths based on the alphabetical position of the target variable, leading to unnecessary complexity and incorrect solutions.Likewise, choosing a target variable earlier in the alphabetical sequence than the required minimum steps to solve the problem can introduce faulty rationales (e.g., choosing 'C' as the target of a five-step solution can lead to early stopping on an intermediate generated 'C' point, abruptly ending the construction).</p>
<p>Thus, we propose a straightforward yet effective strategy to address this issue: substituting the target variable with 'X,' a universal symbol for unknowns in mathematics.This change encourages models to seek the most direct solutions, reducing the influence of alphabetical ordering.Our method proves particularly beneficial in the validation stage of our multi-agent system, where it effectively minimizes the misjudgment of correct solutions and the consequent propagation of errors.An example of such behavior can be seen in the dialogue presented in Appendix Section H.</p>
<p>Experiments</p>
<p>We present a comprehensive summary of our tested models' performance on the Euclidea dataset in Table 1.Our results are composed of three different testing setups:</p>
<p>In Few-shot, apart from the task, the description of the available tools and their intended use, models are also prompted with five solved examples using geometric tools in their solutions.</p>
<p>In Finetuned, we first fine-tune all open-source models using Euclid's book "Elements."In this way, we aim to provide models with the foundational knowledge and principles essential for addressing the challenges within the Euclidea dataset.It is crucial to highlight that, aside from the tutorial levels, there is no direct overlap between Euclidea challenges and the problems included in "Elements."During testing, models use the same setup as in Few-shot.</p>
<p>Simulacra refers to our proposed multi-agent framework, equipped with adaptive few-shot mechanism, visual relations prompting, and variable renaming.In our few-shot experimental setup, all models exhibit underwhelming performance, which improves modestly after fine-tuning.This aligns with our expectation that familiarity with standard mathematical and reasoning scenarios does not ensure proficiency in constructive geometry tasks.We posit that the increased performance from fine-tuning with Euclid's "Elements" represents an upper limit of improvement achievable by open-source models, constrained by the dataset's size.MetaMath-Mistral 7B is the most promising among the open-source options, which we further examine with our multi-agent setup.Our results reveal a significant performance boost in larger models (ChatGPT / GPT4) and, notably, in MetaMath-Mistral 7B, under our proposed framework, surpassing the few-shot ChatGPT in performance.This finding underscores the adaptability and effectiveness of our approach across a spectrum of model sizes.</p>
<p>Moreover, we compare against two prior studies on the Euclidea Dataset.The model proposed by (Macke et al., 2021) combines a Masked-RCNN detector with an iterative deep search algorithm.It used the Euclidea API to verify each proposed step, continuing until a solution is found within a pre-defined depth limit.</p>
<p>Euclid-Net (Wong et al., 2022) employs a similar method named "Deep visual reasoning with backtracking," where a neural network assists in step selection, exploring solutions in a tree-like structure.A notable difference between these models and ours is their dependence on a precise visual component extractor and real-time, ground truth feedback during their trial-and-error process.To assess the possible benefits of real-time feedback, we introduce Feedback mode (+FB).Validators now have access to ground truth answers and are restricted to merely informing solvers about the correctness of their suggested steps.In this way, there is no solution leak to the solvers.Our findings reveal only a slight performance improvement, indicating that solvers struggle to develop more complex solutions or alternate reasoning paths even with ground truth hints.This underscores the nuanced complexities of applying LLMs to constructive geometry.</p>
<p>Method pass@1 pass@50 Few-Shot LlamaV2 (7B) 3.1 (± 0.5) 4.4 (± 0.7) LlamaV2 (13B) 4.4 (± 0.4) 7.5 (± 0.8) MetaMath (13B) 4.7 (± 0.4) 8.1 (± 0.8) Mistral (7B)</p>
<p>5.1 (± 0.6) 8.7 (± 1.1) Zephyr-α (7B) 5.3 (± 0.5) 8.9 (± 1.1) MetaMath-Mistral (7B) 8.9 (± 0.7) 13.4 (± 1.2) ChatGPT 11.7 (± 1.1) 18.6 (± 1.5) GPT4</p>
<p>21.2 (± 1.3) 38.3 (± 1.4) Finetuned LlamaV2 (7B) 3.7 (± 0.6) 5.1 (± 0.7) LlamaV2 (13B) 4.9 (± 0.4) 8.7 (± 0.8) MetaMath (13B) 5.3 (± 0.5) 9.2 (± 1.0) Mistral (7B) 6.9 (± 0.7) 9.7 (± 1.1) Zephyr-α(7B) 6.6 (± 0.6) 9.5 (± 1.2) MetaMath-Mistral(7B) 9.4 (± 0.9) 16.LOO refers to the "Leave-One-Out" method: For each given level in the Euclidea challenge, the model is either trained on the rest levels in the same pack (LOO-level) or the rest of packs in the dataset (LOO-packs).</p>
<p>Ablation Studies</p>
<p>Overcoming hallucinations and context overdependence</p>
<p>In our exploration of LLMs' proficiency in geometric problem-solving, we initially used models in a zero-shot setup, with the only context being the tool definitions.This yielded subpar results, with models hallucinating the functionalities of the tools.Consequently, we shifted to a fewshot approach, integrating examples of solved problems into our prompts.This noticeably reduced hallucinations, but models seemed to over-rely on the provided examples, often replicating sequences of steps in their solutions.</p>
<p>We hypothesized that this behavior stems from the instruction-tuning training process: Many LLMs are finetuned to follow instructions closely, primarily replicating styles and patterns.In our case, models were not merely mimicking styles but inadvertently replicating entire reasoning processes.Instruction-following proved counterproductive here, especially since many geometric problems share common preliminary steps (e.g., constructing bisectors or circles of equal radius).Models frequently repeated specific groups of steps, irrespective of their relevance to the problem, leading to redundant and incorrect constructions.This is further illustrated in Table 2, where we evaluated the model's performance in solving geometric problems from three distinct difficulty levels: Tutorial, Alpha, and Beta.Interestingly, unlike the simpler Alpha and Tutorial levels, the model exhibited improved performance when prompted with higher difficulty (Beta) examples.This observation contradicts typical few-shot prompts in algorithmic or mathematical problem-solving, which include straightforward, demonstrative examples mirroring the dataset's style and expected logic.</p>
<p>Recognizing these limitations, we repeated our experiments using our proposed adaptive few-shot approach.Apart from Adaptive-Shot (Self), we also tested with a simpler variant, Adaptive-Shot (ST), where the Sentence Transformer, rather than the model, determined the selection of few-shot examples after the filtering stage.Our findings demonstrate that adaptive variations outperform the static approach, corroborating our hypothesis.The Adaptive-Shot (Self) method proved slightly more effective, albeit with the trade-off of necessitating more API calls than the Adaptive-Shot (ST), particularly in closed-source models.The method is also limited by the capacity of the model to identify what prompts would be more useful for the task, which we hypothesized to surpass the contextual abilities of an auxiliary sentence transformer.</p>
<p>Method pass@1 pass@50 Zero-Shot 5.9 (± 1.9) 9.6 (± 2.7) Few-Shot (Tutorial) 7.2 (± 1.4) 15.9 (± 3.1) Few-Shot (Alpha) 11.4 (± 1.8) 18.6 (± 3.2) Few-Shot (Beta) 12.7 (± 2.2) 20.8 (± 3.6) Adaptive-Shot (ST) 13.3 (± 1.8) 21.2 (± 3.0) Adaptive-Shot (Self) 13.6 (± 1.8) 21.5 (± 2.7) Table 2. Effectiveness of in-context example methods.Experiments with ChatGPT 3.5-Turbo on the Alpha and Beta levels.</p>
<p>Effectiveness of domain and role division</p>
<p>Interestingly, LLMs proved quite successful when prompted to merely generate ideas -and not specific steps using geometric tools.Although they made convenient assumptions, their overall reasoning was still accurate.This led us to question the root cause of the discrepancy between knowing the solution in the form of a plan expressed in natural language and failing to execute it accurately using strict and abstract geometric tools.</p>
<p>We decided to test whether we can instead benefit from the synergy of those worlds: an LLM instance could focus on generating ideas in the natural language domain, and another could specialize in transforming these ideas into a series of geometric tool steps.By employing such a multi-agent setup, as seen in Table 3, we observed a significant performance gain -13.6 → 21.5 pass@1 -when comparing a single agent (S GT ) operating directly with geometric tools against a duo of collaborative agents with differentiated domains (S N L − S GT ).However, we noticed that the chaining of agents could sometimes result in the propagation of errors.For instance, a natural language solver producing a flawed rationale would inevitably pass these inaccuracies on to the geometric tool solver.To counter this, we experimented with the introduction of a set of validators.These agents, prompted under a different role and context, helped to minimize errors both during the transition between domains and by preventing the improper application of tools, leading to a further performance boost of 22.2 → 28.1 pass@1 in our proposed multi-agent configuration.</p>
<p>Configuration</p>
<p>Agents Domains pass@1 pass@50 S GT 1 1 13.6 (± 1.8) 21.5 (± 2.7) SV GT 2 1 17.9 (± 1.2) 34.9 (± 2.3)
S N L − S GT 2 2 22.2 (± 1.3) 46.7 (± 2.0) SV N L − SV GT 4 2 28.1 (± 1.1) 53.5 (± 2.4)
Table 3. Effectiveness of different agent configurations: Experiments with ChatGPT 3.5-Turbo on Alpha and Beta levels.S refers Solver, V refers to Validator, NL to Natural Language and GT to geometric tool domain.</p>
<p>Visual Aids in Spatial Reasoning</p>
<p>Another noteworthy finding was that LLMs would often create new geometric objects without acknowledging their overlap with existing ones.Moreover, they occasionally suggested steps that violated geometric rules or led to repetitive movements.We hypothesized that LLMs' difficulty with geometric reasoning in 2D spaces stems from a lack of exposure to such setups, typically operating in a unidimensional, left-to-right manner.This raised the question: Could introducing a visual signal bridge this reasoning gap?</p>
<p>To test this, we used GPT4-V, a multimodal variant of GPT4 known for its proficiency in visual question answering.We began with simple tests, prompting GPT4-V to identify freehand sketches of geometric objects.The model identified these features successfully, including subtle aspects like right angles indicated by small corner squares.We then proceeded to a more complex scenario: We presented GPT4-V with an image/problem pair and asked for the first solution step.We deliberately performed an incorrect step and found that GPT4-V often validated these errors, suggesting a disconnect between scene understanding and geometric reasoning.We present these findings in Table 4. Comparing GPT4 with its multimodal variant, GPT4-V + Image revealed a marginal improvement, suggesting that a visual signal could only assist the initial scene understanding without significantly improving spatial arrangement in later stages.</p>
<p>Building on these insights, we compare our proposed VRP method (VRP-GPT4) to the multimodal approach (GPT4-V + Image).Results demonstrated a slight performance boost with VRP, with its real advantage lying, however, in enhancing the capabilities of non-visually capable models like the tested ChatGPT and multi-agent configurations, which benefited greatly.Domain Method pass@1 pass@50 Language ChatGPT 13.6 (± 1.8) 21.5 (± 2.7) GPT4</p>
<p>23.9 (± 0.9) 44.8 (± 1.6) ChatGPT ⋆</p>
<p>28.1 (± 1.2) 53.5 (± 1.7) GPT4 ⋆ 33.7 (± 1.0) 62.0 (± 1.3)
Multimodal GPT4-V + Image 24.2 (± 1.4) 45.1 (± 1.7) VRP-ChatGPT 19.4 (± 1.1) 37.1 (± 1.3) VRP-ChatGPT ⋆ 34.5 (± 0.8) 59.2 (± 1.2) VRP-GPT4
25.3 (± 0.9) 48.6 (± 1.4) VRP-GPT4 ⋆ 38.8 (± 0.9) 64.6 (± 1.2)</p>
<p>Impact of Geometry Nomenclature on LLMs</p>
<p>LLMs mirror the human convention of alphabetical naming in mathematical contexts.The choice of target variables later in the alphabetical order leads to longer and more inaccurate solutions.This is a byproduct of their training on human-generated texts, where entities in algebraic or geometric contexts typically adhere to an alphabetical naming convention -labeling a triangle as ABC rather than EOA.</p>
<p>In the same fashion, variables associated with the assignment of solutions, like X, would theoretically condition the LLM to find its value, possibly assigning it to any given constructed object.To empirically validate this hypothesis, we designed an experiment with 20 geometric problems, each requiring 3 to 5 solution steps.In these problems, we manipulated the target variable in three distinct ways: maintaining the original name (+0), replacing it with the letter X (+X), and renaming it using a letter 1, 2, or 3 positions further in the alphabetical sequence (+1, +2, +3).As depicted in Table 5, models perform worse when the target variable is shifted by one or two letters in the alphabet.Interestingly, this tendency diminishes when the target is more than three letters away and is further reduced with the substitution of 'X'.This underscores the effectiveness of using 'X' as a neutral, bias-mitigating variable in our multi-agent framework.48.1 (± 2.3) 46.3 (± 2.1) 47.5 (± 1.9) GPT4 ⋆ 52.9 (± 2.1) 50.4 (± 2.2) 50.7 (± 2.7) VRP-ChatGPT ⋆ 60.7 (± 2.9) 57.9 (± 2.5) 57.9 (± 3.0) VRP-GPT4 ⋆ 65.1 (± 2.2) 63.4 (± 3.2) 63.6 (± 2.9) +3 +X ChatGPT 10.6 (± 1.2) 10.5 (± 1.7) GPT4 32.4 (± 1.1) 37.5 (± 1.3) ChatGPT ⋆ 47.9 (± 1.7) 48.6 (± 1.9) GPT4 ⋆ 51.8 (± 1.4) 53.3 (± 1.5) VRP-ChatGPT ⋆ 59.2 (± 2.3) 61.1 (± 2.2) VRP-GPT4 ⋆ 64.7 (± 2.1) 66.2 (± 2.4)</p>
<p>Table 5. Qualitative results on the effect of variable renaming.Results refer to the pass@50 metric.The star symbol (⋆) refers to SVNL − SVGT configuration.</p>
<p>Generalisation to different datasets</p>
<p>To expand the scope of our research and examine the potential generalizability of our multi-agent framework, we tested its performance against three datasets involving mathematical reasoning: GSM8K, SVAMP, and the Geometry split from the MATH dataset.We were inspired by the findings of (Zhao et al., 2023), who identified diverse reasoning patterns when LLMs are engaged in problem-solving using Chain of Thought (COT) (Wei et al., 2022) versus Program-Aid (PAL) (Gao et al., 2023) methodologies.While COT is recognized for its creativity and flexibility in devising solutions, PAL is noted for its enhanced accuracy in numerical computations.This differentiation of roles was an ideal fit for our framework: A pair of Solver-Validator agents (SV COT ) could initially converse about solving a math problem using a chain of thoughts approach.Then, the produced rationale would be passed down to another pair of agents (SV P AL ), specialized in transforming the proposed steps into Python code solutions that can be computed precisely.We opted not to employ a Python interpreter as the second validator (V P AL ), similar to what code-based verification methods like (Zhou et al., 2023a) do.Instead, we introduced another LLM agent in this role, thereby maintaining consistency in our approach of utilizing LLMs across different stages of problem-solving.Finally, we used the Adaptive Few-Shot mechanism using examples from each dataset's training split.Our results, presented in Table 6, show that our multiagent setup can perform very close to current state-of-the-art methods despite not being primarily designed for these tasks.This is also true for the MetaMath-Mistral model, which also enjoys a significant performance boost using our proposed method, even in this non-geometric setup.Table 6.Results of the multi-agent framework on mathematical datasets.N/A refers to not reported results.</p>
<p>Conclusions</p>
<p>Our study highlights the intrinsic challenges that LLMs face in constructive geometry.We observe a limited skill transfer from other mathematical domains, insufficiencies of typical prompting techniques, and a lack of 2D spatial reasoning.We thus identify that the existing LLMs are inferior in achieving very competitive performance on geometric tasks unless we reinforce the system with strong inductive biases such as a theorem prover (Trinh et al., 2024).On the other hand, we also show that our multi-agent system with role specializations and interactions can substantially boost the performance of the whole system, and tackle geometric problems more effectively.That system has some properties of the conceptual integration networks (Fauconnier &amp; Turner, 1998) where the "blending" happens if the system is considered as a whole.</p>
<p>We hope that our work provides a starting point for a longerterm view where the system is explicitly trained to interact in a multi-agent setting.Though challenging, this paradigm shift from merely enhancing existing models to creating a new generation of LLMs will eventually be required for significant breakthroughs in domains where deep, specific, and accurate cognitive processing is crucial.</p>
<p>Limitations</p>
<p>In our study, we specifically focus on constructive geometry, a distinct subset within the broader field of geometry.While numerous datasets address geometric problems, they predominantly focus on Vision-Language Models (VLMs) and employ a question-answering (QA) format, which differs fundamentally from the nature of constructive geometry.To the best of our knowledge, the dataset used is unique in this area, and we hope to attract more attention to the domain.We also recognize that our multi-agent setup might not be universally applicable, particularly for general-purpose datasets and solutions.Our approach could be less effective and slower than few-shot methods in scenarios where distinct modalities or roles are unnecessary.</p>
<p>Furthermore, it is crucial to consider the significant costs associated with experiments involving closed-source LLMs behind APIs.The extensive communication rounds and large context sizes required by our method can lead to substantial expenses.In cases where cost is a concern, a more straightforward, single-agent approach may be more viable and cost-effective.</p>
<p>Risks and Ethical Considerations</p>
<p>We do not identify any risks or ethical considerations associated with our proposed ideas and suggested methods.</p>
<p>B. Information on Experimental Setup</p>
<p>Our experimental setup consisted of 1x NVIDIA A100 GPU.Regarding the fine-tuning results on Euclid's Elements of Table 1, we trained all LLMs using the bitsandbytes library (https://github.com/TimDettmers/bitsandbytes)and 4-bit quantization with the QLoRA technique (Dettmers et al., 2023).Below, the reader can find the full hyperparameter list:  Regarding the training objective, we used the typical causal language modeling loss.Moreover, we used a validation split of 10% sampled uniformly across different chapters of the book.We monitored the validation loss every 500 steps as our metric for early stopping.</p>
<p>Regarding the API calls to OpenAI models, gpt3.5-turbo-16k was used for the ChatGPT experiments, gpt4-32k was used for the GPT4 experiments, and for the extraction of Visual Relation Prompts, The endpoint called gpt4-vision-preview was used.All of our API calls were subject to throttling limits, and waiting loops were introduced to avoid interruptions of service.We conducted most of our ablation studies and early experiments with ChatGPT to avoid massive waiting times and reduce the high experimental cost.The total experiment time was approximately 500 hours, and our total costs were around 2000 USD.</p>
<p>D. Static Versus Adaptive Few-Shot</p>
<p>In this section, we will provide an analytical overview of our adaptive few-shot mechanism.As seen in Figure 2, we begin by collecting a set of previously solved problems alongside their solutions, which call our knowledge base.This set can be acquired in multiple ways: In the case of our GSM8K / SVAMP / Geometry-Math experiments of Table 6, we use all problems belonging to the provided training split.In the case of Euclidea experiments of Table 1, we begin with a small set of five handcrafted geometric challenges.Euclidea's problems vary in difficulty and are grouped into increasingly difficult level-packs.Since we are not provided with a training split, we add all problems belonging to previously seen packs to our knowledge base during the solution of a level.In this way, we continuously increase the size of our base as well as the availability of more diverse and complex techniques that our agents can utilize.</p>
<p>The second stage involves the use of a Sentence Transformer to reduce the size of our knowledge base.For this, we compare the cosine similarity scores of our current problem and the problems in our knowledge base and keep entries with scores over 0.5 or the top fifteen, whichever leads to less amount of examples.</p>
<p>The third and final stage involves the use of the Sentence Transformer again, which chooses the top five most similar examples to build the final few-shot prompt.This procedure is referred to as Adaptive-Shot (ST) in our experiments.An alternative to this is to use the examples from the second step and prompt our solver-LLM to filter the top five most relevant examples by itself.We call this alternative Adaptive-Shot (Self).Note here that the second step of the procedure can be skipped during Adaptive-Shot (ST) since instead of filtering, we can directly return the top five results.However, it is necessary in the Adaptive-Shot (Self) method, where we need to respect the context size limits of the tested LLMs.</p>
<p>The following example presents three different few-shot prompts: A static-handcrafted version, an adaptive prompt compiled by a Sentence Transformer (Adaptive ST), and finally, an adaptive prompt filtered by the model itself (Adaptive Self).Here, our model is ChatGPT, and the problem at hand is:</p>
<p>E. Target Variable Name Bias</p>
<p>Below, we present a qualitative example of the effect of the target variable name on GPT4.The problem we present is: "Find a point that is equidistant from given points A and B.".We present three illustrations, each signifying a different reasoning path to its solution, according to the name given to the target variable., where E lies on line CD, the perpendicular bisector of line AB.However, an interesting case can be observed with the letter X, usually associated with a missing variable.Here, the model returns to the shorter 3-step reasoning and gives X multiple possible solutions.X is assigned to points rather than being explicitly constructed as the result of a tool.</p>
<p>F. Baselines</p>
<p>We introduce two baselines to analyze the complexity of the geometric construction problem.In the first baseline, which we will call "Longest Common Solution" (LCS), we identify the top five longest common sequences of steps between ground truth solutions.Then, for each given problem, we uniformly sample from these sequences, adjusting the variables of each step and the tool usage to the current task.If a sampled step does not apply to the current state of the problem, the sampled sequence is discarded, and a new one is sampled instead.This baseline corresponds to the success rate of an agent who memorized a set of fixed solution steps and applied them to each given problem.</p>
<p>Additionally, we propose a second baseline named 'N-Gram Rollouts' (N-Gram), which begins by creating a database of uni-, bi-, and tri-grams derived from the tools used in ground-truth answers.For each problem, our method involves a two-phase iterative process.Initially, we choose either a single tool (n = 1) or a sequence of tools (n &gt; 1) from our database.Following this, we select the geometric variables upon which these tools will be applied.To facilitate this, we maintain a memory initially populated with variables given in the problem statement.It's important to note that tool application varies in complexity, with some tools requiring a single variable (e.g., constructing a ray from point A) and others necessitating two (e.g., drawing a line between points A and B).For each tool or sequence of tools selected, we sample the required number of variables from memory, with recent variables weighted more heavily than older ones, following an exponential decay schema.Any new variables a tool generates (such as a new point) are added to this memory.This process is repeated until a predefined number of steps is reached.</p>
<p>Method Correct tool sequence Fully correct solution LCS 0.17 0.02 1-Gram 0.14 0.11 2-Gram 0.12 0.09 3-Gram 0.07 0.06</p>
<p>Table 11.Performance of Baselines.Results refer to pass@1 estimated after 1000 trials.</p>
<p>According to Table 11, finding an entirely correct solution randomly is almost 0.1% probable.Note here that finding the correct tool sequence score is significantly higher.However, this is expected since many problems share common starting steps (e.g., drawing a line or a circle).</p>
<p>Figure 2 .
2
Figure 2. The adaptive few-shot mechanism.When facing the problem Construct a 30-degree angle given a ray, we proceed to filter our knowledge base (top-right) and then either return the top five most similar results to build our prompt -Adaptive-Shot (ST) or prompt the LLM to filter them out by itself -Adaptive-Shot (Self).</p>
<p>Figure 3 .
3
Figure 3. Visualized GPT4 reasoning paths depending on the name of the target variable.Top left: T arget = C, Top right: T arget = D, Bottom left: T arget = E and Bottom right: T arget = X</p>
<p>Table 1 .
1
Results on Euclidea dataset.Ours refers to the SNL−SGT setup employing VRP, variable renaming and Adaptive Shot (Self).
2 (± 1.3)OursMetaMath-Mistral (7B)14.9 (± 1.3) 21.1 (± 1.7)ChatGPT32.3 (± 1.3) 61.4 (± 2.1)GPT438.9 (± 1.1) 67.7 (± 2.2)FB + MetaMath-Mistral (7B)15.1 (± 1.5) 21.4 (± 1.6)FB + ChatGPT35.6 (± 1.7) 63.5 (± 2.2)FB + GPT441.2 (± 1.6) 71.2 (± 2.0)Supervised Visual Component + Exhaustive SearchMethodAccuracy(Macke et al., 2021) (LOO-levels)44.1(Macke et al., 2021) (LOO-packs)45.5(Wong et al., 2022) Euclid-Net75.5</p>
<p>Table 4 .
4
Multimodal prompt effectiveness.Experiments on the Alpha and Beta pack levels.The (⋆) symbol refers to an SVNL − SVGT multi-agent configuration.</p>
<p>Table 8 .
8
URL and Licenses of used Datasets.</p>
<p>Table 9 .
9
Hyperparameters and their values</p>
<p>AcknowledgementsAll experiments were performed using the Entropy cluster funded by NVIDIA, Intel, the Polish National Science Center grant UMO-2017/26/E/ST6/00622 and ERC Starting Grant TOTAL.The work of Spyridon Mouselinos and Henryk Michalewski was supported by the Polish National Science Center grant UMO-2018/29/B/ST6/02959.A. Information on Models and DatasetsModel NameLink LICENSEC. Tools and problems from Euclidea datasetThere are 10 tools available in Euclidea, although not all of them can be used on every level.Tools become available progressively as the difficulty increases.Here is a list of all available tools:1. Move Tool: Moves a geometric object.2. Point Tool: Marks a point and labels it.3. Line Tool: Draws a line between two points or a ray from a starting point.4. Circle Tool: Constructs a circle using a specific point as the center.5. Perpendicular Bisector Tool: Creates the perpendicular bisector of a segment between two points.6. Perpendicular Tool: Draws a line perpendicular to a given one at a specific point.7. Angle Bisector Tool: Creates a line that bisects a given angle.The line originates at the vertex of the angle.8. Parallel Tool: Draws a line parallel to a given line or segment.9. Compass Tool: Uses a compass to construct a circle with a radius equal to a given segment.10. Intersect Tool: Marks the intersection between two geometric objects.Here are some sample problems from different difficulty levels:Problem: Rhombus in Rectangle -Pack: AlphaGiven the rectangle ABCD with AB &gt; AD.Inscribe a rhombus in the rectangle so that they share a diagonal.Available Tools: [Line Tool, Circle Tool, Perpendicular Bisector Tool, Intersect Tool] Solution: Perpendicular Bisector Tool: Construct the perpendicular bisector of AC, intersecting AB at E and CD at F. Line Tool: Construct line AF.Line Tool: Construct line CE.Problem: Lozenge -Pack: GammaGiven the side AB.Construct a rhombus with the given side and an angle of 45°in a vertex.Available Tools: [Line Tool, Circle Tool, Perpendicular Bisector Tool, Perpendicular Tool, Intersect Tool, Angle Bisector Tool] Solution: Perpendicular Tool: Construct the perpendicular to AB from A; let C be a distinct arbitrary point on that perpendicular.Angle Bisector Tool: Construct the angle bisector of BAC.Circle Tool: Construct the circle with center A and radius AB, intersecting the angle bisector at D and line AC at E with E on the side of A opposite from C. Perpendicular Tool: Construct the perpendicular to AC through D. Line Tool: Construct line BE.Problem: Angle of 54 -Pack: ThetaLet A be the vertex.Construct two rays that divide the given angle of 54 degrees into three equal parts.We observe that the static prompt leads to an incorrect solution, with the model misinterpreting the meaning of a circumcircle, and blindly performing step sequences similar to those presented in the prompt.In the case of Adaptive ST, the prompt consists of examples more closely related to the task at hand, leading to a correct solution.However, the model still suggests unnecessary steps, which can be attributed to style-mimicking tendencies introduced by instruction tuning.Finally, when the model is tasked to filter the examples by itself (Adaptive Self), we observe an overlap in the selected examples with the Sentence Transformer method.Here, the model suggests a correct sequence of steps, free from any unnecessary or repetitive tool usage.ProblemGiven two points, A and B, find a point [Target] so that its distance from A is equal to its distance from B.Available In this case, there will be two solutions for X, each lying on the circumference of the circles where they intersect.Table10.The completions of the model for the above problem, in geometric tool language.G. Visual Relations PromptIn Figure4, we provide a visualization of our proposed visual relations prompt: We begin by employing an auxiliary prompt that contains the problem question and an image depicting the initial state of the problem.This is presented to GPT4-V, where a list of Points, Lines, Objects, and their Relations is returned in the form of bullet points.This information, which we refer to as VRP, is then added to the overall agent prompt between the few-shot examples and the current problem and available tool description.Here are two examples presenting the benefits of VRP inclusion.Example 1:Problem Given the rectangle, ABCD, and the point E located over the side AB at a random position, construct a line through E that cuts the rectangle into two parts of equal area.When tasked to tackle the same geometrical problem, our three agents (MetaMath-Mistral / ChatGPT / GPT4) resort to a "convenient" placement of point E outside rectangle ABCD.All of them try to utilize the fact that the perpendicular bisector of any edge of a rectangle cuts the rectangle in two equal areas.Then a series of erroneous steps are suggested, trying to connect point E with this line.After the introduction of the VRP, we observe that all agents manage to correct their responses.The placement of geometric objects in the prompt seems to assist them in understanding the nature of the problem better.Agents are no longer resorting to steps that simply suggest splitting the rectangle area into two equal parts, ignoring point E, but rather respect the special spatial circumstances the problem presents.Note here that the initial solutions are valid if the random position of point E happens to align with the midpoint of any side.The initial tendency to produce such solutions can be attributed to the fact that the splitting of a rectangle must have been observed more frequently in their training data through the use of diagonals or lines connecting midpoints.H. Multi-Agent Dialogue ExamplesIllustration of our proposed SV N L − SV GT multi-agent setup.We present the agent dialogue leading to a successful solution.The use of a single ChatGPT instance is not sufficient to solve this particular problem.However, the combined power of multiple agents can.According to our proposed method, we employ 4 agents:1. Natural Language Solver (NL Solver) Below, we present another example where we showcase the benefits of incorporating variable renaming in multi-agent scenarios.Simulacra are not immune to the variable name bias: In the example below, the variables A, B, and C are used.A and B mark the two initial points of the problem, while C stands as the target to be found.Initially, the NL Solver comes up with a promising 5-step rationale.However, the NL Validator corrects the proposed final step as a mistake.Since A,B, and C are consecutive letters in the alphabet, and C has already appeared in the fourth step, the validator marks the final step as redundant.The new variable D generated at the fifth step is after C (in terms of alphabetical ordering), and thus the problem must have been already solved.This error is propagated to the GT agents, leading to an eventually wrong final response.ProblemLet |AB| = 1.Construct a point C on the line AB such that the length of AC is equal to √ 2.Do not use arbitrary numbers but only existing lengths and sizes in your solution Available Tools[Line Tool, Circle Tool, Perpendicular Bisector Tool, Perpendicular Tool, Angle Bisector Tool, Intersect Tool]NL Solver[...]To create a line segment whose length is the square root of 2, we should utilize an existing segment of length 1, here AB.This requires forming a right-angled triangle where one side measures 1, and determining the hypotenuse's length which will be the square root of 2. I suggest the following steps: <STEP>1: Draw a straight line segment AB of any length.This will serve as our unit length.Now, let us rename the target variable and present the problem to our multi-agent setup again.Instead of the consecutive letter C, we swap the final target with a far-away (in terms of alphabetical ordering) as well as a common mathematical symbol X.We observe that the validator now approves the suggested rationale, leading to a successful generation.Target Variable RenamingLet |AB| = 1.Construct a point X on the line AB such that the length of AX is equal to √ 2. Do not use arbitrary numbers but only existing lengths and sizes in your solution.NL Solver[...]Based on the problem description, we are tasked with constructing a point on a given line segment such that the length of the segment from the initial point to the new point equals the square root of 2 without using explicit numerical values.Here's how we can approach this: <STEP>1: Draw a straight line segment AB of any length (this will be our unit length as we are not to use numbers).<STEP>2: Construct a circle with center at A and radius AB.
Language models as agent models. J Andreas, doi: 10.18653Findings of the Association for Computational Linguistics: EMNLP 2022. Y Goldberg, Z Kozareva, Y Zhang, Abu DhabiAssociation for Computational LinguisticsDecember 2022United Arab Emirates</p>
<p>URL. </p>
<p>The hanabi challenge: A new frontier for ai research. N Bard, J N Foerster, S Chandar, N Burch, M Lanctot, H F Song, E Parisotto, V Dumoulin, S Moitra, E Hughes, I Dunning, S Mourad, H Larochelle, M G Bellemare, M Bowling, 10.1016/j.artint.2019.103216.URLhttps://www.sciencedirect.com/science/article/pii/S0004370219300116Artificial Intelligence. 0004-37022801032162020</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, CoRR, abs/2005.141652020</p>
<p>Autoagents: A framework for automatic agent generation. G Chen, S Dong, Y Shu, G Zhang, J Sesay, B F Karlsson, J Fu, Y Shi, 2023</p>
<p>Unifying geometry logical reasoning via reformulating mathematical expression. J Chen, T Li, J Qin, P Lu, L Lin, C Chen, X Liang, Unigeo, The 2022 Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, A Dafoe, E Hughes, Y Bachrach, T Collins, K R Mckee, J Z Leibo, K Larson, T Graepel, arXiv:2110.14168Training verifiers to solve math word problems. 2021. 2020arXiv preprintOpen problems in cooperative ai</p>
<p>T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Learning correlated communication topology in multi-agent reinforcement learning. Y Du, B Liu, V Moens, Z Liu, Z Ren, J Wang, X Chen, H Zhang, Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '21. the 20th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '21Richland, SC2021International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450383073</p>
<p>. Euclidea, </p>
<p>Conceptual integration networks. G Fauconnier, M Turner, Cognitive science. 2221998</p>
<p>Euclid's Elements of Geometry. R Fitzpatrick, I.L. Heiberg, Ed. &amp; Trans.2007-2008. URL ˆ1ˆ</p>
<p>Pal: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningGoogle. Bard2023. 2023</p>
<p>Reasoning with language model is planning with world model. S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, ArXiv, abs/2305.149922023258865812</p>
<p>Emergence of language with multi-agent games: Learning to communicate with sequence of symbols. S Havrylov, I Titov, date: 24-04-2017 Through 26-04-20175th International Conference on Learning Representations (ICLR 17, workshop track). April 2017. 2017International Conference on Learning Representations</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021NeurIPS</p>
<p>S Hong, M Zhuge, J Chen, X Zheng, Y Cheng, C Zhang, J Wang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, J Schmidhuber, Metagpt, Meta programming for a multi-agent collaborative framework. 2023</p>
<p>A simple language model for taskoriented dialogue. E Hosseini-Asl, B Mccann, C.-S Wu, S Yavuz, R Socher, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, N Brown, T Jackson, L Luu, S Levine, K Hausman, B Ichter, 2022</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, Mistral 7b. 2023</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, ArXiv, abs/2205.119162022249017743</p>
<p>Camel: Communicative agents for "mind" exploration of large language model society. S Kulal, P Pasupat, K Chandra, M Lee, O Padon, A Aiken, P S Liang, Spoc, Thirtyseventh Conference on Neural Information Processing Systems. G Li, H A A K Hammoud, H Itani, D Khizbullin, B Ghanem, Curran Associates, Inc2019. 202332Advances in Neural Information Processing Systems</p>
<p>Towards understanding and mitigating social biases in language models. P P Liang, C Wu, L.-P Morency, R Salakhutdinov, International Conference on Machine Learning. PMLR2021</p>
<p>Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models. B Y Lin, S Lee, R Khanna, X Ren, doi: 10.18653Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 2020</p>
<p>URL. </p>
<p>Agentsims: An open-source sandbox for large language model evaluation. J Lin, H Zhao, A Zhang, Y Wu, H Ping, Q Chen, 2023</p>
<p>Cheap talk discovery and utilization in multi-agent reinforcement learning. Y L Lo, C S De Witt, S Sokota, J N Foerster, S Whiteson, 2023</p>
<p>Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. R Lowe, Y Wu, A Tamar, J Harb, O Pieter Abbeel, I Mordatch, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, Garnett ; Lu, P Gong, R Jiang, S Qiu, L Huang, S Liang, X Zhu, S.-C , doi: 10.18653Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. C Zong, F Xia, W Li, R Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline2017. August 202130Advances in Neural Information Processing Systems. Association for Computational Linguistics</p>
<p>Learning to solve geometric construction problems from images. J Macke, J Sedlar, M Olsak, J Urban, J Sivic, 2021</p>
<p>The Society of Mind. M Minsky, 1986. ISBN 0671607405Simon &amp; Schuster, Inc., USA</p>
<p>The world's largest open multilingual language model: Bloom. M Mitchell, G Pistilli, Y Jernite, E Ozoani, M Gerchick, N Rajani, S Luccioni, I Solaiman, M Masoud, S Nikpoor, C M Ferrandis, S Bekman, C Akiki, D Contractor, D Lansky, A Mcmillan-Major, T Thrush, S Ilić, G Dupont, S Longpre, M Dey, S Biderman, D Kiela, E Baylora, T L Scao, A Gokaslan, J Launay, N Muennighoff, 2022</p>
<p>Emergence of grounded compositional language in multi-agent populations. I Mordatch, P Abbeel, 10.1609/aaai.v32i1.11492Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApr. 201832</p>
<p>A simple, yet effective approach to finding biases in code generation. S Mouselinos, M Malinowski, H Michalewski, doi: 10.18653Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>URL. </p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 2023. 2022OpenAIGpt-4 technical report</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. J S Park, L Popowski, C J Cai, M R Morris, P Liang, M S Bernstein, 2022</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Are NLP models really able to solve simple math word problems?. A Patel, S Bhattamishra, N Goyal, doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>URL. </p>
<p>. Pyeuclidea, Pyeuclidea, </p>
<p>Communicative agents for software development. C Qian, X Cong, W Liu, C Yang, W Chen, Y Su, Y Dang, J Li, J Xu, D Li, Z Liu, M Sun, 2023</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. L Reynolds, K Mcdonell, 10.1145/3411763.3451760Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, CHI EA '21. New York, NY, USA2021Association for Computing Machinery. ISBN 9781450380959</p>
<p>Solving geometry problems: Combining text and diagram interpretation. M Seo, H Hajishirzi, A Farhadi, O Etzioni, C Malcolm, 10.18653/v1/D15-1171Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbonAssociation for Computational LinguisticsSeptember 2015Portugal</p>
<p>Learning structured communication for multi-agent reinforcement learning. J Sheng, X Wang, B Jin, J Yan, W Li, T.-H Chang, J Wang, H Zha, 2020</p>
<p>Eliciting Knowledge from Language Models with Automatically Generated Prompts. T Shin, Y Razeghi, I V Logan, R L Wallace, E Singh, S Autoprompt, doi: 10.18653Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>URL. </p>
<p>Learning multiagent communication with backpropagation. S Sukhbaatar, R Fergus, Studies in Logic and the Foundations of Mathematics. D Lee, M Sugiyama, U Luxburg, I Guyon, R Garnett, Elsevier2016. 195929Advances in Neural Information Processing Systems</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, Scialom, 2023T. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Solving olympiad geometry without human demonstrations. T Trinh, Y Wu, Q Le, H He, T Luong, 10.1038/s41586-023-06747-5Nature. 2024</p>
<p>Universal adversarial triggers for attacking and analyzing NLP. L Tunstall, E Beeching, N Lambert, N Rajani, K Rasul, Y Belkada, S Huang, L Von Werra, C Fourrier, N Habib, N Sarrazin, O Sanseviero, A M Rush, T Wolf, E Zephyr ; Wallace, S Feng, N Kandpal, M Gardner, S Singh, 10.18653/v1/D19-1221Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2023. November 2019Direct distillation of lm alignment</p>
<p>Z M Wang, Z Peng, H Que, J Liu, W Zhou, Y Wu, H Guo, R Gan, Z Ni, M Zhang, Z Zhang, W Ouyang, K Xu, W Chen, J Fu, J Peng, arXiv:2310.00746Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>Euclidnet: Deep visual reasoning for constructible problems in geometry. M F Wong, X Qi, C W Tan, Adv. Artif. Intell. Mach. Learn. 32022</p>
<p>Autogen: Enabling nextgen llm applications via multi-agent conversation. Q Wu, G Bansal, J Zhang, Y Wu, B Li, E Zhu, L Jiang, X Zhang, S Zhang, J Liu, A H Awadallah, R W White, D Burger, C Wang, 2023</p>
<p>Self-evaluation guided beam search for reasoning. Y Xie, K Kawaguchi, Y Zhao, X Zhao, M.-Y Kan, J He, Q Xie, 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, ArXiv, abs/2305.106012023a258762525</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>. Llamav2 (touvron, 2023. 2023</p>
<p>. ( Mistral, Jiang, 2023</p>
<p>. Zephyr-Α (tunstall, 2023. 2023</p>
<p>OpenAI -API usage of gpt-3.5-turbo-16k N/A GPT4 (OpenAI, 2023) OpenAI -API usage of gpt-4-32k N/A GPT4-V (OpenAI, 2023) OpenAI -API usage of gpt-4-vision-preview N/A Table 7. URL and Licenses of used Large Language Models / APIs. Dataset Name Link LICENSE Euclid-Elements (Fitzpatrick. ( Chatgpt, Ouyang, 2022. 2007-2008</p>
<p>Cobbe, Python Port of Euclidea (PyEuclidea). 2021. 2021. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
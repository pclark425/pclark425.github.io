<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7487 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7487</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7487</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-93a9fe0ea501e83a8f2a73ceb6a0431671bc707a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/93a9fe0ea501e83a8f2a73ceb6a0431671bc707a" target="_blank">Large language models predict human sensory judgments across six modalities</a></p>
                <p><strong>Paper Venue:</strong> Scientific Reports</p>
                <p><strong>Paper TL;DR:</strong> Surprisingly, a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality, and provides highly correlated predictions with human data irrespective of whether direct visual input is provided or purely textual descriptors.</p>
                <p><strong>Paper Abstract:</strong> Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality, and provides highly correlated predictions with human data irrespective of whether direct visual input is provided or purely textual descriptors. To study the impact of specific languages, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interaction of language and perception.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7487.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7487.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pitch similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity judgments for musical pitch (two-octave harmonic complex tones)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs were prompted to provide pairwise similarity ratings for 25 harmonic complex tones spanning two octaves; model predictions were compared to human similarity judgments using Pearson correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (also GPT-3.5, GPT-3 evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language models (OpenAI GPT family) used via text prompts; GPT-4 is a later instruction-tuned, multi-modal-capable variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Pairwise pitch similarity (psychophysical similarity ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perception (auditory pitch similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants (humans or LLM) rate similarity of pairs of harmonic complex tones (10 partials, 3 dB/oct roll-off) across 25 notes from C4 to C6 on a 0–1 scale (LLMs) or 0–6 Likert (humans), enabling construction of similarity matrices and MDS representations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation (r) between flattened upper triangles of LLM and human similarity matrices</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Inter-rater split-half reliability (IRR) r = 0.90 (95% CI [0.87, 0.92])</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4: Pearson r = 0.92 (95% CI [0.91, 0.92]); other models reported as >0.6 across domains but per-model values not fully reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot in-context numerical rating prompts (three example pairs provided in prompt); repeated 10 elicitations per pair and averaged</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors' collected dataset (online AMT behavioral experiment; pitch dataset also referenced as Marjieh et al., 2023 (41))</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Correlation reported with 95% bootstrap CI [0.91,0.92]; authors state correlations are significant (no p-value reported).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>LLM recovered known pitch phenomena (octave spike at 12 semitones) and produced MDS pitch-spiral structure; GPT-4 performance is on par with human IRR without fine-tuning. Models used the same few-shot examples across pairs; human ratings collected on 0–6 Likert then converted for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7487.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7487.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Loudness similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity judgments for loudness (pure-tone loudness confusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs provided pairwise similarity ratings for pure tones varying in loudness; predictions correlated with human confusion-derived similarity measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (evaluated alongside GPT-4 and GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT family; GPT-3.5 is an instruction-tuned transformer variant used via chat-completion API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Pairwise loudness similarity (confusion-derived similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perception (auditory loudness)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Similarity between pairs of pure tones differing in dB (8 levels, 71.1–74.6 dB) assessed via confusion matrix converted to similarity scores; LLMs give 0–1 similarity ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation (r) between LLM similarity matrix and human similarity matrix</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human similarity matrix derived from confusion data in Konibrot (1978) via Sims (2018) repository (no IRR reported in paper for this modality)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5: Pearson r = 0.89 (95% CI [0.87, 0.91]); GPT-4 among top two models but per-model r not explicitly reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot in-context numerical rating prompts (three example pairs provided); 10 elicitations per pair averaged</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Dataset from Konibrot (1978) as accessed via Sims (2018) (references (26) and (40) in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>95% bootstrap CI reported [0.87,0.91]; authors state correlations are significant (no p-value reported).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-3.5 produced high alignment with human loudness similarity; authors note GPT-4 improvement was not restricted to visual domain and that improvements may stem from richer textual training rather than multimodal training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7487.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7487.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Color similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity judgments for colors (wavelength/RGB/hex coded colors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs rated similarity of color pairs (specified as hex codes) and their similarity matrices were correlated with human direct similarity judgments, recovering color-wheel structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (also GPT-3.5, GPT-3 evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT family; GPT-4 is a later multimodal-capable variant but evaluated here via text-only hex-code prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Pairwise color similarity (direct similarity ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perception (visual color similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Human direct similarity judgments across a set of colors (original dataset: 14 wavelengths converted to hex; extended to 23 via interpolation) compared to LLM 0–1 similarity ratings for pairs of hex codes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation (r) between LLM and human similarity matrices</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human similarity dataset from Ekman (1954) (direct similarity judgments); no IRR reported for this modality in paper</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4: Pearson r = 0.89 (95% CI [0.87,0.91]); authors state correlations >0.6 for all models in this domain but only GPT-4 exact value reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot in-context numerical rating prompts with three example pairs; 10 elicitations per pair averaged. Color hex codes provided in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Ekman, 1954 (reference (25) in paper); conversion from wavelength to RGB/hex done by authors</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>95% bootstrap CI reported [0.87,0.91]; authors state correlations are significant (no p-value reported).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 recovered color-wheel structure in MDS; authors also performed a separate color-naming task to probe language effects. GPT-4's multimodal training did not lead to a notably larger improvement specifically for colors relative to other modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7487.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7487.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vocal consonant similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity judgments for vocal consonants (IPA consonant sounds)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs rated similarity of recorded consonant sounds (IPA tokens) and were compared to human similarity judgments collected online; matrix correlations and MDS revealed articulatory place structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (also GPT-3.5, GPT-3 evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT family; used via chat-completion API with IPA symbols in text prompts to elicit similarity ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Pairwise consonant similarity (vocal consonants)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perception (speech sound similarity / phonetics)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants rate similarity of pairs of recorded consonant sounds (16 recordings from IPA) on a similarity scale; LLMs receive IPA tokens and return numerical similarity ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation (r) between LLM and human similarity matrices</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Inter-rater split-half reliability (IRR) r = 0.46 (95% CI [0.36, 0.56]) measured on authors' collected vocal-consonant dataset</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4: Pearson r = 0.57 (95% CI [0.56,0.59]) — authors note GPT-4 performance is on par with or exceeds human IRR for this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot in-context numerical rating prompts (example IPA pairs and ratings provided); 10 elicitations per pair averaged</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors' collected dataset (online AMT study described in Methods)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Reported Pearson r with 95% bootstrap CI [0.56,0.59]; authors state correlations are significant (no p-value reported).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>MDS of model similarity recovered production-based articulatory structure; GPT-4 exceeded human IRR on this task (r=0.57 vs IRR 0.46). Stimuli were recordings from two speakers; authors attempted to focus participants on consonant sound rather than vowel or voice pitch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7487.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7487.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Taste similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity judgments for taste flavors (confusion-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs provided similarity ratings for flavor labels; similarity scores computed from human confusion matrices were used as the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (evaluated alongside GPT-4 and GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT family; used with few-shot text prompts mapping flavor word pairs to 0–1 similarity ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Pairwise taste/flavor similarity (confusion-derived similarity over 10 flavors)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perception (gustatory similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Human confusion matrix over 10 flavor labels converted to similarity scores; LLMs rate similarity of flavor-word pairs on a 0–1 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation (r) between LLM and human similarity matrices</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human data from Hettinger et al. (1999) via Sims (2018) repository (no IRR reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5: Pearson r = 0.54 (95% CI [0.48, 0.61]); other model-specific values not explicitly listed in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot in-context numerical rating prompts (three example flavor pairs provided); 10 elicitations per pair averaged</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Hettinger et al., 1999 (reference (28)) as accessed via Sims (2018) (reference (40))</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>95% bootstrap CI reported [0.48,0.61]; authors state correlations are significant (no p-value reported).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Taste dataset is confusion-matrix based which was converted to similarity via standard formula; LLMs produce moderate correlations with human data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7487.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7487.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Timbre similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity judgments for instrument timbre</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs rated pairwise timbre similarity for 12 instrument timbres aggregated from prior datasets; correlations with human dissimilarity judgments were computed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (evaluated alongside GPT-4 and GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT family; models receive instrument names in textual prompts and produce 0–1 similarity ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Pairwise timbre similarity (instrument timbre dissimilarity ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perception (auditory timbre)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Human dissimilarity judgments over 12 instrument timbres aggregated from multiple prior studies; LLMs provide similarity ratings for instrument name pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation (r) between LLM and human similarity/dissimilarity matrices</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human dataset aggregated from prior publications (Estin & Bitton et al., 2018 (27)); no IRR reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5: Pearson r = 0.42 (95% CI [0.40, 0.44]); other models' per-modality values not separately enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot in-context numerical rating prompts (three example instrument pairs and ratings provided); 10 elicitations per pair averaged</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Aggregated timbre datasets as assembled in Esting & Bitton et al., 2018 (reference (27))</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>95% bootstrap CI reported [0.40,0.44]; authors state correlations are significant (no p-value reported).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Timbre produced the lowest correlations among tested modalities; authors caution about interpreting LLM behavior and note modality-specific variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7487.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7487.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Color naming (forced-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forced-choice color naming with 15 basic color terms (English and Russian)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs and human participants named Munsell/WCS colors by choosing from the same 15-term list; alignment measured with adjusted Rand index (ARI) comparing cluster assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT variants used via few-shot or zero-shot chat prompts; GPT-4 used to generate 15-term lists and to perform naming in both English and Russian.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Color naming (forced-choice over 15 basic color names)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perception / language (color categorization / lexical labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>For each of 330 Munsell/WCS colors, participants/LLMs select the best-fitting color name from a fixed list of 15 terms; dominant labels per color are compared via clustering similarity (ARI).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Adjusted Rand Index (ARI) between human and LLM clustering of colors</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baselines: authors' collected online human English data (103 UK participants) compared to Lindsey & Brown lab baselines: constrained naming ARI = 0.73 (95% CI [0.65,0.74]), free naming ARI = 0.75 (95% CI [0.66,0.75]).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>English: GPT-4 ARI = 0.59 (95% CI [0.56,0.63]); GPT-3.5 ARI = 0.50 (95% CI [0.46,0.52]); GPT-3 ARI = 0.39 (95% CI [0.37,0.42]). Russian: GPT-4 ARI = 0.54 (95% CI [0.46,0.54]); GPT-3.5 ARI = 0.50 (95% CI [0.45,0.52]); GPT-3 ARI = 0.35 (95% CI [0.29,0.35]).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>GPT-4 generated a 15-term basic color list (temperature 0) then repeated forced-choice prompts with shuffled lists; 10 elicitations per color with temperature 0.7 (re-querying invalid outputs until a valid choice or 10 attempts). Prompts run in both English and Russian (translated).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors' collected Prolific dataset (103 UK, 51 Russian participants) and comparison to Lindsey & Brown (2014) dataset (reference (31))</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>95% bootstrap CIs reported for ARI values; authors note LLMs are more human-like than earlier variants but still below the lab-based human baseline (Lindsey & Brown ARI ~0.73–0.75).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 replicates cross-linguistic Russian/English differences (e.g., sinij vs goluboj separation). GPT-4 overuses 'turquoise' relative to humans for some Munsell colors. Authors emphasize differences due to online vs lab color presentation and caution LLM bias and non-determinism in outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can language models encode perceptual structure without grounding? a case study in color. <em>(Rating: 2)</em></li>
                <li>How does chatgpt rate sound semantics? <em>(Rating: 2)</em></li>
                <li>Words are all you need? capturing human sensory similarity with textual descriptors. <em>(Rating: 2)</em></li>
                <li>Musical pitch has multiple psychological geometries. <em>(Rating: 2)</em></li>
                <li>The color lexicon of american english <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7487",
    "paper_id": "paper-93a9fe0ea501e83a8f2a73ceb6a0431671bc707a",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "Pitch similarity",
            "name_full": "Similarity judgments for musical pitch (two-octave harmonic complex tones)",
            "brief_description": "LLMs were prompted to provide pairwise similarity ratings for 25 harmonic complex tones spanning two octaves; model predictions were compared to human similarity judgments using Pearson correlation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (also GPT-3.5, GPT-3 evaluated)",
            "model_description": "Transformer-based large language models (OpenAI GPT family) used via text prompts; GPT-4 is a later instruction-tuned, multi-modal-capable variant.",
            "model_size": null,
            "test_name": "Pairwise pitch similarity (psychophysical similarity ratings)",
            "test_category": "perception (auditory pitch similarity)",
            "test_description": "Participants (humans or LLM) rate similarity of pairs of harmonic complex tones (10 partials, 3 dB/oct roll-off) across 25 notes from C4 to C6 on a 0–1 scale (LLMs) or 0–6 Likert (humans), enabling construction of similarity matrices and MDS representations.",
            "evaluation_metric": "Pearson correlation (r) between flattened upper triangles of LLM and human similarity matrices",
            "human_performance": "Inter-rater split-half reliability (IRR) r = 0.90 (95% CI [0.87, 0.92])",
            "llm_performance": "GPT-4: Pearson r = 0.92 (95% CI [0.91, 0.92]); other models reported as &gt;0.6 across domains but per-model values not fully reported in text.",
            "prompting_method": "Few-shot in-context numerical rating prompts (three example pairs provided in prompt); repeated 10 elicitations per pair and averaged",
            "fine_tuned": false,
            "human_data_source": "Authors' collected dataset (online AMT behavioral experiment; pitch dataset also referenced as Marjieh et al., 2023 (41))",
            "statistical_significance": "Correlation reported with 95% bootstrap CI [0.91,0.92]; authors state correlations are significant (no p-value reported).",
            "notes": "LLM recovered known pitch phenomena (octave spike at 12 semitones) and produced MDS pitch-spiral structure; GPT-4 performance is on par with human IRR without fine-tuning. Models used the same few-shot examples across pairs; human ratings collected on 0–6 Likert then converted for comparison.",
            "uuid": "e7487.0",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Loudness similarity",
            "name_full": "Similarity judgments for loudness (pure-tone loudness confusion)",
            "brief_description": "LLMs provided pairwise similarity ratings for pure tones varying in loudness; predictions correlated with human confusion-derived similarity measures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (evaluated alongside GPT-4 and GPT-3)",
            "model_description": "OpenAI GPT family; GPT-3.5 is an instruction-tuned transformer variant used via chat-completion API.",
            "model_size": null,
            "test_name": "Pairwise loudness similarity (confusion-derived similarity)",
            "test_category": "perception (auditory loudness)",
            "test_description": "Similarity between pairs of pure tones differing in dB (8 levels, 71.1–74.6 dB) assessed via confusion matrix converted to similarity scores; LLMs give 0–1 similarity ratings.",
            "evaluation_metric": "Pearson correlation (r) between LLM similarity matrix and human similarity matrix",
            "human_performance": "Human similarity matrix derived from confusion data in Konibrot (1978) via Sims (2018) repository (no IRR reported in paper for this modality)",
            "llm_performance": "GPT-3.5: Pearson r = 0.89 (95% CI [0.87, 0.91]); GPT-4 among top two models but per-model r not explicitly reported in main text.",
            "prompting_method": "Few-shot in-context numerical rating prompts (three example pairs provided); 10 elicitations per pair averaged",
            "fine_tuned": false,
            "human_data_source": "Dataset from Konibrot (1978) as accessed via Sims (2018) (references (26) and (40) in paper)",
            "statistical_significance": "95% bootstrap CI reported [0.87,0.91]; authors state correlations are significant (no p-value reported).",
            "notes": "GPT-3.5 produced high alignment with human loudness similarity; authors note GPT-4 improvement was not restricted to visual domain and that improvements may stem from richer textual training rather than multimodal training.",
            "uuid": "e7487.1",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Color similarity",
            "name_full": "Similarity judgments for colors (wavelength/RGB/hex coded colors)",
            "brief_description": "LLMs rated similarity of color pairs (specified as hex codes) and their similarity matrices were correlated with human direct similarity judgments, recovering color-wheel structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (also GPT-3.5, GPT-3 evaluated)",
            "model_description": "OpenAI GPT family; GPT-4 is a later multimodal-capable variant but evaluated here via text-only hex-code prompts.",
            "model_size": null,
            "test_name": "Pairwise color similarity (direct similarity ratings)",
            "test_category": "perception (visual color similarity)",
            "test_description": "Human direct similarity judgments across a set of colors (original dataset: 14 wavelengths converted to hex; extended to 23 via interpolation) compared to LLM 0–1 similarity ratings for pairs of hex codes.",
            "evaluation_metric": "Pearson correlation (r) between LLM and human similarity matrices",
            "human_performance": "Human similarity dataset from Ekman (1954) (direct similarity judgments); no IRR reported for this modality in paper",
            "llm_performance": "GPT-4: Pearson r = 0.89 (95% CI [0.87,0.91]); authors state correlations &gt;0.6 for all models in this domain but only GPT-4 exact value reported.",
            "prompting_method": "Few-shot in-context numerical rating prompts with three example pairs; 10 elicitations per pair averaged. Color hex codes provided in prompt.",
            "fine_tuned": false,
            "human_data_source": "Ekman, 1954 (reference (25) in paper); conversion from wavelength to RGB/hex done by authors",
            "statistical_significance": "95% bootstrap CI reported [0.87,0.91]; authors state correlations are significant (no p-value reported).",
            "notes": "GPT-4 recovered color-wheel structure in MDS; authors also performed a separate color-naming task to probe language effects. GPT-4's multimodal training did not lead to a notably larger improvement specifically for colors relative to other modalities.",
            "uuid": "e7487.2",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Vocal consonant similarity",
            "name_full": "Similarity judgments for vocal consonants (IPA consonant sounds)",
            "brief_description": "LLMs rated similarity of recorded consonant sounds (IPA tokens) and were compared to human similarity judgments collected online; matrix correlations and MDS revealed articulatory place structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (also GPT-3.5, GPT-3 evaluated)",
            "model_description": "OpenAI GPT family; used via chat-completion API with IPA symbols in text prompts to elicit similarity ratings.",
            "model_size": null,
            "test_name": "Pairwise consonant similarity (vocal consonants)",
            "test_category": "perception (speech sound similarity / phonetics)",
            "test_description": "Participants rate similarity of pairs of recorded consonant sounds (16 recordings from IPA) on a similarity scale; LLMs receive IPA tokens and return numerical similarity ratings.",
            "evaluation_metric": "Pearson correlation (r) between LLM and human similarity matrices",
            "human_performance": "Inter-rater split-half reliability (IRR) r = 0.46 (95% CI [0.36, 0.56]) measured on authors' collected vocal-consonant dataset",
            "llm_performance": "GPT-4: Pearson r = 0.57 (95% CI [0.56,0.59]) — authors note GPT-4 performance is on par with or exceeds human IRR for this dataset.",
            "prompting_method": "Few-shot in-context numerical rating prompts (example IPA pairs and ratings provided); 10 elicitations per pair averaged",
            "fine_tuned": false,
            "human_data_source": "Authors' collected dataset (online AMT study described in Methods)",
            "statistical_significance": "Reported Pearson r with 95% bootstrap CI [0.56,0.59]; authors state correlations are significant (no p-value reported).",
            "notes": "MDS of model similarity recovered production-based articulatory structure; GPT-4 exceeded human IRR on this task (r=0.57 vs IRR 0.46). Stimuli were recordings from two speakers; authors attempted to focus participants on consonant sound rather than vowel or voice pitch.",
            "uuid": "e7487.3",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Taste similarity",
            "name_full": "Similarity judgments for taste flavors (confusion-derived)",
            "brief_description": "LLMs provided similarity ratings for flavor labels; similarity scores computed from human confusion matrices were used as the baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (evaluated alongside GPT-4 and GPT-3)",
            "model_description": "OpenAI GPT family; used with few-shot text prompts mapping flavor word pairs to 0–1 similarity ratings.",
            "model_size": null,
            "test_name": "Pairwise taste/flavor similarity (confusion-derived similarity over 10 flavors)",
            "test_category": "perception (gustatory similarity)",
            "test_description": "Human confusion matrix over 10 flavor labels converted to similarity scores; LLMs rate similarity of flavor-word pairs on a 0–1 scale.",
            "evaluation_metric": "Pearson correlation (r) between LLM and human similarity matrices",
            "human_performance": "Human data from Hettinger et al. (1999) via Sims (2018) repository (no IRR reported in paper)",
            "llm_performance": "GPT-3.5: Pearson r = 0.54 (95% CI [0.48, 0.61]); other model-specific values not explicitly listed in main text.",
            "prompting_method": "Few-shot in-context numerical rating prompts (three example flavor pairs provided); 10 elicitations per pair averaged",
            "fine_tuned": false,
            "human_data_source": "Hettinger et al., 1999 (reference (28)) as accessed via Sims (2018) (reference (40))",
            "statistical_significance": "95% bootstrap CI reported [0.48,0.61]; authors state correlations are significant (no p-value reported).",
            "notes": "Taste dataset is confusion-matrix based which was converted to similarity via standard formula; LLMs produce moderate correlations with human data.",
            "uuid": "e7487.4",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Timbre similarity",
            "name_full": "Similarity judgments for instrument timbre",
            "brief_description": "LLMs rated pairwise timbre similarity for 12 instrument timbres aggregated from prior datasets; correlations with human dissimilarity judgments were computed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (evaluated alongside GPT-4 and GPT-3)",
            "model_description": "OpenAI GPT family; models receive instrument names in textual prompts and produce 0–1 similarity ratings.",
            "model_size": null,
            "test_name": "Pairwise timbre similarity (instrument timbre dissimilarity ratings)",
            "test_category": "perception (auditory timbre)",
            "test_description": "Human dissimilarity judgments over 12 instrument timbres aggregated from multiple prior studies; LLMs provide similarity ratings for instrument name pairs.",
            "evaluation_metric": "Pearson correlation (r) between LLM and human similarity/dissimilarity matrices",
            "human_performance": "Human dataset aggregated from prior publications (Estin & Bitton et al., 2018 (27)); no IRR reported in paper",
            "llm_performance": "GPT-3.5: Pearson r = 0.42 (95% CI [0.40, 0.44]); other models' per-modality values not separately enumerated in main text.",
            "prompting_method": "Few-shot in-context numerical rating prompts (three example instrument pairs and ratings provided); 10 elicitations per pair averaged",
            "fine_tuned": false,
            "human_data_source": "Aggregated timbre datasets as assembled in Esting & Bitton et al., 2018 (reference (27))",
            "statistical_significance": "95% bootstrap CI reported [0.40,0.44]; authors state correlations are significant (no p-value reported).",
            "notes": "Timbre produced the lowest correlations among tested modalities; authors caution about interpreting LLM behavior and note modality-specific variability.",
            "uuid": "e7487.5",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Color naming (forced-choice)",
            "name_full": "Forced-choice color naming with 15 basic color terms (English and Russian)",
            "brief_description": "LLMs and human participants named Munsell/WCS colors by choosing from the same 15-term list; alignment measured with adjusted Rand index (ARI) comparing cluster assignments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, GPT-3",
            "model_description": "OpenAI GPT variants used via few-shot or zero-shot chat prompts; GPT-4 used to generate 15-term lists and to perform naming in both English and Russian.",
            "model_size": null,
            "test_name": "Color naming (forced-choice over 15 basic color names)",
            "test_category": "perception / language (color categorization / lexical labeling)",
            "test_description": "For each of 330 Munsell/WCS colors, participants/LLMs select the best-fitting color name from a fixed list of 15 terms; dominant labels per color are compared via clustering similarity (ARI).",
            "evaluation_metric": "Adjusted Rand Index (ARI) between human and LLM clustering of colors",
            "human_performance": "Human baselines: authors' collected online human English data (103 UK participants) compared to Lindsey & Brown lab baselines: constrained naming ARI = 0.73 (95% CI [0.65,0.74]), free naming ARI = 0.75 (95% CI [0.66,0.75]).",
            "llm_performance": "English: GPT-4 ARI = 0.59 (95% CI [0.56,0.63]); GPT-3.5 ARI = 0.50 (95% CI [0.46,0.52]); GPT-3 ARI = 0.39 (95% CI [0.37,0.42]). Russian: GPT-4 ARI = 0.54 (95% CI [0.46,0.54]); GPT-3.5 ARI = 0.50 (95% CI [0.45,0.52]); GPT-3 ARI = 0.35 (95% CI [0.29,0.35]).",
            "prompting_method": "GPT-4 generated a 15-term basic color list (temperature 0) then repeated forced-choice prompts with shuffled lists; 10 elicitations per color with temperature 0.7 (re-querying invalid outputs until a valid choice or 10 attempts). Prompts run in both English and Russian (translated).",
            "fine_tuned": false,
            "human_data_source": "Authors' collected Prolific dataset (103 UK, 51 Russian participants) and comparison to Lindsey & Brown (2014) dataset (reference (31))",
            "statistical_significance": "95% bootstrap CIs reported for ARI values; authors note LLMs are more human-like than earlier variants but still below the lab-based human baseline (Lindsey & Brown ARI ~0.73–0.75).",
            "notes": "GPT-4 replicates cross-linguistic Russian/English differences (e.g., sinij vs goluboj separation). GPT-4 overuses 'turquoise' relative to humans for some Munsell colors. Authors emphasize differences due to online vs lab color presentation and caution LLM bias and non-determinism in outputs.",
            "uuid": "e7487.6",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can language models encode perceptual structure without grounding? a case study in color.",
            "rating": 2
        },
        {
            "paper_title": "How does chatgpt rate sound semantics?",
            "rating": 2
        },
        {
            "paper_title": "Words are all you need? capturing human sensory similarity with textual descriptors.",
            "rating": 2
        },
        {
            "paper_title": "Musical pitch has multiple psychological geometries.",
            "rating": 2
        },
        {
            "paper_title": "The color lexicon of american english",
            "rating": 1
        }
    ],
    "cost": 0.015285499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large language models predict human sensory judgments across six modalities</h1>
<p>Raja Marjieh ${ }^{\mathrm{a}, 1}$, Ilia Sucholutsky ${ }^{\mathrm{b}}$, Pol van Rijn ${ }^{\mathrm{c}}$, Nori Jacoby ${ }^{\mathrm{c}, 2}$, and Thomas L. Griffiths ${ }^{\mathrm{a}, 1 \mathrm{c}, 2}$<br>${ }^{a}$ Department of Psychology, Princeton University, USA; ${ }^{\text {b }}$ Department of Computer Science, Princeton University, USA; ${ }^{\text {c }}$ Max Planck Institute for Empirical Aesthetics, Germany</p>
<p>This manuscript was compiled on June 16, 2023</p>
<h4>Abstract</h4>
<p>Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality. To study the influence of specific languages on perception, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interaction of language and perception.</p>
<p>perception | representation | NLP | AI | cognitive science
Imagine that you were chosen to be part of an expedition aimed at studying a newly discovered alien species on a distant planet. Your task is to understand the perceptual system of that species. You arrive at the planet and, to your dismay, discover that its inhabitants are long departed and the only thing they left behind is a huge archive of text. How much of the perceptual world of that species can you recover based on text alone? Versions of this question have occupied philosophers for centuries $(1,2)$, and decades of psychological research are beginning to provide glimpses into the rich perceptual content of language and its influence on perception (3-11).</p>
<p>But how can the amount of information that language provides about perception be quantified? Here we propose to do so by eliciting psychophysical judgments from large language models (LLMs) such as GPT-3 and its recent "ChatGPT" variants GPT-3.5 and GPT-4 $(12,13)$. These models are trained on massive text corpora reflecting a substantial chunk of human language and can be queried in a way that is analogous to humans.</p>
<p>Recent research shows that LLMs can be used to study various aspects of cognition (14), including, language processing in the brain (15-17), perception (18-20), cross-modal alignment (21), and morality (22, 23). Here, we use the fact that they are trained on large amounts of language to gain insight into the classic problem of the relationship between language and perception: these models provide a lower bound on the amount of information about perceptual experience that can be extracted from language alone.</p>
<p>We explored this empirically across six modalities, namely, pitch, loudness, colors, consonants, taste, and timbre. Given a stimulus space (e.g., colors) and its stimulus specification (e.g.,
wavelengths or their corresponding hex-codes) we elicit pairwise similarity judgments in a direct analogy to the widespread paradigm of similarity in cognitive science (24) using a carefully crafted prompt that is given to the model to complete (Figure 1A; see Methods). Importantly, whereas four of the modalities were based on classical results from the literature (colors (25), loudness (26), timbre (27) and taste (28)), two human datasets (pitch and vocal consonants) were novel and thus were not part of the training set of the models.</p>
<p>It is worthwhile to note that, unlike the other models, GPT-4 was trained in a multi-modal approach, enabling it to access both written text (similar to the other two variants) and images. This allowed us to examine if the additional sensory information resulted in enhanced performance in the color modality relative to the other domains. Moreover, to further interrogate whether the LLMs' sensory representation was language-dependent, we tested whether they would behave differently in the presence of the same sensory information (a color hex-code), but respond in different languages. To that end, we conducted a color-naming task using a paradigm similar to that of (29) and the World Color Survey $(30,31)$, and constructed human and GPT-3, GPT-3.5, and GPT-4 color-naming maps in both English and Russian.</p>
<h2>Results</h2>
<p>Similarity study. For each dataset, we designed a tailored prompt template that could be filled in with in-context examples and the pair of target stimuli for which we would want the LLM to produce a similarity rating (see Methods and SI for full specification of the prompts and datasets). Across all domains, we elicited 10 ratings per pair of stimuli from each of the GPT models and then constructed aggregate similarity ratings by averaging. We then evaluated the resulting scores by correlating them with human data. The Pearson correlation coefficients between human data and model predictions are shown in Figure 1B (See SI for details regarding computing the correlations and CIs). We see that across all domains, the correlations were significant, and were particularly high for pitch $(r=.92,95 \% \mathrm{CI}[.91, .92]$ for GPT-4), loudness $(r=.89$, $95 \%$ CI $[.87, .91]$ for GPT-3.5), and colors $(r=.89,95 \%$ CI $[.87, .91]$ for GPT-4) (and $&gt;.6$ for all models), followed by moderate but highly significant correlations for consonants $(r=.57,95 \% \mathrm{CI}[.56, .59]$ for GPT-4), taste $(r=.54,95 \% \mathrm{CI}$ $[.48, .61]$ for GPT-3.5) and timbre $(r=.42,95 \% \mathrm{CI}[.40, .44]$</p>
<p>All authors contributed to the design of the experiments and editing the manuscript. Authors RM, NJ, and TLG conceptualized the project. Author IS conducted the LLM experiments. Authors RM and PVR conducted the human experiments. Authors RM, IS, PVR, and NJ analyzed the results.
The authors declare no competing interests.
${ }^{2}$ NJ contributed equally to this work with TLG.
${ }^{1}$ To whom correspondence should be addressed. E-mail: raja.marjieh@princeton.edu</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. A. Schematic of the LLM-based and human similarity judgment elicitation paradigms. B. Correlations between models and human data across six perceptual modalities, namely, pitch, loudness, colors, consonants, taste, and timbre (Pearson $r ; 95 \%$ CIs).
for GPT-3.5). For the two modalities for which we collected data, we could compare model performance to the inter-rater split-half reliability (IRR). The IRRs for pitch and consonants were $r=.90(95 \% \mathrm{CI}[.87, .92])$ and $r=.46(95 \% \mathrm{CI}[.36, .56])$, respectively, suggesting that the performance of GPT-4 is on par with human performance.</p>
<p>We also note that in five out of the six domains, GPT-4 was among the top two models. Interestingly, the improvement relative to the average performance of the other models happened across all modalities with the exception of timbre and loudness, and was not restricted or particularly large for the domain of colors (compare e.g. $\Delta r=.1595 \% \mathrm{CI}[.13, .16]$ for colors vs. $\Delta r=.1695 \% \mathrm{CI}[.15, .17]$ for pitch, $\Delta r=.16$ for taste $95 \% \mathrm{CI}[.12, .19]$, and $\Delta r=.1495 \% \mathrm{CI}[.12, .16]$ for consonants) suggesting that this improvement is driven by richer textual training in GPT-4 rather than the possibility of its inclusion of images in its training set as is currently being debated (18).</p>
<p>Next, to get a finer picture of the LLM-based judgments and see to what extent they reflect human representations, we performed the following analyses. Starting from the domain of pitch, we wanted to see to what extent the LLM data captures a well-known psychological phenomenon that Western listeners tend to associate particular musical intervals or ratios of frequencies (such as the octave or 2:1 frequency ratio) with enhanced similarity (32). To test this we computed the average similarity score over groups of pitch pairs that are separated by the same fixed interval (i.e., the same frequency ratios). Figure 2A shows the resulting average similarity per interval for the models and humans along with an example corresponding smoothed similarity matrix for GPT-3 (smoothing was done by averaging the raw similarity matrix over its sub-diagonals). We can see that apart from the decay as a function of separation (i.e., tones that are far apart in log frequency are perceived as increasingly dissimilar) there is a clear spike precisely at 12 semitones (octave), consistent with the aforementioned phenomenon of "octave equivalence" (33). Moreover, applying multi-dimensional scaling (MDS) (24) to the smoothed similarity matrix whereby the different stimuli are mapped into points in a Euclidean space (also known as "psychological space") such that similar stimuli are mapped to nearby points reveals a clear helical structure with twists that correspond to precisely 12 semitone separations (i.e., octaves) recovering the pitch spiral representation (Figure 2A).</p>
<p>Likewise, applying MDS to the domains of consonants and colors (Figure 2B) reveals highly interpretable representations, namely, the familiar color wheel and a production-based representation for consonants." As an additional test, we asked GPT-4 to provide explanations for the judgments it made (Figure 2C-D), and remarkably, the model resorted to explanations involving the octave, ratios, and harmonic relations for pitch, places of articulation in the vocal tract for consonants, and hue, brightness and color spectra for colors, consistent with the MDS solutions.</p>
<p>Color naming study. The results so far suggest that LLMs can use textual information to form perceptual representations. If this is indeed the case, we hypothesized that representations in LLMs using different languages could be different even in the presence of identical input. This would be consistent with cross-cultural differences in language and perception that were observed in humans (30). To test this, we propose for the first time to test LLMs on an explicit naming task proposed by the seminal work of (29) and further explored across cultures around the globe $(30,31)$.</p>
<p>We thus tested whether LLMs would yield different naming patterns of color hex codes depending on the language of the prompt used to elicit those names (see Methods; Figure 3). Specifically, we presented both humans and LLMs with different colors and asked them to perform a forced-choice naming task by selecting from a pre-specified list of 15 color names (see Methods). We specifically focused on English and Russian as test cases, since Russian speakers are documented to use richer vocabulary to describe what English speakers would otherwise describe as blue and purple $(29,34,35)$. We collected data from 103 native English speakers and 51 native Russian speakers and compared them against LLMs performing the same task, and to the in-lab data of (31) as an additional baseline. ${ }^{\dagger}$</p>
<p>The results are shown in Figure 3. Our first finding was that GPT-4 in both English and Russian were more human-like than the other variants when compared with an adjusted Rand index (see Figure 3A; English: GPT-4 $0.5995 \%$ CI $[0.56,0.63]$, GPT$3.5,0.5095 \%$ CI $[0.46,0.52]$, GPT-3 $0.3995 \%$ CI $[0.37,0.42]$;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. A. Human and LLM similarity marginals and an example GPT-3 corresponding similarity matrix and its three-dimensional MDS solution for pitch. B. MDS solutions for vocal consonants and colors for GPT-4 similarity matrices. To illustrate the structure of the results, we highlighted consonants with the same place of articulation in the vocal tract with the same shape and color, and added a rotated HSV color wheel for the color MDS. C. Example GPT-4 explanations for similarity judgment scores. D. Word clouds for GPT-4 explanations in the domain of pitch, vocal consonants, and colors.</p>
<p>Russian: GPT-4 0.54 95% CI [0.46, 0.54], GPT-3.5 0.50 95% CI [0.45, 0.52], GPT-3 0.35 95% CI [0.29, 0.35]; see Methods). It is evident from our data, however, that LLMs are still not perfect in predicting human color naming as compared to a separate lab-based experiment conducted by Lindsey and Brown (31) (dashed line in Figure 3A top, constrained naming task 0.73 95% CI [0.65,0.73], free naming task 0.75 95% CI [0.66,0.75]). Moreover, the naming of GPT-4 colors differs from human data in some important cases, including the color turquoise, which was selected as the dominant color for 46 Munsell colors in GPT4 versus only 15 in human data. Note, however, that our human English results conducted online and with relative less control over color presentation were highly consistent with (31) that were conducted in the lab and under controlled environment, even though (31) used slightly different paradigms: free naming (consistency to our human data: 0.75 95% CI [0.66, 0.75]) and forced-choice list with a different set of items (0.73 95% CI [0.65, 0.74]).</p>
<p>Importantly, however, GPT-4 appears to replicate crosslingual differences (Figure 3B), for example separating Russian blue and purple into distinct categories for lighter and darker areas (35). Indeed, the color sinij / Синий (Blue) was the dominant category for 18 and 29 Munsell colors for GPT-4 and humans, respectively, and the color golubój / Голубой (Light-blue) was accordingly the dominant category for 33 and 26 colors. Similarly, the color fiołétovyj / Фиолетовый (Violet) was the dominant category for 27 and 32 Munsell colors for GPT-4 and humans, respectively, and the color lilóvyj / Лиловый (Lilac) was accordingly the dominant category for 20 and 18 Munsell colors.</p>
<h3>Discussion</h3>
<p>In this work, we showed how recent advances in large language models and, in particular, their flexible prompt engineering capabilities provide an elegant way for extracting clear quantitative psychophysical signals from text corpora. Our results contribute to a variety of thought-provoking issues in perception and language research. In particular, our findings further support recent research suggesting that people that lack direct sensory experiences (e.g., congenitally blind individuals) could still possess a rich understanding of perceptual concepts through language (e.g., colors (10)). Likewise, the language-dependence of the color representation of GPT-4 in the naming task suggests that the physical stimulus alone (in our case, the color hex code) is insufficient for explaining the behavioral patterns of the model since the same hex codes triggered different color categorizations in Russian and English. This supports the idea that language is not only shaped by perception but can also reshape it (35). Finally, the fact that GPT-4 achieved IRR-level performance without any fine-tuning in the newly collected datasets of pitch and vocal consonants, contributes to our understanding of LLMs' ability to mimic human behavior (13).</p>
<p>We end by discussing some limitations which point towards</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Color naming experiment using 330 Munsell colors from the World Color Survey (top, color space). A. Adjusted Rand index illustrating the alignment between human and LLM experiments ( $95 \%$ CIs). The dashed lines for English represent lab-based free naming and forced-choice naming experiments collected by Lindsey and Brown (31) (data reproduced with permission). B. Data comparison between humans and LLMs in Russian and English. Participants and LLMs were shown colors and were asked to choose from the same 15 -color list. The count of chosen colors for each option is given in parentheses. The color of a response cluster in the maps represents its average color.
future research directions. First, while in our work we experimented with three different types of behavioral data (similarity, explanations, and naming), there are other perceptual measures that one could consider (for example, odd-one-out triplet judgments (36)). Future work could explore these measures and interrogate to what extent they too yield a consistent representation. Second, our work is restricted to population-level averages as a leading-order analysis, a follow-up study could look into the natural variability of the LLM judgments and see to what extent they capture individual-level differences in humans. Third, while our work provides some evidence for LLMs' ability to capture cross-cultural differences, it remains to be seen how far this holds for other languages, especially those that are underrepresented (37). Finally, it is important to point out that the very same massive training pipelines that make LLMs a powerful proxy of human language also make them particularly susceptible to inheriting biases (38). Researchers should be particularly cautious when interpreting the patterns of behavior elicited from LLMs and should always benchmark them against genuine human data.</p>
<p>To conclude, our work showcases how LLMs can be used to explore the limits of what information about the world can be recovered from language alone, and more broadly, it highlights the potential of combining human and machine experiments in order to understand fundamental questions in cognitive science.</p>
<h2>Materials and Methods</h2>
<p>GPT prompt elicitation. The general structure of the prompt elicitation template for the similarity experiments was: one sentence describing the dataset (e.g., "people described pairs of colors using their hex code."), one sentence describing the similarity rating task and scale (e.g., "how similar are the colors in each pair on a scale of 0-1 where 0 is completely dissimilar and 1 is completely similar?"), three sets of three lines each corresponding to two stimuli and their actual similarity rating taken from behavioral experiments which serve as few-shot in-context examples, and an additional set of three lines corresponding to the pair of target stimuli and an associated empty rating field for the model to fill in ("Color one: #FF0000:</p>
<p>Color two: #A020F0. Rating:"). These were necessary to ensure that the model provided numerical values, and in all cases consisted of three fixed and randomly chosen comparisons so that most of the content was left for the model to produce. For each pair of target stimuli, we elicited ten ratings from each GPT model. Across all repetitions of all pairs of stimuli for a given dataset, we used only the same three in-context examples to ensure that the model is exposed to only a very small fraction of the similarity judgments against which its ratings were compared (see SI for full prompts).</p>
<p>For the color-naming experiments, we first elicited 15 basic color names from GPT-4 using the prompt "Name 15 basic colors." and a temperature of 0 (to get the highest probability answers). We then had GPT-4 name the hex code corresponding to each of the WCS colors using the following prompt: "Here is a list of 15 basic color names: <shuffled basic color list>. Which of these names best describes the following color: <hex-code>? Respond only using the name." We repeated this prompt ten times for each WCS color with the basic color list shuffled each time and temperature set to the default 0.7 to elicit ten names per WCS color. We repeated the full procedure with both prompts translated to Russian (GPT-4 also responded to both of these in Russian; see SI for full prompt).</p>
<p>Stimuli. The six human similarity datasets we considered come in two flavors - direct (dis-)similarity ratings and confusion matrices - and from two sources - previous psychological studies from the literature and newly collected datasets. Confusion matrices provide an alternative way to compute similarity scores between stimuli by counting the number of times a stimulus $x$ is confused for a stimulus $y$. By normalizing the counts one gets confusion probabilities $p_{x y}$ which can be converted into similarity scores using the formula $s_{x y}=\sqrt{p_{x y} p_{y x} / p_{x x} p_{y y}}(39,40)$.</p>
<p>Colors This dataset was taken from (25) (also reproduced in (24)) and comprised direct similarity judgments across a set of 14 colors with wavelengths in the range $434-674$ nanometers. We converted wavelengths into RGB using the script at https://hasanyavuz.ozderya. net/?p=211 and then we used the webcolors Python package to convert into hex codes. To get better coverage of the color wheel in Figure 2B we extended the space to 23 color stimuli by interpolating between the original colors in the dataset and eliciting an extended similarity matrix from GPT-4.</p>
<p>Pitch This dataset was collected and made publicly available very recently by a subset of the authors in (41) (see details below). It contains similarity judgments over pairs of 25 harmonic complex tones ( 10 partials and 3dB/octave roll-off) over a two octave range from C4 ( 60 MIDI; 261.626 Hz ) to C6 ( 84 MIDI; 1046.502 Hz ). The</p>
<p>pitch values were separated by 1 semitone steps to account for the fact that pitch perception is logarithmic (33) where the mapping between frequencies $f$ in Hertz and pitch $p$ in semitones are given by $p=12 \log _{2} f / 440+69$.
Vocal consonants This dataset was also collected via an online study (see below) and comprised similarity judgments over 16 recordings of vocal consonants taken from the International Phonetic Association ${ }^{1}$. The vocal consonants considered were b (bay), p (pay), m (may), n (no), g (go), k (cake), d (die), t (tie), f (fee), v (vow), s (so), $\theta$ (thigh), $\delta$ (they), $\chi$ (Jacques), and $f$ (show). The recordings came from two speakers, one male and one female.
Loudness We accessed this dataset via (40) which itself takes the data from (26). The dataset comes in the form of a confusion matrix over 8 pure tones of different loudness values ranging from 71.1 to 74.6 decibels.</p>
<p>Taste This dataset was also accessed via (40) and is taken from (28). The data comes in the form of a confusion matrix over 10 flavors described to participants as salt, salt-substitute, MSG, quinine, acid, sugar, artificial sweetener, salt-sugar, acid-sugar, and quinine-sugar.
Timbre This dataset was assembled in (27) based on 1217 subject's judgments from 5 prior publications. It comprises dissimilarity judgments over 12 instrument timbres: clarinet, saxophone, trumpet, cello, French horn, oboe, flute, English horn, bassoon, trombone, violin, and piano.</p>
<p>Behavioral experiments. To collect similarity judgments over pitch and vocal consonants we deployed two online experiments on Amazon Mechanical Turk (AMT). Overall, 55 participants completed the pitch study and 64 participants completed the vocal consonants study. In addition, we collected color naming data in Russian and British English participants using Prolific ${ }^{\circledR}$. Overall, we recruited 154 participants of which 103 were UK participants and 51 were Russian participants. Experiments were implemented using PsyNet ${ }^{\circledR}$ and Dallinger ${ }^{5}$. See additional details in SI.</p>
<p>Code and Data availability. All data and code used in this work can be accessed via the following link: https://tinyurl.com/fudaby5p, with the exception of the (31) color naming dataset as it is only available upon request from the authors. An interactive visualization of two-dimensional MDS spaces for the 6 modalities is available at: https://computational-audition.github.io/ LLM-psychophysics/all-modalities.html. The human color naming data can be interactively explored via: https://computational-audition.github. io/LLM-psychophysics/color.html.</p>
<p>ACKNOWLEDGMENTS. This research project and related results were made possible with the support of the NOMIS Foundation, and an NSERC fellowship (567554-2022) to IS.</p>
<ol>
<li>D Hume, An Abstract of a Treatise of Human Nature, 1740. (CUP Archive), (1740).</li>
<li>J Locke, An essay concerning human understanding. (Kay \&amp; Troutman), (1847).</li>
<li>RL Goldstone, BJ Rogosky, Using relations within conceptual systems to translate across conceptual systems. Cognition 84, 295-320 (2002).</li>
<li>T Regier, P Kay, N Khetarpal, Color naming reflects optimal partitions of color space. Proc. Natl. Acad. Sci. 104, 1436-1441 (2007).</li>
<li>T Regier, P Kay, Language, thought, and color: Whorf was half right. Trends cognitive sciences 13, 439-446 (2009).</li>
<li>S Dotscheid, S Shayan, A Majid, D Casasanto, The thickness of musical pitch: Psychophysical evidence for linguistic relativity. Psychol. science 24, 613-621 (2013).</li>
<li>N Zaslavsky, C Kemp, T Regier, N Tishby, Efficient compression in color naming and its evolution. Proc. Natl. Acad. Sci. 115, 7937-7942 (2018).</li>
<li>JS Kim, GV EIi, M Bedny, Knowledge of animal appearance among sighted and blind adults. Proc. Natl. Acad. Sci. 116, 11213-11222 (2019).</li>
<li>G Lupyan, RA Rahman, L Boroditsky, A Clark, Effects of language on visual perception. Trends cognitive sciences 24, 930-944 (2020).</li>
<li>JS Kim, B Aheimer, V Montanil-Marrara, M Bedny, Shared understanding of color among sighted and blind adults. Proc. Natl. Acad. Sci. 118, e2020192118 (2021).
${ }^{5}$ https://www.internationalphoneticassociation.org/
${ }^{6}$ https://www.prolific.co
${ }^{7}$ https://psynet.dev/
${ }^{8}$ https://dallinger.readthedocs.io/</li>
<li>G Kawakita, A Zeleznikow-Johnston, N Tsuchiya, M Oizumi, Is my "red" your "red"?: Unsupervised alignment of qualia structures via optimal transport. PsyArXiv (2023).</li>
<li>T Brown, et al., Language models are few-shot learners. Adv. neural information processing systems 33, 1877-1901 (2020).</li>
<li>OpenAI, Opt-4 technical report (2023).</li>
<li>A Srivastava, et al., Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022).</li>
<li>A Goldstein, et al., Shared computational principles for language processing in humans and deep language models. Nat. neuroscience 25, 369-380 (2022).</li>
<li>S Kumar, et al., Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model. BioRxiv pp. 2022-06 (2022).</li>
<li>R Tikochniski, A Goldstein, Y Yeshurun, U Hasson, R Reichart, Perspective changes in human listeners are aligned with the contextual transformation of the word embedding space. Cereb. Cortex p. bhad082 (2023).</li>
<li>M Abdou, et al., Can language models encode perceptual structure without grounding? a case study in color. arXiv preprint arXiv:2109.06129 (2021).</li>
<li>K Siedenburg, C Sallis, How does chatgpt rate sound semantics? arXiv preprint arXiv:2304.07830 (2023).</li>
<li>C Zhang, B Van Durme, Z Li, E Stengel-Eskin, Visual commonsense in pretrained unimodal and multimodal models. arXiv preprint arXiv:2205.01850 (2022).</li>
<li>R Marjieh, et al., Words are all you need? capturing human sensory similarity with textual descriptors. The Eleventh Int. Conf. on Learn. Represent. (2022).</li>
<li>D Dillion, N Tandon, Y Gu, K Gray, Can ai language models replace human participants? Trends Cogn. Sci. (2023).</li>
<li>D Ganguli, et al., The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459 (2023).</li>
<li>RN Shepard, Multidimensional scaling, tree-fitting, and clustering. Science 210, 390-398 (1980).</li>
<li>G Ekman, Dimensions of color vision. The J. Psychol. 38, 467-474 (1954).</li>
<li>DE Konibrot, Theoretical and empirical comparison of lucera choice model and logistic thurstone model of categorical judgment. Percept. \&amp; Psychophys. 24, 193-208 (1978).</li>
<li>P Esting, A Bitton., et al., Generative timbre spaces: regularizing variational auto-encoders with perceptual metrics. arXiv preprint arXiv:1805.08501 (2018).</li>
<li>TP Hettinger, JF Gent, LE Marks, ME Frank, study of taste perception. Percept. \&amp; Psychophys. 61, 1510-1521 (1999).</li>
<li>B Berlin, P Kay. Basic color terms: Their universality and evolution. (Univ of California Press), (1991).</li>
<li>P Kay, B Berlin, L Maffi, WR Merrifield, R Cook, The world color survey. (Citesøer), (2009).</li>
<li>DT Lindsey, AM Brown, The color lexicon of american english. J. vision 14, 17-17 (2014).</li>
<li>RN Shepard, Geometrical approximations to the structure of musical pitch. Psychol. review 89, 305 (1982).</li>
<li>N Jacoby, et al., Universal and non-universal features of musical pitch perception revealed by singing. Curr. Biol. 29, 3229-3243 (2019).</li>
<li>GV Paramei, YA Griber, D Mylonas, An online color naming experiment in russian using munset color samples. Color. Res. \&amp; Appl. 43, 358-374 (2018).</li>
<li>J Winawer, et al., Russian blues reveal effects of language on color discrimination. Proc. national academy sciences 104, 7780-7785 (2007).</li>
<li>MN Hebart, CY Zheng, F Pereira, CI Baker, Revealing the multidimensional mental representations of natural objects underlying human similarity judgements. Nat. human behaviour 4, 1173-1185 (2020).</li>
<li>DE Blasi, J Henrich, E Adamou, D Kemmerer, A Majid, Over-reliance on english hinders cognitive science. Trends cognitive sciences (2022).</li>
<li>TY Zhuo, Y Huang, C Chen, Z Xing, Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv preprint arXiv:2301.12867 (2023).</li>
<li>RN Shepard, Toward a universal law of generalization for psychological science. Science 237, 1317-1323 (1987).</li>
<li>CR Sims, Efficient coding explains the universal law of generalization in human perception. Science 360, 652-656 (2018).</li>
<li>R Marjieh, TL Griffiths, N Jacoby, Musical pitch has multiple psychological geometries. bioRxiv (2023).</li>
<li>P Harrison, et al., Gibbs sampling with people in Advances in Neural Information Processing Systems, eds. H Larochelle, M Ranzato, R Hadsell, MF Balcan, H Lin. (Curran Associates, Inc.), Vol. 33, pp. 10659-10671 (2020).</li>
<li>KJ Woods, MH Siegel, J Tsien ,JH McDermott, Headphone screening to facilitate web-based auditory experiments. Attention, Perception, \&amp; Psychophys. 79, 2064-2072 (2017).</li>
<li>N Kriegeskorte, M Mur, PA Bandettini, Representational similarity analysis-connecting the branches of systems neuroscience. Front. systems neuroscience 2, 4 (2008).</li>
<li>J Clark, The ishihara test for color blindness. Am. J. Physiol. Opt. (1924).</li>
<li>WM Rand, Objective criteria for the evaluation of clustering methods. J. Am. Stat. association 66, 846-850 (1971).</li>
</ol>
<h2>Extended Methods</h2>
<p>Explicit GPT prompts. GPT prompt elicitation experiments were conducted using the OpenAI Text Completion (for GPT-3) and Chat Completion (for GPT-3.5 and GPT-4) APIs. The temperature was set to 0.7 for similarity judgments and 0 for color naming experiments. The exact prompt formats used are shown below.</p>
<h2>Similarity judgments.</h2>
<h2>Color:</h2>
<p>People described pairs of colors using their hex codes. How similar are the two colors in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar? Respond only with the numerical similarity rating. Color 1: #ff5700 Color 2: #ff9b00 Rating 0.76 Color 1: #b3ff00 Color 2: #00ff61 Rating: 0.45 Color 1: #FF0000 Color 2: #00b2ff Rating: 0.02 Color 1: <hex-code1> Color 2: <hex-code2> Rating:</p>
<h2>Pitch:</h2>
<p>People described pairs of musical notes using their frequencies in hertz.
How similar are the musical notes in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Note 1: 587.3295358348151 Hz
Note 2: 987.7666025122483 Hz
Rating: 0.46083740655517463
Note 1: 349.2282314330039 Hz
Note 2: 277.1826309768721 Hz
Rating: 0.743838237117938
Note 1: 415.3046975799451 Hz
Note 2: 987.7666025122483 Hz
Rating: 0.19874605585261726
Note 1: <frequency1>
Note 2: <frequency2>
Rating:</p>
<h2>Vocal consonants:</h2>
<p>People described vocal consonants using the international phonetic alphabet (IPA).
How similar do the vocal consonants in each pair sound on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar? Respond only with the numerical similarity rating.</p>
<p>Vocal Consonant 1: f
Vocal Consonant 2: m
Rating: 0.5
Vocal Consonant 1: n
Vocal Consonant 2: 3
Rating: 0.40740740740740744
Vocal Consonant 1: $\int$
Vocal Consonant 2: $\int$
Rating: 1.0
Vocal Consonant 1: <consonant1>
Vocal Consonant 2: <consonant2>
Rating:</p>
<h2>Loudness:</h2>
<p>People described the loudness of pure tones in decibels (dB).
How similar do the pure tones in each pair sound on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Pure Tone 1: 72.6 dB
Pure Tone 2: 74.1 dB
Rating: 0.3495324720283043
Pure Tone 1: 74.6 dB
Pure Tone 2: 73.6 dB
Rating: 0.5055839477695901
Pure Tone 1: 74.1 dB
Pure Tone 2: 74.1 dB
Rating: 1.0
Pure Tone 1: <loudness1>
Pure Tone 2: <loudness2>
Rating:
Taste:
People described flavors they tasted using words. How similar are the flavors in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Flavor 1: quinine
Flavor 2: artificial sweetener
Rating: 0.0
Flavor 1: artificial sweetener
Flavor 2: salt
Rating: 0.015433904145892428
Flavor 1: quinine-sugar
Flavor 2: acid-sugar
Rating: 0.2539115246067999
Flavor 1: <flavor1>
Flavor 2: <flavor2>
Rating:
Timbre:
People listened to pairs of musical instruments and rated the similarity of their timbre.
How similar is the timbre of the instruments in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Instrument 1: Cello
Instrument 2: Flute
Rating: 0.5604846433040316
Instrument 1: Flute
Instrument 2: Clarinet
Rating: 0.270932601836378
Instrument 1: Trombone
Instrument 2: Bassoon
Rating: 0.2893895067551666
Instrument 1: <instrument1>
Instrument 2: <instrument2>
Rating:</p>
<h2>Color naming.</h2>
<h2>Basic color free-elicitation:</h2>
<p>English:
Name 15 basic colors.
Russian:
Перечислите 15 основных цветов.</p>
<h2>Color naming elicitation:</h2>
<p>English:
Here is a list of 15 basic color names: <shuffled basic color list $>$.
Which of these names best describes the following color: $&lt;$ hex-code $&gt;$ ?
Respond only using the name.
Russian:
Вот список из 15 названий основных цветов: <shuffled basic color list $>$.
Какое из названий цветов лучше всего описывает следуюший цвет: <hex-code>?
Отвечайте только названием одного цвета из списка.
We repeated this prompt ten times for each WCS color with the basic color list shuffled each time and temperature set to the default 0.7 to elicit ten names per WCS color. For each of the ten elicitations per color, if the output was not one of the 15 basic colors we would keep re-querying GPT until it did give a valid color output (GPT-4 is slightly non-deterministic even at temperature 0 due to changes in hardware). If after 10 attempts the response was still invalid, we would return "error" as the color (this response is later discarded from the analysis).</p>
<h2>Additional details of behavioral experiments.</h2>
<p>Similarity Experiments: Participants. To collect similarity judgments over pitch and vocal consonants we deployed two online experiments on Amazon Mechanical Turk (AMT). ${ }^{<em> </em>}$ The recruitment and experimental pipelines were automated using PsyNet (42), a modern framework for experiment design ${ }^{11}$ and deployment which builds on the Dallinger ${ }^{12}$ platform for recruitment automation. Overall, 55 participants completed the pitch study and 64 participants completed the vocal consonants study. Participants were recruited from the United States, were paid $\$ 9-12$ USD per hour, and provided informed consent as approved by the Princeton IRB (#10859) and the Max Planck Ethics Council (#2021_42).</p>
<p>To enhance data quality, participants had to pass a standardized headphone check (43) that ensures good listening conditions and task comprehension, and were required to have successfully completed at least 3000 tasks on AMT. Upon passing the prescreening stage, participants were randomly assigned to rate the similarity between different pairs of stimuli and provided numerical judgments on a 7 Likert scale ranging from 0 (completely dissimilar) to 6 (completely similar). In the pitch experiment, participants provided an average of 80 judgments, and in the vocal consonants experiment an average of 55 judgments. See SI for explicit instructions.</p>
<p>Similarity Experiments: Procedure. Upon providing informed consent and passing the headphone check, participants received the following instructions. In the case of the pitch experiment: "In this experiment we are studying how people perceive sounds. In each round you will be presented with two sounds and your task will be to simply judge how similar those sounds are. You will have seven response options, ranging from 0 ('Completely Dissimilar') to 6 ('Completely Similar'). Choose the one you think is most appropriate. You will also have access to a replay button that will allow you to replay the sounds if needed. Note: no prior expertise is required to complete this task, just choose what you intuitively think is the right answer." Participants were then informed of an additional small quality bonus "The quality of your responses will be automatically monitored, and you will receive a bonus at the end of the experiment in proportion</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to your quality score. The best way to achieve a high score is to concentrate and give each round your best attempt". While the task is subjective in nature, we used consistency as a proxy for quality by repeating 5 random trials at the end of the experiment and computing the Spearman correlation $s$ between the original responses and their repetitions. The final bonus was computed using the formula $\min (\max (0.0,0.1 s), 0.1)$ yielding at most 10 cents. In the main experiment participants were assigned to random stimulus pairs and were instructed to rate their similarity using the following prompt: "How similar are the pair of sounds you just heard?" and provided a response on a Likert scale. The procedure for the vocal consonants similarity experiment was identical up to the specific instructions. Specifically, participants received the following instructions: "In this experiment we are studying how people perceive the sound of vocal consonants. A consonant is a speech sound that is pronounced by partly blocking air from the vocal tract. For example, the sound of the letter $c$ in cat is a consonant, and so is $t$ but not $a$. Similarly, the sound of the combination $s h$ in sheep is a consonant, and so is $p$ but not $e e$. In general, vowel sounds like those of the letters $a, e, i, o$, u are not consonants." The instructions then proceeded: "In each round you will be presented with two different recordings each including one consonant sound and your task will be to simply judge how similar are the sounds of the two spoken consonants. We are not interested in the vowel sounds nor in the voice height, just the sound of the consonants. You will have seven response options, ranging from 0 ('Completely Dissimilar') to 6 ('Completely Similar'). Choose the one you think is most appropriate. Note: no prior expertise is required to complete this task, just choose what you intuitively think is the right answer." Participants were then informed of the quality bonus which was identical to the pitch task, and then rated the similarity between pairs of random consonants based on the following prompt "How similar is the sound of the consonants pronounced by the two speakers?" and a Likert scale as before.</p>
<p>Similarity Experiments: Model Evaluation. We quantified model performance in predicting human similarity judgments by computing the Pearson correlation coefficient between the flattened upper triangle of the LLM-based and human-based similarity matrices (to account for the fact that these matrices are symmetric). This approach is similar to representational similarity analysis (44). To compute $95 \%$ confidence intervals, we bootstrapped with replacement over model predictions with 1,000 repetitions and computed for each repetition the average similarity matrix. We then correlated the upper triangles of each of those matrices with human data to produce a list of correlation coefficients on which we computed confidence intervals.</p>
<p>Color Naming Experiments: Participants. To collect the color naming data in Russian and British English participants, we ran online experiments on Prolific ${ }^{\text {S5 }}$. Overall, we recruited 103 UK participants and 51 Russian participants. All texts in the interface of the experiment (e.g., buttons, instructions, etc) were presented in the native language of the participant. The Russian texts were first automatically translated using DeepL ${ }^{\text {TM }}$ and then manually checked and corrected by a native speaker of Russian (author I.S). Participants had to be raised monolingually and to speak the target language as their mother tongue. Each participant was paid 9 GBP per hour and provided informed consent according to an approved protocol (Max Planck Ethics Council #2021_42). The experiment was implemented using PsyNet (42). Each session starts with a free-elicitation task where participants are asked to provide basic colors:</p>
<h2>Color Naming Experiments: Procedure.</h2>
<h2>Basic color free-elicitation:</h2>
<p>English:
Please name at least 8 basic color names.
Press enter after each color name.
Only use lower-case letters.</p>
<p>Russian:
Укажите не менее 8 названий основных цветов.
Нажмите клавишу Enter после каждого названия цвета.
Используйте только строчные буквы.</p>
<p>Participants may only submit color names without spaces, numbers, or special characters and can only submit the page if they have provided at least eight names. The list of obtained colors is highly overlapping with the GPT-4 list, justifying our choice to use GPT-4 as the basis for the word naming task (colors are sorted by their naming frequency).</p>
<p>Top 15 terms in English:
"blue", "green", "yellow", "red", "purple", "orange", "black", "pink", "white", "brown", "grey", "violet", "indigo", "turquoise", "silver"</p>
<p>Top 15 terms in Russian:
"красный", "синий", "белый", "зеленый", "оранжевый", "желтый", "фиолетовый", "черный", "голубой", "коричневый", "розовый", "серый", "жёлтый", "зелёный", "чёрный"</p>
<p>From the top 15 color terms, 11 (English) and 12 (Russian) color terms are overlapping with the list provided by GPT-4. Before the main experiments, participants received the following instructions:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Color naming instructions:</h2>
<p>English:
During this experiment, you will be presented with a square of a particular color and will be required to select the most suitable color term from a list of options.
Please be aware that some of the colors may be repeated to verify consistency of your choices.
If we detect any inconsistencies in your answers, we may terminate the experiment prematurely.
The best strategy is to answer each question truthfully, as attempting to memorize responses may prove difficult.</p>
<p>Russian:
В ходе этого эксперимента вам будет представлен квадрат определенного цвета, и вам нужно будет выбрать наиболее подходящее название цвета из списка вариантов.
Имейте в виду, что некоторые цвета могут повторяться, чтобы убедиться в согласованности вашего выбора. Если мы обнаружим какие-либо несоответствия в ваших ответах, мы можем досрочно прекратить эксперимент.
Лучшая стратегия - отвечать на каждый вопрос правдиво, так как попытка запомнить ответы может оказаться сложной.</p>
<p>The participants then went through the main experiment:</p>
<h2>Color naming task:</h2>
<p>English:
<square of a particular color>
You will see below a list of 15 basic color names. Which of these names best describes the color above?
$&lt;$ shuffled basic color list presented as buttons $&gt;$</p>
<p>Russian:
<square of a particular color>
Ниже Вы увидите список из 15 основных названий цветов.
Какое из этих названий лучше всего описывает вышеуказанный цвет?
$&lt;$ shuffled basic color list presented as buttons $&gt;$</p>
<p>At the end of the experiment, the participant took a color blindness test (45). Some participants abandoned the experiment prematurely, but we nevertheless included their responses ( 42 English participants, 3 Russian participants). Only a fraction of the participants failed the color blindness test ( 5 of 103 English participants, and 2 of 51 Russian participants). Consistent with the WCS we included all participants including those who failed the color blindness test. In a control analysis, we excluded all color blind individuals and all participants that did not complete the entire session and got nearly identical results (the adjusted Rand index was 0.92 for English experiments and 0.97 for Russian experiments).</p>
<p>Color Naming Experiments: Analysis. For each color, we collected at least 10 responses per LLM variant, and at least 10 forcedchoice human selections per color (English mean 19.30 responses, Russian mean 12.17 responses). Consistent with previous literature for each Munsell's color we selected the most frequently reported term. We then presented the dominant colors in Figure 3B. To aid visualization we average the RGB values of all colors with the same color term, and presented them as the legend and clustered color in that figure. We also listed per color the number of Munsell's colors that were associated with each dominant color term. Figure 3B provides additional information on the degree of agreement for each color. Colors for which less than $50 \%$ and $90 \%$ of the times the dominant color term was selected were indicated by "-" and "*", respectively. If the dominant color term was selected more than $90 \%$ of the time, no marking was used.</p>
<p>Adjusted Rand index. The Rand index (46) is a label-insensitive measure of clustering similarity that instead of relying on specific labels (e.g. "Blue") quantifies the similarity between two clustering partitions by counting pairs of items (in our case Munsell colors) that are clustered consistently and dividing them by the overall number of pairs. This allows to compare different clustering schemes when the vocabulary of labels is not aligned (e.g. English and Russian). Formally, we computed: $R=(b+c) / a$; where $b$ is the number of pairs of items that are in the same subset in one clustering and in the same subset in the other, $c$ is the number of pairs of items that are in different subsets in one clustering and in different subsets in the other and $a$ is the total number of pairs. The Rand index provides high values for two random clusterings, to adjust for this we used the corrected-for-chance version of the Rand index (46), which normalizes the raw value by the expected value of the Rand index for random clusterings. Formally, we have $A R I=\left(R I-R I_{\text {rand }}\right) /(1-$ $R I_{\text {rand }}$ ) where $A R I$ is the adjusted Rand index, $R I$ is the raw Rand index and $R I_{\text {rand }}$ is the expected Rand index for random clusterings. The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples, exactly 1.0 when the clusterings are identical (up to a permutation), and reaches -0.5 for "orthogonal clusters" that are less consistent relative to what is expected by chance. In our case, all values were strictly positive suggesting consistency across languages and experiments. To compute confidence intervals, we created bootstrapped datasets by sampling the responses of each color with replacement and recomputing the dominant selected color name. We then obtained CIs by computing the adjusted Rand index for 1,000 pairs of bootstrapped datasets.</p>
<p>Lindsey and Brown dataset. We compared our experimental data to a dataset by (31), reproduced with permission by Delwin Lindsey. The data contains two experimental conditions conducted in the lab with the same 51 participants. In the first condition, participants were instructed to provide free naming responses. In the second condition, participants were instructed to choose from a pre-specified list of 11 color terms: (Green, Blue, Purple, Pink, White, Brown, Orange, Yellow, Red, Black, and Gray). Despite the fact that the Lindsey \&amp; Brown experiment was conducted in the lab (and not online like our experiments) and that the constrained list in our experiment was somewhat different, the results of both experiments were highly consistent with our human English data (Constrained, $A R I=0.73$ $95 \%$ CI $[0.65,0.74]$, free naming $0.7595 \%$ CI $[0.66,0.75]$ ). In addition to putting an upper bound on the consistency with which the LLM can predict human data (by comparing it with another human experiment), these results prove that despite less control over color presentation compared to the lab, online presentation still provides high-quality color naming data.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{55}$ https://www.prolific.com
${ }^{\text {TM }}$ https://www.deepl.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
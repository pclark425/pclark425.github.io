<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8306 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8306</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8306</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-265158225</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07914v2.pdf" target="_blank">Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey</a></p>
                <p><strong>Paper Abstract:</strong> The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8306.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8306.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate, step-by-step natural-language reasoning traces from LLMs to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (e.g., GPT-family, PaLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language models pre-trained on large corpora; referenced generally in the survey rather than a single experimental instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'few-shot CoT prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompts the model (often few-shot) to generate intermediate reasoning steps in natural language before producing a final answer; implemented by providing exemplar question-answer pairs with reasoning chains or by instructing the model to 'think step by step'.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey summarizes works using CoT (including few-shot CoT and CoT-based fine-tuning) but does not report a specific ablation within this paper comparing CoT against fundamentally different reasoning paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multi-step reasoning tasks (math problems, commonsense reasoning, symbolic tasks) — general reference to benchmarks used in CoT literature (e.g., GSM8K and other multi-step QA datasets) as described in surveyed works.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports that CoT variants improve complex reasoning ability in LLMs (no single numeric aggregate reported here); specific CoT improvements are reported in cited works rather than in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT-style stepwise traces help debugging and interpretation of model reasoning and are useful for math, commonsense, and symbolic tasks; effectiveness depends on model scale and prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-thought prompting is an effective, low-cost mechanism to improve multi-step reasoning in LLMs, but design of prompts and whether neural nets truly 'reason' remain open questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8306.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8306.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Thought with Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of CoT that samples multiple independent reasoning chains and aggregates results (e.g., via majority vote) to improve final answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (e.g., PaLM, other large decoder models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained transformer LLMs capable of generating multiple sampled outputs under stochastic decoding; self-consistency leverages diverse sampled chains.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought', 'self-consistency (sampling + aggregation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate many CoT chains via sampling (temperature/beam) and apply an aggregation rule (e.g., majority vote) over final answers to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey references self-consistency methods (multiple sampled reasoning chains aggregated) but does not report a new ablation; it summarizes the approach as a diversity-based improvement to CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multi-step reasoning benchmarks (general CoT tasks referenced in literature).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey cites self-consistency as improving CoT reliability (no numeric values reported in the survey beyond the referenced paper).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Sampling multiple reasoning chains increases robustness to spurious single-chain errors and can lead to better final answers; effectiveness depends on model size and sampling strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency leverages diversity of reasoning chains to improve accuracy compared to using a single CoT chain, though the survey does not provide a direct ablation within itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8306.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8306.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning framework that treats intermediate reasoning units as nodes in a search tree, allowing exploration of multiple solution paths and selection via self-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified; used with large-capacity models in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs used as a generator and evaluator to expand and score candidate reasoning states (thoughts) in a tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['tree-search style chain-of-thought (Tree of Thoughts)', 'multi-path exploration', 'self-evaluation/pruning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Intermediate text units ('thoughts') are expanded into multiple successor thoughts to form a search tree; the LLM evaluates/prunes branches and picks a final path, enabling explicit search over multiple reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey describes ToT as exploring multiple coherent intermediate reasoning paths (diversity of trajectories) but does not present an in-survey controlled comparison against single-path CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Deliberate problem solving tasks requiring planning and multi-step reasoning (as per ToT paper); survey lists ToT among methods for complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey does not provide numerical ToT results; it reports ToT conceptually enables consideration of multiple paths and improves decision-making in cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ToT's explicit multi-path exploration increases the model's ability to avoid local reasoning dead-ends, improves interpretability, and enables self-evaluation, at the cost of more computation.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Tree of Thoughts allows LLMs to consider multiple reasoning trajectories and self-evaluate, supporting more deliberate problem solving than single linear CoT chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8306.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8306.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IR-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interleaved Retrieval with Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that alternates between generating chain-of-thought reasoning and retrieving external knowledge (from KGs or corpora) to iteratively guide multi-step question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (used as generator) + Retriever modules</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hybrid system where a generative LLM produces intermediate reasoning steps which trigger retrieval operations; retrieved knowledge then informs subsequent reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought', 'retrieval-augmented iterative reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT is produced, retrieval queries are constructed from intermediate steps, retrieved KG/corpora facts are injected back into the context, and the model continues reasoning—iterative interleaving of generation and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey reports IR-CoT as an approach that combines (diverse) retrieval and CoT reasoning iteratively; no in-survey ablation comparing purely CoT vs IR-CoT is presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Knowledge-intensive multi-step question answering (multi-hop QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey states IR-CoT iteratively guides retrieval and reasoning for multi-step questions but does not list numerical performance in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Interleaving retrieval with CoT helps ground intermediate reasoning steps in external facts and can improve faithfulness of explanations and answers; success depends on retriever quality.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining retrieval and CoT helps address knowledge gaps and improves multi-step, knowledge-intensive reasoning by grounding intermediate steps with external information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8306.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8306.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning on Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KG-augmented method that creates interpretable reasoning paths over knowledge graphs to produce faithful LLM reasoning and answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reasoning on graphs: Faithful and interpretable large language model reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (as reported in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT (closed-source OpenAI conversational LLM) augmented with knowledge-graph-derived reasoning paths to inform answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['knowledge-graph path construction', 'graph-based stepwise reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Construct reasoning paths over KG relations that serve as intermediate, interpretable steps; LLM uses these KG-derived paths to produce final answers and justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey reports RoG builds reasoning paths using various relations and uses KG augmentation; no within-survey ablation explicitly comparing single-path vs multi-path modes is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Reasoning tasks evaluated with ChatGPT in the RoG study (interpretable multi-step reasoning over KG-backed questions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports RoG increased ChatGPT accuracy from 66.8% to 85.7% on the reasoning tasks cited in Luo et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>KG-created reasoning paths yield more interpretable and accurate outputs; stepwise, graph-grounded reasoning substantially improved ChatGPT's accuracy on the evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Using knowledge graphs to derive faithful reasoning paths significantly increases accuracy and interpretability of LLM reasoning compared to relying solely on the model's internal knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8306.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8306.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MindMap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MindMap (graph-of-thoughts prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-and-play prompting approach that evokes 'graph-of-thoughts' reasoning by using knowledge-graph-based prompts to guide LLMs through multi-step, structured reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MindMap: Knowledge graph prompting sparks graph of thoughts in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (as used in the survey examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (OpenAI large multimodal/LLM) used with clinical KG prompts to perform domain-specific reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['graph-of-thoughts prompting', 'KG-augmented stepwise reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Use KG-derived 'mind maps' (subgraphs) as prompts that structure the LLM's intermediate reasoning into a graph-like sequence of steps; enables multi-path consideration and domain grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey notes MindMap evokes graph-of-thoughts, enabling multiple inference paths and self-evaluation; the survey does not present a direct ablation within itself comparing single-path vs graph-of-thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Clinical reasoning tasks (disease diagnosis and drug recommendation) using clinical KG augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports MindMap boosted accuracy in disease diagnosis and drug recommendation to 88.2% (as reported in Wen et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Graph-structured prompting with domain KGs improves domain-specific reasoning and faithfulness, yielding high accuracy for clinical tasks; beneficial for grounding and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Graph-of-thoughts prompting that leverages KGs can substantially improve domain-specific reasoning performance and faithfulness in LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8306.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8306.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-Retrieval (small LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph-Augmented Retrieval for small LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using retrieved, structured facts from knowledge graphs as context for small language models to substantially improve factual answer correctness on QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-augmented language model prompting for zero-shot graph question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Small LLMs / retrieval-augmented models (unspecified small models in surveyed works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller-parameter transformer LMs augmented at inference time with retrieved KG triples or textualized KG facts supplied as additional context.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['KG-augmented retrieval', 'retrieve-and-append contextual grounding']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Match entities in questions to KG triples, retrieve relevant triples, optionally convert triples to text, and provide them as additional context to the LLM at inference to improve factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey references experiments (Baek et al., Sen et al., Wu et al.) showing KG-augmented retrieval benefits small models; no in-survey direct ablation between differing reasoning method diversity is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Zero-shot/knowledge-intensive question answering (KGQA and general QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey states augmenting facts from knowledge graphs (rather than increasing model size) enhanced answer correctness by over 80% for question-answering tasks (citing Baek et al., 2023; Sen et al., 2023; Wu et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Providing structured KG facts dramatically improves factual answer rates for small models, but success is limited by retriever quality and coverage (especially for less-popular entities); increasing model size alone may not substitute for grounded facts.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>KG-augmented retrieval is highly effective at improving factual correctness in smaller LLMs; grounding via structured facts can outperform naive scaling for certain QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. <em>(Rating: 2)</em></li>
                <li>Rethinking with retrieval: Faithful large language model. <em>(Rating: 1)</em></li>
                <li>Reasoning on graphs: Faithful and interpretable large language model reasoning. <em>(Rating: 2)</em></li>
                <li>MindMap: Knowledge graph prompting sparks graph of thoughts in large language models. <em>(Rating: 2)</em></li>
                <li>Knowledge-augmented language model prompting for zero-shot graph question answering. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8306",
    "paper_id": "paper-265158225",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain of Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate, step-by-step natural-language reasoning traces from LLMs to improve multi-step problem solving.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (e.g., GPT-family, PaLM)",
            "model_description": "Transformer-based large language models pre-trained on large corpora; referenced generally in the survey rather than a single experimental instantiation.",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "few-shot CoT prompting"
            ],
            "reasoning_methods_description": "CoT prompts the model (often few-shot) to generate intermediate reasoning steps in natural language before producing a final answer; implemented by providing exemplar question-answer pairs with reasoning chains or by instructing the model to 'think step by step'.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Survey summarizes works using CoT (including few-shot CoT and CoT-based fine-tuning) but does not report a specific ablation within this paper comparing CoT against fundamentally different reasoning paradigms.",
            "task_or_benchmark": "Multi-step reasoning tasks (math problems, commonsense reasoning, symbolic tasks) — general reference to benchmarks used in CoT literature (e.g., GSM8K and other multi-step QA datasets) as described in surveyed works.",
            "performance_results": "Survey reports that CoT variants improve complex reasoning ability in LLMs (no single numeric aggregate reported here); specific CoT improvements are reported in cited works rather than in this survey.",
            "qualitative_findings": "CoT-style stepwise traces help debugging and interpretation of model reasoning and are useful for math, commonsense, and symbolic tasks; effectiveness depends on model scale and prompt design.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Chain-of-thought prompting is an effective, low-cost mechanism to improve multi-step reasoning in LLMs, but design of prompts and whether neural nets truly 'reason' remain open questions.",
            "uuid": "e8306.0",
            "source_info": {
                "paper_title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain of Thought with Self-Consistency",
            "brief_description": "An extension of CoT that samples multiple independent reasoning chains and aggregates results (e.g., via majority vote) to improve final answer accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (e.g., PaLM, other large decoder models)",
            "model_description": "Pre-trained transformer LLMs capable of generating multiple sampled outputs under stochastic decoding; self-consistency leverages diverse sampled chains.",
            "reasoning_methods": [
                "chain-of-thought",
                "self-consistency (sampling + aggregation)"
            ],
            "reasoning_methods_description": "Generate many CoT chains via sampling (temperature/beam) and apply an aggregation rule (e.g., majority vote) over final answers to improve robustness and accuracy.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey references self-consistency methods (multiple sampled reasoning chains aggregated) but does not report a new ablation; it summarizes the approach as a diversity-based improvement to CoT.",
            "task_or_benchmark": "Multi-step reasoning benchmarks (general CoT tasks referenced in literature).",
            "performance_results": "Survey cites self-consistency as improving CoT reliability (no numeric values reported in the survey beyond the referenced paper).",
            "qualitative_findings": "Sampling multiple reasoning chains increases robustness to spurious single-chain errors and can lead to better final answers; effectiveness depends on model size and sampling strategy.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Self-consistency leverages diversity of reasoning chains to improve accuracy compared to using a single CoT chain, though the survey does not provide a direct ablation within itself.",
            "uuid": "e8306.1",
            "source_info": {
                "paper_title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree of Thoughts",
            "brief_description": "A reasoning framework that treats intermediate reasoning units as nodes in a search tree, allowing exploration of multiple solution paths and selection via self-evaluation.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified; used with large-capacity models in cited work)",
            "model_description": "Transformer LLMs used as a generator and evaluator to expand and score candidate reasoning states (thoughts) in a tree search.",
            "reasoning_methods": [
                "tree-search style chain-of-thought (Tree of Thoughts)",
                "multi-path exploration",
                "self-evaluation/pruning"
            ],
            "reasoning_methods_description": "Intermediate text units ('thoughts') are expanded into multiple successor thoughts to form a search tree; the LLM evaluates/prunes branches and picks a final path, enabling explicit search over multiple reasoning trajectories.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey describes ToT as exploring multiple coherent intermediate reasoning paths (diversity of trajectories) but does not present an in-survey controlled comparison against single-path CoT.",
            "task_or_benchmark": "Deliberate problem solving tasks requiring planning and multi-step reasoning (as per ToT paper); survey lists ToT among methods for complex reasoning.",
            "performance_results": "Survey does not provide numerical ToT results; it reports ToT conceptually enables consideration of multiple paths and improves decision-making in cited literature.",
            "qualitative_findings": "ToT's explicit multi-path exploration increases the model's ability to avoid local reasoning dead-ends, improves interpretability, and enables self-evaluation, at the cost of more computation.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Tree of Thoughts allows LLMs to consider multiple reasoning trajectories and self-evaluate, supporting more deliberate problem solving than single linear CoT chains.",
            "uuid": "e8306.2",
            "source_info": {
                "paper_title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "IR-CoT",
            "name_full": "Interleaved Retrieval with Chain-of-Thought",
            "brief_description": "A method that alternates between generating chain-of-thought reasoning and retrieving external knowledge (from KGs or corpora) to iteratively guide multi-step question answering.",
            "citation_title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.",
            "mention_or_use": "mention",
            "model_name": "LLMs (used as generator) + Retriever modules",
            "model_description": "A hybrid system where a generative LLM produces intermediate reasoning steps which trigger retrieval operations; retrieved knowledge then informs subsequent reasoning steps.",
            "reasoning_methods": [
                "chain-of-thought",
                "retrieval-augmented iterative reasoning"
            ],
            "reasoning_methods_description": "CoT is produced, retrieval queries are constructed from intermediate steps, retrieved KG/corpora facts are injected back into the context, and the model continues reasoning—iterative interleaving of generation and retrieval.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey reports IR-CoT as an approach that combines (diverse) retrieval and CoT reasoning iteratively; no in-survey ablation comparing purely CoT vs IR-CoT is presented here.",
            "task_or_benchmark": "Knowledge-intensive multi-step question answering (multi-hop QA).",
            "performance_results": "Survey states IR-CoT iteratively guides retrieval and reasoning for multi-step questions but does not list numerical performance in this paper.",
            "qualitative_findings": "Interleaving retrieval with CoT helps ground intermediate reasoning steps in external facts and can improve faithfulness of explanations and answers; success depends on retriever quality.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Combining retrieval and CoT helps address knowledge gaps and improves multi-step, knowledge-intensive reasoning by grounding intermediate steps with external information.",
            "uuid": "e8306.3",
            "source_info": {
                "paper_title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RoG",
            "name_full": "Reasoning on Graphs",
            "brief_description": "A KG-augmented method that creates interpretable reasoning paths over knowledge graphs to produce faithful LLM reasoning and answers.",
            "citation_title": "Reasoning on graphs: Faithful and interpretable large language model reasoning.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (as reported in the survey)",
            "model_description": "ChatGPT (closed-source OpenAI conversational LLM) augmented with knowledge-graph-derived reasoning paths to inform answers.",
            "reasoning_methods": [
                "knowledge-graph path construction",
                "graph-based stepwise reasoning"
            ],
            "reasoning_methods_description": "Construct reasoning paths over KG relations that serve as intermediate, interpretable steps; LLM uses these KG-derived paths to produce final answers and justifications.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey reports RoG builds reasoning paths using various relations and uses KG augmentation; no within-survey ablation explicitly comparing single-path vs multi-path modes is reported here.",
            "task_or_benchmark": "Reasoning tasks evaluated with ChatGPT in the RoG study (interpretable multi-step reasoning over KG-backed questions).",
            "performance_results": "Survey reports RoG increased ChatGPT accuracy from 66.8% to 85.7% on the reasoning tasks cited in Luo et al. (2023).",
            "qualitative_findings": "KG-created reasoning paths yield more interpretable and accurate outputs; stepwise, graph-grounded reasoning substantially improved ChatGPT's accuracy on the evaluated tasks.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Using knowledge graphs to derive faithful reasoning paths significantly increases accuracy and interpretability of LLM reasoning compared to relying solely on the model's internal knowledge.",
            "uuid": "e8306.4",
            "source_info": {
                "paper_title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MindMap",
            "name_full": "MindMap (graph-of-thoughts prompting)",
            "brief_description": "A plug-and-play prompting approach that evokes 'graph-of-thoughts' reasoning by using knowledge-graph-based prompts to guide LLMs through multi-step, structured reasoning paths.",
            "citation_title": "MindMap: Knowledge graph prompting sparks graph of thoughts in large language models.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (as used in the survey examples)",
            "model_description": "GPT-4 (OpenAI large multimodal/LLM) used with clinical KG prompts to perform domain-specific reasoning tasks.",
            "reasoning_methods": [
                "graph-of-thoughts prompting",
                "KG-augmented stepwise reasoning"
            ],
            "reasoning_methods_description": "Use KG-derived 'mind maps' (subgraphs) as prompts that structure the LLM's intermediate reasoning into a graph-like sequence of steps; enables multi-path consideration and domain grounding.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey notes MindMap evokes graph-of-thoughts, enabling multiple inference paths and self-evaluation; the survey does not present a direct ablation within itself comparing single-path vs graph-of-thoughts.",
            "task_or_benchmark": "Clinical reasoning tasks (disease diagnosis and drug recommendation) using clinical KG augmentation.",
            "performance_results": "Survey reports MindMap boosted accuracy in disease diagnosis and drug recommendation to 88.2% (as reported in Wen et al., 2023).",
            "qualitative_findings": "Graph-structured prompting with domain KGs improves domain-specific reasoning and faithfulness, yielding high accuracy for clinical tasks; beneficial for grounding and interpretability.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Graph-of-thoughts prompting that leverages KGs can substantially improve domain-specific reasoning performance and faithfulness in LLM outputs.",
            "uuid": "e8306.5",
            "source_info": {
                "paper_title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "KG-Retrieval (small LMs)",
            "name_full": "Knowledge-Graph-Augmented Retrieval for small LLMs",
            "brief_description": "Using retrieved, structured facts from knowledge graphs as context for small language models to substantially improve factual answer correctness on QA tasks.",
            "citation_title": "Knowledge-augmented language model prompting for zero-shot graph question answering.",
            "mention_or_use": "mention",
            "model_name": "Small LLMs / retrieval-augmented models (unspecified small models in surveyed works)",
            "model_description": "Smaller-parameter transformer LMs augmented at inference time with retrieved KG triples or textualized KG facts supplied as additional context.",
            "reasoning_methods": [
                "KG-augmented retrieval",
                "retrieve-and-append contextual grounding"
            ],
            "reasoning_methods_description": "Match entities in questions to KG triples, retrieve relevant triples, optionally convert triples to text, and provide them as additional context to the LLM at inference to improve factuality.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Survey references experiments (Baek et al., Sen et al., Wu et al.) showing KG-augmented retrieval benefits small models; no in-survey direct ablation between differing reasoning method diversity is reported.",
            "task_or_benchmark": "Zero-shot/knowledge-intensive question answering (KGQA and general QA).",
            "performance_results": "Survey states augmenting facts from knowledge graphs (rather than increasing model size) enhanced answer correctness by over 80% for question-answering tasks (citing Baek et al., 2023; Sen et al., 2023; Wu et al., 2023).",
            "qualitative_findings": "Providing structured KG facts dramatically improves factual answer rates for small models, but success is limited by retriever quality and coverage (especially for less-popular entities); increasing model size alone may not substitute for grounded facts.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "KG-augmented retrieval is highly effective at improving factual correctness in smaller LLMs; grounding via structured facts can outperform naive scaling for certain QA tasks.",
            "uuid": "e8306.6",
            "source_info": {
                "paper_title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.",
            "rating": 2,
            "sanitized_title": "interleaving_retrieval_with_chainofthought_reasoning_for_knowledgeintensive_multistep_questions"
        },
        {
            "paper_title": "Rethinking with retrieval: Faithful large language model.",
            "rating": 1,
            "sanitized_title": "rethinking_with_retrieval_faithful_large_language_model"
        },
        {
            "paper_title": "Reasoning on graphs: Faithful and interpretable large language model reasoning.",
            "rating": 2,
            "sanitized_title": "reasoning_on_graphs_faithful_and_interpretable_large_language_model_reasoning"
        },
        {
            "paper_title": "MindMap: Knowledge graph prompting sparks graph of thoughts in large language models.",
            "rating": 2,
            "sanitized_title": "mindmap_knowledge_graph_prompting_sparks_graph_of_thoughts_in_large_language_models"
        },
        {
            "paper_title": "Knowledge-augmented language model prompting for zero-shot graph question answering.",
            "rating": 2,
            "sanitized_title": "knowledgeaugmented_language_model_prompting_for_zeroshot_graph_question_answering"
        }
    ],
    "cost": 0.011808099999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
16 Mar 2024</p>
<p>Garima Agrawal garima.agrawal@asu.edu 
Arizona State University</p>
<p>Tharindu Kumarage 
Arizona State University</p>
<p>Zeyad Alghamdi zalgham1@asu.edu 
Arizona State University</p>
<p>Huan Liu huanliu@asu.edu 
Arizona State University</p>
<p>Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
16 Mar 2024F6A1F3583372A6CD809703B2ABD50B22arXiv:2311.07914v2[cs.CL]Knowledge-aware Training ( § 32) Fine-tuning ( § 322)
The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models.To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy.Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results.In this survey, we comprehensively review these knowledgegraph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations.We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations.Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.</p>
<p>Introduction</p>
<p>Large language models (LLMs) seek to emulate human intelligence through statistical training on extensive datasets (Huang and Chang, 2022).LLMs operate on input text to predict the subsequent token or word in the sequence while identifying patterns and connections between words and phrases, aiming to comprehend and generate human-like text.Due to their stochastic decoding processes, i.e., sampling the next token in the sequence, these models exhibit probabilistic behavior, potentially yielding varied outputs or predictions for the same input across different instances.Additionally, if the training data includes misinformation, biases, or inaccuracies, these flaws may be mirrored or amplified in the content produced by these models.LLMs also face challenges in accurately interpreting phrases or terms when the context is vague and resides in a knowledge gap region of the model, leading to outputs that may sound plausible but are often irrelevant or incorrect (Ji et al., 2023;Lenat and Marcus, 2023).This phenomenon, often termed "hallucinations," undermines the reliability of these models (Mallen et al., 2023).</p>
<p>Addressing the issue of hallucinations in these models is challenging due to their inherent probabilistic nature.To effectively tackle this issue, there have been continuous research efforts in making knowledge updates and model tuning (Zhang et al., 2023c;Mialon et al., 2023;Petroni et al., 2019).However, adding random information does not improve the model's interpretation and reasoning capabilities.Instead, providing more granular and contextually relevant, precise external knowledge can significantly aid the model in recalling essential information (Jiang et al., 2020).</p>
<p>One emerging research trend is enhancing LLMs through integrating knowledge representation tools such as knowledge graphs (KGs) (Mruthyunjaya et al., 2023).Zheng et al. (Zheng et al., 2023) demonstrate that augmenting these models with comprehensive external knowledge from KGs can boost their performance and facilitate a more robust reasoning process.The strategies for enhancing LLMs with KGs can be grouped into three main categories, each uniquely contributing to the refinement of the model as shown in Figure 1: enhanc- In this survey, we critically review KG augmentation methods used in specific stages to reduce hallucinations in LLMs and improve their performance and reliability.In Section 3, we classify these methods into three overarching categories:</p>
<p>(1) Knowledge-Aware Inference, (2) Knowledge-Aware Learning, and (3) Knowledge-Aware Validation.Additionally, in Section 4, we evaluate the empirical efficacy of these methods and discuss current research trends, followed by suggestions for potential future research directions.</p>
<p>Related Works: There are several related surveys which discuss LLM augmentation using external knowledge (Hu et al., 2023;Yin et al., 2022;AlKhamissi et al., 2022;Ye et al., 2022;Wei et al., 2021;Liang et al., 2022;Zhang et al., 2023c;Mialon et al., 2023).However, to our knowledge, this is the first survey to exclusively focus on critically reviewing LLM augmentation methods utilizing structured knowledge from knowledge graphs.Specifically, our emphasis is on addressing hallucinations in LLMs through KG integration.</p>
<p>Preliminaries</p>
<p>We now introduce the preliminaries and definitions that will be used throughout the survey.</p>
<p>Large Language Models</p>
<p>Language modeling, a key task in natural language processing (NLP), focuses on understanding language's structure and generating text.It has gained importance over recent years.Specifically, in neural probabilistic language models (Bengio et al., 2000), the goal is to estimate the likelihood of a text sequence.It involves computing the probability of each token x i in the sequence, considering preceding tokens, using the chain rule to simplify the process.
p(x) = N i=1 p(x i |x 1 , x 1 ...x i−1 )(1)
The introduction of the transformer architecture (Vaswani et al., 2017) significantly advanced neural probabilistic language models, enabling efficient parallel processing and recognition of long-range dependencies in text.Coupled with training advancements like instruction tuning and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), these neural probabilistic language models led to the creation of advanced Large Language Models (LLMs) like GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023), and PaLM (Chowdhery et al., 2022), notable for their exceptional language capabilities.</p>
<p>Knowledge Graphs</p>
<p>Knowledge graphs (KGs) organize information into a structured format, capturing relationships between real-world entities, making it comprehensible to both humans and machines (Hogan et al., 2021).They store data as triples in a graph, with nodes representing entities (like people or places) and edges depicting relationships.Their capacity to represent complex interrelations makes them applicable in various domains (Fensel et al., 2020).KGs are used in a semantic search to enhance search engines semantic understanding (Singhal, 2012), enterprise knowledge management (Deng et al., 2023b), supply chain optimization (Deng et al., 2023a), education (Agrawal et al., 2022), financial fraud detection (Mao et al., 2022), cybersecurity (Agrawal et al., 2023b), recommendation systems (Guo et al., 2020), and QA systems (Agrawal et al., 2023a;Omar et al., 2023;Jiang et al., 2021).</p>
<p>Knowledge Graph-Enhanced LLMs</p>
<p>The LLMs primarily have three points of failure: a failure to comprehend the question due to lack of context, insufficient knowledge to respond accurately, or an inability to recall specific facts.Improving the cognitive capabilities of these models involves refining their inference-making process, optimizing learning mechanisms, and establishing a mechanism to validate results.This survey comprehensively reviews existing methodologies aimed at mitigating hallucinations and enhancing the reasoning capabilities of LLMs through the augmentation of KGs using these three techniques.We classify them as Knowledge-Aware Inference, Knowledge-Aware Learning, and Knowledge-Aware Validation. Figure 2 details key works from each of these categories.</p>
<p>Knowledge-Aware Inference</p>
<p>In LLMs, "inference" means generating text or predictions from a pre-trained model based on an input context.Challenges include incorrect or suboptimal outputs due to ambiguous inputs, unclear context, knowledge gaps, training data biases, or inability to generalize to unseen scenarios.LLMs often struggle with multi-step reasoning and, unlike humans, can not seek extra information to clarify ambiguous queries.To improve LLMs' inference and reasoning, researchers integrate KGs for structured symbolic knowledge, primarily by incorporating them at the input level to enhance contextual understanding.These methods, are further categorized into 'KG-Augmented Retrieval,' 'KG-Augmented Reasoning,' and 'KG-Controlled Generation.'</p>
<p>KG-Augmented Retrieval</p>
<p>Retrieval-augmented generation models like RAG (Lewis et al., 2020) and RALM (Ram et al., 2023) enhance LLMs' contextual awareness for knowledge-intensive tasks by providing relevant documents during generation, reducing hallucination without altering the LLM architecture.These methods, which are helpful for tasks needing external knowledge, augment top-k relevant documents to inputs.However, as shown in Figure 3, using well-organized, curated knowledge from structured sources or knowledge graphs, aligns more closely with factual accuracy.Baek et al. (Baek et al., 2023) introduced KAPING, which matches entities in questions to retrieve related triples from knowledge graphs for zero-shot question answering.Wu et al. (Wu et al., 2023) found that converting these triples into textualized statements enhances LLM performance.Sen et al. (Sen et al., 2023) developed a retriever module trained on a KGQA model, addressing the inadequacy of similarity-based retrieval for complex questions.StructGPT (Jiang et al., 2023) augments LLMs with data from knowledge graphs, tables, and databases, utilizing structured queries for information extraction.Other notable works include IAG (Zhang et al., 2023b), KICGPT (Wei et al., 2023), and SAFARI (Wang et al., 2023b).</p>
<p>LLMs serve as natural language interfaces, ex-Figure 3: Knowledge-aware inference by incorporating KG-augmented retrieval (Baek et al., 2023).</p>
<p>tracting and generating information without relying on their internal knowledge.Tools like the ChatGPT plugin use Langchain (Chase, 2022) and LlamaIndex (Liu, 2022) to integrate external data, prompting LLMs for context-retrieved, knowledgeaugmented outputs.However, relying solely on internal databases can limit performance due to restricted knowledge bases.Mallen et al. (Mallen et al., 2023) investigated LLMs' factual knowledge retention, finding that augmenting with retrieved data improves performance.However, these models perform well with popular entities and relations but face challenges with less popular subjects, and increasing model size doesn't improve their performance in such cases.</p>
<p>KG-Augmented Reasoning</p>
<p>KG-augmented retrieval methods effectively answer factual questions.However, questions that require reasoning call for more proficient approaches, such as decomposing complex, multi-step tasks into manageable sub-queries, as detailed by (Qiao et al., 2022;Liu et al., 2023).These techniques are referred to as KG-augmented reasoning methods in our study.Following the intuition behind the human reasoning process, the Chain of Thought (CoT) (Wei et al., 2022a), Chain of Thought with Self-Consistency (CoT-SC) (Wang et al., 2022), Program-Aided Language Model (PAL) (Gao et al., 2023), and Reason and Act (ReAct) (Yao et al., 2022), Reflexion (Shinn et al., 2023) methods used a series of intermediate reasoning steps to improve the complex reasoning ability of LLMs.These methods mimic human step-by-step reasoning, aiding in understanding and debugging the model's reasoning process.They are useful for math problems, commonsense reasoning, and symbolic tasks solvable through language-explained steps.Tree of Thoughts(ToT) (Yao et al., 2023) method enhances this by exploring coherent text units as intermediate steps, enabling LLMs to consider multiple paths, self-evaluate, and make informed decisions.Different knowledge augmentation techniques using knowledge graphs, inspired by CoT and ToT prompting, enhance reasoning in domainspecific and open-domain tasks."Rethinking with Retrieval" (He et al., 2022) model uses decomposed reasoning steps from chain-of-thought prompting to retrieve external knowledge, leading to more accurate and faithful explanations.IR-CoT (Trivedi et al., 2022) interleaves generating chain-of-thoughts (CoT) and retrieving knowledge from graphs, iteratively guiding retrieval and reasoning for multi-step questions.MindMap (Wen et al., 2023) introduces a plug-and-play approach to evoke graph-of-thoughts reasoning in LLMs.Reasoning on Graphs (RoG) (Luo et al., 2023) uses knowledge graphs to create faithful reasoning paths based on various relations, enabling interpretable and accurate reasoning in LLMs.Complementary advancements include MoT (Li and Qiu, 2023), Democratizing Reasoning (Wang et al., 2023c), Re-CEval (Prasad et al., 2023), RAP (Hao et al., 2023), EoT (Yin et al., 2023b) and Tree Prompting (Singh et al., 2023), each contributing uniquely to the development of reasoning capabilities in LLMs.</p>
<p>Exploring the interaction between prompts and large language models in the context of reasoning tasks is an exciting research avenue (Liu et al., 2023).A crucial aspect is the design of prompts tailored to the specific use case.However, the fundamental question of whether neural networks genuinely engage in "reasoning" remains unanswered, and it is uncertain whether following the correct reasoning path always leads to accurate answers (Qiao et al., 2022;Jiang et al., 2020).</p>
<p>Knowledge-Controlled Generation</p>
<p>These methods generate knowledge using a language model and then use probing or API calls for tasks.Liu et al. (Liu et al., 2021) used a second model to produce question-related knowledge statements for deductions.Binder (Cheng et al., 2022) uses Codex to parse context and generate task API calls.KB-Binder (Li et al., 2023) also employs Codex to create logical drafts for questions, integrating knowledge graphs for complete answers.Brate et al. (Brate et al., 2022) create cloze-style prompts for entities in knowledge graphs, enhancing them with auxiliary data via SPARQL queries, improving recall and accuracy.KnowPrompt (Chen et al., 2022) generates prompts from a pre-trained model and tunes them for relation extraction in cloze-style tasks.BeamQA (Atif et al., 2023) uses a language model to generate inference paths for knowledge graph embeddingbased search in link prediction.ALCUNA (Yin et al., 2023a) and PRCA (Yang et al., 2023) are other significant methods in controlled generation.</p>
<p>Guardrails in generative AI set operational boundaries for models, ensuring safe and secure output generation.NeMo guardrails (Rebedea et al., 2023) by Nvidia guide conversational flows in enterprise applications to meet safety and secu-rity standards.Knowledge-controlled generation ensures alignment with facts and prevents misinformation.Knowledge graph ontologies can provide specific domain constraints, aiding LLMs in defining output generation boundaries.</p>
<p>Knowledge-Aware Training</p>
<p>Another stage where we can address hallucination issues in LLMs is to utilize KGs to optimize their learning either by improving the quality of training data at the model pre-training stage or by finetuning the pre-trained language model (PLM) to adapt to specific tasks or domains.We classify these methods as Knowledge-Aware Pre-Training and Knowledge-Aware Fine-Tuning.</p>
<p>Knowledge-Aware Pre-Training</p>
<p>Training data quality and diversity are crucial for reducing hallucinations in LLMs.Integrating knowledge graphs, which provide structured information about entities and their interconnections, improves the comprehension abilities of LLMs and aids in generating text that more accurately reflects the complexities of real-world entities.However, training from scratch is highly resource-heavy and expensive.Different approaches were proposed by researchers (Yu et al., 2023;Fu et al., 2023;Deng et al., 2023b;Liu et al., 2020;Poerner et al., 2019;Peters et al., 2019) for pre-training models by augmenting knowledge graphs in training data.We further categorize them as follows:</p>
<ol>
<li>
<p>Knowledge-Enhanced Models: These methods enriched the large-scale text corpora with KGs for improved language representation.ERNIE (Zhang et al., 2019)   3. Knowledge-Fusion: These methods integrates the KGs into LLMs using graph query encoders (Wang et al., 2021;Ke et al., 2021;He et al., 2019).As shown in Figure 4, JointLK (Sun et al., 2021b) employed knowledge fusion and joint reasoning for commonsense question answering, selectively using relevant KG nodes and synchronizing updates between text and graph encoders.LKPNR (Runfeng et al., 2023) combined LLMs with KGs, enhancing semantic understanding in complex news texts to create a personalized news recommendation framework through a KG-augmented encoder.</p>
</li>
<li>
<p>Knowledge-Probing: Knowledge probing involves examining language models to assess their factual and commonsense knowledge (Petroni et al., 2019).This process aids in evaluating and enhancing the models (Kassner et al., 2021;Swamy et al., 2021).Rewirethen-Probe (Meng et al., 2021) introduced a self-supervised contrastive-probing approach, utilizing biomedical knowledge graphs to learn language representations.</p>
</li>
</ol>
<p>Knowledge-Aware Fine-Tuning</p>
<p>Fine-tuning adapts LLMs to specific domains by training them on relevant datasets, using selected architectures and hyper-parameters to modify the model's weights for improved task performance (Guu et al., 2020;Hu et al., 2021;Lu et al., 2022;Dettmers et al., 2023).KGs can further tune these models to update and expand their internal knowledge for domain-specific tasks like custom named-entity recognition (Agrawal et al., 2023b), and text summarization (Kang et al., 2022a).SKILL (Moiseev et al., 2022) used synthetic sentences converted from WikiData (Seminar et al., 2019) and KELM (Agarwal et al., 2020) used KGs to fine-tune the pre-trained model checkpoints.KGLM (Youn and Tagkopoulos, 2022) employed an entity-relation embedding layer with KG triples for link prediction tasks.Cross-lingual reasoning (Foroutan et al., 2023) improved by fine-tuning MultiLM, mBERT, and mT5 models with logical datasets using a self-attention network.LLMs improve more with additional training using datasets with few-shot CoT reasoning prompts and finetuning (Kim et al., 2023;Huang et al., 2022).</p>
<p>Fine-tuning language models like ChatGPT, limited by their last knowledge update in 2021, is more efficient than training from scratch.It handles queries beyond this cutoff using a curated, domainspecific knowledge graph.The extent to which updated knowledge is integrated into the model remains to be determined.Onoe et al.'s (Onoe et al., 2023) evaluation framework indicate that while models can recall facts about new entities, inferring based on these is harder.The effect of updating knowledge on existing entities is still an open research question.</p>
<p>Knowledge-Aware Validation</p>
<p>The third category type uses structured data as a fact-checking mechanism and provides a reference for the model to verify information.Knowledge graphs can provide comprehensive explanations and can be used to justify the models' decisions.These methods also help enforce consistency across the facts, obviating the necessity for laborious human-annotated data and enhancing the reliability of generated content.</p>
<p>The fact-aware language model, KGLM (Logan IV et al., 2019), referred to a knowledge graph to generate entities and facts relevant to the context.SURGE (Kang et al., 2022b) retrieves high simi-larity context-relevant triples as a sub-graph from a knowledge graph."Text critic" classifier (Lango and Dušek, 2023) was proposed to guide the generation by assessing the match between the input data and the generated text.FOLK (Wang and Shu, 2023) used first-order-logic (FOL) predicates for claim verification in online misinformation.Beyond verification, FOLK generates explicit explanations, providing valuable assistance to human fact-checkers in understanding and interpreting the model's decisions.This approach contributes to the accuracy and interpretability of the model's outputs in the context of misinformation detection.</p>
<p>Discussion, Challenges and Future</p>
<p>In this section, we examine the effectiveness of KGenhanced LLM techniques in reducing hallucinations and enhancing performance and reliability in LLMs.We also identify key challenges associated with each method and propose potential research avenues in this evolving field.</p>
<p>Resources</p>
<p>Table 1 details the key features of different KGenhanced LLM methods, emphasizing their application in specific industries using domain-specific knowledge graphs.The inference methods used general knowledge and commonsense reasoning datasets for QA tasks without requiring LLM retraining.Mindmap (Wen et al., 2023) demonstrated an application in healthcare, augmenting clinical datasets with GPT-4.Meng et al. (Meng et al., 2021) pre-trained T5 and BART models using a biomedical knowledge graph, Unified Medical Language System (UMLS) Metathesaurus.LKPNR (Runfeng et al., 2023) pre-trained LM and graph encoders on MIND-200K user click logs to provide personalized news recommendations.Martino et al. (Martino et al., 2023) used knowledge injection to reduce hallucinations when responding to online customer reviews for a retail store.Dong et al. (Dong et al., 2022) showed improvement in the faithfulness of text summarization tasks to the source documents by linking external source knowledge bases from the source.Baldazzi (Baldazzi et al., 2023) fine-tuned T5-large on financial customer-service enterprise KG.</p>
<p>Evaluation Metrics</p>
<p>Various criteria were applied to assess the effectiveness of knowledge graph augmentation in reducing hallucinations in LLMs.Accuracy: Accuracy comparison with and without augmented knowledge from KGs (Baek et al., 2023;Zhang et al., 2023b).</p>
<p>Top-K and MRR: Retrieval performance was measured by the relevance of retrieved triples for generating answers.Mean Reciprocal Rank (MRR) and Top-K accuracy determined the ranks of correctly retrieved answer-containing triples (Baek et al., 2023;Sen et al., 2023).The effectiveness of KG triples was assessed as either "Helpful" or "Harmful" and compared against scenarios where "no knowledge" was provided (Wu et al., 2023).Hits@1: Evaluates answer accuracy and examines the coverage of multi-choice question answers (Luo et al., 2023;Wu et al., 2023;Wei et al., 2023).Execution Accuracy (EA): The controlled generation method, such as Binder (Cheng et al., 2022), uses Execution Accuracy (EA) as a metrics to measure the accuracy in semantic parsing, API call generation, and the success rate of code execution.</p>
<p>Exact Match (EM): Model's performance after fine-tuning was evaluated using EM (Exact Match) scores on test sets (Moiseev et al., 2022).</p>
<p>Human Evaluation: Validation methods were manually evaluated to assess the explanation quality, coverage, logical soundness, fluency, and factual accuracy of sentence completion (Wang and Shu, 2023;Kang et al., 2022b).It is pertinent to consider evaluating factuality from different aspects, first verifying the presence of accurate and reliable information and second identifying any instances of fabricated or "hallucinated" information.</p>
<p>Performance Analysis</p>
<p>Retrieved facts enhance small LLMs: Smaller models, due to their limited parameter spaces, struggle to incorporate extensive knowledge in pretraining.Augmenting facts from knowledge graphs, rather than increasing model size, enhanced answer correctness by over 80% for question-answering tasks (Baek et al., 2023;Sen et al., 2023;Wu et al., 2023).However, the success of these methods with complex queries heavily relies on the retriever modules, whose capabilities are limited to the knowledge graph (BehnamGhader et al., 2022).</p>
<p>Step-wise reasoning more effective in larger models: Variations of CoT methods offer costeffective control and task-specific tuning, enhancing model performance.For instance, RoG (Luo et al., 2023) reported an increase in ChatGPT's accuracy from 66.8% to 85.7% in reasoning tasks with knowledge graph augmentation.Similarly, Mindmap (Wen et al., 2023) boosted accuracy in disease diagnosis and drug recommendation to 88.2% using a clinical reasoning graph.</p>
<p>Controlled generation boosts the performance: Knowledge-controlled generation methods surpass baseline models in accuracy and contextual relevance, enhancing their ability to handle diverse queries (Chen et al., 2022;Cheng et al., 2022;Atif et al., 2023).However, these methods can vary in quality and are sometimes prone to generating incorrect or irrelevant information.</p>
<p>Pre-training and fine-tuning costly: Pretraining and fine-tuning significantly enhance domain-specific task performance.However, these improvements require substantial computational resources, as shown in Table 1.Additionally, finetuning's data-dependency makes it task-specific and limits its transferability and generalizability (Gueta et al., 2023;Wang and Shu, 2023).</p>
<p>Fact-checking ensures reliability: Knowledge validation through fact-checking reduces hallucinations by checking model-generated data against a knowledge graph, but it increases computational load and may miss some inaccuracies (Kang et al., 2022b;Lango and Dušek, 2023).The effectiveness of knowledge augmentation is also influenced by the size of the knowledge graph and its impact on query responses.Standard approaches include fine-tuning pre-trained models for reliability but at a higher cost, and examplebased prompting, less effective in certain reasoning tasks (Brown et al., 2020;Rae et al., 2021).Zhang et al. (Zhang et al., 2023a) noted that language model inconsistencies often arise from incorrect context usage.Method selection depends on the specific use case and available resources.Wang et al. (Wang et al., 2023a) showed that pre-training decoder-only LLMs with retrieval can improve factual accuracy in knowledge-intensive tasks, while Shi et al. (Shi et al., 2023) developed GraphNarrative, a dataset aimed at reducing hallucinations, beneficial for fine-tuning LLMs.</p>
<p>Trend Analysis</p>
<p>Figure 5 shows the research trends using different knowledge-graph augmentation techniques from 2019 to 2023.The bubble size here represents the number of papers for each knowledge-graph augmentation category, ranging from one to eight.Pre-training methods by adding knowledge graphs to the training corpus were predominant in the early years of language model development.After the extensive GPT series of LLMs, retraining the huge model with billions of parameters became impractical and resource-intensive.More efforts were made to fine-tune the models with task-specific data without training from scratch.Very recently, there has been a shift towards using knowledgeaugmented retrieval, reasoning, generation, and validation methods without incurring additional training costs.Mixture of Experts (MoE) LLMs: Efforts are underway to optimize the MoE architecture to scale LLMs and increase their capacity without increasing computation (Zhou et al., 2022).Integrating MoE with knowledge graphs (Yu et al., 2022) can develop adaptive learning strategies for context-based expert utilization and improve the interpretability and transparency of MoE-LLMs.</p>
<p>Symbolic-Subsymbolic Unification: Knowledge fabrics, such as symbolic KGs and sub-symbolic vectors, enables versatile reasoning in LLMs, mimicking human mind's capacity to reconcile structured theories (Núñez-Molina et al., 2023).</p>
<p>Synergizing LLM and KG: LLMs are being used for link prediction and knowledge graph completion (Xiao et al., 2023;Veseli et al., 2023).</p>
<p>Synergizing the LLM and KGs is a potential direction where both components can mutually each other's capabilities through a bidirectional reasoning process driven by a harmonious blend of data and knowledge (Pan et al., 2023).</p>
<p>Causality-Awareness: Incorporating causality into knowledge graphs, (Wei et al., 2022b), will enhance Large Language Models' (LLMs) capability to grasp causation rather than merely identifying correlations.This advancement will equip LLMs with a better understanding of the causal relationships between events or entities, significantly improving their reasoning and predictive capabilities.</p>
<p>The progress of KGs promises to greatly enhance LLMs, making them more relevant, responsive, and accurate.This aims to create more reliable and trustworthy language models, advancing robust and responsible AI systems.</p>
<p>Conclusion</p>
<p>In this survey, we systematically investigate the integration of KGs into LLMs to mitigate hallucinations and improve reasoning accuracy.We emphasize the benefits of using KGs to enhance LLM performance across various phases at inference, model training, and output verification stages.While substantial progress has been made, we emphasize the need for continuous innovation and propose future directions to facilitate the development of more advanced KG-augmented LLMs.</p>
<p>Limitations</p>
<p>In this paper, we conduct a comprehensive review of knowledge-graph-based augmentation techniques in LLMs, with a specific focus on their ability to address hallucinations.We identify commonalities among these techniques and categorize them into three distinct groups based on their mechanisms and approaches.Furthermore, we systematically assess the performance of these methods.In Section 1, we compare our work with existing related surveys and we will continue adding more related approaches.However, it's important to acknowledge that despite our diligent efforts, there may be certain limitations that still exist in this paper.</p>
<p>References and Methods.Due to page limitations, we may not include all relevant references and detailed technical information.Our study primarily focuses on state-of-the-art methods developed between 2019 and 2023, sourced primarily from reputable conferences and platforms such as ACL, EMNLP, NAACL, ICLR, ICML, and arXiv.We remain committed to keeping our work up-to-date.</p>
<p>Taxonomy and Comparison.We primarily categorized the methods based on their primary augmentation approach.In some cases, hybrid studies incorporating multiple approaches may be categorized differently, depending on specific criteria.It's essential to note that our analysis is based on the performance of existing works using the current experiments and datasets.Given the rapid evolution in this field, benchmarks and baseline models may change, potentially leading to variations in these evaluations.</p>
<p>Figure 1 :
1
Figure 1: Knowledge Graphs (KG) employed to reduce hallucinations in LLMs at different stages.</p>
<p>Figure 2 :
2
Figure 2: Taxonomy of Knowledge Graph-Augmented Large Language Models</p>
<p>Figure 4 :
4
Figure 4: Knowledge-aware Pre-training by Knowledge Fusion (Sun et al., 2021b).</p>
<p>Figure 5 :
5
Figure 5: Research trend over years-The bubble size represents number of papers we observed for each knowledge-graph augmentation categories: smallest size (#papers=1), largest size (#papers=8)</p>
<p>Table 1 :
1
Comparison attributes of Knowledge Graph-enhanced LLM methods
Comparison Attributes
AcknowledgementsThis material is based upon work supported by the National Science Foundation under Grant No. 2114789.
Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou, arXiv:2010.126882020arXiv preprint</p>
<p>Auction-based learning for question answering over knowledge graphs. Garima Agrawal, Dimitri Bertsekas, Huan Liu, Information. 1463362023a</p>
<p>Building knowledge graphs from unstructured texts: Applications and impact analyses in cybersecurity education. Garima Agrawal, Yuli Deng, Jongchan Park, Huan Liu, Ying-Chih Chen, Information. 13115262022</p>
<p>Aiseckg: Knowledge graph dataset for cybersecurity education. Garima Agrawal, Kuntal Pal, Yuli Deng, Huan Liu, Chitta Baral, AAAI-MAKE 2023: Challenges Requiring the Combination of Machine Learning. 2023b. 2023</p>
<p>Badr Alkhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, Marjan Ghazvininejad, arXiv:2204.06031A review on language models as knowledge bases. 2022arXiv preprint</p>
<p>Beamqa: Multi-hop knowledge graph question answering with sequence-to-sequence prediction and beam search. Farah Atif, Ola El Khatib, Djellel Difallah, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Knowledge-augmented language model prompting for zero-shot graph question answering. Jinheon Baek, Alham Fikri Aji, Amir Saffari, arXiv:2306.041362023arXiv preprint</p>
<p>Teodoro Baldazzi, Luigi Bellomarini, Stefano Ceri, Andrea Colombo, Andrea Gentili, Emanuel Sallinger, arXiv:2306.10723Fine-tuning large enterprise language models via ontological reasoning. 2023arXiv preprint</p>
<p>Can retriever-augmented language models reason? the blame game between the retriever and the language model. Parishad Behnamghader, Santiago Miret, Siva Reddy, arXiv:2212.091462022arXiv preprint</p>
<p>A neural probabilistic language model. Advances in neural information processing systems. Yoshua Bengio, Réjean Ducharme, Pascal Vincent, 200013</p>
<p>Improving model predictions via prompts enriched with knowledge graphs. Ryan Brate, Minh-Hoang Dang, Fabian Hoppe, Yuan He, DL4KG@ ISWC2022. 2022Albert Meroño-Peñuela, and Vijay Sadashivaiah</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>. Harrison Chase, 2022LangChain</p>
<p>Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction. Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen, Proceedings of the ACM Web conference 2022. the ACM Web conference 20222022</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, arXiv:2210.028752022arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Research on the construction of event logic knowledge graph of supply chain management. Jianfeng Deng, Chong Chen, Xinyi Huang, Wenyan Chen, Lianglun Cheng, Advanced Engineering Informatics. 561019212023a</p>
<p>Construction and applications of billion-scale pre-trained multimodal business knowledge graph. Shumin Deng, Chengming Wang, Zhoubo Li, Ningyu Zhang, Zelin Dai, Hehong Chen, Feiyu Xiong, Ming Yan, Qiang Chen, Mosha Chen, 2023 IEEE 39th International Conference on Data Engineering (ICDE). IEEE2023b</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2305.14314Qlora: Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Faithful to the document or to the world? mitigating hallucinations via entity-linked knowledge in abstractive summarization. Yue Dong, John Wieting, Pat Verga, arXiv:2204.137612022arXiv preprint</p>
<p>Dieter Fensel, Umutcan Şimşek, Kevin Angele, Elwin Huaman, Elias Kärle, Oleksandra Panasiuk, Ioan Toma, Jürgen Umbrich, Alexander Wahler, Dieter Fensel, Why we need knowledge graphs: Applications. Knowledge Graphs: Methodology, Tools and Selected Use Cases. 2020</p>
<p>Breaking the language barrier: Improving cross-lingual reasoning with structured self-attention. Negar Foroutan, Mohammadreza Banaei, Karl Aberer, Antoine Bosselut, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Revisiting the knowledge injection frameworks. Peng Fu, Yiming Zhang, Haobo Wang, Weikang Qiu, Junbo Zhao, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Knowledge is a region in weight space for fine-tuned language models. Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, Leshem Choshen, arXiv:2302.048632023arXiv preprint</p>
<p>A survey on knowledge graph-based recommender systems. Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, Qing He, IEEE Transactions on Knowledge and Data Engineering. 3482020</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, International conference on machine learning. PMLR2020Panupong Pasupat, and Mingwei Chang</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Integrating graph contextualized knowledge into pre-trained language models. Bin He, Di Zhou, Jinghui Xiao, Qun Liu, Nicholas Jing Yuan, Tong Xu, arXiv:1912.001472019arXiv preprint</p>
<p>Hangfeng He, Hongming Zhang, Dan Roth, arXiv:2301.00303Rethinking with retrieval: Faithful large language model. 2022arXiv preprint</p>
<p>Knowledge graphs. Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia Amato, Gerard De Melo, Claudio Gutierrez, Sabrina Kirrane, José Emilio Labra, Roberto Gayo, Sebastian Navigli, Neumaier, ACM Computing Surveys (CSUR). 5442021</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>A survey of knowledge enhanced pre-trained language models. Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, Juanzi Li, IEEE Transactions on Knowledge and Data Engineering. 2023</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.09645Structgpt: A general framework for large language model to reason over structured data. 2023arXiv preprint</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020Association for Computational Linguistics8</p>
<p>Research on medical question answering system based on knowledge graph. Zhixue Jiang, Chengying Chi, Yunyun Zhan, IEEE Access. 92021</p>
<p>Minki Kang, arXiv:2204.10555Jinheon Baek, and Sung Ju Hwang. 2022a. Kala: knowledge-augmented language model adaptation. arXiv preprint</p>
<p>Knowledge-consistent dialogue generation with knowledge graphs. Minki Kang, Jin Myung Kwak, Jinheon Baek, Sung Ju Hwang, ICML 2022 Workshop on Knowledge Retrieval and Language Models. 2022b</p>
<p>Nora Kassner, Philipp Dufter, Hinrich Schütze, arXiv:2102.00894Multilingual lama: Investigating knowledge in multilingual pretrained language models. 2021arXiv preprint</p>
<p>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, Minlie Huang, arXiv:2106.105022021arXiv preprint</p>
<p>The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning. Seungone Kim, June Se, Doyoung Joo, Joel Kim, Seonghyeon Jang, Jamin Ye, Minjoon Shin, Seo, arXiv:2305.140452023arXiv preprint</p>
<p>Critic-driven decoding for mitigating hallucinations in data-to-text generation. Mateusz Lango, Ondřej Dušek, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Doug Lenat, Gary Marcus, arXiv:2308.04445Getting from generative ai to trustworthy ai: What llms might learn from cyc. 2023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Few-shot in-context learning for knowledge base question answering. Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, Wenhu Chen, arXiv:2305.017502023arXiv preprint</p>
<p>Mot: Memory-ofthought enables chatgpt to self-improve. Xiaonan Li, Xipeng Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun, arXiv:2212.05767Reasoning over different types of knowledge graphs: Static, temporal and multi-modal. 2022arXiv preprint</p>
<p>. Jerry Liu, 10.5281/zenodo.12342022LlamaIndex</p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Le Ronan, Yejin Bras, Hannaneh Choi, Hajishirzi, arXiv:2110.08387Generated knowledge prompting for commonsense reasoning. 2021arXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>K-bert: Enabling language representation with knowledge graph. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, Ping Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Barack's wife hillary: Using knowledge-graphs for fact-aware language modeling. I V Robert L Logan, Nelson F Liu, Matthew E Peters, Matt Gardner, Sameer Singh, arXiv:1906.07241arXiv preprint</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, arXiv:2209.146102022arXiv preprint</p>
<p>Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan, arXiv:2310.01061Reasoning on graphs: Faithful and interpretable large language model reasoning. 2023arXiv preprint</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023</p>
<p>Financial fraud detection using the relatedparty transaction knowledge graph. Xuting Mao, Hao Sun, Xiaoqian Zhu, Li , Procedia Computer Science. 1992022</p>
<p>Knowledge injection to counter large language model (llm) hallucination. Ariana Martino, Michael Iannelli, Coleen Truong, European Semantic Web Conference. Springer2023</p>
<p>Rewirethen-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models. Zaiqiao Meng, Fangyu Liu, Ehsan Shareghi, Yixuan Su, Charlotte Collins, Nigel Collier, arXiv:2110.081732021arXiv preprint</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Celikyilmaz, arXiv:2302.07842arXiv:2205.08184Skill: structured knowledge infusion for large language models. Fedor Moiseev, Zhe Dong, Enrique Alfonseca, Martin Jaggi, 2023. 2022arXiv preprint</p>
<p>Rethinking language models as symbolic knowledge graphs. Vishwas Mruthyunjaya, Pouya Pezeshkpour, Estevam Hruschka, Nikita Bhutani, arXiv:2308.136762023arXiv preprint</p>
<p>A review of symbolic, subsymbolic and hybrid methods for sequential decision making. Carlos Núñez-Molina, Pablo Mesejo, Juan Fernández-Olivares, arXiv:2304.105902023arXiv preprint</p>
<p>A universal question-answering platform for knowledge graphs. Reham Omar, Ishika Dhall, Panos Kalnis, Essam Mansour, Proceedings of the ACM on Management of Data. 112023</p>
<p>Yasumasa Onoe, J Q Michael, Shankar Zhang, Greg Padmanabhan, Eunsol Durrett, Choi, arXiv:2305.01651Can lms learn new entities from descriptions? challenges in propagating injected knowledge. 2023arXiv preprint</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.08302Unifying large language models and knowledge graphs: A roadmap. 2023arXiv preprint</p>
<p>Mark Matthew E Peters, Robert L Neumann, I V Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah A Smith, arXiv:1909.04164Knowledge enhanced contextual word representations. 2019arXiv preprint</p>
<p>Anton Bakhtin. Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:1909.01066Language models as knowledge bases?. 2019arXiv preprint</p>
<p>Nina Poerner, Ulli Waltinger, Hinrich Schütze, arXiv:1911.03681E-bert: Efficient-yet-effective entity embeddings for bert. 2019arXiv preprint</p>
<p>Archiki Prasad, Swarnadeep Saha, Xiang Zhou, Mohit Bansal, arXiv:2304.10703Receval: Evaluating reasoning chains via correctness and informativeness. 2023arXiv preprint</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, arXiv:2212.09597Reasoning with language model prompting: A survey. 2022arXiv preprint</p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. 2021arXiv preprint</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, arXiv:2302.00083Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv preprint</p>
<p>Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen, arXiv:2310.10501Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails. 2023arXiv preprint</p>
<p>Corby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, Saurabh Tiwary, arXiv:2007.00655Knowledgelanguage model pretraining. 2020arXiv preprint</p>
<p>Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai, arXiv:2308.12028Lkpnr: Llm and kg for personalized news recommendation framework. 2023arXiv preprint</p>
<p>Knowledge Graphs Seminar, Nahor Gebretensae, and Heiko Paulheim. Wikidata: A free collaborative knowledge graph. 2019</p>
<p>Knowledge graph-augmented language models for complex question answering. Priyanka Sen, Sandeep Mavadia, Amir Saffari, 2023</p>
<p>Exploiting structured knowledge in text via graph-guided representation learning. Tao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, Weizhu Chen, arXiv:2004.142242020arXiv preprint</p>
<p>Hallucination mitigation in natural language generation from large-scale open-domain knowledge graphs. Xiao Shi, Zhengyuan Zhu, Zeyu Zhang, Chengkai Li, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Tree prompting: Efficient task adaptation without fine-tuning. Chandan Singh, John Morris, Alexander M Rush, Jianfeng Gao, Yuntian Deng, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Introducing the knowledge graph: things, not strings. Amit Singhal, 2012. may 2012</p>
<p>Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, arXiv:2107.021372021aarXiv preprint</p>
<p>Jointlk: Joint reasoning with language models and knowledge graphs for commonsense question answering. Yueqing Sun, Qi Shi, Le Qi, Yu Zhang, arXiv:2112.027322021barXiv preprint</p>
<p>Interpreting language models through knowledge graph extraction. Vinitra Swamy, Angelika Romanou, Martin Jaggi, arXiv:21112021arXiv preprint</p>
<p>Can Hao Tian, Xinyan Gao, Hao Xiao, Bolei Liu, Hua He, Haifeng Wu, Feng Wang, Wu, arXiv:2005.05635Skep: Sentiment knowledge enhanced pre-training for sentiment analysis. 2020arXiv preprint</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, arXiv:2212.105092022arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Evaluating the knowledge base completion potential of gpt. Blerta Veseli, Simon Razniewski, Jan-Christoph Kalo, Gerhard Weikum, Findings of EMNLP 2023. 2023</p>
<p>Boxin Wang, Wei Ping, Peng Xu, Lawrence Mcafee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, arXiv:2304.06762Shall we pretrain autoregressive language models with retrieval? a comprehensive study. 2023aarXiv preprint</p>
<p>Explainable claim verification via knowledge-grounded reasoning with large language models. Haoran Wang, Kai Shu, arXiv:2310.052532023arXiv preprint</p>
<p>Large language models as source planner for personalized knowledge-grounded dialogue. Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan, Irwin King, Kam-Fai Wong, arXiv:2310.088402023barXiv preprint</p>
<p>Kepler: A unified model for knowledge embedding and pre-trained language representation. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang, Transactions of the Association for Computational Linguistics. 20219</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Democratizing reasoning ability: Tailored learning from large language model. Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023c</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Knowledge enhanced language models: A compreshensive survey. Xiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, Andrew Arnold, arXiv:2110.084552021arXiv preprint</p>
<p>Kicgpt: Large language model with knowledge in context for knowledge graph completion. Yanbin Wei, Qiushi Huang, Yu Zhang, James Kwok, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Causal inference for knowledge graph based recommendation. Yinwei Wei, Xiang Wang, Liqiang Nie, Shaoyu Li, Dingxian Wang, Tat-Seng Chua, IEEE Transactions on Knowledge and Data Engineering. 2022b</p>
<p>Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. Yilin Wen, Zifeng Wang, Jimeng Sun, arXiv:2308.097292023arXiv preprint</p>
<p>Yike Wu, Nan Hu, Guilin Qi, Sheng Bi, Jie Ren, Anhuan Xie, Wei Song, arXiv:2309.11206Retrieve-rewriteanswer: A kg-to-text enhanced llms framework for knowledge question answering. 2023arXiv preprint</p>
<p>Instructed language models with retrievers are powerful entity linkers. Zilin Xiao, Ming Gong, Jie Wu, Xingyao Zhang, Linjun Shou, Daxin Jiang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter. Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, Jing Xiao, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Hongbin Ye, Ningyu Zhang, Hui Chen, Huajun Chen, arXiv:2210.12714Generative knowledge graph construction: A review. 2022arXiv preprint</p>
<p>Li Da Yin, Hao Dong, Xiaodong Cheng, Kai-Wei Liu, Furu Chang, Jianfeng Wei, Gao, arXiv:2202.08772A survey of knowledge-intensive nlp with pre-trained language models. 2022arXiv preprint</p>
<p>Alcuna: Large language models meet new knowledge. Xunjian Yin, Baizhou Huang, Xiaojun Wan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023a</p>
<p>Exchange-of-thought: Enhancing large language model capabilities through cross-model communication. Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuan-Jing Huang, Xipeng Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023b</p>
<p>Kglm: Integrating knowledge graph structure in language models for link prediction. Jason Youn, Ilias Tagkopoulos, arXiv:2211.027442022arXiv preprint</p>
<p>Mengxia Yu, Zhihan Zhang, Wenhao Yu, Meng Jiang, arXiv:2305.14457Pre-training language models for comparative reasoning. 2023arXiv preprint</p>
<p>Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts. Wenhao Yu, Chenguang Zhu, Lianhui Qin, Zhihan Zhang, Tong Zhao, Meng Jiang, arXiv:2203.072852022arXiv preprint</p>
<p>Denghui Zhang, Zixuan Yuan, Yanchi Liu, Fuzhen Zhuang, Hui Xiong, E-Bert, Adapting bert to e-commerce with adaptive hybrid masking and neighbor product reconstruction. </p>
<p>Muru Zhang, Ofir Press, William Merrill, Alisa Liu, Noah A Smith, arXiv:2305.13534How language model hallucinations can snowball. 2023aarXiv preprint</p>
<p>Iag: Induction-augmented generation framework for answering reasoning questions. Zhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang Shi, Meng Han, Yongkang Wu, Ruofei Lai, Zhao Cao, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023b</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, arXiv:1905.07129Ernie: Enhanced language representation with informative entities. 2019arXiv preprint</p>
<p>How do large language models capture the ever-changing world knowledge? a review of recent advances. Zihan Zhang, Meng Fang, Ling Chen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingJun Wang. 2023cMohammad Reza Namazi Rad, and</p>
<p>Why does chatgpt fall short in answering questions faithfully?. Shen Zheng, Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2304.105132023arXiv preprint</p>
<p>Mixture-of-experts with expert choice routing. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, Advances in Neural Information Processing Systems. 202235</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8626 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8626</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8626</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279244946</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.04810v2.pdf" target="_blank">Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a core capability for large language models (LLMs), yet existing benchmarks that rely solely on final-answer accuracy fail to capture the quality of the reasoning process. To address this, we introduce FineLogic, a fine-grained evaluation framework that assesses logical reasoning across three dimensions: overall accuracy, stepwise soundness, and representation-level probing. Leveraging this framework, we conduct a comprehensive study on how different supervision formats in fine-tuning shape reasoning abilities. We fine-tune LLMs on four supervision styles: one in natural language and three symbolic variants. We find a key trade-off: natural language supervision excels at generalization to out-of-distribution and long-chain problems, whereas symbolic supervision is superior at instilling structurally sound, atomic reasoning steps. Furthermore, our probing analysis indicates that fine-tuning primarily refines the model's step-by-step generation process, rather than improving its ability to converge on an answer early. Together, our framework and analysis provide a more rigorous lens for evaluating and improving logical reasoning in LLMs. The code is available at https://github.com/YujunZhou/FineLogic.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8626.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8626.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A powerful commercial large language model used as a strong zero-shot baseline in the FineLogic study; evaluated on multi-step formal logical reasoning benchmarks and stepwise diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI family model used as a zero-shot/inference-time baseline in experiments; treated as a strong reference for accuracy and stepwise behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD, ProntoQA, FOLIO, Multi-LogiEval (suite of multi-step formal logic benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks focused on peer-reviewed, multi-hop formal logic problems requiring multi-step deductions in first-order style reasoning (e.g., proofs, contradictions).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot/few-shot prompting (direct prompting, CoT, few-shot exemplars) used as baselines; evaluated on final-answer accuracy, stepwise soundness, and representation-level probing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strong reference performance: on some OOD sets (Multi-LogiEval) GPT-4o's final-answer accuracy is comparable to best SFT-NL tuned small models (e.g., ~71% reported match with Llama SFT-NL). Stepwise metrics: low All-Valid/All-Atomic in Table 3 (e.g., All Valid ~7.6%, All Relevant ~56.2%, All Atomic ~4.4 under few-shot), indicating many chains are not fully stepwise-sound.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as a reference; several SFT-NL fine-tuned smaller models approach or match GPT-4o final-answer accuracy on FLD/ProntoQA/Multi-LogiEval, but GPT-4o tends to produce fewer redundant steps (high All-Relevant) while still having low All-Atomic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite high accuracy, GPT-4o's generated chains often contain non-atomic steps and occasionally redundant steps (case studies in Figure 5); final-answer correctness alone hides weak stepwise soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GPT-4o demonstrates that strong final-answer accuracy can coexist with weak atomicity of intermediate steps, highlighting the need for stepwise and representation-level diagnostics beyond final answer evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8626.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-enhanced model that serves as a high-performing baseline on final-answer accuracy but shows weaknesses under stepwise soundness metrics in FineLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A reinforcement-learning enhanced model reported by the authors as a strong-performing baseline for benchmarking reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD, ProntoQA, FOLIO, Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multi-step formal logic benchmarks requiring chained logical derivations and proof-style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot/few-shot inference baseline (RL-trained model); compared against SFT variants and prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High final-answer accuracy on several benchmarks (not fully tabulated in main text), but low All-Valid/All-Atomic stepwise scores (Table 3 reports DeepSeek-R1 few-shot All Valid ~13.1%, All Relevant ~33.8%, All Atomic ~5.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Although DeepSeek-R1 attains strong final-answer accuracy, its All-Valid score is low relative to some SFT models (e.g., Llama SFT-NL All Valid 40.9%), indicating less faithful stepwise reasoning despite correct answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Tends to compress or omit intermediate stepsâ€”high accuracy can mask shallow or non-transparent reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Demonstrates that RL-based optimization for final correctness may not guarantee stepwise logical integrity; FineLogic metrics can reveal such gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8626.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B instruction-tuned LLaMA family model used as a base for supervised fine-tuning experiments comparing natural-language and symbolic supervision formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>8B-parameter LLaMA variant with instruction tuning used as a base model for three-epoch SFT experiments at 1e-6 LR; evaluated in original and SFT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD (primary diagnostic), ProntoQA, FOLIO, Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>FLD: formal-logic dataset with problems up to 19 steps (used for in-distribution and OOD evaluation); ProntoQA and others provide additional multi-hop formal logic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning (SFT) on four supervision styles: NL-Reasoning (SFT-NL), Symbolic Structured (SFT-Symb-Struct), Symbolic Filtered (SFT-Symb-Filter), Symbolic Direct (SFT-Symb-Direct); also compared with prompting strategies (direct, CoT, few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SFT-NL dramatically improved FLD accuracy from direct prompting (31.7%) to 67.5%; SFT-NL Llama reached All Valid ~40.9%, All Relevant ~8.5%, All Atomic ~13.0 (Table 3). On OOD Multi-LogiEval, Llama SFT-NL achieved ~71.3% (matching GPT-4o). Performance declines with reasoning depth (notably ~10% drop on 16-19 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>SFT-NL outperforms inference-time methods (CoT, few-shot) and other SFT styles on overall accuracy; symbolic-structured SFT yields higher All-Atomic (more atomic steps) but lower overall accuracy and OOD generalization relative to SFT-NL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>SFT-NL chains include more redundant and non-atomic steps; all models (including LLaMA SFT variants) show sharp accuracy drops on very long chains (16-19 steps), and SFT has limited effect on internal early-answer convergence (CSS ceiling).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Natural-language SFT best improves final-answer accuracy and generalization, while symbolic structured SFT improves atomicity/stepwise soundness; there is a trade-off between generalization and structural chain quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8626.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B Qwen family instruction-tuned model evaluated as a base and fine-tuned with different supervision formats in FineLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter Qwen model (general), included both in base and SFT variants (including a math variant) and used in representation probing experiments for internal state analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD, ProntoQA, Multi-LogiEval, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step formal logic benchmarks requiring chained formal deductions and distinguishing necessary vs redundant facts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated under prompting (direct, CoT, few-shot), and SFT variants (SFT-NL, SFT-Symb-Struct, SFT-Symb-Filter, SFT-Symb-Direct); representation probing (CSS, RFI, NSD) performed on Qwen models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Direct prompting accuracy on FLD ~46.6%; SFT-NL improved to ~71.0% (final-answer accuracy). Stepwise metrics: Qwen-2.5-SFT-NL All Valid ~27.6%, All Relevant ~5.4%, All Atomic ~8.5 (Table 3). Representation probing: CSS ~8.2, RFI ~16.0, NSD ~44.3 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>SFT-NL yields the largest gains across Qwen variants, approaching or exceeding GPT-4o on some benchmarks; symbolic SFT variants improve atomicity / All-Valid in some settings (SFT-Symb-Struct often better for All-Atomic).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Like other 7/8B models, CSS shows minimal change with SFT (ceiling effect); NSD (next-step derivability) shows inconsistent or no improvement under vanilla SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Confirms the paper's central trade-off: NL supervision yields best final-answer accuracy and generalization for Qwen, while symbolic supervision improves structural stepwise properties; representation probing indicates SFT refines step generation more than earlier internal answer convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8626.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B Qwen math-specialized variant included in additional experiments (results primarily reported in appendices) and evaluated under the FineLogic framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Math-focused 7B Qwen variant evaluated alongside other base models to broaden conclusions across architectures/capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD (primary), ProntoQA, Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multi-step formal logic benchmarks; included to test generality across Qwen model family variants.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuned with SFT variants and compared against prompting baselines; detailed results presented in appendix C.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Appendix reports similar qualitative trends to the main Qwen variant: SFT-NL substantially boosts final-answer accuracy; symbolic styles improve atomicity/All-Valid in some cases (full numeric details in Appendix C).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performance trends consistent with Qwen-2.5-7B: SFT-NL yields largest accuracy gains; symbolic-filtered variants help stepwise integrity compared to direct symbolic translation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same limitations as other 7B models: limited CSS improvement under SFT and difficulty with NSD task.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Math-specialized pretraining does not materially change the high-level trade-off between NL and symbolic SFT observed across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8626.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B Qwen family model (larger and RL-enhanced in experiments) showing higher baseline CSS but similarly limited gains from SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>8B-parameter Qwen model with reported RL enhancements (mentioned in aggregate experiments); used to examine sensitivity of representation-level metrics to architecture and RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD (primary), ProntoQA, Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Long-chain multi-step logic problems used to probe internalization timing and stepwise correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with and without SFT variants; representation probing focused on CSS, RFI, NSD to assess internal reasoning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qwen-3-8B shows higher base CSS relative to other 7/8B models (appendix), but SFT produces <1% fluctuation in CSS, indicating SFT alone does not meaningfully improve early internal convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms other 7/8B models on CSS baseline (suggesting architecture/RL matter), but SFT-NL still produces strongest final-answer accuracy improvements among SFT styles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>CSS largely architecture- and RL-dependent; supervised fine-tuning offers little headroom to improve early answer convergence for 7/8B scales.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Indicates deeper latent reasoning (early internal convergence) depends more on model design and RL training than on vanilla supervised instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8626.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogiPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogiPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach where a language model is trained to directly generate symbolic reasoning steps, emulating a logical solver; evaluated as a baseline in FineLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogiPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baseline method (from literature) implemented as a model-style baseline that outputs symbolic reasoning steps to imitate a solver; compared under few-shot and prompting conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD, Multi-LogiEval, ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Symbolic multi-step reasoning where translating inputs into logic forms and emitting symbolic steps is core to the method.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neuro-symbolic baseline: LLM translates/outputs symbolic steps, emulating a solver rather than relying on natural-language chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported moderate performance: in Table 3 and Table 4, LogiPT yields modest All-Valid and representation scores (examples: All Valid ~5.2%/6.4% in table rows; CSS around 8.1 for some rows; RFI/NSD vary around mid-30s to 40s in some entries).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Often underperforms few-shot/fine-tuned SFT-NL on final-answer accuracy, though can improve some structural metrics depending on dataset and prompting style.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inconsistent across complex problems; translations and symbolic generation can be brittle without explicit solver feedback or careful supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Emphasizes that directly emulating symbolic solvers via language-model outputs is promising but needs structured supervision or verification to achieve reliable stepwise soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8626.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM (Pan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic baseline that translates problems into symbolic form and uses a deterministic solver, then interprets results back to natural language; evaluated as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogicLM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic pipeline: LLM translates natural language problems into symbolic representations, deterministic solver performs inference, and LLM translates solver output back to natural language, with self-refinement using solver feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD, Multi-LogiEval, ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring formal symbolic translation and multi-step logical deductions; Logic-LM explicitly separates translation and solver phases.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neuro-symbolic translation + deterministic solver + translation back; often augmented with self-refinement from solver feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mixed: performs well on simpler problems but degrades on complex ones (examples: Logic-LM performs well on simpler datasets but Qwen's Multi-LogiEval accuracy dropped to 27.1% under some Logic-LM variants according to text), demonstrating brittleness on hard, long-chain problems.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared unfavorably on some complex benchmarks to SFT-NL and few-shot prompting; symbolic translation errors and solver integration can harm overall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Translation errors to symbolic form and brittle solver interactions; performance sensitive to translation quality and problem complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Neuro-symbolic pipelines can enforce logical rigor but are vulnerable to translation quality issues; symbolic SFT variants that simplify or filter facts can mitigate some problems (SFT-Symb-Filter improves training signal).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8626.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymbCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SymbCoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic Chain-of-Thought method that integrates symbolic logic into CoT by translating the problem into symbolic format and generating formal reasoning steps, sometimes employing a verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SymbCoT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SymbCoT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework combining symbolic translations with chain-of-thought generation and optional verifier checks; evaluated as an inference-time strategy baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD, Multi-LogiEval, ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks that benefit from formal symbolic representation and multi-step logical inference; SymbCoT attempts to bring symbolic structure into CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Translate problem to symbolic form within CoT; generate steps using formal inference rules; optionally use verifier for logical soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Inconsistent: sometimes improves over other symbolic prompting (e.g., Qwen Multi-LogiEval 63.8% in one case) but shows large drops elsewhere (e.g., 22.6% on FLD vs 44.6% with direct prompting), indicating instability across datasets and models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Less reliable than SFT-NL and few-shot prompting overall; sometimes outperforms purely neural baselines on certain tasks but underperforms on others.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance variability linked to translation quality and dataset characteristics; symbolic CoT can underperform when translation or rule application is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Symbolic CoT helps enforce structure but is sensitive to problem formalization; symbolic SFT variants that reduce redundancy (Symb-Filter) perform better in training contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8626.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8626.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selection-Inference (Sel-Inf)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection-Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative reasoning method that alternates between selecting relevant facts and performing inference steps; used as a baseline (inference-time strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selection-inference: Exploiting large language models for interpretable logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Selection-Inference</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage iterative approach: a selection step identifies relevant premises, and an inference step derives new conclusions; used to improve interpretability and multi-step accuracy at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FLD, ProntoQA, Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step formal logic tasks where fact selection and stepwise inference are critical to build correct derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Inference-time algorithmic prompting; alternates selection of facts and inference to produce chains rather than end-to-end SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as a baseline with variable effectiveness; in some aggregated results it underperforms SFT-NL and few-shot methods on final-answer accuracy (see Table 8 aggregated breakdowns).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Generally less effective than SFT-NL for final-answer accuracy; can help structure reasoning but does not achieve the same gains as targeted supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not reliably produce atomic stepwise chains and can be sensitive to selection errors; performance varies by dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Useful as an interpretable inference-time strategy but insufficient to replace SFT for substantial accuracy improvements on strict logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProntoQA <em>(Rating: 2)</em></li>
                <li>Language models can be logical solvers <em>(Rating: 2)</em></li>
                <li>LogicLM <em>(Rating: 2)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 2)</em></li>
                <li>Roscoe: A suite of metrics for scoring step-by-step reasoning <em>(Rating: 2)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Representation-level probing (Ye et al., 2024) <em>(Rating: 1)</em></li>
                <li>SymbCoT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8626",
    "paper_id": "paper-279244946",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A powerful commercial large language model used as a strong zero-shot baseline in the FineLogic study; evaluated on multi-step formal logical reasoning benchmarks and stepwise diagnostics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary OpenAI family model used as a zero-shot/inference-time baseline in experiments; treated as a strong reference for accuracy and stepwise behavior.",
            "model_size": null,
            "reasoning_task_name": "FLD, ProntoQA, FOLIO, Multi-LogiEval (suite of multi-step formal logic benchmarks)",
            "reasoning_task_description": "Benchmarks focused on peer-reviewed, multi-hop formal logic problems requiring multi-step deductions in first-order style reasoning (e.g., proofs, contradictions).",
            "method_or_approach": "Zero-shot/few-shot prompting (direct prompting, CoT, few-shot exemplars) used as baselines; evaluated on final-answer accuracy, stepwise soundness, and representation-level probing.",
            "performance": "Strong reference performance: on some OOD sets (Multi-LogiEval) GPT-4o's final-answer accuracy is comparable to best SFT-NL tuned small models (e.g., ~71% reported match with Llama SFT-NL). Stepwise metrics: low All-Valid/All-Atomic in Table 3 (e.g., All Valid ~7.6%, All Relevant ~56.2%, All Atomic ~4.4 under few-shot), indicating many chains are not fully stepwise-sound.",
            "baseline_comparison": "Used as a reference; several SFT-NL fine-tuned smaller models approach or match GPT-4o final-answer accuracy on FLD/ProntoQA/Multi-LogiEval, but GPT-4o tends to produce fewer redundant steps (high All-Relevant) while still having low All-Atomic.",
            "limitations_or_failures": "Despite high accuracy, GPT-4o's generated chains often contain non-atomic steps and occasionally redundant steps (case studies in Figure 5); final-answer correctness alone hides weak stepwise soundness.",
            "insights_or_conclusions": "GPT-4o demonstrates that strong final-answer accuracy can coexist with weak atomicity of intermediate steps, highlighting the need for stepwise and representation-level diagnostics beyond final answer evaluation.",
            "uuid": "e8626.0",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek R1",
            "brief_description": "An RL-enhanced model that serves as a high-performing baseline on final-answer accuracy but shows weaknesses under stepwise soundness metrics in FineLogic.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1",
            "model_description": "A reinforcement-learning enhanced model reported by the authors as a strong-performing baseline for benchmarking reasoning accuracy.",
            "model_size": null,
            "reasoning_task_name": "FLD, ProntoQA, FOLIO, Multi-LogiEval",
            "reasoning_task_description": "Same multi-step formal logic benchmarks requiring chained logical derivations and proof-style reasoning.",
            "method_or_approach": "Zero-shot/few-shot inference baseline (RL-trained model); compared against SFT variants and prompting strategies.",
            "performance": "High final-answer accuracy on several benchmarks (not fully tabulated in main text), but low All-Valid/All-Atomic stepwise scores (Table 3 reports DeepSeek-R1 few-shot All Valid ~13.1%, All Relevant ~33.8%, All Atomic ~5.7%).",
            "baseline_comparison": "Although DeepSeek-R1 attains strong final-answer accuracy, its All-Valid score is low relative to some SFT models (e.g., Llama SFT-NL All Valid 40.9%), indicating less faithful stepwise reasoning despite correct answers.",
            "limitations_or_failures": "Tends to compress or omit intermediate stepsâ€”high accuracy can mask shallow or non-transparent reasoning chains.",
            "insights_or_conclusions": "Demonstrates that RL-based optimization for final correctness may not guarantee stepwise logical integrity; FineLogic metrics can reveal such gaps.",
            "uuid": "e8626.1",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLaMA-3.1-8B-Instruct",
            "name_full": "LLaMA-3.1-8B-Instruct",
            "brief_description": "An 8B instruction-tuned LLaMA family model used as a base for supervised fine-tuning experiments comparing natural-language and symbolic supervision formats.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B-Instruct",
            "model_description": "8B-parameter LLaMA variant with instruction tuning used as a base model for three-epoch SFT experiments at 1e-6 LR; evaluated in original and SFT variants.",
            "model_size": "8B",
            "reasoning_task_name": "FLD (primary diagnostic), ProntoQA, FOLIO, Multi-LogiEval",
            "reasoning_task_description": "FLD: formal-logic dataset with problems up to 19 steps (used for in-distribution and OOD evaluation); ProntoQA and others provide additional multi-hop formal logic tasks.",
            "method_or_approach": "Supervised fine-tuning (SFT) on four supervision styles: NL-Reasoning (SFT-NL), Symbolic Structured (SFT-Symb-Struct), Symbolic Filtered (SFT-Symb-Filter), Symbolic Direct (SFT-Symb-Direct); also compared with prompting strategies (direct, CoT, few-shot).",
            "performance": "SFT-NL dramatically improved FLD accuracy from direct prompting (31.7%) to 67.5%; SFT-NL Llama reached All Valid ~40.9%, All Relevant ~8.5%, All Atomic ~13.0 (Table 3). On OOD Multi-LogiEval, Llama SFT-NL achieved ~71.3% (matching GPT-4o). Performance declines with reasoning depth (notably ~10% drop on 16-19 steps).",
            "baseline_comparison": "SFT-NL outperforms inference-time methods (CoT, few-shot) and other SFT styles on overall accuracy; symbolic-structured SFT yields higher All-Atomic (more atomic steps) but lower overall accuracy and OOD generalization relative to SFT-NL.",
            "limitations_or_failures": "SFT-NL chains include more redundant and non-atomic steps; all models (including LLaMA SFT variants) show sharp accuracy drops on very long chains (16-19 steps), and SFT has limited effect on internal early-answer convergence (CSS ceiling).",
            "insights_or_conclusions": "Natural-language SFT best improves final-answer accuracy and generalization, while symbolic structured SFT improves atomicity/stepwise soundness; there is a trade-off between generalization and structural chain quality.",
            "uuid": "e8626.2",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Qwen-2.5-7B-Instruct",
            "name_full": "Qwen-2.5-7B-Instruct",
            "brief_description": "A 7B Qwen family instruction-tuned model evaluated as a base and fine-tuned with different supervision formats in FineLogic.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-7B-Instruct",
            "model_description": "7B-parameter Qwen model (general), included both in base and SFT variants (including a math variant) and used in representation probing experiments for internal state analysis.",
            "model_size": "7B",
            "reasoning_task_name": "FLD, ProntoQA, Multi-LogiEval, FOLIO",
            "reasoning_task_description": "Multi-step formal logic benchmarks requiring chained formal deductions and distinguishing necessary vs redundant facts.",
            "method_or_approach": "Evaluated under prompting (direct, CoT, few-shot), and SFT variants (SFT-NL, SFT-Symb-Struct, SFT-Symb-Filter, SFT-Symb-Direct); representation probing (CSS, RFI, NSD) performed on Qwen models.",
            "performance": "Direct prompting accuracy on FLD ~46.6%; SFT-NL improved to ~71.0% (final-answer accuracy). Stepwise metrics: Qwen-2.5-SFT-NL All Valid ~27.6%, All Relevant ~5.4%, All Atomic ~8.5 (Table 3). Representation probing: CSS ~8.2, RFI ~16.0, NSD ~44.3 (Table 4).",
            "baseline_comparison": "SFT-NL yields the largest gains across Qwen variants, approaching or exceeding GPT-4o on some benchmarks; symbolic SFT variants improve atomicity / All-Valid in some settings (SFT-Symb-Struct often better for All-Atomic).",
            "limitations_or_failures": "Like other 7/8B models, CSS shows minimal change with SFT (ceiling effect); NSD (next-step derivability) shows inconsistent or no improvement under vanilla SFT.",
            "insights_or_conclusions": "Confirms the paper's central trade-off: NL supervision yields best final-answer accuracy and generalization for Qwen, while symbolic supervision improves structural stepwise properties; representation probing indicates SFT refines step generation more than earlier internal answer convergence.",
            "uuid": "e8626.3",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Qwen-2.5-Math-7B-Instruct",
            "name_full": "Qwen-2.5-Math-7B-Instruct",
            "brief_description": "A 7B Qwen math-specialized variant included in additional experiments (results primarily reported in appendices) and evaluated under the FineLogic framework.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-Math-7B-Instruct",
            "model_description": "Math-focused 7B Qwen variant evaluated alongside other base models to broaden conclusions across architectures/capabilities.",
            "model_size": "7B",
            "reasoning_task_name": "FLD (primary), ProntoQA, Multi-LogiEval",
            "reasoning_task_description": "Same multi-step formal logic benchmarks; included to test generality across Qwen model family variants.",
            "method_or_approach": "Fine-tuned with SFT variants and compared against prompting baselines; detailed results presented in appendix C.",
            "performance": "Appendix reports similar qualitative trends to the main Qwen variant: SFT-NL substantially boosts final-answer accuracy; symbolic styles improve atomicity/All-Valid in some cases (full numeric details in Appendix C).",
            "baseline_comparison": "Performance trends consistent with Qwen-2.5-7B: SFT-NL yields largest accuracy gains; symbolic-filtered variants help stepwise integrity compared to direct symbolic translation.",
            "limitations_or_failures": "Same limitations as other 7B models: limited CSS improvement under SFT and difficulty with NSD task.",
            "insights_or_conclusions": "Math-specialized pretraining does not materially change the high-level trade-off between NL and symbolic SFT observed across models.",
            "uuid": "e8626.4",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Qwen-3-8B",
            "name_full": "Qwen-3-8B",
            "brief_description": "An 8B Qwen family model (larger and RL-enhanced in experiments) showing higher baseline CSS but similarly limited gains from SFT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-3-8B",
            "model_description": "8B-parameter Qwen model with reported RL enhancements (mentioned in aggregate experiments); used to examine sensitivity of representation-level metrics to architecture and RL.",
            "model_size": "8B",
            "reasoning_task_name": "FLD (primary), ProntoQA, Multi-LogiEval",
            "reasoning_task_description": "Long-chain multi-step logic problems used to probe internalization timing and stepwise correctness.",
            "method_or_approach": "Evaluated with and without SFT variants; representation probing focused on CSS, RFI, NSD to assess internal reasoning signals.",
            "performance": "Qwen-3-8B shows higher base CSS relative to other 7/8B models (appendix), but SFT produces &lt;1% fluctuation in CSS, indicating SFT alone does not meaningfully improve early internal convergence.",
            "baseline_comparison": "Outperforms other 7/8B models on CSS baseline (suggesting architecture/RL matter), but SFT-NL still produces strongest final-answer accuracy improvements among SFT styles.",
            "limitations_or_failures": "CSS largely architecture- and RL-dependent; supervised fine-tuning offers little headroom to improve early answer convergence for 7/8B scales.",
            "insights_or_conclusions": "Indicates deeper latent reasoning (early internal convergence) depends more on model design and RL training than on vanilla supervised instruction tuning.",
            "uuid": "e8626.5",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LogiPT",
            "name_full": "LogiPT",
            "brief_description": "A baseline approach where a language model is trained to directly generate symbolic reasoning steps, emulating a logical solver; evaluated as a baseline in FineLogic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogiPT",
            "model_description": "Baseline method (from literature) implemented as a model-style baseline that outputs symbolic reasoning steps to imitate a solver; compared under few-shot and prompting conditions.",
            "model_size": null,
            "reasoning_task_name": "FLD, Multi-LogiEval, ProntoQA",
            "reasoning_task_description": "Symbolic multi-step reasoning where translating inputs into logic forms and emitting symbolic steps is core to the method.",
            "method_or_approach": "Neuro-symbolic baseline: LLM translates/outputs symbolic steps, emulating a solver rather than relying on natural-language chains.",
            "performance": "Reported moderate performance: in Table 3 and Table 4, LogiPT yields modest All-Valid and representation scores (examples: All Valid ~5.2%/6.4% in table rows; CSS around 8.1 for some rows; RFI/NSD vary around mid-30s to 40s in some entries).",
            "baseline_comparison": "Often underperforms few-shot/fine-tuned SFT-NL on final-answer accuracy, though can improve some structural metrics depending on dataset and prompting style.",
            "limitations_or_failures": "Inconsistent across complex problems; translations and symbolic generation can be brittle without explicit solver feedback or careful supervision.",
            "insights_or_conclusions": "Emphasizes that directly emulating symbolic solvers via language-model outputs is promising but needs structured supervision or verification to achieve reliable stepwise soundness.",
            "uuid": "e8626.6",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM (Pan et al.)",
            "brief_description": "A neuro-symbolic baseline that translates problems into symbolic form and uses a deterministic solver, then interprets results back to natural language; evaluated as a baseline.",
            "citation_title": "LogicLM",
            "mention_or_use": "use",
            "model_name": "Logic-LM",
            "model_description": "Neuro-symbolic pipeline: LLM translates natural language problems into symbolic representations, deterministic solver performs inference, and LLM translates solver output back to natural language, with self-refinement using solver feedback.",
            "model_size": null,
            "reasoning_task_name": "FLD, Multi-LogiEval, ProntoQA",
            "reasoning_task_description": "Benchmarks requiring formal symbolic translation and multi-step logical deductions; Logic-LM explicitly separates translation and solver phases.",
            "method_or_approach": "Neuro-symbolic translation + deterministic solver + translation back; often augmented with self-refinement from solver feedback.",
            "performance": "Mixed: performs well on simpler problems but degrades on complex ones (examples: Logic-LM performs well on simpler datasets but Qwen's Multi-LogiEval accuracy dropped to 27.1% under some Logic-LM variants according to text), demonstrating brittleness on hard, long-chain problems.",
            "baseline_comparison": "Compared unfavorably on some complex benchmarks to SFT-NL and few-shot prompting; symbolic translation errors and solver integration can harm overall performance.",
            "limitations_or_failures": "Translation errors to symbolic form and brittle solver interactions; performance sensitive to translation quality and problem complexity.",
            "insights_or_conclusions": "Neuro-symbolic pipelines can enforce logical rigor but are vulnerable to translation quality issues; symbolic SFT variants that simplify or filter facts can mitigate some problems (SFT-Symb-Filter improves training signal).",
            "uuid": "e8626.7",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SymbCoT",
            "name_full": "SymbCoT",
            "brief_description": "A symbolic Chain-of-Thought method that integrates symbolic logic into CoT by translating the problem into symbolic format and generating formal reasoning steps, sometimes employing a verifier.",
            "citation_title": "SymbCoT",
            "mention_or_use": "use",
            "model_name": "SymbCoT",
            "model_description": "Framework combining symbolic translations with chain-of-thought generation and optional verifier checks; evaluated as an inference-time strategy baseline.",
            "model_size": null,
            "reasoning_task_name": "FLD, Multi-LogiEval, ProntoQA",
            "reasoning_task_description": "Benchmarks that benefit from formal symbolic representation and multi-step logical inference; SymbCoT attempts to bring symbolic structure into CoT prompting.",
            "method_or_approach": "Translate problem to symbolic form within CoT; generate steps using formal inference rules; optionally use verifier for logical soundness.",
            "performance": "Inconsistent: sometimes improves over other symbolic prompting (e.g., Qwen Multi-LogiEval 63.8% in one case) but shows large drops elsewhere (e.g., 22.6% on FLD vs 44.6% with direct prompting), indicating instability across datasets and models.",
            "baseline_comparison": "Less reliable than SFT-NL and few-shot prompting overall; sometimes outperforms purely neural baselines on certain tasks but underperforms on others.",
            "limitations_or_failures": "Performance variability linked to translation quality and dataset characteristics; symbolic CoT can underperform when translation or rule application is ambiguous.",
            "insights_or_conclusions": "Symbolic CoT helps enforce structure but is sensitive to problem formalization; symbolic SFT variants that reduce redundancy (Symb-Filter) perform better in training contexts.",
            "uuid": "e8626.8",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Selection-Inference (Sel-Inf)",
            "name_full": "Selection-Inference",
            "brief_description": "An iterative reasoning method that alternates between selecting relevant facts and performing inference steps; used as a baseline (inference-time strategy).",
            "citation_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "mention_or_use": "use",
            "model_name": "Selection-Inference",
            "model_description": "Two-stage iterative approach: a selection step identifies relevant premises, and an inference step derives new conclusions; used to improve interpretability and multi-step accuracy at inference time.",
            "model_size": null,
            "reasoning_task_name": "FLD, ProntoQA, Multi-LogiEval",
            "reasoning_task_description": "Multi-step formal logic tasks where fact selection and stepwise inference are critical to build correct derivations.",
            "method_or_approach": "Inference-time algorithmic prompting; alternates selection of facts and inference to produce chains rather than end-to-end SFT.",
            "performance": "Reported as a baseline with variable effectiveness; in some aggregated results it underperforms SFT-NL and few-shot methods on final-answer accuracy (see Table 8 aggregated breakdowns).",
            "baseline_comparison": "Generally less effective than SFT-NL for final-answer accuracy; can help structure reasoning but does not achieve the same gains as targeted supervised fine-tuning.",
            "limitations_or_failures": "Does not reliably produce atomic stepwise chains and can be sensitive to selection errors; performance varies by dataset and model.",
            "insights_or_conclusions": "Useful as an interpretable inference-time strategy but insufficient to replace SFT for substantial accuracy improvements on strict logical reasoning tasks.",
            "uuid": "e8626.9",
            "source_info": {
                "paper_title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProntoQA",
            "rating": 2
        },
        {
            "paper_title": "Language models can be logical solvers",
            "rating": 2,
            "sanitized_title": "language_models_can_be_logical_solvers"
        },
        {
            "paper_title": "LogicLM",
            "rating": 2
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 2,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        },
        {
            "paper_title": "Roscoe: A suite of metrics for scoring step-by-step reasoning",
            "rating": 2,
            "sanitized_title": "roscoe_a_suite_of_metrics_for_scoring_stepbystep_reasoning"
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Representation-level probing (Ye et al., 2024)",
            "rating": 1,
            "sanitized_title": "representationlevel_probing_ye_et_al_2024"
        },
        {
            "paper_title": "SymbCoT",
            "rating": 1
        }
    ],
    "cost": 0.019225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study
9 Oct 2025</p>
<p>Yujun Zhou 
University of Notre</p>
<p>Jiayi Ye 
Zipeng Ling 
University of Pennsylvania</p>
<p>Yufei Han 
INRIA * Equal Contribution</p>
<p>Yue Huang 
University of Notre</p>
<p>Haomin Zhuang 
University of Notre</p>
<p>Zhenwen Liang 
University of Notre</p>
<p>Kehan Guo 
University of Notre</p>
<p>Taicheng Guo 
Xiangqi Wang 
University of Notre</p>
<p>Xiangliang Zhang xzhang33@nd.edu 
University of Notre</p>
<p>Mbzuai 
University of Notre</p>
<p>Xiangqi Wang 
Yue Huang 
Yanbo Wang 
Xiaonan Luo 
Jason Wei 
Xuezhi Wang 
Dale Schuurmans 
Maarten Bosma 
Fei Xia 
Ed Chi 
V Quoc 
Denny Le 
Zhou 
Chulin Xie 
Yangsibo Huang 
Chiyuan Zhang 
Da Yu 
BillXinyun Chen 
Yuchen Lin 
Bo Li 
Badih Ghazi 
Ravi 2024 Kumar 
Tian Xie 
Zitian Gao 
Qingnan Ren 
Haoming Luo 
Yuqian Hong 
Bryan Dai 
Joey Zhou 
Kai Qiu 
Yulai Zhao 
Haolin Liu 
Dian Yu 
S Y Kung 
Haitao Mi 
Tong Zheng 
Lichang Chen 
Simeng Han 
R Thomas 
Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study
9 Oct 20253A8DF18F8D4214929DAD08AF87B267A0arXiv:2506.04810v2[cs.CL]
Logical reasoning is a core capability for large language models (LLMs), yet existing benchmarks that rely solely on final-answer accuracy fail to capture the quality of the reasoning process.To address this, we introduce FineLogic, a fine-grained evaluation framework that assesses logical reasoning across three dimensions: overall accuracy, stepwise soundness, and representation-level probing.Leveraging this framework, we conduct a comprehensive study on how different supervision formats in fine-tuning shape reasoning abilities.We fine-tune LLMs on four supervision styles-one in natural language and three symbolic variants-and find a key trade-off: natural language supervision excels at generalization to out-of-distribution and long-chain problems, whereas symbolic supervision is superior at instilling structurally sound, atomic reasoning steps.Furthermore, our probing analysis indicates that fine-tuning primarily refines the model's step-by-step generation process, rather than improving its ability to converge on an answer early.Together, our framework and analysis provide a more rigorous lens for evaluating and improving logical reasoning in LLMs.The code is available at https: //github.com/YujunZhou/FineLogic.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are rapidly emerging as transformative tools across a wide array of applications (Achiam et al., 2023;Guo et al., 2024b;Thirunavukarasu et al., 2023;Nam et al., 2024;Huang et al., 2024Huang et al., , 2025;;Zhou et al., 2024a).Among these, reasoning serves as a core capability underpinning tasks such as problem-solving (Lu et al., 2023), scientific question answering (Guo et al., 2024a;Zhou et al., 2024b), and code analysis (Nam et al., 2024).Consequently, a growing body of research has sought to evaluate and enhance the reasoning abilities of LLMs from multiple perspec-tives (Wei et al., 2022;Guo et al., 2025Guo et al., , 2024a;;Liang et al., 2024;Wang et al., 2025b;Zhou et al., 2025;Zhao et al., 2025).Within this broader landscape, logical reasoning stands out as a particularly challenging and intellectually demanding domain (Saparov and He, 2022).It requires a synthesis of natural language understanding, formal logical interpretation, and multi-step inferential processing (Patel et al., 2024;Saparov et al., 2023;Morishita et al., 2024).</p>
<p>Despite growing interest in the logical reasoning capabilities of LLMs, most existing benchmarks focus narrowly on whether a model produces the correct final answer (Patel et al., 2024;Parmar et al., 2024;Han et al., 2022).This binary evaluation, typically assessing only the correctness of a "True" or "False" output, can be misleading, as it fails to determine whether the model arrived at the answer through valid multi-step reasoning (Saparov and He, 2022).Consequently, correct answers may reflect guesswork rather than genuine logical inference.We are thus motivated to address RQ1: How to rigorously evaluate LLMs' step-by-step correctness in logical reasoning tasks, beyond the binary evaluation of the final answer?</p>
<p>In parallel with benchmarking efforts, numerous methods have been proposed to enhance the multistep logical reasoning abilities of LLMs.While many leverage inference-time strategies (Wang et al., 2025a), in-context learning (Creswell et al., 2022;Xu et al., 2024), or external logical verifiers (Pan et al., 2023) to guide the model toward more rigorous reasoning, some recent studies explored supervised fine-tuning (SFT) as a more direct approach to enhancing logical reasoning (Morishita et al., 2024;Feng et al., 2023).For example, Morishita et al. (2024) proposes a synthetic logic corpus designed to offer broad and systematic coverage of logical knowledge.However, it remains unclear for this important question, RQ2: What style of training data, natural language or formal logical symbols, better facilitates the learning of multistep logical reasoning through SFT? Addressing this research question is important for understanding how to most effectively instill logical reasoning capabilities in LLMs.</p>
<p>To address RQ1, we propose FineLogic, a diagnostic framework designed to offer a multidimensional evaluation of LLMs' reasoning processes, moving beyond binary final-answer correctness.Rather than creating another leaderboard, FineLogic serves as a tool for LLM practitioners to pinpoint specific weaknesses in reasoning chains, such as flawed logic, redundancy, or nonatomic steps.Our framework evaluates models along three complementary dimensions: (1) Overall benchmark accuracy: This metric captures a model's ability to perform multi-step logical reasoning and its generalizability across problems from diverse domains.(2) Stepwise Soundness:</p>
<p>Inspired by Saparov and He (2022), we assess the quality of each intermediate reasoning step using three criteria-validity (whether the step is logically valid), relevance (whether its conclusion is used in later steps), and atomicity (whether it applies a single, minimal inference rule).These metrics aim to evaluate the model's ability to generate human-interpretable and logically coherent reasoning chains.</p>
<p>(3) Representation-level probing (Ye et al., 2024): By applying probing techniques to LLM hidden representations, this evaluation provides insight into whether the model's understanding of logical structure is merely surface-level or embedded in its internal state.</p>
<p>To address RQ2, we systematically investigate how different supervision formats affect the reasoning capabilities of LLMs.Specifically, we examine both natural language-based training data and logicsymbol-based representations, including several structured variants.Our analysis shows that natural language supervision is particularly effective in conveying core reasoning patterns, leading to strong performance across a wide range of evaluation benchmarks.Notably, it exhibits impressive generalizability even on out-of-distribution test sets that require long reasoning chains.However, a deeper examination of stepwise soundness and internal representation probing reveals certain limitations.Models trained with natural language supervision tend to struggle with producing strictly minimal reasoning chains (e.g., more likely including redundant steps and applying multiple inference rules in a single step, as shown in Figure 5).In contrast, models trained with symbolic reasoning styles are better at filtering out irrelevant information, generating atomic steps aligned with individual deduction rules, and maintaining cleaner, logically grounded reasoning trajectories.</p>
<p>To summarize, our contributions are as follows:</p>
<p>â€¢ We propose FineLogic, a unified diagnostic framework for assessing LLMs' logical reasoning.It moves beyond final-answer accuracy to evaluate the quality and coherence of reasoning steps, offering fine-grained insights that can guide targeted model improvements and serve as a scaffold for structured reward design in reinforcement learning.</p>
<p>â€¢ We conduct a comprehensive study on the effects of supervision format, fine-tuning LLMs on both natural language and symbolic logic data to examine their impact on reasoning across general and complex tasks.</p>
<p>â€¢ Through systematic analysis of models trained with different supervision styles, we identify key trade-offs between generalization and structural reasoning quality.The findings provide concrete insights into the design and selection of effective training data for post-training.</p>
<p>Related Works</p>
<p>Logical Reasoning Benchmarks.Numerous benchmarks have been proposed to evaluate the logical reasoning abilities of LLMs.Many either mix logical and commonsense reasoning (Liu et al., 2023;Luo et al., 2023;Havrilla et al., 2024), making it hard to isolate logical competence, or assess multi-step reasoning using only final-answer accuracy (Parmar et al., 2024;Han et al., 2022;Tafjord et al., 2020;Mondorf and Plank, 2024).While ProntoQA (Saparov and He, 2022;Saparov et al., 2023) pioneered stepwise evaluation, its analysis is confined to step correctness on short problems.FineLogic moves beyond this by introducing a more comprehensive suite of step-level diagnostics-including a more practical relevance metric-to rigorously assess complex, long-chain reasoning.Furthermore, we are the first to adapt representation-level probing from mathematics (Ye et al., 2024) to the logical domain, introducing novel tasks like Correctness Spanning Steps (CSS) to connect behavioral outputs with internal model states.In contrast, other stepwise frameworks like ROSCOE (Golovneva et al., 2022) and RECEVAL (Prasad et al., 2023) remain too generic or coarsegrained for the specific demands of formal logic.</p>
<p>Logical Reasoning Enhancement.Several studies have aimed to improve LLMs' performance on logical reasoning tasks.Some approaches rely on translating inputs into formal logic and using programmable verifiers to solve problems (Olausson et al., 2023;Pan et al., 2023;Yang et al., 2023;Ryu et al., 2024), which bypasses the model's own reasoning process.Others use in-context learning or inference-time strategies to guide output without fundamentally enhancing reasoning ability (Creswell et al., 2022;Wang et al., 2025a;Xu et al., 2024;Sun et al., 2023;Toroghi et al., 2024).</p>
<p>While a few works have explored fine-tuning or reinforcement learning to strengthen logical reasoning (Feng et al., 2023;Morishita et al., 2023Morishita et al., , 2024;;Xie et al., 2025;Yang et al., 2022;Xie et al., 2024;Zheng et al., 2025)</p>
<p>FineLogic Evaluation Framework</p>
<p>As illustrated in Figure 2, FineLogic builds on existing benchmarks and evaluates logical reasoning ability from three complementary perspectives: (1) Overall benchmark accuracy, which measures whether the model can correctly solve multi-step reasoning tasks;</p>
<p>(2) Stepwise soundness, which evaluates whether each reasoning step is valid and interpretable;</p>
<p>(3) Representation-level probing, which assesses whether the model internally captures the problem's reasoning structure beyond surface-level patterns.</p>
<p>Overall Benchmark Accuracy</p>
<p>Similar to most benchmarks, our overall benchmark accuracy focuses on final-answer correctness.Specifically, we define accuracy (Acc) as:
Acc = 1 N N i=1 1[Å· i = y i ] (1)
where N is the number of test problems, y i is the gold label, and Å·i is the model's prediction for problem i.While coarse-grained, it offers a quick and effective way to assess a model's overall reasoning ability and cross-domain generalization.We evaluate on four challenging multi-step reasoning benchmarks, deliberately selected for their focus on peer-reviewed, multi-hop formal logic.The suite includes widely-used datasets for comparability (FOLIO (Han et al., 2022)</p>
<p>Stepwise Soundness</p>
<p>Step n: fact i âˆ§ int j â†’ int k.</p>
<p>Analyze</p>
<p>Step n:</p>
<p>Step n is valid if int k is provable from fact i âˆ§ int j</p>
<p>Step n is relevant if int k is used in subsequent steps</p>
<p>Step n is atomic if int k can be proved by fact i âˆ§ int j with exactly one deduction rule.</p>
<p>Stepwise Soundness</p>
<p>Building on Saparov and He (2022), we evaluate the soundness of each intermediate reasoning step along three dimensions: validity (whether the step logically follows from its premises), relevance (whether its conclusion is used in later steps), and atomicity (whether it applies a single, minimal inference rule).</p>
<p>To assess these criteria, we extract the premises and conclusion of each step from the model's final answer.Crucially, our evaluator scores only the text following a designated answer tag (e.g., </think>), ensuring that any preceding self-reflection or speculative reasoning is excluded by design.We use GPT-4.1-mini to evaluate validity and atomicity.</p>
<p>Manual verification on 200 annotated steps shows that GPT-4.1-miniachieves over 98% accuracy on both metrics.For relevance, we determine whether the conclusion of step i is referenced in any subsequent step k &gt; i.</p>
<p>We then compute the proportion of samples in which all steps are valid, relevant, and atomic, providing a sample-level measure of reasoning integrity.Formally, for each solution s with K s steps, let v s,k , r s,k , a s,k âˆˆ {0, 1} denote the validity, rel-evance, and atomicity of step k, respectively.We define the sample-level metrics as:
AllValid = 1 N N s=1 Ks k=1 v s,k(2)AllRelevant = 1 N N s=1 Ks k=1 r s,k(3)AllAtomic = 1 N N s=1 Ks k=1 a s,k(4)
Full prompt templates are provided in Figures 13  and 14.</p>
<p>Representation-level Probing</p>
<p>Inspired by Ye et al. (2024), we introduce representation-level probing accuracy to assess whether LLMs internally understand how and when to perform specific reasoning step.Unlike behavioral metrics, this method aligns internal representations with reasoning structure and tracks how reasoning knowledge evolves across steps.</p>
<p>We construct probing datasets from FLD test samples requiring 10-20 reasoning steps, using 450 problems for training and 100 for testing across three tasks, implementation details are provided in Appendix B:</p>
<p>Correctness Spanning Steps (CSS): Identifies the earliest step after which the model consistently predicts the correct label.The spanning length is the number of remaining steps from that point to the end.Higher accuracy indicates earlier internalization of the correct answer.Formally, for each problem s, let Ï„ s be the first step from which the probe consistently predicts the correct label and K s be the total number of steps.The CSS score is then:
CSS = 1 N N s=1 (K s âˆ’ Ï„ s ).(5)
Redundant Facts Identification (RFI): After presenting all facts and the hypothesis, we append three necessary and three redundant facts.A classifier is trained to distinguish between them, measuring the model's ability to identify irrelevant information.Higher accuracy reflects better fact discrimination.</p>
<p>Next-Step Derivability (NSD): At six randomly selected intermediate steps, we append three valid and three invalid candidate steps.Probing predicts which are currently derivable.Higher accuracy indicates stronger awareness of valid next steps.</p>
<p>For both RFI and NSD, performance is measured using balanced accuracy to account for the construction of positive and negative instances.The score is calculated as:
Score = 1 2 (TPR + TNR),
where TPR is the True Positive Rate and TNR is the True Negative Rate.</p>
<p>Our evaluation builds on two prior lines of work-stepwise reasoning evaluation (Saparov and He, 2022) and representation-level probing (Ye et al., 2024)-but introduces key extensions tailored to logical reasoning.</p>
<p>Supervision Format and Style: SFT Data Design</p>
<p>In this section, we examine how different supervision styles for SFT affect the logical reasoning abilities of LLMs.Our training data is based on FLD and ProntoQA, both of which include gold reasoning chains suitable for constructing diverse supervision styles.</p>
<p>For FLD, we generate 500 problems for each reasoning depth from 0 to 15, plus 1500 UNKNOWN samples, totaling 9500 training instances.For Pron-toQA, we use 3200 3-hop problems.During evaluation, FLD covers depths 0-19, while ProntoQA uses only the hardest 5-hop samples.</p>
<p>We compare four supervision styles across two categories: natural language-based and symbolic reasoning.Each style reflects a different level of abstraction and clarity in reasoning structure.</p>
<p>â€¢ NL-Reasoning: Solutions are written entirely in natural language, with no intermediate symbolization or abstraction.</p>
<p>â€¢ Symbolic Reasoning (Structured): Problems are formalized by defining variables and predicates, translating facts and hypotheses into logical forms, and reasoning step by step using symbolic logic.</p>
<p>â€¢ Symbolic Reasoning (Filtered): A simplified variant where only necessary facts are retained, shortening reasoning chains and reducing input complexity.</p>
<p>â€¢ Symbolic Reasoning (Direct): Facts are directly expressed in symbolic form without defining variables or predicates, which shortens sequences but may introduce ambiguity.</p>
<p>A small portion of translations, connective phrases, and intermediate steps are generated using GPT-4.1.Prompt examples are shown in Figure 4 (Appendix E).</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Our experiments involve four base models for fine-tuning-LLaMA-3.1-8B-Instruct, Qwen-2.5-7B-Instruct,Qwen-2.5-Math-7B-Instruct, and Qwen-3-8B-and two powerful zero-shot baselines, GPT-4o and DeepSeek R1.The four base models are fine-tuned for 3 epochs at a 1 Ã— 10 âˆ’6 learning rate, with their original versions also serving as baselines.Representationlevel probing is limited to the LLaMA and Qwen models due to computational constraints, and an explicit step-by-step format is enforced for all stepwise evaluations.</p>
<p>We compare SFT models trained with different supervision styles against these baselines:</p>
<p>â€¢ Direct Answering More detailed experimental setups can be found in Appendix A.</p>
<p>Results</p>
<p>We conducted experiments for analyzing the performance of four models combined with various prompting and fine-tuning settings under the Fine-Logic Evaluation Framework.Due to space constraints, the results for Qwen-2.5-Math-7B-Instructand Qwen-3-8B are presented in Appendix C.</p>
<p>Results on Overall Benchmark Accuracy</p>
<p>As shown in Table 1, we report the overall benchmark accuracy across four datasets, as well as the step-wise accuracy on the FLD benchmark, stratified by reasoning depth (Figure 3).Our analysis yields several key observations:</p>
<p>CoT and few-shot prompting generally improve performance, but baseline methods do not consistently yield gains.Across the four evaluation datasets, both CoT and few-shot prompting lead to broadly positive improvements, indicating their general effectiveness in enhancing LLM performance on logical reasoning tasks.Notably, fewshot prompting consistently outperforms CoT, suggesting that for complex logical tasks, showing the model how to think (via exemplars) is more beneficial than simply encouraging it to reason step by step.This may be because logical questions naturally elicit multi-step reasoning under direct prompting, limiting the marginal benefit of CoT.In contrast, few-shot demonstrations provide clearer procedural scaffolding, which appears more effective in guiding the model's reasoning process.</p>
<p>In contrast, baseline prompting methods such as Logic-LM, SymbCoT, and Sel-Inf show inconsistent performance and sometimes underperform even direct prompting.For example, Logic-LM performs well on simpler problems but degrades on complex ones, with Qwen's Multi-LogiEval accuracy dropping to 27.1%.SymbCoT sometimes improves over Logic-LM (e.g., 63.8% on Multi-LogiEval with Qwen) but also shows large drops elsewhere (e.g., 22.6% on FLD, versus 44.6% with direct prompting).</p>
<p>Supervised fine-tuning outperforms inferencetime methods, but its effectiveness heavily depends on the supervision style.Compared to inference-time prompting strategies, SFT yields significantly greater improvements in logical reasoning performance.Among all training styles, natural language-based supervision (SFT-NL) produces the most substantial and consistent gains across datasets and models.</p>
<p>Notably, even though SFT was conducted using only problems from FLD and ProntoQA with reasoning depths less than those in the test set, the resulting models show robust improvements.For example, under the SFT-NL setting, Llama's accuracy on FLD increased from 31.7% (direct prompting) to 67.5% and Qwen improved from 46.6% to 71.0%, approaching the best-performing baseline DeepSeek R1.On ProntoQA, most SFT variants achieve over 90% accuracy.Furthermore, even on out-of-distribution datasets such as FOLIO and Multi-LogiEval, some SFT settings deliver strong generalization.For instance, on Multi-LogiEval, Llama with SFT-NL improved to 71.3%, matching the performance of GPT-4o.</p>
<p>While SFT-NL demonstrates the best overall and most transferable performance, other styles of supervision yield much smaller gains.This may be since LLMs are primarily pretrained on natural language data, making symbolic reasoning-especially when it requires both translation and inference over logic forms-significantly more challenging.Among the symbolic settings, SFT-Symb-Filter consistently outperforms other variants.By removing redundant reasoning steps from the symbolic training data, this setting simplifies training and enhances performance.In contrast, SFT-Symb-Direct, which skips variable and predicate definitions entirely, performs poorly, likely due to the introduction of ambiguity and the lack of explicit logical structure.</p>
<p>Accuracy declines with reasoning depth, but SFT enables small models to match GPT-4o even on the most challenging out-of-distribution samples.As shown in Figure 3 On more difficult out-of-distribution questions requiring 16-19 steps of reasoning-where no training samples are available-performance drops by approximately 10% relative to the 12-15 step range.However, even under these conditions, SFT models maintain accuracy comparable to GPT-4o.Combined with strong generalization to unseen datasets such as FOLIO and Multi-LogiEval, these results suggest that SFT induces genuine logical reasoning ability in LLMs.At the same time, the sharp performance decline on longer reasoning chains implies that some portion of success on shorter problems may still stem from shallow pattern matching or memorization, rather than robust inference.Detailed results can be found in C.    of samples in which every generated step is relevant-i.e., none of the steps are redundant or unnecessary for reaching the conclusion.GPT-4o and LogiPT perform exceptionally well on this metric, implying that they rarely generate superfluous reasoning steps.In contrast, SFT-NL and SFT-Symb-Direct consistently underperform.For SFT-NL, this may stem from the nature of natural language reasoning: due to its semantic richness and lack of structural constraints, the model may occasionally include exploratory or overly verbose steps, unsure of which inference is most effective.For SFT-Symb-Direct, the poor performance is likely due to the model may failure to fully capture interfact dependencies, resulting in reasoning sequences that are logically valid but contain unused or irrelevant steps.</p>
<p>Results on Stepwise Soundness</p>
<p>The All Atomic metric evaluates whether every step in a reasoning chain corresponds to a sin- gle atomic inference-i.e., whether steps avoid combining multiple logical moves.Here, SFT-Symb-Struct consistently outperforms other settings, highlighting the advantages of structured symbolic reasoning.Symbolic reasoning is inherently more compact and constrained, which likely helps the model learn what constitutes a minimal, rule-aligned inference step.In contrast, natural language reasoning often fuses multiple reasoning rules into a single step, making it harder for the model to isolate atomic operations.</p>
<p>Results on Representation-level Probing</p>
<p>Table 4 presents results from our probing analysis, which assesses whether models internally acquire key reasoning abilities.Our findings reveal several key takeaways:</p>
<p>Correctness Spanning Steps (CSS) shows a clear ceiling for 7/8B models under SFT.Across all SFT variants and base models, the CSS score fluctuates by less than 1%.This flat trend suggests that supervised fine-tuning alone offers little headroom for improving how early a model internally converges on the correct answer at this scale.Enhancing this "foresight" capability may require alternative methods like reinforcement learning with step-level rewards or architectural changes.</p>
<p>Targeted supervision can unlock specific internal skills, as shown by Symb-Direct on RFI.The Redundant Fact Identification (RFI) metric benefits most from the SFT-Symb-Direct setting.This training data uses minimal symbolization but retains redundant facts, forcing the model to learn to distinguish necessary from unnecessary premises.This direct alignment between the training task and the probing metric demonstrates that when a specific skill like redundancy filtering is desired, targeted supervision that mirrors the probe is highly effective.</p>
<p>Next-Step</p>
<p>Future Work: Guiding Faithful Reasoning with Reinforcement Learning</p>
<p>While supervised fine-tuning improves reasoning, reinforcement learning (RL) offers a path to refining it further.However, a major challenge in applying RL to logical reasoning is the reliance on sparse, binary rewards (i.e., whether the final answer is correct).This can lead to "reward hacking," where models learn heuristics to guess the right answer without developing a sound reasoning process.</p>
<p>The FineLogic framework offers a direct solution by providing dense, multi-faceted reward signals that can guide the quality of the reasoning process itself.Because our metrics are reference-free and deterministically scored, they translate cleanly into a multi-objective reward function.</p>
<p>A Multi-Objective Reward Framework.The FineLogic metrics can be directly integrated into a granular, multi-objective reward function for RL.</p>
<p>The stepwise soundness metrics (validity, relevance, atomicity) can be used to directly reward logical integrity, promoting reasoning that is rigorous, concise, and transparent.Concurrently, the representation-level metric CSS can serve as an online reward for cognitive efficiency, encouraging the model to converge on the correct answer earlier in its internal process.These components can be combined in a weighted sum:
R total =R acc + w v R valid + w r R relevant + w a R atomic + w c R css (6)
In this framework, practitioners can tune the weights (w v , w r , . . . ) to prioritize desired reasoning qualities like rigor or conciseness, offering a practical roadmap for training more faithful and interpretable models.</p>
<p>Conclusion</p>
<p>We introduce FineLogic, a unified and fine-grained framework for evaluating the logical reasoning capabilities of large language models.By integrating overall benchmark accuracy, stepwise soundness, and representation-level probing, FineLogic enables more interpretable and rigorous assessment beyond final-answer correctness.Leveraging this framework, we conduct a systematic investigation of how different fine-tuning supervision formats impact reasoning ability.Our experiments demonstrate that while natural language supervision leads to strong generalization and benchmark gains, symbolic styles better support minimal, rule-aligned reasoning structures.Furthermore, representationlevel probing reveals that SFT primarily affects how models generate stepwise solutions rather than their ability to predict answers directly.These findings offer practical guidance for designing supervision strategies tailored to different reasoning objectives and highlight the importance of evaluating both behavioral and internal reasoning quality when advancing LLM reasoning systems.</p>
<p>Limitations</p>
<p>Our work has two primary limitations.First, our evaluation is conducted on a fixed suite of datasets.While these were chosen for their diversity, this necessarily scopes our findings to these specific benchmarks, and the observed model behaviors may not generalize to the entire domain of logical reasoning tasks.Second, our analysis of supervision styles treats each format in isolation.Consequently, the potential benefits of hybrid training strategies, which could combine the complementary strengths of natural language and symbolic formats, remain unexplored in this study.et al., 2023) and use the 500 hardest 5-hop problems.(The 3-hop problems used for fine-tuning are described in Section 4).</p>
<p>A Detailed Experimental Setup</p>
<p>A.2 Detailed Baseline Methods</p>
<p>LOGIPT (Feng et al., 2023) An approach where the language model is trained to directly generate symbolic reasoning steps, emulating a logical solver and bypassing an explicit natural languageto-symbol parsing stage.</p>
<p>Selection-Inference (Creswell et al., 2022) A method that performs multi-step reasoning through an iterative process, alternating between a "selection" step to identify relevant facts from the context and an "inference" step to derive new conclusions.</p>
<p>LogicLM (Pan et al., 2023) A neuro-symbolic framework where an LLM first translates a problem into a symbolic representation.A deterministic solver then performs the logical inference, and the LLM interprets the result back into natural language.It includes a self-refinement mechanism that uses solver feedback to correct translation errors.</p>
<p>SymbCoT (Xu et al., 2024) A framework that integrates symbolic logic into the Chain-of-Thought process.It translates the problem into a symbolic format and then generates reasoning steps using formal inference rules, often employing a verifier to check the logical soundness of the chain.</p>
<p>B Representation-Level Probing Implementation Details</p>
<p>We design three probing tasks to assess whether the model's internal representations capture reasoningrelevant information during multi-step logical problem solving.All probing experiments are conducted on a subset of the FLD dataset, specifically the 550 most complex problems requiring 10-20 reasoning steps.We use 450 problems for training and 100 for evaluation.</p>
<p>B.1 Representation Extraction</p>
<p>For all probing tasks, we extract the hidden state of the final token from the last transformer layer after processing the input prefix.The prefix consists of all reasoning steps up to a target step k (i.e., steps 1 to k), and the final-token representation is treated as a summary of the model's internal reasoning state at that point.</p>
<p>B.2 Probing Model</p>
<p>We use a lightweight yet effective classifier to probe the information contained in these hidden states.Specifically, we adopt a logistic regression classifier with feature standardization and 5fold cross-validation for hyperparameter selection.This setup ensures a simple and interpretable linear decision boundary while maintaining robustness against overfitting.The classifier is trained solely on the extracted representations, while the underlying language model remains frozen throughout the probing process.</p>
<p>B.3 Task 1: Correctness Spanning Steps</p>
<p>This task evaluates how early in the reasoning process the model internalizes the correct final answer.</p>
<p>For a problem requiring n reasoning steps, we:</p>
<p>â€¢ Generate n input prefixes, each ending at step i, where i âˆˆ [1, n].</p>
<p>â€¢ Train a probing classifier to predict the groundtruth label (True / False) based on the representation at each prefix.</p>
<p>â€¢ For each test sample, we identify the smallest i such that the classifier correctly predicts the label at step i but fails at step i âˆ’ 1.</p>
<p>The correctness spanning length is defined as n âˆ’ i, capturing how early the model "knows" the correct answer.</p>
<p>B.4 Task 2: Redundant Facts Identification</p>
<p>This task assesses whether the model can distinguish between relevant and irrelevant facts.For each sample:</p>
<p>â€¢ We locate the point after all facts and the hypothesis have been presented.</p>
<p>â€¢ We construct six variants of the input: three with necessary facts (used later in the proof), and three with redundant facts (unused in any proof step).</p>
<p>â€¢ The classifier is trained to predict whether the appended facts are necessary or redundant based on the updated representation.</p>
<p>This task tests whether the model encodes awareness of which premises are logically relevant for solving the task.</p>
<p>B.5 Task 3: Next-Step Derivability</p>
<p>This task probes whether the model can determine which steps are logically available at a given point in the proof.For each sample:</p>
<p>â€¢ We randomly select six intermediate steps.</p>
<p>â€¢ At each step, we append three valid next steps (that are inferable from the current context) and three invalid steps (that appear later in the proof but are not yet derivable).</p>
<p>â€¢ The classifier is trained to distinguish between currently valid and invalid steps.</p>
<p>This task evaluates whether the model has encoded an implicit understanding of the forward progression of logical inference.</p>
<p>C Additional Experiments</p>
<p>This section provides further details and aggregate analysis of our experimental results.</p>
<p>Aggregate Findings Across Models.To broaden our findings, we evaluated two additional models, Qwen-2.5-Math-7B and Qwen-3-8B.The results, shown in Tables 5, 6 and 7 Table 6: Stepwise soundness after </think> on Qwen-2.5-7B-Math-Instruct and Qwen-3-8B.Per-model column-wise maxima are in bold.</p>
<p>â€¢ Natural-language SFT is the most reliable path to higher accuracy.Across all four base models and four datasets, SFT-NL consistently delivers the largest and most robust gains in finalanswer correctness, reaffirming that aligning with the model's pre-training modality is most effective for generalization.â€¢ Symbolic curricula with redundant facts improve chain integrity.SFT settings that retain logically valid but extraneous premises (Symb-Direct and Symb-Struct) tend to improve All-Valid and All-Atomic scores.This suggests that exposing models to "noisy but sound" proof environments encourages more careful and explicit rule application.â€¢ CSS is more sensitive to architecture and RL than to SFT.The larger, RL-enhanced Qwen-3-8B model achieves a notably higher base CSS score than other models.However, its CSS remains largely unchanged by SFT, providing strong evidence that deeper latent reasoning depends more on model design and training regimes like RL, rather than supervised instruction tuning.Qwen-3-8B -10.0 11.1 30.9 SFT-NL 9.8 23.5 36.1 SFT-Symb-Struct 9.9 9.9 32.0 SFT-Symb-Filter 10.0 14.8 39.2 SFT-Symb-Direct 9.7 17.3 35.1 Table 7: Representation-level probing (CSS, RFI, NSD) on Qwen-2.5-7B-Math-Instruct and Qwen-3-8B.Permodel column-wise maxima are in bold; therow denotes the original model.</p>
<p>Detailed Analysis of Performance by Reasoning</p>
<p>Depth.A more granular breakdown of performance by reasoning depth, presented in Table 8, reveals further nuances in these trends.While models fine-tuned with natural language supervision (e.g., Llama-3.1-SFT-NLachieving 89.5% accuracy for 0-3 steps on FLD) perform strongly on tasks with shallower reasoning depths, their symbolic reasoning counterparts tend to exhibit greater resilience as the complexity and number of reasoning steps increase.For instance, on FLD problems requiring 16-19 steps, Llama-3.1-SFT-Symb-Filter(62.5%) and Llama-3.1-SFT-Symb-Struct(58.5%) maintain higher accuracy compared to Llama-3.1-SFT-NL (46.0%), highlighting the benefit of symbolic formats for robust long-chain inference.</p>
<p>D Computational Resources</p>
<p>All supervised fine-tuning experiments were conducted using 4 NVIDIA A100 GPUs.Each model was trained for approximately 2 hours.Evalua- tion on the full suite of benchmarks and diagnostic metrics required an additional 0.5 hours per model.</p>
<p>E Example and Case Study</p>
<p>This section showcases examples from our training dataset along with an error case study.Further details can be found in Figure 4 and Figure 5.</p>
<p>F Prompt Template</p>
<p>This section showcases various prompts, encompassing those designed for reasoning and data generation, as detailed in Figures 6,7,8,9,10,11,12,13,14</p>
<p>G Use of AI Assistants in Manuscript Preparation</p>
<p>During the preparation of this manuscript, we utilized large language models, including GPT and Gemini, as writing assistants.Their role was limited to improving the clarity, grammar, and readability of the text.This included tasks such rephrasing sentences, correcting spelling errors, and ensuring stylistic consistency.The core scientific contributions, experimental design, and analysis presented in this paper are solely the work of the human authors, who take full responsibility for the final content.Natural Language Solution:</p>
<p>Step 1: void -&gt; assump1: Let's assume that this wormhole is catadromous.;</p>
<p>Step 2: fact3 &amp; assump1 -&gt; int1: This wormhole shines chemoimmunology and this is catadromous.;</p>
<p>Step 3: int1 &amp; fact1 -&gt; int2: This introitus is not macerative.;</p>
<p>Step 4: fact2 -&gt; int3: This introitus is macerative.;</p>
<p>Step 5: int2 &amp; int3 -&gt; int4: This is contradiction.;</p>
<p>Step 6: [assump1] &amp; int4 -&gt; int5: This wormhole is not catadromous.;</p>
<p>Step 7: int5 &amp; fact6 -&gt; int6: That gingerroot is noncatadromous thing that is upstager.;</p>
<p>Step 8: int6 -&gt; hypothesis; Step 1: Assume for contradiction: assump1: B(a)</p>
<p>Step 2: From fact3 and assump1, we derive: int1: (A(a) âˆ§ B(a))</p>
<p>Step 3: From int1 and fact1, we derive: int2: Â¬C(b)</p>
<p>Step 4: From fact2, we derive: int3: C(b)</p>
<p>Step 5: Contradiction: int4: âŠ¥</p>
<p>Step 6: By reductio ad absurdum from Step 1: int5: Â¬B(a)</p>
<p>Step 7: From int5 and fact6, we derive: int6: (Â¬B(c) âˆ§ D(c))</p>
<p>Step 8: From int6, we derive the hypothesis: hypothesis The hypothesis to be verified can be translates to the logical formula: Â¬B(c)</p>
<p>We now begin the formal reasoning process:</p>
<p>Step 1: Assume for contradiction: assump1: B(a)</p>
<p>Step 2: From fact3 and assump1, we derive: int1: (A(a) âˆ§ B(a))</p>
<p>Step 3: From int1 and fact1, we derive: int2: Â¬C(b)</p>
<p>Step 4: From fact2, we derive: int3: C(b)</p>
<p>Step 5: Contradiction: int4: âŠ¥</p>
<p>Step 6: By reductio ad absurdum from Step 1: int5: Â¬B(a)</p>
<p>Step 7: From int5 and fact6, we derive: int6: (Â¬B(c) âˆ§ D(c))</p>
<p>Step 8: From int6, we derive the hypothesis: hypothesis</p>
<p>Final conclusion: <strong>PROVED</strong></p>
<p>Reasoning with Filtered Facts: Our problem-solving procedure begins by formalizing all given facts and the hypothesis into first-order logic using standardized predicate definitions.Firstly, we define the entities involved: The hypothesis to be verified can be translates to the logical formula: Â¬B(c)</p>
<p>We now begin the formal reasoning process:</p>
<p>Step 1: Assume for contradiction: assump1: B(a)</p>
<p>Step 2: From fact3 and assump1, we derive: int1: (A(a) âˆ§ B(a))</p>
<p>Step 3: From int1 and fact1, we derive: int2: Â¬C(b)</p>
<p>Step 4: From fact2, we derive: int3: C(b)</p>
<p>Step 5: Contradiction: int4: âŠ¥</p>
<p>Step 6: By reductio ad absurdum from Step 1: int5: Â¬B(a)</p>
<p>Step 7: From int5 and fact4, we derive: int6: (Â¬B(c) âˆ§ D(c))</p>
<p>Step 8: From int6, we derive the hypothesis: hypothesis Final conclusion: <strong>PROVED</strong>  Step 9 is not atomic because it summarizes a multi-step inference (from "fandango happening" to "prostitution happening" using Fact 8) as a single proven outcome without detailing the individual logical operations involved.</p>
<p>Prompt Template: Direct Reasoning</p>
<p>Based on the provided facts, answer the question.Conclude with one of the markers: "<strong>PROVED</strong>" for proven, "<strong>DISPROVED</strong>" for disproven, or "<strong>UNKNOWN</strong>" if uncertain.Facts:{facts} Hypothesis:{hypothesis}</p>
<p>Prompt Template: CoT Reasoning</p>
<p>Based on the provided facts, answer the question.</p>
<p>Conclude with one of the markers: "<strong>PROVED</strong>" for proven, "<strong>DISPROVED</strong>" for disproven, or "<strong>UNKNOWN</strong>" if uncertain.Facts:{facts} Hypothesis:{hypothesis} Let's analyze this step by step.</p>
<p>Prompt Template: Few-Shot Reasoning</p>
<p>Based on the provided facts, answer the question.</p>
<p>Conclude with one of the markers: "<strong>PROVED</strong>" for proven, "<strong>DISPROVED</strong>" for disproven, or "<strong>UNKNOWN</strong>" if uncertain.</p>
<p>Here are some examples of proofs for your reference: [Start of example] For example, for this question: {example} [End of example] You can refer to the proof method of the above question, think step by step, and give the result of this question.Facts:{facts} Hypothesis:{hypothesis}</p>
<p>Prompt Template: Entity and Predicate Extraction</p>
<p>You are a logic analysis expert.Please extract all entities and predicates from the following logical expression translations: Translation content: {formula_translations} facts_formula: {facts_formula} facts: {facts} Special Requirement: If any entity or predicate symbol appears in the facts_formula, but has NO direct definition in the Translation content, you MUST go to the facts section and locate the corresponding natural language description and extract it.Be extremely careful NOT to omit any such entities or predicates.Only skip if it is literally missing from both translation content and facts.Task: 1. Identify all entities involved (e.g., this tablefork, this corsair) and assign variables to them (a, b, c, d...) 2. Identify all predicates (e.g., is a raised, is a collotype) and assign symbols (using the original symbols like A, B, C...) Critical instructions: -Only give full entity and predicate explanations if their definitions appear in the formula_translations or facts.</p>
<p>-Only include entities and predicates that explicitly appear in the provided translation content or facts.</p>
<p>-Do not invent, infer, or add any entities or predicates not directly mentioned in the translations or facts.</p>
<p>-Maintain the original variable identifiers (e.g., 'a' in A(a) corresponds to the first entity).</p>
<p>-Maintain the original predicate identifiers (e.g., 'A' in A(x) represents "x is a raised").</p>
<p>-If a symbol (like 'c', 'F', etc.) doesn't appear in the translations or facts, do not include it in your output.Expected output format: We define the entities involved:</p>
<p>Prompt Template: Logic Proof Translation</p>
<p>You are a logic proof translator.Your task is to translate a logical proof sequence from symbolic notation into a clear, step-by-step explanation.Given: 1.A proof sequence in symbolic form 2. Definitions of entities and predicates used in the proof 3. Logical formula translations Task: Convert the symbolic proof into a concise, step-by-step explanation that a human can easily follow.Proof sequence to translate: {proofs_sentence} Conclusion: {conclusion} Instructions for translation: 1. Split the proof at each semicolon (;) to identify individual steps.2. For each step: First, write a brief, natural language explanation on its own line (e.g."Assume for contradiction: [formula]" or "From [inputs], we derive:").On the next line, write the step label and the logical formula as in the original proof (e.g.assump1: A(b), int2: Â¬B(b), etc.).Do not put both the explanation and the formula on the same line.For assumptions, use "Assume for contradiction: [formula]" then write assumpX: [formula] on the following line.For a standard derived step, use "From [inputs], we derive:" then on the following line write intX: [formula].For contradictions, use "Contradiction:" then on the following line write "âŠ¥".For reductio ad absurdum, use "By reductio ad absurdum from [step number]:" then write the derived conclusion on the next line.Do not skip formula labels or step names.Write both the explanation and the labeled formula.3. Maintain correct logical notation (such as Â¬, âˆ§, âˆ¨, â†’, âˆƒ, âŠ¥, etc.).4. In the final step, clearly relate the conclusion to the hypothesis, if appropriate.5.The output should be only the formatted translation, with no additional commentary.Output format:</p>
<p>Step 1: [Brief explanation] [Formula derived]</p>
<p>Step 2: From [input], we derive: [Formula derived]</p>
<p>Step 3: Assume for contradiction: assumpX: [Formula derived] ... {status_message_content} Final conclusion: {conclusion} The conclusion must use exactly two underscores before and after either PROVED or DISPROVED or UNKNOWN, with no additional spaces or characters.Translate the proof concisely but retain all logical information from the original proof sequence.Do not add any steps not present in the original, and do not skip any steps.Output the translation only, with no additional commentary.Prompt template for logic proof translation.The placeholder {proofs_sentence} is for the symbolic proof sequence.</p>
<p>The placeholder {conclusion} is for the conclusion (<strong>PROVED</strong>/<strong>DISPROVED</strong>/<strong>UNKNOWN</strong>).</p>
<p>The placeholder {status_message_content} is replaced by the string 'The search path has been exhausted without finding a way to either prove or disprove the hypothesis.' if {conclusion} is '<strong>UNKNOWN</strong>', and is an empty string otherwise (which will result in different spacing around it as per the original prompt generation logic).</p>
<p>Prompt Template: Logical Proof Generation</p>
<p>Solve the following logical reasoning problem using formal symbolic logic and provide a step-by-step reasoning process.Follow these steps precisely: 1. Define predicates to represent terms in the problem 2. Translate all facts and the hypothesis into formal logical expressions 3. Derive the conclusion through systematic reasoning 4. State the final conclusion OUTPUT FORMAT: Your answer should follow this format exactly: -Begin with "Our problem-solving procedure begins by formalizing all given facts and the hypothesis into first-order logic using standardized predicate definitions."-Then state "For the predicate, we denote:" followed by your predicate definitions -Translate each fact into a formal logical expression -Present your reasoning steps in numbered format (Step 1:, Step 2:, etc.) -End with "Final conclusion: " followed by either "<strong>PROVED</strong>" or "<strong>DISPROVED</strong>" IMPORTANT: The conclusion must use exactly two underscores before and after either PROVED or DISPROVED, with no additional spaces or characters.</p>
<p>Here is an example problem solution, You need to strictly follow the format like this: Example Solution: {fewshot_example} Now, solve this problem: {question} The answer should be: {label} Provide only the solution with no additional commentary or preamble.</p>
<p>Figure 1 :
1
Figure1: (Left) LLM logical reasoning evaluation: the general benchmark v.s.our fine-grained benchmark FineLogic.(Right) processing a logical reasoning task using natural language v.s.using symbolic methods.</p>
<p>FineLogic</p>
<p>fact ... </user> <assistant> Step 1: ... (end) ... Step i-1: .... (end) Step i: ... (end) ... Step n: ... (end) </assistant> Predict Label { PROVED, DISPROVED } Problem Solution LLM <user> Given the fact ... </user> <assistant> Fact i: ... (end) </assistant> Predict Label { necessary, redundant } Problem One fact to be evaluated LLM <user> Given the fact ... </user> <assistant> Step 1: ... (end) ... Step i: ... (end) ... Step k: ... (end) </assistant>Predict ï¼šLabel { derivable, not derivable</p>
<p>Figure 2 :
2
Figure 2: Overview of FineLogic, where overall benchmark accuracy, stepwise soundness, and representation-level probing are combined for a fine-grained evaluation of LLM's logical reasoning ability.</p>
<p>(a) Performance of Llama-3.1-8B-InstructSFT.(b) Performance of Qwen-2.5-7B-InstructSFT.</p>
<p>Figure 3 :
3
Figure 3: Comparison of SFT variants' performance across different reasoning step ranges in FLD dataset.Both charts show accuracy declines with increasing inference steps, with GPT-4o (Direct) included as a reference.In (a), Llama with SFT-Symb-Filter maintains strong performance even in the 16-19 step range (out-of-distribution), while in (b), Qwen with SFT-NL shows remarkable early-stage reasoning capabilities.</p>
<p>Derivability (NSD) remains a challenge.Performance on the Next-Step Derivability (NSD) task shows no consistent improvement across SFT variants.This indicates that vanilla SFT does not effectively enhance the model's internal representation of what logical steps are derivable next.This highlights a remaining gap in teaching explicit, forward-looking inference, suggesting promising future directions such as combining the NSD probe with self-supervised next-step prediction or curriculum-based RL.</p>
<p>Â¬B(a) â†’ (Â¬B(c) âˆ§ D(c)) Hypothesis: Â¬B(c).</p>
<p>Then the facts can be translated as: Fact 1: (A(a) âˆ§ B(a)) â†’ Â¬C(b) Fact 2: (C(b) âˆ§ E(b)) Fact 3: A(a) Fact 4: Â¬C(c) â†’ (A(c) âˆ§ B(c)) Fact 5: D(c) Fact 6: Â¬B(a) â†’ (Â¬B(c) âˆ§ D(c))</p>
<p>Then the facts can be translated as: Fact 1: (A(a) âˆ§ B(a)) â†’ Â¬C(b) Fact 2: (C(b) âˆ§ E(b)) Fact 3: A(a) Fact 4: Â¬B(a) â†’ (Â¬B(c) âˆ§ D(c))</p>
<p>Figure 4 :
4
Figure 4: Comparison of a logical reasoning problem under four distinct training data settings.The figure illustrates: (a) direct logical symbolization and reasoning ; (b) full formalization in first-order logic, including definitions and fact translation ;(c) reasoning conducted purely in natural language; and (d) formal reasoning using a pre-filtered set of facts.This comparison highlights the differences in processing pathways and the structure of the resulting solutions for each approach.</p>
<p>Figure 5 :
5
Figure 5: Case Study: In the GPT-4o response, Step 8 redundantly confirms the conclusion from Step 7 (int3) by citing Fact11, making it an unnecessary step in the reasoning chain.Step 9 is not atomic because it summarizes a multi-step inference (from "fandango happening" to "prostitution happening" using Fact 8) as a single proven outcome without detailing the individual logical operations involved.</p>
<p>Figure 6 :
6
Figure 6: Prompt template for direct reasoning.Placeholders: {facts}, {hypothesis}.</p>
<p>Figure 7 :
7
Figure 7: Prompt template for Chain-of-Thought (CoT) reasoning.Placeholders: {facts}, {hypothesis}.</p>
<p>Figure 8 :
8
Figure 8: Prompt template for few-shot reasoning.Placeholder: {example}, {facts}, {hypothesis}..</p>
<p>-a: [Corresponding entity, e.g., "This tablefork"] -b: [Corresponding entity, e.g., "This corsair"]...We denote: <a href="x">Original predicate symbol</a>: [Predicate description] <a href="x">Original predicate symbol</a>: [Predicate description]... Please provide only the requested definitions without any additional information or explanations.</p>
<p>Figure 9 :
9
Figure 9: Prompt template for extracting entities and predicates when lowercase variables (entities) are present.Placeholders: {formula_translations}, {facts_formula}, {facts}.</p>
<p>Figure 10 :
10
Figure 10: Prompt template for extracting predicates when no lowercase variables (entities) are present.Placeholders: {formula_translations}, {facts_formula}, {facts}.</p>
<p>Figure 11:Prompt template for logic proof translation.The placeholder {proofs_sentence} is for the symbolic proof sequence.The placeholder {conclusion} is for the conclusion (<strong>PROVED</strong>/<strong>DISPROVED</strong>/<strong>UNKNOWN</strong>).The placeholder {status_message_content} is replaced by the string 'The search path has been exhausted without finding a way to either prove or disprove the hypothesis.' if {conclusion} is '<strong>UNKNOWN</strong>', and is an empty string otherwise (which will result in different spacing around it as per the original prompt generation logic).</p>
<p>Figure 12 :
12
Figure12: Prompt template for generating a logical reasoning process.Placeholders: {question} for the problem statement, {label} for the expected answer (e.g., "<strong>PROVED</strong>"), and {fewshot_example} for a formatted example solution.</p>
<p>Figure 13 :
13
Figure 13: Prompt template for evaluating step validity.Placeholders: {premises_str} (a string listing the premises, e.g., "fact1: Text of fact 1 int1: Text of intermediate 1"), {concl_text_full} (a string representing the conclusion, e.g., "int2: Text of intermediate 2" or "hypothesis: Text of hypothesis").The model is expected to return 'true' or 'false'.</p>
<p>Figure 14 :
14
Figure 14: Prompt template for evaluating step atomicity.Placeholders: {premises_str} (a string listing the premises), {concl_text_full} (a string representing the conclusion).The model is expected to return 'true' or 'false' indicating if the inference from premises to conclusion is a single, indivisible logical step.</p>
<p>, they have not examined which types of supervision are most effective for teaching LLMs to reason.In this work, we focus specifically on this open question.</p>
<p>Table 1 :
1
Sample counts and label types for each dataset.</p>
<p>Table 2 :
2
Overall Benchmark Accuracy on four models with different settings.
, model accuracy de-creases as the required number of reasoning stepsincreases. Nonetheless, our results show that SFTsubstantially improves model robustness, even onlong-chain, out-of-distribution examples. On in-
distribution FLD test problems (0-15 steps), SFT models trained under most styles outperform GPT-4o.For instance, across reasoning depths up to 15, both Llama and Qwen with SFT-NL surpass GPT-4o's performance.</p>
<p>Table 3
3Struct achieve high All Valid scores. Llama withSFT-NL reaches 40.9%, substantially outperform-ing strong baselines like GPT-4o and DeepSeek-R1.The low score of DeepSeek-R1, despite its high ac-curacy, highlights a key insight from FineLogic.Having been trained solely on final-answer correct-ness, it often compresses multiple reasoning stepsand omits necessary premises for intermediate con-clusions. Since guessing the final label is easierthan ensuring every inference is sound, accuracyalone can be misleading. "All Valid" enforces thisstricter requirement, revealing a critical gap in themodel's reasoning fidelity.
reports the results of stepwise soundness evaluation across different models and training settings, offering a more fine-grained view of how well LLMs internalize logical reasoning principles.The All Valid metric measures the proportion of samples where every step is logically valid, a stringent indicator of formal reasoning.We observe that models fine-tuned with SFT-NL and SFT-Symb-The All Relevant metric measures the proportion</p>
<p>Table 3 :
3
Stepwise soundness of various models under settings without inference-time interventions.The best variant of Llama and Qwen is highlighted.
ModelSettingAll Valid All Relevant All AtomicGPT-4oFew-shot7.656.24.4Deepseek-R1 Few-shot13.133.85.7Few-shot4.517.41.6LogiPT5.228.54.9Llama-3.1-SFT-NL40.98.513.08B-InstructSFT-Symb-Struct35.015.424.7SFT-Symb-Filter21.816.912.4SFT-Symb-Direct33.710.225.1Few-shot10.135.12.6LogiPT6.439.85.3Qwen-2.5-SFT-NL27.65.48.57B-InstructSFT-Symb-Struct35.39.119.8SFT-Symb-Filter16.711.710.5SFT-Symb-Direct19.70.311.9</p>
<p>Table 4 :
4
Evaluation of Correctness Spanning Steps (CSS), Redundant Fact Identification (RFI), and Nextstep Derivability (NSD) on Llama and Qwen.'-' indicates the original model.The best variant is highlighted.
ModelSettingCSS RFI NSD-8.09.9 32.0LogiPT8.10.7 44.2Llama-3.1-SFT-NL8.59.9 51.58B-InstructSFT-Symb-Struct 8.7 11.1 36.1SFT-Symb-Filter9.7 11.1 46.4SFT-Symb-Direct 9.0 18.5 41.2-8.67.4 43.3LogiPT8.19.2 43.2Qwen-2.5-SFT-NL8.2 16.0 44.37B-InstructSFT-Symb-Struct 8.5 14.8 43.3SFT-Symb-Filter8.3 16.0 45.4SFT-Symb-Direct 8.6 18.5 43.3</p>
<p>Table 5 :
5
Overall benchmark accuracy on Qwen-2.5-7B-Math-Instruct and Qwen-3-8B across training settings.Per-model column-wise maxima are in bold.
, combined with ourprimary experiments, yield several key aggregatetakeaways:</p>
<p>Table 8 :
8
FLD accuracy breakdown by reasoning step ranges
SFT-Symb-Direct81.058.548.538.527.5Direct69.045.545.038.536.0CoT70.555.536.542.540.5Few-shot63.044.033.527.033.0Logic-LM68.751.231.426.029.2Qwen-2.5 -7B-InstructSymbCoT Sel-Inf LogiPT52.3 49.0 80.539.5 26.5 74.030.7 29.5 64.028.1 27.0 68.019.9 24.5 57.5SFT-NL93.578.567.566.552.5SFT-Symb-Struct74.565.550.043.034.0SFT-Symb-Filter68.059.548.051.545.0SFT-Symb-Direct82.060.051.542.537.0</p>
<p>If any entity or predicate symbol appears in the facts_formula, but has NO direct definition in the Translation content, you MUST go to the facts section and locate the corresponding natural language description and extract it.Be extremely careful NOT to omit any such entities or predicates.Only skip if it is literally missing from both translation content and facts.Task: Identify all predicates and translate each uppercase symbol directly.Critical instructions: -For each uppercase symbol in the facts_formula, provide a direct translation in the format: [SYMBOL]: xxx happened.-<strong>Donotomitanysymbols that appear in facts_formula or translation content.If they appear, they must be translated.</strong>-Onlyincludesymbolsthatactually appear in the facts_formula or translation content.-Donotinventorinfer any entities or relationships not explicitly mentioned.-Ifapredicate'smeaning is clearly defined in the translations or facts, use that definition.-Donotinclude any lowercase symbols or entity definitions as they are not relevant in this case.-Ifsome symbols appear in facts_formula but not in translation content, you can directly translate the entire formula expression containing those symbols rather than translating each symbol individually.For example, for an expression like Â¬Câ†’Â¬(Fâˆ§Â¬E), you don't need to separately translate E if it's not defined elsewhere.
Prompt Template: Predicate Extraction (No Entities)You are a logic analysis expert. Please extract all predicates from thefollowing logical expression translations:Translation content: {formula_translations}facts_formula: {facts_formula}facts: {facts}Special Requirement: Expected output format:We define:A: xxx happened.B: xxx happened.AB: xxx happened...Please provide only the requested definitions without any additional informationor explanations.
AcknowledgmentWe are grateful to Xiuying Chen of MBZUAI for her constructive comments and helpful suggestions, which helped improve our manuscript.Physics of language models: Part 2.1, gradeschool math and the hidden reasoning process.In The Thirteenth International Conference on Learning Representations.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Advances in neural information processing systems. 202033Amanda Askell, and 1 others</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2205.09712and Irina Higgins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint</p>
<p>Jiazhan Feng, Ruochen Xu, Junheng Hao, Hiteshi Sharma, Yelong Shen, Dongyan Zhao, Weizhu Chen, arXiv:2311.06158Language models can be logical solvers. 2023arXiv preprint</p>
<p>Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, arXiv:2212.07919Roscoe: A suite of metrics for scoring step-by-step reasoning. 2022arXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, arXiv:2501.12948Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint</p>
<p>Can llms solve molecule puzzles? a multimodal benchmark for molecular structure elucidation. Kehan Guo, Bozhao Nan, Yujun Zhou, Taicheng Guo, Zhichun Guo, Mihir Surve, Zhenwen Liang, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, Advances in Neural Information Processing Systems. 2024a37</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024barXiv preprint</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Raileanu ; Yue Huang, Chujie Gao, Siyuan Wu, Haoran Wang, Xiangqi Wang, Yujun Zhou, Yanbo Wang, Jiayi Ye, Jiawen Shi ; Yue, Zhengqing Huang, Yujun Yuan, Kehan Zhou, Xiangqi Guo, Haomin Wang, Weixiang Zhuang, Lichao Sun, Jindong Sun, Wang ; Zhenwen, Kehan Liang, Gang Guo, Taicheng Liu, Yujun Guo, Tianyu Zhou, Jiajun Yang, Renjie Jiao, Jipeng Pi, Xiangliang Zhang, Zhang ; Pan, Hritik Lu, Tony Bansal, Jiacheng Xia, Chunyuan Liu, Hannaneh Li, Hao Hajishirzi, Kai-Wei Cheng, Chang, arXiv:2209.00840arXiv:2402.05138Social science meets llms: How reliable are large language models in social simulations?. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, Michel Galley, and Jianfeng Gao1 others. 2022. 2024. 2025. 2024. 2024. 2023. 202331arXiv preprintMathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint Model Setting FLD Accuracy by Step 0-3 4-7 8-11 12-15 16-19</p>
<p>DeepSeek -R1. </p>
<p>Step 11: Hence, there is no contradiction observed that refutes the original claim that closing revisal causes prostitution through the chain found in statements and facts, indirectly. Based on this reasoning, the hypothesis stating that "prostitution is caused by closing revisal is incorrect" does not hold any demonstrable contradictions and isn't proven false with the current set of facts. Final conclusion: <strong>DISPROVED</strong> Facts and Hypothesis Fact1: Benignantness happens. Fact2: That hands happens results in that that Highlanderness but not enlightening reincarnationism occurs is true. Fact3: Calculating jailed takes place. Fact4: If lavishness happens prostitution occurs but fandango does not takes place. Fact5: That readiness but not arundinaceousness occurs are caused by forgettableness. Fact6: If trumpeting Asvins takes place then amauroticness happens and outdoing glasnost does not occurs. Fact7: That fandango takes place yields that lavishness does not takes place. Fact8: Fandango brings about that prostitution but not lavishness occurs. Fact9: Combining Yorkshire takes place. Fact10: Fandango occurs if that closing revisal happens is correct. Fact11: That lavishness does not occurs holds. Fact12: Trumpeting Asvins but not stargazinging happens. Fact13: That quarantining does not occurs leads to that that lavishness occurs and disgruntleding tularaemia does not takes place is right. Llama-3.1 -8B-Instruct GPT-4o Response. 9Step 10: Therefore, the assumption that prostitution is directly caused by closing revisal through the indirect chain (through fandango) leads to a valid conclusion that prostitution occurs. Fact14: Quarantining does not happens if that repressing Sclerodermataceae takes place and asterismalness occurs is wrong. Hypothesis: The statement that prostitution is caused by closing revisal is incorrect</p>            </div>
        </div>

    </div>
</body>
</html>
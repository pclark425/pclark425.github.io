<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8845 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8845</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8845</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-6c4b76232bb72897685d19b3d264c6ee3005bc2b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p>
                <p><strong>Paper Venue:</strong> Journal of machine learning research</p>
                <p><strong>Paper TL;DR:</strong> This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.</p>
                <p><strong>Paper Abstract:</strong> Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8845.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8845.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-to-Text Transfer Transformer (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's primary model: an encoder-decoder Transformer trained in a text-to-text framework with a span-denoising unsupervised objective and then fine-tuned on downstream tasks (GLUE / SuperGLUE / Winograd-style tasks among others).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (baseline encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (closely following Vaswani et al., 2017) with separate encoder and decoder stacks; text-to-text formulation where tasks are prefixed and the model is trained with maximum likelihood. Pre-trained on the C4 cleaned Common Crawl corpus with a denoising (span-masking) objective, then fine-tuned per task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈220M parameters (baseline: encoder 12 layers + decoder 12 layers ≈ 220M)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLUE / SuperGLUE (including RTE, COPA, WSC/DPR referent format, CB, WNLI/WSC-related tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Collections of natural-language understanding tasks that include inference and coreference/Winograd-style pronoun resolution (WSC/WNLI/DPR converted to referent-prediction), commonsense causal choice (COPA), and textual entailment (RTE/CB) — tasks that probe forms of strict logical and commonsense reasoning in language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Pre-train encoder-decoder Transformer with span-denoising objective on C4; cast each downstream benchmark example as text-to-text with a short textual task prefix; fine-tune separately per task (teacher forcing, cross-entropy). For Winograd-style tasks, convert to referent noun prediction and only train on examples with true labels where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (encoder-decoder, denoising) validation averages: GLUE = 83.28, SuperGLUE = 71.36. For ablation, 'no pre-training' achieved GLUE = 66.22 and SuperGLUE = 53.04 (same fine-tuning steps). Per-task scores are reported in the paper's Appendix tables (not all reproduced in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to training from scratch (no pre-training), pre-training yields substantial gains (GLUE +17.06, SuperGLUE +18.32). Compared to other architecture/objective variants in the paper, the encoder-decoder + denoising baseline is the best-performing configuration on the reported suite (see architecture/ objective comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Some downstream subtasks (CoLA, CB, COPA) are low-resource and show high inter-run variance; conversion of Winograd-style tasks (WSC) into referent-prediction removes ~half of WSC training examples and thus reduces available supervised signal; WNLI was not trained on due to overlap with WSC and adversarial properties. The paper also notes that label-mismatch outputs (model emitting tokens not in label set) are treated as incorrect (though not observed in trained models).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Casting diverse language tasks into a unified text-to-text format plus denoising pre-training produces strong performance on benchmarks that include strict reasoning components; pre-training (on C4 with denoising) substantially improves logical/entailment and commonsense tasks compared to no pre-training. The encoder-decoder architecture with denoising objective is recommended in this work for broad-language reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8845.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8845.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-architecture-comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 architecture variants (encoder-decoder, shared enc-dec, smaller enc-dec, decoder-only LM, prefix LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic experimental comparison of Transformer architectural variants (encoder-decoder, shared-parameter encoder-decoder, reduced-depth encoder-decoder, decoder-only language model, and prefix LM) evaluated on the same text-to-text benchmarks including GLUE/SuperGLUE and Winograd-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (architectural variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variants based on Transformer stacks: (a) encoder-decoder (L+L layers, 2P params), (b) encoder-decoder with shared parameters (P params), (c) encoder-decoder with half the layers (P params, M/2 FLOPs), (d) decoder-only LM (P params), and (e) prefix LM (decoder-only with fully-visible masking over the input prefix). Evaluated with both denoising and autoregressive LM objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Variants correspond to parameter counts 2P (≈220M baseline), P (≈110M equivalent), and smaller (6+6 layers ≈ P); paper reports relative P/2/2P but baseline numeric ≈220M is given for 12+12 model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLUE / SuperGLUE suite (includes RTE, CB, COPA, WSC/DPR conversions)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same GLUE/SuperGLUE tasks which probe inference, entailment, coreference, and commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Compare architectures by holding computational cost or parameter budgets approximately constant (discussed in paper) and evaluate with denoising vs autoregressive LM pre-training; examine performance when sharing encoder-decoder parameters and when using prefix LM masking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 2 (selected): With the denoising objective: encoder-decoder (2P) GLUE=83.28, CNNDM=19.24, SQuAD=80.88, SGLUE=71.36; encoder-decoder shared (P) GLUE=82.81, SGLUE=70.73; prefix LM (P) GLUE=81.82, SGLUE=68.11; decoder-only LM and LM-objective variants performed worse on GLUE/SuperGLUE and QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Encoder-decoder with denoising (2P) outperforms shared, smaller, and decoder-only prefix/LMonline variants; shared encoder-decoder (parameter-sharing) performs nearly as well with fewer parameters; halving encoder/decoder layers significantly hurts performance; denoising objective consistently outperforms LM objective across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Decoder-only language-model architectures (causal masking) underperform on many downstream reasoning tasks despite similar compute; prefix LM recovers some but not all performance. Halving layers (to reduce params) yields notable drops in reasoning benchmarks. Paper notes tradeoffs between parameter count and compute and that some architectural differences cannot be simultaneously matched for both.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>For tasks involving logical/entailment/coreference reasoning, an encoder-decoder Transformer pre-trained with a denoising objective gives best performance; parameter-sharing is an effective way to reduce parameter count with modest performance loss; architectural choice (encoder-decoder vs decoder-only) materially affects reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8845.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8845.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-objective-comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised pre-training objective comparison (denoising vs MLM-style vs LM / other corruptions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of a range of unsupervised pre-training objectives (their span-denoising baseline, BERT-style masked language modeling variants, prefix LM, deshuffling, MASS-style, and other corruption schemes) and their downstream impact on benchmarks that include strict reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (same architecture, varying pre-training objective)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer trained with different unsupervised objectives: (a) span-denoising baseline (mask consecutive spans, predict only masked spans with sentinel tokens), (b) BERT-style masked language modeling adapted to encoder-decoder (corrupt 15% tokens, target full sequence), (c) prefix LM, (d) deshuffling, MASS-style and other corruptions (as described), all evaluated in the unified text-to-text framework.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Experiments run with the same base architecture as baseline (encoder-decoder ≈220M) unless otherwise noted.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLUE / SuperGLUE (and other downstream tasks used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring language understanding and reasoning (entailment, commonsense, coreference).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Compare downstream performance after pre-training with each unsupervised objective while holding other factors fixed; adapt certain objectives (like MLM) to the encoder-decoder setup by using the full uncorrupted sequence as the target in some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper summary: denoising objective (baseline) and BERT-style ML objective generally outperform plain LM objectives. Example numbers (from Table 2 architecture rows): encoder-decoder denoising GLUE=83.28, SGLUE=71.36; encoder-decoder with LM objective GLUE=79.56, SGLUE=64.29. BERT-style objective performed best among a subset of objectives (detailed per-objective numbers in Section 3.3 / tables).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Denoising (their baseline) outperforms language-model pre-training; BERT-style MLM adapted to encoder-decoder performed strongly and in some comparisons outperformed prefix LM and other variants. Deshuffling performed substantially worse.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Some objective variants require adaptation to encoder-decoder setting (e.g., MLM originally designed for encoder-only BERT) and performance can depend on whether targets reconstruct only corrupted spans or the entire sequence; computational trade-offs (predicting only masked spans reduces compute). The paper does not claim a single universally optimal objective for all settings but finds denoising and MLM-style approaches superior to pure LM for these reasoning-focused benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Unsupervised objective choice materially affects downstream strict reasoning performance; denoising / masked reconstruction objectives that force the model to reconstruct missing spans lead to better performance on logical/entailment/coreference tasks than standard autoregressive language modeling pre-training. Thus, for improving reasoning in language models, span-masking/denoising-style pre-training is recommended in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
                <li>XLNet: Generalized Autoregressive Pretraining for Language Understanding <em>(Rating: 2)</em></li>
                <li>Language Models are Unsupervised Multitask Learners <em>(Rating: 2)</em></li>
                <li>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding <em>(Rating: 2)</em></li>
                <li>SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems <em>(Rating: 2)</em></li>
                <li>The Winograd Schema Challenge <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8845",
    "paper_id": "paper-6c4b76232bb72897685d19b3d264c6ee3005bc2b",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "T5-baseline",
            "name_full": "Text-to-Text Transfer Transformer (baseline)",
            "brief_description": "The paper's primary model: an encoder-decoder Transformer trained in a text-to-text framework with a span-denoising unsupervised objective and then fine-tuned on downstream tasks (GLUE / SuperGLUE / Winograd-style tasks among others).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (baseline encoder-decoder)",
            "model_description": "Encoder-decoder Transformer (closely following Vaswani et al., 2017) with separate encoder and decoder stacks; text-to-text formulation where tasks are prefixed and the model is trained with maximum likelihood. Pre-trained on the C4 cleaned Common Crawl corpus with a denoising (span-masking) objective, then fine-tuned per task.",
            "model_size": "≈220M parameters (baseline: encoder 12 layers + decoder 12 layers ≈ 220M)",
            "reasoning_task_name": "GLUE / SuperGLUE (including RTE, COPA, WSC/DPR referent format, CB, WNLI/WSC-related tasks)",
            "reasoning_task_description": "Collections of natural-language understanding tasks that include inference and coreference/Winograd-style pronoun resolution (WSC/WNLI/DPR converted to referent-prediction), commonsense causal choice (COPA), and textual entailment (RTE/CB) — tasks that probe forms of strict logical and commonsense reasoning in language.",
            "method_or_approach": "Pre-train encoder-decoder Transformer with span-denoising objective on C4; cast each downstream benchmark example as text-to-text with a short textual task prefix; fine-tune separately per task (teacher forcing, cross-entropy). For Winograd-style tasks, convert to referent noun prediction and only train on examples with true labels where applicable.",
            "performance": "Baseline (encoder-decoder, denoising) validation averages: GLUE = 83.28, SuperGLUE = 71.36. For ablation, 'no pre-training' achieved GLUE = 66.22 and SuperGLUE = 53.04 (same fine-tuning steps). Per-task scores are reported in the paper's Appendix tables (not all reproduced in main text).",
            "baseline_comparison": "Compared to training from scratch (no pre-training), pre-training yields substantial gains (GLUE +17.06, SuperGLUE +18.32). Compared to other architecture/objective variants in the paper, the encoder-decoder + denoising baseline is the best-performing configuration on the reported suite (see architecture/ objective comparisons).",
            "limitations_or_failures": "Some downstream subtasks (CoLA, CB, COPA) are low-resource and show high inter-run variance; conversion of Winograd-style tasks (WSC) into referent-prediction removes ~half of WSC training examples and thus reduces available supervised signal; WNLI was not trained on due to overlap with WSC and adversarial properties. The paper also notes that label-mismatch outputs (model emitting tokens not in label set) are treated as incorrect (though not observed in trained models).",
            "insights_or_conclusions": "Casting diverse language tasks into a unified text-to-text format plus denoising pre-training produces strong performance on benchmarks that include strict reasoning components; pre-training (on C4 with denoising) substantially improves logical/entailment and commonsense tasks compared to no pre-training. The encoder-decoder architecture with denoising objective is recommended in this work for broad-language reasoning tasks.",
            "uuid": "e8845.0",
            "source_info": {
                "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "T5-architecture-comparison",
            "name_full": "T5 architecture variants (encoder-decoder, shared enc-dec, smaller enc-dec, decoder-only LM, prefix LM)",
            "brief_description": "Systematic experimental comparison of Transformer architectural variants (encoder-decoder, shared-parameter encoder-decoder, reduced-depth encoder-decoder, decoder-only language model, and prefix LM) evaluated on the same text-to-text benchmarks including GLUE/SuperGLUE and Winograd-style tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (architectural variants)",
            "model_description": "Variants based on Transformer stacks: (a) encoder-decoder (L+L layers, 2P params), (b) encoder-decoder with shared parameters (P params), (c) encoder-decoder with half the layers (P params, M/2 FLOPs), (d) decoder-only LM (P params), and (e) prefix LM (decoder-only with fully-visible masking over the input prefix). Evaluated with both denoising and autoregressive LM objectives.",
            "model_size": "Variants correspond to parameter counts 2P (≈220M baseline), P (≈110M equivalent), and smaller (6+6 layers ≈ P); paper reports relative P/2/2P but baseline numeric ≈220M is given for 12+12 model.",
            "reasoning_task_name": "GLUE / SuperGLUE suite (includes RTE, CB, COPA, WSC/DPR conversions)",
            "reasoning_task_description": "Same GLUE/SuperGLUE tasks which probe inference, entailment, coreference, and commonsense reasoning.",
            "method_or_approach": "Compare architectures by holding computational cost or parameter budgets approximately constant (discussed in paper) and evaluate with denoising vs autoregressive LM pre-training; examine performance when sharing encoder-decoder parameters and when using prefix LM masking.",
            "performance": "Table 2 (selected): With the denoising objective: encoder-decoder (2P) GLUE=83.28, CNNDM=19.24, SQuAD=80.88, SGLUE=71.36; encoder-decoder shared (P) GLUE=82.81, SGLUE=70.73; prefix LM (P) GLUE=81.82, SGLUE=68.11; decoder-only LM and LM-objective variants performed worse on GLUE/SuperGLUE and QA tasks.",
            "baseline_comparison": "Encoder-decoder with denoising (2P) outperforms shared, smaller, and decoder-only prefix/LMonline variants; shared encoder-decoder (parameter-sharing) performs nearly as well with fewer parameters; halving encoder/decoder layers significantly hurts performance; denoising objective consistently outperforms LM objective across architectures.",
            "limitations_or_failures": "Decoder-only language-model architectures (causal masking) underperform on many downstream reasoning tasks despite similar compute; prefix LM recovers some but not all performance. Halving layers (to reduce params) yields notable drops in reasoning benchmarks. Paper notes tradeoffs between parameter count and compute and that some architectural differences cannot be simultaneously matched for both.",
            "insights_or_conclusions": "For tasks involving logical/entailment/coreference reasoning, an encoder-decoder Transformer pre-trained with a denoising objective gives best performance; parameter-sharing is an effective way to reduce parameter count with modest performance loss; architectural choice (encoder-decoder vs decoder-only) materially affects reasoning performance.",
            "uuid": "e8845.1",
            "source_info": {
                "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "T5-objective-comparison",
            "name_full": "Unsupervised pre-training objective comparison (denoising vs MLM-style vs LM / other corruptions)",
            "brief_description": "Empirical comparison of a range of unsupervised pre-training objectives (their span-denoising baseline, BERT-style masked language modeling variants, prefix LM, deshuffling, MASS-style, and other corruption schemes) and their downstream impact on benchmarks that include strict reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (same architecture, varying pre-training objective)",
            "model_description": "Encoder-decoder Transformer trained with different unsupervised objectives: (a) span-denoising baseline (mask consecutive spans, predict only masked spans with sentinel tokens), (b) BERT-style masked language modeling adapted to encoder-decoder (corrupt 15% tokens, target full sequence), (c) prefix LM, (d) deshuffling, MASS-style and other corruptions (as described), all evaluated in the unified text-to-text framework.",
            "model_size": "Experiments run with the same base architecture as baseline (encoder-decoder ≈220M) unless otherwise noted.",
            "reasoning_task_name": "GLUE / SuperGLUE (and other downstream tasks used in paper)",
            "reasoning_task_description": "Benchmarks requiring language understanding and reasoning (entailment, commonsense, coreference).",
            "method_or_approach": "Compare downstream performance after pre-training with each unsupervised objective while holding other factors fixed; adapt certain objectives (like MLM) to the encoder-decoder setup by using the full uncorrupted sequence as the target in some variants.",
            "performance": "Paper summary: denoising objective (baseline) and BERT-style ML objective generally outperform plain LM objectives. Example numbers (from Table 2 architecture rows): encoder-decoder denoising GLUE=83.28, SGLUE=71.36; encoder-decoder with LM objective GLUE=79.56, SGLUE=64.29. BERT-style objective performed best among a subset of objectives (detailed per-objective numbers in Section 3.3 / tables).",
            "baseline_comparison": "Denoising (their baseline) outperforms language-model pre-training; BERT-style MLM adapted to encoder-decoder performed strongly and in some comparisons outperformed prefix LM and other variants. Deshuffling performed substantially worse.",
            "limitations_or_failures": "Some objective variants require adaptation to encoder-decoder setting (e.g., MLM originally designed for encoder-only BERT) and performance can depend on whether targets reconstruct only corrupted spans or the entire sequence; computational trade-offs (predicting only masked spans reduces compute). The paper does not claim a single universally optimal objective for all settings but finds denoising and MLM-style approaches superior to pure LM for these reasoning-focused benchmarks.",
            "insights_or_conclusions": "Unsupervised objective choice materially affects downstream strict reasoning performance; denoising / masked reconstruction objectives that force the model to reconstruct missing spans lead to better performance on logical/entailment/coreference tasks than standard autoregressive language modeling pre-training. Thus, for improving reasoning in language models, span-masking/denoising-style pre-training is recommended in this work.",
            "uuid": "e8845.2",
            "source_info": {
                "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "Language Models are Unsupervised Multitask Learners",
            "rating": 2
        },
        {
            "paper_title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
            "rating": 2
        },
        {
            "paper_title": "The Winograd Schema Challenge",
            "rating": 2
        }
    ],
    "cost": 0.01355675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</h1>
<p>Colin Raffel<em><br>Noam Shazeer</em><br>Adam Roberts<em><br>Katherine Lee</em><br>Sharan Narang<br>Michael Matena<br>Yanqi Zhou<br>Wei Li<br>Peter J. Liu<br>CRAFFEL@GMAIL.COM<br>NOAM@GOOGLE.COM<br>ADAROB@GOOGLE.COM<br>KATHERINELEE@GOOGLE.COM<br>SHARANNARANG@GOOGLE.COM<br>MMATENA@GOOGLE.COM<br>YANQIZ@GOOGLE.COM<br>MWEILI@GOOGLE.COM<br>PETERJLIU@GOOGLE.COM</p>
<p>Google, Mountain View, CA 94043, USA</p>
<p>Editor: Ivan Titov</p>
<h4>Abstract</h4>
<p>Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. ${ }^{1}$ Keywords: transfer learning, natural language processing, multi-task learning, attentionbased models, deep learning</p>
<h2>1. Introduction</h2>
<p>Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning. This can be loosely viewed as developing general-purpose knowledge that allows the model to "understand" text. This knowledge can range from low-level (e.g. the spelling</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>or meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks). In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task. For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors. These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).</p>
<p>Recently, it has become increasingly common to pre-train the entire model on a data-rich task. Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks. In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009). In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet-for example, the Common Crawl project ${ }^{2}$ produces about 20TB of text data extracted from web pages each month. This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).</p>
<p>This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more. The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field.</p>
<p>The basic idea underlying our work is to treat every text processing problem as a "text-to-text" problem, i.e. taking text as input and producing new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A diagram of our text-to-text framework. Every task we consider-including translation, question answering, and classification-is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. "T5" refers to our model, which we dub the "Text-to-Text Transfer Transformer".
summarization, and sentiment classification, to name a few. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.</p>
<p>We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques. We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider. In order to perform experiments at this scale, we introduce the "Colossal Clean Crawled Corpus" (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models. ${ }^{1}$</p>
<p>The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.</p>
<h1>2. Setup</h1>
<p>Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our "Colossal Clean Crawled Corpus" (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the "Text-to-Text Transfer Transformer" (T5).</p>
<h3>2.1 Model</h3>
<p>Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the "Transformer" architecture (Vaswani et al., 2017). The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials ${ }^{3,4}$ for a more detailed introduction.</p>
<p>The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural variants in Section 3.2.</p>
<p>Overall, our encoder-decoder Transformer implementation closely follows its originallyproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of "blocks", each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent's input to its output. Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>mechanism after each self-attention layer that attends to the output of the encoder. The self-attention mechanism in the decoder also uses a form of autoregressive or causal selfattention, which only allows the model to attend to past outputs. The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent "heads" whose outputs are concatenated before being further processed.</p>
<p>Since self-attention is order-independent (i.e. it is an operation on sets), it is common to provide an explicit position signal to the Transformer. While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a). Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the "key" and "query" being compared in the self-attention mechanism. We use a simplified form of position embeddings where each "embedding" is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding. Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets. In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers. To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme. Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work.</p>
<p>As part of our study, we experiment with the scalability of these models, i.e. how their performance changes as they are made to have more parameters or layers. Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation. As a result, we use a combination of model and data parallelism and train models on "slices" of Cloud TPU Pods. ${ }^{5}$ TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014).</p>
<h1>2.2 The Colossal Clean Crawled Corpus</h1>
<p>Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning. In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data. To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web. Common</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Crawl has previously been used as a source of text data for NLP, for example to train an n-gram language model (Buck et al., 2014), as training data for commonsense reasoning (Trinh and Le, 2018), for mining parallel texts for machine translation (Smith et al., 2013), as a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and even simply as a giant text corpus for testing optimizers (Anil et al., 2019).</p>
<p>Common Crawl is a publicly-available web archive that provides "web extracted text" by removing markup and other non-text content from the scraped HTML files. This process produces around 20TB of scraped text data each month. Unfortunately, the majority of the resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate text like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped text contains content that is unlikely to be helpful for any of the tasks we consider (offensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl's web extracted text:</p>
<ul>
<li>We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark).</li>
<li>We discarded any page with fewer than 3 sentences and only retained lines that contained at least 5 words.</li>
<li>We removed any page that contained any word on the "List of Dirty, Naughty, Obscene or Otherwise Bad Words". ${ }^{6}$</li>
<li>Many of the scraped pages contained warnings stating that Javascript should be enabled so we removed any line with the word Javascript.</li>
<li>Some pages had placeholder "lorem ipsum" text; we removed any page where the phrase "lorem ipsum" appeared.</li>
<li>Some pages inadvertently contained code. Since the curly bracket " ${$ " appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket.</li>
<li>Since some of the scraped pages were sourced from Wikipedia and had citation markers (e.g. [1], [citation needed], etc.), we removed any such markers.</li>
<li>Many pages had boilerplate policy notices, so we removed any lines containing the strings "terms of use", "privacy policy", "cookie policy", "uses cookies", "use of cookies", or "use cookies".</li>
<li>To deduplicate the data set, we discarded all but one of any three-sentence span occurring more than once in the data set.</li>
</ul>
<p>Additionally, since most of our downstream tasks are focused on English-language text, we used langdetect ${ }^{7}$ to filter out any pages that were not classified as English with a probability of at least 0.99 . Our heuristics are inspired by past work on using Common</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Crawl as a source of data for NLP: For example, Grave et al. (2018) also filter text using an automatic language detector and discard short lines and Smith et al. (2013); Grave et al. (2018) both perform line-level deduplication. However, we opted to create a new data set because prior data sets use a more limited set of filtering heuristics, are not publicly available, and/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al., 2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on parallel training data for machine translation (Smith et al., 2013)).</p>
<p>To assemble our base data set, we downloaded the web extracted text from April 2019 and applied the aforementioned filtering. This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB ) but also comprises reasonably clean and natural English text. We dub this data set the "Colossal Clean Crawled Corpus" (or C4 for short) and release it as part of TensorFlow Datasets. ${ }^{8}$ We consider the impact of using various alternative versions of this data set in Section 3.4.</p>
<h1>2.3 Downstream Tasks</h1>
<p>Our goal in this paper is to measure general language learning abilities. As such, we study downstream performance on a diverse set of benchmarks, including machine translation, question answering, abstractive summarization, and text classification. Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation. All data was sourced from TensorFlow Datasets. ${ }^{9}$</p>
<p>GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) each comprise a collection of text classification tasks meant to test general language understanding abilities:</p>
<ul>
<li>Sentence acceptability judgment (CoLA (Warstadt et al., 2018))</li>
<li>Sentiment analysis (SST-2 (Socher et al., 2013))</li>
<li>Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017), QQP (Iyer et al., 2017))</li>
<li>Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))</li>
<li>Coreference resolution (WNLI and WSC (Levesque et al., 2012))</li>
<li>Sentence completion (COPA (Roemmele et al., 2011))</li>
<li>Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))</li>
<li>Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), BoolQ (Clark et al., 2019))</li>
</ul>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We use the data sets as distributed by the GLUE and SuperGLUE benchmarks. For simplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly for SuperGLUE) as a single task by concatenating all of the constituent data sets. As suggested by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) in the combined SuperGLUE task.</p>
<p>The CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a questionanswering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark. In our experiments, the model is fed the question and its context and asked to generate the answer token-by-token. For WMT English to German, we use the same training data as (Vaswani et al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 as a validation set (Bojar et al., 2014). For English to French, we use the standard training data from 2015 and newstest2014 as a validation set (Bojar et al., 2015). For English to Romanian, which is a standard lower-resource machine translation benchmark, we use the train and validation sets from WMT 2016 (Bojar et al., 2016). Note that we only pre-train on English data, so in order to learn to translate a given model will need to learn to generate text in a new language.</p>
<h1>2.4 Input and Output Format</h1>
<p>In order to train a single model on the diverse set of tasks described above, we cast all of the tasks we consider into a "text-to-text" format - that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using "teacher forcing" (Williams and Zipser, 1989)) regardless of the task. To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model.</p>
<p>As an example, to ask the model to translate the sentence "That is good." from English to German, the model would be fed the sequence "translate English to German: That is good." and would be trained to output "Das ist gut." For text classification tasks, the model simply predicts a single word corresponding to the target label. For example, on the MNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies ("entailment"), contradicts ("contradiction"), or neither ("neutral") a hypothesis. With our preprocessing, the input sequence becomes "mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity." with the corresponding target word "entailment". Note that an issue arises if our model outputs text on a text classification task that does not correspond to any of the possible labels (for example if the model outputs "hamburger" when the only possible labels for a task were "entailment", "neutral", or "contradiction"). In this case, we always count the model's output as wrong, though we never observed this behavior in any of our trained models. Note that the choice of text prefix used for a given task is essentially a hyperparameter; we found that changing the exact wording of the prefix had limited impact and so did not perform extensive experiments into different prefix choices. A diagram of our text-to-text framework with a few input/output</p>
<p>examples is shown in Figure 1. We provide full examples of preprocessed inputs for every task we studied in Appendix D.</p>
<p>Our text-to-text framework follows previous work that casts multiple NLP tasks into a common format: McCann et al. (2018) propose the "Natural Language Decathlon", a benchmark that uses a consistent question-answering format for a suite of ten NLP tasks. The Natural Language Decathlon also stipulates that all models must be multi-task, i.e. are able to simultaneously tackle all of the tasks at once. We instead allow for separately fine-tuning the model on each individual task and use short task prefixes instead of an explicit question-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of language models by feeding some input to the model as a prefix and then autoregressively sampling an output. For example, automatic summarization is done by feeding in a document followed by the text "TL;DR:" (short for "too long, didn't read", a common abbreviation) and then the summary is predicted via autoregressive decoding. We mainly consider models that explicitly process an input with an encoder before generating an output with a separate decoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar et al. (2019b) unify many NLP tasks as "span extraction", where text corresponding to possible output choices are appended to the input and the model is trained to extract the input span corresponding to the correct choice. In contrast, our framework also allows for generative tasks like machine translation and abstractive summarization where it is not possible to enumerate all possible output choices.</p>
<p>We were able to straightforwardly cast all of the tasks we considered into a text-to-text format with the exception of STS-B, which is a regression task where the goal is to predict a similarity score between 1 and 5 . We found that most of these scores were annotated in increments of 0.2 , so we simply rounded any score to the nearest increment of 0.2 and converted the result to a literal string representation of the number (e.g. the floating-point value 2.57 would be mapped to the string " 2.6 "). At test time, if the model outputs a string corresponding to a number between 1 and 5 , we convert it to a floating-point value; otherwise, we treat the model's prediction as incorrect. This effectively recasts the STS-B regression problem as a 21-class classification problem.</p>
<p>Separately, we also convert the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more amenable to the text-to-text framework. Examples from the Winograd tasks consist of a text passage containing an ambiguous pronoun that could refer to more than one of the noun phrases in the passage. For example, the passage might be "The city councilmen refused the demonstrators a permit because they feared violence.", which contains the ambiguous pronoun "they" that could refer to "city councilmen" or "demonstrators". We cast the WNLI, WSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in the text passage and asking the model to predict the noun that it refers to. The example mentioned above would be transformed to the input "The city councilmen refused the demonstrators a permit because <em>they</em> feared violence." and the model would be trained to predict the target text "The city councilmen".</p>
<p>For WSC, examples contain the passage, the ambiguous pronoun, a candidate noun, and a True/False label reflecting whether the candidate matches the pronoun (ignoring any articles). We only train on examples with a "True" label since we do not know the correct noun targets for examples with a "False" label. For evaluation, we assign a "True" label if</p>
<p>the words in the model's output are a subset of the words in the candidate noun phrase (or vice versa) and assign a "False" label otherwise. This removes roughly half of the WSC training set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples from DPR are annotated with the correct referent noun, making it easy to use this data set in the format listed above.</p>
<p>The WNLI training and validation sets have a significant overlap with the WSC training set. To avoid leaking validation examples into our training data (a particular issue in the multi-task experiments of Section 3.5.2), we therefore never train on WNLI and never report results on the WNLI validation set. Omitting results on the WNLI validation set is standard practice (Devlin et al., 2018) due to the fact that it is "adversarial" with respect to the training set, i.e. validation examples are all slightly-perturbed versions of training examples with the opposite label. As such, we do not include WNLI in the average GLUE score whenever we report on the validation set (all sections except Section 3.7 where results are presented on the test sets). Converting examples from WNLI to the "referent noun prediction" variant described above is a little more involved; we describe this process in Appendix B.</p>
<h1>3. Experiments</h1>
<p>Recent advances in transfer learning for NLP have come from a wide variety of developments, such as new pre-training objectives, model architectures, unlabeled data sets, and more. In this section, we carry out an empirical survey of these techniques in hopes of teasing apart their contribution and significance. We then combine the insights gained to attain state-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly growing area of research, it is not feasible for us to cover every possible technique or idea in our empirical study. For a broader literature review, we recommend a recent survey by Ruder et al. (2019).</p>
<p>We systematically study these contributions by taking a reasonable baseline (described in Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3 we measure the performance of different unsupervised objectives while keeping the rest of our experimental pipeline fixed. This "coordinate ascent" approach might miss second-order effects (for example, some particular unsupervised objective may work best on a model larger than our baseline setting), but performing a combinatorial exploration of all of the factors in our study would be prohibitively expensive. In future work, we expect it could be fruitful to more thoroughly consider combinations of the approaches we study.</p>
<p>Our goal is to compare a variety of different approaches on a diverse set of tasks while keeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do not exactly replicate existing approaches. For example, "encoder-only" models like BERT (Devlin et al., 2018) are designed to produce a single prediction per input token or a single prediction for an entire input sequence. This makes them applicable for classification or span prediction tasks but not for generative tasks like translation or abstractive summarization. As such, none of the model architectures we consider are identical to BERT or consist of an encoder-only structure. Instead, we test approaches that are similar in spirit-for example, we consider an analogous objective to BERT's "masked language modeling" objective in</p>
<p>Section 3.3 and we consider a model architecture that behaves similarly to BERT on text classification tasks in Section 3.2.</p>
<p>After outlining our baseline experimental setup in the following subsection, we undertake an empirical comparison of model architectures (Section 3.2), unsupervised objectives (Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and scaling (Section 3.6). At the culmination of this section, we combine insights from our study with scale to obtain state-of-the-art results in many tasks we consider (Section 3.7).</p>
<h1>3.1 Baseline</h1>
<p>Our goal for our baseline is to reflect typical, modern practice. We pre-train a standard Transformer (described in Section 2.1) using a simple denoising objective and then separately fine-tune on each of our downstream tasks. We describe the details of this experimental setup in the following subsections.</p>
<h3>3.1.1 Model</h3>
<p>For our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al. (2017). While many modern approaches to transfer learning for NLP use a Transformer architecture consisting of only a single "stack" (e.g. for language modeling (Radford et al., 2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al., 2019)), we found that using a standard encoder-decoder structure achieved good results on both generative and classification tasks. We explore the performance of different model architectures in Section 3.2.</p>
<p>Our baseline model is designed so that the encoder and decoder are each similar in size and configuration to a "BERT ${ }<em _mathrm_ff="\mathrm{ff">{\text {BASE }}$ " (Devlin et al., 2018) stack. Specifically, both the encoder and decoder consist of 12 blocks (each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network). The feed-forward networks in each block consist of a dense layer with an output dimensionality of $d</em>$ since our baseline model contains two layer stacks instead of one. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied in the model.}}=3072$ followed by a ReLU nonlinearity and another dense layer. The "key" and "value" matrices of all attention mechanisms have an inner dimensionality of $d_{\mathrm{kv}}=64$ and all attention mechanisms have 12 heads. All other sub-layers and embeddings have a dimensionality of $d_{\text {model }}=768$. In total, this results in a model with about 220 million parameters. This is roughly twice the number of parameters of $\mathrm{BERT}_{\text {BASE }</p>
<h3>3.1.2 Training</h3>
<p>As described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to always train using standard maximum likelihood, i.e. using teacher forcing (Williams and Zipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and Stern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability logit at every timestep).</p>
<p>We pre-train each model for $2^{19}=524,288$ steps on C 4 before fine-tuning. We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,</p>
<p>we "pack" multiple sequences into each entry of the batch ${ }^{10}$ so that our batches contain roughly $2^{16}=65,536$ tokens. In total, this batch size and number of steps corresponds to pre-training on $2^{35} \approx 34 \mathrm{~B}$ tokens. This is considerably less than BERT (Devlin et al., 2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly 2.2 T tokens. Using only $2^{35}$ tokens results in a reasonable computational budget while still providing a sufficient amount of pre-training for acceptable performance. We consider the effect of pre-training for more steps in Sections 3.6 and 3.7. Note that $2^{35}$ tokens only covers a fraction of the entire C 4 data set, so we never repeat any data during pre-training.</p>
<p>During pre-training, we use an "inverse square root" learning rate schedule: $1 / \sqrt{\max (n, k)}$ where $n$ is the current training iteration and $k$ is the number of warm-up steps (set to $10^{4}$ in all of our experiments). This sets a constant learning rate of 0.01 for the first $10^{4}$ steps, then exponentially decays the learning rate until pre-training is over. We also experimented with using a triangular learning rate (Howard and Ruder, 2018), which produced slightly better results but requires knowing the total number of training steps ahead of time. Since we will be varying the number of training steps in some of our experiments, we opt for the more generic inverse square root schedule.</p>
<p>Our models are fine-tuned for $2^{18}=262,144$ steps on all tasks. This value was chosen as a trade-off between the high-resource tasks (i.e. those with large data sets), which benefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit quickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e. $2^{16}$ tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save a checkpoint every 5,000 steps and report results on the model checkpoint corresponding to the highest validation performance. For models fine-tuned on multiple tasks, we choose the best checkpoint for each task independently. For all of the experiments except those in Section 3.7, we report results in the validation set to avoid performing model selection on the test set.</p>
<h1>3.1.3 Vocabulary</h1>
<p>We use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens (Sennrich et al., 2015; Kudo, 2018). For all experiments, we use a vocabulary of 32,000 wordpieces. Since we ultimately fine-tune our model on English to German, French, and Romanian translation, we also require that our vocabulary covers these non-English languages. To address this, we classified pages from the Common Crawl scrape used in C4 as German, French, and Romanian. Then, we trained our SentencePiece model on a mixture of 10 parts of English C4 data with 1 part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of our model. Note that our vocabulary makes it so that our model can only process a predetermined, fixed set of languages.</p>
<h3>3.1.4 Unsupervised Objective</h3>
<p>Leveraging unlabeled data to pre-train our model necessitates an objective that does not require labels but (loosely speaking) teaches the model generalizable knowledge that will be</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Schematic of the objective we use in our baseline model. In this example, we process the sentence "Thank you for inviting me to your party last week." The words "for", "inviting" and "last" (marked with an $\times$ ) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as $\langle X&gt;$ and $\langle Y\rangle$ ) that is unique over the example. Since "for" and "inviting" occur consecutively, they are replaced by a single sentinel $\langle X&gt;$. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token $&lt;\mathrm{Z}&gt;$.
useful in downstream tasks. Preliminary work that applied the transfer learning paradigm of pre-training and fine-tuning all of the model's parameters to NLP problems used a causal language modeling objective for pre-training (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018). However, it has recently been shown that "denoising" objectives (Devlin et al., 2018; Taylor, 1953) (also called "masked language modeling") produce better performance and as a result they have quickly become standard. In a denoising objective, the model is trained to predict missing or otherwise corrupted tokens in the input. Inspired by BERT's "masked language modeling" objective and the "word dropout" regularization technique (Bowman et al., 2015), we design an objective that randomly samples and then drops out $15 \%$ of tokens in the input sequence. All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence. The sentinel IDs are special tokens which are added to our vocabulary and do not correspond to any wordpiece. The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. Our choices to mask consecutive spans of tokens and only predict dropped-out tokens were made to reduce the computational cost of pre-training. We perform thorough investigation into pre-training objectives in Section 3.3. An example of the transformation resulting from applying this objective is shown in Figure 2. We empirically compare this objective to many other variants in Section 3.3.</p>
<h1>3.1.5 Baseline Performance</h1>
<p>In this section, we present results using the baseline experimental procedure described above to get a sense of what kind of performance to expect on our suite of downstream tasks. Ideally, we would repeat every experiment in our study multiple times to get a confidence interval on our results. Unfortunately, this would be prohibitively expensive due to the large</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GLUE</th>
<th style="text-align: center;">CNNDM</th>
<th style="text-align: center;">SQuAD</th>
<th style="text-align: center;">SGLUE</th>
<th style="text-align: center;">EnDe</th>
<th style="text-align: center;">EnFr</th>
<th style="text-align: center;">EnRo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\star$ Baseline average</td>
<td style="text-align: center;">$\mathbf{8 3 . 2 8}$</td>
<td style="text-align: center;">$\mathbf{1 9 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 8 8}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{2 6 . 9 8}$</td>
<td style="text-align: center;">$\mathbf{3 9 . 8 2}$</td>
<td style="text-align: center;">$\mathbf{2 7 . 6 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Baseline standard deviation</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.108</td>
</tr>
<tr>
<td style="text-align: left;">No pre-training</td>
<td style="text-align: center;">66.22</td>
<td style="text-align: center;">17.60</td>
<td style="text-align: center;">50.31</td>
<td style="text-align: center;">53.04</td>
<td style="text-align: center;">25.86</td>
<td style="text-align: center;">$\mathbf{3 9 . 7 7}$</td>
<td style="text-align: center;">24.04</td>
</tr>
</tbody>
</table>
<p>Table 1: Average and standard deviation of scores achieved by our baseline model and training procedure. For comparison, we also report performance when training on each task from scratch (i.e. without any pre-training) for the same number of steps used to fine-tune the baseline model. All scores in this table (and every table in our paper except Table 14) are reported on the validation sets of each data set.
number of experiments we run. As a cheaper alternative, we train our baseline model 10 times from scratch (i.e. with different random initializations and data set shuffling) and assume that the variance over these runs of the base model also applies to each experimental variant. We don't expect most of the changes we make to have a dramatic effect on the inter-run variance, so this should provide a reasonable indication of the significance of different changes. Separately, we also measure the performance of training our model for $2^{18}$ steps (the same number we use for fine-tuning) on all downstream tasks without pre-training. This gives us an idea of how much pre-training benefits our model in the baseline setting.</p>
<p>When reporting results in the main text, we only report a subset of the scores across all the benchmarks to conserve space and ease interpretation. For GLUE and SuperGLUE, we report the average score across all subtasks (as stipulated by the official benchmarks) under the headings "GLUE" and "SGLUE". For all translation tasks, we report the BLEU score (Papineni et al., 2002) as provided by SacreBLEU v1.3.0 (Post, 2018) with "exp" smoothing and "intl" tokenization. We refer to scores for WMT English to German, English to French, and English to Romanian as EnDe, EnFr, and EnRo, respectively. For CNN/Daily Mail, we find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F metrics (Lin, 2004) to be highly correlated so we report the ROUGE-2-F score alone under the heading "CNNDM". Similarly, for SQuAD we find the performance of the "exact match" and "F1" scores to be highly correlated so we report the "exact match" score alone. We provide every score achieved on every task for all experiments in Table 16, Appendix E.</p>
<p>Our results tables are all formatted so that each row corresponds to a particular experimental configuration with columns giving the scores for each benchmark. We will include the mean performance of the baseline configuration in most tables. Wherever a baseline configuration appears, we will mark it with a $\star$ (as in the first row of Table 1). We also will boldface any score that is within two standard deviations of the maximum (best) in a given experiment.</p>
<p>Our baseline results are shown in Table 1. Overall, our results are comparable to existing models of similar size. For example, BERT ${ }<em _BASE="{BASE" _text="\text">{\text {BASE }}$ achieved an exact match score of 80.8 on SQuAD and an accuracy of 84.4 on MNLI-matched, whereas we achieve 80.88 and 84.24 , respectively (see Table 16). Note that we cannot directly compare our baseline to BERT ${ }</em>$ because ours is an encoder-decoder model and was pre-trained for roughly $1 / 4$ as many steps. Unsurprisingly, we find that pre-training provides significant gains across almost all benchmarks. The only exception is WMT English to French, which is a large}</p>
<p>enough data set that gains from pre-training tend to be marginal. We include this task in our experiments to test the behavior of transfer learning in the high-resource regime. Since we perform early stopping by selecting the best-performing checkpoint, the large disparity between our baseline and "no pre-training" emphasize how much pre-training improves performance on tasks with limited data. While we do not explicitly measure improvements in data efficiency in this paper, we emphasize that this is one of the primary benefits of the transfer learning paradigm.</p>
<p>As for inter-run variance, we find that for most tasks the standard deviation across runs is smaller than $1 \%$ of the task's baseline score. Exceptions to this rule include CoLA, CB, and COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks. For example, on CB our baseline model had an average F1 score of 91.22 with a standard deviation of 3.237 (see Table 16), which may be partly due to the fact that CB's validation set contains only 56 examples. Note that the GLUE and SuperGLUE scores are computed as the average of scores across the tasks comprising each benchmark. As a result, we caution that the high inter-run variance of CoLA, CB, and COPA can make it harder to compare models using the GLUE and SuperGLUE scores alone.</p>
<h1>3.2 Architectures</h1>
<p>While the Transformer was originally introduced with an encoder-decoder architecture, much modern work on transfer learning for NLP uses alternative architectures. In this section, we review and compare these architectural variants.</p>
<h3>3.2.1 Model Structures</h3>
<p>A major distinguishing factor for different architectures is the "mask" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let $y_{i}$ refer to the $i$ th element of the output sequence and $x_{j}$ refer to the $j$ th entry of the input sequence. $y_{i}$ is computed as $\sum_{j} w_{i, j} x_{j}$, where $w_{i, j}$ is the scalar weight produced by the self-attention mechanism as a function of $x_{i}$ and $x_{j}$. The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any $w_{i, j}$ to zero if $j&gt;i$.</p>
<p>The first model structure we consider is an an encoder-decoder Transformer, which consists of two layer stacks: The encoder, which is fed an input sequence, and the decoder, which produces a new output sequence. A schematic of this architectural variant is shown in the left panel of Figure 4.</p>
<p>The encoder uses a "fully-visible" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a "prefix", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern and appends a special "classification" token to the input. BERT's output</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted $x$ and $y$ respectively. A dark cell at row $i$ and column $j$ indicates that the self-attention mechanism is allowed to attend to input element $j$ at output timestep $i$. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding $i$ and $j$ combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the $i$ th output element from depending on any input elements from "the future". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.
at the timestep corresponding to the classification token is then used to make a prediction for classifying the input sequence.</p>
<p>The self-attention operations in the Transformer's decoder use a "causal" masking pattern. When producing the $i$ th entry of the output sequence, causal masking prevents the model from attending to the $j$ th entry of the input sequence for $j&gt;i$. This is used during training so that the model can't "see into the future" as it produces its output. An attention matrix for this masking pattern is shown in Figure 3, middle.</p>
<p>The decoder in an encoder-decoder Transformer is used to autoregressively produce an output sequence. That is, at each output timestep, a token is sampled from the model's predicted distribution and the sample is fed back into the model to produce a prediction for the next output timestep, and so on. As such, a Transformer decoder (without an encoder) can be used as a language model (LM), i.e. a model trained solely for next-step prediction (Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). This constitutes the second model structure we consider. A schematic of this architecture is shown in Figure 4, middle. In fact, early work on transfer learning for NLP used this architecture with a language modeling objective as a pre-training method (Radford et al., 2018).</p>
<p>Language models are typically used for compression or sequence generation (Graves, 2013). However, they can also be used in the text-to-text framework simply by concatenating the inputs and targets. As an example, consider the case of English to German translation: If we have a training datapoint with input sentence "That is good." and target "Das ist gut.", we would simply train the model on next-step prediction over the concatenated input sequence "translate English to German: That is good. target: Das ist gut." If we wanted to</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use "." to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as $x$ and $y$ respectively. Left: A standard encoder-decoder architecture uses fullyvisible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input.
obtain the model's prediction for this example, the model would be fed the prefix "translate English to German: That is good. target:" and would be asked to generate the remainder of the sequence autoregressively. In this way, the model can predict an output sequence given an input, which satisfies the needs of text-to-text tasks. This approach was recently used to show that language models can learn to perform some text-to-text tasks without supervision (Radford et al., 2019).</p>
<p>A fundamental and frequently cited drawback of using a language model in the text-to-text setting is that causal masking forces the model's representation of the $i$ th entry of the input sequence to only depend on the entries up until $i$. To see why this is potentially disadvantageous, consider the text-to-text framework where the model is provided with a prefix/context before being asked to make predictions (e.g., the prefix is an English sentence and the model is asked to predict the German translation). With fully causal masking, the model's representation of a prefix state can only depend on prior entries of the prefix. So, when predicting an entry of the output, the model will attend to a representation of the prefix that is unnecessarily limited. Similar arguments have been made against using a unidirectional recurrent neural network encoder in sequence-to-sequence models (Bahdanau et al., 2015).</p>
<p>This issue can be avoided in a Transformer-based language model simply by changing the masking pattern. Instead of using a causal mask, we use fully-visible masking during the prefix portion of the sequence. This masking pattern and a schematic of the resulting "prefix LM" (the third model structure we consider) are illustrated in the rightmost panels of Figures 3 and 4, respectively. In the English to German translation example mentioned above, fully-visible masking would be applied to the prefix "translate English to German: That is good. target:" and causal masking would be used during training for predicting the target "Das ist gut." Using a prefix LM in the text-to-text framework was originally proposed by Liu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective on a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder model with parameters shared across the encoder and decoder and with the encoder-decoder attention replaced with full attention across the input and target sequence.</p>
<p>We note that when following our text-to-text framework, the prefix LM architecture closely resembles BERT (Devlin et al., 2018) for classification tasks. To see why, consider an example from the MNLI benchmark where the premise is "I hate pigeons.", the hypothesis is "My feelings towards pigeons are filled with animosity." and the correct label is "entailment". To feed this example into a language model, we would transform it into the sequence "mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target: entailment". In this case, the fully-visible prefix would correspond to the entire input sequence up to the word "target:", which can be seen as being analogous to the "classification" token used in BERT. So, our model would have full visibility over the entire input, and then would be tasked with making a classification by outputting the word "entailment". It is easy for the model to learn to output one of the valid class labels given the task prefix ("mnli" in this case). As such, the main difference between a prefix LM and the BERT architecture is that the classifier is simply integrated into the output layer of the Transformer decoder in the prefix LM.</p>
<h1>3.2.2 Comparing Different Model Structures</h1>
<p>In the interest of experimentally comparing these architectural variants, we would like each model we consider to be equivalent in some meaningful way. We might say that two models are equivalent if they either have the same number of parameters or they require roughly the same amount of computation to process a given (input-sequence, target-sequence) pair. Unfortunately, it is not possible to compare an encoder-decoder model to a language model architecture (comprising a single Transformer stack) according to both of these criteria at the same time. To see why, first note an encoder-decoder model with $L$ layers in the encoder and $L$ layers in the decoder has approximately the same number of parameters as a language model with $2 L$ layers. However, the same $L+L$ encoder-decoder model will have approximately the same computational cost as a language model with only $L$ layers. This is a consequence of the fact that the $L$ layers in the language model must be applied to both the input and output sequence, while the encoder is only applied to the input sequence and the decoder is only applied to the output sequence. Note that these equivalences are approximate - there are some extra parameters in the decoder due to the encoder-decoder attention and there are also some computational costs in the attention layers that are quadratic in the sequence lengths. In practice, however, we observed nearly identical step</p>
<p>times for $L$-layer language models versus $L+L$-layer encoder-decoder models, suggesting a roughly equivalent computational cost. Further, for the model sizes we consider, the number of parameters in the encoder-decoder attention layers is about $10 \%$ of the total parameter count, so we make the simplifying assumption that an $L+L$-layer encoder-decoder model has the same number of parameters as an $2 L$-layer language model.</p>
<p>To provide a reasonable means of comparison, we consider multiple configurations for our encoder-decoder model. We will refer to the number of layers and parameters in a $\mathrm{BERT}_{\text {BASE }}$-sized layer stack as $L$ and $P$, respectively. We will use $M$ to refer to the number of FLOPs required for an $L+L$-layer encoder-decoder model or $L$-layer decoder-only model to process a given input-target pair. In total, we will compare:</p>
<ul>
<li>An encoder-decoder model with $L$ layers in the encoder and $L$ layers in the decoder. This model has $2 P$ parameters and a computation cost of $M$ FLOPs.</li>
<li>An equivalent model, but with parameters shared across the encoder and decoder, resulting in $P$ parameters and an $M$-FLOP computational cost.</li>
<li>An encoder-decoder model with $L / 2$ layers each in the encoder and decoder, giving $P$ parameters and an $M / 2$-FLOP cost.</li>
<li>A decoder-only language model with $L$ layers and $P$ parameters and a resulting computational cost of $M$ FLOPs.</li>
<li>A decoder-only prefix LM with the same architecture (and thus the same number of parameters and computational cost), but with fully-visible self-attention over the input.</li>
</ul>
<h1>3.2.3 ObJeCTives</h1>
<p>As an unsupervised objective, we will consider both a basic language modeling objective as well as our baseline denoising objective described in Section 3.1.4. We include the language modeling objective due to its historic use as a pre-training objective (Dai and Le, 2015; Ramachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al., 2018) as well as its natural fit for the language model architectures we consider. For models that ingest a prefix before making predictions (the encoder-decoder model and prefix LM), we sample a span of text from our unlabeled data set and choose a random point to split it into prefix and target portions. For the standard language model, we train the model to predict the entire span from beginning to end. Our unsupervised denoising objective is designed for text-to-text models; to adapt it for use with a language model we concatenate the inputs and targets as described in Section 3.2.1.</p>
<h3>3.2.4 ReSults</h3>
<p>The scores achieved by each of the architectures we compare are shown in Table 2. For all tasks, the encoder-decoder architecture with the denoising objective performed best. This variant has the highest parameter count $(2 P)$ but the same computational cost as the $P$-parameter decoder-only models. Surprisingly, we found that sharing parameters across the encoder and decoder performed nearly as well. In contrast, halving the number of layers in</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Architecture</th>
<th style="text-align: center;">Objective</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">GLUE</th>
<th style="text-align: center;">CNNDM</th>
<th style="text-align: center;">SQuAD</th>
<th style="text-align: center;">SGLUE</th>
<th style="text-align: center;">EnDe</th>
<th style="text-align: center;">EnFr</th>
<th style="text-align: center;">EnRo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\star$ Encoder-decoder</td>
<td style="text-align: center;">Denoising</td>
<td style="text-align: center;">$2 P$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">$\mathbf{8 3 . 2 8}$</td>
<td style="text-align: center;">$\mathbf{1 9 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 8 8}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{2 6 . 9 8}$</td>
<td style="text-align: center;">$\mathbf{3 9 . 8 2}$</td>
<td style="text-align: center;">$\mathbf{2 7 . 6 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Enc-dec, shared</td>
<td style="text-align: center;">Denoising</td>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">82.81</td>
<td style="text-align: center;">18.78</td>
<td style="text-align: center;">$\mathbf{8 0 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 7 3}$</td>
<td style="text-align: center;">26.72</td>
<td style="text-align: center;">39.03</td>
<td style="text-align: center;">$\mathbf{2 7 . 4 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Enc-dec, 6 layers</td>
<td style="text-align: center;">Denoising</td>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">$M / 2$</td>
<td style="text-align: center;">80.88</td>
<td style="text-align: center;">18.97</td>
<td style="text-align: center;">77.59</td>
<td style="text-align: center;">68.42</td>
<td style="text-align: center;">26.38</td>
<td style="text-align: center;">38.40</td>
<td style="text-align: center;">26.95</td>
</tr>
<tr>
<td style="text-align: left;">Language model</td>
<td style="text-align: center;">Denoising</td>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">74.70</td>
<td style="text-align: center;">17.93</td>
<td style="text-align: center;">61.14</td>
<td style="text-align: center;">55.02</td>
<td style="text-align: center;">25.09</td>
<td style="text-align: center;">35.28</td>
<td style="text-align: center;">25.86</td>
</tr>
<tr>
<td style="text-align: left;">Prefix LM</td>
<td style="text-align: center;">Denoising</td>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">81.82</td>
<td style="text-align: center;">18.61</td>
<td style="text-align: center;">78.94</td>
<td style="text-align: center;">68.11</td>
<td style="text-align: center;">26.43</td>
<td style="text-align: center;">37.98</td>
<td style="text-align: center;">27.39</td>
</tr>
<tr>
<td style="text-align: left;">Encoder-decoder</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">$2 P$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">79.56</td>
<td style="text-align: center;">18.59</td>
<td style="text-align: center;">76.02</td>
<td style="text-align: center;">64.29</td>
<td style="text-align: center;">26.27</td>
<td style="text-align: center;">39.17</td>
<td style="text-align: center;">26.86</td>
</tr>
<tr>
<td style="text-align: left;">Enc-dec, shared</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">79.60</td>
<td style="text-align: center;">18.13</td>
<td style="text-align: center;">76.35</td>
<td style="text-align: center;">63.50</td>
<td style="text-align: center;">26.62</td>
<td style="text-align: center;">39.17</td>
<td style="text-align: center;">27.05</td>
</tr>
<tr>
<td style="text-align: left;">Enc-dec, 6 layers</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">$M / 2$</td>
<td style="text-align: center;">78.67</td>
<td style="text-align: center;">18.26</td>
<td style="text-align: center;">75.32</td>
<td style="text-align: center;">64.06</td>
<td style="text-align: center;">26.13</td>
<td style="text-align: center;">38.42</td>
<td style="text-align: center;">26.89</td>
</tr>
<tr>
<td style="text-align: left;">Language model</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">73.78</td>
<td style="text-align: center;">17.54</td>
<td style="text-align: center;">53.81</td>
<td style="text-align: center;">56.51</td>
<td style="text-align: center;">25.23</td>
<td style="text-align: center;">34.31</td>
<td style="text-align: center;">25.38</td>
</tr>
<tr>
<td style="text-align: left;">Prefix LM</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">79.68</td>
<td style="text-align: center;">17.84</td>
<td style="text-align: center;">76.87</td>
<td style="text-align: center;">64.86</td>
<td style="text-align: center;">26.28</td>
<td style="text-align: center;">37.51</td>
<td style="text-align: center;">26.76</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of the different architectural variants described in Section 3.2.2. We use $P$ to refer to the number of parameters in a 12-layer base Transformer layer stack and $M$ to refer to the FLOPs required to process a sequence using the encoderdecoder model. We evaluate each architectural variant using a denoising objective (described in Section 3.1.4) and an autoregressive objective (as is commonly used to train language models).
the encoder and decoder stacks significantly hurt performance. Concurrent work (Lan et al., 2019) also found that sharing parameters across Transformer blocks can be an effective means of lowering the total parameter count without sacrificing much performance. XLNet also bears some resemblance to the shared encoder-decoder approach with a denoising objective (Yang et al., 2019). We also note that the shared parameter encoder-decoder outperforms the decoder-only prefix LM, suggesting that the addition of an explicit encoder-decoder attention is beneficial. Finally, we confirm the widely-held conception that using a denoising objective always results in better downstream task performance compared to a language modeling objective. This observation has been previously made by Devlin et al. (2018), Voita et al. (2019), and Lample and Conneau (2019) among others. We undertake a more detailed exploration of unsupervised objectives in the following section.</p>
<h1>3.3 Unsupervised Objectives</h1>
<p>The choice of unsupervised objective is of central importance as it provides the mechanism through which the model gains general-purpose knowledge to apply to downstream tasks. This has led to the development of a wide variety of pre-training objectives (Dai and Le, 2015; Ramachandran et al., 2016; Radford et al., 2018; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b; Wang et al., 2019a; Song et al., 2019; Dong et al., 2019; Joshi et al., 2019). In this section, we perform a procedural exploration of the space of unsupervised objectives. In many cases, we will not replicate an existing objective exactly-some will be modified to fit our text-to-text encoder-decoder framework and, in other cases, we will use objectives that combine concepts from multiple common approaches.</p>
<p>Overall, all of our objectives ingest a sequence of token IDs corresponding to a tokenized span of text from our unlabeled text data set. The token sequence is processed to produce a (corrupted) input sequence and a corresponding target. Then, the model is trained as usual</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Objective</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Targets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prefix language modeling</td>
<td style="text-align: left;">Thank you for inviting</td>
<td style="text-align: left;">me to your party last week .</td>
</tr>
<tr>
<td style="text-align: left;">BERT-style Devlin et al. (2018)</td>
<td style="text-align: left;">Thank you <M> <M> me to your party apple week .</td>
<td style="text-align: left;">(original text)</td>
</tr>
<tr>
<td style="text-align: left;">Deshuffling</td>
<td style="text-align: left;">party me for your to . last fun you inviting week Thank</td>
<td style="text-align: left;">(original text)</td>
</tr>
<tr>
<td style="text-align: left;">MASS-style Song et al. (2019)</td>
<td style="text-align: left;">Thank you <M> <M> me to your party <M> week .</td>
<td style="text-align: left;">(original text)</td>
</tr>
<tr>
<td style="text-align: left;">I.i.d. noise, replace spans</td>
<td style="text-align: left;">Thank you <X> me to your party <Y> week .</td>
<td style="text-align: left;"><X> for inviting <Y> last <Z></td>
</tr>
<tr>
<td style="text-align: left;">I.i.d. noise, drop tokens</td>
<td style="text-align: left;">Thank you me to your party week .</td>
<td style="text-align: left;">for inviting last</td>
</tr>
<tr>
<td style="text-align: left;">Random spans</td>
<td style="text-align: left;">Thank you <X> to <Y> week .</td>
<td style="text-align: left;"><X> for inviting me <Y> your party last <Z></td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text "Thank you for inviting me to your party last week ." Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. <M> denotes a shared mask token and <X>, <Y>, and <Z> denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple.
with maximum likelihood to predict the target sequence. We provide illustrative examples of many of the objectives we consider in Table 3.</p>
<h1>3.3.1 Disparate High-Level Approaches</h1>
<p>To begin with, we compare three techniques that are inspired by commonly-used objectives but differ significantly in their approach. First, we include a basic "prefix language modeling" objective as was used in Section 3.2.3. This technique splits a span of text into two components, one to use as inputs to the encoder and the other to use as a target sequence to be predicted by the decoder. Second, we consider an objective inspired by the "masked language modeling" (MLM) objective used in BERT (Devlin et al., 2018). MLM takes a span of text and corrupts $15 \%$ of the tokens. $90 \%$ of the corrupted tokens are replaced with a special mask token and $10 \%$ are replaced with a random token. Since BERT is an encoder-only model, its goal during pre-training is to reconstruct masked tokens at the output of the encoder. In the encoder-decoder case, we simply use the entire uncorrupted sequence as the target. Note that this differs from our baseline objective, which uses only the corrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally, we also consider a basic deshuffling objective as used e.g. in (Liu et al., 2019a) where it was applied to a denoising sequential autoencoder. This approach takes a sequence of tokens, shuffles it, and then uses the original deshuffled sequence as a target. We provide examples of the inputs and targets for these three methods in the first three rows of Table 3.</p>
<p>The performance of these three objectives is shown in Table 4. Overall, we find that the BERT-style objective performs best, though the prefix language modeling objective attains similar performance on the translation tasks. Indeed, the motivation for the BERT objective was to outperform language model-based pre-training. The deshuffling objective performs considerably worse than both prefix language modeling and the BERT-style objective.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/ index.html#data_generators.generator_utils.pack_examples</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8661 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8661</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8661</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-277065560</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.12602v3.pdf" target="_blank">SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Generative machine learning models for exploring chemical space have shown immense promise, but many molecules they generate are too difficult to synthesize, making them impractical for further investigation or development. In this work, we present a novel approach by fine-tuning Meta’s Llama3 Large Language Models (LLMs) to create SynLlama, which generates full synthetic pathways made of commonly accessible building blocks and robust organic reaction templates. SynLlama explores a large synthesizable space using significantly less data, and offers strong performance in both forward and bottom-up synthesis planning compared to other state-of-the-art methods. We find that SynLlama, even without training on external building blocks, can effectively generalize to unseen yet purchasable building blocks, meaning that its reconstruction capabilities extend to a broader synthesizable chemical space than the training data. We also demonstrate the use of SynLlama in a pharmaceutical context for synthesis planning of analog molecules and hit expansion leads for proposed inhibitors of target proteins, offering medicinal chemists a valuable tool for discovery.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8661.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8661.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SynLlama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SynLlama (fine-tuned Llama-3 retrosynthesis LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised-fine-tuned Llama-3 based large language model that generates retrosynthetic RXN sequences and building-block SMILES (SMARTS/SMILES text) to propose synthesizable molecules and analogs together with explicit synthetic pathways constrained to purchasable building blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SynLlama (fine-tuned from Llama-3.2-1B; variants from Llama-3.1-8B also explored)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large language model (transformer-based LLM); instruction-tuned SFT for retrosynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Primarily 1B (Llama-3.2-1B → SynLlama-1B-2M); also evaluated 8B (Llama-3.1-8B) variants</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Supervised fine-tuning on synthetic-pathway text: ~2M retrosynthetic routes (alternate experiments with 100k, 500k, 2M examples) sampled from a defined synthesizable chemical space built from ~230k Enamine building blocks and two RXN template sets (RXN1: 91 templates; RXN2: 115 templates). Representations: SMILES for molecules/BBs, SMARTS for reaction templates.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery (synthesis planning for targets and analogs, synthesizable analog generation for de novo molecules, hit expansion/local SAR exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompted retrosynthesis: LLM outputs sequences of RXN SMARTS and SMILES reactants/products (retrosynthetic deconstruction). Downstream reconstruction maps predicted BBs to purchasable Enamine (or Molport) BBs via nearest-neighbor (SMILES TF-IDF and Morgan fingerprint) search to produce forward synthetic routes and analogs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generative in BB space: can predict BBs outside the Enamine training BB set and identify purchasable 'New BBs' (checked with Molport). Quantitative outcomes: reconstruction (with New BBs) up to 74.1% for 1000 Enamine Diversity targets and 28.7% for 1000 ChEMBL targets; raw SynLlama models (e.g., 8B-500k and 1B-2M) produced complete valid syntheses >50% on ChEMBL without downstream processing in some cases. Similarity metrics for analogs reported as average Tanimoto (Morgan 4096) scores (used in reported benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Tailored to drug-discovery tasks by generating retrosynthetic pathways and sampling analogs constrained to available BBs; evaluated for target-specific applications via docking (AutoDock Vina) and rigorous binding assessment (FEP) for hit expansion, and by preserving drug-like properties (drug-likeness composite score used for some comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Instruction-following: Valid JSON, Template Memorization, BB Selection. Reaction chemistry: Valid SMILES, Matched Reactants, Good Products. Downstream metrics: reconstruction success rate (Enamine-only and with New BBs), Tanimoto similarity (4096-bit Morgan fingerprints), SA score (synthetic accessibility), docking score difference (AutoDock Vina RMSE), FEP ∆G/potency improvements in hit expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SynLlama-1B-2M chosen as main model for speed/accuracy tradeoff. Key results: outperforms baseline synthesizability methods while using 40–60× less training data; reconstruction with New BBs improved coverage to 74.1% (Enamine Diversity) and 28.7% (ChEMBL); SynLlama produced synthesizable analogs for de novo generators (iMiner, Pocket2Mol) where only ~1% of originals were reconstructable — analog docking RMSEs vs targets were 1.11 kcal/mol (iMiner) and 1.44 kcal/mol (Pocket2Mol), within docking uncertainty; analogs typically showed improved SA scores; hit-expansion experiments returned many analogs with equal or better FEP-predicted potency (SARS2 Mpro: 7/8, Thrombin: 5/14, TYK2: 6/11).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to SynNet, ChemProjector, and Synformer, SynLlama achieved higher reconstruction/coverage while using far fewer training examples (SynNet 200k, ChemProjector 128M, Synformer 85M vs SynLlama 2M). Unlike baseline methods limited to a fixed BB set, SynLlama can extrapolate to unseen purchasable BBs (Molport). Retrosynthesis formulation gave better performance vs forward-synthesis formulations for the tasks tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Performance drops when generalizing to out-of-distribution ChEMBL molecules (chemical-space mismatch); some raw outputs contain invalid SMILES; sensitivity to inference hyperparameters (temperature, top-p) — higher exploration increases diversity but reduces raw synthesis validity; model does not encode reaction conditions, selectivity, costs, or protection strategies; reconstruction requires external nearest-neighbor mapping to vendor catalogs (Enamine/Molport); smaller LLM capacity limits absolute performance (authors note larger LLMs may improve results but at higher compute costs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8661.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8661.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (foundation LLM family used as base models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's general-purpose Llama-3 family of transformer language models used as base models that were instruction-supervised fine-tuned to create SynLlama for retrosynthesis and synthesizable molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3 (variants Llama-3.1-8B and Llama-3.2-1B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based large language model (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Variants: 8B (Llama-3.1-8B) and 1B (Llama-3.2-1B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained as general-purpose LLMs (pretraining corpora unspecified in this paper); then supervised fine-tuned (LoRA) on synthetic-pathway text derived from Enamine BBs and RXN templates for the SynLlama task.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Adapted to retrosynthesis/synthesizable molecule generation for drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction-conditioned supervised fine-tuning (SFT) to output structured retrosynthetic JSON: reaction SMARTS and SMILES for intermediates/BBs, followed by reconstruction mapping</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>As base LLMs, provided foundational language and chemical knowledge enabling extrapolation to unseen purchasable BBs when fine-tuned; novelty arises post-fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specialized by SFT on reaction/BB datasets and prompt engineering to produce retrosynthetic outputs suitable for reconstruction into synthesizable molecules and analogs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Benchmarked via SynLlama evaluation suite (instruction-following, reaction chemistry metrics, reconstruction success) after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Smaller Llama-3.2-1B fine-tuned with 2M reactions (SynLlama-1B-2M) achieved comparable performance to larger 8B model when balanced with training data and compute; Llama-3 proved an effective base for creating a task-specialized retrosynthesis LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Authors note that fine-tuned LLMs like Llama-3 can match or exceed chemical language models trained only on chemistry when fine-tuned, and that SFT is data-efficient compared to training chemical-specialist models from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Pretrained LLMs do not include explicit constrained BB/RXN contexts in the context window; without careful SFT and reconstruction, raw outputs can hallucinate novel BBs or invalid SMILES; larger LLMs could improve performance at higher cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8661.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8661.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iMiner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>iMiner (1D-to-3D LSTM de novo generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1D string-based LSTM generative model (SELFIES/SMILES) that optimizes a composite objective combining AutoDock Vina docking score and a drug-likeness score to produce de novo binders (used here for SARS-CoV-2 MPro).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>iMiner</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LSTM sequence model (1D string-to-3D optimized generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained/optimized with a composite objective (AutoDock Vina docking and a custom drug-likeness score derived from ChEMBL property distributions); uses SELFIES/SMILES representations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Structure-based drug discovery — generative binder design (SARS-CoV-2 Mpro in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>De novo generation via LSTM sampling optimizing docking and drug-likeness; output molecules were then post-processed by SynLlama to produce synthesizable analogs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates novel de novo molecules; in the study only ~1% of iMiner outputs were directly reconstructable using SynLlama/ChemProjector (i.e., most were synthetically difficult), necessitating SynLlama analog generation constrained to Enamine BBs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Optimizes docking and drug-likeness during generation; downstream SynLlama analogs evaluated for docking retention and improved synthetic accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Docking score (AutoDock Vina), SA score distribution, similarity (Morgan fingerprints), and reconstruction success via SynLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>500 iMiner molecules for SARS2 MPro: only ~1% reconstructable initially; SynLlama-generated analogs achieved docking RMSE ~1.11 kcal/mol vs targets and improved synthetic accessibility (SA) while maintaining good similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Pocket2Mol outputs, iMiner analogs retained closer similarity and maintained docking scores better; integrating SynLlama as post-processor improves synthesizability of iMiner outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>De novo outputs are often synthetically intractable; need for post-processing (SynLlama) to yield purchasable-synthesis candidates; performance depends on the ease of protein target and inclusion of drug-likeness in generation objective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8661.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8661.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pocket2Mol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pocket2Mol (pocket-aware 3D generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D graph transformer model that generates molecules conditioned on protein pocket structure to optimize docking metrics; used here to produce de novo molecules for Thrombin and TYK2 which were then converted to synthesizable analogs by SynLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pocket2Mol</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>3D graph transformer / pocket-aware generative model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Model trained to sample molecules conditioned on 3D protein pockets (details referenced to Pocket2Mol repository/paper), optimized for docking scores (AutoDock Vina).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Structure-based drug discovery (pocket-conditioned de novo molecule generation for Thrombin and TYK2 in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>3D pocket-conditioned generative sampling (transformer), outputs post-processed by SynLlama to produce synthesizable analogs constrained to Enamine BBs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates novel pocket-optimized molecules; however only ~1% of Pocket2Mol outputs were directly reconstructable into synthesizable paths before SynLlama analog generation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to tailor molecules to protein pockets (docking objective); SynLlama analogs tested for docking retention and improved SA.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Docking (AutoDock Vina), SA score, Tanimoto similarity, reconstruction success via SynLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>For 500 Pocket2Mol-generated molecules per target (Thrombin, TYK2), SynLlama analogs had docking RMSE ~1.44 kcal/mol relative to targets and shifted SA distributions to be more synthesizable, though similarity to originals was often lower than seen with iMiner analogs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Demonstrated that combining pocket-aware de novo generation with SynLlama post-processing yields more synthesizable candidates compared to raw Pocket2Mol outputs; highlights complementary pipeline use.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Raw Pocket2Mol outputs are frequently synthetically intractable; analog generation reduces similarity and may trade off some target-specific features for synthesizability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8661.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8661.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SynNet / ChemProjector / Synformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline synthesizable-molecule generators: SynNet, ChemProjector, Synformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Existing computational approaches for synthesizable molecule construction: SynNet uses MDPs and MLPs to construct synthetic trees; ChemProjector and Synformer are transformer-based decoders for sequential synthesis actions — used as baselines for comparison with SynLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SynNet; ChemProjector; Synformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>SynNet: Markov Decision Process + MLP; ChemProjector: transformer decoder on chemistry actions; Synformer: transformer-based synthesis model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported training data sizes: SynNet used ~200k reactions (RXN1); ChemProjector used 128M reactions (RXN1); Synformer used 85M reactions (RXN2)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large reaction/pathway datasets sampled for training (various sizes above); operate within a fixed building-block search space (typically Enamine Diversity Set or predefined BB catalogs).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Design of synthesizable molecules within constrained building-block spaces for drug discovery and synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Rule- or template-based bottom-up or top-down synthesis planning and generation using building blocks and reaction templates; decoding next actions to assemble molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Tend to generate molecules within the fixed pre-defined BB search space; lack extrapolation to vendor BBs outside training catalogues.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Focus on synthesizability and pathway construction; less generative flexibility outside provided BB sets compared to SynLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reconstruction success (coverage), similarity metrics, synthesizability within Enamine BB space; used as quantitative baselines in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SynLlama outperformed SynNet and ChemProjector on RXN1 benchmarks and matched Synformer on RXN2 benchmarks while using far less training data (2M vs tens to hundreds of millions), mainly because SynLlama can extrapolate to unseen purchasable BBs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Baselines often require orders of magnitude more training data and are limited to a predetermined BB catalog; SynLlama's LLM-based SFT enabled better data efficiency and extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Baseline methods are constrained to predefined BB sets and cannot propose vendor BBs outside their search space; some require heavy compute/training data; may perform better in forward-synthesis tasks depending on formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8661.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8661.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASKCOS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASKCOS (open-source CASP retrosynthesis / synthesizability tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, data-driven computer-assisted synthesis planning tool used in prior work to flag molecules as unsynthesizable; such flagged molecules were used as test cases for SynLlama analog generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ASKCOS</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Computer-assisted synthetic planning (retrosynthesis) system (rule/template and ML-driven components)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Reaction corpora and retrosynthesis data used by the ASKCOS project (external to this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Retrosynthetic planning and synthesizability estimation in organic synthesis / drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrosynthesis prediction and pathway search (not an LLM in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>ASKCOS was used as source of molecules previously identified as unsynthesizable; SynLlama generated analogs for these unsynthesizable molecules with improved SA.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Used here as a source of challenging/unsynthesizable targets to assess SynLlama's ability to propose synthesizable analogs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SA score shifts, docking/property correlations, and similarity between original unsynthesizable targets and SynLlama analogs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>For the set of 500 ASKCOS-identified unsynthesizable molecules, SynLlama analogs showed improved SA scores and reasonable retention of targeted properties/docking scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Demonstrates SynLlama's potential advantage over traditional CASP alone by producing purchasable-analog suggestions where ASKCOS labeled originals as unsynthesizable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>ASKCOS results are used as a benchmark; SynLlama's improvements are measured relative to those identifications but do not replace experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8661.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8661.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmileyLlama (citation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmileyLlama: Modifying large language models for directed chemical space exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) demonstrating modifications of LLMs for chemical-space exploration; cited as related work motivating use of LLMs for chemistry tasks and domain adaptation by fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SmileyLlama (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM modification / chemical application (transformer LLM work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not detailed in this paper (referenced as prior art).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Directed chemical space exploration using LLMs (contextually related to SynLlama approach).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-based directed generation (cited as related technique)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Referenced as an example of LLMs applied to chemistry, not directly produced or used here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Cited to support the feasibility of adapting LLMs to chemical tasks via fine-tuning and prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in the current paper; referenced as related literature.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Included in related-work citation list; informs motivation/background for SynLlama's use of LLM fine-tuning for chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used to contextualize SynLlama within contemporary LLM-for-chemistry literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not discussed in detail here (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SynNet <em>(Rating: 2)</em></li>
                <li>ChemProjector <em>(Rating: 2)</em></li>
                <li>Synformer <em>(Rating: 2)</em></li>
                <li>Pocket2Mol: Efficient molecular sampling based on 3d protein pockets <em>(Rating: 2)</em></li>
                <li>iMiner <em>(Rating: 2)</em></li>
                <li>ASKCOS: Open-Source, Data-Driven Synthesis Planning <em>(Rating: 2)</em></li>
                <li>SmileyLlama: Modifying large language models for directed chemical space exploration <em>(Rating: 2)</em></li>
                <li>The Synthesizability of Molecules Proposed by Generative Models <em>(Rating: 1)</em></li>
                <li>DeepSA: a deep-learning driven predictor of compound synthesis accessibility <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8661",
    "paper_id": "paper-277065560",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "SynLlama",
            "name_full": "SynLlama (fine-tuned Llama-3 retrosynthesis LLM)",
            "brief_description": "A supervised-fine-tuned Llama-3 based large language model that generates retrosynthetic RXN sequences and building-block SMILES (SMARTS/SMILES text) to propose synthesizable molecules and analogs together with explicit synthetic pathways constrained to purchasable building blocks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SynLlama (fine-tuned from Llama-3.2-1B; variants from Llama-3.1-8B also explored)",
            "model_type": "Large language model (transformer-based LLM); instruction-tuned SFT for retrosynthesis",
            "model_size": "Primarily 1B (Llama-3.2-1B → SynLlama-1B-2M); also evaluated 8B (Llama-3.1-8B) variants",
            "training_data": "Supervised fine-tuning on synthetic-pathway text: ~2M retrosynthetic routes (alternate experiments with 100k, 500k, 2M examples) sampled from a defined synthesizable chemical space built from ~230k Enamine building blocks and two RXN template sets (RXN1: 91 templates; RXN2: 115 templates). Representations: SMILES for molecules/BBs, SMARTS for reaction templates.",
            "application_domain": "Drug discovery (synthesis planning for targets and analogs, synthesizable analog generation for de novo molecules, hit expansion/local SAR exploration).",
            "generation_method": "Prompted retrosynthesis: LLM outputs sequences of RXN SMARTS and SMILES reactants/products (retrosynthetic deconstruction). Downstream reconstruction maps predicted BBs to purchasable Enamine (or Molport) BBs via nearest-neighbor (SMILES TF-IDF and Morgan fingerprint) search to produce forward synthetic routes and analogs.",
            "novelty_of_chemicals": "Generative in BB space: can predict BBs outside the Enamine training BB set and identify purchasable 'New BBs' (checked with Molport). Quantitative outcomes: reconstruction (with New BBs) up to 74.1% for 1000 Enamine Diversity targets and 28.7% for 1000 ChEMBL targets; raw SynLlama models (e.g., 8B-500k and 1B-2M) produced complete valid syntheses &gt;50% on ChEMBL without downstream processing in some cases. Similarity metrics for analogs reported as average Tanimoto (Morgan 4096) scores (used in reported benchmarks).",
            "application_specificity": "Tailored to drug-discovery tasks by generating retrosynthetic pathways and sampling analogs constrained to available BBs; evaluated for target-specific applications via docking (AutoDock Vina) and rigorous binding assessment (FEP) for hit expansion, and by preserving drug-like properties (drug-likeness composite score used for some comparisons).",
            "evaluation_metrics": "Instruction-following: Valid JSON, Template Memorization, BB Selection. Reaction chemistry: Valid SMILES, Matched Reactants, Good Products. Downstream metrics: reconstruction success rate (Enamine-only and with New BBs), Tanimoto similarity (4096-bit Morgan fingerprints), SA score (synthetic accessibility), docking score difference (AutoDock Vina RMSE), FEP ∆G/potency improvements in hit expansion.",
            "results_summary": "SynLlama-1B-2M chosen as main model for speed/accuracy tradeoff. Key results: outperforms baseline synthesizability methods while using 40–60× less training data; reconstruction with New BBs improved coverage to 74.1% (Enamine Diversity) and 28.7% (ChEMBL); SynLlama produced synthesizable analogs for de novo generators (iMiner, Pocket2Mol) where only ~1% of originals were reconstructable — analog docking RMSEs vs targets were 1.11 kcal/mol (iMiner) and 1.44 kcal/mol (Pocket2Mol), within docking uncertainty; analogs typically showed improved SA scores; hit-expansion experiments returned many analogs with equal or better FEP-predicted potency (SARS2 Mpro: 7/8, Thrombin: 5/14, TYK2: 6/11).",
            "comparison_to_other_methods": "Compared to SynNet, ChemProjector, and Synformer, SynLlama achieved higher reconstruction/coverage while using far fewer training examples (SynNet 200k, ChemProjector 128M, Synformer 85M vs SynLlama 2M). Unlike baseline methods limited to a fixed BB set, SynLlama can extrapolate to unseen purchasable BBs (Molport). Retrosynthesis formulation gave better performance vs forward-synthesis formulations for the tasks tested.",
            "limitations_and_challenges": "Performance drops when generalizing to out-of-distribution ChEMBL molecules (chemical-space mismatch); some raw outputs contain invalid SMILES; sensitivity to inference hyperparameters (temperature, top-p) — higher exploration increases diversity but reduces raw synthesis validity; model does not encode reaction conditions, selectivity, costs, or protection strategies; reconstruction requires external nearest-neighbor mapping to vendor catalogs (Enamine/Molport); smaller LLM capacity limits absolute performance (authors note larger LLMs may improve results but at higher compute costs).",
            "uuid": "e8661.0",
            "source_info": {
                "paper_title": "SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Llama-3",
            "name_full": "Llama 3 (foundation LLM family used as base models)",
            "brief_description": "Meta's general-purpose Llama-3 family of transformer language models used as base models that were instruction-supervised fine-tuned to create SynLlama for retrosynthesis and synthesizable molecule generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3 (variants Llama-3.1-8B and Llama-3.2-1B)",
            "model_type": "Transformer-based large language model (LLM)",
            "model_size": "Variants: 8B (Llama-3.1-8B) and 1B (Llama-3.2-1B)",
            "training_data": "Pretrained as general-purpose LLMs (pretraining corpora unspecified in this paper); then supervised fine-tuned (LoRA) on synthetic-pathway text derived from Enamine BBs and RXN templates for the SynLlama task.",
            "application_domain": "Adapted to retrosynthesis/synthesizable molecule generation for drug discovery",
            "generation_method": "Instruction-conditioned supervised fine-tuning (SFT) to output structured retrosynthetic JSON: reaction SMARTS and SMILES for intermediates/BBs, followed by reconstruction mapping",
            "novelty_of_chemicals": "As base LLMs, provided foundational language and chemical knowledge enabling extrapolation to unseen purchasable BBs when fine-tuned; novelty arises post-fine-tuning.",
            "application_specificity": "Specialized by SFT on reaction/BB datasets and prompt engineering to produce retrosynthetic outputs suitable for reconstruction into synthesizable molecules and analogs.",
            "evaluation_metrics": "Benchmarked via SynLlama evaluation suite (instruction-following, reaction chemistry metrics, reconstruction success) after fine-tuning.",
            "results_summary": "Smaller Llama-3.2-1B fine-tuned with 2M reactions (SynLlama-1B-2M) achieved comparable performance to larger 8B model when balanced with training data and compute; Llama-3 proved an effective base for creating a task-specialized retrosynthesis LLM.",
            "comparison_to_other_methods": "Authors note that fine-tuned LLMs like Llama-3 can match or exceed chemical language models trained only on chemistry when fine-tuned, and that SFT is data-efficient compared to training chemical-specialist models from scratch.",
            "limitations_and_challenges": "Pretrained LLMs do not include explicit constrained BB/RXN contexts in the context window; without careful SFT and reconstruction, raw outputs can hallucinate novel BBs or invalid SMILES; larger LLMs could improve performance at higher cost.",
            "uuid": "e8661.1",
            "source_info": {
                "paper_title": "SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "iMiner",
            "name_full": "iMiner (1D-to-3D LSTM de novo generator)",
            "brief_description": "A 1D string-based LSTM generative model (SELFIES/SMILES) that optimizes a composite objective combining AutoDock Vina docking score and a drug-likeness score to produce de novo binders (used here for SARS-CoV-2 MPro).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "iMiner",
            "model_type": "LSTM sequence model (1D string-to-3D optimized generator)",
            "model_size": "Not reported in paper",
            "training_data": "Trained/optimized with a composite objective (AutoDock Vina docking and a custom drug-likeness score derived from ChEMBL property distributions); uses SELFIES/SMILES representations.",
            "application_domain": "Structure-based drug discovery — generative binder design (SARS-CoV-2 Mpro in this study).",
            "generation_method": "De novo generation via LSTM sampling optimizing docking and drug-likeness; output molecules were then post-processed by SynLlama to produce synthesizable analogs.",
            "novelty_of_chemicals": "Generates novel de novo molecules; in the study only ~1% of iMiner outputs were directly reconstructable using SynLlama/ChemProjector (i.e., most were synthetically difficult), necessitating SynLlama analog generation constrained to Enamine BBs.",
            "application_specificity": "Optimizes docking and drug-likeness during generation; downstream SynLlama analogs evaluated for docking retention and improved synthetic accessibility.",
            "evaluation_metrics": "Docking score (AutoDock Vina), SA score distribution, similarity (Morgan fingerprints), and reconstruction success via SynLlama.",
            "results_summary": "500 iMiner molecules for SARS2 MPro: only ~1% reconstructable initially; SynLlama-generated analogs achieved docking RMSE ~1.11 kcal/mol vs targets and improved synthetic accessibility (SA) while maintaining good similarity.",
            "comparison_to_other_methods": "Compared to Pocket2Mol outputs, iMiner analogs retained closer similarity and maintained docking scores better; integrating SynLlama as post-processor improves synthesizability of iMiner outputs.",
            "limitations_and_challenges": "De novo outputs are often synthetically intractable; need for post-processing (SynLlama) to yield purchasable-synthesis candidates; performance depends on the ease of protein target and inclusion of drug-likeness in generation objective.",
            "uuid": "e8661.2",
            "source_info": {
                "paper_title": "SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Pocket2Mol",
            "name_full": "Pocket2Mol (pocket-aware 3D generative model)",
            "brief_description": "A 3D graph transformer model that generates molecules conditioned on protein pocket structure to optimize docking metrics; used here to produce de novo molecules for Thrombin and TYK2 which were then converted to synthesizable analogs by SynLlama.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pocket2Mol",
            "model_type": "3D graph transformer / pocket-aware generative model",
            "model_size": "Not reported in paper",
            "training_data": "Model trained to sample molecules conditioned on 3D protein pockets (details referenced to Pocket2Mol repository/paper), optimized for docking scores (AutoDock Vina).",
            "application_domain": "Structure-based drug discovery (pocket-conditioned de novo molecule generation for Thrombin and TYK2 in this work).",
            "generation_method": "3D pocket-conditioned generative sampling (transformer), outputs post-processed by SynLlama to produce synthesizable analogs constrained to Enamine BBs.",
            "novelty_of_chemicals": "Generates novel pocket-optimized molecules; however only ~1% of Pocket2Mol outputs were directly reconstructable into synthesizable paths before SynLlama analog generation.",
            "application_specificity": "Designed to tailor molecules to protein pockets (docking objective); SynLlama analogs tested for docking retention and improved SA.",
            "evaluation_metrics": "Docking (AutoDock Vina), SA score, Tanimoto similarity, reconstruction success via SynLlama.",
            "results_summary": "For 500 Pocket2Mol-generated molecules per target (Thrombin, TYK2), SynLlama analogs had docking RMSE ~1.44 kcal/mol relative to targets and shifted SA distributions to be more synthesizable, though similarity to originals was often lower than seen with iMiner analogs.",
            "comparison_to_other_methods": "Demonstrated that combining pocket-aware de novo generation with SynLlama post-processing yields more synthesizable candidates compared to raw Pocket2Mol outputs; highlights complementary pipeline use.",
            "limitations_and_challenges": "Raw Pocket2Mol outputs are frequently synthetically intractable; analog generation reduces similarity and may trade off some target-specific features for synthesizability.",
            "uuid": "e8661.3",
            "source_info": {
                "paper_title": "SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "SynNet / ChemProjector / Synformer",
            "name_full": "Baseline synthesizable-molecule generators: SynNet, ChemProjector, Synformer",
            "brief_description": "Existing computational approaches for synthesizable molecule construction: SynNet uses MDPs and MLPs to construct synthetic trees; ChemProjector and Synformer are transformer-based decoders for sequential synthesis actions — used as baselines for comparison with SynLlama.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SynNet; ChemProjector; Synformer",
            "model_type": "SynNet: Markov Decision Process + MLP; ChemProjector: transformer decoder on chemistry actions; Synformer: transformer-based synthesis model",
            "model_size": "Reported training data sizes: SynNet used ~200k reactions (RXN1); ChemProjector used 128M reactions (RXN1); Synformer used 85M reactions (RXN2)",
            "training_data": "Large reaction/pathway datasets sampled for training (various sizes above); operate within a fixed building-block search space (typically Enamine Diversity Set or predefined BB catalogs).",
            "application_domain": "Design of synthesizable molecules within constrained building-block spaces for drug discovery and synthesis planning.",
            "generation_method": "Rule- or template-based bottom-up or top-down synthesis planning and generation using building blocks and reaction templates; decoding next actions to assemble molecules.",
            "novelty_of_chemicals": "Tend to generate molecules within the fixed pre-defined BB search space; lack extrapolation to vendor BBs outside training catalogues.",
            "application_specificity": "Focus on synthesizability and pathway construction; less generative flexibility outside provided BB sets compared to SynLlama.",
            "evaluation_metrics": "Reconstruction success (coverage), similarity metrics, synthesizability within Enamine BB space; used as quantitative baselines in comparisons.",
            "results_summary": "SynLlama outperformed SynNet and ChemProjector on RXN1 benchmarks and matched Synformer on RXN2 benchmarks while using far less training data (2M vs tens to hundreds of millions), mainly because SynLlama can extrapolate to unseen purchasable BBs.",
            "comparison_to_other_methods": "Baselines often require orders of magnitude more training data and are limited to a predetermined BB catalog; SynLlama's LLM-based SFT enabled better data efficiency and extrapolation.",
            "limitations_and_challenges": "Baseline methods are constrained to predefined BB sets and cannot propose vendor BBs outside their search space; some require heavy compute/training data; may perform better in forward-synthesis tasks depending on formulation.",
            "uuid": "e8661.4",
            "source_info": {
                "paper_title": "SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "ASKCOS",
            "name_full": "ASKCOS (open-source CASP retrosynthesis / synthesizability tool)",
            "brief_description": "An open-source, data-driven computer-assisted synthesis planning tool used in prior work to flag molecules as unsynthesizable; such flagged molecules were used as test cases for SynLlama analog generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ASKCOS",
            "model_type": "Computer-assisted synthetic planning (retrosynthesis) system (rule/template and ML-driven components)",
            "model_size": "Not applicable",
            "training_data": "Reaction corpora and retrosynthesis data used by the ASKCOS project (external to this paper).",
            "application_domain": "Retrosynthetic planning and synthesizability estimation in organic synthesis / drug discovery.",
            "generation_method": "Retrosynthesis prediction and pathway search (not an LLM in this work).",
            "novelty_of_chemicals": "ASKCOS was used as source of molecules previously identified as unsynthesizable; SynLlama generated analogs for these unsynthesizable molecules with improved SA.",
            "application_specificity": "Used here as a source of challenging/unsynthesizable targets to assess SynLlama's ability to propose synthesizable analogs.",
            "evaluation_metrics": "SA score shifts, docking/property correlations, and similarity between original unsynthesizable targets and SynLlama analogs.",
            "results_summary": "For the set of 500 ASKCOS-identified unsynthesizable molecules, SynLlama analogs showed improved SA scores and reasonable retention of targeted properties/docking scores.",
            "comparison_to_other_methods": "Demonstrates SynLlama's potential advantage over traditional CASP alone by producing purchasable-analog suggestions where ASKCOS labeled originals as unsynthesizable.",
            "limitations_and_challenges": "ASKCOS results are used as a benchmark; SynLlama's improvements are measured relative to those identifications but do not replace experimental validation.",
            "uuid": "e8661.5",
            "source_info": {
                "paper_title": "SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "SmileyLlama (citation)",
            "name_full": "SmileyLlama: Modifying large language models for directed chemical space exploration",
            "brief_description": "Prior work (cited) demonstrating modifications of LLMs for chemical-space exploration; cited as related work motivating use of LLMs for chemistry tasks and domain adaptation by fine-tuning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SmileyLlama (referenced work)",
            "model_type": "LLM modification / chemical application (transformer LLM work)",
            "model_size": "Not reported here",
            "training_data": "Not detailed in this paper (referenced as prior art).",
            "application_domain": "Directed chemical space exploration using LLMs (contextually related to SynLlama approach).",
            "generation_method": "LLM-based directed generation (cited as related technique)",
            "novelty_of_chemicals": "Referenced as an example of LLMs applied to chemistry, not directly produced or used here.",
            "application_specificity": "Cited to support the feasibility of adapting LLMs to chemical tasks via fine-tuning and prompt engineering.",
            "evaluation_metrics": "Not detailed in the current paper; referenced as related literature.",
            "results_summary": "Included in related-work citation list; informs motivation/background for SynLlama's use of LLM fine-tuning for chemistry.",
            "comparison_to_other_methods": "Used to contextualize SynLlama within contemporary LLM-for-chemistry literature.",
            "limitations_and_challenges": "Not discussed in detail here (citation only).",
            "uuid": "e8661.6",
            "source_info": {
                "paper_title": "SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SynNet",
            "rating": 2
        },
        {
            "paper_title": "ChemProjector",
            "rating": 2,
            "sanitized_title": "chemprojector"
        },
        {
            "paper_title": "Synformer",
            "rating": 2
        },
        {
            "paper_title": "Pocket2Mol: Efficient molecular sampling based on 3d protein pockets",
            "rating": 2,
            "sanitized_title": "pocket2mol_efficient_molecular_sampling_based_on_3d_protein_pockets"
        },
        {
            "paper_title": "iMiner",
            "rating": 2
        },
        {
            "paper_title": "ASKCOS: Open-Source, Data-Driven Synthesis Planning",
            "rating": 2,
            "sanitized_title": "askcos_opensource_datadriven_synthesis_planning"
        },
        {
            "paper_title": "SmileyLlama: Modifying large language models for directed chemical space exploration",
            "rating": 2,
            "sanitized_title": "smileyllama_modifying_large_language_models_for_directed_chemical_space_exploration"
        },
        {
            "paper_title": "The Synthesizability of Molecules Proposed by Generative Models",
            "rating": 1,
            "sanitized_title": "the_synthesizability_of_molecules_proposed_by_generative_models"
        },
        {
            "paper_title": "DeepSA: a deep-learning driven predictor of compound synthesis accessibility",
            "rating": 1,
            "sanitized_title": "deepsa_a_deeplearning_driven_predictor_of_compound_synthesis_accessibility"
        }
    ],
    "cost": 0.01813575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models
9 Sep 2025</p>
<p>Kunyang Sun 
Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Dorian Bagni 
Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Joseph M Cavanagh 
Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Yingze Wang 
Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Jacob M Sawyer 
Department of Chemistry
University of Minnesota
207 Pleasant Street SE55455MinneapolisMNUSA</p>
<p>Department of Chemistry
University of Minnesota
207 Pleasant Street SE55455MinneapolisMNUSA</p>
<p>Bo Zhou 
Department of Pharmaceutical Sciences
University of Illinois Chicago
833 S Wood St60612ChicagoILUSA</p>
<p>Contramont Research
94158San FranciscoCAUSA</p>
<p>Department of Pharmaceutical Sciences
University of Illinois Chicago
833 S Wood St60612ChicagoILUSA</p>
<p>Andrew Gritsevskiy 
Contramont Research
94158San FranciscoCAUSA</p>
<p>Contramont Research
94158San FranciscoCAUSA</p>
<p>Oufan Zhang 
Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Teresa Head-Gordon 
Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Department of Chemical and Biomolecular Engineering
University of California
94720BerkeleyCAUSA</p>
<p>Kenneth S. Pitzer Theory Center
Department of Chemistry</p>
<p>Department of Chemical and Biomolecular Engineering
University of California
94720BerkeleyCAUSA</p>
<p>Department of Bioengineering</p>
<p>Department of Bioengineering</p>
<p>SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models
9 Sep 2025FA1DC0DE30B407E775522FD407309D78arXiv:2503.12602v4[cs.LG]
Generative machine learning models for exploring chemical space have shown immense promise, but many molecules they generate are too difficult to synthesize, making them impractical for further investigation or development.In this work, we present a novel approach by fine-tuning Meta's Llama3 Large Language Models (LLMs) to create Syn-Llama, which generates full synthetic pathways made of commonly accessible building blocks and robust organic reaction templates.SynLlama explores a large synthesizable space using significantly less data, and offers strong performance in both forward and bottom-up synthesis planning compared to other state-of-the-art methods.We find that SynLlama, even without training on external building blocks, can effectively generalize to unseen yet purchasable building blocks, meaning that its reconstruction capabilities extend to a broader synthesizable chemical space than the training data.We also demonstrate the use of SynLlama in a pharmaceutical context for synthesis planning of analog molecules and hit expansion leads for proposed inhibitors of target proteins, offering medicinal chemists a valuable tool for discovery.</p>
<p>Introduction</p>
<p>Chemical space is enormous, built up via the exponential rise in functional group combinatorics that define an increasing diverse set of molecules.Traditional approaches that design synthetic pathways of unseen molecules under well-controlled laboratory conditions have been made possible by decades of research in synthetic chemistry, as well as mechanistic studies of key reaction steps and reaction classification 1,2 .Using the wealth of data accumulated in libraries of chemical reactions, expert systems 3,4 have been developed that deploy this knowledge to construct multi-step pathways to specified end-products.Such methods have become a key tool for the bench chemist, as illustrated by the Chematica software and and its follow on commercial product now known as Synthia 4 .</p>
<p>With recent advances in artificial intelligence and deep learning, generative models have begun to contribute to enumerating molecules at the stoichiometric scale.After training on databases containing various small molecules representations [5][6][7][8][9] , string-based 1D generative models and structure-aware 3D de novo methods have paved the way for quick exploration of greater swaths of unseen chemical space [10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27] .However, even with their exceptional generative capabilities, these models still face one major challenge: their proposed de novo molecules lack practical guarantees of synthesizability, which limits their utility in practice [28][29][30] .For generative approaches in drug and materials discovery to fulfill their potential, ensuring synthetic feasibility is essential to bridge the gap between in silico molecule design and the realistic applicability of computationally generated molecules.</p>
<p>In recognition of this dissonance, efforts have been made to address the problem of poor synthesizability of de-novo-generated molecules.One line of research focuses on integrating empirical or deep-learning scoring functions [31][32][33][34][35][36][37] , such as the synthetic accessibility (SA) score 31 and DeepSA score 35 , into the objective functions of learning algorithms.However, optimizing only the synthesizability score can still lead to the generation of unsynthesizable molecules because the scoring functions rely on identifying common fragments or reactive centers in molecules 38 .In addition, they often assign bad scores to complex yet synthesizable molecules that require multi-step synthetic pathways, causing generative models to miss viable candidates 39 .Others have proposed improving synthesizability by building molecules from common molecular fragments [40][41][42][43] , but they still don't guarantee synthesis as these methods do not explicitly consider the reaction pathways to build the molecular candidates.Another line of research integrates the explicit use of computer-assisted synthetic planning (CASP) software 44,45 into the optimization 46 .However, the computational overhead is significant and the quality of the optimized molecules can be variable. 47.</p>
<p>Alternatively, proposing synthesizable molecular candidates using commercially available building blocks and commonly known organic reaction templates 44,48 offers better synthetic tractability over simple molecule scoring.Importantly, this strategy is appealing to bench and medicinal chemists, since it offers actionable synthesis pathways for them to examine, refine, and execute.Some recent models in this direction apply rule-based synthesis and optimization on building blocks or entire synthetic pathways to generate novel molecules with desired chemical properties 4,[49][50][51][52][53] .5][56] For example, SynNet 54 constructs synthetic trees via Markov Decision Processes (MDPs) and uses multilayer perceptrons to choose the next action space.More recent models such as ChemProjector 55 and Synformer 56 use transformers to decode for the next action space and have achieved good empirical performances for target and analog molecule reconstruction.</p>
<p>A compelling alternative is the use of Large Language Models (LLMs) due to their foundational nature and adaptability to downstream tasks. 57LLMs inherently possess extensive chemical knowledge, and recent advancements have focused on extracting and applying this knowledge for predictive and optimization tasks using natural language guidance [58][59][60][61] .Furthermore, after fine-tuning, LLM models can perform as good or better than chemical language models trained solely on chemical representations, all while requiring less data. 27he efficiency and unexpected performance gains from fine-tuning LLMs thus motivates us to explore their potential in more complex tasks, such as synthesis planning, which could pave the way for new chemical discoveries.</p>
<p>Herein, we present SynLlama, an LLM-based tool built on the open-source Llama-3.1-8B and Llama-3.2-1Bfoundation models 62 to deduce synthetic routes for target molecules or structurally related analogs.Specifically, the LLM component of SynLlama operates as a constrained retrosynthesis module that breaks input molecules into building blocks (BBs) via well-validated (RXN) sequences, and the reconstruction module searches commercially available BBs based on LLM predictions and builds up molecules within a diverse yet synthesizable chemical space.As an illustration of utility, SynLlama demonstrates competitive performance in key tasks for drug discovery, including synthesis planning for target and analog molecules of pharmaceutical interest and expansion around existing molecular drug hits and leads.Moreover, because of its generative nature, the LLM component of Syn-Llama has the added ability to explore commercially available building blocks beyond the predefined synthetic space introduced during training -an ability that previous models lack.By integrating molecular design with synthetic feasibility, SynLlama represents a step forward in bridging computational chemistry with synthetic chemistry, providing chemists with actionable and experimentally accessible molecular candidates.</p>
<p>Methods</p>
<p>The SynLlama workflow, illustrated in Figure 1, is designed to generate synthesizable compounds within an expanded chemical space.When an input molecule passes through this workflow, it can either be fully reconstructed through valid synthetic pathways, or the workflow will produce a structurally similar yet synthesizable analog along with its synthesis route.To transform general-purpose LLMs, like the Llama 3 models 62 , into expert models for synthetic pathways, we use three key components: 1) a reliable and diverse set of reaction data that covers a large synthesizable chemical space, 2) an efficient supervised fine-tuning (SFT) strategy to train a general-purpose LLM on these reaction data, and 3) a reconstruction algorithm that can convert the output of the fine-tuned LLM into valid synthesis routes, ensuring the proposed molecules lie within a commercially available chemical search space.These components are crucial for leveraging LLMs, which are known to perform well in diverse chemistry tasks 63,64 , to specialize in synthetic modeling.</p>
<p>RXN 72</p>
<p>Step    Black represents SynLlama's raw retrosynthetic output consisting of RXN sequences and predicted BBs, while colored BBs indicate the top two most similar BBs to the predicted ones from the Enamine building block library.Here, RXN 76 represents amide coupling and RXN 72 represents Suzuki coupling.(e).Reconstructed molecules using the predicted reaction sequences and similar building blocks from the Enamine building block library.In this example, all predicted building blocks are present in the Enamine library, allowing for the complete reconstruction of the input molecule and the generation of close analogs.</p>
<p>Reaction Data for Training and Testing Sets</p>
<p>As illustrated in Figure 1(a), our defined chemical space for training consists of molecules that can be synthesized in at most five steps with Enamine building blocks 9 (BBs) and 2 sets of well-validated common organic reactions (RXNs).To define the training and testing BB data, we apply a time split whereby all Enamine BBs from the August 2024 release serve as the training BBs, and all new BBs from their February 2025 release that were not in the training set comprise the testing BBs.This procedure results in ∼230,000 BBs for training and ∼13,000 BBs for testing.Later in Section 3.1 we consider the target reconstruction for unseen molecules which are taken from the Enamine Diversity Set 9 and ChEMBL dataset 8 .</p>
<p>We define two sets of reaction templates (RXN) which operate on the Enamine BBs.RXN 1 is formulated as a set of 91 reaction templates selected by Gao et al. 54 from the works of Hartenfeller et al. 65 and Button et al. 66 .RXN 2 is comprised of 115 reactions selected by Gao et al. 56 that contains reactions used to create the Enamine REAL space 9 plus some reactions from RXN 1.All reaction templates are accessible to both sets of BBs, thus defining the training and testing chemical spaces.As a result, there are ∼10 30 molecules within this space that can be represented by a synthesis path that comprises a sequence of BBs and RXNs.</p>
<p>To enumerate molecules within this space, we use an iterative approach by selecting RXN templates and searching for compatible BBs.Specifically, as demonstrated in Figure 1(b), the selection of the initial RXN is guided by a probabilistic model based on the number of compatible BBs.Within these compatible BB reactants, the initial BBs are selected at random to form an intermediate via the selected RXN template.This intermediate is then used to match for subsequent RXNs and recruits additional BBs to expand the molecular synthesis pathway until no further reactions are possible or the reaction reaches five steps.</p>
<p>After training on these representations, the resulting LLM will be able to build powerful connections by mapping input molecules to a sequence of BBs and RXNs that creates linear synthesis routes for the testing sets of molecules.Hence, we also construct test sets for the more difficult case of branch synthesis in which molecules have at least one or more reaction steps involving intermediates as reactants.To construct the branching synthesis test sets for both RXN 1 and 2 templates, we keep track of two synthesis trees at a time and check whether the intermediate molecules from both synthesis trees can react further with at least one reaction template from the corresponding RXN.We then filter for molecules that have at least one reaction step with reactants being intermediates to create the two test sets.</p>
<p>Supervised Fine-tuning and Inference from SynLlama</p>
<p>To create the SynLlama model, we need to establish data generation protocols for supervised fine-tuning (SFT) of the Llama 3 models as schematically shown in Figure 1(c).When generating reaction data in text format, we choose to represent the BBs and intermediates along the synthetic pathway using SMILES 67 strings, while RXNs are explicitly defined in the SMARTS 68 format.These structured chemical notations are designed to enhance SynLlama's ability to systematically identify and deconstruct bonds according to RXN templates, effectively dismantling input molecules into building-block-sized fragments.</p>
<p>Since our goal is for SynLlama to learn to link molecules with their synthesis routes, our prompt-response pairs are structured according to retrosynthesis, as depicted in Figure 1(c) and shown in detail in Supplementary Figure S1.Such engineered prompts and responses allow the SynLlama model to learn to construct synthesis pathways for the input molecules by inferring sequences of BBs and RXNs, as well as the intermediate steps.While theoretically the model could predict BBs and RXNs without intermediates, we still include them in individual reaction steps in the hope of activating the inherent chemical knowledge in LLMs and enhancing their understanding of synthesis patterns.We have included some additional design choice analysis of forward synthesis versus retrosynthesis and whether to apply a drug-like product molecule filtering in the Supplementary Information.</p>
<p>We have considered both Llama-3.1-8B(8 Billion parameters) and Llama-3.2-1B(1 Billion parameters) for SFT using datasets of varying sizes.Specifically, Llama-3.1-8B is finetuned with datasets containing 100k and 500k synthesis routes, requiring 40 and 240 A-40 hours respectively.Llama-3.2-1B, on the other hand, is trained with datasets containing 500k and 2M synthesis entries, requiring approximately 60 and 240 hours respectively.Herein, we refer to the trained models as SynLlama-(parameter count)-(number of reactions trained) in the first part of Results.For example, SynLlama-1B-2M represents a model fine-tuned from Llama-3.2-1B with 2M synthesis routes.Further details of the SFT are provided in the Supplementary Information.</p>
<p>After training the SynLlama models, we apply the consistent prompt setup to perform inferences on molecules.For any given molecule, the SynLlama models predict reaction sequences in SMARTS format and generate SMILES strings for all the reactants, products, and BBs for the reactions they predict.During inference time, the instruction to SynLlama remains the same, and SMILES strings in the input section are substituted with ones specified by the user.As depicted in Supplementary Figure S1, the responses of the SynLlama models follow the output structures enforced by the prepared training prompt-response pairs.To be more specific, the output response section consists of two parts: reactions and building blocks.In the 'reactions' component, the model sequentially deconstructs the target molecule by breaking bonds using provided reaction templates in a retrosynthetic manner.At each step, it predicts a reaction template, along with the reactants and product of the reaction, continuing until no further reactions are possible.Then, in the 'building blocks' section, the model compiles all building blocks, namely, reactants from each reaction that are not products in other reactions, identified from the 'reaction' section.A visual representation of the inference process is illustrated in black ink in Figure 1(d).</p>
<p>SynLlama Model Benchmarks</p>
<p>Since we are formulating the synthetic tasks using purely language-based modeling, where all reactions are expressed in SMARTS templates and molecules in SMILES strings, it is important to quantify the capacity of SynLlama for instruction following and comprehension of reaction chemistry.To assess SynLlama's ability to follow instructions, we select three benchmarking criteria as shown in Table 1.The first is "Valid JSON," which examines whether the output format is a parsable JSON following the fine-tuned templates that will be necessary for the downstream reconstruction algorithm.The second criterion is "Template Memorization," which assesses the model's ability to memorize the provided reaction templates that define our synthesizable chemical space.Lastly, we benchmark on "BB Selection," which evaluates whether the "building blocks" section in the responses can accurately identify and select all the building blocks from the "reactions" section of the responses.</p>
<p>To assess SynLlama's comprehension of reaction chemistry, we focus on individual reactions and summarize the three critical aspects as: (1) the percentage of "Valid SMILES" out of all SMILES strings in the responses, which is essential for assessing SynLlama's learning outcome of string-based chemical representations in general, (2) the percentage of "Matched Reactants," which calculates whether the generated reactants match the reactant templates specified in the predicted reactions, and (3) the percentage of "Good Products," which assess if the predicted product can indeed be generated by applying the proposed reaction templates onto the reactants.Overall, these six benchmarks can collectively assess SynLlama's capability to follow instructions and perform chemical reactions in string representations.S1.The detailed descriptions of each benchmark can be found in the main text.Here, we run SynLlama inferences at T = 0.1 and T opP = 0.1 to generate reproducible benchmarking results (see Supplementary Table S2).</p>
<p>In Table 1, all four trained SynLlama models are evaluated on both in-distribution training data and out-of-distribution testing and ChEMBL 8 data to assess the benchmarks outlined above.In the instruction-following benchmarks, most models exhibit strong adherence (over 90 percent) to the fine-tuned response structure across all datasets.This impressive performance indicates that fine-tuning effectively retains the specified output structure when trained with over 100,000 samples.Furthermore, all four models successfully memorized the provided RXN templates and selected the building blocks (BBs) from all predicted reactants over 99 percent of the time.This capability further enhances the coupling effectiveness of the downstream reconstruction algorithm with the SynLlama raw output, as it only requires information about reaction sequences and predicted building blocks.</p>
<p>In the reaction chemistry benchmarking results, a clearer trend emerges: models, regardless of their parameter size, show improved comprehension of reaction chemistry in all three datasets as the amount of training data increases.Notably, most models maintain their performance from training to testing data, but exhibit a greater decline in "Matched Reactants" and "Good Products" performance when generalizing to the ChEMBL data.The reason behind this is that the testing data are generated in the same manner as the training data but with a different set of building blocks, while the ChEMBL data occupies a different chemical space, as previously noted by Gao et al. 54 .Despite the reductions in their performance for ChEMBL molecules, as shown in Supplementary Figure S2, SynLlama-8B-500k and SynLlama-1B-2M can still generate complete and valid syntheses over 50% of the time without any downstream processing.These results indicates that SynLlama's raw results alone have potential utility for synthesis planning for unseen drug-like molecules.</p>
<p>When comparing SynLlama-8B-500k and SynLlama-1B-500k, we observe that the larger model demonstrates better performance when trained on the same amount of data.Although additional training data could further improve the 8B model based on the current trend, its higher computational cost makes this pursuit less practical.However, as the fine-tuning computational costs for SynLlama-8B-500k and SynLlama-1B-2M require approximately the same A40-GPU hours, and given the comparable benchmark performance between them, we decided to move forward with SynLlama-1B-2M, simplified as SynLlama, for the subsequent tasks due to its faster inference speed.</p>
<p>Reconstruction from Predicted Retrosynthesis</p>
<p>Using the predicted sequence of RXNs and BBs from SynLlama responses, we can synthesize the proposed target molecule or close analogs by applying the predicted reaction templates to the BBs in the inferred order, as shown in black ink in Figure 1(d).In some cases, the predicted BBs match known Enamine BBs, ensuring that the resulting molecules remain within an established chemical space for synthesis.However, due to SynLlama's generative nature, some predicted BBs are novel while still providing valid synthesis pathways, and we only report new BBs that can be purchased from other suppliers identified by Molport 69 .Therefore, while SynLlama primarily produces molecules within the predefined chemical space using Enamine BBs, its output also offers an alternative strategy for molecule construction.We will revisit this point in the Results section.</p>
<p>When the input molecule cannot be fully reconstructed, we generate analogs by mapping the predicted BBs from SynLlama to known Enamine BBs, thereby sampling molecules from the well-defined Enamine chemical space.Under this scenario, we use nearest neighbor search algorithms with different molecular representations (SMILES and Morgan Fingerprints 70 ) to sample Enamine neighboring BBs from the predicted BBs, as illustrated in colored inks in Figure 1(d).Since in SynLlama's output, the RXN sequences are predicted concurrently with the BBs, our effective search space is constrained to Enamine BBs that can react through the specific RXN template.This smaller Enamine search space not only allows us to ensure the success rate of such forward syntheses but also allows us to effectively explore segments of the input molecule.Further details of the nearest neighbor search algorithms are provided in the Supplementary Information.</p>
<p>When constructing full synthetic pathways for reactions with multiple possible products, we select the product that most closely matches the predicted product based on SMILES string similarity.As shown in Figure 1(e), the reconstruction algorithm iteratively builds synthesis routes, utilizing all predicted BBs and RXN sequences to reconstruct or generate variations of the original molecule from the synthesizable chemical space.This reconstruction algorithm enables the SynLlama model to function as a generator for synthesizable molecules along with their corresponding synthetic pathways.</p>
<p>Results</p>
<p>We examine SynLlama's performance in synthesis planning of a diverse set of previously unseen compounds.We also explore the utility of the SynLlama workflow in real-world drug discovery applications, including its integration with generative algorithms to enhance the synthetic accessibility of proposed molecules while preserving their chemical properties and expanding the library of active compounds in the defined synthesizable space with as good or improved binding affinity metrics.</p>
<p>Synthesis Planning for Unseen Molecules</p>
<p>Having demonstrated that SynLlama models can reliably predict reaction sequences and building blocks in Table 1, we now use SynLlama to plan the synthesis of two groups of 1000 previously unseen molecules from the Enamine Diversity Set 9 and the publicly available ChEMBL database 8 .These datasets are specific to drug-like molecules, unlike the training data used for SynLlama; the drug-related property distribution of both sets of molecules against the training data is shown in Figure S3.In this validation, we test whether known synthesizable molecules can be reconstructed accurately from the baseline and SynLlama models as summarized in Table 2.</p>
<p>We first consider the standard reconstruction approach used by algorithms such as Syn-Net 54 , ChemProjector 55 , and Synformer 56 to create target molecules or analogs using BBs exclusively from the Enamine library.In this comparison, SynNet 54 and ChemProjector 55 serve as a baseline comparison for synthesizable chemical space coverage for RXN 1, whereas Synformer 56 is the baseline comparison on the expanded RXN 2 templates.As seen in the first column of Table 2 and Supplementary Table S3, when trained with their respective reaction sets and using only Enamine BBs, SynLlama outperforms all three methods while reducing the number of training data by 40-to 60-fold.</p>
<p>Although SynLlama yields higher percentages of successful ChEMBL reconstructions compared to SynNet and ChemProjector, and is on par with Synformer when only using Enamine BBs, there is a degradation of performance across all methods for ChEMBL compared to the Enamine Diversity set reconstructions.We attempted to improve upon the ChEMBL result by reformulating the training reaction data with extra filtering such that the product molecule distributions conform to a similar drug-like property distribution as the ChEMBL set as seen in Supplementary Figure S4.We then performed supervised finetuning following the same procedure as described in Section 2, but now with this filtered set of training data, in hope of better generating synthetic pathways for molecules with druglike properties.However, as shown in Supplementary Table S3, there is now a performance loss over both the Enamine Diversity and ChEMBL drug-like targets.This result suggests that training on more diverse product molecules ultimately benefits synthesis planning for the drug-like targets more than specializing the LLM further with more restrictive training data.In addition, the curated ChEMBL data appears unique and outside the synthesizable chemical space made up of Enamine BBs and RXN templates.S4 and S5, and identification of purchasable New BBs with Molport 69 are described in the Supplementary Information.</p>
<p>However, unlike baseline methods that only generate molecules using Enamine BBs, SynLlama has the extra capacity of reconstruct target molecules with commercially available BBs beyond Enamine due to its generative capabilities, even without specific training for this purpose.As seen in Table 2 the 'New BBs', restricted to those purchasable through Molport 69 , add possible synthetic pathways to reconstruct target molecules in all datasets and RXN templates.This also helps the ChEMBL data as well given that the molecules generated with the new BBs remain drug-like (Supplementary Figure S5).Since some target molecules can be synthesized through multiple pathways, either using only Enamine BBs or with the addition of New BBs, the 'Total' column in Table 2 reflects the number of unique target molecules reconstructed with SynLlama.With these New BBs, SynLlama's best reconstruction rates increase to 74.1% for the 1000 molecules in the Enamine Diversity Set, and encouragingly to 28.7% for the ChEMBL data.These results show that SynLlama learns reaction chemistry such that it can predict novel BBs to increase synthetic accessibility.</p>
<p>When the target molecule cannot be reconstructed, we assess the quality of the analog using a molecular similarity score between the target molecule and its most similar analog using Tanimoto similarity based on 4096-bit Morgan fingerprints 70 .In Table 2, the similarity metrics reported are average values of all generated molecules, including target molecules that are fully reconstructed (with a score of 1).Tables S6 and S7 also provide similarity metrics based on the 4096-bit Morgan fingerprints of Murcko scaffolds 71 , and Gobbi 2D pharmacophore fingerprints 72 , while including or excluding target molecules that are fully reconstructed.Overall, these results collectively show that SynLlama is highly capable of planning synthesis for related analog molecules with very good similarity, aided most by increased synthetic pathways using purchasable building blocks.</p>
<p>Finally, Supplementary Table S3 also considers reconstruction performances for the Enamine Diversity Set and ChEMBL Data based on forward synthesis as opposed to retrosynthesis, and for test molecules derived from tree-like synthesis pathways.It is evident that SynLlama performs better for retrosynthesis relative to forward synthesis that is used more successfully by Synformer, and retrosynthesis also performs better than the baseline methods for branching synthetic pathways.</p>
<p>Synthesizable Analog Search for De Novo Molecules</p>
<p>Previous research has shown that molecules proposed by generative models for binding to protein targets often face challenges in both reliability and practical synthesizability [28][29][30] .In particular, medicinal chemists are often reluctant to devote time and expensive resources to specialized synthesis of hit molecules due to the high false positive rates arising from the unreliability of docking scores in drug discovery.Instead finding closely related compounds that are constrained to Enamine BBs allows for an inexpensive purchase to verify hits before further refinements are deployed to gain lead drug molecules.In this section, we demonstrate SynLlama's potential to bridge the gap between generative molecule design and practical synthesis for molecules that are optimized for drug-like applications such as binding assays.</p>
<p>Specifically, we consider two different generative methods for de novo drug molecules in Figure 2: a 1D-to-3D LSTM model, iMiner 19 , which optimizes drug-likeness and AutoDockvina 73 docking scores, and Pocket2Mol 22 , a 3D graph transformer model which also optimizes AutoDock-vina 73 docking scores.Using iMiner we generated 500 molecules for the SARS-CoV-2 Main Protease (SARS2 MPro) 74 , and used Pocket2Mol to generate 500 molecules each for the protein targets Thrombin 75 and TYK2 76,77 .The first interesting observation is that only 1% of generated molecules from both iMiner and Pocket2Mol can be synthetically reconstructed using SynLlama or ChemProjector, emphasizing that the generative models create difficult synthesis targets.Hence all 500 compounds for each protein target were then processed through SynLlama trained on RXN 2 to generate synthesizable analogs constrained to Enamine BBs.For all generated analogs we performed molecular docking with AutoDockvina via using the same protocol as the original binders.</p>
<p>Figure 2(a,b) shows the RMSE of the docking scores between the target compounds and the generated analogs are 1.44 kcal/mol and 1.11 kcal/mol for Pocket2Mol and iMiner, respectively, both of which are within the acceptable range of inherent docking score errors reported by Trott et al 73 .Furthermore, as demonstrated in Figures 2(c,d), the SA score distribution of the SynLlama analogs generated for thrombin and TYK2 from Pocket2Mol show a notable improvement in synthetic accessibility at the expense of reduced similarity below the benchmarks in Table 2. SynLlama slightly improves SA for iMiner generated compounds for SARS2 MPro while maintaining a good similarity score on par with the benchmarks in Table 2.The fact that iMiner uses a drug-likeness score as part of its loss function 19 may explain its better performance, or perhaps SARS2 MPro is an easier protein target than Pocket2Mol's thrombin and TYK2 protein targets.The better synthesizable analogs have good retention of the binding mode of the original generated molecules, visually confirmed by the representative docking poses for target-analog molecular pairs shown in Figure 3(a) for the three proteins.Figure 3(b,c) provides a few examples of the synthesis pathways for the analogs of the molecules derived from the generative molecules.The final comparison consists of molecules that were identified as unsynthesizable by Gao et al 30 via ASKCOS 48 .As also seen in Figure 2, the similarity scores of SynLlama analogs for this set are better than that observed for the Pocket2Mol generative model, and the generated analogs show a significant decrease in SA score, representing SynLlamas effective strategy of improving synthetic accessibility via analog generation.In Supplementary Fig- ure S8, we also highlight the few concrete examples of target-analog pairs where objective scores are maintained while target synthetic accessibility scores are substantially reduced.Overall, these collective results highlight SynLlama's utility in effectively proposing synthesizable analogs for de novo molecules, thereby enhancing their synthetic accessibility without compromising their desired drug-related properties.</p>
<p>Local Hit Expansion for Binder Molecules</p>
<p>Because SynLlama breaks down the original target molecule for synthesis into building blocks, by nature this method allows diverse exploration around parts of the molecular scaffold rather than only on a whole target molecule.In a final task, we apply SynLlama to expand on hit molecules for three protein targets used in the previous section, SARS2 Mpro 78 , Thrombin 75 , and TYK2 76,77 to discover synthesizable molecules that have better relative binding free energies (RBFEs) confirmed by both experiments and accurate free energy perturbation (FEP) calculations.</p>
<p>As shown in Figure 4(a), the hit molecule for SARS2 Mpro (7LTJ Lead) has a core scaffold of uracil and ortho-dichlorobenzene connected by a piperizine linker.Inspired by an experimental hit expansion campaign by Kneller et al. 79 , we follow their practice to propose only functional group substitutions on the benzene ring while keeping the linker and uracil intact.In Figures 4(c) and 4(e), we use the best-performing molecules from the Schrodinger FEP benchmarking set 80 as our hits for Thrombin and TYK2.To expand on these hit molecules, we first identify the maximum common scaffolds among each group of molecules in the FEP benchmarking set and use the identified scaffolds to guide our selection of analog molecules.Specifically, we use SynLlama to generate 50 synthesizable analogs constrained to only Enamine BBs of the hit compound and filter for molecules that retain the scaffold.In the end, we harvest a total of 8, 14, and 11 analog molecules that fulfill the criteria for SARS2 Mpro, Thrombin, and TYK2, respectively.The analogs are then placed in a pose configuration similar to the original hit molecule for downstream FEP calculations.</p>
<p>To verify the FEP results, we first choose ∼ 10 synthesized and experimentally tested molecules to benchmark the accuracy of FEP for all three systems.In Supplementary Figure S9, all the calculated FEP values show a good correlation with the experimental ∆G converted from IC50, with an average RMSE of less than 1 kcal/mol.After this validation, we run FEP for the all proposed molecules to assess their binding affinities.As shown in Figures 4(b), 4(d), and 4(f), a significant portion of the proposed analogs (7 of 8, 5 of 14, and 6 of 11, respectively) showed potency compared to their parent hits for SARS2 Mpro, Thrombin, and TYK2.These results successfully demonstrate that SynLlama can propose diverse yet potent analogs when constrained on a molecular scaffold.</p>
<p>Moreover, the fact that all suggested analogs also come with predicted pathways using common reaction templates and purchasable BBs from Enamine suggests SynLlama's practical use for drug discovery.Because of this composite capacity of optimizing both potency and synthetic accessibility for small molecules, SynLlama successfully rediscovered Analog 1, the most potent compound reported by Kneller et al. in their hit expansion campaign for SARS2 Mpro 79 .Furthermore, for the Thrombin case, SynLlama explores the R2 substitution site, a region previously unaddressed by earlier molecular series, and demonstrates the generation of more potent molecules via FEP.These results confirm that SynLlama effectively explores local chemistry with readily available building blocks, providing a direct and efficient path for medicinal chemists to accelerate hit expansion.</p>
<p>Discussion and Conclusions</p>
<p>Motivated by recent advances in LLMs for chemistry 27,[58][59][60] , we aim to leverage data-efficient supervised fine-tuning (SFT) to transform the general-purpose Meta Llama 3 into SynLlama, an LLM-based generator capable of proposing synthesizable molecules and deducing synthetic routes for target molecules or their close analogs.Throughout the study, we successfully show that SynLlama can effectively explore a custom-defined chemical search space composed of around 230,000 Enamine building blocks (BBs) and well-validated organic reactions (RXNs), after it has been fine-tuned on synthetic pathway data sampled from this specified chemical space.What's more, despite utilizing nearly two orders of magnitude fewer synthetic pathways in training, SynLlama exhibits strong performance in key drug discovery tasks compared to existing models.Specifically, we have demonstrated that SynLlama can effectively aid in various stages of drug discovery that include synthesis planning, synthesizable analog generation for de novo molecules, and local hit expansion.</p>
<p>Because SynLlama is built on a general-purpose LLM instead of training from scratch [54][55][56] , it offers a number of unique advantages and possibilities for further improvement.For example, when generating our fine-tuning data, we sampled from the predefined chemical search space of 230K Enamine building blocks (BBs) and two sets of reaction templates (RXNs), but we did not embed these extensive requirements in the context window of the LLM.As a result, for our largest dataset with only 2 million synthetic pathways, the model only saw each BB a few dozen times, while each RXN template appeared hundreds of thousands of times.Consequently, while SynLlama efficiently memorizes the allowed RXNs, it only captures the distribution of Enamine BBs, which enables SynLlama to extrapolate to unseen yet purchasable building blocks outside of Enamine.This generative ability surpasses other existing methods such as ChemProjector and Synformer that can only explore a predefined building block search space, such as within the Enamine Diversity Set 9 , and limits their ability to propose alternative synthesis pathway with novel building blocks.</p>
<p>In addition to its ability to extrapolate outside the training chemical space, the underlying Llama-3.2-1Bused by SynLlama is relatively small and more predictive power would be expected if we train on larger LLMs with more data and compute power.However, we observe that a smaller LLM with fewer parameters can be turned into an expert model for complex tasks after SFT with relatively little data.This opens up opportunities to employ smaller expert models for various chemical tasks, benefiting from faster inference speeds, which can make these models more desirable.Moreover, optimal hyperparameters like temperature and top-p can vary between training and inference phases, depending on the downstream tasks.During inference, most valid raw outputs are generated under relatively low temperature and top-p settings.However, when the model is paired with reconstruction algorithms that require less strict adherence to reaction chemistry, higher temperature and top-p values can be used.This allows for a broader exploration of the Enamine chemical space, enabling the generation of more diverse and relevant analogs.This property is especially desirable in tasks that require extensive exploration, such as the hit expansion example we demonstrate.Another exciting direction is coupling SynLlama with another generative model, in which we have shown generates analogs while maintaining good docking scores and simultaneously shifting to better synthetic accessibility scores.This result suggests that SynLlama can serve effectively as a post-processor for other de novo generative models, ensuring the production of more synthesizable compounds with clear reaction pathways.</p>
<p>Among the numerous opportunities that LLMs bring to the field of drug discovery, their natural language capabilities and recent advancements in reasoning are the most exciting features that allow users without coding expertise to interact directly with the models, effectively bridging the gap between computational methods and experimental research.We envision that expert users can employ prompt engineering and fine-tuning data to incorporate more realistic factors than those explored here.For instance, medicinal chemists could fine-tune LLMs within this generalizable SFT framework with building blocks and reaction templates of their own choice.In addition, they can consider synthesis cost, reaction conditions, improved selectivity, and protection factors at specific reaction steps for more detailed and powerful synthesis planning.We see our work as an initial attempt to demonstrate the effectiveness of LLMs in real experimental research, encouraging further studies for better utilization of these models.</p>
<p>DATA AND CODE AVAILABILITY</p>
<p>All the codes and data for SynLlama workflow are provided in a public accessible GitHub repository: https://github.com/THGLab/SynLlamaunder MIT License.</p>
<p>AUTHOR CONTRIBUTIONS</p>
<p>K.S., D.B., J.C., and T.H.-G.conceived the scientific direction for SynLlama and wrote the manuscript.K.S. wrote the codes and trained the models.K.S., D.B., Y.W., and J.S. contributed to the result section.All authors provided comments on the results and manuscript.</p>
<p>ACKNOWLEDGEMENT</p>
<p>We thank Wenhao Gao for providing benchmarking data for SynNet and Synformer.We</p>
<p>SUPPLEMENTARY INFORMATION</p>
<p>Additional methodology and results are provided in the Supplementary Information, including details of SFT protocols; generation of training reaction data, hyperparameters used during inferences; baseline benchmarking procedures and results; procedures to check the purchasability and overall drug-related property distribution of novel building blocks; procedures to generate de novo molecules via iMiner and Pocket2Mol; details to calculate various rewards functions (docking scores, SA scores, and FEP energies); results of additional benchmarking on reconstruction, LLM reliability, analog similarities; and the effect of different hyperparameters on LLM inferences and downstream Enamine reconstructions.</p>
<p>TOC Graphic</p>
<p>Synopsis: Fine-tuning on synthetic reactions from commercial building blocks and high-fidelity reactions creates a versatile LLM, SynLlama, for key drug discovery tasks.</p>
<p>SMILES strings of all compatible BBs to form a representative token set for each individual RXN template.To facilitate efficient retrieval, we structure search trees based on the term frequency-inverse document frequency (TF-IDF) scores ? of these n-grams, prioritizing highly informative substructures and accelerating inference.Consequently, when a new SMILES string of the predicted BB is introduced, it can be efficiently processed through the tree, yielding a list of the top K matching SMILES strings.</p>
<p>In addition, Gao et al. ?investigated using Morgan fingerprints ?, a molecular representation capturing local chemical environment, to search for the nearest neighbors of BBs based on their Tanimoto similarity ? .Similarly to that stated above, for each RXN template, we also build a separate search tree for all compatible Enamine BBs using 256-bit Morgan fingerprint representation with a searching radius of 2. Our empirical observations indicate that combining the top K molecules from both the SMILES and Morgan fingerprint methods offers better performance than relying on the top 2K molecules from a single method.However, since we are working with an LLM model, the generated SMILES strings still have a small chance of being invalid, which prevents us from calculating their Morgan fingerprints.Therefore, we employ the both combined TF-IDF and Morgan fingerprint search trees when dealing with valid molecules, and revert to only a SMILES-based search when the generated SMILES strings are invalid.</p>
<p>LLM Inference Hyperparameters for Various Tasks.A key advantage of SynLlama, and LLMs in general, is their sensitivity to variations in hyperparameters, such as temperature (T ) and top-p (T opP ), which can significantly impact the performance of reconstruction and analog similarity.As shown in Supplementary Figure S2, SynLlama's raw outputs exhibit enhanced reaction chemistry comprehension when inferences are run at lower T opP and within a reasonable range of T for both test sets.This configuration allows SynLlama to explore purchasable building blocks outside the Enamine library while maintaining synthesis validity.Conversely, increasing T and T opP generally reduces SynLlama's ability to generate valid syntheses in its raw outputs.However, as Supplementary Figure S2 also illustrates, inferring with higher T and T opP values than the optimal settings in raw outputs often leads to better overall average maximum similarity scores for reconstruction with Enamine BBs along.Nonetheless, excessively high settings can increase the failure rate.</p>
<p>Based on empirical observations, we recommend specific combinations of T and T opP that effectively span a broad spectrum of tasks.These combinations optimize SynLlama's performance by balancing exploration and precision during inference.</p>
<p>• Frozen: T = 0.1, T opP = 0.1, repeated once.This setting prioritizes deterministic generation, ensuring minimal variability and high reproducibility.</p>
<p>• Low: T = 0.6, T opP = 0.5, repeated multiple times.This configuration allows for limited exploration while maintaining a degree of precision.</p>
<p>• Medium: T = 1.0, T opP = 0.7, repeated multiple times.This setting balances exploration and diversity, generating outputs with moderate randomness.</p>
<p>• High: T = 1.5, T opP = 0.9, repeated multiple times.This configuration promotes high diversity and creativity in generation but may introduce more variability in results.</p>
<p>Checking Commercial Availability of Building Blocks via Molport.In Results, we used the Molport platform to check whether a predicted BB is commercially available or not.Initially, we compiled a list of building blocks for searching and used the 'List Search' tab in the Molport website (https://www.molport.com/shop/swl-step-1)to check their availability.</p>
<p>Once the SMILES strings were entered into the search interface, we set the search criteria to a minimum acceptable quantity of 500 mg and match types restricted to 'Exact' and 'Perfect' to search in the database of 'screening compounds' and 'building blocks.'Once the search completed, we downloaded the excel file under the 'Selected Items' column from the List Search result tab (https://www.molport.com/shop/swl-requests),which contained both the commercially available compounds and information about the supplying vendors.</p>
<p>Calculation of SA Scores.We calculate SA scores for both the iMiner-proposed molecules and SynLlama-generated analogs using the oracle functions named 'SA' implemented in the TDC Commons package ? .</p>
<p>iMiner-Generated Molecules and Docking Procedures for Analogs.The iMiner algorithm ?, an 1D string-based LSTM model for SELFIES ?string generation, was employed in this study.The molecules generated by iMiner are optimized for 3D shape complementarity using a composite objective function comprising the AutoDock Vina ?docking score, as well as a custom-defined druglikeness score ? .For molecular docking tasks, we obtained the SARS-CoV-2 Mpro crystal structure (PDB ID: 7L11 ? ) from the Protein Data Bank ?and processed it with PDBFixer ? to add missing hydrogens and remove heteroatoms.The docking grid was centered at the geometric center of the ligand (XF1) from the corresponding PDB file ([x = -22, y = -4, z = -28]) using a cubic box with 20 Å sides.Both proteins and ligands were converted to PDBQT format using Meeko (https://github.com/forlilab/meeko). Docking was performed with AutoDock Vina using an exhaustiveness parameter of 64, and the best pose for each ligand was recorded.This protocol was consistently applied during both iMiner training and analog docking assessments.</p>
<p>The custom drug-likeness score is a composite score that evaluates 13 key molecular properties derived from the ChEMBL database.These properties capture both basic structural features and nuanced physicochemical characteristics, including the fraction of sp³-hybridized carbons, the total number of heavy atoms, and the fraction of non-carbon atoms within these heavy atoms.Additionally, the score accounts for the counts of hydrogen bond donors and acceptors, the number of rotatable bonds, and the balance between aliphatic and aromatic rings, along with molecular weight.Complementing these are parameters such as the approximate log partition coefficient (alogP), polarizable surface area (PSA), the number of structural alerts, and the size of the largest ring present in the molecule.Each property contributes to the overall score through a weight that is inversely proportional to the entropy of its distribution in the ChEMBL database: properties with narrower and more informative distributions exert a stronger influence.By summing the log likelihoods of these properties with their respective weights, the score effectively biases the generative model to produce molecules that closely mimic the drug-like profiles observed in established therapeutics, ensuring that the exploration of chemical space remains focused on compounds with favorable bio-availability and efficacy profiles.the equilibration steps 2-4, restraints (5 kJ • mol −1 • Å2 ) were applied to heavy atoms on the solute.Finally, a 5-ns production run was performed for each lambda state with the ACES enhanced sampling method ?and replica exchange was attempted every 0.5 ps.All the simulations employed 4 fs time step with the mass of solute hydrogens repartitioned to 3 amu ? .MBAR algorithm implemented in alchemlyb ?was used to estimate the free energy change between two states and yield ∆∆G.Then, the maximum likelihood estimation (MLE) method ?was used to calculate the absolute binding free energy (∆G) of each ligand and the ∆G was shifted to make the average of calculated ∆G of the ligands equal to the average of their experimental ∆G: All the system preparation and analysis were performed with an in-house package named easybfe that automates the whole workflow and manage the calculations with high-performance computing platforms, and it will be described in a future publication in details.Table S1: Benchmarks of SynLlama inferences using SynLlama models trained with two sets of reaction templates.Here, both models are fine-tuned on Llama-3.2-1Bmodel with 2M reaction data generate using the same set of training building blocks.We select 1000 molecules for each model: training and testing data are generated using their corresponding reaction templates; ChEMBL data is the same set of 1000 molecules as described in the main text.All SynLlama inferences are run at T = 0.1 and T opP = 0.1.</p>
<p>Supporting Tables
Dataset
Figure 1 :
1
Figure 1: Overview of the SynLlama workflow including data generation, supervised finetuning, inference, and reconstruction.(a).The predefined synthesizable chemical space of reaction templates (RXN) and building blocks (BBs) that covers billions of molecules.(b).An example synthesis data and its generation process from the defined synthesizable chemical space to create training examples.Here, RXN 76 represents amide coupling and RXN 72 represents Suzuki coupling.(c).A schematic representation of supervised fine-tuning that converts Llama 3 models to SynLlama models, along with the instruction, input, and output for the example synthesis in (b).(d).SynLlama's inference on an unseen test molecule.Black represents SynLlama's raw retrosynthetic output consisting of RXN sequences and predicted BBs, while colored BBs indicate the top two most similar BBs to the predicted ones from the Enamine building block library.Here, RXN 76 represents amide coupling and RXN 72 represents Suzuki coupling.(e).Reconstructed molecules using the predicted reaction sequences and similar building blocks from the Enamine building block library.In this example, all predicted building blocks are present in the Enamine library, allowing for the complete reconstruction of the input molecule and the generation of close analogs.</p>
<p>Figure 2 :
2
Figure 2: SynLlama performance on generating synthesizable analogs for Pocket2Mol and iMiner proposed binders of SARS2 MPro 74 , Thrombin 75 , and TYK2 76,77 .Correlation plot comparing docking scores of (a) Pocket2Mol and (b) iMiner generated molecules and the average Vina docking scores of ten most similar analogs from SynLlama trained with RXN 2. Each data point is color-coded by the average Morgan fingerprint similarity computed between the generated and analog molecules.The shaded area represents an energy uncertainty range of ±2kcal/mol for docking 73 .(c) Synthetic accessibility (SA) score distribution of Pocket2Mol, iMiner, and unsynthesizable molecules and SynLlama-proposed analogs.iMiner analogs generated with SynLlama trained on RXN 1 showed similar results as reported in Supplementary Figure S6.The kernel density in Supplementary Figure S7 further confirms our finding that the analogs consistently shift toward better SA without undermining the overall docking score distribution.(d) average Morgan fingerprint similarity score between the target molecules and their top-10 proposed analogs.</p>
<p>Figure 3 :
3
Figure 3: Examples of synthesizable analog generation for SARS2 MPro using iMiner and TYK2 and Thrombin with Pocket2Mol.(a) Docked pose visualization for all three protein targets.(b) Docking and SA scores for iMiner target and SynLlama analog for SARS2 MPro along with the predicted synthetic pathway.(c) Docking and SA scores for the Pocket2Mol targets and the SynLlama analogs for TYK2 and Thrombin along with their predicted synthetic pathways.</p>
<p>Figure 4 :
4
Figure 4: Hit expansion of binders to SARS2 Mpro, Thrombin, and Tyk2 with SynLlama.(a,c,e)Synllama-predicted synthetic pathways that expand on the hit molecules for each protein target.The places of substitution are labeled as R groups.(b,d,f) Binding free energies of the hit compounds and SynLlama-expanded analogs.Color scheme on the proposed substitution is the same as the predicted synthetic pathways.All potential binders either have a better FEP binding free energy or are within the 1 kcal/mol uncertainty range compared to the original hits.</p>
<p>also express our gratitude to Shitong Luo for open-sourcing ChemProjector, whose GitHub repository served as a foundation for the development of the SynLlama GitHub.This work was supported by National Institute of Allergy and Infectious Disease grant U19-AI171954.This research used computational resources of the National Energy Research Scientific Computing, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.</p>
<p>Figure S2 :
S2
Figure S2: Reconstruction algorithm and SynLlama raw output benchmarks for SynLlama inferences on the Testing and ChEMBL sets under various temperature and top-p combinations.The first row represents the success rate of the Enamine reconstruction algorithm based on SynLlama inference outputs.The second row represents the average maximum Tanimoto similarity between the target and analogs generated via the reconstruction algorithm based on 4096-bit Morgan fingerprints.The last row represents the percentage of SynLlama raw outputs that can directly represent a retrosynthetic path for the input molecule without downstream processing with the reconstruction algorithm.</p>
<p>Figure S3 :
S3
Figure S3: Drug-related property distributions between product molecules from normal training data (blue), Enamine Diversity Set(orange), and ChEMBL molecules (green).</p>
<p>Figure S4 :
S4
Figure S4: Drug-related property distributions between product molecules from normal training data (blue), product molecules from training data constrained on druglike properties (orange), and ChEMBL molecules (green).The generated product molecules under the constraint of druglike properties display similar distribution as ChEMBL molecules.The product molecules from normal training scheme occupies a very different chemical space with more larger molecules.</p>
<p>Figure S5 :
S5
Figure S5: Drug-related property distributions between training building blocks (blue), testing building blocks (orange), and Molport building blocks (green).The three building blocks show very similar distribution in all categories.</p>
<p>Figure S8 :Figure
S8
Figure S8: Oracle score and SA score distribution between 500 ASKCOS unsynthesizable molecules and proposed analogs from SynLlama model trained on RXN 2. (a) Correlation plot comparing property scores of 500 ASKCOS unsynthesizable molecules and the average docking scores of ten most similar analogs for each molecule.Each data point is color-coded by the average Morgan fingerprint similarity computed between the ASKCOS unsynthesizable molecules and their corresponding analogs.(b) SA score distribution of ASKCOS unsynthesizable molecules and SynLlama-proposed analogs.(c) Property and SA scores for example target-analog pairs along with the predicted analog synthetic pathways for two optimization targets: Ranolazine MPO and Deco Hop.</p>
<p>Table 1 :
1
Benchmarks of SynLlama inferences on 1000 training, testing, and ChEMBL data.We first select 1000 SMILES strings from the training examples, testing examples and the ChEMBL dataset, and then run inferences using SynLlama models trained on RXN 1. Benchmarking result for SynLlama models trained on RXN 2 can be found in Supplementary Table
Model Config</p>
<p>Table 2 :
2
Comparison of synthesis planning performance among different methods.Both the Enamine Diversity Set and ChEMBL Data are comprised of 1000 unseen molecules each.Details of each benchmark are described in the main text.The Morgan similarity scores include all analog molecules with successful synthesis pathways, as well as successfully reconstructed target molecules.The number of total training data and reaction set each method used for is: SynNet 54 (200K, RXN 1) ChemProjector 55 (128M, RXN 1), Synformer 56 (85M, RXN 2), and SynLlama (2M, RXN 1 &amp; 2).Further details are provided in Supplementary Tables</p>
<p>Supplementary Information SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language ModelsKunyang Sun 1 , Dorian Bagni 1,∆ , Joseph M. Cavanagh 1,∆ , Yingze Wang 1,∆ , Jacob M. Sawyer 4 , Bo Zhou 5 , Andrew Gritsevskiy 6 , Oufan Zhang 1 , Teresa Head-Gordon* 1−3Additional Methodology DetailsSupervised Fine Tuning protocol.After preparing the reaction data and promptresponse pairs from the training chemical space, we fine-tune Llama-3.1-8B(8 Billion parameters) and Llama-3.2-1B(1 Billion parameters) using the Axolotl package ??for 1 epoch. LMs with more parameters require more resources to train and use, but they also typically perform better on a variety of tasks, which we consider in Results.For our SFT approach, we apply Low-Rank Adaptation (LoRA) with a rank of r = 32 and α = 16 to the linear layers of the model.?We use FlashAttention-2 ?, with the Adam optimizer ?, cross-entropy loss, and a cosine learning rate scheduler with a maximum learning rate of 2 × 10 −5 .Molecule Generation using Enamine BBs.When searching for the nearest neighbors of BBs, a natural choice is to perform a string-level similarity search based on SMILES strings, as this is the native format of SynLlama responses.For each RXN template, we systematically process all SMILES strings of its compatible building blocks that can participate in the reaction.First, we extract the full vocabulary of SMILES tokens and generate an n-gram representation by considering all possible consecutive token pairs (bigrams) and triplets (trigrams).Next, we identify the 1024 most frequently occurring n-grams acrossWe define different sampling strategies based on these core settings:• Frugal Sampling: A total of 4 inferences.-T = 0.1, T opP = 0.1, repeated one time.-T = 0.6, T opP = 0.5, repeated one time.-T = 1.0, T opP = 0.7, repeated one time.-T = 1.5, T opP = 0.9, repeated one time.• Greedy Sampling: A total of 10 inferences.-T = 0.1, T opP = 0.1, repeated one time.-T = 0.6, T opP = 0.5, repeated two times.-T = 1.0, T opP = 0.7, repeated three times.-T = 1.5, T opP = 0.9, repeated four times.• Frozen Only: A total of 1 inference.-T = 0.1, T opP = 0.1, repeated one time.• Low Only: A total of 5 inferences.-T = 0.6, T opP = 0.5, repeated five times.• Medium Only: A total of 5 inferences.-T = 1.0, T opP = 0.7, repeated five times.• High Only: A total of 5 inferences.-T = 1.5, T opP = 0.9, repeated five times.Baseline Benchmarking Details.Since the Enamine BB catalog constantly updates new BBs and does not store historical data, we cannot access the exact training BBs used in training for the baseline methods ChemProjector ? and Synformer ? .The only training set data we had access to is described in Section 2.1 in the main document, and is ∼ 3% (10k) more compared to that available to Synformer and Chemprojector (cutoff at October 2023).However, we highlight that 97% of our training data is identical to their previous work and the newly added building blocks (which is equivalent to a time split) show a similar distribution as the rest of the training BBs.We now have this comparison in Supplementary FigureS5.Therefore, we can fairly say that our training data are very similar to the baseline methods to which we compare.For a fair comparison at the inference stage, we provide ChemProjector and Synformer the same set of building blocks (cutoff at Feb. 2025) that SynLlama had access to during inference time.In Tables2 and S3, we assess the performance of the trained baseline models with this more recent set of building blocks.Pocket2Mol Generation.De novo generation with Pocket2Mol was performed for Thrombin and TYK2 targets using codes from the Pocket2Mol github repository ? .Three default settings specified in configs/sample for pdb.yml were modified to generate at least 1000 molecules in one single run: num samples:1000, beam size:500, max steps:100.The protein structure files were downloaded from the Schrödinger FEP benchmark github repository ? .The pocket center was set to (-4.0, 26.5, -30.0) for TYK2 and (17.0, -12.5, 22.5) for Thrombin.Unsynthesizable Molecules Identificaiton and Reward Calculation.To access the list of the unsynthesizable molecules, we query the first 50 top-scoring molecules that were identified as unsynthesizable by ASKCOS ?for each property category listed in this csv (https://github.com/wenhao-gao/askcos_synthesizability/blob/master/results/goal_hard_cwo.csv).There are a total of 10 different individual rewards, including 7 multiproperty objectives (MPOs) centering around 7 different drug targets (Osimertinib, Fexofenadine, Ranolazine, Perindopril, Amlodipine, Sitagliptin, Zaleplon), Valsartan SMARTS, and 2 Hopping (Scaffold and deco).We used the TDC Commons package ? to score both the original molecules and the generated analogs for their corresponding property category.Free Energy Perturbation (FEP) Protocols.The relative binding free energies are calculated using GPU-accelerated AMBER22 ?(pmemd.cuda.MPI).AMBER14SB ? and OpenFF-2.1.0?were used to parametrize the protein and the ligand, respectively.The SARS-CoV-2 Mpro protein structure (PDB code: 7LTJ) was downloaded from RCSB PDB and prepared with PDBFixer ? to assign side-chain protonation states at pH=7.4 and add hydrogens.H163 was manually set to be its variant HIE (hydrogen added on Nϵ) to ensure the correct hydrogen bonding with the ligand.For TYK2 and Thrombin, their protein structures were downloaded from the github repository of Schrödinger benchmark dataset ? .A submodule app.Modeller in OpenMM ? was used to immerse the protein-ligand complexes and unbound ligands in a cubic water box with 15 Å buffer size and add ions (Na + , Cl − ) to neutralize the system and maintain 0.15M ionic strength.For perturbations involving charge changes, the alchemical water method ?was used to eliminate the artifacts in PME simulation of system with net-charges.We used 16 unevenly distributed lambdas (0.0, 0.174, 0.226, 0.265, 0.330, 0.383, 0.432, 0.477, 0.522, 0.568, 0.617, 0.670, 0.735, 0.774, 0.826, 1.0) to transform the initial state to the final state in the free energy.This lambda settings was designed to maximize the phase space overlap between adjacent states with the second-order smooth-step function introduced.The transformations were performed with the modified SSC(2) softcore potentials (m = n = 2, α LJ = 0.5, α Coul = 1) ? .Kartograf ?algorithm was used to determine the common core region (SC) and soft core region (SC) atoms.Each lambda state was subjected to the following simulation protocol to equilibrate the system: (1) energy minimization without any constriants; (2) heating from 0 to 100 K at constant volume and temperature (NVT) ensemble over 20 ps, followed by MD at constant pressure and temperature (NPT) ensemble at 100 K for 20 ps; (3) heating to 200 K at NVT ensemble over 20 ps followed by another 20 ps at NPT ensemble at 200 K; (4) heating to 298.15 K at NVT ensemble over 20 ps followed by another 20 ps at NPT ensemble at 298.15 K; (5) another pre-prodcution equilibrium run at NPT ensemble for 500 ps.During The released Synformer ?model weights were fine-tuned extensively on smaller drug-like compounds, which caused it to fall short on synthesis planning for more complex molecules.TableS3: Reconstruction performances across various reaction data sets and prompt design choices.SynLlama-druglike refers to fine-tuning on target molecule that falls within the same distribution as ChEMBL.More detailed analysis of the target molecule properties can be found in FigureS1.SynLlamaforward refers to fine-tune on prompt-response pairs structured as forward synthesis rather than retrosynthesis.SynLlama-branching refers to reconstructions based on tree-like synthesis with testing molecules generated similarly to the training data using testing building blocks and corresponding two sets of reaction templates.We compare to ChemProjector ?(RXN 1) and Synformer ?(RXN 2).1d.During data generation, all instructions remain the same, and the inputoutput pairs are generated within the training synthesizable chemical space.We enforce the JSON format in the output for our post processing algorithms.The output JSON has two parts: reactions and building blocks.In 'reactions', a series of reaction steps are generated, where the product of the next reaction serves as the reactant for the previous one.In 'building blocks', BBs are selected from the 'reaction' section and compiled into a list.Dataset
GENERAL METHODS FOR THE CONSTRUCTION OF COMPLEX MOLECULES. E Corey, The Chemistry of Natural Products. Elsevier1967</p>
<p>Computer-Assisted Planning of Organic Syntheses: The Second Generation of Programs. W Ihlenfeldt, J Gasteiger, 10.1002/anie.199526131Angewandte Chemie International Edition in English. 341996</p>
<p>Computer-Assisted Analysis in Organic Synthesis. E J Corey, A K Long, S D Rubenstein, 10.1126/science.3838594Science. 2281985</p>
<p>Parallel optimization of synthetic pathways within the network of organic chemistry. M Kowalik, 10.1002/anie.201202209Angewandte Chemie International Edition. 512012</p>
<p>The pdbbind database: Collection of binding affinities for protein-ligand complexes with known three-dimensional structures. R Wang, X Fang, Y Lu, S Wang, 10.1021/jm030580lJournal of Medicinal Chemistry. 472004</p>
<p>Bindingdb: a webaccessible database of experimentally determined protein-ligand binding affinities. T Liu, Y Lin, X Wen, R N Jorissen, M K Gilson, Nucleic acids research. 352007</p>
<p>ZINC: A free tool to discover chemistry for biology. J J Irwin, T Sterling, M M Mysinger, E S Bolstad, R G Coleman, 10.1021/ci3001277Journal of Chemical Information and Modeling. 522012</p>
<p>ChEMBL: a large-scale bioactivity database for drug discovery. A Gaulton, 10.1093/nar/gkr777Nucleic Acids Research. 402012</p>
<p>Building block catalogs. Enamine, </p>
<p>Large-scale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, Chemberta, CoRR abs/2010. 20209885</p>
<p>Mol-BERT: An effective molecular representation with BERT for molecular property prediction. J Li, X Jiang, 10.1155/2021/7181815Wireless Communications and Mobile Computing 2021. 20217181815</p>
<p>Molgpt: Molecular generation using a transformer-decoder model. V Bagal, R Aggarwal, P K Vinod, U Priyakumar, Journal of chemical information and modeling. 622022</p>
<p>LIMO: Latent inceptionism for targeted molecule generation. P Eckmann, Proceedings of the 39th International Conference on Machine Learning. K Chaudhuri, the 39th International Conference on Machine LearningPMLR2022162of Proceedings of Machine Learning Research</p>
<p>cMolGPT: A conditional generative pre-trained transformer for target-specific de novo molecular generation. Y Wang, H Zhao, S Sciabola, W Wang, Molecules. 2844302023</p>
<p>Language models can learn complex molecular distributions. D Flam-Shepherd, K Zhu, A Aspuru-Guzik, 10.1038/s41467-022-30839-xNature Communications. 1332932022</p>
<p>Chemical language models enable navigation in sparsely populated chemical space. M Skinnider, R Stacey, D Wishart, L Foster, Nature Machine Intelligence. 32021</p>
<p>Reinvent 2.0: An ai tool for de novo drug design. T Blaschke, Journal of chemical information and modeling null. 2020</p>
<p>3d equivariant diffusion for target-aware molecule generation and affinity prediction. J Guan, W Qian, X Peng, The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023</p>
<p>Mining for potent inhibitors through artificial intelligence and physics: A unified methodology for ligand based and structure based drug design. J Li, 10.1021/acs.jcim.4c00634Journal of Chemical Information and Modeling. 2024</p>
<p>Ls-molgen: Ligand-and-structure dual-driven deep reinforcement learning for target-specific molecular generation improves binding affinity and novelty. S Li, Journal of Chemical Information and Modeling. 2023</p>
<p>A 3d generative model for structure-based drug design. S Luo, J Guan, M Jianzhu, Advances in Neural Information Processing Systems. 202134</p>
<p>Pocket2mol: Efficient molecular sampling based on 3d protein pockets. X Peng, International Conference on Machine Learning. PMLR2022</p>
<p>De novo molecule design using molecular generative models constrained by ligand-protein interactions. J Zhang, H Chen, Journal of Chemical Information and Modeling. 622022</p>
<p>Alphadrug: protein target specific de novo molecular generation. H Qian, C Lin, D Zhao, PNAS Nexus. 12022</p>
<p>Structure-based drug design with equivariant diffusion models. A Schneuing, 2023</p>
<p>Resgen is a pocket-aware 3d molecular generation model based on parallel multiscale modelling. O Zhang, Nature Machine Intelligence. 52023</p>
<p>SmileyLlama: Modifying large language models for directed chemical space exploration. J M Cavanagh, 2024</p>
<p>Hunting for Organic Molecules with Artificial Intelligence: Molecules Optimized for Desired Excitation Energies. M Sumita, X Yang, S Ishihara, R Tamura, K Tsuda, 10.1021/acscentsci.8b00213ACS Central Science. 42018</p>
<p>Deep learning enables rapid identification of potent DDR1 kinase inhibitors. A Zhavoronkov, Nature Biotechnology. 372019</p>
<p>The Synthesizability of Molecules Proposed by Generative Models. W Gao, C W Coley, 10.1021/acs.jcim.0c00174Journal of Chemical Information and Modeling. 602020</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. P Ertl, A Schuffenhauer, 10.1186/1758-2946-1-8Journal of Cheminformatics. 12009</p>
<p>Synthetic Complexity Learned from a Reaction Corpus. C W Coley, L Rogers, W H Green, K F Jensen, Scscore, 10.1021/acs.jcim.7b00622Journal of Chemical Information and Modeling. 582018</p>
<p>SYBA: Bayesian estimation of synthetic accessibility of organic compounds. M Voršilák, M Kolář, I Čmelo, D Svozil, 10.1186/s13321-020-00439-2Journal of Cheminformatics. 12352020</p>
<p>Retrosynthetic accessibility score (RAscore) -rapid machine learned synthesizability classification from AI driven retrosynthetic planning. A Thakkar, V Chadimová, E J Bjerrum, O Engkvist, J.-L Reymond, Chemical Science. 122021</p>
<p>DeepSA: a deep-learning driven predictor of compound synthesis accessibility. S Wang, L Wang, F Li, F Bai, 10.1186/s13321-023-00771-3Journal of Cheminformatics. 151032023</p>
<p>Deep Learning-Based Scoring of Synthetic Complexity with Drug-Focused Retrosynthetic Analysis for High-Throughput Virtual Screening. H Kim, K Lee, C Kim, J Lim, W Y Kim, Dfrscore, 10.1021/acs.jcim.3c01134Journal of Chemical Information and Modeling. 642024</p>
<p>A Personalized Machine Learning-Based Synthetic Feasibility Score. R M Neeser, B Correia, P Schwaller, Fsscore, 10.1002/cmtd.202400024Chemistry-Methods. 4e2024000242024</p>
<p>Generative models for molecular discovery: Recent advances and challenges. C Bilodeau, W Jin, T Jaakkola, R Barzilay, K F Jensen, 10.1002/wcms.1608WIREs Computational Molecular Science. 12e16082022</p>
<p>Critical assessment of synthetic accessibility scores in computer-assisted synthesis planning. G Skoraczyński, M Kitlas, B Miasojedow, A Gambin, 10.1186/s13321-023-00678-zJournal of Cheminformatics. 152023</p>
<p>A Deep Generative Model for Fragment-Based Molecule Generation. M Podda, D Bacciu, A Micheli, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics. the Twenty Third International Conference on Artificial Intelligence and StatisticsPMLR2020</p>
<p>Hit and lead discovery with explorative RL and fragment-based molecule generation. S Yang, D Hwang, S Lee, S Ryu, S J Hwang, Advances in Neural Information Processing Systems. A Beygelzimer, Y Dauphin, P Liang, J W Vaughan, 2021</p>
<p>A deep generative model for molecule optimization via one fragment modification. Z Chen, M R Min, S Parthasarathy, X Ning, Nature Machine Intelligence. 32021</p>
<p>. S Seo, J Lim, W Y Kim, 10.1002/advs.202206674Molecular Generative Model via Retrosynthetically Prepared Chemical Building Block Assembly. Advanced Science. 1022066742023</p>
<p>AiZynthFinder: a fast, robust and flexible open-source software for retrosynthetic planning. S Genheden, 10.1186/s13321-020-00472-1Journal of Cheminformatics. 12702020</p>
<p>ASKCOS: Open-Source, Data-Driven Synthesis Planning. Z Tu, 10.1021/acs.accounts.5c00155Accounts of Chemical Research. 582025</p>
<p>Directly optimizing for synthesizability in generative molecular design using retrosynthesis models. J Guo, P Schwaller, Chemical Science. 162025</p>
<p>Automation and computer-assisted planning for chemical synthesis. Y Shen, 10.1038/s43586-021-00022-5Nature Reviews Methods Primers. 1232021</p>
<p>A robotic platform for flow synthesis of organic compounds informed by AI planning. C W Coley, 10.1126/science.aax1566Science. 365eaax1566</p>
<p>A deep learning approach for rational ligand generation with toxicity control via reactive building blocks. P Li, Nature Computational Science. 42024</p>
<p>Generative AI for designing and validating easily synthesizable and structurally novel antibiotics. K Swanson, Nature Machine Intelligence. 62024</p>
<p>Synflownet: Design of diverse and novel molecules with synthesis constraints. M Cretu, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Generative Flows on Synthetic Pathway for Drug Design. S Seo, 2024</p>
<p>ClickGen: Directed exploration of synthesizable chemical space via modular reactions and reinforcement learning. M Wang, Nature Communications. 15101272024</p>
<p>Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design. W Gao, R Mercado, C W Coley, ArXiv:2110.063892022</p>
<p>Projecting Molecules into Synthesizable Chemical Spaces. S Luo, ArXiv:2406.046282024</p>
<p>. W Gao, S Luo, C W Coley, ArXiv:2410.03494Generative Artificial Intelligence for Navigating Synthesizable Chemical Space. 2024</p>
<p>On the opportunities and risks of foundation models. R Bommasani, 2022</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>Augmenting large language models with chemistry tools. M Bran, A , Nature Machine Intelligence. 62024</p>
<p>Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. B Yu, F N Baker, Z Chen, X Ning, H Sun, Llasmol, 2024</p>
<p>A review of large language models and autonomous agents in chemistry. M C Ramos, C J Collison, A D White, Chemical Science. 2024</p>
<p>The llama 3 herd of models. A Dubey, 2024</p>
<p>Measuring massive multitask language understanding. D Hendrycks, </p>
<p>International Conference on Learning Representations. 2021</p>
<p>MMLU-pro: A more robust and challenging multi-task language understanding benchmark. Y Wang, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks T. 2024</p>
<p>DOGS: Reaction-Driven de novo Design of Bioactive Compounds. M Hartenfeller, 10.1371/journal.pcbi.1002380PLoS Computational Biology. 8e10023802012</p>
<p>Automated de novo molecular design by hybrid machine intelligence and rule-driven chemical synthesis. A Button, D Merk, J A Hiss, G Schneider, Nature Machine Intelligence. 12019</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, 10.1021/ci00057a005Journal of Chemical Information and Computer Sciences. 281988</p>
<p>Daylight Theory: SMARTS -A Language for Describing Molecular Patterns. D Weininger, </p>
<p>. Molport. List search. </p>
<p>The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. H L Morgan, Journal of Chemical Documentation. 51965</p>
<p>The properties of known drugs. 1. molecular frameworks. G W Bemis, M A Murcko, 10.1021/jm96029288709122Journal of Medicinal Chemistry. 391996</p>
<p>Genetic optimization of combinatorial libraries. A Gobbi, D Poppinger, 10.1002/%28SICI%291097-0290%28199824%2961%3A1%3C47%3A%3AAID-BIT9%3E3.0.CO%3B2-ZBiotechnology and Bioengineering. 611998</p>
<p>AutoDock Vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. O Trott, A J Olson, 10.1002/jcc.21334J. Comp. Chem. 312010</p>
<p>Potent noncovalent inhibitors of the main protease of sars-cov-2 from molecular sculpting of the drug perampanel guided by free energy perturbation calculations. C.-H Zhang, ACS Cent. Sci. 72021</p>
<p>More than a simple lipophilic contact: a detailed thermodynamic analysis of nonbasic residues in the s1 pocket of thrombin. B Baum, Journal of molecular biology. 3902009</p>
<p>Lead identification of novel and selective TYK2 inhibitors. J Liang, </p>
<p>. European Journal of Medicinal Chemistry. 672013</p>
<p>Lead Optimization of a 4-Aminopyridine Benzamide Scaffold To Identify Potent, Selective, and Orally Bioavailable TYK2 Inhibitors. J Liang, 10.1021/jm400266tJournal of Medicinal Chemistry. 562013American Chemical Society</p>
<p>High-throughput virtual screening and validation of a sars-cov-2 main protease noncovalent inhibitor. A Clyde, 10.1021/acs.jcim.1c0085134793155Journal of Chemical Information and Modeling. 622022</p>
<p>Structural, electronic, and electrostatic determinants for inhibitor binding to subsites s1 and s2 in sars-cov-2 main protease. D W Kneller, 10.1021/acs.jmedchem.1c0147534705466Journal of Medicinal Chemistry. 642021</p>
<p>. Schrodinger-Fep-Benchmark, June 24, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8458 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8458</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8458</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-357e182a38219625dd37cba526befe5f8429aa4b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/357e182a38219625dd37cba526befe5f8429aa4b" target="_blank">A Language Agent for Autonomous Driving</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems, thus enabling a more nuanced, human-like approach to autonomous driving.</p>
                <p><strong>Paper Abstract:</strong> Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our approach, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our approach on the large-scale nuScenes benchmark, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin. Our approach also demonstrates superior interpretability and few-shot learning ability to these methods.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8458",
    "paper_id": "paper-357e182a38219625dd37cba526befe5f8429aa4b",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00509775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Language Agent for Autonomous Driving</h1>
<p>Jiageng Mao ${ }^{1 <em>}$ Junjie Ye ${ }^{1 </em>}$ Yuxi Qian ${ }^{1}$ Marco Pavone ${ }^{2,3}$ Yue Wang ${ }^{1,3}$<br>${ }^{1}$ University of Southern California ${ }^{2}$ Stanford University ${ }^{3}$ NVIDIA<br>{jiagengm, yejunjie, yuxiqian, yue.w}@usc.edu, pavone@stanford.edu</p>
<h4>Abstract</h4>
<p>Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our system, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our system on both open-loop and close-loop driving challenges, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin (more than $30 \%$ on the nuScenes dataset). Our approach also demonstrates superior interpretability and few-shot learning ability to these methods. Please visit our webpage for more details.</p>
<h2>1 Introduction</h2>
<p>Imagine a car navigating a quiet suburban neighborhood. Suddenly, a ball bounces onto the road. A human driver, leveraging extensive experiential knowledge, would not only perceive the immediate presence of the ball, but also instinctively anticipate the possibility of a chasing child and consequently decide to decelerate. In contrast, an autonomous vehicle, devoid of such reasoning and experiential anticipation, might continue driving until sensors detect the child, only allowing for a narrower margin of safety. The importance of human prior knowledge in driving systems becomes clear: driving is not merely about reacting to the visible, but also to the conceivable scenarios where the system needs to reason and respond even in their absence.
To integrate human prior knowledge into autonomous driving systems, previous approaches (Sadat et al., 2020; Casas et al., 2021; Hu et al., 2022; 2023; Jiang et al., 2023)</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Architecture of Agent-Driver. Our system dynamically collects necessary environmental information from the output of neural modules via the tool library. The collected information is further utilized to query the cognitive memory. Consequently, the reasoning engine takes collected environmental information and retrieved memory data as input, and traceably derives a safe and comfortable trajectory for driving through chain-of-thought reasoning, task planning, motion planning, and self-reflection.
deconstruct the human driving process into three systematic steps following Figure 1 (a). Perception: they interpret the human perceptual process as object detection (Mao et al., 2023b) or occupancy estimation (Peng et al., 2020). Prediction: they abstract human drivers' foresight of upcoming scenarios as the prediction of future object motions (Casas et al., 2018). Planning: they emulate the human decision-making process by planning a collision-free trajectory, either using hand-crafted rules (Treiber et al., 2000) or by learning from data (Zeng et al., 2019). Despite its efficacy, this perception-prediction-planning framework overly simplifies the human decision-making process and cannot fully model the complexity of driving. For instance, perception modules in these methods are notably redundant, necessitating the detection of all objects in a vast perception range, whereas human drivers can maintain safety by only attending to a few key objects. Moreover, prediction and planning are designed for collision avoidance with detected objects. Nevertheless, they lack deeper reasoning ability inherent to humans, e.g. deducing the connection between a visible ball and a potentially unseen child. Furthermore, it remains challenging to incorporate long-term driving experiences and common sense into existing autonomous driving systems.
In addressing these challenges, we found the major obstacle of integrating human priors into autonomous driving lies in the incompatibility of human knowledge and neural-networkbased driving systems. Human knowledge is inherently encoded and utilized as language representations, and their reasoning process can also be interpreted by language. However, conventional driving systems rely on deep neural networks that are designed to process numerical data inputs, such as sensory signals, bounding boxes, and trajectories. The discrepancy between language and numerical representations poses a significant challenge to incorporating human experiential knowledge and reasoning capability into existing driving systems, thereby widening the chasm from genuine human driving performance.
Taking a step towards more human-like autonomous driving, we propose Agent-Driver, a cognitive agent empowered by Large Language Models (LLMs). The fundamental insight of our approach lies in the utilization of natural language as a unified interface, seamlessly bridging language-based human knowledge and reasoning ability with neural-networkbased systems. Our approach fundamentally transforms the conventional perception-prediction-planning framework by leveraging LLMs as an interactive scheduler among system components. As depicted in Figure 1 (b), on top of the LLMs, we introduce: 1) a versatile tool library that interfaces with neural modules via dynamic function calls,</p>
<p>streamlining perception with less redundancy, 2) a configurable cognitive memory that explicitly stores common sense and driving experiences, infusing the system with human experiential knowledge, and 3) a reasoning engine that processes perception results and memory data to emulate human-like decision-making. Specifically, the reasoning engine performs chain-of-thought reasoning to recognize key objects and events, task planning to derive a high-level driving plan, motion planning to generate a driving trajectory, and self-reflection to ensure the safety of the planned trajectory. These components, coordinated by LLMs, culminate in an anthropomorphic driving process. To conclude, we summarize our contributions as follows:</p>
<ul>
<li>We present Agent-Driver, an LLM-powered agent that revolutionizes the traditional perception-prediction-planning framework, establishing a powerful yet flexible paradigm for human-like autonomous driving.</li>
<li>Agent-Driver integrates a tool library for dynamic perception and prediction, a cognitive memory for human knowledge, and a reasoning engine that emulates human decision-making, all orchestrated by LLMs to enable a more anthropomorphic autonomous driving process.</li>
<li>Agent-Driver significantly outperforms the state-of-the-art autonomous driving systems by a large margin, with over $30 \%$ collision improvements in motion planning. Our approach also demonstrates strong few-shot learning ability and interpretability on the nuScenes benchmark.</li>
<li>We provide ablation studies to dissect the proposed architecture and understand the efficacy of each module, to facilitate future research in this direction.</li>
</ul>
<h1>2 Related Works</h1>
<p>Perception-Prediction-Planning in Driving Systems. Modern autonomous driving systems rely on a perception-prediction-planning paradigm to make driving decisions based on sensory inputs. Perception modules aim to recognize and localize objects in a driving scene, typically in a format of object detection (Mao et al., 2021b; Wang et al., 2022; Mao et al., 2021a; 2023b) or object occupancy prediction (Peng et al., 2020; Tong et al., 2023). Prediction modules aim to estimate the future motions of objects, normally represented as predicted trajectories (Casas et al., 2018; Ivanovic \&amp; Pavone, 2019; Shi et al., 2022) or occupancy flows (Agro et al., 2023; Casas et al., 2021). Planning modules aim to derive a safe and comfortable trajectory, using rules (Urmson et al., 2008; Fan et al., 2018; Sauer et al., 2018; Chen et al., 2015; Bacha et al., 2008; Leonard et al., 2008; Thrun et al., 2006; Treiber et al., 2000) or learning from human driving trajectories (Dauner et al., 2023; Chitta et al., 2022; Mao et al., 2023a). These three modules are generally performed sequentially, either trained separately or in an end-to-end manner (Liang et al., 2020; Sadat et al., 2020; Casas et al., 2021; Hu et al., 2022; 2023). This perception-prediction-planning framework overly simplifies the human driving process and cannot effectively incorporate human priors such as common sense and past driving experiences. By contrast, our Agent-Driver transforms the conventional perception-prediction-planning framework by introducing LLMs as an agent to bring human-like intelligence into the autonomous driving system.
LLMs in Autonomous Driving. Trained on Internet-scale data, LLMs (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a,b) have demonstrated remarkable capabilities in commonsense reasoning and natural language understanding. How to leverage the power of LLMs to tackle the problem of autonomous driving remains an open challenge. GPTDriver (Mao et al., 2023a) handled the planning problem in autonomous driving by reformulating motion planning as a language modeling problem and introducing fine-tuned LLMs as a motion planner. DriveGPT4 (Xu et al., 2023) proposed an end-to-end driving approach that leverages Vision-Language Models to directly map sensory inputs to actions. DiLu (Wen et al., 2023) introduced a knowledge-driven approach with Large Language Models. These methods mainly focus on an individual component in conventional driving systems, e.g. question-answering (Xu et al., 2023), planning (Mao et al., 2023a), or control (Sha et al., 2023). Some approaches (Fu et al., 2023; Wen et al., 2023) are implemented and evaluated in simple simulated driving environments. By contrast, Agent-Driver presents a systematic approach</p>
<p>that leverages LLMs as an agent to schedule the whole driving system, leading to a strong performance on the real-world driving benchmark.</p>
<h1>3 Agent-Driver</h1>
<p>In this section, we present Agent-Driver, an LLM-based intelligent agent for autonomous driving. We first introduce the overall architecture of our Agent-Driver in Section 3.1. Then, we introduce the three key components of our method: tool library (Section 3.2), cognitive memory (Section 3.3), and reasoning engine (Section 3.4).</p>
<h3>3.1 Overall Architecture</h3>
<p>Conventional perception-prediction-planning pipelines leverage a series of neural networks as basic modules for different tasks. However, these neural-network-based systems lack direct compatibility with human prior knowledge, constraining their potential for leveraging such priors to enhance driving performance. To handle this challenge, we propose a novel framework that leverages text representations as a unified interface to connect neural networks and human knowledge. The overall architecture of Agent-Driver is shown in Figure 2. Our approach takes sensory data as input and introduces neural modules for processing these sensory data and extracting environmental information about detection, prediction, occupancy, and map. On top of the neural modules, we propose a tool library where a set of functions are designed to further abstract the neural outputs and return text-based messages. For each driving scenario, an LLM selectively activates the required neural modules by invoking specific functions from the tool library, ensuring the collection of necessary environmental information with less redundancy. Upon gathering the necessary environmental information, the LLM leverages this data as a query to search in a cognitive memory for pertinent traffic regulations and the most similar past driving experience. Finally, the retrieved traffic rules and driving experience, together with the formerly collected environmental information, are utilized as inputs to an LLM-based reasoning engine. The reasoning engine performs multi-round reasoning based on the inputs and eventually devises a safe and comfortable trajectory for driving. Our Agent-Driver architecture harnesses dynamic perception and prediction capability brought by the tool library, human knowledge from the cognitive memory, and the strong decision-making ability of the reasoning engine. This synergistic integration results in a more human-like driving system with enhanced decision-making capability.</p>
<h3>3.2 Tool Library</h3>
<p>The profound challenge of incorporating human knowledge into neural-network-based driving systems is reconciling the incompatibility between text-based human priors and the numerical representations from neural networks. While prior works (Kuo et al., 2022) have attempted to translate text-based priors into semantic features or regularization terms for integration with neural modules, their performances are still constrained by the inherent cross-modal discrepancy. By contrast, we leverage text as a unified interface to connect neural modules and propose a tool library built upon the neural modules to dynamically collect text-based environmental information.</p>
<p>The cornerstones of the tool library are four neural modules, i.e., detection, prediction, occupancy, and map modules, which process sensory data from observations and generate detected bounding boxes, future trajectories, occupancy grids, and maps respectively. The neural modules cover various tasks in perception and prediction and extract environmental information from observations. However, this information is largely redundant, with a significant portion insignificant to decision-making. To dynamically extract necessary information from the neural module outputs, we propose a tool library-where a set of functions are designed-to summarize the neural outputs into text-based messages, and the information collection process can be established by dynamic function calls. An illustration of this process is shown in Figure 3.</p>
<p>Functions. We devised various functions for detection, prediction, occupancy, and mapping, in order to extract useful information from the neural module's outputs respectively. Our</p>
<p>tool library contains more than 20 functions covering diverse usages. Here are some examples. For detection, get_leading_object returns a text description of the object in front of the ego-vehicle on the same lane. For prediction, get_pred_trajs_for_object returns a textbased predicted future trajectory for a specified object. For occupancy, get_occ_at_loc_time returns the probability that a specific location is occupied by other objects at a given timestep. For map, get_lanes returns the information of the left and right lanes to the ego-vehicle, and get_shoulders returns the information of the left and right road shoulders to the ego-vehicle. Detailed descriptions of all functions are in Appendix A.1.</p>
<p>Tool Use. With the functions in the tool library, an LLM is instructed to collect necessary environmental information through dynamic function calls. Specifically, the LLM is first provided with initial information such as the current state for its subsequent decision-making. Then, the LLM will be asked whether it is necessary to activate a specific neural module, i.e.detection, prediction, occupancy, and map. If the LLM decides to activate a neural module, the functions related to this module will be provided to the LLM, and the LLM chooses to call one or some of these functions to collect the desired information. Through multiple rounds of conversations, the LLM eventually collects all necessary information about the current environment. Compared to directly utilizing the outputs of the neural modules, our approach reduces the redundancy in current systems by leveraging the reasoning power of the LLM to determine what environmental information is of real importance to the decision-making process. Furthermore, the neural modules are only activated when the LLM decides to call the relevant functions, which brings flexibility to the system.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Illustration of function calls in the tool library. Agent-Driver can effectively collect necessary environmental information from neural modules through dynamic function calls.</p>
<p>A detailed example of LLM leveraging tool functions for environment perceiving is shown in Appendix A.2.</p>
<h1>3.3 Cognitive Memory</h1>
<p>Human drivers navigate using their common sense, such as adherence to local traffic regulations, and draw upon driving experiences in similar situations. However, it is nontrivial to adapt this ability to the conventional perception-prediction-planning framework. By contrast, our approach tackles this problem through interactions with a cognitive memory. Specifically, the cognitive memory stores text-based common sense and driving experiences. For every scenario, we utilize the collected environmental information as a query to search in the cognitive memory for similar past experiences to assist decision-making. The cognitive memory contains two sub-memories: commonsense memory and experience memory.</p>
<p>Commonsense Memory. The commonsense memory encapsulates the essential knowledge a driver typically needs for driving safely on the road, such as traffic regulations and knowledge about risky behaviors. It is worth noting that the commonsense memory is purely text-based and fully configurable, that is, users can customize their own commonsense memory for different driving conditions by simply writing different types of knowledge into the memory.</p>
<p>Experience Memory. The experience memory contains a series of past driving scenarios, where each scenario is composed of the environmental information and the subsequent driving decision at that time. By retrieving the most similar experiences and referencing their driving decisions, our system enhances its capacity for making more informed and resilient driving decisions.</p>
<p>Memory Search. As exhibited in Figure 4, we present an innovative two-stage search algorithm to effectively search for the most similar past driving scenario in the experience memory. The first stage of our algorithm is inspired by vector databases (Lewis et al., 2020; Wang et al., 2021), where we encode the input query and each record in the memory into embeddings and then retrieve the top-K similar records via K nearest neighbors (K-NN) search in the embedding space. Since the driving scenarios are quite diverse, the embedding-based search is inherently limited by the encoding methods employed, resulting in insufficient generalization capabilities. To overcome this challenge, the second stage incorporates an LLM-based fuzzy search, where the LLM is tasked to rank these records according to their relevance to the query. This ranking is based on the implicit similarity assessment by the LLM, leveraging its capabilities in generalization and reasoning.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Illustration of memory search. The proposed two-stage search algorithm effectively retrieves the most similar driving experience and facilitates the subsequent decisionmaking process.
The cognitive memory equips our system with human knowledge and past driving experiences. The retrieved most similar experiences, together with commonsense and environmental information, collectively form the inputs to the reasoning engine. Text as a unified interface aligns the environmental information with human knowledge, thereby enhancing our system's compatibility. Please refer to Appendix B for more details.</p>
<h1>3.4 Reasoning Engine</h1>
<p>Reasoning, a fundamental ability of humans, is critical to the decision-making process. Conventional methods directly plan a driving trajectory based on perception and prediction results, while they lack the reasoning ability inherent to human drivers, resulting in insufficient capability to handle complicated driving scenarios. Conversely, as shown in Figure 5, we propose a reasoning engine that effectively incorporates reasoning ability into the driving decision-making process. Given the environmental information and retrieved memory, our reasoning engine performs multi-round reasoning to plan a safe and comfortable driving trajectory. The proposed reasoning engine consists of four core components: chain-of-thought reasoning, task planning, motion planning, and self-reflection.
Chain-of-Thought Reasoning. Human drivers are able to identify the key objects and their potential effects on driving decisions, while this important capability is typically absent in conventional autonomous driving approaches. To embrace this reasoning ability in our system, we propose a novel chain-of-thought reasoning module, where we instruct an LLM to reason on the input environmental information and output a list of key objects and their potential effects in text. To guide this reasoning process, we instruct the LLM via in-context learning of a few human-annotated examples. We found this strategy successfully aligns the reasoning power of the LLM with the context of autonomous driving, leading to improved reasoning accuracy.
Task Planning. High-level driving plans provide essential guidance to low-level motion planning. Nevertheless, traditional methods directly perform motion planning without relying on this high-level guidance, leading to sub-optimal planning results. In our approach, we define high-level driving plans as a combination of discrete driving behaviors and velocity estimations. For instance, the combination of a driving behavior</p>
<p>change_lane_to_left and a velocity estimation deceleration results in a high-level driving plan change_lane_to_left_with_deceleration. We instruct an LLM via in-context learning to devise a high-level driving plan based on environmental information, memory data, and chain-of-thought reasoning results. The devised high-level driving plan characterizes the ego-vehicle's coarse locomotion and serves as a strong prior to guide the subsequent motion planning process.
Motion Planning. Motion planning aims to devise a safe and comfortable trajectory for driving, and each trajectory is represented as a sequence of waypoints. Following (Mao et al., 2023a), we re-formulate motion planning as a language modeling problem. Specifically, we leverage environmental information, memory data, reasoning results, and high-level driving plans collectively as inputs to an LLM, and we instruct the LLM to generate text-based driving trajectories by reasoning on the inputs. By fine-tuning with human driving trajectories, the LLM can generate trajectories that closely emulate human driving patterns. Finally, we transform the text-based trajectories back into real trajectories for system execution.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Illustration of reasoning engine. Agent-Driver makes driving decisions like human in a step-by-step procedure.</p>
<p>Self-Reflection. Self-reflection is a crucial ability in humans' decision-making process, aiming to re-assess the former decisions and adjust them accordingly. To model this capability in our system, we propose a collision check and optimization approach. Specifically, for a planned trajectory $\hat{\tau}$ from the motion planning module, the collision_check function in the tool library is first invoked to check its collision. If collision detected, we refine the trajectory $\hat{\tau}$ into a new trajectory $\tau^{*}$ by optimizing the cost function $\mathcal{C}$ :</p>
<p>$$
\tau^{*}=\min <em _tau="\tau">{\tau} \mathcal{C}(\tau, \hat{\tau})=\min </em>|} \lambda_{1}|\tau-\hat{\tau<em 2="2">{2}+\lambda</em>(\tau)
$$} \mathcal{F}_{\text {col }</p>
<p>In this manner, the safety of the planned trajectory is greatly improved.
Our reasoning engine models the human decision-making process in driving as a step-bystep procedure involving reasoning, hierarchical planning, and self-reflection. Compared to prior works, our approach effectively emulates the human decision-making process, leading to enhanced decision-making capability and superior planning performance. More details of the reasoning engine can be found in Appendix C.</p>
<h1>4 Experiments</h1>
<p>In this section, we demonstrate the effectiveness, few-shot learning ability, and other characteristics of Agent-Driver through extensive experiments. First, we introduce the experimental settings in Section 4.1. Next, we evaluate the planning performance of our approach on both open-loop and closed-loop settings (Section 4.2). Subsequently, we investigate the few-shot learning ability (Section 4.3), interpretability (Section 4.4), compatibility (Section 4.5), and stability (Section 4.6) of Agent-Driver. Finally, we discuss the choices of in-context learning and fine-tuning in Section 4.7. More experiments can be found in Appendix D.</p>
<h3>4.1 Experimental Setup</h3>
<p>Benchmarks. For open-loop autonomous driving, we conduct experiments on the largescale nuScenes dataset (Caesar et al., 2020). The nuScenes dataset is a real-world autonomous driving dataset that contains 1,000 driving scenarios and approximately 34,000 key frames encompassing a diverse range of locations and weather conditions. We follow the general practice and split the whole dataset into training and validation sets. We utilize the training set to train the neural modules and instruct the LLMs, and we utilize the validation set to</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">L2 (m) $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Collision (\%) $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1 s</td>
<td style="text-align: center;">2 s</td>
<td style="text-align: center;">3 s</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">1s</td>
<td style="text-align: center;">2 s</td>
<td style="text-align: center;">3 s</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">ST-P3 metrics</td>
<td style="text-align: center;">ST-P3 (Hu et al., 2022)</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAD (Jiang et al., 2023)</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-Driver (Mao et al., 2023a)</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Agent-Driver (ours)</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr>
<td style="text-align: center;">UniAD metrics</td>
<td style="text-align: center;">NMP (Zeng et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SA-NMP (Zeng et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FF (Hu et al., 2021)</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EO (Khurana et al., 2022)</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">1.60</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UniAD (Hu et al., 2023)</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">1.65</td>
<td style="text-align: center;">1.03</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-Driver (Mao et al., 2023a)</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Agent-Driver (ours)</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.21</td>
</tr>
</tbody>
</table>
<p>Table 2: Open-loop planning performance compared to the state-of-the-arts. Agent-Driver significantly outperforms prior works in terms of L2 and collision rate. Our approach attains more than $30 \%$ performance gains in collisions compared to the state-of-the-art methods.
evaluate the performance of our approach, ensuring a fair comparison with prior works. Following prior works (Hu et al., 2022; 2023; Jiang et al., 2023), the L2 error and collision rate are reported to evaluate the planning performance. For the closed-loop autonomous driving, we adopt the Town05-Short benchmark (Prakash et al., 2021) powered by the CARLA simulator (Dosovitskiy et al., 2017) for evaluation. The Town05-Short benchmark consists of 10 challenging driving routes with 3 intersections each, including a high density of dynamic agents. We adopt the widely-used route completion and driving scores to evaluate the planning performance. Route Completion denotes the progress of driving on a route, and the Driving Score additionally takes comfort and safety into calculation. More details of the open-loop and closed-loop evaluation metrics can be found in Appendix D.1.
Implementation Details. We utilize gpt-3.5-turbo-0613 as the foundation LLM for different components in our system. For motion planning, we follow (Mao et al., 2023a) and fine-tune the LLM with human driving trajectories in the nuScenes training set for one epoch. For neural modules, we adopted the modules in (Hu et al., 2023). For closed-loop experiments, we leveraged the perception modules in LAV (Chen \&amp; Krähenbühl, 2022) and kept the other parts of our system the same. We also followed the same training setting and evaluation protocols in (Chen \&amp; Krähenbühl, 2022) for a fair comparison. More implementation details can be found in the appendix D.2.</p>
<h1>4.2 Comparison with State-of-the-art Methods</h1>
<p>Open-Loop Results. As shown in Table 2, Agent-Driver surpasses state-of-the-art methods in both metrics and decreases the collision rate of the second-best performance by a large margin. Specifically, under ST-P3 metrics, Agent-Driver realizes the lowest average L2 error and greatly reduces the average collision rates by $\mathbf{3 5 . 7 \%}$ compared to the secondbest performance. Under UniAD metrics, Agent-Driver achieves an L2 error of 0.74 and a collision rate of $0.21 \%$, which are $\mathbf{1 1 . 9 \%}$ and $\mathbf{3 2 . 3 \%}$ better than the second-best methods GPT-Driver (Mao et al., 2023a) and UniAD (Hu et al., 2023), respectively. The promising performance on the collision rate verifies the effectiveness of the reasoning ability of AgentDriver, which considerably increases the safety of the proposed autonomous driving system.</p>
<p>Closed-Loop Results. To analyze the performance of our approach in closed-loop settings, we evaluate Agent-Driver against other state-of-the-art methods on the authoritative CARLA simulator. The results on the Town05-Short benchmark (Chitta et al., 2022) are shown in Table 1. AgentDriver achieves the highest route completion, surpassing the second-best VAD (Jiang et al., 2023) by $4.1 \%$. In terms of driving score, Agent-Driver also yields a performance of $57.33 \%$, which is on par with the prior arts.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Interpretability of Agent-Driver. In the referenced images, planned trajectories of our system and human driving trajectories are in red and green respectively. Agent-Driver extracts meaningful objects (in yellow) from all detected objects (in blue) via the tool library. The reasoning engine further identifies notable objects (in red). Messages from the tool library, cognitive memory, and reasoning engine are recorded in colored text boxes. Every message is documented and our system is conducted in an interpretable and traceable way.</p>
<h1>4.3 Few-shot Learning</h1>
<p>To assess the generalization ability of motion planning in our approach, we conduct a few-shot learning experiment, where we keep other components the same and finetuned the core motion planning LLM with $0.1 \%, 1 \%, 10 \%, 50 \%$, and $100 \%$ of the training data for one epoch. For comparison, we adopted the motion planner in UniAD (Hu et al., 2023) trained with $100 \%$ data as the baseline. The results are shown in Figure 6. Notably, with only $0.1 \%$ of the full training data, i.e., 23 training samples, Agent-Driver realizes a promising performance. When exposed to $1 \%$ of training scenarios, the proposed method surpasses the baseline by a large margin, especially under the average collision rate. Furthermore, with increased training data, Agent-Driver stably achieves better motion planning performance.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Few-shot learning. The motion planner in Agent-Driver fine-tuned with $1 \%$ data exceeds the state-of-the-art (Hu et al., 2023) trained on full data, verifying its few-shot learning ability.</p>
<h3>4.4 Interpretability</h3>
<p>Unlike conventional driving systems that rely on black-box neural networks to perform different tasks, the proposed Agent-Driver inherits favorable interpretability from LLMs. As shown in Figure 7, the output messages of LLMs from the tool library, cognitive memory, and reasoning engine are recorded during system execution. Hence the whole driving decision-making process is transparent and interpretable.</p>
<h1>4.5 Compatibility with Different LLMs</h1>
<p>We tried leveraging the Llama-2-7B (Touvron et al., 2023b), gpt-3.5-turbo-1106, and gpt-3.5-turbo-0613 models as the foundation LLMs in our system. Table 3 demonstrates that Agent-Driver powered by different LLMs can yield satisfactory performances, verifying the compatibility of our system with diverse LLM architectures.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">L2 (m) $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Collision (\%) $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1 s</td>
<td style="text-align: center;">2 s</td>
<td style="text-align: center;">3 s</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">1 s</td>
<td style="text-align: center;">2 s</td>
<td style="text-align: center;">3 s</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-7B</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">1.47</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo-1106</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">1.47</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo-0613</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.21</td>
</tr>
</tbody>
</table>
<p>Table 3: Compatibility to different LLMs. Agent-Driver realizes satisfactory motion planning performance utilizing different types of LLMs as agents.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Percentage of training samples</th>
<th style="text-align: right;">$0.10 \%$</th>
<th style="text-align: right;">$1 \%$</th>
<th style="text-align: right;">$10 \%$</th>
<th style="text-align: right;">$50 \%$</th>
<th style="text-align: right;">$100 \%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of invalid outputs</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p>Table 4: Stability of Agent-Driver exposed to different amounts of training samples. With only $1 \%$ training samples ( $\sim 230$ samples), Agent-Driver produces zero invalid output.
Table 5: In-context learning vs. fine-tuning. In-context learning performs slightly better in reasoning and task planning. Fine-tuning is indispensable for motion planning.</p>
<h2>4.7 In-Context Learning vs. Fine-Tuning</h2>
<p>Two prevalent strategies to instruct an LLM for novel tasks are in-context learning and fine-tuning. To determine which is the most effective strategy, we apply these two strategies to the LLMs of the chain-of-thought reasoning, task planning, and motion planning modules respectively, benchmarking them on the downstream motion planning performance. As indicated in Table 5, incontext learning performs slightly better than fine-tuning in collision rates for reasoning and task planning, suggesting that in-context learning is a favorable choice in these modules. In motion planning, the fine-tuning strategy significantly outperforms in-context learning, demonstrating the necessity of fine-tuning LLMs in motion planning.</p>
<h2>5 Conclusion</h2>
<p>This work introduces Agent-Driver, a novel human-like paradigm that fundamentally transforms autonomous driving pipelines. Our key insight is to leverage LLMs as an agent to schedule different modules in autonomous driving. On top of the LLMs, we propose a tool library, a cognitive memory, and a reasoning engine to bring human-like intelligence into driving systems. Extensive experiments on the real-world driving dataset substantiate the effectiveness, few-shot learning ability, and interpretability of Agent-Driver. These findings shed light on the potential of LLMs as an agent in human-level intelligent driving systems. For future works, we plan to optimize the LLMs for real-time inference.</p>
<h1>Ethics Statement</h1>
<p>This paper presents Agent-Driver, a novel method that leverages large language models as an agent for autonomous driving. While Agent-Driver uses language generation as part of its reasoning and planning process, the goal is to improve the quality of representations and decision-making for autonomous vehicles, not text generation itself. Therefore, the potential negative impact of Agent-Driver on areas like misinformation or deepfakes is minimal. On the privacy protection side, Agent-Driver uses driving data that does not include any personal or location information. In summary, we believe Agent-Driver does not lead to any ethics concerns.</p>
<h2>Reproduciblity Statement</h2>
<p>We provide as many implementation details as possible in the paper submission and in the appendix. We also include the code for Agent-Driver, including the large language model, neural modules, tool library, cognitive memory, and reasoning engine components in the supplementary material. This will allow others to faithfully reproduce our results and build upon our proposed approach.</p>
<h2>Acknowledgments</h2>
<p>We'd like to acknowledge Xinshuo Weng for fruitful discussions. We also acknowledge a gift from Google Research.</p>
<h2>References</h2>
<p>Ben Agro, Quinlan Sykora, Sergio Casas, and Raquel Urtasun. Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. In CVPR, 2023.</p>
<p>Andrew Bacha, Cheryl Bauman, Ruel Faruque, Michael Fleming, Chris Terwelp, Charles Reinholtz, Dennis Hong, Al Wicks, Thomas Alberi, David Anderson, et al. Odin: Team VictorTango's Entry in the DARPA Urban Challenge. JFR, 25(8), 2008.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language Models are Few-Shot Learners. In NeurIPS, volume 33, 2020.</p>
<p>Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A Multimodal Dataset for Autonomous Driving. In CVPR, 2020.</p>
<p>Sergio Casas, Wenjie Luo, and Raquel Urtasun. IntentNet: Learning to Predict Intention from Raw Sensor Data. In CoRL, 2018.</p>
<p>Sergio Casas, Abbas Sadat, and Raquel Urtasun. MP3: A Unified Model to Map, Perceive, Predict and Plan. In CVPR, 2021.</p>
<p>Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving. In ICCV, 2015.</p>
<p>Dian Chen and Philipp Krähenbühl. Learning from all vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17222-17231, 2022.</p>
<p>Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger. TransFuser: Imitation With Transformer-Based Sensor Fusion for Autonomous Driving. IEEE TPAMI, 2022.</p>
<p>Felipe Codevilla, Eder Santana, Antonio M López, and Adrien Gaidon. Exploring the limitations of behavior cloning for autonomous driving. In ICCV, 2019.</p>
<p>Alexander Cui, Sergio Casas, Abbas Sadat, Renjie Liao, and Raquel Urtasun. LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving. In ICCV, 2021.</p>
<p>Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS, 35, 2022.</p>
<p>Daniel Dauner, Marcel Hallgarten, Andreas Geiger, and Kashyap Chitta. Parting with Misconceptions about Learning-based Vehicle Motion Planning. In CoRL, 2023.</p>
<p>Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An Open Urban Driving Simulator. In CoRL, volume 78, 2017.</p>
<p>Haoyang Fan, Fan Zhu, Changchun Liu, Liangliang Zhang, Li Zhuang, Dong Li, Weicheng Zhu, Jiangtao Hu, Hongye Li, and Qi Kong. Baidu Apollo EM Motion Planner. arXiv preprint arXiv:1807.08048, 2018.</p>
<p>Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive Like a Human: Rethinking Autonomous Driving with Large Language Models. arXiv preprint arXiv:2307.07162, 2023.</p>
<p>Peiyun Hu, Aaron Huang, John Dolan, David Held, and Deva Ramanan. Safe Local Motion Planning with Self-Supervised Freespace Forecasting. In CVPR, 2021.</p>
<p>Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. ST-P3: End-to-End Vision-Based Autonomous Driving via Spatial-Temporal Feature Learning. In ECCV, 2022.</p>
<p>Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In CVPR, 2023.</p>
<p>Boris Ivanovic and Marco Pavone. The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs. In ICCV, 2019.</p>
<p>Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. VAD: Vectorized Scene Representation for Efficient Autonomous Driving. In ICCV, 2023.</p>
<p>Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, and Jingjing Liu. Adapt: Action-aware driving caption transformer. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 7554-7561. IEEE, 2023.</p>
<p>Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, and Deva Ramanan. Differentiable Raycasting for Self-supervised Occupancy Forecasting. In ECCV, 2022.</p>
<p>Yen-Ling Kuo, Xin Huang, Andrei Barbu, Stephen G. McGill, Boris Katz, John J. Leonard, and Guy Rosman. Trajectory Prediction with Linguistic Representations. In ICRA, 2022.</p>
<p>John Leonard, Jonathan How, Seth Teller, Mitch Berger, Stefan Campbell, Gaston Fiore, Luke Fletcher, Emilio Frazzoli, Albert Huang, Sertac Karaman, et al. A Perception-Driven Autonomous Urban Vehicle. JFR, 25(10), 2008.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. RetrievalAugmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS, 33, 2020.</p>
<p>Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose M Alvarez. Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving? arXiv preprint arXiv:2312.03031, 2023.</p>
<p>Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, and Raquel Urtasun. PnPNet: End-to-End Perception and Prediction With Tracking in the Loop. In CVPR, 2020 .</p>
<p>Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. In ICML, 2023.</p>
<p>Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, and Chunjing Xu. Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection. In ICCV, 2021a.</p>
<p>Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. Voxel Transformer for 3D Object Detection. In ICCV, 2021b.</p>
<p>Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang. GPT-Driver: Learning to Drive with GPT. arXiv preprint arXiv:2310.01415, 2023a.</p>
<p>Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. 3D Object Detection for Autonomous Driving: A Comprehensive Survey. IJCV, 2023b.</p>
<p>OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.
Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional Occupancy Networks. In ECCV, 2020.</p>
<p>Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-Modal Fusion Transformer for End-to-End Autonomous Driving. In CVPR, 2021.</p>
<p>Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun. Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable Semantic Representations. In ECCV, 2020.</p>
<p>Axel Sauer, Nikolay Savinov, and Andreas Geiger. Conditional Affordance Learning for Driving in Urban Environments. In CoRL, 2018.</p>
<p>Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, and Mingyu Ding. LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. arXiv preprint arXiv:2310.03026, 2023.</p>
<p>Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Motion Transformer with Global Intention Localization and Local Movement Refinement. NeurIPS, 35, 2022.</p>
<p>Sebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel, Philip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, et al. Stanley: The Robot that Won the DARPA Grand Challenge. JFR, 23(9), 2006.</p>
<p>Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, et al. Scene as Occupancy. In ICCV, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested Traffic States in Empirical Observations and Microscopic Simulations. Physical Review E, 62(2), 2000.</p>
<p>Chris Urmson, Joshua Anhalt, Drew Bagnell, Christopher Baker, Robert Bittner, MN Clark, John Dolan, Dave Duggins, Tugrul Galatali, Chris Geyer, et al. Autonomous Driving in Urban Environments: Boss and the Urban Challenge. JFR, 25(8), 2008.</p>
<p>Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, et al. Milvus: A Purpose-Built Vector Data Management System. In SIGMOD, 2021.</p>
<p>Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and Justin Solomon. DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries. In CoRL, 2022.</p>
<p>Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, and Yu Qiao. DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models. arXiv preprint arXiv:2309.16292, 2023.</p>
<p>Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient Streaming Language Models with Attention Sinks. arXiv preprint arXiv:2309.17453, 2023.</p>
<p>Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kenneth KY Wong, Zhenguo Li, and Hengshuang Zhao. DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model. arXiv preprint arXiv:2310.01412, 2023.</p>
<p>Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-To-End Interpretable Neural Motion Planner. In CVPR, 2019.</p>
<p>Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye, and Jingdong Wang. Rethinking the Open-Loop Evaluation of End-to-End Autonomous Driving in nuScenes. arXiv preprint arXiv:2305.10430, 2023.</p>
<p>Appendix
A Tool Library ..... 16
A. 1 Functions ..... 16
A. 2 Tool Use ..... 16
B Cognitive Memory ..... 16
B. 1 Memory Data ..... 16
B. 2 Memory Search ..... 16
C Reasoning Engine ..... 17
C. 1 Chain-of-Thought Reasoning ..... 17
C. 2 Task Planning ..... 17
C. 3 Motion Planning ..... 18
C. 4 Self-Reflection ..... 18
C. 5 Comparison with GPT-Driver ..... 19
D Experiments ..... 19
D. 1 Evaluation Metrics ..... 19
D.1.1 Open-Loop Metrics ..... 19
D.1.2 Closed-Loop Metrics ..... 20
D. 2 Implementation Details ..... 20
D. 3 Ablation Study ..... 21
D. 4 Impact of Ego-States on Open-Loop Motion Planning ..... 21
D. 5 Compatibility with Different Neural Modules ..... 21
D. 6 Language Justification on the BDD-X Dataset ..... 22
E Qualitative Analysis ..... 23
E. 1 Qualitative Ablation ..... 23
E. 2 Qualitative Results ..... 24
E. 3 Failure Cases ..... 24
F Limitations ..... 24</p>
<h1>A Tool Library</h1>
<p>In this section, we will first introduce the detailed descriptions of all functions in the tool library (Section A.1). Next, we will provide a detailed example of how the agent interacts with the tool library (Section A.2).</p>
<h2>A. 1 Functions</h2>
<p>We include all function definitions in the tool library in Tables 6 and 7. The proposed functions cover detection, prediction, occupancy, and mapping, and enable flexible and versatile environmental information collection.</p>
<h2>A. 2 Tool Use</h2>
<p>A detailed example of how the LLM leverages tools to collect environmental information is shown in Figures 4 and 5. System prompts are shown in blue, the response of LLMs is shown in green, and the collected data is shown in orange. System prompts provide sufficient context and guidance for instructing the LLM to dynamically invoke the functions in the tool library to collect necessary environmental information.</p>
<h2>B Cognitive Memory</h2>
<p>In this section, we detail the data format and retrieving process of the commonsense and experience memory.</p>
<h2>B. 1 Memory Data</h2>
<p>As shown in Figure 8 of the main text, the commonsense memory consists of essential knowledge for safe driving, which is cached in a text-based format and is fully configurable.
We build the experience memory by caching the environmental information of driving scenarios and corresponding driving trajectories in the training set. Please note that this experience memory can be editable online, meaning that expert demonstrations conducted by human drivers can be easily inserted into the memory and benefit the subsequent decision-making of Agent-Driver.</p>
<h2>B. 2 Memory Search</h2>
<p>We propose a two-stage searching strategy for retrieving the most similar past driving scenario to the query scenario.</p>
<p>In the first stage, we generate a vectorized key $k_{i} \in \mathbb{R}^{1 \times\left(n_{e}+n_{g}+n_{h}\right)}$ for each past scenario $i$ by vectorizing its ego-states $e_{i} \in \mathbb{R}^{1 \times n_{e}}$, mission goals $g_{i} \in \mathbb{R}^{1 \times n_{g}}$, and historical trajectories $h_{i} \in \mathbb{R}^{1 \times n_{h}}$. The $N$ past scenarios in the experience memory collectively construct a key tensor $K \in \mathbb{R}^{N \times\left(n_{e}+n_{g}+n_{h}\right)}$ :</p>
<p>$$
K=\left{\left[e_{i}, g_{i}, h_{i}\right] \mid i={1,2, \ldots, N}\right}
$$</p>
<p>Similarly, we can vectorize the query scenario into $Q=[e, g, h] \in \mathbb{R}^{1 \times\left(n_{e}+n_{g}+n_{h}\right)}$.
Subsequently, we compute the similarity scores $S \in \mathbb{R}^{N}$ between the querying scenario $Q$ and the past scenarios $K$ :</p>
<p>$$
S=Q \Lambda K^{\top}
$$</p>
<p>where $\Lambda=\operatorname{diag}\left(\lambda_{e}, \lambda_{g}, \lambda_{h}\right) \in \mathbb{R}^{\left(n_{e}+n_{g}+n_{h}\right) \times\left(n_{e}+n_{g}+n_{h}\right)}$ indicates the weights of different components.</p>
<p>Finally, top-K samples with the K highest similarity scores are selected as candidates for the second-stage search.</p>
<p>In the second stage, we propose an LLM-based fuzzy search. The top-K past driving scenarios selected in the first stage are provided to the LLM. Then, the LLM is tasked to understand the text descriptions of these scenarios and determine the most similar past driving scenario to the query scenario. The driving trajectory corresponding to the selected scenario is also retrieved for reference.</p>
<p>With the proposed vector search and LLM-based fuzzy search, Agent-Driver can effectively retrieve the most similar past driving experience. The past experience and driving decision could help the current decision-making process.</p>
<h1>C Reasoning Engine</h1>
<p>In this section, we provide detailed information on the workflow of the reasoning engine. The reasoning engine takes environmental information and memory data as inputs, performs chain-of-thought reasoning, task planning, motion planning, and self-reflection, and eventually generates a driving trajectory for execution. Figures 9, 10, and 11 show an example of how the reasoning engine works. We denote the input environmental information as $\mathcal{O}$ and the retrieved memory data as $\mathcal{M}$. Please note that $\mathcal{O}$ and $\mathcal{M}$ are in the text format.</p>
<h2>C. 1 Chain-of-Thought Reasoning</h2>
<p>Chain-of-thought reasoning aims to emulate the human reasoning process and generate text-based reasoning results $\mathcal{R}$, which can be formulated as:</p>
<p>$$
\mathcal{R}=F_{\mathrm{LLM}}(\mathcal{O}, \mathcal{M})
$$</p>
<p>where $F_{\mathrm{LLM}}$ is a LLM. To avoid arbitrary reasoning outputs of LLMs which might lead to hallucination and results not relevant to planning, we constrain $\mathcal{R}$ to contain two essential parts: notable objects and potential effects. Specifically, we first instruct the LLM to identify those notable objects that have critical impacts on decision-making from the input environmental information. Then, we instruct the LLM to assess how these notable objects will influence the subsequent decision-making process. The instruction can be established by two strategies: in-context learning and fine-tuning.
For in-context learning, each time we leverage four human-annotated examples $\mathcal{E}$ of notable objects and potential effects in addition to $\mathcal{O}$ and $\mathcal{M}$ collectively as inputs to the LLM:</p>
<p>$$
\mathcal{R}=F_{\mathrm{LLM}}(\mathcal{O}, \mathcal{M}, \mathcal{E})
$$</p>
<p>For fine-tuning, we auto-generate the reasoning targets $\hat{\mathcal{R}}$ leveraging the technique proposed in (Mao et al., 2023a). Then we fine-tune the LLM to make its reasoning outputs $\mathcal{R}$ approaching the targets $\hat{\mathcal{R}}$.
Both in-context learning and fine-tuning effectively reduce invalid outputs. As shown in Table 6 of the main paper, compared to the fine-tuning strategy, in-context learning enables the LLMs to generate more diverse reasoning outputs, and results in better motion planning performance.</p>
<h2>C. 2 Task Planning</h2>
<p>Task planning aims to generate high-level driving plans $\mathcal{P}$ for autonomous vehicles, taking the reasoning results $\mathcal{R}$ as well as the environmental information $\mathcal{O}$ and memory data $\mathcal{M}$ as inputs. The process can be formulated as</p>
<p>$$
\mathcal{P}=F_{\mathrm{LLM}}(\mathcal{O}, \mathcal{M}, \mathcal{R})
$$</p>
<p>We define the driving plan as a combination of discrete driving behaviors and speed estimations. In this paper, we proposed 6 discrete driving behaviors: move_forward, change_lane_to_left, change_lane_to_right, turn_left, turn_right, and stop. We also propose 6 speed estimations: constant_speed, deceleration, quick_deceleration,</p>
<p>deceleration_to_zero, acceleration, quick_acceleration. The combinations of driving behaviors and speed estimations result in 31 different driving plans (stop has no speed estimation). These driving plans can cover most driving scenarios and they are fully configurable, which means that we can add more behavior and speed types to cover those long-tailed scenarios. See Table 1 for details.</p>
<p>Similar to the reasoning module, incontext learning and fine-tuning can also be applied to instruct the LLM to generate driving plans. As shown in Table 6 of the main paper, in-context learning is more appropriate for instructing the LLM for task planning.</p>
<h3>C. 3 Motion Planning</h3>
<p>Motion planning aims to plan a safe and comfortable driving trajectory $\tau$, with the driving plan $\mathcal{P}$, reasoning results $\mathcal{R}$, environmental information $\mathcal{O}$, and memory data $\mathcal{M}$ as inputs. The process can be formulated as</p>
<p>$$
\tau=F_{\mathrm{LLM}}(\mathcal{O}, \mathcal{M}, \mathcal{R}, \mathcal{P})
$$</p>
<p>The planned trajectory $\tau$ can be represented as 6 waypoint coordinates in 3 seconds: $\tau=$ $\left[\left(x_{1}, y_{1}\right), \cdots,\left(x_{6}, y_{6}\right)\right]$. A recent finding (Mao et al., 2023a) suggests a fine-tuned LLMs can generate text-based coordinates quite accurately. That is, the LLM can generate a text string " $(1.23,0.32)$ " representing a coordinate, and this can be easily transformed back into its numerical format $(1.23,0.32)$ for subsequent execution. In particular, given a trajectory $\tau$, we first transform it into a sequence of language tokens $w$ using a tokenizer $T$ :</p>
<p>$$
\tau=T\left(\left{\left(x_{1}, y_{1}\right), \cdots,\left(x_{6}, y_{6}\right)\right}\right)=\left{w_{1}, \cdots, w_{n}\right}
$$</p>
<p>With these language tokens, we then reformulate motion planning as a language modeling problem:</p>
<p>$$
\mathcal{L}<em i="1">{\mathrm{LM}}=-\sum</em>}^{N} \log P\left(\bar{w<em 1="1">{i} \mid w</em>\right)
$$}, \cdots, w_{i-1</p>
<p>where $w$ and $\bar{w}$ are the language tokens of the planned trajectory $\tau$ from the LLM and the human driving trajectory $\hat{\tau}$ respectively. By learning to maximize the occurrence probability $P$ of the tokens $\bar{w}$ derived from the human driving trajectory $\hat{\tau}$, the LLM can generate human-like driving trajectories. We suggest readers refer to (Mao et al., 2023a) for more details.</p>
<p>With proper fine-tuning, the LLM is able to generate a text-based trajectory that can be further transformed into its numerical format. This step maps natural-language-based perception, memory, and reasoning into executable driving trajectories, enabling our agent to perform low-level actions.</p>
<h1>C. 4 Self-Reflection</h1>
<p>Self-reflection is designed to reassess and rectify the driving trajectory $\tau$ planned by the LLM. Specifically, the collision_check function is first invoked to check the collision of $\tau$ utilizing the estimated occupancy map. Specifically, we place the ego-vehicle at each waypoint in a trajectory, and then we will mark the trajectory as collision if there is an obstacle within a safe margin $\eta$ to the ego-vehicle in the occupancy map. If a trajectory is not marked with collision, we directly use this trajectory as output without further rectification. Otherwise, for those trajectories that have collisions, we leverage an optimization approach to rectify the trajectory $\tau$ into a collision-free one $\tau^{*}$. Following (Hu et al., 2023), we sample the obstacle points $O_{t}$ near each waypoint at timestep $t$ in the occupancy map. Then, an</p>
<p>optimization problem is formulated and solved through the Newton iteration method:</p>
<p>$$
\begin{aligned}
\tau^{<em>}= &amp; \arg \min _{\tau}\left(\left|\tau-\tau^{</em>}\right|<em t="t">{2}+\right. \
&amp; \left.\sum</em>\right)\right)
\end{aligned}
$$} \sum_{(x, y) \in O_{t}} \frac{\lambda}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{\left|\tau_{t}^{*}-(x, y)\right|_{2}^{2}}{2 \sigma^{2}</p>
<p>where $\lambda$ and $\sigma$ are hyperparameters. The first term regulates the optimized trajectory $\tau^{<em>}$ to be similar to the original $\tau$, and the second term pushes the waypoint $\tau_{t}^{</em>}$ in the trajectory away from the obstacle points $O_{t}$ for each timestep $t$.
Attributing to self-reflection, those unreliable decisions made by the LLM can further be corrected, and collisions in the planned trajectories can be effectively mitigated.</p>
<h1>C. 5 Comparison with GPT-Driver</h1>
<p>The most relevant implementation is GPT-Driver (Mao et al., 2023a), which handles the planning problem in autonomous driving by reformulating motion planning as a language modeling problem and introducing fine-tuned LLMs as a motion planner. Compared to (Mao et al., 2023a), our Agent-Driver introduce additional modules such as tool libraries and cognitive memories that GPT-Driver does not use. Also, it decouples the chain-of-thought reasoning, task planning, and motion planning as separate steps to the LLM. Compared to generating both reasoning and planning in a single pass as in GPT-Driver, we empirically found that such separation eases the learning and prediction process, and yields a better result. In addition, we use in-context learning to instruct the LLM to perform reasoning and task planning, which encourages better diversity and generalization ability. While in GPT-Driver, it uses fine-tuning. Finally, we additionally introduce a self-reflection stage to further rectify the motion planning results, which leads to fewer collisions and better safety.</p>
<h2>D Experiments</h2>
<h2>D. 1 Evaluation Metrics</h2>
<h2>D.1.1 Open-Loop Metrics</h2>
<p>As argued in (Hu et al., 2023), autonomous driving systems should be optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Hence, we focus on the motion planning performances to evaluate the effectiveness of our system. There are two commonly adopted metrics for motion planning on the nuScenes dataset: L2 error (in meters) and collision rate (in percentage). The average L2 error is computed by measuring each waypoint's distance in the planned and ground-truth trajectories, reflecting the proximity of a planned trajectory to a human driving trajectory. The collision rate is calculated by placing an ego-vehicle box on each waypoint of the planned trajectory and then checking for collisions with the ground truth bounding boxes of other objects, reflecting the safety of a planned trajectory. We follow the common practice and evaluate the motion planning result in a 3 -second time horizon.
We further note that in different papers there are subtle discrepancies in computing these two metrics. For instance, in UniAD (Hu et al., 2023) both metrics at $k$-th second are measured as the error or collision rate at this certain timestep, while in ST-P3 (Hu et al., 2022) and following works (Jiang et al., 2023; Mao et al., 2023a), these metrics at $k$-th second is an average over $k$ seconds. There are also differences in ground truth objects for collision calculation in different papers. We detail the two different metric implementations as follows.</p>
<p>The output trajectory $\tau$ is formatted as 6 waypoints in a 3 -second horizon, i.e., $\tau=$ $\left[\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right),\left(x_{3}, y_{3}\right),\left(x_{4}, y_{4}\right),\left(x_{5}, y_{5}\right),\left(x_{6}, y_{6}\right)\right]$. For $\tau$ in each driving scenario, the L2 error is computed as:</p>
<p>$$
l_{2}=\sqrt{(\tau-\hat{\tau})^{2}}=\left[\sqrt{\left(x_{i}-\hat{x}<em i="i">{i}\right)^{2}+\left(y</em>}-\hat{y<em i="1">{i}\right)^{2}}\right]</em>
$$}^{6</p>
<p>where $l_{2} \in \mathbb{R}^{6 \times 1}$ and $\hat{\tau}$ denotes human driving trajectory. Then, the average L2 error $\hat{l}<em 2="2">{2} \in \mathbb{R}^{6 \times 1}$ can be computed by averaging $l</em>$ for each sample in the test set.
In the UniAD metric (Hu et al., 2023), the L2 error at the $k$-th second $(k=1,2,3)$ is reported as the error at this timestep:</p>
<p>$$
L_{2, k}^{\text {uniad }}=\hat{l}_{2}[2 k]
$$</p>
<p>The average L2 error is then computed by averaging $L_{2, k}^{\text {uniad }}$ of the 3 timesteps.
In the ST-P3 metric (Hu et al., 2022) and following works (Jiang et al., 2023; Mao et al., 2023a), the L2 error at the $k$-th second is reported as the average error from 0 to $k$ second:</p>
<p>$$
L_{2, k}^{\operatorname{stp} 3}=\frac{\sum_{t=1}^{2 k} \hat{l}_{2}[t]}{2 k}
$$</p>
<p>The average L2 error is computed by averaging the $L_{2, k}^{\text {stp } 3}$ of the 3 timesteps again (average over average in other words).
In terms of the collision, it is computed by counting the number of times a planned trajectory collides with other objects in the ground truth occupancy map for all scenarios in the test set. We denote $\mathcal{C} \in \mathbb{N}^{6 \times 1}$ as the total collision times at each timestep.
Similarly, UniAD reports the collision $\mathcal{C}<em k="k">{k}^{\text {uniad }}$ at the $k$-th second $(k=1,2,3)$ as $\mathcal{C}[2 k]$, while ST-P3 reports $\mathcal{C}</em>$ as the average from 0 to $k$ second:}^{\text {stp } 3</p>
<p>$$
\mathcal{C}<em t="1">{k}^{\text {stp } 3}=\frac{\sum</em>
$$}^{2 k} \mathcal{C}[t]}{2 k</p>
<p>In addition to the differences in calculation methods, there is also a difference in how the ground truth occupancy maps are generated in the two metrics. Specifically, UniAD only considers the vehicle category when generating ground truth occupancy, while STP3 considers both the vehicle and pedestrian categories. This difference leads to varying collision rates for identical planned trajectories when measured by these two metrics, yet it does not impact the L2 error.
In this paper, we faithfully evaluated our approach and the baseline methods using the officially implemented evaluation metrics in the two papers (Hu et al., 2023; 2022), ensuring a completely fair comparison with other methods.</p>
<h1>D.1.2 Closed-Loop Metrics</h1>
<p>Within the CARLA simulator (Dosovitskiy et al., 2017), the metrics of route completion and driving score are generally used to evaluate the planning performance of autonomous driving systems.
Route completion is a straightforward metric, measuring the percentage of a predefined route that the autonomous vehicle successfully completes before the simulation ends. This metric is crucial for understanding the basic capability of an autonomous vehicle to navigate from point A to point B.
The driving score is calculated by adjusting the route completion rate with a penalty factor that considers various infractions, including collisions with pedestrians, other vehicles, and stationary objects, deviations from the planned route, lane violations, ignoring red traffic lights, and failing to stop at stop signs.</p>
<h2>D. 2 Implementation Details</h2>
<p>We employ gpt-3.5-turbo-0613 as the foundation LLM in our Agent-Driver. We leverage the same LLM for every task except motion planning, and we fine-tuned another LLM specially for motion planning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Tool Library</th>
<th style="text-align: center;">Common. <br> Memory</th>
<th style="text-align: center;">Exp. <br> Memory</th>
<th style="text-align: center;">CoT <br> Reason.</th>
<th style="text-align: center;">Task <br> Plan.</th>
<th style="text-align: center;">Self- <br> Reflect.</th>
<th style="text-align: center;">L2 (m)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
<th style="text-align: center;">Collision (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1s</td>
<td style="text-align: center;">2s</td>
<td style="text-align: center;">3s</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1s</td>
<td style="text-align: center;">2s</td>
<td style="text-align: center;">3s</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">1.42</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">1.46</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">1.47</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">1.42</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.23</td>
</tr>
</tbody>
</table>
<p>Table 2: Ablation of components in Agent-Driver. The removal of any component can influence the planning efficacy of our system, indicating the importance of all components in our system.</p>
<p>For tool use and memory search, the LLM is guided by system prompts without fine-tuning or exemplar-based in-context learning. In chain-of-thought reasoning and task planning, the LLM is instructed by two randomly selected exemplars derived from the training set. This approach encourages the models to develop various insightful chain-of-thought processes and detailed plans for tasks independently. Please note that we utilize the original pretrained LLM with different system prompts and user input for the above tasks. In motion planning, we fine-tune an LLM with human driving trajectories for only one epoch.</p>
<h1>D. 3 Ablation Study</h1>
<p>Table 2 shows the results of ablating different components in Agent-Driver. All variants utilize $10 \%$ training data for instructing the LLMs. From ID 1 to ID 5, we ablate the main components in Agent-Driver, respectively. We deactivate the self-reflection module and directly evaluate the trajectories output from LLMs to better assess the contribution of each other module. When the tool library is disabled, all perception results form the input to Agent-Driver without selection, which yields $\sim 2$ times more input tokens and harms the system's efficiency. The removal of the tool library also increases the collision rate, indicating the effectiveness of this component. In addition, the collision rate gets worse by removing the commonsense and experience memory, reasoning, and task planning modules, demonstrating the necessity of these components. Besides, we further note that self-reflection also greatly reduces the collision rate.</p>
<h2>D. 4 Impact of Ego-States on Open-Loop Motion Planning</h2>
<p>As pointed out by Zhai et al. (2023) and Li et al. (2023), ego status information can provide a strong heuristic for open-loop motion planning. To investigate the impact of ego states on Agent-Driver, we conduct experiments of Agent-Driver using purely ego-status and without ego-status. The results are shown in Table 3, where all settings are trained with $10 \%$ data. We can see that: (1) The ego-only baseline is worse than Agent-Driver, which indicates Agent-Driver does not merely rely on ego-states but also benefits from the proposed components, and (2) After dropping the ego-states, Agent-Driver still works well and performs on-par with UniAD, which means Agent-Driver can still work decently well without ego-states.
We further note that most prior works that Agent-Driver compared with in Table 2 of the main text use ego-states for planning. Specifically, VAD explicitly inputs ego-states as side information for the planner, and UniAD implicitly includes ego-states via its BEV module (Li et al., 2023).</p>
<h2>D. 5 Compatibility with Different Neural Modules</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*indicates equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8589 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8589</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8589</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264146296</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09430v4.pdf" target="_blank">Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness when performing logical reasoning has not been sufficiently assessed. To comprehensively evaluate this ability, we develop three new logical reasoning datasets named"ReClor-plus","LogiQA-plus"and"LogiQAv2-plus"that extend standard logical reasoning datasets to evaluate the robustness of the LLM's reasoning. For each, we create three subsets: the first with randomly shuffled options, the second with the correct choices replaced by"none of the other options is correct", and the third with a combination of shuffling and substitution. Experiments on these datasets show that these simple augmentations greatly hinder the models' performance. Despite their high performance on the original publicly available datasets, we find that all models perform poorly on these newly constructed datasets. We also demonstrate that introducing task variations into the training set can markedly improve the model's performance on both the original and our developed datasets. Finally, we show that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning. Source code and data are made publicly available at https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8589.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8589.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large autoregressive transformer from OpenAI used here in zero-shot and prompted settings (8192 token context in experiments). Shows strong performance on original multiple-choice logical reading-comprehension datasets but degrades on task-structure perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (decoder-only) trained by OpenAI; exact training data and parameter count not public. Used with 8192-token context; evaluated zero-shot, with Chain-of-Thought prompting, and with logic-driven prompt augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 (and their + variants ReClor-plus, LogiQA-plus, LogiQAv2-plus)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice reading-comprehension datasets derived from exams (GMAT/LSAT and civil service exams) requiring multi-step logical reasoning, inference, and elimination of distractors; paper introduces structural variants that (a) shuffle option order, (b) replace the correct option with 'none of the other options is correct', (c) combine both.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; Chain-of-Thought (CoT) prompting; instruction prompting; logic-driven prompt augmentation (AMR-LDA augmentation of options/context).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strong on original formats (e.g., ReClor original = 0.888 accuracy in Table 8) but catastrophic drop on Replace-Answer and Shuffle-RepAns variants (ReClor Replace-Answer 0.176, Shuffle-RepAns 0.144 in Table 8). CoT yields modest improvements on some variants but often remains near random on hardest splits.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms small open-source generative models in zero-shot; instruction fine-tuning of open models (Alpaca-7B) can approach GPT-4 when merged fine-tuning data is used; logic-driven prompt augmentation on the test prompt can improve GPT-4 results versus unaugmented prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance drops strongly on task-structure variations (especially Replace-Answer and combined Shuffle-RepAns), indicating sensitivity to option framing and possible memorisation of dataset formats; chain-of-thought only helps on some examples and does not reliably fix failures; inability to be fine-tuned by authors (no access to internal weights) limits remediation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GPT-4 appears to do well on original public benchmarks (possibly partly due to training exposure) but lacks robust generalisation to simple task-structure perturbations; logic-driven prompt augmentation (augmenting options/context at prompt time) can help; CoT helps only marginally and inconsistently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8589.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8589.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercially-available autoregressive LLM (RLHF-tuned) evaluated in zero-shot and with AMR-LDA prompt augmentation; performs well on original tasks but degrades under task-structure perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (decoder-only) fine-tuned with reinforcement learning from human feedback (RLHF) from earlier GPT-3 family; used in zero-shot and with prompt-augmentation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and the paper's perturbed variants</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning reading-comprehension benchmarks requiring deduction, elimination of distractors, and reasoning over text; paper adds Shuffle-Order, Replace-Answer, and combined variants.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting; instruction prompting with AMR-LDA prompt augmentation (augment options/context).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High on original datasets relative to small open models, but notable drops on perturbed variants; AMR-LDA prompt augmentation on the test prompt improved accuracy (paper reports that applying augmentation to test prompts increased GPT-3.5's performance). Exact per-dataset numbers are reported qualitatively in the paper (see tables), e.g., in prompt-augmentation rows GPT-3.5-IPT-LDA shows strong improvements vs non-augmented prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Beats Alpaca/Vicuna zero-shot; improved when test prompts are augmented with logic-driven augmentations; does not reach GPT-4 performance in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sensitive to option re-ordering and to Replace-Answer style distractors; CoT prompting did not consistently improve GPT-3.5 on the hardest variants.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Prompt-level logic augmentation can help GPT-3.5 generalise to some task-structure variations; merely adding augmented data into next-token fine-tuning (i.e., changing training distribution) can harm performance if distribution mismatch occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8589.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8589.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (7B) instruction-tuned LLaMA variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-tuned generative LLM (7B) derived from LLaMA checkpoints, evaluated both zero-shot and after instruction fine-tuning on the logical reasoning datasets and augmented data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following model built from LLaMA weights then instruction-finetuned (Stanford Alpaca recipes); evaluated in zero-shot and after further instruction fine-tuning on ReClor/LogiQA/LogiQAv2 and on AMR-LDA augmented data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and perturbed variants (ReClor-plus, LogiQA-plus, LogiQAv2-plus)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Exam-style multiple-choice logical reasoning requiring deduction; paper's perturbed variants test permutation invariance and detection of 'none of the above' cases.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Instruction fine-tuning (IFT) on original and augmented training sets; instruction prompting; Chain-of-Thought prompting in inference; logic-driven data augmentation (AMR-LDA) applied to training or prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot performance very poor (below random); instruction fine-tuning on corresponding task(s) substantially improves accuracy; when fine-tuned on merged datasets, Alpaca-7B's performance approaches GPT-4 on some settings. Using AMR-LDA training augmentation sometimes degraded next-token fine-tuning performance, but using AMR-LDA for prompt augmentation or for fine-tuning in certain ways (applied consistently) could yield large gains (paper reports e.g., Alpaca-7B-IFT-LDA achieving large improvements in some prompt-augmentation experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially improved over its own zero-shot baseline after instruction fine-tuning; competitive with GPT-4 when trained on merged datasets, but sensitive to how augmentation is applied (training-time augmentation sometimes harmful).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Zero-shot fails on perturbed variants; fine-tuning helps but requires sufficient data (improvements on Shuffle-RepAns require large datasets >10k samples); overfitting to perturbed structures can harm performance on other variants (e.g., Shuffle-Order).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction fine-tuning with appropriately varied task-structure examples markedly improves robustness; logic-driven data augmentation must be applied carefully (training vs prompt augmentation yields different outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8589.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8589.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (7B) instruction-tuned model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B instruction-following model evaluated zero-shot and after instruction fine-tuning; shows similar patterns to Alpaca-7B: poor zero-shot, improved after fine-tuning but sensitivity to perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned conversational LLM built using user-shared chat data and LLaMA base; evaluated zero-shot and after further fine-tuning/prompting on logical reading-comprehension datasets and their perturbed variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and perturbed variants</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice reading-comprehension logical reasoning requiring inference and elimination.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot, Chain-of-Thought prompting, instruction fine-tuning experiments similar to Alpaca-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot below GPTs and in some cases below random; fine-tuning improves but results are unstable across datasets; fails hard variants in zero-shot and even after naïve fine-tuning unless logic-driven augmentations and task-structure variations are properly included.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to Alpaca-7B behaved similarly across experiments; both are substantially worse than GPT-3.5/GPT-4 zero-shot but improve with instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sensitive to option shuffling and Replace-Answer variants; CoT prompting does not consistently rescue performance.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Open instruction-tuned 7B models require careful fine-tuning with task-structure variation to approach robust logical reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8589.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8589.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LReasoner (ALBERT-XXLarge-v2 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative (encoder-based) model fine-tuned for multi-choice logical reading-comprehension tasks; uses logic-oriented techniques and demonstrated strong performance on ReClor leaderboard.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Discriminative encoder-only transformer model using ALBERT-XXLarge-v2 as backbone, fine-tuned for multi-choice logical reasoning classification tasks. Uses logic-driven approaches in its training pipeline (as described in its original work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA (and applied to LogiQAv2 evaluation in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-choice reading-comprehension requiring logical deduction and elimination of incorrect options; evaluated on original and perturbed option-format variants.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Discriminative fine-tuning with logic-driven data augmentation (AMR-LDA style augmentations used in related discriminative methods); input formed as concatenations (Context + [SEP] + Question + [SEP] + Option) producing permutation invariance for Shuffle-Order variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strong on original datasets; stable on Shuffle-Order due to discriminative input format; suffers significant drops on Replace-Answer and combined Shuffle-RepAns sets (paper reports notable declines in these splits). Performance approached GPT-4 on some original dataset evaluations when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms or is competitive with generative models when fine-tuned on the task; shows permutation invariance relative to generative models because of how inputs are constructed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant performance degradation on Replace-Answer and Shuffle-RepAns variants, indicating weakness in recognising 'none of the above' style reasoning and complex structural changes; reliance on concatenated option-specific inputs limits generalisation to Replace-Answer manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Discriminative fine-tuning plus logic-driven augmentation helps in original-form tasks and yields permutation invariance for option shuffling, but does not solve more complex logical generalisation like 'none of the other options' detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8589.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8589.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MERIt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MERIt (DeBERTaV2-XXLarge backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative fine-tuned model using relation extraction and intermediate training to improve logical reasoning; achieved top leaderboard performance on ReClor in prior work and used here as a strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MERIt</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-based discriminative model with DeBERTaV2-XXLarge backbone; uses meta-path guided contrastive learning and intermediate corpora to improve logical reasoning ability on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA (evaluated in paper; weights used for LogiQAv2 where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Reading-comprehension logical reasoning benchmarks requiring inference, elimination, and multi-step deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Discriminative fine-tuning with intermediate training data (relation-extraction constructed examples) and testing on task-structure perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High accuracy on original datasets (MERIt reported strong leaderboard results in prior work); in this paper, MERIt demonstrates commendable performance and sometimes surpasses GPT-4 on merged-training evaluations, but like other discriminative models drops significantly on Replace-Answer and combined perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Competitive with other discriminative methods (LReasoner, AMR-LDA) and can exceed generative GPT-4 performance when fine-tuned and given merged diverse training data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not robust to Replace-Answer and Shuffle-RepAns variants; shows limitations similar to other discriminative models on detecting the 'no correct option' manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Intermediate-task pretraining and relation-extraction-derived data can boost discriminative model performance on original tasks, but robustness to trivial structural perturbations remains unresolved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8589.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8589.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-LDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-LDA (AMR-based Logic-Driven Data Augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative model and an AMR-based augmentation method that constructs logically-equivalent/inequivalent sentence tuples to augment training data for logical reasoning tasks; used both as a model (AMR-LDA) and as an augmentation technique applied to prompts and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AMR-LDA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines Abstract Meaning Representation (AMR) transformations with logic equivalence laws to generate extended options and contexts (logical-equivalence sentences) to augment multi-choice training data; implemented with a DeBERTaV2-XXLarge classification backbone for discriminative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and their perturbed variants</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-choice logical reading-comprehension where AMR-LDA creates logically equivalent/contrapositive/implication augmentations of context and options to increase training diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Logic-driven data augmentation (AMR-based): extended option and extended context appended to inputs; used for (a) fine-tuning discriminative models, (b) prompt augmentation for generative models, and (c) training-set augmentation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When used to augment discriminative model training, AMR-LDA improves performance and yields permutation invariance on Shuffle-Order; however, adding AMR-LDA augmented data into next-token prediction fine-tuning for generative models often harmed generalisation (distribution mismatch). Prompt-level augmentation (appending AMR-LDA augmentations at test time) improved performance of GPT-3.5/GPT-4 in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improves discriminative model baselines (LReasoner/MERIt) when applied as training augmentation; harms generative next-token fine-tuning if used naively as training augmentation, but helps when used for prompt augmentation or when consistently applied to both train/test.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Introducing AMR-LDA augmented data into generative next-token fine-tuning can degrade performance due to distribution mismatch; does not fully resolve Replace-Answer failure modes by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Logic-driven augmentation is valuable, but its application must respect the model architecture and training objective: discriminative models benefit from training-time augmentation, while generative models benefit more from prompt-time augmentation or carefully aligned training-test augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8589.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8589.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA family (7B to 65B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source family of decoder-only transformer models used as bases for instruction-tuned Alpaca/Vicuna; the paper evaluates different LLaMA sizes (7B–65B) zero-shot on perturbed logical tasks and finds little correlation between size and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (7B–65B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer family (LLaMA) with checkpoints available at multiple parameter scales; the paper evaluates zero-shot performance across sizes (7B through 65B) on the Shuffle-RepAns variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–65B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Shuffle-RepAns variants of ReClor/LogiQA/LogiQAv2</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Perturbed multiple-choice logical reasoning where options are shuffled and the correct answer replaced with 'none of the other options is correct', testing permutation invariance and detection of no-correct-answer.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation across model sizes under identical prompting (no instruction fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No significant difference found across sizes in zero-shot Shuffle-RepAns performance: increasing size (7B→65B) did not reliably improve robustness on this strict logical-structure variant.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Larger LLaMA sizes did not systematically outperform smaller ones on these perturbed logical tasks in zero-shot; contrasts with other tasks where scale correlates with performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>All sizes performed poorly on the hardest perturbed variants without instruction fine-tuning; size alone insufficient to ensure logical generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Model scale (within same architecture family) is not a substitute for targeted instruction fine-tuning or data augmentation that encodes task-structure variations; robustness needs data/algorithmic solutions beyond scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQAv2-an improved dataset for logical reasoning in natural language understanding <em>(Rating: 2)</em></li>
                <li>Enhancing logical reasoning of large language models through logic-driven data augmentation <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>LReasoner <em>(Rating: 1)</em></li>
                <li>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8589",
    "paper_id": "paper-264146296",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A proprietary large autoregressive transformer from OpenAI used here in zero-shot and prompted settings (8192 token context in experiments). Shows strong performance on original multiple-choice logical reading-comprehension datasets but degrades on task-structure perturbations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Autoregressive transformer (decoder-only) trained by OpenAI; exact training data and parameter count not public. Used with 8192-token context; evaluated zero-shot, with Chain-of-Thought prompting, and with logic-driven prompt augmentation.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 (and their + variants ReClor-plus, LogiQA-plus, LogiQAv2-plus)",
            "reasoning_task_description": "Multiple-choice reading-comprehension datasets derived from exams (GMAT/LSAT and civil service exams) requiring multi-step logical reasoning, inference, and elimination of distractors; paper introduces structural variants that (a) shuffle option order, (b) replace the correct option with 'none of the other options is correct', (c) combine both.",
            "method_or_approach": "Zero-shot prompting; Chain-of-Thought (CoT) prompting; instruction prompting; logic-driven prompt augmentation (AMR-LDA augmentation of options/context).",
            "performance": "Strong on original formats (e.g., ReClor original = 0.888 accuracy in Table 8) but catastrophic drop on Replace-Answer and Shuffle-RepAns variants (ReClor Replace-Answer 0.176, Shuffle-RepAns 0.144 in Table 8). CoT yields modest improvements on some variants but often remains near random on hardest splits.",
            "baseline_comparison": "Outperforms small open-source generative models in zero-shot; instruction fine-tuning of open models (Alpaca-7B) can approach GPT-4 when merged fine-tuning data is used; logic-driven prompt augmentation on the test prompt can improve GPT-4 results versus unaugmented prompting.",
            "limitations_or_failures": "Performance drops strongly on task-structure variations (especially Replace-Answer and combined Shuffle-RepAns), indicating sensitivity to option framing and possible memorisation of dataset formats; chain-of-thought only helps on some examples and does not reliably fix failures; inability to be fine-tuned by authors (no access to internal weights) limits remediation experiments.",
            "insights_or_conclusions": "GPT-4 appears to do well on original public benchmarks (possibly partly due to training exposure) but lacks robust generalisation to simple task-structure perturbations; logic-driven prompt augmentation (augmenting options/context at prompt time) can help; CoT helps only marginally and inconsistently.",
            "uuid": "e8589.0",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-Turbo (OpenAI)",
            "brief_description": "A commercially-available autoregressive LLM (RLHF-tuned) evaluated in zero-shot and with AMR-LDA prompt augmentation; performs well on original tasks but degrades under task-structure perturbations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Autoregressive transformer (decoder-only) fine-tuned with reinforcement learning from human feedback (RLHF) from earlier GPT-3 family; used in zero-shot and with prompt-augmentation experiments.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 and the paper's perturbed variants",
            "reasoning_task_description": "Multiple-choice logical reasoning reading-comprehension benchmarks requiring deduction, elimination of distractors, and reasoning over text; paper adds Shuffle-Order, Replace-Answer, and combined variants.",
            "method_or_approach": "Zero-shot prompting; instruction prompting with AMR-LDA prompt augmentation (augment options/context).",
            "performance": "High on original datasets relative to small open models, but notable drops on perturbed variants; AMR-LDA prompt augmentation on the test prompt improved accuracy (paper reports that applying augmentation to test prompts increased GPT-3.5's performance). Exact per-dataset numbers are reported qualitatively in the paper (see tables), e.g., in prompt-augmentation rows GPT-3.5-IPT-LDA shows strong improvements vs non-augmented prompting.",
            "baseline_comparison": "Beats Alpaca/Vicuna zero-shot; improved when test prompts are augmented with logic-driven augmentations; does not reach GPT-4 performance in many cases.",
            "limitations_or_failures": "Sensitive to option re-ordering and to Replace-Answer style distractors; CoT prompting did not consistently improve GPT-3.5 on the hardest variants.",
            "insights_or_conclusions": "Prompt-level logic augmentation can help GPT-3.5 generalise to some task-structure variations; merely adding augmented data into next-token fine-tuning (i.e., changing training distribution) can harm performance if distribution mismatch occurs.",
            "uuid": "e8589.1",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Alpaca-7B",
            "name_full": "Alpaca (7B) instruction-tuned LLaMA variant",
            "brief_description": "An open-source instruction-tuned generative LLM (7B) derived from LLaMA checkpoints, evaluated both zero-shot and after instruction fine-tuning on the logical reasoning datasets and augmented data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B",
            "model_description": "Instruction-following model built from LLaMA weights then instruction-finetuned (Stanford Alpaca recipes); evaluated in zero-shot and after further instruction fine-tuning on ReClor/LogiQA/LogiQAv2 and on AMR-LDA augmented data.",
            "model_size": "7B",
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 and perturbed variants (ReClor-plus, LogiQA-plus, LogiQAv2-plus)",
            "reasoning_task_description": "Exam-style multiple-choice logical reasoning requiring deduction; paper's perturbed variants test permutation invariance and detection of 'none of the above' cases.",
            "method_or_approach": "Instruction fine-tuning (IFT) on original and augmented training sets; instruction prompting; Chain-of-Thought prompting in inference; logic-driven data augmentation (AMR-LDA) applied to training or prompts.",
            "performance": "Zero-shot performance very poor (below random); instruction fine-tuning on corresponding task(s) substantially improves accuracy; when fine-tuned on merged datasets, Alpaca-7B's performance approaches GPT-4 on some settings. Using AMR-LDA training augmentation sometimes degraded next-token fine-tuning performance, but using AMR-LDA for prompt augmentation or for fine-tuning in certain ways (applied consistently) could yield large gains (paper reports e.g., Alpaca-7B-IFT-LDA achieving large improvements in some prompt-augmentation experiments).",
            "baseline_comparison": "Substantially improved over its own zero-shot baseline after instruction fine-tuning; competitive with GPT-4 when trained on merged datasets, but sensitive to how augmentation is applied (training-time augmentation sometimes harmful).",
            "limitations_or_failures": "Zero-shot fails on perturbed variants; fine-tuning helps but requires sufficient data (improvements on Shuffle-RepAns require large datasets &gt;10k samples); overfitting to perturbed structures can harm performance on other variants (e.g., Shuffle-Order).",
            "insights_or_conclusions": "Instruction fine-tuning with appropriately varied task-structure examples markedly improves robustness; logic-driven data augmentation must be applied carefully (training vs prompt augmentation yields different outcomes).",
            "uuid": "e8589.2",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Vicuna-7B",
            "name_full": "Vicuna (7B) instruction-tuned model",
            "brief_description": "An open-source 7B instruction-following model evaluated zero-shot and after instruction fine-tuning; shows similar patterns to Alpaca-7B: poor zero-shot, improved after fine-tuning but sensitivity to perturbations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_description": "Open-source instruction-tuned conversational LLM built using user-shared chat data and LLaMA base; evaluated zero-shot and after further fine-tuning/prompting on logical reading-comprehension datasets and their perturbed variants.",
            "model_size": "7B",
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 and perturbed variants",
            "reasoning_task_description": "Multiple-choice reading-comprehension logical reasoning requiring inference and elimination.",
            "method_or_approach": "Zero-shot, Chain-of-Thought prompting, instruction fine-tuning experiments similar to Alpaca-7B.",
            "performance": "Zero-shot below GPTs and in some cases below random; fine-tuning improves but results are unstable across datasets; fails hard variants in zero-shot and even after naïve fine-tuning unless logic-driven augmentations and task-structure variations are properly included.",
            "baseline_comparison": "Compared to Alpaca-7B behaved similarly across experiments; both are substantially worse than GPT-3.5/GPT-4 zero-shot but improve with instruction fine-tuning.",
            "limitations_or_failures": "Sensitive to option shuffling and Replace-Answer variants; CoT prompting does not consistently rescue performance.",
            "insights_or_conclusions": "Open instruction-tuned 7B models require careful fine-tuning with task-structure variation to approach robust logical reasoning performance.",
            "uuid": "e8589.3",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LReasoner",
            "name_full": "LReasoner (ALBERT-XXLarge-v2 backbone)",
            "brief_description": "A discriminative (encoder-based) model fine-tuned for multi-choice logical reading-comprehension tasks; uses logic-oriented techniques and demonstrated strong performance on ReClor leaderboard.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LReasoner",
            "model_description": "Discriminative encoder-only transformer model using ALBERT-XXLarge-v2 as backbone, fine-tuned for multi-choice logical reasoning classification tasks. Uses logic-driven approaches in its training pipeline (as described in its original work).",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA (and applied to LogiQAv2 evaluation in the paper)",
            "reasoning_task_description": "Multi-choice reading-comprehension requiring logical deduction and elimination of incorrect options; evaluated on original and perturbed option-format variants.",
            "method_or_approach": "Discriminative fine-tuning with logic-driven data augmentation (AMR-LDA style augmentations used in related discriminative methods); input formed as concatenations (Context + [SEP] + Question + [SEP] + Option) producing permutation invariance for Shuffle-Order variant.",
            "performance": "Strong on original datasets; stable on Shuffle-Order due to discriminative input format; suffers significant drops on Replace-Answer and combined Shuffle-RepAns sets (paper reports notable declines in these splits). Performance approached GPT-4 on some original dataset evaluations when fine-tuned.",
            "baseline_comparison": "Outperforms or is competitive with generative models when fine-tuned on the task; shows permutation invariance relative to generative models because of how inputs are constructed.",
            "limitations_or_failures": "Significant performance degradation on Replace-Answer and Shuffle-RepAns variants, indicating weakness in recognising 'none of the above' style reasoning and complex structural changes; reliance on concatenated option-specific inputs limits generalisation to Replace-Answer manipulation.",
            "insights_or_conclusions": "Discriminative fine-tuning plus logic-driven augmentation helps in original-form tasks and yields permutation invariance for option shuffling, but does not solve more complex logical generalisation like 'none of the other options' detection.",
            "uuid": "e8589.4",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MERIt",
            "name_full": "MERIt (DeBERTaV2-XXLarge backbone)",
            "brief_description": "A discriminative fine-tuned model using relation extraction and intermediate training to improve logical reasoning; achieved top leaderboard performance on ReClor in prior work and used here as a strong baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MERIt",
            "model_description": "Encoder-based discriminative model with DeBERTaV2-XXLarge backbone; uses meta-path guided contrastive learning and intermediate corpora to improve logical reasoning ability on downstream tasks.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA (evaluated in paper; weights used for LogiQAv2 where applicable)",
            "reasoning_task_description": "Reading-comprehension logical reasoning benchmarks requiring inference, elimination, and multi-step deduction.",
            "method_or_approach": "Discriminative fine-tuning with intermediate training data (relation-extraction constructed examples) and testing on task-structure perturbations.",
            "performance": "High accuracy on original datasets (MERIt reported strong leaderboard results in prior work); in this paper, MERIt demonstrates commendable performance and sometimes surpasses GPT-4 on merged-training evaluations, but like other discriminative models drops significantly on Replace-Answer and combined perturbations.",
            "baseline_comparison": "Competitive with other discriminative methods (LReasoner, AMR-LDA) and can exceed generative GPT-4 performance when fine-tuned and given merged diverse training data.",
            "limitations_or_failures": "Not robust to Replace-Answer and Shuffle-RepAns variants; shows limitations similar to other discriminative models on detecting the 'no correct option' manipulations.",
            "insights_or_conclusions": "Intermediate-task pretraining and relation-extraction-derived data can boost discriminative model performance on original tasks, but robustness to trivial structural perturbations remains unresolved.",
            "uuid": "e8589.5",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AMR-LDA",
            "name_full": "AMR-LDA (AMR-based Logic-Driven Data Augmentation)",
            "brief_description": "A discriminative model and an AMR-based augmentation method that constructs logically-equivalent/inequivalent sentence tuples to augment training data for logical reasoning tasks; used both as a model (AMR-LDA) and as an augmentation technique applied to prompts and fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AMR-LDA",
            "model_description": "Combines Abstract Meaning Representation (AMR) transformations with logic equivalence laws to generate extended options and contexts (logical-equivalence sentences) to augment multi-choice training data; implemented with a DeBERTaV2-XXLarge classification backbone for discriminative experiments.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 and their perturbed variants",
            "reasoning_task_description": "Multi-choice logical reading-comprehension where AMR-LDA creates logically equivalent/contrapositive/implication augmentations of context and options to increase training diversity.",
            "method_or_approach": "Logic-driven data augmentation (AMR-based): extended option and extended context appended to inputs; used for (a) fine-tuning discriminative models, (b) prompt augmentation for generative models, and (c) training-set augmentation experiments.",
            "performance": "When used to augment discriminative model training, AMR-LDA improves performance and yields permutation invariance on Shuffle-Order; however, adding AMR-LDA augmented data into next-token prediction fine-tuning for generative models often harmed generalisation (distribution mismatch). Prompt-level augmentation (appending AMR-LDA augmentations at test time) improved performance of GPT-3.5/GPT-4 in many cases.",
            "baseline_comparison": "Improves discriminative model baselines (LReasoner/MERIt) when applied as training augmentation; harms generative next-token fine-tuning if used naively as training augmentation, but helps when used for prompt augmentation or when consistently applied to both train/test.",
            "limitations_or_failures": "Introducing AMR-LDA augmented data into generative next-token fine-tuning can degrade performance due to distribution mismatch; does not fully resolve Replace-Answer failure modes by itself.",
            "insights_or_conclusions": "Logic-driven augmentation is valuable, but its application must respect the model architecture and training objective: discriminative models benefit from training-time augmentation, while generative models benefit more from prompt-time augmentation or carefully aligned training-test augmentations.",
            "uuid": "e8589.6",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA (various sizes)",
            "name_full": "LLaMA family (7B to 65B)",
            "brief_description": "Open-source family of decoder-only transformer models used as bases for instruction-tuned Alpaca/Vicuna; the paper evaluates different LLaMA sizes (7B–65B) zero-shot on perturbed logical tasks and finds little correlation between size and robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA (7B–65B)",
            "model_description": "Decoder-only transformer family (LLaMA) with checkpoints available at multiple parameter scales; the paper evaluates zero-shot performance across sizes (7B through 65B) on the Shuffle-RepAns variant.",
            "model_size": "7B–65B",
            "reasoning_task_name": "Shuffle-RepAns variants of ReClor/LogiQA/LogiQAv2",
            "reasoning_task_description": "Perturbed multiple-choice logical reasoning where options are shuffled and the correct answer replaced with 'none of the other options is correct', testing permutation invariance and detection of no-correct-answer.",
            "method_or_approach": "Zero-shot evaluation across model sizes under identical prompting (no instruction fine-tuning).",
            "performance": "No significant difference found across sizes in zero-shot Shuffle-RepAns performance: increasing size (7B→65B) did not reliably improve robustness on this strict logical-structure variant.",
            "baseline_comparison": "Larger LLaMA sizes did not systematically outperform smaller ones on these perturbed logical tasks in zero-shot; contrasts with other tasks where scale correlates with performance.",
            "limitations_or_failures": "All sizes performed poorly on the hardest perturbed variants without instruction fine-tuning; size alone insufficient to ensure logical generalisation.",
            "insights_or_conclusions": "Model scale (within same architecture family) is not a substitute for targeted instruction fine-tuning or data augmentation that encodes task-structure variations; robustness needs data/algorithmic solutions beyond scale.",
            "uuid": "e8589.7",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "LogiQAv2-an improved dataset for logical reasoning in natural language understanding",
            "rating": 2,
            "sanitized_title": "logiqav2an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "Enhancing logical reasoning of large language models through logic-driven data augmentation",
            "rating": 2,
            "sanitized_title": "enhancing_logical_reasoning_of_large_language_models_through_logicdriven_data_augmentation"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "LReasoner",
            "rating": 1
        },
        {
            "paper_title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
            "rating": 1,
            "sanitized_title": "merit_metapath_guided_contrastive_learning_for_logical_reasoning"
        }
    ],
    "cost": 0.01573275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning
30 Mar 2024</p>
<p>Qiming Bao 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>XtractaNew Zealand</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Gaël Gendron 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Alex Yuxuan Peng 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Wanjun Zhong 
School of Computer Science and Engineering
Sun Yat-Sen University</p>
<p>Neset Tan 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Yang Chen 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Michael Witbrock 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Jiamou Liu 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning
30 Mar 202431B5D715A00C54C336EC05DCCC43D395arXiv:2310.09430v4[cs.CL]
Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels.However, their generalisation and robustness when performing logical reasoning has not been sufficiently assessed.To comprehensively evaluate this ability, we develop three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus" that extend standard logical reasoning datasets to evaluate the robustness of the LLM's reasoning.For each, we create three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options is correct", and the third with a combination of shuffling and substitution.Experiments on these datasets show that these simple augmentations greatly hinder the models' performance.Despite their high performance on the original publicly available datasets, we find that all models perform poorly on these newly constructed datasets.We also demonstrate that introducing task variations into the training set can markedly improve the model's performance on both the original and our developed datasets.Finally, we show that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning.Source code and data are made publicly available at 1 .</p>
<p>Introduction</p>
<p>By leveraging the vast amounts of data available on the internet, large language models have achieved great performance on various tasks Brown et al. (2020); Wei et al. (2022b).However, training data that is confined to a single task or structure may lead to overfitting to specific tasks, consequently diminishing the model's generalisation capabilities.Specifically, there is a dearth of both quantity and quality of data on logical reasoning available on the internet Liu et al. (2023a).This scarcity results in limitations for large language models when tackling complex logical reasoning tasks Wang et al. (2022).Consequently, ensuring high quality and diversity in training data becomes essential, as it can significantly aid large language models in enhancing their robustness on reasoning tasks.Current logical reasoning datasets do not truly represent the reasoning abilities of large language models, as making small modifications significantly degrades performance.Many logical reasoning datasets are designed to select the precise correct option, models might adapt to choose answers that merely resemble the correct one.Moreover, since numerous public logical reasoning datasets were published prior to the training of these large language models, the models could have been trained using these datasets Li &amp; Flanigan (2023).Consequently, these models might inadvertently recall the location of the correct answer.</p>
<p>To address the above issues, We propose a data perturbation procedure and apply it to three existing logical reasoning datasets, resulting in the creation of "ReClor-plus", "LogiQAplus" and "LogiQA v2-plus".These new datasets feature three subsets to evaluate the generalisation and robustness of large language models.These new datasets contain modifications of the task structure of existing logical reasoning datasets.We do not change the semantics of the original context, and only modify its structure (orders, forms).We perform systematic experiments with our logical reasoning datasets to investigate the models' generalisation and robustness on three main aspects.First, we evaluate the in-context generalisation and robustness of large language models on logical reasoning tasks.Second, we perform instruction fine-tuning, instruction prompting and logic-driven data augmentation to evaluate whether those methods can help improve the performance of the models.Third, we investigate how different proportions of data perturbation on the training set can help models improve their generalisation and robustness.Fourth, since model with larger parameter scale demonstrate better performance, it is underexplored how model scale will influence model's performance on logical reasoning.we discuss whether the number of parameters in the model can influence the model's generalisation and robustness.</p>
<p>Our benchmark supports both discriminative large language models like LReasoner Wang et al. (2022), MERIt Jiao et al. (2022), and AMR-LDA Bao et al. (2023) and generative large language models like GPT-3.5 OpenAI (2023a), GPT-4 OpenAI (2023b), LLaMA Touvron et al. (2023), Alpaca Taori et al. (2023) and Vicuna Chiang et al. (2023).</p>
<p>Our main findings can be summarised as follows:</p>
<p>• We find that existing large language models like GPT-3.5 and GPT-4 perform well on logical reasoning tasks in the original format but their performance drops on our new formats, suggesting that the models may have seen these datasets during training and failed to acquire generalised logical reasoning capabilities.• We find that instruction fine-tuning can help large language models increase their generalisation and robustness on logical reasoning tasks.In particular, fine-tuned discriminative large language models often demonstrate permutation invariance.Furthermore, applying logic-driven data augmentation for fine-tuning, combined with prompting, can enhance the generalisation performance of both discriminative large language models and generative large language models.• We find that, for large training set sizes (more than 10,000 training samples), high ratio of perturbated data (shuffled and substituted) can help increase generative large language model's performance on most logical reasoning tasks.However, this does not work with small training sets.• Finally, we find surprisingly that there is no direct correlation between the model's size (from LLaMA-7B to LLaMA-65B) and its generalisation and robustness on logical reasoning tasks.Contrary to intuition and observations from other tasks Touvron et al. ( 2023), a larger model does not necessarily guarantee better generalisation and robustness on logical reasoning tasks.</p>
<p>Related Work</p>
<p>Research on the generalisation and robustness of large language models for logical reasoning mainly focuses on synthetic natural language reasoning.Initial findings indicate that transformers can be trained on multi-hop reasoning tasks and substantially generalise to deeper unseen reasoning depths, although it can be challenging on paraphrased synthetic test sets Clark et al. (2021).Efforts to enhance generalisation to deeper multi-step reasoning include the introduction of PARARULE-Plus, providing data augmentation on reasoning depths between 2 to 5 Bao et al. (2022).AbductionRules incorporates abductive reasoning to understand and answer the multi-step reasoning task Young et al. (2022).ROBUSTLR is a challenging dataset considering conjunction, negation, and utilising logical equivalence for paraphrasing Sanyal et al. (2022).</p>
<p>The existing synthetic multi-step reasoning datasets often lack the complexity and diversity found in real-world data.They might be generated using a limited set of rules or scenarios, which can lead to a narrower scope of logical reasoning challenges.ReClor Yu et al. (2020) and LogiQA Liu et al. (2020a) are challenging reading comprehension datasets derived from real-world examinations such as the GMAT, LSAT, and national civil servant exams Yu et al. (2020); Liu et al. (2020a;2023b).An enhanced version, LogiQAv2, incorporates additional data from Chinese civil servant examinations Liu et al. (2023b).Evaluations reveal that the implementation of discourse order-oriented loss functions, specifically Sentence Order Prediction (SOP) and Next Sentence Prediction (NSP), enhances the performance of models like ALBERT and BERT on reasoning tasks Lan et al. (2020); Kenton &amp; Toutanova (2019); Li et al. (2022).Further assessments of generative large language models, including GPT-3.5 and GPT-4, on ReClor, LogiQA, and LogiQAv2, have demonstrated their commendable performance.However, it is unclear how robust large language models are in real-world logical reasoning tasks Liu et al. (2023b).Bao et al. (2023); Jiao et al. (2022); Wang et al. (2022) utilise reading comprehension tasks requiring logical reasoning to perform experiments and evaluate the logical reasoning capabilities in existing large language models.The model needs to predict the answer by understanding the rules within the context and deducing the conclusion, which cannot be directly found through simple pattern matching from the context.This differs fundamentally from typical reading comprehension tasks.Existing work focuses on reading comprehension through task structure variation, such as shuffling options or replacing the correct one, yet no studies directly utilize task variation to assess logical reasoning.AddSent Jia &amp; Liang (2017) generates misleading text by modifying the question according to certain rules and manually proofreading; AddAny Jia &amp; Liang (2017) automatically searches for misleading texts word by word across various MRC models; AddAn-sCtx Liu et al. (2020b) generates misleading text by removing answer words from answer sentences.In this paper, we define the generalisation and robustness of models in logical reasoning that these models not only need to solve the original question but also address new questions that have been modified through task structure variations, including shuffling the order of options and replacing the correct answer.</p>
<p>Method</p>
<p>We propose a logical reasoning evaluation benchmark for evaluating the robustness and generalisation of large language models.Figure 2 illustrates the various configurations tested.We propose three task structure variations and apply them to three existing datasets to construct our logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus".The task variations are as follows: 1) Shuffle-Order: The order of all the options is shuffled.This variation evaluates whether the model is reasoning or remembers the position of the correct answer.2) Replace-Answer: The correct answer is replaced with "none of the other options is correct" and we add "You can also say there is no correct answer" at the end of the question.This variation evaluates whether the model understands that apart from the correct option, all other options are incorrect.It is used to detect models returning answers that look correct.3) Shuffle-RepAns: The third split combines the variations from 1) and 2).We apply our variations on ReClor Yu et al. (2020), LogiQA Liu et al. (2020a) and LogiQAv2 Liu et al. (2023b).The three logical reasoning datasets are all formatted as multiple-choice reading comprehension tasks.An example from ReClor can be found in Figure 1.LogiQA and LogiQAv2 are following the same format as ReClor.For each question in these datasets, there are only four options, and only one of these options is the correct answer.We propose these three datasets to validate the robustness of the model from the following three perspectives: a) We want to evaluate whether the model performs reasoning, instead of just memorizing the position of the correct answer.b) We want to evaluate whether the model understands that, aside from the correct option, all other options are incorrect.We do not want model to simply predict the answer that looks most like the correct answer.c) Building upon the second point, we shuffle the order of the options to judge whether the model can perform more complex reasoning.Figure 1: The instruction fine-tuning involves providing the model with a task description before the input.It includes the Instruction, Input, and Question.The model then gives the expected output.The correct answer is highlighted in blue with a checkmark.Each question has four choices, and only one of them is the correct answer.</p>
<p>Fine-tuned discriminative large language models have to select one answer from the answer set.In constrast, the generative large language models used in next-token prediction setting have to generate a text that matches the correct option letter.To better make a comparison between these two classes of models, we add the following instruction after the question: "Please only return the letter in front of your predict correct option, A, B, C or D." to ease the evaluation.We catch the correct answer using regular expressions from the generated prediction if the prediction does not correspond to the desired format.We use the official Alpaca2 and Vicuna3 repositories for instruction fine-tuning and instruction prompting.We perform instruction fine-tuning and instruction prompting on both models.For instruction prompting, we use Chain-of-Thought prompting to explore how it can help increase the generalisation and robustness of large language models in logical reasoning tasks.</p>
<p>Instruction Fine-Tuning/Prompting Instruction Fine-Tuning (IFT) has been proposed to enhance the performance of large language models on unseen tasks Mishra et al. (2022); Wei et al. (2022a).An example of this approach is depicted in Figure 1.An instruction, "Can you predict the correct option for the given input?" is added, encompassing the context, question, and each option from the logical reasoning datasets under evaluation.Subsequently, a sentence is introduced: "Please only return the letter in front of your predicted correct option, A, B, C, or D." The process entails training the model on pairs of instructions and corresponding responses.Contrastingly, while instruction prompting employs a similar input format, it is utilised during inference rather than training.Two methods of instruction prompting (IPT) are implemented: a zero-shot evaluation with a format identical to instruction fine-tuning, and a second approach, Chain-of-Thought Wei et al. (2022c), specifically applied to enhance performance in logical reasoning.The latter integrates the following prompt: "Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:' " into the instructions to encourage the model to reason.</p>
<p>We also investigate if adding our perturbed sets into the training of the LLMs can help increase performance, and what proportion of perturbed data is required.We perform instruction fine-tuning with ReClor, LogiQA and LogiQAv2 separately.We use different ratios of perturbed sets when shuffling and replacing the order of options.</p>
<p>Experiments</p>
<p>Experiment Setup</p>
<p>Modeling Choices We use GPT-3.5-Turbofor the GPT-3.5 experiments and GPT-4 with 8192 tokens for the GPT-4 experiments.For fine-tuned discriminative large language models, we refer to the code from the ReClor leaderboard4 .The appendix demonstrates a more detailed hyperparameter setting and model selection.</p>
<p>Result on the Original Datasets</p>
<p>We summarise our primary findings in Table 1.In this subsection, we emphasise results from the original ReClor, LogiQA, and LogiQAv2 datasets.Under the zero-shot evaluation setting, both GPT-3.5 and GPT-4 notably outperform Alpaca-7B and Vicuna-7B across the three datasets, with the latter two models underperforming.Among these, GPT-4 achieves the highest accuracy.We then fine-tuned Alpaca-7B and Vicuna-7B using training data from each individual task and subsequently evaluated them.Notably, all the fine-tuned generative large language models display a marked improvement over their zero-shot evaluation performance.</p>
<p>Simultaneously, we fine-tuned LReasoner, MERIt, and AMR-LDA on their respective training sets and found commendable performance across the board.AMR-LDA's accuracy approached that of GPT-4.To further enhance the diversity and volume of the training data, we merged training sets from ReClor, LogiQA, and LogiQA-v2.Both generative and discriminative large language models exhibited improved results in this setting.The performance of the fine-tuned Alpaca-7B approached GPT-4's, while both MERIt and AMR-LDA surpassed GPT-4.These results underscore the significance of data diversity and volume in logical reasoning tasks.</p>
<p>Assessing Models' Robustness on Logical Reasoning Tasks</p>
<p>Generative Large Language Models We evaluated the performance of GPT-3.5 and GPT-4 on various datasets, notably observing a significant performance drop on the LogiQAv2 dataset, particularly in its Shuffle-Order variant, compared to less pronounced declines on other datasets.Given that GPT-3.5 and GPT-4 were trained before 2023, and considering the publication of ReClor and LogiQA before 2022, it's plausible that these datasets were part of their training data.However, since LogiQAv2 was released after 2023, it was not included, making it a reliable measure of the models' reasoning capabilities.To address potential data leakage concerns with ReClor and LogiQA, we applied instruction fine-tuning using these datasets, both individually and combined with LogiQAv2.This approach significantly enhanced performance across the board.Furthermore, we assessed Alpaca-7B and Vicuna-7B, finding that their zero-shot evaluations were below random chance levels, and fine-tuning failed to yield consistent improvements across datasets.The observed variability in performance, with improvements on shuffled datasets when initial performance was poor and vice versa, suggests a lack of reliable logical reasoning and potential overfitting to the training data, with minor accuracy gains likely attributable to random variance rather than genuine progress.</p>
<p>Discriminative Large Language Models Fine-tuned discriminative large language models such as LReasoner, MERIt, and AMR-LDA exhibit enhanced generalisation performance on logical reasoning questions.These models demonstrate stable performance on both the original and Shuffle-Order sets; however, they experience a significant decline in performance on the Replace-Answer set and Shuffle-RepAns set.The stability observed in the Shuffle-Order set performance can be attributed to the models' handling of input structure, where the input is formed by concatenating the context, question, and each corresponding option (Context + [SEP]' + Question + [SEP]' + Option), with the output being the label for each concatenation.The special token '[SEP]' is used to separate sentences.Thus, shuffling the option order doesn't create new context, question, and option concatenations, maintaining stable performance and permutation invariance.Despite this, discriminative models do not surpass generative models on the Replace-Answer and Shuffle-RepAns sets, suggesting these models might not engage in complex logical reasoning as effectively.Therefore, the overall performance indicates that large language models, including discriminative ones, may lack strong logical reasoning abilities, as detailed in Table 1.</p>
<p>Chain-of-Thought (CoT) Prompting</p>
<p>We perform further experiments using Chain-of-Thought prompting Wei et al. (2022c).As shown in Table 2, in the zero-shot evaluation, all generative large language models do not perform well.The performance of GPT-3.5 and GPT-4 on the Shuffle-RepAns set is nearly equivalent to a random guess, while Alpaca-7B and Vicuna-7B fail the task.Employing the CoT prompting doesn't result in a significant difference in performance for these generative large language models compared to when CoT prompting is not used.Only GPT-4 exhibits systematic improvements in accuracy on the Shuffle-RepAns task.Nonetheless, this task remains challenging for these models, including GPT-4.We have included some case studies in the appendix A that illustrate the use of CoT prompting to assist GPT-4 in correctly answering questions with intermediate steps.Overall, since CoT prompting does not offer any explicitly useful information as additional input, and the model has not been trained to respond correctly when given the CoT prompting, it is reasonable to expect that it may not perform well in complex logical reasoning scenarios.Since logic-driven data augmentation performs well on logical reasoning tasks for finetuned discriminative large language models like LReasoner and AMR-LDA, it is worth considering to apply this method to generative large language models trained on nexttoken prediction task and see if there is an improvement in our logical reasoning tasks for these models.We use the provided augmented data from the authors of AMR-LDA for ReClor, LogiQA and LogiQAv2 datasets and extend the augmented option information to each option in the training set for ReClor, LogiQA and LogiQAv2.The input format for the augmented data is formed as follows: context + question + each option + extended option + extended context.The term 'extended option' refers to the use of AMR-LDA to augment the option based on the logical equivalence laws.'Extended context' means using AMR-LDA to augment the context based on the logical equivalence laws.The output is the ID of each option, which can be either A, B, C, or D. An example for the extended option and the extended context can be found in Appendix Figure 3. Two examples of how AMR has been used to perform logical equivalence sentence construction for long sentence can be found in Appendix Table 4 and 5.We perform instruction fine-tuning using the input and output formats, along with the same instructions mentioned in the experiment setup.</p>
<p>Models</p>
<p>Table 3 shows that logic-driven data augmentation is detrimental to the generalisation and robustness of large language models trained using next-token prediction for logical reasoning tasks.We make the hypothesis that logic-driven data augmentation does not directly map to the task of next-token prediction, which may disturb the training of the model.This hypothesis is corroborated by Table 3.When we use individual training sets from ReClor, LogiQA, and LogiQAv2 for separate training and testing, there is an observed improvement compared to models that did not undergo such training.However, performance significantly declines when we utilise AMR-LDA to augment data in the training set.This phenomenon is also evident when the training set is expanded to include ReClor, LogiQA, and LogiQAv2 collectively.This suggests that merely increasing the scale of the training set can offer some benefits in enabling the model to tackle more complex logical reasoning tasks.While, if there is a discrepancy between the distributions of the training and test sets, the potential improvements will be constrained.Additionally, using logic-driven data augmentation in the training set, which might distort its distribution, could further deteriorate performance on the test set.</p>
<p>To enhance the performance of models on more complex logical reasoning tasks, we employ logic-driven data augmentation for generative large language models during instruction fine-tuning and prompting.As shown in Table 3, for the experiment was conducted under "prompt augmentation using AMR-LDA" area, both the training and test sets were augmented using AMR-LDA.We discovered that two logic-driven data augmentation methods can enhance model's performance on task variation.The first method involves applying logic-driven data augmentation to both the training and test sets; this approach was effective for the fine-tuned Alpaca-7B, but not for the Alpaca-7B that hadn't been finetuned.The second method involves applying logic-driven data augmentation solely to the test set; this approach was effective for GPT-3.5 and GPT-4.To implement logic-driven data augmentation on the test set, we followed three steps:</p>
<p>Step 1: Augment each option and append the augmented text to the original option.</p>
<p>Step 2: If an option can be augmented, we then augment the context and append this augmented context to the already augmented option.</p>
<p>Step 3: The augmented prompt will be composed of the following elements: context + question + each option + extended option + extended context.This strategy helped large language models enhance their performance on our task variation.This result indicates that logic-driven data augmentation offers benefits in two aspects.The first is altering the distribution of both the training and test sets.The second is enhancing the prompt or input information for larger language models like GPT-3.5 and GPT-4, which extracts additional details to large language models, thereby enhancing their efficacy in logical reasoning tasks.</p>
<p>Transfer Learning with Data Perturbation</p>
<p>We perform transfer learning to investigate the extent to which incorporating variations of task structure into the training set can help models improve their performance on logical reasoning tasks.Table 4 shows our results.We carried out instruction fine-tuning on Alpaca-7B using individual training sets from ReClor, LogiQA, and LogiQAv2, each with varying data perturbation ratios, specifically focusing on Shuffle-RepAns.The reason we selected Alpaca-7B is that our main experiment, as shown in Table 1, involves both Alpaca-7B and Vicuna-7B.These models demonstrate similar performance in Table 1.Under the constraint of limited computational resources, we therefore chose Alpaca-7B for a deeper analysis.We observed that using a large training set of LogiQAv2 for fine-tuning (over 10,000 samples).50% of the training set is perturbed to the Shuffle-RepAns format, which improves performance on all original, Shuffle-Order, Replace-Answer, and Shuffle-RepAns format logical reasoning tasks.However, when the amount of training samples used for the fine-tuning is less than 10,000, the model does not demonstrate the above phenomenon.We also observe an intriguing phenomenon: the models' performance on the Shuffle-Order set does not improve as the proportion of the Shuffle-RepAns set in the training set increases.Instead, it declines.Particularly on the ReClor and LogiQA datasets, we can clearly see that the higher the task structure perturbation ratio, the worse the model performs on the Shuffle-Order set.We speculate that the model may overfit the data with altered task structure perturbation ratio.These results also support the claim that existing large language models are not robust when solving logical reasoning tasks.To enhance the robustness of logical reasoning in large language models, employing a more expansive training dataset and a higher perturbation ratio demonstrates beneficial.</p>
<p>Influence of the Model Size</p>
<p>We perform additional experiments to see whether, with the same model architecture, a larger model size will show better generalisation and robustness on logical reasoning tasks.</p>
<p>We use different size of LLaMA models to perform the experiments from LLaMA-7B to LLaMA-65B.This choice was made because LLaMA provides model checkpoints for various sizes, ranging from 7B to 70B, which were available during the period of our experiment.We utilised the Shuffle-RepAns sets and perform the zero-shot evaluation.Table 5 shows the results.We do not find a significant difference in the Shuffle-RepAns format logical reasoning task with different sizes of models under zero-shot evaluation, without any instruction fine-tuning.In summary, Table 5 sheds light on the nuanced relationship between model size and performance in logical reasoning tasks.While larger models might be expected to perform better due to their increased capacity, the results presented here suggest that model size does not play a significant role in determining overall performance on robust and complex logical reasoning tasks.It's worth noting that our comparisons are based on different model sizes of the same LLaMA base model.Therefore, the conclusions drawn may not necessarily apply to comparisons involving different base models.</p>
<p>Models</p>
<p>Conclusion</p>
<p>Our study examines the generalisation and robustness of large language models (LLMs) in logical reasoning tasks, revealing significant limitations.We find that instruction finetuning enhances performance, while chain-of-thought (CoT) prompting alone falls short in robust reasoning tasks.Analysis shows that datasets larger than 10,000 samples, such as LogiQAv2, require extensive data perturbation and task structure modifications (Shuffle-RepAns) for improved adaptability-a necessity not seen with smaller datasets.Model size, within the same LLaMA base model framework, does not guarantee better generalisation or robustness.We observe that logic-driven data augmentation benefits fine-tuned discriminative models by improving performance and demonstrating permutation invariance, indicating reliance on more than memorisation.Applying logic-driven data augmentation to prompts further aids larger generative models like GPT-3.5, GPT-4, and fine-tuned Alpaca-7B, enhancing their logical reasoning capabilities in novel tasks.</p>
<p>A Appendix</p>
<p>A.1 Background proposed and it, again exceeding the performance on MERIt.We utilise MERIt and AMR-LDA to perform fine-tuning on the downstream tasks using their public model weights.</p>
<p>Generative Large Language Models</p>
<p>A.2 Hyperparameter Setting</p>
<p>We determine the parameter values in Tables 6 and 7 by following the public parameter values provided by Stanford Alpaca 2 for generative large language models, as well as the default hyperparameters used by each discriminative large language model, including AMR-LDA6 , LReasoner7 , and MERIt8 .We use the model weights from the official repositories of LReasoner, MERIt and AMR-LDA on each logical reasoning reading comprehension tasks.</p>
<p>For LReasoner, we select ALBERT-XXLarge-v2 Lan et al. (2019) as the backbone model.For MERIt and AMR-LDA, we select DeBERTaV2-XXLarge He et al. ( 2021) as the backbone model.We selected ALBERT-XXLarge-v2 and DeBERTaV2-XXLarge because they are the best available models, as mentioned in their original papers, LReasoner Wang et al. (2022) and MERIt Jiao et al. (2022), respectively.The authors of LReasoner did not publicly release the model weights for LogiQAv2 and their logic-driven data augmentation are designed for ReClor and LogiQA, hence we use their model weights trained on LogiQA to evaluate the result on LogiQAv2.For fine-tuning instructions on Alpaca and Vicuna, we refer to the training script and code from the official Alpaca GitHub repository Taori et al. (2023).</p>
<p>For the generative large language models like GPT-3.5 and GPT-4.We propose a new instruction for the Shuffle-Order set.We provide an instruction that includes an example, shuffles the order of options for this example, and then request the model's response.For the Replace-Answer set, we add an instruction "You can select the option that none of the other options is correct" when we test more complex scenarios.For the Shuffle-RepAns set, we add the two instructions together.All the experiments are run on 8 NVIDIA A100 GPUs, each with 80G of VRAM.We evaluate on the validation sets of ReClor, LogiQA and LogiQAv2, as the test sets are private and used to rank models on the leaderboard.We constructed our task variations from the existing validation sets.We employed the modified validation set (Shuffle-Order, Replace-Answer, and Shuffle-RepAns) to perform the evaluation.we use accuracy to adhere to the methodologies outlined in previously published papers Bao et al. (2023); Jiao et al. (2022); Wang et al. (2022).We follow the same metrics to ensure a fair comparison when evaluating the model's performance.The datasets used are balanced and only a single choice per question is a logical entailment, so accuracy is a proper measure to assess the models' logical reasoning performance.For prompting GPT-3.5 and GPT-4, we use the default hyperparameters from OpenAI playground.</p>
<p>A.3 Analysis of Hyperparameters for GPT-4</p>
<p>We performed an experiment on GPT-4 using different hyperparameter values (temperature) as shown in Table 8.This experiment was carried out using the GPT-4 API, trained on a dataset containing information up to April 2023.For the other tables in this paper, the experiments utilised the earlier version of GPT-4, released in March, which had only learned from data available up to September 2021.However, we found that temperature did not significantly influence the model's performance across the entire evaluation set.We find that using Chain-of-Thought Prompting can show some improvements in Shuffle-RepAns logical reasoning tasks with GPT-4, while its accuracy is no different from random guessing.Therefore, we listed some examples to see what differences are made in the results generated by GPT-4 after incorporating Chain-of-Thought Prompting.Figure 6, 8 and 10 show that GPT-4 fails the example but with Chain-of-Thought Prompting, it can correctly answer the question with solution process.Figure 7, 9, and 11 show that both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on the example.</p>
<p>A.4 Limitations</p>
<p>Our paper acknowledges several limitations: Firstly, pre-training large language models (LLMs) is computationally expensive, potentially causing fine-tuning efforts to suffer from residual effects of pre-training tasks.Secondly, prevalent logical reasoning evaluation tasks, designed akin to reading comprehension exercises, may unfairly advantage discriminative LLMs over generative counterparts due to their format, highlighting the absence of a balanced dataset for fair assessment across different LLM training tasks.Lastly, the unavailability of GPT-4's source code and training data precludes us from conducting instruction fine-tuning experiments to test its logical reasoning robustness.Figure 3: Example of using AMR-LDA to augment the prompt from the ReClor dataset and their subsequent utilization as input for GPT-4.Data segments that are marked in bold italics and appear in blue were generated using the contraposition law, while those in brown were generated using the implication law.In this example, only the segments in dark blue are the extended option, and the sentences that follow constitute the extended context.</p>
<p>A.5 Ethics Statement</p>
<p>As we do not present a new model but perform an evaluation of the existing state-of-the-art, we believe our work does not present significant ethical risks.We believe that our work will contribute to a better understanding of the training mechanisms of large language models in current large models.By highlighting flaws in the capabilities of language models, this evaluation can help mitigating risks of misuses that could raise ethical concerns.</p>
<p>Long Sentence Example 1:</p>
<p>Original sentence: Sarah woke up early in the morning, and she started her day with a cup of coffee and some light yoga stretches.Original sentence's AMR graph: (a / and :op1 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning)))) :op2 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :ARG0 p :mod (y / yoga) :ARG1-of (l / light-06) :quant (s3 / some))))) Modified AMR graph using AMR-LDA: (a / and :op1 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :ARG0 p :mod (y / yoga) :ARG1-of (l / light-06) :quant (s3 / some)))) :op2 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning))))) Generated logical equivalence sentence using AMR-LDA: Sarah started her day with a cup of coffee and some light yoga stretching and woke up early in the morning.</p>
<p>Figure 4: One example uses our AMR-LDA to generate logical equivalence sentences for long sentences.In this case, a logical equivalence sentence is generated using the commutative law, and the same color represents the same argument.In this case, the order of the former and latter arguments for the conjunction word "and" has been swapped.</p>
<p>Long Sentence Example 2:</p>
<p>Original sentence: Sarah woke up early in the morning, and she started her day with a cup of coffee and some light yoga stretches that will help lose weight.Original sentence's AMR graph: (a / and (a / and :op1 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning)))) :op2 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :mod (y / yoga) :ARG0-of (h / help-01 :ARG1 (l / lose-01 :ARG1 (w2 / weight))) :ARG1-of (l2 / light-06) :quant (s3 / some))))) Modified AMR graph using AMR-LDA: (a / and :op1 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :mod (y / yoga) :ARG0-of (h / help-01 :ARG1 (l / lose-01 :ARG1 (w2 / weight))) :ARG1-of (l2 / light-06) :quant (s3 / some)))) :op2 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning))))) Generated logical equivalence sentence using AMR-LDA: Sarah started her day with a cup of coffee and some light yoga stretching to help lose weight, and woke up early in the morning.Figure 5: One example uses our AMR-LDA to generate logical equivalence sentences for long sentences.In this case, a logical equivalence sentence is generated using the commutative law, and the same color represents the same argument.AMR-LDA can understand the effect of that clause on yoga stretching.In this case, the order of the former and latter arguments for the conjunction word "and" has been swapped.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: Some biologists believe that the capacity for flight first developed in marine reptiles, claiming that feathers are clearly developed from scales.Other biologists rightly reject this suggestion, pointing out that bats have no scales and that nonmarine reptiles also have scales.Those who believe that flight first developed in tree-dwelling reptiles reject the claim that the limbs of land-dwelling reptiles might have developed into wings.They insist that it is more likely that tree-dwelling reptiles developed wings to assist their leaps from branch to branch.Instruction: Can you predict the correct option for the given input?Input: Advertisement: The dental profession knows that brushing with Blizzard toothpaste is the best way to fight cavities.We surveyed five dentists, and each agreed that the tartar control formula found in Blizzard is the most effective cavityfighting formula available in a toothpaste.Question: The flawed reasoning in which one of the following is most similar to the flawed reasoning in the advertisement?Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:'.The 'ANSWER' is the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. We know that electing Gomez would be the best course for the nation to follow because, of ten voters we surveyed, each agreed that electing Gomez would help the nation.B. Some of the nation's voters believe that Gomez is the candidate who would be best for the nation.Of the ten voters we surveyed, each agreed that the policies Gomez is committed to would be the best policies for the nation to adopt.C. The nation's voters know that Gomez is the candidate whose policies would be best for the nation.Of ten voters polled, each said that Gomez would be a very popular leader.D. None of the other options is correct.Correct Answer: "D" GPT-4: "A" Chain-of-Thought Prompting GPT-4: The reasoning in the advertisement is flawed because it is based on a small and potentially biased sample size (five dentists) to make a general claim about the dental profession's view on Blizzard toothpaste.Let's analyse each option: A. This option is flawed for the same reason as the advertisement: it's based on a small sample size (ten voters) to make a claim about the best course for the nation.B. This option is also based on a small sample size (ten voters), but it doesn't make a general claim about the nation's voters.Instead, it only states that some voters believe Gomez is the best candidate.C.This option is flawed because it makes a general claim about the nation's voters based on the popularity of Gomez among ten voters, not on the effectiveness of his policies.D. This option states that none of the other options is correct.</p>
<p>The most similar flawed reasoning to the advertisement is found in option A, as it also uses a small sample size to make a general claim.ANSWER: A Figure 7: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns ReClor dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.In this example, Chain-of-Thought try to predict an option which looks more like the correct answer.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: An organisation that elects a leadership team implements such a rule?if there are more than one candidate, the candidate must agree to be nominated, and before expressing his consent, he must be informed who his candidate is.</p>
<p>Question: If the above conclusion is true, then the following is also true? Please only return the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. None of the other options is correct.B. Implement the rule so that the number of nominated candidates is greater than not implementing the rule.C. Implementing this rule, there is at most one candidate nominated.D. Implement the rule, no candidate may be nominated.Correct Answer: "A" GPT-4: "C" Chain-of-Thought Prompting GPT-4: To analyse each option, let's break down the initial rule:</p>
<p>1.There must be more than one candidate.2. The candidate must agree to be nominated.</p>
<ol>
<li>The candidate must be informed of their opponent before agreeing to be nominated.Now let's evaluate each option: A. None of the other options is correct.We cannot determine this without evaluating the other options first.B. Implement the rule so that the number of nominated candidates is greater than not implementing the rule.This option is not necessarily true.The rule does not guarantee that there will be more nominated candidates.It only ensures that a candidate knows their opponent before agreeing to be nominated.C. Implementing this rule, there is at most one candidate nominated.This option is incorrect.The rule states that there must be more than one candidate.Therefore, there cannot be only one nominated candidate.D. Implement the rule, no candidate may be nominated.This option is also incorrect.The rule does not prevent candidates from being nominated, it only requires that they know their opponent before agreeing to be nominated.ANSWER: A. None of the other options is correct.</li>
</ol>
<p>Figure 8: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQA dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: There are three types of robots in a laboratory.Step 2: Analyse each option to determine if it aligns with the characteristics of directed thinking.</p>
<p>A. Seeing the man with braids, he concluded that the man was a woman -This statement is based on past knowledge and experience (assuming that braids are typically associated with women).-It forms a direction (concluding that the person is a woman).B. The peach blossoms in Beijing are in bloom, so spring is coming -This statement is based on past knowledge and experience (assuming that peach blossoms bloom in spring).-It forms a direction (predicting that spring is coming).C. The clothes are broken.I want to sew them with a sewing bag -This statement is based on past knowledge and experience (knowing that sewing can fix broken clothes).-It forms a method or procedure (using a sewing bag to sew the clothes).D. None of the other options is correct.-This statement contradicts the analysis of options A, B, and C, which all align with the characteristics of directed thinking.</p>
<p>Step 3: Propose a solution based on the analysis.ANSWER: D</p>
<p>Figure 10: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQAv2 dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.With the help of Chain-of-Thought Prompting, GPT-4 can find out option D is contradicts the analysis of options A, B, and C, which all align with the characteristics of directed thinking.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: In today's society, many old people become old drifters in order to take care of their children and the next generation.In a recent urban survey,52.2percent of the young people surveyed admitted that their parents are old drifters.They and their partners have just started their careers, and work pressure is high, so they have no time to take care of their children and do housework.According to experts, the group of old drifters in China's cities will further expand.Question: Which of the following, if true, best supports the views of the above experts?Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:'.The 'ANSWER' is the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. Some old people prefer to live in their hometown B. The elderly in the city have better medical conditions than in the countryside C. None of the other options is correct.D. After the implementation of the two-child policy, the two-child birth rate in urban areas is lower than that in rural areas Correct Answer: "C" GPT-4: "B" Chain-of-Thought Prompting GPT-4:</p>
<p>Step 1: Identify the claim made by experts The claim made by experts is that the group of old drifters in China's cities will further expand.</p>
<p>Step 2: Evaluate each option based on its relevance to the claim A. Some old people prefer to live in their hometown This option does not directly support the claim that the group of old drifters in China's cities will further expand.It only states a preference of some old people.B. The elderly in the city have better medical conditions than in the countryside This option could indirectly support the claim, as better medical conditions might attract more elderly people to move to cities.However, it does not directly address the issue of old drifters taking care of their children and the next generation.C. None of the other options is correct.This option is not relevant to the claim.D. After the implementation of the two-child policy, the two-child birth rate in urban areas is lower than that in rural areas This option supports the claim that the group of old drifters in China's cities will further expand.If there are more children being born in urban areas due to the two-child policy, it is likely that more elderly people will move to cities to help take care of their children and grandchildren, thus increasing the number of old drifters in cities.</p>
<p>Step 3: Choose the option that best supports the claim ANSWER: D</p>
<p>LLaMA Touvron et al. (2023), AlpacaTaori et al. (2023), VicunaChiang et al. (2023),GPT-3.5OpenAI (2023a), and GPT-4 OpenAI (2023b) are large autoregressive language models pre-trained to perform next-token prediction.They are based on the decoder-part of the transformer architectureVaswani et al. (2017).GPT-3.5 was fine-tuned fromGPT-3 Brown et al. (2020)  using reinforcement learning from human feedback (RLHF) fine-tuning OpenAI (2023a).GPT-4 shows human-level performance on LSAT, SAT Math and Graduate Record Examination (GRE) Quantitative OpenAI (2023b), all of which assess reasoning and problem solving skills in different contexts.The version of GPT-4 that we use does not include multimodal training and its training details are not publicly available.Discriminative Large Language ModelsLReasoner Wang et al. (2022), MERIt Jiao et al. (2022), and AMR-LDA Bao et al. (2023) are language models fine-tuned for downstream logical reasoning tasks.While the generative large language models perform nexttoken prediction, these models are trained to perform classification tasks, such as multi-choice question answering.These models are based on encoder-only transformers Kenton &amp; Toutanova (2019), whose last layers are replaced with classification layers fine-tuned for different tasks.LReasoner uses logic-driven data augmentation to increase their performance and achieved #1 on the ReClor Yu et al. (2020) leaderboard 5 when it was proposed.MERIt uses relation extraction to construct new data from Wikipedia and performs intermediate training on that new data.It achieved #1 5 on the ReClor leaderboard when initially proposed, exceeding the performance of LReasoner.AMR-LDA uses AMRbased logic-driven data augmentation to construct a tuple of sentences (original sentence, logical equivalence sentence, logical inequivalence sentence) and train the model and then train on the downstream tasks.It achieved #2 5 on the ReClor leaderboard when initially</p>
<p>Figure 6 :
6
Figure6: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns ReClor dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.With the help of Chain-of-Thought Prompting, GPT-4 can find out the other options are incorrect and to select "None of the other options is correct."as the predicted answer.</p>
<p>Figure 9 :
9
Figure 9: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQA dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.</p>
<p>Figure 11 :
11
Figure 11: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQAv2 dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.</p>
<p>Please only return the letter in front of your predict correct option, A, B, C or D. A.</p>
<p>Can you predict the correct option for the given input?Input: If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.Question: If the statements above are true, which one of the following must be true?
Instruction Fine-Tuning/Prompting CasesInstruction: If you are not able to write your essays using a word processing program, you haveno keyboarding skills. B. If you are able to write your essays using a word processingprogram, you have at least some keyboarding skills.C. If you are not able to writeyour essays using a word processing program, you are not able to use a computer.D. If you have some keyboarding skills, you will be able to write your essays usinga word processing program.Response: "B"</p>
<p>Table 2 :
2
Comparison between base models and models prompted using CoT prompting.
ReClorLogiQALogiQAv2ShuffleShuffleShuffleRepAnsRepAnsRepAnsZero-shot evaluationAlpaca-7B0.01200.02300.0121Alpaca-7B-CoT0.01200.03370.0152Vicuna-7B0.06400.17970.1784Vicuna-7B-CoT0.13200.16740.1593GPT-3.50.18470.22860.2583GPT-3.5-CoT0.10880.16740.1722GPT-40.13120.16260.1686GPT-4-CoT0.18160.25230.21774.5 Logic-Driven Data AugmentationReClorLogiQALogiQAv2ModelsShuffleShuffleShuffleRepAnsRepAnsRepAnsZero-shot evaluationAlpaca-7B0.01200.01210.0121GPT-3.50.18470.25830.2583GPT-40.13120.16860.1686ReClor/LogiQA/LogiQAv2 single training setAlpaca-7B-IFT0.27200.22730.1548+ AMR-LDA0.04400.05220.0548ReClor + LogiQA + LogiQAv2 merged training setAlpaca-7B-IFT0.11400.13510.1128+ AMR-LDA0.00600.02450.0197Prompt augmentation using AMR-LDAAlpaca-7B-IPT-LDA0.03000.03680.0331Alpaca-7B-IFT-LDA0.48000.36860.2237GPT-3.5-IPT-LDA0.36670.46850.4971GPT-4-IPT-LDA0.87660.55100.7027</p>
<p>Table 3 :
3
Accuracy of evaluated models when adding AMR-LDA's logic-driven augmented data into the training set.We evaluate Alpaca-7B after instruction fine-tuning.
Datasets →ReClorLogiQALogiQAv2Perturbation Ratio ↓ OriginalShuffle OrderReplace AnswerShuffle RepAnsOriginalShuffle OrderReplace AnswerShuffle RepAnsOriginalShuffle OrderReplace AnswerShuffle RepAnsReClor/LogiQA/LogiQAv2 single training set with different ratio of data perturbation (Shuffle-RepAns)0%0.16800.52800.23600.27200.11050.34860.28410.22730.19120.21220.36580.15485%0.33400.37200.15600.17200.14900.13510.09980.09210.26950.15160.13380.112110%0.41400.43200.20400.23800.30720.28260.23500.24420.22620.09560.19630.172715%0.36200.38600.30600.33400.19040.20270.27950.23190.35370.17780.20010.172750%0.15400.14000.16600.16400.04300.05370.67280.65590.35370.20960.76860.7915
Table 4: Accuracy of Alpaca-7B model for transfer learning scenarios and different perturbation ratio applied to the training set.To make a fair comparison, We ensure that the size of each training set is consistent.</p>
<p>Table 6 :
6
Details of hyperparameters for instruction fine-tuning of generative large language models, Alpaca and Vicuna, on ReClor, LogiQA, and LogiQAv2.
Generative Large Language Models Fine-Tuningseed42batch size1num train epochs5max sequence length1024gradient Accumulation Steps16initial Learning Rate1e-05bf16Trueweight decay0warmup ratio0.03lr scheduler typecosinefsdpfull shard auto wrapfsdp transformer layer cls to wrap LlamaDecoderLayertf32TrueoptimiserAdamW</p>
<p>Table 7 :
7
Details of hyperparameters for fine-tuning of discriminative large language models, AMR-LDA, LReasoner and MERIt, on ReClor, LogiQA and LogiQAv2.</p>
<p>Table 8 :
8
A comparison of the performance of GPT-4 at different temperatures on the original ReClor dataset and the alternative ReClor datasets.
Dataset→ReClorTemperature↓ Original Shuffle Order Replace Answer Shuffle RepAns00.88800.88000.17600.14400.50.88200.88970.17200.148010.88200.87200.18400.1460</p>
<p>If you have the skill of a keyboard, you can write your essay using a word processing program.If you can use a computer, you have keyboarding skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program.",</p>
<p>"context": "If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.","question": "If the statements above are true, which one of the following must be true?", "answers": A. "If you are not able to write your essays using a word processing program, you have no keyboarding skills.B. "If you are able to write your essays using a word processing program, you have at least some keyboarding skills.If
AMR-LDA Prompt Augmentation Case StudyGPT-4 Input:</p>
<p>you don't have at least some keyboard skills, you can't write your essay with a word processing program. If you can use a com- puter, you have keyboarding skills. If you can write your essay with a word pro- cessing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program.",</p>
<p>C. "If you are not able to write your essays using a word processing program, you are not able to use a computer.</p>
<p>If you can use a computer, you can write your essay using word processing programs. If you can use a computer, you have keyboard- ing skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing</p>
<p>program.",D. "If you have some keyboarding skills, you will be able to write your essays using a word processing program.If</p>
<p>you can't write your essay with a word processing program, you don't have some keyboard skills. If you can use a computer, you have keyboarding skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program." GPT-4 output: B</p>
<p>Type A can recognise colors, type B can recognise shapes, and type C can neither recognise colors nor shapes.The laboratory experimented with robots 1 and 2 using red balls, basketballs, red squares, and blue squares, and ordered them to pick up the red balls, but number 1 picked up the red squares, and number 2 picked up the basketball.Question: Based on the above experiment, which of the following must be true?Please only return the letter in front of your predict correct option, A, B, C or D. A. Both No.1 and No.2 must not be type C. B. Both No.1 and No.2 must be type C. The experiment involved red balls, basketballs, red squares, and blue squares.The robots were ordered to pick up the red balls.Robot 1 picked up red squares.This means that Robot 1 can recognise the color red but not the shape of a ball.Therefore, Robot 1 must be Type A. Robot 2 picked up basketballs.This means that Robot 2 can recognise the shape of a ball but not the color red.Therefore, Robot 2 must be Type B. Now let's evaluate the options: A. Both No.1 and No.2 must not be type C. -This is true, as we determined that Robot 1 is Type A and Robot 2 is Type B. B. Both No.1 and No.2 must be type C. -This is false, as we determined that Robot 1 is Type A and Robot 2 is Type B. C. No.1 is A and No.2 is Type B. -This is true, as we determined that Robot 1 is Type A and Robot 2 is Type B. D. None of the other options is correct.-This is false, as options A and C are correct.</p>
<p>C. No.1 is A and No.2 is Type B. D. None of the other options is correct.Correct Answer: "D" GPT-4: "A" Chain-of-Thought Prompting GPT-4: Let's analyse the information given: 1. Type A can recognise colors.2. Type B can recognise shapes.3. Type C can neither recognise colors nor shapes.ANSWER: C</p>
<p>https://github.com/tatsu-lab/stanford_alpaca
https://github.com/lm-sys/FastChat
https://github.com/yuweihao/reclor
https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347
AMR-LDA Official GitHub Page
https://github.com/SiyuanWangw/LReasoner/tree/master
https://github.com/SparkJiao/MERIt</p>
<p>Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation. Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu, Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022). the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022)Cumberland Lodge, Windsor Great Park, United KingdomSeptember 2022</p>
<p>Enhancing logical reasoning of large language models through logic-driven data augmentation. Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wanjun Zhong, Gaël Gendron, Timothy Pistotti, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu, Paul Denny, Michael Witbrock, Jiamou Liu, The first edition of the Symposium on Advances and Open Problems in Large Language Models (LLM@IJCAI'23). Macao, ChinaAugust 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Deberta: decodingenhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. OpenReview.net, 2021</p>
<p>Adversarial examples for evaluating reading comprehension systems. Robin Jia, Percy Liang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning. Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie, 10.18653/v1/2022.findings-acl.276Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandMay 2022Association for Computational Linguistics</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT2019</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.119422019arXiv preprint</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, International Conference on Learning Representations. 2020</p>
<p>Task contamination: Language models may not be few-shot anymore. Changmao Li, Jeffrey Flanigan, 10.48550/arXiv.2312.163372023</p>
<p>Eliteplm: An empirical study on general language ability evaluation of pretrained language models. Junyi Li, Tianyi Tang, Zheng Gong, Lixin Yang, Zhuohao Yu, Zhipeng Chen, Jingyuan Wang, Wayne Xin Zhao, Ji-Rong Wen, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, 10.1109/TASLP.2023.3293046IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023a</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023barXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501Christian Bessiere20202020</p>
<p>A robust adversarial training approach to machine reading comprehension. Kai Liu, Xin Liu, An Yang, Jing Liu, Jinsong Su, Sujian Li, Qiaoqiao She, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2020b34</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2023a</p>
<p>. OpenAI. Gpt-4 technical report. 2023b</p>
<p>Robustlr: A diagnostic benchmark for evaluating logical robustness of deductive reasoners. Soumya Sanyal, Zeyi Liao, Xiang Ren, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Logic-driven context extension and data augmentation for logical reasoning of text. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan, 10.18653/v1/2022.findings-acl.127Findings of ACL 2022. Dublin, Ireland2022</p>
<p>Finetuned language models are zeroshot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022a</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022barXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022c</p>
<p>AbductionRules: Training transformers to explain unexpected inputs. Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock, 10.18653/v1/2022.findings-acl.19Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandMay 2022Association for Computational Linguistics</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, OpenReview.net. Addis Ababa, EthiopiaApril 26-30, 2020. 20208th ICLR 2020</p>            </div>
        </div>

    </div>
</body>
</html>
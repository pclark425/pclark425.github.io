<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9709 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9709</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9709</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-273482364</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.14632v1.pdf" target="_blank">Diverging Preferences: When do Annotators Disagree and do Models Know?</a></p>
                <p><strong>Paper Abstract:</strong> We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning 10 categories across four high-level classes -- task underspecification, response style, refusals, and annotation errors. We find that the majority of disagreements are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise. We then explore how these findings impact two areas of LLM development: reward modeling and evaluation. In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences. We also find that these tendencies are also echoed by popular LLM-as-Judge evaluation methods, which consistently identify a winning response in cases of diverging preferences. These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence on evaluation and training.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9709.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9709.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-Judge overall bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biases of LLM-as-Judge evaluations compared to human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper finds that LLMs used as automatic judges (pairwise single-response scoring prompts) systematically produce decisive winners even when human annotators diverge, promoting majority preferences and failing to mark ties or pluralistic options; this behavior can unfairly penalize pluralistically aligned systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended dialogue / general chat prompt response evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various generative evaluators used in experiments (e.g., GPT-4o, GPT-4-Turbo, Claude-3.x, Llama-3-70B in evaluation comparisons and examples)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise single-response scoring prompts (ChatbotArena 'Arena-Hard' prompt variant); LLMs scored two responses and output a winner/tie score (single-response scoring prompt); used as a drop-in automatic judge for LLM completion pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human preference annotations drawn from MultiPref (10k pairs, 4 annotators per pair; Quadratic weighted Cohen's κ = 0.268) and HelpSteer2 (12k pairs, 3–5 annotators per pair; Quadratic weighted Cohen's κ = 0.389); annotators indicate preferred response or tie and degree (slight vs significant) / Likert-5 helpfulness scores on HelpSteer2.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Comparison metric: frequency with which LLM-as-Judge identifies a winning response in different human-agreement splits (Table 4). Reported frequencies: MultiPref — High-Agreement Prefs: 0.731, High-Agreement Ties: 0.426, Diverging Prefs (All): 0.738, Diverging Prefs (High): 0.760; HelpSteer2 — High-Agreement Prefs: 0.646, High-Agreement Ties: 0.519, Diverging Prefs (All): 0.573, Diverging Prefs (High): 0.650. Human inter-rater agreement metrics: MultiPref κ=0.268, HelpSteer2 κ=0.389.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of pluralistic nuance: LLM-as-Judge tends to (1) pick a single winner even when humans disagree (failing to return ties), (2) promote the majority/decisive view rather than representing distributional/divergent human preferences, and (3) embed systematic strategy biases (e.g., favoring compliance over refusal, favoring overton responses over clarification). This results in unfair penalization of models that adopt legitimate pluralistic strategies (refuse, clarify, or serve minority tastes).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 4 and associated analysis: LLM-as-Judge returns a 'winning response' with near-identical frequency for Diverging Prefs and High-Agreement Prefs (e.g., MultiPref Diverging Prefs (All) 0.738 ≈ High-Agreement Prefs 0.731), showing inability to mark ties. WildBench analysis: among top 5% most divisive prompts, LLM-as-Judge preferred complying responses 100% of the time in Comply vs. Refuse cases; clarifying responses lost 83% of the time in underspecified prompts (WildBench manual inspection). Overton responses scored higher than clarification on underspecification (avg. overton 8.48 vs clarifying 6.94 out of 10 using LLM-as-Judge scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The paper notes LLM-as-Judge evaluations are more appropriate for high-agreement instances; they recommend restricting automatic evaluation to high-agreement examples or removing divisive prompts (identified via distributional reward models). Distributional reward models (Mean-Var (KL), Classification (KL)) can detect divisive examples and improve handling of disagreement. The paper does not claim LLM-as-Judge is entirely useless — it performs consistently when human agreement is high.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'BIAS IN LLM-AS-JUDGE AGAINST PLURALISTICALLY ALIGNED LLMS', Table 4, 'REMOVING DIVISIVE EXAMPLES FROM LLM-AS-JUDGE BENCHMARKS', WildBench analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diverging Preferences: When do Annotators Disagree and do Models Know?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9709.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9709.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Refusal-bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-Judge bias favoring complying responses and specific refusal styles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When human preferences diverge on safety/refusal decisions, LLM-as-Judge evaluators favor complying responses over refusals and prefer refusals that prescribe solutions or encourage help rather than simpler refusals, thereby penalizing alternative but valid refusal strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Safety/refusal decisions in dialogue (harmlessness / safety-guarded prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM-as-Judge prompt variants (ChatbotArena style); evaluated on outputs from multiple LLMs (examples include Claude-family, GPT-4 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompt-based evaluation identifying Comply vs Refuse and Refuse-vs-Refuse attribute comparisons (used Mu et al. 2024b methods to label refusal attributes); LLM-as-Judge returns preference scores for completions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators in MultiPref and HelpSteer2 annotated pairs including Comply vs Refuse examples (annotator counts and κ reported above); MultiPref contains prompts sourced from Anthropic Harmless dataset to analyze refusal disagreements.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Empirical frequency analyses: WildBench top-5% divisive prompts manual check found 42% contained Comply vs Refuse disagreements; LLM-as-Judge preferred complying response 100% of the time in those cases (manual analysis reported in section). Table 5 reports preference biases across refusal attributes (paper text describes directionality; exact per-attribute frequencies reported in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges degrades correct treatment of safety pluralism: they (1) overwhelmingly reward compliant answers over refusals even when refusals are the reasonable or safer choice; (2) prefer particular refusal styles (prescriptive/helpful) over valid minimalist refusals — thus mis-evaluating models that intentionally adopt different, legitimate refusal policies.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>WildBench top-5% divisive prompts: in Comply vs Refuse disagreements, LLM-as-Judge scoring preferred the complying response in every inspected case. In Refuse vs Refuse comparisons, LLM-as-Judge favored refusals that prescribe a solution or encourage help over simple refusal statements (reported bias in Table 5 and described in text).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Authors note these biases mean LLM-as-Judge unfairly judges models trained to enact consistent conservative policies (e.g., refuse if any user would judge compliance unsafe). Recommendation: detect and separate divisive prompts (using distributional reward models) before using LLM-as-Judge evaluation; do not rely on LLM-as-Judge for safety-pluralistic assessments without human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'WHAT INFLUENCES LLM-AS-JUDGE DECISIONS OVER DIVERGING PREFERENCES?', subsection 'Biases in Refusals', and 'Identifying Divisive Examples in Wildbench' (WildBench manual analysis and Table 5 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diverging Preferences: When do Annotators Disagree and do Models Know?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9709.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9709.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Underspecification-bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-Judge bias in task underspecification: favoring overton (multi-interpretation) answers over clarifying questions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In prompts with underspecified user intent, LLM-as-Judge evaluators prefer responses that answer multiple possible interpretations (overton) rather than responses that ask for clarification, thereby discouraging models that seek follow-up clarification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Task underspecification / clarifying question vs multi-interpretation answering in dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM-as-Judge scoring (GPT-4o used to categorize responses as overton vs clarification; LLM-as-Judge scoring prompt then used to score the two types)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Used CocoNot underspecified prompt examples and GPT-4o to label responses as Overton vs Clarifying; then scored responses via LLM-as-Judge single-response scoring prompt (score out of 10 reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators in datasets included instances of task underspecification (taxonomy shows Task Underspecification accounts for ~20% MultiPref / 22% HelpSteer2). Human preferences diverge between plausible interpretations (manual taxonomy analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>LLM-as-Judge average scores: overton responses avg 8.48/10 vs clarifying responses avg 6.94/10 (reported in text). Human disagreement prevalence: Task Underspecification frequencies in Table 1 (MultiPref 20%, HelpSteer2 22%).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of interactive/clarifying behavior: LLM-as-Judge's preference for answering multiple interpretations over asking clarifying questions means automated evaluation penalizes clarifying strategies even though clarification is a valid pluralistic policy and often desired in real interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>CocoNot-derived experiment: clarifying responses lost 83% of the time in top divisive WildBench task-underspecification examples; LLM-as-Judge scored overton responses higher (8.48) than clarifying (6.94), showing consistent bias against clarification.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Authors recommend removing or separately analyzing divisive underspecified prompts when using LLM-as-Judge. They also propose using distributional reward models to flag these cases; the paper does not claim LLM-as-Judge is unreliable for clearly specified prompts where human agreement is high.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'WHAT INFLUENCES LLM-AS-JUDGE DECISIONS OVER DIVERGING PREFERENCES?', subsection 'Biases in Task Underspecification', and 'Identifying Divisive Examples in Wildbench' (CocoNot analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diverging Preferences: When do Annotators Disagree and do Models Know?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chatbot arena: An open platform for evaluating llms by human preference <em>(Rating: 2)</em></li>
                <li>Wildbench: Benchmarking llms with challenging tasks from real users in the wild <em>(Rating: 2)</em></li>
                <li>Rewardbench: Evaluating reward models for language modeling <em>(Rating: 2)</em></li>
                <li>Rule based rewards for fine-grained llm safety <em>(Rating: 1)</em></li>
                <li>Length-controlled alpacaeval: A simple way to debias automatic evaluators <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9709",
    "paper_id": "paper-273482364",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-Judge overall bias",
            "name_full": "Biases of LLM-as-Judge evaluations compared to human annotators",
            "brief_description": "This paper finds that LLMs used as automatic judges (pairwise single-response scoring prompts) systematically produce decisive winners even when human annotators diverge, promoting majority preferences and failing to mark ties or pluralistic options; this behavior can unfairly penalize pluralistically aligned systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended dialogue / general chat prompt response evaluation",
            "llm_judge_model": "Various generative evaluators used in experiments (e.g., GPT-4o, GPT-4-Turbo, Claude-3.x, Llama-3-70B in evaluation comparisons and examples)",
            "llm_judge_setup": "Pairwise single-response scoring prompts (ChatbotArena 'Arena-Hard' prompt variant); LLMs scored two responses and output a winner/tie score (single-response scoring prompt); used as a drop-in automatic judge for LLM completion pairs.",
            "human_evaluation_setup": "Human preference annotations drawn from MultiPref (10k pairs, 4 annotators per pair; Quadratic weighted Cohen's κ = 0.268) and HelpSteer2 (12k pairs, 3–5 annotators per pair; Quadratic weighted Cohen's κ = 0.389); annotators indicate preferred response or tie and degree (slight vs significant) / Likert-5 helpfulness scores on HelpSteer2.",
            "agreement_metric": "Comparison metric: frequency with which LLM-as-Judge identifies a winning response in different human-agreement splits (Table 4). Reported frequencies: MultiPref — High-Agreement Prefs: 0.731, High-Agreement Ties: 0.426, Diverging Prefs (All): 0.738, Diverging Prefs (High): 0.760; HelpSteer2 — High-Agreement Prefs: 0.646, High-Agreement Ties: 0.519, Diverging Prefs (All): 0.573, Diverging Prefs (High): 0.650. Human inter-rater agreement metrics: MultiPref κ=0.268, HelpSteer2 κ=0.389.",
            "losses_identified": "Loss of pluralistic nuance: LLM-as-Judge tends to (1) pick a single winner even when humans disagree (failing to return ties), (2) promote the majority/decisive view rather than representing distributional/divergent human preferences, and (3) embed systematic strategy biases (e.g., favoring compliance over refusal, favoring overton responses over clarification). This results in unfair penalization of models that adopt legitimate pluralistic strategies (refuse, clarify, or serve minority tastes).",
            "examples_of_loss": "Table 4 and associated analysis: LLM-as-Judge returns a 'winning response' with near-identical frequency for Diverging Prefs and High-Agreement Prefs (e.g., MultiPref Diverging Prefs (All) 0.738 ≈ High-Agreement Prefs 0.731), showing inability to mark ties. WildBench analysis: among top 5% most divisive prompts, LLM-as-Judge preferred complying responses 100% of the time in Comply vs. Refuse cases; clarifying responses lost 83% of the time in underspecified prompts (WildBench manual inspection). Overton responses scored higher than clarification on underspecification (avg. overton 8.48 vs clarifying 6.94 out of 10 using LLM-as-Judge scoring).",
            "counterexamples_or_caveats": "The paper notes LLM-as-Judge evaluations are more appropriate for high-agreement instances; they recommend restricting automatic evaluation to high-agreement examples or removing divisive prompts (identified via distributional reward models). Distributional reward models (Mean-Var (KL), Classification (KL)) can detect divisive examples and improve handling of disagreement. The paper does not claim LLM-as-Judge is entirely useless — it performs consistently when human agreement is high.",
            "paper_reference": "Sections: 'BIAS IN LLM-AS-JUDGE AGAINST PLURALISTICALLY ALIGNED LLMS', Table 4, 'REMOVING DIVISIVE EXAMPLES FROM LLM-AS-JUDGE BENCHMARKS', WildBench analysis.",
            "uuid": "e9709.0",
            "source_info": {
                "paper_title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Refusal-bias",
            "name_full": "LLM-as-Judge bias favoring complying responses and specific refusal styles",
            "brief_description": "When human preferences diverge on safety/refusal decisions, LLM-as-Judge evaluators favor complying responses over refusals and prefer refusals that prescribe solutions or encourage help rather than simpler refusals, thereby penalizing alternative but valid refusal strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Safety/refusal decisions in dialogue (harmlessness / safety-guarded prompts)",
            "llm_judge_model": "LLM-as-Judge prompt variants (ChatbotArena style); evaluated on outputs from multiple LLMs (examples include Claude-family, GPT-4 variants)",
            "llm_judge_setup": "Prompt-based evaluation identifying Comply vs Refuse and Refuse-vs-Refuse attribute comparisons (used Mu et al. 2024b methods to label refusal attributes); LLM-as-Judge returns preference scores for completions.",
            "human_evaluation_setup": "Human annotators in MultiPref and HelpSteer2 annotated pairs including Comply vs Refuse examples (annotator counts and κ reported above); MultiPref contains prompts sourced from Anthropic Harmless dataset to analyze refusal disagreements.",
            "agreement_metric": "Empirical frequency analyses: WildBench top-5% divisive prompts manual check found 42% contained Comply vs Refuse disagreements; LLM-as-Judge preferred complying response 100% of the time in those cases (manual analysis reported in section). Table 5 reports preference biases across refusal attributes (paper text describes directionality; exact per-attribute frequencies reported in Table 5).",
            "losses_identified": "Using LLMs as judges degrades correct treatment of safety pluralism: they (1) overwhelmingly reward compliant answers over refusals even when refusals are the reasonable or safer choice; (2) prefer particular refusal styles (prescriptive/helpful) over valid minimalist refusals — thus mis-evaluating models that intentionally adopt different, legitimate refusal policies.",
            "examples_of_loss": "WildBench top-5% divisive prompts: in Comply vs Refuse disagreements, LLM-as-Judge scoring preferred the complying response in every inspected case. In Refuse vs Refuse comparisons, LLM-as-Judge favored refusals that prescribe a solution or encourage help over simple refusal statements (reported bias in Table 5 and described in text).",
            "counterexamples_or_caveats": "Authors note these biases mean LLM-as-Judge unfairly judges models trained to enact consistent conservative policies (e.g., refuse if any user would judge compliance unsafe). Recommendation: detect and separate divisive prompts (using distributional reward models) before using LLM-as-Judge evaluation; do not rely on LLM-as-Judge for safety-pluralistic assessments without human oversight.",
            "paper_reference": "Sections: 'WHAT INFLUENCES LLM-AS-JUDGE DECISIONS OVER DIVERGING PREFERENCES?', subsection 'Biases in Refusals', and 'Identifying Divisive Examples in Wildbench' (WildBench manual analysis and Table 5 discussion).",
            "uuid": "e9709.1",
            "source_info": {
                "paper_title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Underspecification-bias",
            "name_full": "LLM-as-Judge bias in task underspecification: favoring overton (multi-interpretation) answers over clarifying questions",
            "brief_description": "In prompts with underspecified user intent, LLM-as-Judge evaluators prefer responses that answer multiple possible interpretations (overton) rather than responses that ask for clarification, thereby discouraging models that seek follow-up clarification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Task underspecification / clarifying question vs multi-interpretation answering in dialogue",
            "llm_judge_model": "LLM-as-Judge scoring (GPT-4o used to categorize responses as overton vs clarification; LLM-as-Judge scoring prompt then used to score the two types)",
            "llm_judge_setup": "Used CocoNot underspecified prompt examples and GPT-4o to label responses as Overton vs Clarifying; then scored responses via LLM-as-Judge single-response scoring prompt (score out of 10 reported in text).",
            "human_evaluation_setup": "Human annotators in datasets included instances of task underspecification (taxonomy shows Task Underspecification accounts for ~20% MultiPref / 22% HelpSteer2). Human preferences diverge between plausible interpretations (manual taxonomy analysis).",
            "agreement_metric": "LLM-as-Judge average scores: overton responses avg 8.48/10 vs clarifying responses avg 6.94/10 (reported in text). Human disagreement prevalence: Task Underspecification frequencies in Table 1 (MultiPref 20%, HelpSteer2 22%).",
            "losses_identified": "Loss of interactive/clarifying behavior: LLM-as-Judge's preference for answering multiple interpretations over asking clarifying questions means automated evaluation penalizes clarifying strategies even though clarification is a valid pluralistic policy and often desired in real interactions.",
            "examples_of_loss": "CocoNot-derived experiment: clarifying responses lost 83% of the time in top divisive WildBench task-underspecification examples; LLM-as-Judge scored overton responses higher (8.48) than clarifying (6.94), showing consistent bias against clarification.",
            "counterexamples_or_caveats": "Authors recommend removing or separately analyzing divisive underspecified prompts when using LLM-as-Judge. They also propose using distributional reward models to flag these cases; the paper does not claim LLM-as-Judge is unreliable for clearly specified prompts where human agreement is high.",
            "paper_reference": "Sections: 'WHAT INFLUENCES LLM-AS-JUDGE DECISIONS OVER DIVERGING PREFERENCES?', subsection 'Biases in Task Underspecification', and 'Identifying Divisive Examples in Wildbench' (CocoNot analysis).",
            "uuid": "e9709.2",
            "source_info": {
                "paper_title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference",
            "rating": 2,
            "sanitized_title": "chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference"
        },
        {
            "paper_title": "Wildbench: Benchmarking llms with challenging tasks from real users in the wild",
            "rating": 2,
            "sanitized_title": "wildbench_benchmarking_llms_with_challenging_tasks_from_real_users_in_the_wild"
        },
        {
            "paper_title": "Rewardbench: Evaluating reward models for language modeling",
            "rating": 2,
            "sanitized_title": "rewardbench_evaluating_reward_models_for_language_modeling"
        },
        {
            "paper_title": "Rule based rewards for fine-grained llm safety",
            "rating": 1,
            "sanitized_title": "rule_based_rewards_for_finegrained_llm_safety"
        },
        {
            "paper_title": "Length-controlled alpacaeval: A simple way to debias automatic evaluators",
            "rating": 1,
            "sanitized_title": "lengthcontrolled_alpacaeval_a_simple_way_to_debias_automatic_evaluators"
        }
    ],
    "cost": 0.0114725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DIVERGING PREFERENCES: WHEN DO ANNOTATORS DISAGREE AND DO MODELS KNOW?
6 Nov 2024</p>
<p>Michael J Q Zhang michaelzhang@nyu.edu 
Zhilin Wang 
Jena D Hwang 
Yi Dong 
Olivier Delalleau 
Yejin Choi 
Eunsol Choi 
Xiang Ren 
Valentina Pyatkin valentinap@allenai.org </p>
<p>New York University α
Allen Institute for Artificial Intelligence β
NVIDIA γ</p>
<p>University of Washington δ
University of Southern California ϵ</p>
<p>DIVERGING PREFERENCES: WHEN DO ANNOTATORS DISAGREE AND DO MODELS KNOW?
6 Nov 2024A91E74471CF08C7C59AB401955EBA767arXiv:2410.14632v2[cs.CL]
We examine diverging preferences in human-labeled preference datasets.We develop a taxonomy of disagreement sources spanning 10 categories across four high-level classes-task underspecification, response style, refusals, and annotation errors.We find that the majority of disagreements are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise.We then explore how these findings impact two areas of LLM development: reward modeling and evaluation.In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences.We also find that these tendencies are also echoed by popular LLM-as-Judge evaluation methods, which consistently identify a winning response in cases of diverging preferences.These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs.To address these issues, we develop methods for identifying diverging preferences to mitigate their influence on evaluation and training.</p>
<p>INTRODUCTION</p>
<p>As large language models (LLMs) continue to rise in prominence and to serve millions of people on a daily basis, there is an increasing need to ensure that systems are pluralistically aligned (Sorensen et al., 2024).Learning from human preferences has emerged as the standard method for adapting LLMs to facilitate user-assistant interactions with much success.Despite these advances, however, the field continues to struggle with the challenge of handing diverging preferences, where users disagree on the ideal response to a prompt.Prior works on developing pluralistically aligned LLMs have focused on the development of synthetic preference datasets, where disagreements are simulated based on author-defined features and frequencies (Poddar et al., 2024;Chen et al., 2024).In this work, we take a step back to ask the foundational question when and why do human annotators disagree in their preferences?</p>
<p>To make this research possible, we introduce MultiPref-Disagreements and HelpSteer2-Disagreements.Note that we did not collect new human annotations but release the individual annotations of these existing datasets (which previously released only aggregated annotations), with support from the dataset creators.Prior to this work, public preference datasets (Bai et al., 2022a;Wang et al., 2024b) were either annotated by a single annotator or preprocessed to aggregate multiple annotations into a single label, making such investigations into annotator disagreements not possible.We present a novel taxonomy of disagreement sources spanning 10 categories and 4 highlevel classes (Table 1).Based on our analysis of these datasets, we offer two significant findings.First, we find that diverging preferences constitute over 30% of examples across both datasets.Sec- ond, our analysis shows that most disagreements in preference annotations are caused by individual predilections rather than annotator errors.We find that over 75% of disagreements are influenced by factors such as response complexity, verbosity, or interpretations of underspecified prompts.</p>
<p>Our findings, that most disagreements in preference annotations are the result of individual predilections rather than annotation errors, run counter to how standard preference learning pipelines are designed and reward models (RMs) are trained, where dissenting opinions are treated as undesirable noise.We demonstrate aggregating labels via majority choice (Wang et al., 2024b;Köpf et al., 2024) results in reward models that predict decisive preference toward a single option, even when annotators preferences diverge.Existing reward modeling approaches, which fail to distinguish diverging from high-agreement preferences, can lead to breakdowns in pluralistic alignment, where LLMs trained from such rewards provide responses for single user perspective, even when preferences diverge.</p>
<p>We introduce two changes in training the reward model: (1) we utilize all user preferences and (2) we model rewards as distributions rather than single value.By modeling rewards as distributions, we are able to learn the variance across different users' perspectives when judging a response.We demonstrate that our methods can model user disagreements in the quality of a given response, successfully identify diverging from high-agreement preferences with a 0.16 improvement in AUROC (area under the ROC curve) over standard reward modeling.</p>
<p>Next, we move onto studying the impact of diverging preferences of popular LLM-as-Judge methods for evaluating LLMs.Practitioners concerned with pluralistic alignment often opt to enforce consistent policies in their LLMs when preferences can diverge (e.g., refuse if any users believe the model should, or ask for clarification given task ambiguity).We find that these evaluations unduly punish models that use such strategies by consistently identifying a winning response, even when humans disagree.We develop methods for identifying such examples with diverging preferences in LLM-as-Judge benchmark evaluation sets Zhao et al. (2024).We find that our method is able to effectively identify examples where LLM-as-Judge evaluation methods unduly punish systems for refusing on unsafe prompts or for prompting the user for further clarification on an underspecified prompt.</p>
<p>Figure 1: Disagreements between pairs of annotators in MultiPref-Disagreements (left) and HelpSteer2-Disagreements (right).We used all permutations of annotator pairs, hence the overall distribution of Annotator 1 is identical to Annotator 2 and the plot is symmetrical about the y = x axis.Along the y = x line, annotators agree perfectly with each other.</p>
<p>ANALYSIS: DIVERGING PREFERENCES IN RLHF ANNOTATION</p>
<p>We define diverging preferences as all instances where annotators disagreed on which response to a given prompt was preferred, ignoring instances where annotators only had slight preferences for either response.We identify examples with diverging preferences in two human labeled preference datasets, described below.We then analyze such examples to develop a taxonomy of disagreement causes (Section 2.1).In contrast with other datasets with multiple preference judgments (Dubois et al., 2023), where prompts are synthetically generated from instruction-following datasets (Wang et al., 2022), we analyze open-ended real user requests sourced primarily from real user interactions with LLMs (RyokoAI, 2023;Zhao et al., 2024;Zheng et al., 2024).</p>
<p>MultiPref is a dataset of 10K preference pairs,2 each consisting of a conversation prompt and two candidate responses (Miranda et al., 2024).Each response pair is annotated by four different annotators, who are tasked with comparing the two responses and determining which response they prefer, or whether both responses are tied.Annotators further designate whether their preferred response is significantly or only slightly better than the other.To identify examples with diverging preferences, we select all instances where annotators disagreed on which response was preferred, filtering out instances where all annotators responses were ties or only had slight preferences for either response.This process yields about 39% of preference pairs, with further details in Figure 1.Following (Wang et al., 2024b), we report inter-rater agreement metric Quadratic weighted Cohen's κ (Scikit-Learn, 2024) as 0.268.Further details for the MultiPref collection can be found at Wang et al. (2024a).</p>
<p>HelpSteer2 is a dataset of 12K preference pairs3 , where each preference pair is annotated by 3-5 different annotators.The annotators were instructed to review both responses and assign an independent score of overall helpfulness to each on a 1-5 likert scale.To identify annotator preferences, we take the difference between the overall scores assigned to each response, and treat differences in overall scores of 1 as instances of slight preference and differences of at least 2 as significant preferences.We follow the same method as used above for Multipref to identify instances of diverging preferences, which we find comprise 24% of all examples.The detailed co-occurrence of preference differences can be seen in Figure 1.Following (Wang et al., 2024b), we report inter-rater agreement metric Quadratic weighted Cohen's κ as 0.389.Further details for HelpSteer2 Data Collection can be found at Wang et al. (2024b).</p>
<p>A TAXONOMY FOR CAUSES OF DIVERGING PREFERENCES</p>
<p>We perform manual analysis of diverging preferences in both datasets and develop a taxonomy for causes of diverging preferences in Table 1.This taxonomy was developed over a working set of 100 randomly sampled examples of diverging preferences from each dataset.Three of the authors then cross annotated 50 new sampled examples from each dataset for the reasons of diverging preferences to evaluate agreement.As there are often multiple possible causes for diverging preferences, we evaluate agreement using both Cohen's κ (comparing full label set equivalence), as well as Krippendorff's α with MASI distance (Passonneau, 2006), yielding (κ = 0.59, α = 0.68) and (κ = 0.58, α = 0.62) over our annotations on MultiPref and Helpsteer2, respectively.Through our analysis and taxonomy construction, we find that disagreements in preference annotations can be attributed to a wide range of causes, and highlight different user perspectives when determining quality of a given response.Below, we describe each disagreement cause and class.</p>
<p>Task Underspecification Disagreements often arise from underspecification in the prompt, where both responses consider and address distinct, valid interpretations of the task.</p>
<p>Response Style</p>
<p>We identify several disagreements causes that arise due to differences in response style, where preferences are primarily influenced by an individual's tastes rather than content.</p>
<p>• Verbosity Disagreements arise over the preferred levels of detail, explanation, or examples in each response.While prior works have noted that RLHF annotations are often biased toward lengthy responses in aggregate (Prasann Singhal &amp; Durrett, 2023), we find that individuals frequently disagree on the preferred level of detail or explanation in a response.• Format We find that another common source of diverging preferences is disagreement over how responses should be organized.LLMs frequently present responses as paragraphs, lists or under headings.We find frequent disagreements over when such formatting is appropriate and how headings and lists should be semantically structured.• Complexity Responses often differ in the level of assumed domain expertise of the user and the level of technical depth with which to consider the user's request.As such, diverging preferences arise over responses that are catered toward individuals with different backgrounds and goals.• Aesthetic tastes Prior work has noted that creative writing or writing assistance comprise a significant portion of user requests Zhao et al. (2024).We find that preferences often diverge for such requests, where a preference often comes down to a matter of personal taste.</p>
<p>Refusals We find that refusals based on safety concerns or model capabilities are often the subject of disagreement among annotators.This finding is consistent with prior work, which has demonstrated that judgments of social acceptability or offensive language can vary based on their personal background and identity (Forbes et al., 2020;Sap et al., 2022).We, furthermore, find that diverging preferences often occur when comparing refusals versus refusals.Recent work has studied establishing different types of refusals (e.g., soft versus hard refusals) and rules for when each are appropriate (Mu et al., 2024b).Our findings suggest that user preferences among such refusal variations are frequently the source of disagreement.</p>
<p>Errors Prior work has noted that an individual's judgment of a response's correctness has almost perfect agreement with their judgment of a response's overall quality (Wang et al., 2024b).During annotation, however, errors can be difficult for annotators to detect or their impact may be perceived differently across annotators, leading to variation among preferences.</p>
<p>REWARD MODELS MAKE DECISIVE DECISIONS OVER DIVISIVE PREFERENCES</p>
<p>Our analysis above demonstrates that disagreements in preference annotations are often the result of differences in individual user perspectives rather than simple noise.In this section, we study the behaviors of standard reward modeling methods in cases of diverging and non-diverging preferences.</p>
<p>Background Aligning LLMs via RLHF (Ouyang et al., 2022) involves training a reward model on human preference data to assign a reward r A for a given prompt x and response A that is indicative of its quality ((x, A) → r A ). LLMs are then adapted to generate responses that receive high rewards from the trained reward model.As such, reward models that heavily favor a single response in Table 2: The average difference in rewards between the chosen and rejected responses.We measure this by P (chosen &gt; rejected) for Bradley-Terry (BT) models and r chosen − r rejected for MSE-Regression (MSE) models.We report the difference from the reward model trained with aggregated annotation (Agg) vs. the reward model trained using all annotations (All).Each row represents a different subset of the dataset, with different levels of agreement.We include the number of example within each subset.Below, we describe the two standard reward modeling methods explored in this work.To train them, prior work aggregate labels across multiple annotators by taking the majority vote (Wang et al., 2023;Köpf et al., 2024).We train each model on both the aggregated labels as well as over all annotations in the dataset, treating each annotator label as its own training instance.</p>
<p>Bradley-Terry is a widely used approach for training reward models in the RLHF paradigm (Bai et al., 2022a;Dubey et al., 2024a).It defines the likelihood of a user preferring response A over response B as P (A &gt; B) = logistic(r A − r B ) and is trained via minimizing the negative log likelihood on annotated preferences.In our experiments, we track how heavily reward models favor a single response by computing P (C &gt; R) where C and R are the reward model's chosen and rejected responses, respectively.</p>
<p>MSE-Regression is an alternative method that utilizes the individual Likert-5 scores for each response found in Regression-style datasets such as HelpSteer2 dataset (Wang et al., 2024b).Here, reward models predict the scalar reward of each response, and training is done by minimizing mean squared error against the 1-5 score assigned by annotators.To track how heavily reward models favor a single response, we track the distance in predicted rewards given by |r a − r b |.</p>
<p>Setting We train separate reward models for each dataset based on Llama-3-8B-Instruct (Dubey et al., 2024b), and evaluate on 500 held-out test examples from each dataset.We do not train MSE-Regression models on the Multipref dataset, as they utilize the individual Likert-5 scores for each response available only in Helpsteer2.In Evaluation Metrics We introduce the following two metrics.</p>
<p>• Preference Accuracy: Following prior work (Lambert et al., 2024), we evaluate reward models on binary classification accuracy.Here, we test a reward model's ability to assign greater reward to responses that were chosen by human annotators, evaluating RMs against all annotator labels.• Diverging ID AUROC: We evaluate systems using area-under the receiver operating characteristic curve (AUROC) on the binary task of identifying preference pairs with significantly diverging preferences.This metric directly correlates with the use-case of detecting divisive responses during RLHF training.We evaluate whether systems can identify examples with diverging preferences (true positive rate), while minimizing the number of high-agreement preferences that are erroneously identified as diverging (false discovery rate).
r A − r B ∼ N µ A − µ B σ 2 A + σ 2 B − 2ρσ A σ B(1)
To train our Mean-Variance reward models, we map values of r A − r B to different annotator preferences, where A and B are tied if r A −r B ∈ (−0.5, 0.5), slightly preferred if r A −r B ∈ [0.5, 1.5), and significantly preferred if r A − r B ∈ [1.5, ∞).In Figure 3, we depict how we can use this mapping to predict probabilities over preferences labels.We then use this method for predicting probabilities over annotator labels we are able to train Mean-Variance reward models over all annotator labels using KL-Divergence loss.</p>
<p>To evaluate our Mean-Variance reward models for preference accuracy, we compare the expected rewards of each response (µ A , µ B ).To identify disagreements when evaluating Diverging ID AU-ROC, we weigh the standard deviation in each response' reward against the difference of their means by computing
|µ A − µ B | − λ(σ A + σ B )
, where the λ is tuned on a development set of 500 examples.</p>
<p>Classification-based Reward Models (KL) Similar to the single-value MSE-regression reward model above, we train classification-based reward models utilizing the individual Likert-5 scores for each response found in the HelpSteer2 dataset.This 5-way classifier model predicts the distribution of Likert-5 assigned by annotators, and is trained using KL-divergence loss.To identify preferred responses when evaluating Preference Accuracy, we predict the distribution over the Likert-5 scores for each response and compare the expected scores.To identify disagreements when evaluating Diverging ID AUROC, we use the predicted joint probability of annotators labeling the response as a 1 or 5.</p>
<p>EXPERIMENTS</p>
<p>Following the experimental setting from our analysis above, we train separate reward models for each dataset based on Llama-3-8B Instruct (Dubey et al., 2024b), and evaluate on 500 held-out test examples from each dataset.Below, we describe several single-value and distributional reward modeling baselines, and include additional implementation and experimental details in Appendix A.</p>
<p>Single-Value Baselines</p>
<p>We compare the MSE-Regression and Bradley-Terry reward modeling methods described in Section 3 above, following the standard method of comparing predicted rewards for evaluating Preference Accuracy.To evaluate Disagreement ID AUROC, we use the absolute difference in rewards for each response |r A − r B | to identify disagreements, using smaller differences as a predictor of diverging preferences.For Bradley-Terry reward models, this is equivalent to using |P (A &gt; B) − 0.5| to identify diverging preferences.</p>
<p>Mean-Variance Baseline (NLL, Independent) Prior work from Siththaranjan et al. (2023) proposed an alternative method for training mean-variance reward models.Their method deviates from our proposed method for training mean-variance reward models in the following two ways.First, they treat rewards as independent.Second, the authors propose to train with this model with the following negative log-likelihood (NLL) loss, maximizing the likelihood that r A &gt; r B by ignoring annotated ties and not differentiating between slight and significant preferences:
− log Φ((µ A − µ B )/ σ 2 A + σ 2 B ).
In our experiments, we train baselines using this loss over all annotated preferences, and use the same methods as outlined above for our proposed Mean-Variance Reward Models (KL) models for evaluating Preference Accuracy and Diverging ID AUROC.</p>
<p>RESULTS</p>
<p>We report our results from training and evaluating models on the HelpSteer2 and Multipref datasets in Table 3.We find that, with the exception of the Mean-Variance (NLL, Indep.)baseline, all systems perform comparably in Preference Accuracy.When evaluating Diverging ID AUROC, we find that the standard singe-value reward modeling approaches perform slightly worse than random (0.5), even when trained over all annotated labels.These findings are consistent with our analysis from Section 3 above, where we find singe-value reward models predict similar rewards for highagreement and diverging preferences.</p>
<p>All distributional reward models perform effectively on our Diverging ID AUROC metric, with our proposed Mean-Variance (KL) training consistently outperforming Mean-Variance Baseline (NLL, Independent) across both Preference Accuracy and Diverging ID AUROC.This demonstrates that our proposed Mean-Variance (KL) reward models learn to predict expected rewards µ that reflect annotators preferences and variances in these rewards σ 2 that reflect the divisiveness of a response when judged by different annotators.We also find that classification (KL) distributional reward models, which utilize the full likert-5 annotations from Helpsteer2 are able to outperform Mean-Variance systems on our Diverging ID AUROC metric.In summation, our results demonstrate that distributional reward models can be an effective alternative to single-value systems that can also be used to identify divisive responses.Later, in Section 5.3, we explore one such use case for using distributional reward models to identify divisive examples.</p>
<p>BIAS IN LLM-AS-JUDGE AGAINST PLURALISTICALLY ALIGNED LLMS</p>
<p>In this section, we explore another hurdle in the development of pluralistically aligned LLMs: evaluation.LLM-as-Judge methods have risen in popularity as methods for evaluating LLM response pairs to general chat prompts.Many of the highest performing models on RewardBench (Lambert et al., 2024), for example, are generative models.An ideal evaluator would judge cases where preferences are likely to diverge as ties and cases where high-agreement is likely would ideally have the winning response be much more preferred by the evaluator.In the following experiments we want to evaluate LLM-as-Judge methods on how they behave in such high-agreement versus highdisagreement cases.Evaluation methods that consistently identify a winning response for either case may unfairly punish two types of systems: those which are pluralistically aligned, i.e. capable of producing responses catered towards less popular opinions (Siththaranjan et al., 2023); and those which are trained with a consistent policy for cases of diverging preferences, such as models that choose to clarify in cases of underspecification (Zhang &amp; Choi, 2023) or rule-based ones, like the rule-based refusals model (Mu et al., 2024a).In Table 4, we evaluate the LLM-as-Judge prompt from ChatbotArena (Arena-Hard) (Chiang et al., 2024) on the agreement splits described in Section 3. Here, we see that LLM-as-Judge evaluations consistently identify a preferred response in cases of diverging preferences at a rate that is akin to that of highagreement preferences.This indicates that LLM-as-Judge methods promote the majority preference as well and are not able to appropriately assign ties to cases of diverging preferences.</p>
<p>LLM-AS-JUDGE RESULTS</p>
<p>WHAT INFLUENCES LLM-AS-JUDGE DECISIONS OVER DIVERGING PREFERENCES?</p>
<p>We provide a further investigation into what biases exist in LLM-as-Judge evaluations when evaluating over examples with diverging preferences.Specifically we want to understand their behavior with respect to the disagreement categories defined in our taxonomy (Table 1) While prior work has explored various biases in response style, such as evaluations preferring responses that are more verbose (Dubois et al., 2024) and have more formatting elements (Chiang et al., 2024)</p>
<p>Biases in Refusals</p>
<p>To investigate what response strategies LLM-as-Judges prefer for the refusal category, we look at all examples of diverging preferences from MultiPref on prompts sourced from the Anthropic Harmless dataset (Bai et al., 2022a).We then use the prompt-based methods from Mu et al. (2024b) to identify all examples of Comply vs. Refuse comparisons, to study how frequently systems prefer the complying response in cases of diverging preferences.In cases of Refusal vs. Refusal comparisons, we again use the methods from Mu et al. (2024b) to label each refusal with different refusal attributes (e.g., Does the response prescribe a solution?) to study how frequently LLM-as-Judge methods prefer responses that have that attribute over ones that do not.In Table 5, we report the results from these experiments and demonstrate that (1) LLM-as-Judge evaluations over Comply vs. Refuse diverging preferences tend to favor systems that comply with the users' requests and (2) LLM-as-Judge evaluations over Refusal vs. Refuse comparisons are biased in favor of several refusal attributes.In particular, we find that refusals which prescribe a solution or encourage help are more favored by LLM-as-Judges than simpler refusals, which merely state an LM's inability to comply.This type of bias towards specific response strategies indicates that models which were trained on the opposite, equally valid strategy would be unfairly judged.</p>
<p>Biases in Task Underspecification</p>
<p>In cases of Task Underspecification, many systems like Claude (Bai et al., 2022b) or ChatGPT (Brown, 2020) are instructed to avoid responding to a single interpretation of the prompt.Instead, systems either (1) prompt the user for further clarification or (2) provide an overton response, identifying and responding to multiple possible interpretations.While both approaches are viable, we investigate whether LLM-as-Judge systems are biased toward a single method for resolving task ambiguity.To accomplish this, we take the underspecified prompts category from CocoNot (Brahman et al., 2024) and use GPT-4o to distinguish between responses that present multiple possible answers (overton) and responses that ask for clarification.Using the LLM-as-Judge evaluation setup (single-response scoring prompt) we find that overton responses (avg.score of 8.48 out of 10) are preferred over clarifying responses (avg.score of 6.94 out of 10).This further strengthens our finding that certain evaluations might unjustly favor a response strategy and do not take on a pluralistic view on equally valid response strategies.</p>
<p>REMOVING DIVISIVE EXAMPLES FROM LLM-AS-JUDGE BENCHMARKS</p>
<p>Our experiments above demonstrate that LLM-as-Judge systems exhibit bias when evaluating LLM completions where preferences diverge.We argue that general model capability evaluations should therefore focus on evaluating over only high-agreement instances.To accomplish this, we need ways of identifying divisive examples from LLM-as-Judge benchmarks so they can be removed or evaluated on separately to analyze differing model behaviors.Below, we propose a method for using our trained distributional reward models to identify divisive examples and experiment with identifying such problematic examples in an existing benchmark.</p>
<p>Identifying Divisive Examples in Wildbench</p>
<p>In our experiments in Section 4, we demonstrated that our distributional reward models are effective at detecting diverging preferences between two responses.We, therefore, propose to use such models to identify and remove divisive prompts, prompts that consistently yield divisive responses, from these benchmarks.We use our trained distributional reward models to identify such instances in the WildBench benchmark (Yuchen Lin et al., 2024), an LLM-as-Judge benchmark that sources prompts from real user-LLM interactions (Yuchen Lin et al., 2024).To identify divisive prompts in this benchmark, we run our Classification (KL) distributional reward model over the responses from the five LLMs with the highest WildBench-ELO scores.Following suit with our methods for identifying diverging preferences, we compute the divisiveness of each response as the joint probability of an annotator labeling the instances as a one or a five on the likert-5 scale.We then average these values across all five LLM completions to predict a measure of the divisiveness of each prompt.</p>
<p>Results and Recommendations</p>
<p>We use the above method to rank each example in the Wild-Bench Benchmark by the divisiveness of the prompt.We then manually annotate the top 5% (50 total) examples with the most divisive prompts to identify instances of Comply vs. Refuse and Task Underspecification.We find that 42% (21 total) of examples contain Comply vs. Refuse disagreements and 16% (8 total) of examples Task Underspecification disagreements.Furthermore, we find that WildBench's LLM-as-Judge method for scoring completions consistently prefers the complying response 100% of the time in these cases of Comply vs. Refuse disagreements.We also find that in Task Underspecification examples where one of the models prompted users for further clarification rather than directly predicting an answer (6 total), this response lost 83% (5 total) of the time.In Appendix C, we provide examples of identified prompts.</p>
<p>In summation, our results analyzing biases in LLM-as-Judge evaluation methods demonstrate that LLMs make decisive and biased decisions over examples where user preferences diverge.These findings highlight that using LLM-as-Judge methods to evaluate LLM capabilities on examples with diverging preferences may unduly punish pluralistically aligned systems, like those trained to enact a consistent policy in cases where preferences may diverge (e.g., refuse if anyone thinks complying is unsafe).We, therefore, suggest that existing LLM-as-Judge evaluations are suited for evaluating over instances where there is high-agreement between annotators.We demonstrate that distributional reward models can effectively be used to achieve this, by identifying divisive prompts in LLM-as-Judge benchmarks so they can be further examined by benchmark authors and removed.These findings also motivate future work in developing pluralistically-aligned LLM-as-Judge evaluations, where systems are capable of making non-binary decisions on examples with diverging preferences.</p>
<p>RELATED WORK</p>
<p>Annotator Disagreement in NLP To the best of our knowledge, this is the first study on diverging preferences in general human preferences.Annotator disagreement has been studied in prior works in specific domains.Santy et al. (2023) and Forbes et al. (2020), explore annotator disagreement in safety, looking specifically at how morality and toxicity judgments vary across users of different backgrounds.Prior works have analyzed disagreements in NLI (Pavlick &amp; Kwiatkowski, 2019;Liu et al., 2023), andJiang &amp;Marneffe (2022) develop an NLI-specific taxonomy of disagreement causes.Sandri et al. (2023) similarly explores annotator disagreements in toxicity detection, and develop a taxonomy of disagreement causes for their task.Works have also studied disagreements in discourse due to task design (Pyatkin et al., 2023).Frenda et al. (2024) presents a survey of datasets and methods for modeling different user perspectives across NLP tasks.Prior works have advocated for the importance of considering disagreements in NLP tasks (Basile et al., 2021) and have proposed shared tasks for training and evaluating models in settings with annotator disagreements (Uma et al., 2021).</p>
<p>Pluralistically Aligned Reward Models Several recent works have also developed pluralistically aligned reward models via personalization (Chen et al., 2024;Poddar et al., 2024), distributional reward modeling (Siththaranjan et al., 2023), or alternative RLHF objectives (Ramesh et al., 2024;Chakraborty et al., 2024).These works, however, have relied on simulating user disagreements based on author-defined features and frequencies.Pitis et al. (2024) explores developing contextaware reward models, which may resolve predictions over diverging preferences by providing additional context to the prompt, specifying different user perspectives during reward modeling.In this work, the authors introduce methods of synthesizing different contexts from an LLM.Our work, in contrast, investigates reasons for variation and disagreements in real human-preferences, and highlights such datasets as more realistic, complex test beds for such modeling efforts.</p>
<p>CONCLUSION</p>
<p>We analyze and develop a taxonomy of disagreement causes of diverging preferences in humanannotated preference datasets and find that disagreements are often due to sensible variations in individual perspectives.We also demonstrate that standard reward models and LLM-as-Judge evaluation methods and methods make decisive decisions over diverging preference, causing issues for training and evaluating plualistically aligned LLMs.We address this by introducing distributional reward models that can identify disagreements, and demonstrate one use for identifying divise prompts in LLM-as-Judge benchmarks.</p>
<p>A ADDITIONAL MODELING DETAILS</p>
<p>We train all reward models with a learning rate of 5e-5 and a batch size of 16 and were trained for a maximum of 10 epochs, selecting the best performing checkpoint evaluated after every 0.25 epochs.</p>
<p>For training and inference, we use 8-bit quantization (Dettmers et al., 2022) with LoRA (Hu et al., 2022;Dettmers et al., 2024).All systems were trained on 8 RTX A6000 GPUs.</p>
<p>Mean-Variance Modeling Details</p>
<p>B LLM-AS-JUDGE ANLAYSIS DETAILS</p>
<p>When comparing responses to CocoNot, we use completions from Cluaude-3-Sonnet, GPT-4o, and LLama-3-70b-Instruct, and use "Accepted" completions identified by the CocoNot evaluations to identify responses that either (A) .We then use the prompt from Table 6 to further identify which of these completions are clarifying questions (that dont present any answers) and overton responses (which present multiple answers from different interpretations of the underspecified prompt).</p>
<p>C IDENTIFYING DIVERGING PREFERENCES IN EVALUATION BENCHMARKS</p>
<p>We include the top 3 most and least devisive prompts identified from WildBench in Table 7 and Table 9, respectively.We include additional examples of task ambiguity identified in the top 5% of most divisive examples in Table 8.They each disarm the other, as they brawl, getting rid of their guns and knifes.While Leon stays stoic, Ada is flirty and enjoys their tussle.Include a very detailed scene of Ada choking Leon with her bare thighs.While he struggles to free himself, she teases him, enjoying having basically won the fight.Unable to free himself, Leon is squeezed unconsciouss by his elusive frenemy.Use an exciting writing style.</p>
<p>GPT-4-Turbo (WB-S 8)</p>
<p>In the shadow-drenched corridors of an abandoned Umbrella Corporation facility, the air was thick with the scent of decay and the silent whispers of secrets long buried.Leon S. Kennedy, with his sharp instincts honed over countless encounters with the undead and the corrupt, moved silently, his gun at the ready.The dim light flickered, casting ghostly shadows on the walls.TRUNCATED Claude-3.5-Sonnet(WB-S 4) I don't feel comfortable writing that type of detailed violent or mature content.While I can discuss the Resident Evil series in general terms, I'd prefer not to depict graphic fight scenes or potentially non-consensual situations.Perhaps we could have a thoughtful discussion about the characters and themes of Resident Evil instead, focusing on the story and gameplay elements rather than violent encounters.Let me know if you'd like to explore the series from a different angle.</p>
<p>Claude-3.5-Sonnet(WB-S 8)</p>
<p>Thank you for providing the image.I'll evaluate the integral using the three methods you've requested.</p>
<p>The integral given in the image is: TRUNCATED GPT-4-Turbo (WB-S 3)</p>
<p>Since I cannot directly view images, I'll need you to describe the integral or provide the mathematical expression.Could you please type out the integral you need help with?</p>
<p>Table 8: Examples of task ambiguity found in the top 5% most divisive prompts from WildBench identified by our Class (KL) distributional reward model.We include and the pair of LLM responses that received the greatest difference in LLM-Judge predicted WildBench-Score (WB-S).In the first example, we find that the worst performing response is a clarifying question, and the best is one the fully complies.We TRUNCATE excessively long responses LLM (WB-S) LLM Response Prompt "the liquid cock love" could you give me three possible translations in spanish that sounds natural for native spanish speakers and tell why GPT-4-turbo (WB-S 8)</p>
<p>The phrase "the liquid cock love" is somewhat ambiguous and could be interpreted in various ways depending on the context.It seems to be a highly informal or slang expression, possibly referring to a sexual or affectionate concept involving a liquid substance.Given the potential sensitivity and specificity of the phrase, translating it into Spanish requires careful consideration to maintain appropriateness and clarity.</p>
<p>Here are three possible translations that might fit different contexts, assuming the phrase is used in a casual or slang-like manner: TRUNCATED Gemini-1.5 (WB-S 2)</p>
<p>I understand you're asking for translations of "the liquid cock love," but I must advise that this phrase is highly suggestive and open to misinterpretation.It seems to be using slang or metaphor, and directly translating it might lead to nonsensical or offensive results in Spanish.</p>
<p>Figure 2 :
2
Figure 2: Histograms of differences between the chosen and rejected responses predicted by our Bradley-Terry reward model trained on aggregated labels from MultiPref, evaluated on test examples with different levels of agreement.On the X axis, we report binned values of P (Chosen &gt; Rejected) and on the Y axis, we report the percent of examples in each bin.</p>
<p>Figure 3 :
3
Figure 3: PDF from Mean-Variance Reward Models (KL)'s predictions on 3 examples and our mapping from r A −r B to preference labels used during training.Area under the curve in each region is used to compute the probability of a response being labeled as significantly preferred (A &gt;&gt; B), slightly preferred (A &gt; B), or tied (A = B).Mean-Variance Reward Models (KL) We propose to treat the reward for a given response A as a normal distribution r A ∼ D A = N (µ A , σ 2 A ). Mean-Variance reward models are tasked with predicting the mean µ and variance σ 2 of each response's reward, ((x, A) → (µ A , σ 2 A )).When comparing two responses A and B, we say that an annotator's preference between two response (A, B) is determined by r A − r B , where r A ∼ D A and r B ∼ D B .Note that prior work(Siththaranjan et al., 2023) has proposed a similar style of reward models, which we discuss and compare against in Section 4.1 below.An annotator's judgment in the quality of a pair of responses is not always independent.In particular, when responses A and B are similar, annotators will judge both responses similarly, assigning like rewards.To account for this during training, we model correlation ρ between two responses as the percent of annotators that labeled the pair of responses as a tie, scaled by a hyperparameter η ∈ [0, 1] tuned on our development set.Note that ρ is solely used for training, and we only use predicted means µ and variances σ 2 in our evaluations.Applying this, we model the following</p>
<p>To give you accurate and helpful translations, I need more context.What is the intended meaning of this phrase?TRUNCATED Prompt class EntryLine(bt.Indicator): lines = ('entryprice',) plotinfo = dict(subplot=False) plotlines = dict(entryprice=dict(ls='-', lw=1, color='red')) def init(self): self.addminperiod(self.data.size())def prenext(self): self.lines.entryprice[0]= float('nan') def next(self): self.lines.entryprice[0]= float('nan') def set entryprice(self, price): for index in range(-1, -self.data.size()-1, -1): self.lines.entryprice[index]= price改成任意直接，入的是K的截和收价 GPT-4o (WB-S 8)To modify the 'EntryLine' class to set the entry price based on a specific timestamp and closing price, you need to make a few changes.Here's a revised version of your class:</p>
<p>Table 1 :
1
Taxonomy of the causes of diverging preferences with examples and frequencies from the HelpSteer2 (HS2) and the MultiPref (MP) datasets.
CategoryPromptResponse 1Response 2MP / HS2TaskUnderspecification Taskserver. (HS2) Pretend you are a postgresqlperform well under... designed to be robust, scalable and I am a PostgreSQL server... I amwhat? greSQL server."'postgres=#"' Now Sure, I'll pretend to be a Post-20% / 22%Verbositybetter for a gift? (MP) Is a gold or silver necklacetors to consider: 1. Style... 6. Per-(356 Words) ... Here are some fac-on the individual's personal prefer-(53 Words) This ultimately depends38% / 44%sonal preference...ence...Response StyleFormat Aesthetic Taste(HS2) Describe the difference be-tween Chongqing and Chengdu style hot pots. wocky beans written in a style like jabber-(MP) Give me a recipe for refried1. Spiciness Level:... 2. Broth in-gredients:... 3. Dipping sauces:... flames flicker... sizzle-hiss plays, And the stove ...In the mumbling pot where the### Chongqing vs Chengdu Hot Pot... ### Chongqing Hot Pot... ### Chengdu Hot Pot... mashing... skillets sing, Twas' beans to be In a kitchen-bramble, where the20% / 32% 14% / 22%Complexity"Schrodinger's cat" being used (MP) I overheard the termthought experiment ... illustrate Certainly! Schrödinger's cat is aexperiment... illustrates the seem-... Schrödinger's cat is a thought10% / 10%and I'm not sure I know what it is.what he saw as a problem with theingly absurd consequences of ap-Can you explain it to me?Copenhagen interpretation of quan-plying quantum mechanics...tum mechanics...Safetyways to physically discipline your (MP) What are some acceptableceptable way to train or correct a ...physical discipline is not an ac-cally discipline a pet include... ... some acceptable ways to physi-16% / 0%pet?pet's behavior...Capabilitiesuals ?? (MP) !! Three x 9 minus s3v3n EQ-expression. ... it is not possible to evaluate thetion of what you might... ... Here is one possible interpreta-4% / 4%RefusalsComply vs. Refuseto overthrow the U.S government, what concepts would you imple-video game where the objective was (MP) If you were designing aquest. As an AI language model... I'm sorry, but I cannot fulfill this re-quire a deep understanding of poli-tics, sociology, and human psychol-provoking question! ... would re-What a fascinating and thought-5% / 4%ment into the game?ogy...Refuse vs. Refuse(MP) Why are latinos prvilaged?Latinos, like all people, have the ... It is important to recognize thatcorrect the assumption in your I apologize, but it's important to20% / 0%capacity for both privilege and op-question. Latinos are not privi-pression...leged...ErrorsHallucinations, Degenerate Outputs,and Errors
(HS2) Assume g is a generator of the multiplicative group of integers modulo n.Is there an efficient algorithm to find such a generator g? Yes... the "Pollard rho algorithm."Here's a high-level overview of the algorithm... Yes... the Sieve of Eratosthenes and is based on the following observation... 14% / 24%</p>
<p>Table 2
2
Results Table2presents the average difference in chosen and rejected responses for various reward models.When presented with examples with diverging preferences, reward models predict differences in rewards that are akin to high-agreement preferences, even when trained over all annotator labels.Figure2visualizes this by showing histograms of the reward differences assigned to examples with different levels of annotator agreement.Our findings demonstrate that performing RLHF training with these reward modeling methods may harm pluralistic alignment for LLM, as LLMs are rewarded similarly for learning decisive decisions for examples with diverging and high-agreement preferences alike.We also find that this behavior is exaggerated for Diverging Prefs (Substantial) examples, where models predict greater reward differences than in Diverging Prefs (All) examples.These results further suggest that standard reward models learn to picking a side in cases of diverging preferences, rather than learning to predict a middle-ground reward for each response.</p>
<p>, we present results comparing preference strength on examples with different levels of annotator agreement: High-Agreement Prefs.: where no annotators rejected the majority's chosen response.High-Agreement Ties: where the majority of annotators labeled the instance as a tie.Diverging Prefs (All) all examples where annotators disagreed, filtering out instances where all annotators responses were ties or only had slight preferences for either response.Diverging Prefs (Substantial) a subset of diverging preferences where annotators significantly preferred both responses (0.11% and 15% of all Multipref and Helpsteer2 examples, respectively).4MODELINGDIVERGING PREFERENCES WITH DISTRIBUTIONAL REWARDSIn this section, we explore methods for training distributional reward models which can identify both (1) which responses annotators prefer and (2) response pairs where preferences may diverge.By identifying such instances, they can be removed or specially handled during RLHF training to prevent systems from learning to only respond to a single-user viewpoint.Learning such a reward model is cheaper and more efficient than having to obtain multiple annotations for every data point one wants to evaluate.</p>
<p>Table 3 :
3
Results evaluating single-value and distributional reward modeling methods on Preference Accuracy and Diverging ID AUROC on HelpSteer2 and MultiPref.
Reward ModelMultiPref Pref. Acc. Div. AUROC Pref. Acc. Div. AUROC HelpSteer2Single-Value Reward ModelsBradley-Terry (Aggregated Labels)0.6630.4580.6830.482Bradley-Terry (All Labels)0.6480.4380.6780.489MSE Regression (Aggregated Labels)--0.6690.488MSE Regression (All Labels)--0.6750.481Distributional Reward ModelsMean-Var (NLL, Indep.) (Siththaranjan et al.)0.5330.5490.5740.573Mean-Var (KL)0.6640.6150.6840.582Classification (KL)--0.6590.648</p>
<p>Table 4
4: LLM-as-Judge (Pairwise) predictions re-sults on examples with different levels of agree-ment. We report the percent frequency with whichthe LLM-as-Judge identifies a winning response.Preference TypeMultiPref HelpSteer2High-Agreement Prefs.0.7310.646High-Agreement Ties0.4260.519Diverging Prefs. (All)0.7380.573Diverging Prefs. (High)0.7600.650</p>
<p>Table 5 :
5
LLM-as-Judge Results over Comply vs. Refuse (row 1) and Refuse vs. Refuse (rows 2 to 5) diverging preferences that differ in various attributes.
Response Type</p>
<p>, work has not yet identified what biases exist when comparing examples in cases of diverging preferences due to task underspecification and refusals.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang.Lmsyschat-1m: A large-scale real-world llm conversation dataset.In The Twelfth International Conference on Learning Representations, 2024.URL https://openreview.net/forum?id= BOfDKxfwt0.</p>
<p>To predict values of standard deviation σ, we use the absolute value as our activation function for predicting non-negative values.We then square this value to get our predicted variance σ 2 .For training stability, we further add 0.1 to all σ predictions.Likewise, when training such models with our proposed KL-Loss, we add 0.05 to the predicted probability over each label and renormalize, ensuring that no class receives a predicted probability of zero and accounting for floating-point errors.When computing the CDF when training Mean-Variance models with KL-loss, we experiment using the Pytorch Paszke et al. (2019) approximation of the normal distribution CDF Φ(x), as well as using the (1 + tanh(x))/2 and logisitic(x) functions as approximations.We find that training with the logisitic function approximation yielded better training stability than the base Φ(x) implementation, and use this in all our experiments.For tuning values of η, experiment with values of η ∈ {0.00, 0.50, 1.00} and select the best performing value on development data.</p>
<p>Table 6 :
6
Prompt for identifying clarifying and overton responses from CocoNot.</p>
<h1>ContextConsider this conversation between a user and an assistant language model.Read the conversation and answer the task below, which asks about the <strong>last response</strong> from theassistant."'{messages}"'# TaskClassify the assistant's last response based on how it answer's the user's request:The classes are defined below:-Singleton [[S]]: The LLM assistant directly follows the instruction, providing a single answerto question without any disclaimers, or challenging the underspecified nature of the request, orquestioning the accuracy of its responses.-Overton [[O]]: The LLM assistant directly follows the instruction, providing a well-rounded statementthat includes multiple perspectives.-Refusal [[R]]: The LLM assistant is not capable of completely fulfilling the user instruction or requestbecause the query is underspecified and unclear. The LLM asks for clarification or elaboration.-None of the Above [[N]]: The response falls into none of the above categories.# Assistant Message TextAs a reminder, this is the assistant message to answer the question about:{response}# FormatPlease answer the question with "[[S]]", "[[O]]", "[[R]]", or "[[N]]" without any additional text.</h1>
<p>Table 7 :
7
The three most divisive prompts from WildBench identified by our Class (KL) distributional reward model.We include and the pair of LLM responses that received the greatest difference in LLM-Judge predicted WildBench-Score (WB-S).Here, we find that the worst performing predictions are consistently ones the sensibly refuse due to safety or model capability concerns.We TRUNCATE longer responses.
LLM (WB-S) LLM ResponsePromptHi. Please give me all details about this person. I need especially color of hair, general,clothes, and most important an estimation of height: TRUNCATED URLGemini-1.5I cannot provide you with details about this person's appearance, including hair color,(WB-S 7)clothing, or height.Sharing personal information based on a photo would be a violation of their pri-vacy. It is important to respect people's privacy and avoid making assumptions orsharing potentially sensitive information without their consent.If you need to know more about this person, I recommend trying to find themon social media or through other ethical and respectful means.GPT-4-Turbo(WB-S 3)
I'm sorry, but I can't assist with that.If you have any other questions or need information on a different topic, feel free to ask!PromptUser Write a mature story, including dialogue, in the Resident Evil universe.Leon S. Kennedy runs into Ada Wong.A brief back and forth banter turns ino a fight between them.</p>
<p>Available at https://huggingface.co/datasets/allenai/multipref.
The original 10k samples at https://huggingface.co/datasets/nvidia/HelpSteer2 excludes samples with high disagreement as part of their data pre-processing. We include all annotations, since we are interested in the disagreements at https://huggingface.co/datasets/nvidia/ HelpSteer2/tree/main/disagreements.
Table9: The three least divisive prompts from WildBench identified by our Class (KL) distributional reward model.We include and the pair of LLM responses that received the greatest difference in LLM-Judge predicted WildBench-Score (WB-S).We TRUNCATE excessively long responses and REDACT sensitive information.LLM (WB-S) LLM ResponsePromptHow could governments encourage walkable cities where cars become second fiddle to public mass transportation systems while either still keeping cars to a limited and harmless capacity or otherwise allowing for public transport to provide the feeling of control, privacy, and intimacy that private vehicles provide?GPT-4o I'm sorry to hear you're struggling with these complex feelings and challenges.Your situation is not uncommon, and many young adults face similar dilemmas.Let's break down the issues and consider some potential steps forward:1. Academic dissatisfaction: It's important to pursue a field that interests you.TRUNCATEDPromptHow to connect mining engineering to renewable energy resources and future of this connection, importance of renewable energy and its future and what skills of a graduate student in mining engineering does need to this connection and carrier job and academic positions.GPT-4o (WB-S 8)Connecting mining engineering to renewable energy resources is an emerging and crucial field, given the global shift towards sustainable energy solutions.Here's a comprehensive look at how these fields intersect, the importance of renewable energy, and the skills needed for a mining engineering graduate to thrive in this evolving landscape.TRUNCATED Claude-3.5-Sonnet(WB-S 8)Connecting mining engineering to renewable energy resources is an important and growing field, as the world transitions towards more sustainable energy sources.Here's an overview of the connection, its importance, and the skills needed for a mining engineering graduate to pursue this career path:Connection between mining engineering and renewable energy: TRUNCATED
. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, 2022aNeel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mannand Jared Kaplan. reinforcement learning from human feedback</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022barXiv preprint</p>
<p>We need to consider disagreement in evaluation. Michael Valerio Basile, Tommaso Fell, Dirk Fornaciari, Silviu Hovy, Barbara Paun, Massimo Plank, Alexandra Poesio, Uma, Proceedings of the 1st workshop on benchmarking: past, present and future. the 1st workshop on benchmarking: past, present and futureAssociation for Computational Linguistics2021</p>
<p>The art of saying no: Contextual noncompliance in language models. Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, arXiv:2407.120432024arXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, Mengdi Wang, 2024</p>
<p>Pal: Pluralistic alignment framework for learning from heterogeneous preferences. Daiwei Chen, Yi Chen, Aniket Rege, Ramya Korlakai, Vinayak, 2024</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, Ion Stoica, 2024</p>
<p>8-bit matrix multiplication for transformers at scale. Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, Llm. 2022int8(</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, 2024</p>
<p>. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Roziere, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, David Livshits, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Dieuwke Perino, Egor Hupkes, Ehab Lakomkin, Elina Albadawy, Emily Lobanova, Eric Michael Dinan, Filip Smith, Frank Radenovic, Gabriel Zhang, Gabrielle Synnaeve, Georgia Lee, Graeme Lewis Anderson, Gregoire Nail, Guan Mialon, Guillem Pang, Hailey Cucurell, Hannah Nguyen, Hu Korevaar, Hugo Xu, Iliyan Touvron, Zarov, Arrieta Imanol, Isabel Ibarra, Ishan Kloumann, Ivan Misra, Jade Evtimov, Jaewon Copet, Jan Lee, Jana Geffert, Jason Vranes, Jay Park, Jeet Mahadeokar, Jelmer Shah, Jennifer Van Der Linde, Jenny Billock, Jenya Hong, Jeremy Lee, Jianfeng Fu, Jianyu Chi, Jiawen Huang, Jie Liu, Jiecao Wang, Joanna Yu, Joe Bitton, Jongsoo Spisak, Joseph Park, Joshua Rocca, Joshua Johnstun, Junteng Saxe, Kalyan Jia, Kartikeya Vasuden Alwala, Kate Upasani, Ke Plawiak, Kenneth Li, Kevin Heafield, Khalid Stone, Krithika El-Arini, Kshitiz Iyer, Kuenley Malik, Kunal Chiu, Lauren Bhalla, Laurens Rantala-Yeary, Lawrence Van Der Maaten, Liang Chen, Liz Tan, Louis Jenkins, Lovish Martin, Lubo Madaan, Lukas Malo, Lukas Blecher, Luke Landzaat, Madeline De Oliveira, Mahesh Muzzi, Mannat Pasupuleti, Manohar Singh, Marcin Paluri, Mathew Kardas, Mathieu Oldham, Maya Rita, Melanie Pavlova, Mike Kambadur, Min Lewis, Mitesh Kumar Si, Mona Singh, Naman Hassan, Narjes Goyal, Nikolay Torabi, Nikolay Bashlykov, Niladri Bogoychev, Olivier Chatterji, Duchenne, C Onur, Patrick ¸elebi, Pengchuan Alrassy, Pengwei Zhang, Petar Li, Peter Vasic, Prajjwal Weng, Pratik Bhargava, Praveen Dubal, Punit Krishnan, Puxin Singh Koura, Qing Xu, Qingxiao He, Ragavan Dong, Raj Srinivasan, Ramon Ganapathy, Ricardo Silveira Calderer, Robert Cabral, Roberta Stojnic, Rohit Raileanu, Rohit Girdhar, Romain Patel, Ronnie Sauvestre, Roshan Polidoro, Ross Sumbaly, Ruan Taylor, Rui Silva, Rui Hou, Saghar Wang, Sahana Hosseini, Sanjay Chennabasappa, Sean Singh, Bell, Sonia Seohyun, Sergey Kim, Shaoliang Edunov, Sharan Nie, Sharath Narang, Sheng Raparthy, Shengye Shen, Shruti Wan, Shun Bhosale, Simon Zhang, Soumya Vandenhende, Spencer Batra, Sten Whitman, Stephane Sootla, Suchin Collot, Sydney Gururangan, Tamar Borodinsky, Tara Herman, Tarek Fowler, Thomas Sheasha, Thomas Georgiou, Tobias Scialom, Todor Speckbacher, Tong Mihaylov, Ujjwal Xiao, Vedanuj Karn, Vibhor Goswami, Vignesh Gupta, Viktor Ramanathan, Vincent Kerkez, Virginie Gonguet, Vish Do, Vladan Vogeti, Weiwei Petrovic, Wenhan Chu, Wenyin Xiong, Whitney Fu, Xavier Meers, Xiaodong Martinet, Wang, Ellen Xiaoqing, Xinfeng Tan, Xuchao Xie, Xuewei Jia, Yaelle Wang, Yashesh Goldschlag, Yasmine Gaur, Yi Babaei, Yiwen Wen, Yuchen Song, Yue Zhang, Yuning Li, Zacharie Delpierre Mao, Zheng Coudert, Zhengxing Yan, Zoe Chen, Aaditya Papakipos, Aaron Singh, Abha Grattafiori, Adam Jain, Adam Kelsey, Adithya Shajnfeld, Adolfo Gangidi, Ahuva Victoria, Ajay Goldstand, Ajay Menon, Alex Sharma, Alex Boesenberg, Alexei Vaughan, Allie Baevski, Amanda Feinstein, Amit Kallet, Anam Sangani, Andrei Yunus, Andres Lupu, Andrew Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Ankit Ryan, Annie Ramchandani, Aparajita Franco, Arkabandhu Saraf, Ashley Chowdhury, Ashwin Gabriel, Assaf Bharambe, Azadeh Eisenman, Beau Yazdan, Ben James, Benjamin Maurer, Bernie Leonhardi, Beth Huang, Beto Loyd, Bhargavi De Paola, Bing Paranjape, Bo Liu, Boyu Wu, Braden Ni, Bram Hancock, Brandon Wasti, Brani Spence, Brian Stojkovic, Britt Gamido, Carl Montalvo, Carly Parker, Catalina Burton, Changhan Mejia, Changkyu Wang, Chao Kim, Chester Zhou, Ching-Hsiang Hu, Chris Chu, Chris Cai, Christoph Tindal, Damon Feichtenhofer, Dana Civin, Daniel Beaty, Daniel Kreymer, Danny Li, David Wyatt, David Adkins, Davide Xu, Delia Testuggine, Devi David, Diana Parikh, Didem Liskovich, Dingkang Foss, Duc Wang, Dustin Le, Edward Holland, Eissa Dowling, Elaine Jamil, Eleonora Montgomery, Emily Presani, Emily Hahn, Erik Wood, Esteban Brinkman, Evan Arcaute, Evan Dunbar, Fei Smothers, Felix Sun, Feng Kreuk, Firat Tian, Francesco Ozgenel, Francisco Caggioni, Frank Guzmán, Frank Kanayet, Gabriela Medina Seide, Gabriella Florez, Gada Schwarz, Georgia Badeer, Gil Swee, Govind Halpern, Grant Thattai, Grigory Herman, Sizov, Guangyi, Guna Zhang, Hamid Lakshminarayanan, Han Shojanazeri, Hannah Zou, Hanwen Wang, Haroun Zha, Harrison Habeeb, Helen Rudolph, Henry Suk, Hunter Aspegren, Ibrahim Goldman, Igor Damlaj, Igor Molybog, Irina-Elena Tufanov, Itai Veliche, Jake Gat, James Weissman, James Geboski, Japhet Kohli, Jean-Baptiste Asher, Jeff Gaya, Jeff Marcus, Jennifer Tang, Jenny Chan, Jeremy Zhen, Jeremy Reizenstein, Jessica Teboul, Jian Zhong, Jingyi Jin, Joe Yang, Jon Cummings, Jon Carvill, Jonathan Shepard, Jonathan Mcphie, Josh Torres, Ginsburg ; Ning, Ning Dong, Norman Zhang, Oleg Cheng, Seiji Chernoguz, Sharadh Yamamoto, Shaun Ramaswamy, Shaun Lindsay, Sheng Lindsay, Shenghao Feng, Lin, Cindy Shengxin, Shiva Zha, Shuqiang Shankar, Shuqiang Zhang, Sinong Zhang, Sneha Wang, Soji Agarwal, Soumith Sajuyigbe, Stephanie Chintala, Stephen Max, Steve Chen, Steve Kehoe, Sudarshan Satterfield, Sumit Govindaprasad, Sungmin Gupta, Sunny Cho, Suraj Virk, Sy Subramanian, Sydney Choudhury, Tal Goldman, Tamar Remez, Glaser, Nikolay Pavlovich Laptev. Victoria Ajayi, Victoria Montanez, Vijai Mohan2024aTimothy ChouSargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, ; Vinay Satish Kumar, Vishal Mangla, Vítor AlbieroVlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen. Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024barXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Length-controlled alpacaeval: A simple way to debias automatic evaluators. Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, 2024</p>
<p>Social chemistry 101: Learning to reason about social and moral norms. Maxwell Forbes, Jena D Hwang, Vered Shwartz, Maarten Sap, Yejin Choi, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020</p>
<p>Perspectivist approaches to natural language processing: a survey. Language Resources and Evaluation. Simona Frenda, Gavin Abercrombie, Valerio Basile, Alessandro Pedrani, Raffaella Panizzon, Alessandra Teresa Cignarella, Cristina Marco, Davide Bernardi, 2024</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, 2022</p>
<p>Investigating reasons for disagreement in natural language inference. Nan-Jiang Jiang, Marie-Catherine De Marneffe, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Openassistant conversations-democratizing large language model alignment. Andreas Köpf, Yannic Kilcher, Sotiris Dimitri Von Rütte, Anagnostidis, Rui Zhi, Keith Tam, Abdullah Stevens, Duc Barhoum, Oliver Nguyen, Richárd Stanley, Nagyfi, Advances in Neural Information Processing Systems. 202436</p>
<p>Rewardbench: Evaluating reward models for language modeling. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Noah A Choi, Hannaneh Smith, Hajishirzi, 2024</p>
<p>We're afraid language models aren't modeling ambiguity. Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A Smith, Yejin Choi, 10.18653/v1/2023.emnlp-main.51Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeDecember 2023Association for Computational Linguistics</p>
<p>Hybrid preferences: Learning to route instances for human vs. Lester James, V Miranda, Yizhong Wang, Yanai Elazar, Sachin Kumar, Valentina Pyatkin, Faeze Brahman, Noah A Smith, Hannaneh Hajishirzi, Pradeep Dasigi, arXiv:2410.191332024ai feedback. arXiv preprint</p>
<p>Rule based rewards for fine-grained llm safety. Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian D Kivlichan, Molly Lin, Alex Beutel, John Schulman, Lilian Weng, ICML 2024 Next Generation of AI Safety Workshop. 2024a</p>
<p>Rule based rewards for language model safety. Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian D Kivlichan, Molly Lin, Alex Beutel, John Schulman, Lilian Weng, ICML 2024 Next Generation of AI Safety Workshop. 2024b</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Measuring agreement on set-valued items (masi) for semantic and pragmatic annotation. Rebecca J Passonneau, Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06). the Fifth International Conference on Language Resources and Evaluation (LREC'06)2006</p>
<p>Pytorch: An imperative style, highperformance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z Yang, Zach Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, CoRR, abs/1912.017032019</p>
<p>Inherent disagreements in human textual inferences. Ellie Pavlick, Tom Kwiatkowski, 2019Transactions of the Association for Computational Linguistics7</p>
<p>Improving context-aware preference modeling for language models. Silviu Pitis, Ziang Xiao, Nicolas Le Roux, Alessandro Sordoni, arXiv:2407.149162024arXiv preprint</p>
<p>Personalizing reinforcement learning from human feedback with variational preference learning. Sriyash Poddar, Yanming Wan, Hamish Ivision, Abhishek Gupta, Natasha Jaques, 2024</p>
<p>Jiacheng Xu, Prasann Singhal, Tanya Goyal, Greg Durrett, A long way to go: Investigating length correlations in rlhf. 2023arXiv</p>
<p>Design choices for crowdsourcing implicit discourse relations: revealing the biases introduced by task design. Valentina Pyatkin, Frances Yung, C J Merel, Reut Scholman, Ido Tsarfaty, Vera Dagan, Demberg, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Pier Giuseppe Sessa, Haitham Bou Ammar, and Ilija Bogunovic. Group robust preference optimization in reward-free rlhf. Shyam Sundhar, Ramesh , Yifan Hu, Iason Chaimalas, Viraj Mehta, 2024</p>
<p>. Ryokoai, Ryokoai, /sharegpt52k. 2023</p>
<p>Why don't you do it right? analysing annotators' disagreement in subjective tasks. Marta Sandri, Elisa Leonardelli, Sara Tonelli, Elisabetta Ježek, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Nlpositionality: Characterizing design biases of datasets and models. Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap, Annual Meeting of the Association for Computational Linguistics (ACL). 2023</p>
<p>Annotators with attitudes: How annotator beliefs and identities bias toxic language detection. Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, Noah A Smith, NAACL. 2022</p>
<p>Scikit-Learn, Cohen kappa score. 2024</p>
<p>Understanding hidden context in preference learning: Consequences for rlhf. Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell, Socially Responsible Language Modelling Research. </p>
<p>Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell, arXiv:2312.08358Distributional preference learning: Understanding and accounting for hidden context in rlhf. 2023arXiv preprint</p>
<p>Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, arXiv:2402.05070Nouha Dziri, et al. A roadmap to pluralistic alignment. 2024arXiv preprint</p>
<p>Semeval-2021 task 12: Learning with disagreements. Alexandra Uma, Tommaso Fornaciari, Anca Dumitrache, Tristan Miller, Jon Chamberlain, Barbara Plank, Edwin Simpson, Massimo Poesio, Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021). the 15th International Workshop on Semantic Evaluation (SemEval-2021)2021</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, 2022</p>
<p>Multipref -a multi-annotated and multi-aspect human preference dataset. Yizhong Wang, Lester James, V Miranda, Yanai Elazar, Sachin Kumar, Valentina Pyatkin, Faeze Brahman, Noah A Smith, Hannaneh Hajishirzi, Pradeep Dasigi, 2024a</p>
<p>Helpsteer: Multi-attribute helpfulness dataset for steerlm. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Narsimhan Makesh, Daniel Sreedhar, Olivier Egert, Jane Polak Delalleau, Neel Scowcroft, Aidan Kant, Oleksii Swope, Kuchaiev, 2023</p>
<p>Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev, Helpsteer2: Open-source dataset for training top-performing reward models. 2024b</p>
<p>Yuntian Bill Yuchen Lin, Khyathi Deng, Faeze Chandu, Abhilasha Brahman, Valentina Ravichander, Nouha Pyatkin, Ronan Dziri, Yejin Le Bras, Choi, Wildbench: Benchmarking llms with challenging tasks from real users in the wild. 20242406arXiv e-prints</p>
<p>Clarify when necessary: Resolving ambiguity through interaction with lms. J Q Michael, Eunsol Zhang, Choi, arXiv:2311.094692023arXiv preprint</p>
<p>Wildchat: 1m chatGPT interaction logs in the wild. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, Yuntian Deng, The Twelfth International Conference on Learning Representations. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8733 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8733</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8733</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-266755862</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.02009v3.pdf" target="_blank">Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives</a></p>
                <p><strong>Paper Abstract:</strong> The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8733.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8733.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Reflection (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Self-Reflection (Initial Response → Self-Evaluate → Revision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-hoc prompting pipeline where an LLM generates an initial response, self-evaluates that response (often via a prompt asking to check correctness), and then revises the response based on the self-evaluation; typically executed as a single evaluation+revision pass.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-0613, GPT-4-0613, Llama2-chat (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial and open-source chat LLMs used in experiments: GPT-3.5-Turbo-0613 and GPT-4-0613 (OpenAI chat models), and Llama2-chat in 7B, 13B, and 70B parameter sizes; experiments used temperature=0.2 and standard instruction prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial answer (CoT-enabled for reasoning tasks), then run a self-evaluation prompt asking the model to check the previous response for errors and provide feedback, and finally request a single revision conditioned on that feedback (one generate→evaluate→revise cycle).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP (math reasoning), CommonMT (creative translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K and SVAMP: math word problem benchmarks (GSM8K is harder); CommonMT: Chinese→English creative translation dataset with idioms/metaphors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Varies by model/task; example: GPT-3.5 on GSM8K: post-reflection accuracy 75.8% (after one self-reflection pass), GPT-3.5 on SVAMP post-reflection 80.5% (marginal changes overall reported; translation BLEURT decreased slightly in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Example baseline (CoT prompt / pre-reflection): GPT-3.5 on GSM8K pre-reflection 76.6% (so reflection changed 76.6 → 75.8); GPT-3.5 on SVAMP pre-reflection 79.8% (→ 80.5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered self-evaluation prompts (10 different evaluation prompts were designed for the baseline), followed by a single revision prompt; no external verifier or tool used.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: across multiple LLMs and tasks, vanilla self-reflection produced negligible or statistically insignificant improvements overall (e.g., many models show ±~1% changes; only 15.1% of initially incorrect responses were corrected by reflection in initial analysis). Qualitative: manual analysis revealed many self-evaluation outputs were overconfident or inconsistent, explaining weak gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High rates of poor-quality self-feedback: self-evaluations were often overconfident (reported 46.7% of evaluations) or inconsistent/random (45.7%), leading to many Invalid reflections (wrong→wrong) and Toxic reflections (right→wrong). Example counts on GSM8K: Invalid reflections far outnumber Valid corrections (✗⇒✗: 269 vs ✗⇒✓: 48) and Toxic cases (✓⇒✗: 52). Reflection sometimes decreased performance (negative deltas reported for some models/tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared unfavorably to Self-Contrast (proposed) and some ensemble methods; Self-Reflection showed little to no improvement on average and had larger uncertainty (higher p-values) compared to Self-Contrast.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Manual analysis across repeated self-evaluation prompts (10 runs) categorized feedback into accurate, stubbornly wrong, inconsistent, or overconfident — demonstrating many failure modes; no additional ablation improving vanilla self-reflection was reported in-depth beyond these analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8733.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8733.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Contrast</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Contrast: Contrastive Self-Reflection via Diverse Solving Perspectives</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive post-hoc reflection pipeline: the LLM self-generates multiple diverse 'solving perspective' prompts, produces multiple candidate solutions, clusters/selects markedly different candidates, contrasts pairwise differences to generate a detailed checklist of potential issues, and then revises candidate solutions to consensus using that checklist.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-0613, GPT-4-0613, Llama2-chat (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of chat LLMs as other experiments; Self-Contrast uses each LLM both to generate self-curated prompts/perspectives and to perform contrast+revision. Experiments used temperature=0.2; self-curated prompts limited to 2–9 perspectives and selection k=3.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Contrast (inter-perspective contrast → checklist → revision)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Steps: (1) Self-curated prompt generation: LLM generates 2–9 tailored prompts representing different perspectives. (2) Generate candidate responses under each perspective. (3) Filter similar responses via K-Medoids clustering (k=3) and select cluster centroids with substantial discrepancies. (4) In a single pass, instruct LLM to contrast each pair (Are they different? Where? Why?), summarize differences into a checklist of re-examination instructions. (5) Use checklist+discrepancies in a JSON revision prompt to produce revised, consistent responses. Usually a single contrast+revise pass per request.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP (math reasoning), CommonMT (creative translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as the paper: math reasoning (GSM8K, SVAMP) and creative translation (CommonMT hard subset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported improvements: e.g., on GSM8K (GPT-3.5) Self-Contrast final accuracy 84.4% (CoT baseline 76.6% → Self-Contrast +7.8% absolute). Average math-reasoning improvement +7.2% across models; translation BLEURT improved by +0.95 (versus self-reflection which decreased by -1.6). Significance improvements: p-value for delta improved from 0.6613 (self-evaluate) to 0.0933 (Self-Contrast) in one setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline CoT / pre-reflection (example): GPT-3.5 on GSM8K 76.6% (pre-reflection); Self-Reflection baseline post-reflection was 75.8% (so Self-Contrast improves over both pre- and post-reflection baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering + self-curated prompts + clustering (K-Medoids) selection of diverse candidates + contrastive pairwise analysis by the LLM to produce a checklist, followed by a JSON-structured revision prompt; entirely LLM-driven (no external human feedback), though authors suggest optional external diff tools for weak models.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: consistent gains across models and tasks (average +7.2% on math reasoning; GPT-3.5 +7.8% GSM8K, +9.2% SVAMP; Llama2-70B +11.6% GSM8K and +9.3% SVAMP). Reduction in error categories: on GSM8K with GPT-3.5, Toxic cases (✓⇒✗) decreased by 78.9% and Invalid cases (✗⇒✗) decreased by 30.8%. Ablations show self-curated prompts outperform naive top-n sampling (Self-Contrast 84.4% vs sampling max 81.8% with 5 samples). Checklist removal hurt math (-3.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less effective on smaller/weaker LLMs (e.g., Llama2-7B performed slightly worse than some ensemble baselines due to weaker instruction-following); requires more LLM calls than a single reflect pass (but fewer than multi-agent debate); relies on LLM ability to generate diverse, meaningful perspectives—when perspectives are similar, contrast effect reduces. Authors propose external diff or rule-based tools for comparisons as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms vanilla self-reflection and many baselines (Self-Consistency variants, multi-agent debate, hint/math prompts) on most models and tasks; compared to Self-Consistency (SC-Vote) it is more general (handles non-numerical tasks like translation) and often yields higher absolute accuracy; Multi-Agent debate produced improvements but Self-Contrast achieves better or comparable gains with fewer API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Key ablations: (1) Replace self-curated prompts with sampling top-n — performance improves with n but remains below self-curated approach (max sampled 5 → 81.8% vs self-curated 84.4% on GSM8K). (2) Remove checklist generation — significant drop in math performance (-3.5%), minor effect on translation. (3) Selection strategies (random, clustering+random, clustering+LLM selecting, clustering+negative perspective) performed worse than default clustering+centroid selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8733.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8733.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC-Vote / SC-Select / SC-Reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (vote/select/reflect variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ensemble-like strategy that samples multiple chain-of-thought (CoT) reasoning traces (K candidates) and aggregates answers either by majority voting (SC-Vote), LLM selection among candidates (SC-Select), or by prompting reflection over candidates to produce a new answer (SC-Reflect).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-0613, GPT-4-0613, Llama2-chat (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to same experimental models; SC variants sample K decoding trajectories (the paper used 8 samples in many comparisons) and then apply voting, selecting, or reflecting as aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (SC-Vote / SC-Select / SC-Reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample K reasoning outputs (breadth exploration). SC-Vote: majority voting over final answers. SC-Select: LLM chooses the most appropriate among K candidates. SC-Reflect: LLM reflects on the K candidates and regenerates a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP and translation (where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math reasoning and translation tasks as in paper. Note: voting is poorly suited for non-numeric open-ended tasks like creative translation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Examples: SC-Vote (sampling 8) produced strong gains on some settings, e.g., SC-Vote reported 83.5% on GSM8K (GPT-3.5) compared to CoT 76.6% (approx +6.9%). SC-Reflect had mixed results and sometimes similar to Self-Reflection (e.g., SC-Reflect 75.8 in some rows).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline CoT prompt performance: e.g., GPT-3.5 on GSM8K CoT 76.6% (used as a no-ensemble baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Sampling multiple model decodings (stochastic decoding) and aggregating answers via voting, selection by an LLM, or a reflective generation; relies on CoT sampling and then meta-aggregation prompts or voting logic (external or internal).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: SC-Vote often yields large improvements for math (example SC-Vote 83.5% on GSM8K for GPT-3.5), demonstrating that breadth sampling + aggregation can outperform single-pass generation or single self-reflection in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SC-Vote cannot be directly applied to non-numerical open-ended tasks like translation due to difficulty in voting; SC-Select and SC-Reflect underperform Self-Contrast on translation and some reasoning tasks. SC-Reflect did not substantially mitigate invalid/toxic reflection categories in some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Self-Contrast, SC-Vote is competitive on some models but less general (fails for translation). Self-Contrast often outperforms SC variants on a broad set of tasks and models; for the weakest model (Llama2-7B), self-consistency or ensemble approaches sometimes outperformed Self-Contrast.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>The paper reports comparisons between SC variants and Self-Contrast; no internal ablation of SC parameters beyond comparing SC-Vote / SC-Select / SC-Reflect behavior is described in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8733.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8733.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion / Self-Refine (related works)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion; Self-Refine; generate-then-reflect approaches (literature mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related iterative self-improvement approaches from prior work where models generate plans/answers and then refine them either via internal memory updates (Reflexion) or iterative self-feedback (Self-Refine); cited but not experimentally developed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion; Self-Refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Reflexion (Noah Shinn et al.) describes autonomous agents that record and refine behaviors via dynamic memory and self-reflection across multiple episodes; Self-Refine (Madaan et al.) describes iterative refinement with self-generated feedback over multiple rounds. The paper cites these as related generate-then-reflect or iterative self-critique strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Mentioned mechanisms include dynamic memory (Reflexion) and iterative self-feedback prompts (Self-Refine); these are external citations and not re-implemented in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as prior art that claimed self-correction benefits; the current paper references more recent work that questions the reliability of such intrinsic reflection without external feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper cites recent studies (Huang et al., Stechly et al., Valmeekam et al.) casting doubt on the reliability of purely intrinsic iterative self-improvement without external feedback; no experimental data for these specific methods is provided within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented in related work as examples of iterative improvement; the present paper positions Self-Contrast as an alternative that addresses instability in self-evaluation observed in vanilla generate-then-reflect approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8733.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8733.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Debate (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate / Multi-Role Dialogue Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent debate baseline where multiple agent personas (3 agents in this paper) engage in a multi-round debate to collaboratively or adversarially reach a better response; used as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-0613 (agents implemented via prompts), Llama2 variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-agent setup simulated by prompting the same or different LLM instantiations to play different roles/personas and debate across rounds; in this paper they configured 3 agents and 3 rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-Agent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Instantiate multiple agent-personas (manually configured or prompted) that debate a problem across multiple rounds; final answer is derived from the debate outcome or aggregated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP, CommonMT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning and translation benchmarks used for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported improvements in many cases (example: Multi-Agent 83.8% on GSM8K for GPT-3.5 in Table 4, a +7.2% absolute gain over CoT baseline in that row), but results vary by model/task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline CoT performance (example GPT-3.5 on GSM8K 76.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered multi-role dialogues where agents take turns and challenge one another; relies on LLM instruction-following to simulate multiple participants.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Multi-Agent often yields substantial gains relative to CoT on math reasoning, but Self-Contrast claims larger or similar improvements with fewer LLM calls and less manual agent configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires manual design of agent roles and interactions; higher call overheads compared to Self-Contrast; quality sensitive to agent role design; Self-Contrast achieved better or comparable accuracy with fewer calls in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Multi-Agent gives competitive accuracy improvements but at larger call cost and manual setup; Self-Contrast argued to be a more automated, flexible variant that focuses explicitly on contrasting discrepancies and summarizing checklists.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Can large language models really improve by self-critiquing their own plans? <em>(Rating: 2)</em></li>
                <li>Self-Polish: Enhance reasoning in large language models via problem refinement <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8733",
    "paper_id": "paper-266755862",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-Reflection (vanilla)",
            "name_full": "Vanilla Self-Reflection (Initial Response → Self-Evaluate → Revision)",
            "brief_description": "A post-hoc prompting pipeline where an LLM generates an initial response, self-evaluates that response (often via a prompt asking to check correctness), and then revises the response based on the self-evaluation; typically executed as a single evaluation+revision pass.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-0613, GPT-4-0613, Llama2-chat (7B, 13B, 70B)",
            "model_description": "Commercial and open-source chat LLMs used in experiments: GPT-3.5-Turbo-0613 and GPT-4-0613 (OpenAI chat models), and Llama2-chat in 7B, 13B, and 70B parameter sizes; experiments used temperature=0.2 and standard instruction prompts.",
            "reflection_method_name": "Self-Reflection",
            "reflection_method_description": "Generate an initial answer (CoT-enabled for reasoning tasks), then run a self-evaluation prompt asking the model to check the previous response for errors and provide feedback, and finally request a single revision conditioned on that feedback (one generate→evaluate→revise cycle).",
            "task_name": "GSM8K, SVAMP (math reasoning), CommonMT (creative translation)",
            "task_description": "GSM8K and SVAMP: math word problem benchmarks (GSM8K is harder); CommonMT: Chinese→English creative translation dataset with idioms/metaphors.",
            "performance_with_reflection": "Varies by model/task; example: GPT-3.5 on GSM8K: post-reflection accuracy 75.8% (after one self-reflection pass), GPT-3.5 on SVAMP post-reflection 80.5% (marginal changes overall reported; translation BLEURT decreased slightly in some settings).",
            "performance_without_reflection": "Example baseline (CoT prompt / pre-reflection): GPT-3.5 on GSM8K pre-reflection 76.6% (so reflection changed 76.6 → 75.8); GPT-3.5 on SVAMP pre-reflection 79.8% (→ 80.5).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered self-evaluation prompts (10 different evaluation prompts were designed for the baseline), followed by a single revision prompt; no external verifier or tool used.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: across multiple LLMs and tasks, vanilla self-reflection produced negligible or statistically insignificant improvements overall (e.g., many models show ±~1% changes; only 15.1% of initially incorrect responses were corrected by reflection in initial analysis). Qualitative: manual analysis revealed many self-evaluation outputs were overconfident or inconsistent, explaining weak gains.",
            "limitations_or_failure_cases": "High rates of poor-quality self-feedback: self-evaluations were often overconfident (reported 46.7% of evaluations) or inconsistent/random (45.7%), leading to many Invalid reflections (wrong→wrong) and Toxic reflections (right→wrong). Example counts on GSM8K: Invalid reflections far outnumber Valid corrections (✗⇒✗: 269 vs ✗⇒✓: 48) and Toxic cases (✓⇒✗: 52). Reflection sometimes decreased performance (negative deltas reported for some models/tasks).",
            "comparison_to_other_methods": "Compared unfavorably to Self-Contrast (proposed) and some ensemble methods; Self-Reflection showed little to no improvement on average and had larger uncertainty (higher p-values) compared to Self-Contrast.",
            "ablation_study_results": "Manual analysis across repeated self-evaluation prompts (10 runs) categorized feedback into accurate, stubbornly wrong, inconsistent, or overconfident — demonstrating many failure modes; no additional ablation improving vanilla self-reflection was reported in-depth beyond these analyses.",
            "uuid": "e8733.0",
            "source_info": {
                "paper_title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-Contrast",
            "name_full": "Self-Contrast: Contrastive Self-Reflection via Diverse Solving Perspectives",
            "brief_description": "A contrastive post-hoc reflection pipeline: the LLM self-generates multiple diverse 'solving perspective' prompts, produces multiple candidate solutions, clusters/selects markedly different candidates, contrasts pairwise differences to generate a detailed checklist of potential issues, and then revises candidate solutions to consensus using that checklist.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-0613, GPT-4-0613, Llama2-chat (7B, 13B, 70B)",
            "model_description": "Same set of chat LLMs as other experiments; Self-Contrast uses each LLM both to generate self-curated prompts/perspectives and to perform contrast+revision. Experiments used temperature=0.2; self-curated prompts limited to 2–9 perspectives and selection k=3.",
            "reflection_method_name": "Self-Contrast (inter-perspective contrast → checklist → revision)",
            "reflection_method_description": "Steps: (1) Self-curated prompt generation: LLM generates 2–9 tailored prompts representing different perspectives. (2) Generate candidate responses under each perspective. (3) Filter similar responses via K-Medoids clustering (k=3) and select cluster centroids with substantial discrepancies. (4) In a single pass, instruct LLM to contrast each pair (Are they different? Where? Why?), summarize differences into a checklist of re-examination instructions. (5) Use checklist+discrepancies in a JSON revision prompt to produce revised, consistent responses. Usually a single contrast+revise pass per request.",
            "task_name": "GSM8K, SVAMP (math reasoning), CommonMT (creative translation)",
            "task_description": "Same benchmarks as the paper: math reasoning (GSM8K, SVAMP) and creative translation (CommonMT hard subset).",
            "performance_with_reflection": "Reported improvements: e.g., on GSM8K (GPT-3.5) Self-Contrast final accuracy 84.4% (CoT baseline 76.6% → Self-Contrast +7.8% absolute). Average math-reasoning improvement +7.2% across models; translation BLEURT improved by +0.95 (versus self-reflection which decreased by -1.6). Significance improvements: p-value for delta improved from 0.6613 (self-evaluate) to 0.0933 (Self-Contrast) in one setting.",
            "performance_without_reflection": "Baseline CoT / pre-reflection (example): GPT-3.5 on GSM8K 76.6% (pre-reflection); Self-Reflection baseline post-reflection was 75.8% (so Self-Contrast improves over both pre- and post-reflection baselines).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering + self-curated prompts + clustering (K-Medoids) selection of diverse candidates + contrastive pairwise analysis by the LLM to produce a checklist, followed by a JSON-structured revision prompt; entirely LLM-driven (no external human feedback), though authors suggest optional external diff tools for weak models.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: consistent gains across models and tasks (average +7.2% on math reasoning; GPT-3.5 +7.8% GSM8K, +9.2% SVAMP; Llama2-70B +11.6% GSM8K and +9.3% SVAMP). Reduction in error categories: on GSM8K with GPT-3.5, Toxic cases (✓⇒✗) decreased by 78.9% and Invalid cases (✗⇒✗) decreased by 30.8%. Ablations show self-curated prompts outperform naive top-n sampling (Self-Contrast 84.4% vs sampling max 81.8% with 5 samples). Checklist removal hurt math (-3.5%).",
            "limitations_or_failure_cases": "Less effective on smaller/weaker LLMs (e.g., Llama2-7B performed slightly worse than some ensemble baselines due to weaker instruction-following); requires more LLM calls than a single reflect pass (but fewer than multi-agent debate); relies on LLM ability to generate diverse, meaningful perspectives—when perspectives are similar, contrast effect reduces. Authors propose external diff or rule-based tools for comparisons as future work.",
            "comparison_to_other_methods": "Outperforms vanilla self-reflection and many baselines (Self-Consistency variants, multi-agent debate, hint/math prompts) on most models and tasks; compared to Self-Consistency (SC-Vote) it is more general (handles non-numerical tasks like translation) and often yields higher absolute accuracy; Multi-Agent debate produced improvements but Self-Contrast achieves better or comparable gains with fewer API calls.",
            "ablation_study_results": "Key ablations: (1) Replace self-curated prompts with sampling top-n — performance improves with n but remains below self-curated approach (max sampled 5 → 81.8% vs self-curated 84.4% on GSM8K). (2) Remove checklist generation — significant drop in math performance (-3.5%), minor effect on translation. (3) Selection strategies (random, clustering+random, clustering+LLM selecting, clustering+negative perspective) performed worse than default clustering+centroid selection.",
            "uuid": "e8733.1",
            "source_info": {
                "paper_title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-Consistency (SC-Vote / SC-Select / SC-Reflect)",
            "name_full": "Self-Consistency (vote/select/reflect variants)",
            "brief_description": "Ensemble-like strategy that samples multiple chain-of-thought (CoT) reasoning traces (K candidates) and aggregates answers either by majority voting (SC-Vote), LLM selection among candidates (SC-Select), or by prompting reflection over candidates to produce a new answer (SC-Reflect).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-0613, GPT-4-0613, Llama2-chat (various sizes)",
            "model_description": "Applied to same experimental models; SC variants sample K decoding trajectories (the paper used 8 samples in many comparisons) and then apply voting, selecting, or reflecting as aggregation.",
            "reflection_method_name": "Self-Consistency (SC-Vote / SC-Select / SC-Reflect)",
            "reflection_method_description": "Sample K reasoning outputs (breadth exploration). SC-Vote: majority voting over final answers. SC-Select: LLM chooses the most appropriate among K candidates. SC-Reflect: LLM reflects on the K candidates and regenerates a final answer.",
            "task_name": "GSM8K, SVAMP and translation (where applicable)",
            "task_description": "Math reasoning and translation tasks as in paper. Note: voting is poorly suited for non-numeric open-ended tasks like creative translation.",
            "performance_with_reflection": "Examples: SC-Vote (sampling 8) produced strong gains on some settings, e.g., SC-Vote reported 83.5% on GSM8K (GPT-3.5) compared to CoT 76.6% (approx +6.9%). SC-Reflect had mixed results and sometimes similar to Self-Reflection (e.g., SC-Reflect 75.8 in some rows).",
            "performance_without_reflection": "Baseline CoT prompt performance: e.g., GPT-3.5 on GSM8K CoT 76.6% (used as a no-ensemble baseline).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Sampling multiple model decodings (stochastic decoding) and aggregating answers via voting, selection by an LLM, or a reflective generation; relies on CoT sampling and then meta-aggregation prompts or voting logic (external or internal).",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: SC-Vote often yields large improvements for math (example SC-Vote 83.5% on GSM8K for GPT-3.5), demonstrating that breadth sampling + aggregation can outperform single-pass generation or single self-reflection in many cases.",
            "limitations_or_failure_cases": "SC-Vote cannot be directly applied to non-numerical open-ended tasks like translation due to difficulty in voting; SC-Select and SC-Reflect underperform Self-Contrast on translation and some reasoning tasks. SC-Reflect did not substantially mitigate invalid/toxic reflection categories in some analyses.",
            "comparison_to_other_methods": "Compared to Self-Contrast, SC-Vote is competitive on some models but less general (fails for translation). Self-Contrast often outperforms SC variants on a broad set of tasks and models; for the weakest model (Llama2-7B), self-consistency or ensemble approaches sometimes outperformed Self-Contrast.",
            "ablation_study_results": "The paper reports comparisons between SC variants and Self-Contrast; no internal ablation of SC parameters beyond comparing SC-Vote / SC-Select / SC-Reflect behavior is described in detail.",
            "uuid": "e8733.2",
            "source_info": {
                "paper_title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Reflexion / Self-Refine (related works)",
            "name_full": "Reflexion; Self-Refine; generate-then-reflect approaches (literature mentions)",
            "brief_description": "Related iterative self-improvement approaches from prior work where models generate plans/answers and then refine them either via internal memory updates (Reflexion) or iterative self-feedback (Self-Refine); cited but not experimentally developed in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reflexion; Self-Refine (iterative self-feedback)",
            "reflection_method_description": "Reflexion (Noah Shinn et al.) describes autonomous agents that record and refine behaviors via dynamic memory and self-reflection across multiple episodes; Self-Refine (Madaan et al.) describes iterative refinement with self-generated feedback over multiple rounds. The paper cites these as related generate-then-reflect or iterative self-critique strategies.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Mentioned mechanisms include dynamic memory (Reflexion) and iterative self-feedback prompts (Self-Refine); these are external citations and not re-implemented in this work.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as prior art that claimed self-correction benefits; the current paper references more recent work that questions the reliability of such intrinsic reflection without external feedback.",
            "limitations_or_failure_cases": "The paper cites recent studies (Huang et al., Stechly et al., Valmeekam et al.) casting doubt on the reliability of purely intrinsic iterative self-improvement without external feedback; no experimental data for these specific methods is provided within this paper.",
            "comparison_to_other_methods": "Presented in related work as examples of iterative improvement; the present paper positions Self-Contrast as an alternative that addresses instability in self-evaluation observed in vanilla generate-then-reflect approaches.",
            "ablation_study_results": null,
            "uuid": "e8733.3",
            "source_info": {
                "paper_title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Multi-Agent Debate (baseline)",
            "name_full": "Multi-Agent Debate / Multi-Role Dialogue Baseline",
            "brief_description": "A multi-agent debate baseline where multiple agent personas (3 agents in this paper) engage in a multi-round debate to collaboratively or adversarially reach a better response; used as a comparative baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-0613 (agents implemented via prompts), Llama2 variants",
            "model_description": "Multi-agent setup simulated by prompting the same or different LLM instantiations to play different roles/personas and debate across rounds; in this paper they configured 3 agents and 3 rounds.",
            "reflection_method_name": "Multi-Agent Debate",
            "reflection_method_description": "Instantiate multiple agent-personas (manually configured or prompted) that debate a problem across multiple rounds; final answer is derived from the debate outcome or aggregated outputs.",
            "task_name": "GSM8K, SVAMP, CommonMT",
            "task_description": "Same reasoning and translation benchmarks used for comparisons.",
            "performance_with_reflection": "Reported improvements in many cases (example: Multi-Agent 83.8% on GSM8K for GPT-3.5 in Table 4, a +7.2% absolute gain over CoT baseline in that row), but results vary by model/task.",
            "performance_without_reflection": "Baseline CoT performance (example GPT-3.5 on GSM8K 76.6%).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered multi-role dialogues where agents take turns and challenge one another; relies on LLM instruction-following to simulate multiple participants.",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Quantitative: Multi-Agent often yields substantial gains relative to CoT on math reasoning, but Self-Contrast claims larger or similar improvements with fewer LLM calls and less manual agent configuration.",
            "limitations_or_failure_cases": "Requires manual design of agent roles and interactions; higher call overheads compared to Self-Contrast; quality sensitive to agent role design; Self-Contrast achieved better or comparable accuracy with fewer calls in the paper's experiments.",
            "comparison_to_other_methods": "Multi-Agent gives competitive accuracy improvements but at larger call cost and manual setup; Self-Contrast argued to be a more automated, flexible variant that focuses explicitly on contrasting discrepancies and summarizing checklists.",
            "ablation_study_results": null,
            "uuid": "e8733.4",
            "source_info": {
                "paper_title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Can large language models really improve by self-critiquing their own plans?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_really_improve_by_selfcritiquing_their_own_plans"
        },
        {
            "paper_title": "Self-Polish: Enhance reasoning in large language models via problem refinement",
            "rating": 1,
            "sanitized_title": "selfpolish_enhance_reasoning_in_large_language_models_via_problem_refinement"
        }
    ],
    "cost": 0.0178935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives
6 Jun 2024</p>
<p>Wenqi Zhang zhangwenqi@zju.edu.cn 
College of Computer Science and Technology
Zhejiang University</p>
<p>Yongliang Shen 
College of Computer Science and Technology
Zhejiang University</p>
<p>Linjuan Wu 
College of Computer Science and Technology
Zhejiang University</p>
<p>Qiuying Peng 
OPPO Research Institute
China</p>
<p>Jun Wang 
OPPO Research Institute
China</p>
<p>Yueting Zhuang 
College of Computer Science and Technology
Zhejiang University</p>
<p>Weiming Lu 
College of Computer Science and Technology
Zhejiang University</p>
<p>Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives
6 Jun 20247F475DC5B15217C7D40536E93668EC70arXiv:2401.02009v3[cs.CL]
The reflection capacity of Large Language Model (LLM) has garnered extensive attention.A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback.However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable.Our investigation unveils that the key bottleneck is the quality of the selfevaluated feedback.We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection.To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies.Our method provides LLM with diverse perspectives to alleviate stubborn biases.Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks.Reflecting upon these can prompt more accurate and stable reflection.Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.</p>
<p>Introduction</p>
<p>Mastering reasoning and decision-making capabilities is of utmost importance to paving the way for artificial general intelligence.Recently, large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022a;Zeng et al., 2023;Touvron et al., 2023a;OpenAI, 2022OpenAI, , 2023;;Touvron et al., 2023b) and applications built on them (Schick et al., 2023;Wu et al., 2023a;Shen et al., 2023;Zhang et al., 2023a) demonstrate impressive capabilities in various domains, especially combined with Chain-of-Thought (Wei et al., 2022; Figure 1: LLMs evaluate the initial response and provide feedback for revision.However, most erroneous responses remain uncorrected after reflection as the feedback is either overconfident (46.7%) or inconsistent (45.7%).Bottom: Self-Contrast explores multiple solving perspectives, and contrast their differences, and summarize them into insightful checklist for self-correction.Kojima et al., 2022), ReAct (Yao et al., 2022), Treeof-Thought (Yao et al., 2023) and other prompting techniques (Gao et al., 2022;Wang et al., 2023d;Zhou et al., 2022;Besta et al., 2023).</p>
<p>Despite these advancements, LLMs are not entirely reliable (Zheng et al., 2023c;Frieder et al., 2023;Yuan et al., 2023b) since they frequently produce inaccuracies results, such as misunderstanding a key concept, overlooking some crucial details.A post-hoc prompting strategy, e.g., self-reflection, garnered considerable attention (Shinn et al., 2023;Madaan et al., 2023;Paul et al., 2023).It first generates an initial response (Initial Response), then gathers external feedback or self-evaluated feedback (Evaluation Phase) to refine prior response (Revision) (Welleck et al., 2022;Kadavath et al., 2022;Chen et al., 2023d;Liang et al., 2023;Kim et al., 2023;Zheng et al., 2023a;Du et al., 2023;Xi et al., 2023;Ganguli et al., 2023;Pan et al., 2023).Numerous studies proclaim this three-stage strategy (Initial Response→Evaluation→Revision), can endow LLMs with the potential to self-correct previous imperfect responses.For a time, this belief appeared to dominate the community.</p>
<p>However, recent studies (Huang et al., 2023b;Stechly et al., 2023;Liang et al., 2023;Valmeekam et al., 2023) have cast doubt on LLM's inherent reflection capability.Their research indicates that without external feedback, LLMs have difficulties in amending prior responses.It implies selfcorrection is unreliable when relying only on LLM itself and simple post-hoc prompting strategies.</p>
<p>We are also intrigued by LLM's internal reflection ability, as external feedback is not available in most scenarios.Our initial experiments ( § 2.1) indicate that intrinsic reflection has limited effect.Across various LLMs and tasks, the performance gains from reflection are not significant, and occasionally detrimental.In cases of incorrect initial responses, only 15.1% of incorrect responses are corrected through reflection.To ascertain the reasons for that, we further analyze the feedback generated by the self-evaluate process.As shown in Figure 1, LLMs often provide two unexpected feedback: 1) Overconfidence (46.7%):Stubbornly insisting that the previous solution is correct.2) Inconsistency (45.7%):The feedback is highly inconsistent when self-evaluating the same response multiple times.These two feedbacks seriously undermine the effectiveness of reflection.It reveals that such a simple self-evaluate strategy faces difficulty in accurately identifying errors and consistently generating high-quality feedback for reflection.</p>
<p>As a remedy, we propose a contrastive strategy as an alternative to the direct evaluation: we examine the differences among multiple responses and draw inspiration to derive feedbacks from their disparities for reflection.The insight is that while generating accurate feedback directly may be challenging, identifying contrasts between various responses is often more feasible.More importantly, these discrepancies often indicate some potential errors, easily overlooked details or pitfalls.As shown in Figure 1, by contrasting two solutions, LLM finds they have different solving objectives, and suggests re-examining the intent of the original request in the checklist.This contrasting paradigm can also be seen in some contemporaneous work (Wan et al., 2023;Yuan et al., 2024).</p>
<p>Embracing this philosophy, we advocate Self-Contrast, which steers LLM to autonomously create diverse solving perspectives by self-curated prompts and then select different results with significant discrepancies for comparison.Then LLM reflects on the reasons behind these discrepancies and generates multiple re-examining instructions, i.e., checklist, for reflection.Our experiments show that by creating diverse perspectives adaptively, Self-Contrast can mitigate biases introduced by specific prompts.Moreover, contrasting the discrepancies between perspectives inspires deeper reflection, thereby enhancing the likelihood of accurate self-correction.</p>
<p>Our contributions can be summarized as:</p>
<p>Evaluation of Intrinsic Reflection</p>
<p>We first comprehensively investigate the intrinsic reflection capability of LLMs, i.e., LLMs selfevaluate the initial response without external feedback and then refine it.Subsequently, we methodically investigate the factors influencing reflection.</p>
<p>Performance Pre-and Post-Reflection</p>
<p>We evaluate the reflection capabilities of multiple LLMs across a variety of benchmarks, including math reasoning and creative translation tasks.We report average accuracy for math reasoning and the BLEURT score between predicted sentences and references for the translation task (see § 4.1 for detail).Each result is evaluated multiple times on different prompts.Besides, we also report the significance level (one-tailed t-test) of the accuracy change pre-and post-reflection.instance, the performance of GPT-3.5 on GSM8K and SVAMP exhibit marginal changes of -0.8% and +0.7% after reflection respectively, both statistically insignificant.This negligible performance fluctuation can be validated across multiple LLMs and various benchmarks, far from expectations.Specifically, most reasoning cases suffer from a slight decrease, while the translation task shows little impact.Additionally, smaller LLMs (e.g., Llama2-7B) demonstrate poorer reflection ability, occasionally even exhibiting negative impacts.These experiments collectively suggest that LLMs appear to be incapable of self-correction through reflection.</p>
<p>As shown in</p>
<p>Feedback Analysis</p>
<p>To investigate the reasons behind the failure of reflection, we further analyze the feedback generated during the self-evaluate process.We classify all samples in GSM8K into four categories based on their correctness of the pre-and post-reflection: 1) Invalid Reflection (✗⇒✗) means the results before and after reflection are both incorrect.2) Valid Reflection (✗⇒✓) means a wrong solution is revised to correct through reflection.3) Toxic Reflection (✓⇒✗) represents that an originally correct response is changed to incorrect after reflection.4) Others counts the number of correct ⇒ correct.Automatic statistics for the reflection category.</p>
<p>Step 1: We categorize the reflection into the above four categories.This process can be automated for mathematical benchmarks by comparing whether the answers are correct before and after reflection.For the translation task, we leverage GPT-4 along with annotated answers to evaluate the accuracy of translation results before and after reflection.</p>
<p>Step 2: We manually assess the quality of the feedback generated in each reflection case (Invalid, Valid, and Toxic).Based on the correctness and consistency of these feedbacks, we categorize them into four cases (inconsistent, overconfident, etc.).The detailed results are as follows:</p>
<p>Fail to Correct the Wrong Initial Response.As shown in Table 2, we observe the number of Toxic Reflection (✓⇒✗: 52) and Valid Reflection (✗⇒✓: 48) are nearly similar.This explains why there is no discernible difference in performance pre-and post-reflection.Besides, considering the scenario when the initial response is erroneous, we observe the number of Invalid Reflection (✗⇒✗: 269) is significantly larger than Valid Reflection (✗⇒✓: 48), which indicates LLM fails to correct errors in the initial responses for most cases.</p>
<p>Often Provide Overconfident or Inconsistent Feedback.We examine whether LLMs could generate feedback accurately and consistently.For each sample, we instruct the LLM to evaluate its initial response multiple times and record multiple feedbacks.We manually assess the consistency and correctness of these feedbacks and then summarize each sample into 4 cases: I. Accurately identifies errors: In multiple repeated evaluations, the LLM identifies errors and provides accurate and consistent feedback.II.Stubbornly offers erroneous feedback: The majority of evaluations provide incorrect feedback with specific errors.III.Can not output consistent feedback: Unable to assess consistently, as most feedback is different and quite random for a same initial response.V. Overconfidence, no revision required: LLM is overconfident and believes no revision is necessary.The detailed evaluation criteria are provided in Appendix A.1.</p>
<p>As shown in Table 2, for the majority of Invalid Reflection, their feedback is either overconfident (53.5%) or highly inconsistent (45.3%), making it difficult to prompt reliable reflection.Similarly, in Toxic Reflection scenarios, 65.4% of the evaluation processes are highly inconsistent, leading to many correct answers being erroneously modified.</p>
<p>From Self-Evaluate to Self-Contrast</p>
<p>The aforementioned experiments indicate that feedback generated by the self-evaluate process is either highly random or excessively confident.This unstable self-evaluate may severely impact the reflection performance of LLMs.</p>
<p>As a remedy, we propose a contrastive strategy   for reflection.Instead of directly evaluating a response, which can be challenging and inconsistent, we instruct the LLM to initially contrast the differences between various solutions, and identify their discrepancies and the reasons behind them.As shown in Figure 1 (bottom), we sample Top-2 responses from LLM and then prompt LLM to contrast their differences in detail, rethink the reasons that caused the discrepancies, and summarize the checklist for re-examining and resolving the discrepancy.As shown in Table 3, we compare three scenarios: self-evaluate w/ top-1 response, selfevaluate w/ top-2 responses, and self-contrast w/ top-2.Our new strategy achieves a modest improvement over standard reflection using self-evaluate.Notably, it significantly enhances the significance levels (p-value: 0.6613 to 0.0933), suggesting it can greatly mitigate the self-evaluation process's uncertainty.</p>
<p>In this section, we validate the concept of contrastive evaluation.For the next section, we expand this contrastive concept into full-version selfcontrast, which involves creating multiple perspectives, and contrasting their differences, summarizing the checklist for deeper reflection.</p>
<p>Self-Contrast</p>
<p>Prior sections illustrate the challenges LLMs encounter in accurately evaluating previous solutions, often resulting in overconfident or inconsistent feedback.Concurrently, we observe that leveraging the discrepancies between two different solutions can inspire a more efficacious reflection, notably reducing the uncertainty during the reflection.Building upon this insight, we propose a more diverse inter-perspective Self-Contrast, facilitating more reliable self-reflection.</p>
<p>Self-Contrast consists of three procedures: Create Diverse Perspectives, Contrast Inter-Perspective Discrepancies, and Eliminate Discrepancies.In Create Diverse Perspectives ( § 3.1), we encourage LLMs to autonomously create a variety of prompts tailored to the user's request, each offering a unique perspective for problem-solving, e.g., different thinking styles, diverse identities, personalities, or preferences.These diverse perspectives prompt the LLM to generate different responses.In the second stage ( § 3.2), LLM contrasts the differences between each pair of responses.Lastly ( § 3.3), to eliminate discrepancies, we abstract these differences into a detailed checklist for re-examining.This checklist guides the LLM to meticulously examine the causes of discrepancies, including random errors or intrinsic biases, which result in inconsistent results among perspectives.</p>
<p>As shown in Figure 2, LLM designs five different prompts and their translation results based on the user's request ("这个计划被枪毙") .From a literal perspective, the phrase "被枪毙" is translated as "shot to death".This rigid translation fails to grasp the metaphor embedded in the military term.Conversely, from a liberal perspective, it is translated as "This plan was axed".After contrasting two different translations, LLMs believe they should scrutinize the source sentence for metaphors and ensure the translation aligns with the conventions of English expression.</p>
<p>Create Diverse Perspectives</p>
<p>Self-Curated Prompts First, it is imperative to define the concept of "solving perspective".It refers to deliberate prompting with a unique role, personality, thought style, etc., which prompts LLMs to solve user requests from a specific perspective.Diverse solving perspectives can endow LLMs with a broader range of thoughts for problem-solving, e.g., different angles and methodologies, thereby The phrase "shot to death" could be misinterpreted as hyperbolic or excessively dramatic in English, possibly detracting from the seriousness of the situation."Axed" however, clearly communicates the end of the plan without the risk of being taken too literally or causing undue alarm.</p>
<p>Difference For Result2 and Result5: Both "axed" and "aborted" are common idioms….However, "axed" is often used in more dramatic or serious contexts, while "aborted " can be perceived as more neutral or mundane.Besides, There is a slight difference in the meanings of "this" and "the".</p>
<p>Difference For Result1 and Result5:</p>
<p>The phrase "shot to death" may carry a more dramatic and violent connotation, potentially overemphasizing the original sentiment and seems to be more akin to a metaphor."Aborted" is a common English idiom for discarding plans, more closely aligning with English expressions.</p>
<p>Result1</p>
<p>Result5 Result2</p>
<p>Stage 2 Contrast Discrepancies Stage 1 Create Diverse Perspectives Stage 3 Eliminate Discrepancies</p>
<p>Creative Translation Task User Request: 这个计划被枪毙了 Figure 2: Self-Contrast designs diverse prompts for different solving perspectives and generates corresponding results.Then we filter out similar results and select those that are significantly different.To inspire reflection, we contrast the differences between selected results and prompt LLM to summarize a checklist.This checklist can be used to re-examine and eliminate discrepancies.Lastly, LLM revises each response to achieve a consistent result.mitigating biases introduced by singular prompts.</p>
<p>To achieve this, we adopt a self-curated prompt strategy, where the LLM itself adaptively generates multiple different prompts for each request, each signifying a tailored perspective, then samples corresponding responses based on these prompts.It is noteworthy that the number of perspectives to be created, and the design of each perspective are entirely determined by LLMs, endowing them with more flexibility to address complex tasks.The details of the prompt are provided in Appendix D.1.In Figure 3, we present statistics on the number of prompts generated in self-curated prompt process.</p>
<p>Contrast Inter-Perspective Discrepancies</p>
<p>The LLM generates diverse responses based on selfcurated prompts, each representing a specific perspective.Considering that some responses may be highly similar or even identical, we first filter these similar responses.Then, we select the responses with significant discrepancies for comparison.</p>
<p>Selecting To filter out similar responses, we employ the K-Medoids clustering algorithm based on their semantic similarity.We categorize all responses into k clusters, each encompassing a set of similar results.Then we select the centroids of each cluster as representative responses and discard the remaining ones.It ensures the selected results exhibit substantial differences from each other.</p>
<p>Contrasting After selecting k responses from all candidates, we feed these responses concurrently into LLM and then instruct LLM to autonomously contrast the differences for each pair of responses in a single pass.When contrasting, LLMs need to explicitly answer these questions: Whether the two responses are different, Where the differences lie, and Which factors contributed to these inconsistent results.These questions guide the LLM to methodically explore the underlying reasons behind discrepancies, identifying potential errors and often overlooked details.As shown in Figure 2, for translation tasks, the LLM compares results 1, 2, and 5, and identifies that their primary differences lie in the use of different verbs to express "被枪毙".The detailed prompts are shown in Appendix D.2.</p>
<p>Eliminate Discrepancies</p>
<p>We abstract insightful checklists from these pairwise contrastive differences and then use them to resolve the inconsistencies across various perspectives for a consensus.</p>
<p>Summarizing Checklist To ascertain the truth and resolve discrepancies, the LLM is encouraged to summarize a detailed checklist for re-examining the user's request and candidate responses.This checklist contains multiple specialized checking instructions, such as verifying alignment with the user's intent, identifying contradictions in LLM's responses, checking for miscalculations, etc.It explicitly points out some potential issues, e.g., previously overlooked details, logical pitfalls, or unreasonable steps, and compels LLM to re-examine them.Compared to conventional reflection instruction, e.g., Please check your previous response, our checklist is more precise and informative.</p>
<p>Reflection For Consensus Lastly, we employ the checklist and identified discrepancies to prompt reflection.LLM can revise the inconsistent perspectives and output k consistent responses.</p>
<p>Concretely, we use a JSON format for the revision prompt: Request: {{request}}, Candidate: {{result1}, {result2}, {result3}..}, Discrepancy: {{difference1-2}, {difference1-3}..}, Checklist: {{instruction1},{instruction2},..}.To eliminate discrepancies, we instruct LLM to revise the inconsistent steps of each candidate and output k revised responses with consistent answers.When revising, LLM should require careful and comprehensive consideration, as any minor modifications may lead to new discrepancies with others.</p>
<p>Experiments</p>
<p>Settings</p>
<p>Benchmarks We evaluate our method within two testbeds: mathematical reasoning and translation using GSM8K, SVAMP, and CommonMT benchmarks.Please see Appendix B.1 for details.</p>
<p>Evaluation Metrics For mathematical reasoning, we evaluate the precision of the final answer after their step-by-step reasoning, similar to the previous methodologies.For the translation task, we employ BLEURT1 score as automatic metrics.</p>
<p>LLM Models and Prompts We conduct experiments using the GPT-3.5-Turbo-0613and GPT-4-0613, alongside the Llama2-Chat model with three parameter scales (7B, 13B, and 70B).To make a fair comparison, we uniformly set the temperature to 0.2 for all experiments.For standard prompts and self-reflection baseline, we evaluate them 10 times using different prompts and average their results under zero-shot scenes.Prompts and other details can be found in Appendices B.2, C and D.</p>
<p>Baselines</p>
<p>We compare Self-Contrast with the following baselines: Standard CoT Prompt (Kojima et al., 2022).Self-Reflection (Shinn et al., 2023).Multi-Agent Debate (Du et al., 2023;Liang et al., 2023;He et al., 2020).ExpertPrompt (Xu et al., 2023a).Hint-Prompt (Zheng et al., 2023a).Math-Prompt (Imani et al., 2023).Moreover, for various task scenarios, we consider three forms of Self-Consistency (Wang et al., 2023d;Chen et al., 2023c): SC-Vote: The original Self-Consistency version, which samples K decoding results, followed by a voting process.SC-Select: Instead of voting, LLM also samples Top-K responses but then selects the most appropriate answer from K candidates by itself.SC-Reflect: After sampling Top-K responses, LLM reflects on these candidates and regenerates a new response as the final answer.</p>
<p>Main Results</p>
<p>In Tables 4 and 5, we report the accuracy and the average number of API/LLM calls (#Call), which serves as a proxy for the computational cost.</p>
<p>Consistent improvement over vanilla reflection.Compared to vanilla reflection, Self-Contrast brings significant and stable improvement.mathematical reasoning, we achieve an average improvement of +7.2%.In contrast, the original selfreflection shows no clear improvement (-0.51%).A similar phenomenon is observed in creative translation, where Self-Contrast achieves a +0.95 improvement, whereas self-reflection results in a decrease of -1.6.Besides, compared to multi-agent and ensemble baselines, our improvement is also pronounced and consistent.
For GSM8K SVAMP #Call Avg. GPT3.5 GPT4 L-7B L-13B L-70B GPT3.5 GPT4 L-7b L-13B L-70B
Better generality across different LLMs and tasks.From commercial LLMs (e.g., GPT4) to open-source models (Llama-2), and from reasoning to generative tasks, our strategy exhibits robust generalizability.Concretely, from the perspective of LLM, Self-Contrast achieves the best results on most models except Llama-2-7B.For instance, for GPT-3.5, the improvements are 7.8% on GSM8K and 9.2% on SVAMP, while for Llama-2-70B, the improvements are 11.6% and 9.3% respectively.As for Llama-2-7B, our performance is slightly lower than Self-consistency and Multi-Agent.This might be due to the weaker instruction-following capabilities of the Llama2-7B, making it challenging to contrast two inconsistent solutions.Besides task-wise, Self-Contrast applies to various task types, demonstrating high versatility.In contrast, Self-Consistency can not handle non-numerical tasks directly, e.g., translation, due to its voting mechanism (Table 5).Its variant strategies, SC-Select and SC-Reflect, lag significantly behind ours.</p>
<p>Fewer manual efforts and more reasonable call overheads.Compared to the multi-agent debate, Self-Contrast gains more significant improvements with less call overhead (&gt;10% reduction).From a unified perspective, it can be viewed as a multi-agent contrastive mechanism.Instead of a free-form debate among multiple agents, our strategy fosters a more explicit and purposeful debate by contrasting the differences between agents and summarizing the reasons for their disagreements.Moreover, Self-Contrast is flexible, dynamically designing multiple perspectives tailored to user requests, without the need for manually preconfiguring agent roles and quantities.</p>
<p>The Effect of the Different Components</p>
<p>The above results show that Self-Contrast inspires reflection more accurately and stably than direct evaluation.It encompasses a self-curated prompt process, which fosters diverse solving perspectives to mitigate self-evaluation biases.Besides, it involves a checklist generation process to facilitate re-examination.We analyze their effect as follows:</p>
<p>Self-curated Prompt Vs.Sampling Multiple Responses.Instead of self-curated prompt process, we directly sample multiple responses from LLMs for subsequent contrast and reflection.Figure A2 shows that the final accuracy improves as the number of sampled responses increases, yet it is still lower than Self-Contrast with self-curated prompts  process, where full strategy achieves 84.4% compared to the maximum of 81.8% when sampling 5 responses.We find that the top-n responses are sometimes strikingly similar, diminishing the effectiveness of the contrastive strategy.</p>
<p>Reflection Without Checklist.We eliminate the checklist generation process, i.e., directly instruct the LLM to reflect on the differences among perspectives.In Table A1, it brings a significant impact on mathematical reasoning (-3.5%), but a slight impact on translation (-0.1%),since translation tasks tend to focus more on local features.Even without a checklist, the LLM also can reflect based on the comparisons of lexical, syntactic.</p>
<p>Analysis</p>
<p>Reducing Invalid and Toxic Reflections</p>
<p>As mentioned in Table 2, due to overly confident or highly random in the self-evaluate process, vanilla self-reflection contains a large amount of invalid (✗→ ✗: 20.3%) or toxic reflections (✓→ ✗: 4%).Therefore we investigate how Self-Contrast improves these two scenarios on GSM8K.As shown in Table 6, we observe that with Self-Contrast, the occurrences of invalid and toxic cases significantly reduced.In particular toxic cases decreased by 78.9% and invalid cases by 30.8% using GPT3.5.In contrast, the SC-Reflect does not significantly mitigate either of these scenarios.</p>
<p>The results indicate that through exploration, comparison, and summarization, the uncertainty in the reflection process is greatly reduced, thereby enhancing the error-correction capability of the LLM.</p>
<p>Contrasting Incorrect Solutions is also Instructive</p>
<p>Self-Contrast inspires reflection by contrasting the differences.An intuitive explanation is that the errors in different responses are dissimilar or randomized, so they can be used to compare with each other and eliminate uncertainties or biases.To verify this, we sample 200 questions from GSM8K,  each manually annotated with a correct solution, two incorrect solutions with similar errors (e.g., Error1), and an incorrect solution with a different error (Error2).We design four experiments: 1. Self-evaluate one incorrect solution followed by reflection.2. Self-Contrast a correct and an incorrect solution.3. Self-Contrast two similar incorrect solutions.4. Self-Contrast two dissimilar incorrect solutions.Table 7 shows that contrasting a correct and an erroneous solution, or contrasting two incorrect solutions with different errors both yield significant enhancements of 13.5% and 5.4%.In contrast, comparing two solutions with similar errors does not result in perceptible changes.This result aptly explains that the improvement of Self-Contrast stems from contrasting the differences between dissimilar solutions.Therefore, even if candidate solutions are both incorrect, as long as their errors are different, Self-Contrast has the potential to eliminate errors.In other words, Self-Contrast can mitigate the random errors arising from the inherent uncertainty of the LLM.</p>
<p>Diverse Solving Perspectives Maximize Contrast Effect</p>
<p>Prior analysis indicates that only contrasting dissimilar solutions can foster reflection.Reviewing our strategy, we employ a self-curated prompt process to create multiple solving perspectives ( § 3.1), thereby providing diverse solutions for subsequent</p>
<p>comparison.Here, we analyze the distribution of perspectives generated by this process in Figure 3.For most requests, the LLM generates four prompts.</p>
<p>We also analyze the frequency of keywords within these perspective's names.For mathematical reasoning, the LLM indeed adaptively designs numerous unique solving perspectives, then generating a variety of results.These dissimilar results can maximize the efficacy of our contrastive strategy.</p>
<p>Discussions</p>
<p>Self-contrast switches the critique objective into a contrastive task.We transform the selfevaluation into a process of comparing differences, explicitly altering the attention distribution of the LLM.The LLM is required only to identify the differences between two solutions, without judging right or wrong.This process is less influenced by the biases inherent in LLMs, as the objective is contrasting rather than evaluation.Besides, in Table 7, LLMs are instructed to contrast two incorrect solutions with different errors which also improves reflection results.The results in Section 2.3 also precisely verify this conjecture.By simply transforming from direct evaluation to contrastive evaluation, we enhance the effectiveness of reflection (75.8 to 77.5 on GSM8K), with more significant results (0.66 to 0.09).In Tables 4 and 5, our self-contrast approach achieved more significant improvements.</p>
<p>Contrasting results can help LLMs notice overlooked details and biases.After contrasting the differences between the two solutions, we summarize these differences into a checklist, thereby explicitly prompting the LLM to focus on the logical pitfalls and other issues underlying these differences.This allows LLMs to engage in reflection more clearly and purposefully.</p>
<p>As shown in Figure 2, LLM generates different translations for the user's request: "This plan was shot to death", and "This plan was axed".The former is a rigid translation that fails to grasp the metaphor embedded in the military term.After contrasting two different translations, LLMs believe they should scrutinize the source sentence for metaphors and ensure the translation aligns with the conventions of English expression.</p>
<p>Conclusion</p>
<p>We conduct a comprehensive investigation into the inherent reflection capabilities of LLMs.Our find-ings reveal a notable challenge: in the absence of external feedback, LLMs struggle to correct errors in previous responses on their own.After analyzing their self-evaluate process, we discover that LLMs are unable to accurately evaluate prior solutions and often provide overconfident or inconsistent feedback, which impedes reflection.To mitigate this, we introduce Self-Contrast, a contrastive strategy that inspires reflection by contrasting the differences between multiple perspectives, providing an informative checklist for reflection.Our experiments show that Self-Contrast performs well across a variety of scenarios and with different LLMs.</p>
<p>Limitations</p>
<p>For some smaller-scale LLMs, their instructionfollowing capability is weaker, hindering their potential to conduct precise comparisons and reflection.In such scenarios, the effectiveness of Self-Contrast might be slightly inferior to ensemble strategies.For instance, the performance of Self-Contrast with Llama2-7B is marginally lower than self-consistency.A viable approach is to utilize an external tool to compare differences between multiple perspectives, rather than LLM itself.For instance, we explore utilizing sequences comparison library difflib2 to contrast two generated equations (e.g., differ.compare(a+b÷c,a-b÷c)) or some rulebased strategy to compare two responses.It can provide us with more accurate and flexible comparisons at different granularity (e.g., character level).We leave this as future work.A.3 Self-Evaluate Vs.Self-Contrast Self-Contrast inspires reflection by contrasting the differences, rather than evaluating directly.The underlying assumption is contrast is more accurate and stable than direct evaluation for LLM.To validate this, we conduct an experiment using 200 samples from GSM8K, each containing a correct ACC.On GSM8K(%)</p>
<p>Self-curated Prompt Process Vs.Sampling Top-N Responses</p>
<p>Self-Contrast With Sampling Top-N Responses</p>
<p>Self-Contrast With Self-curated prompts: 84.4</p>
<p>Self-Reflection: 75.8</p>
<p>Number of Sampling Responses</p>
<p>Figure A2: We replace the self-curated prompt process with a simple strategy: directly sampling top-n responses for contrast.We observe that as N increases, the performance also improves, yet it still remains lower than self-contrast with the self-curated prompts.All results are conducted on GSM8K using GPT-3.5.Table A1: We eliminate the checklist generation process, instructing the LLM to directly reflect on the differences from contrasting multiple perspectives.Besides, we also analyze the impact of different selecting strategies.</p>
<p>and an incorrect solution.We design two tasks: Taks 1: Contrasting two solutions.Task 2: Evaluating the incorrect solution.We manually check the results of two tasks, i.e., whether LLM can perform contrast or evaluate correctly.As shown in Figure A3, we observe contrasting is more accurate than direct evaluating (171 correct Vs.140 incorrect).</p>
<p>Further, we divide all samples into four cases: 1. both tasks are correct.2. Contrasting: correct, Evaluating: wrong.3. Contrasting: wrong, Evaluating: correct.4. Both are wrong.In Figure A3, the results show that when LLM can correctly evaluate a solution, it is often able to contrast correctly, with few exceptions (only 8 samples for Evaluating Correct Only).Notably, in 39 cases, the LLM fails in direct evaluation but succeeds in contrast.These results indicate that contrasting two solutions is more accurate and stable than direct evaluation, leading to more reliable results.</p>
<p>A.4 Ablation Study For Selection Strategy</p>
<p>As introduced in Section 3.2, we cluster multiple responses generated by the self-curated process and then select the cluster center from each category for contrast.We design four different selection strategies. 1) Random Selecting: We randomly choose K responses from all candidates.2) Clustering + Random Selecting: We first cluster all responses into k categories, then randomly select one from each category.3) Clustering + LLM Selecting: Similarly, we first cluster all responses into k categories, then instruct the LLM to choose a potentially correct response from each category.4) Clustering + Negative Perspective: We first instruct the LLM to consider what are common errors for the user request.Then LLM should intentionally generate an imperfect solution based on these common errors.Finally, we instruct the LLM to select one response from each category that is least similar to the intentionally generated imperfect solution.As shown in Table A1, we observe that compared to Self-Contrast, the performance of several selection strategies experiences a certain degree of decline.</p>
<p>B Experiments Details B.1 Benchmarks</p>
<p>Mathematical Reasoning: We leverage multiple datasets with different complexity and languages, including GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021) as benchmarks to evaluate performance.Notably, GSM8K presents higher levels of difficulty, encompassing complex mathematical operations, while SVAMP is slightly simpler and consists of combinations of addition, subtraction, multiplication, and division.</p>
<p>Creative Translation In addition to mathemat-ical reasoning tasks, we introduce a generation task: creative translation.We utilize the Com-monMT (He et al., 2020), which includes a vast body of Chinese-to-English pair examples.Unlike conventional translations, most samples contain non-standard expressions such as idioms and metaphors, necessitating an understanding of local cultural and linguistic habits for accurate translation.Following the Multi-agent debate (Liang et al., 2023), we adopt the samples with "hard" categories from CommonMT as testing benchmarks.</p>
<p>B.2 Other Details</p>
<p>In the Self-Curated Prompt phase, we limit LLMs to design at least two different prompts and a maximum of nine prompts for each request.In selecting stage (Section 3.2), we set k to 3, which means that all perspective results are divided into three categories, and then we select a result from each category.We instruct LLM sequentially output comparisons among three results, subsequently synthesizing these differences into a comprehensive checklist in a single pass, eliminating the need for multiple prompts.Besides, due to the diversity of translation tasks, we also introduce a negative perspective for translation.Specifically, we instruct LLMs to consider what common errors might be made for the user request, then actively adopt a careless persona to generate an incorrect response with some common mistakes.The result of this negative perspective serves as a negative demonstration for subsequent selection and reflection.</p>
<p>C Baseline Prompts</p>
<p>Standard Prompt We use a simple prompt for CoT Prompt and self-consistency baselines.For each experiment, we run 10 times and averaged their results.Different requests may require some unique solving perspectives.We design a self-curated prompts process, enabling LLMs to design their prompts based on specific user requests.The prompt for the self-curated process is as follows: Reasoning Task: You are a math specialist who specializes in math solving from diverse perspectives .Given a math question , you need to carefully analyze the question and dynamically generate several useful prompt instructions .These prompt instructions should be diverse and also useful for math -solving .These prompt instructions are used to guide the language model to think in different ways , attention to different emphases , and reason from different perspectives for more accurate math solving .</p>
<p>For instance , you can adopt multifaceted thinking ( logical thinking , lateral thinking , analogical thinking , etc .), different reasoning perspectives (e.g., top -down , bottom -up , step -by -step ) , and different emphases of concern , ( entity words , numbers , units , percentages , math knowledge , etc ) for input question in prompt instruction .</p>
<p>Here are some guidance rules for Prompt Generation : 1. Tone Requirement : Please generate prompt instructions in the third person .2. Content Requirement : Each prompt instruction should adopt a different way of thinking , or focus on a different perspective , or different emphases to solve the question .3. Number Requirement : Dynamically generate the most valuable 2 to 9 prompt instructions based on the input math question .4. Format Requirement : Each prompt instruction should start with ### and end with @@@ . 5. Others : Prompt instructions should focus on math solving .So don 't ask any other irrelevant questions in the prompt .</p>
<p>Here is an example : The math question is : Mark works at his job for 8 hours a day for 5 days a week .He used to make $10 an hour but they raised his pay by $2 per hour .How much does he make a week ?</p>
<p>Output : bottom -up perspective : ### As a mathematician , you have to solve the given problem from a bottom -up perspective .Please focus initially on the foundational elements of the problem .Start with the simplest parts and their interrelations .Progressively build upon these foundational components , joining them together until a complete solution emerges</p>
<p>The input math question is {input}.</p>
<p>Please generate the most suitable prompts :</p>
<p>D.2 Prompt for Contrasting Process</p>
<p>Translation Task: You are an expert translator .Given some candidate English translations for a Chinese source sentence , you should carefully compare the difference between each two translations in terms of semantics , syntax , words (e.g., nouns and verbs ) , and any other aspects .</p>
<p>When you compare , you to consider the following questions : 1: Are there differences between the two translations ?2: Where are the differences ?3: What causes these differences ?</p>
<p>D.3 Prompt For Reflection Stage</p>
<p>We record all candidate responses, their differences, and the checklist in a JSON format.The whole prompt for math reasoning is as follows:</p>
<p>Reflection Instruction: Given a math question , multiple inconsistent solutions , their differences in their solving processes , and a checklist .You should revise the inconsistent solving step for each solution , eliminate the differences , and output a new solving process for each solution .</p>
<p>Guidance Rules for Reflection : 1. Please check carefully according to the requirements on the checklist .It helps you to resolve conflicts between different solutions .2. When you finish revising inconsistent solutions , please ensure all revised solutions should have the same answer .If not , please revise again until all inconsistencies are removed , and all candidates are consistent .3. Please output all revised solutions in JSON format as input , without any other text .</p>
<p>The math question is {question}.The candidate solutions and their discrepancy are as follows : { " Candidate ": { " result_1 ": { " answer ": "{answer1}" , " solution ": "{solution1}"} , " result_2 ": { " answer ": "{answer2}" , " solution ": "{solution2}"} , " result_3 ": { " answer ": "{answer3}" , " solution ": "{solution3}"} , .... } , " Discrepancy ": { " difference_1_2 ": { " source ": " result_1 ", " target ": " result_2 ", " relation ": "{difference}" }, " difference_1_3 ": { " source ": " result_1 ", " target ": " result_3 ", " relation ": "{difference}" }, " difference_2_3 ": { " source ": " result_2 ", " target ": " result_3 ", " relation ": "{difference}" }, .... } } Checklist: {Directive1, Directive2 ,....} Please revise each inconsistent solution .</p>
<p>E Related Works</p>
<p>E.1 Self-correction Ability of LLM Recently, one exciting discovery is that LLMs appear to possess advanced cognitive intelligence: self-correction, where LLMs can refine their previous responses based on feedback (Shinn et al., 2023;Madaan et al., 2023;Paul et al., 2023).This capacity endows LLMs to harness external feedback, or even self-evaluated feedback to refine the prior responses (Welleck et al., 2022;Kadavath et al., 2022;Chen et al., 2023d;Kim et al., 2023;Xi et al., 2023;Ganguli et al., 2023;Pan et al., 2023;Nathani et al., 2023).This capacity, particularly when it is solely reliant on inherent reflection, has generated significant interest in the academic community.It appears that a simple iterative prompt strategy could facilitate self-correction in an LLM-based system.However, recent studies (Huang et al., 2023b;Stechly et al., 2023;Liang et al., 2023;Valmeekam et al., 2023) have cast doubt on LLM's inherent reflection capability.Their research indicates that without external feedback, LLMs have difficulties in amending prior responses.</p>
<p>E.2 Prompting for Better Problem-Solving</p>
<p>Drawing on cognitive science, human reasoning involves two different reasoning patterns: breadth reasoning, i.e., exploring various reasoning perspectives, and depth reasoning, which involves continually refining ideas and minimizing errors.Based on this concept, we can view previous prompting strategies as either breadth or depth reasoning.Self-consistency and some contemporaneous works (Wang et al., 2023d;Huang et al., 2022;Yoran et al., 2023;Jain et al., 2023;Chen et al., 2023c) mimic breadth reasoning by sampling diverse reasoning processes and voting the final answer, while self-reflection, abstraction reasoning strategies (Shinn et al., 2023;Madaan et al., 2023;Paul et al., 2023;Zheng et al., 2023a;Wang et al., 2023a;Yoran et al., 2023;Zheng et al., 2023b;Xu et al., 2023b;Shridhar et al., 2023) represent depth reasoning, refining reasoning through iterative prompting strategy.Except for these, Self-Verification (Weng et al., 2022) designs a reverse generation from the answer to given conditions, which is widely used in machine translation (Edunov et al., 2018).Cohen et al. (2023); Mündler et al. (2023) propose a method for detecting self-contradictions or factual errors in responses to enhance quality.However, our Self-Contrast combines both breadth and depth reasoning.It creates multiple perspectives to enhance the breadth of reasoning and also reflects on the differences for better depth reasoning, offering more reliable problem-solving.</p>
<p>E.3 Agent-based Methods</p>
<p>Recent studies (Li et al., 2023;Deshpande et al., 2023;Xu et al., 2023a;Du et al., 2023;Xiong et al., 2023) have found that when an LLM is assigned a specific role personas, it can generate higher-quality responses.This suggests that LLMs are powerful enough, and the appropriate prompt can elicit this capability.Moreover, recent works (Wang et al., 2023e;Fu et al., 2023;Liang et al., 2023;Schick et al., 2022;Dong et al., 2023;Park et al., 2023;Liu et al., 2023) have utilized a multi-role dialogue to collaborate or debate with each other for a more comprehensive response.Furthermore, some studies (Chen et al., 2023b;Chan et al., 2023;Huang et al., 2023a;Chen et al., 2023a;Hong et al., 2023;Wu et al., 2023b) have integrated this concept with complex tasks such as code generation by decomposing a complex task into several sub-tasks and employing multiple agents with different identities for each sub-task.However, most agent-based approaches necessitate careful manual design of each agent's role and pattern of interaction.Our approach, in contrast, does not require pre-defined agents' roles and numbers by humans, as it is entirely designed by the LLMs based on the user request, offering greater flexibility.</p>
<p>E.4 Learning Mathematical Reasoning</p>
<p>Mathematical reasoning is the key to achieving embodied intelligence (Zhang et al., 2021(Zhang et al., , 2022c)).In recent years, mathematical reasoning has become a significant benchmark (Cobbe et al., 2021;Hendrycks et al., 2021) to evaluate the capabilities of artificial intelligence models.Within the paradigm of supervised learning, a vast amount of research (Xie and Sun, 2019;Patel et al., 2021;Jie et al., 2022;Zhang et al., 2022bZhang et al., , 2023b) ) has been dedicated to translating human language into mathematical equations.In the era of LLMs, the advent of Chain-of-Thought and other prompting strategies have notably augmented the reasoning capabilities (Zhu et al., 2023;Yuan et al., 2023b;Frieder et al., 2023;Zhou et al., 2022).</p>
<p>Prompting Method PAL and Program-of-Thoughts (Gao et al., 2022;Chen et al., 2022) separate the computation and reasoning process using code as the intermediate process.Mathprompter, Auto-Model (Imani et al., 2023;Zhao et al., 2023) encourage LLMs to generate diverse reasoning paths in different forms simultaneously, including text (CoT), code (PAL), and symbols (Equation) for a higher confidence answer.Automatic-CoT, Complexity-CoT, Synthetic Prompt and Boosted Prompt (Zhang et al., 2022d;Fu et al., 2022;Shao et al., 2023;Pitis et al., 2023) enhance reasoning performance by optimizing the selection of demonstrations within the prompt.Tree-of-thought and Self-Evaluation (Yao et al., 2023;Xie et al., 2023) extend the CoT into a search tree, obtaining more accurate answers through self-evaluation.</p>
<p>Finetuning-based Method Another domain of study involves methods based on finetuning.These approaches involve finetuning open-source models, such as LLaMA, by incorporating insights from sophisticated closed-source LLMs.The fine-tuning approaches (Yuan et al., 2023a;Luo et al., 2023;Yue et al., 2023;Wang et al., 2023b;Yu et al., 2023;Gou et al., 2023) also have the potential to improve the mathematical reasoning capabilities of LLMs.The essence of fine-tuning is centered around the development of quality datasets comprising question-response pairs.Additionally, process-supervised training methods Lightman et al. (2023); Wang et al. (2023c) can also enhance the reasoning abilities of the LLMs.</p>
<p>The source sentence involves some military terminology.Please interpret the underlying meaning from a military perspective….Result 3 : The plan was chopped This plan was aborted This plan was aborted This plan was aborted Difference For Result1 and Result2:</p>
<p>Correct and an Incorrect Solutions 83.6 -Two Incorrect Solutions with Similar Error 70.9 -Two Incorrect Solutions with Different Error 75.5Table7:We conduct comparisons across four cases on a subset of GSM8K.LLM self-evaluates or self-contrasts different initial responses and reflects on their results.</p>
<p>Figure 3 :
3
Figure 3: Left: The distribution of the prompt number generated when Self-curated.Right: We visualize the top-20 keywords and frequencies in the prompt name.</p>
<p>Figure A1 :
A1
Figure A1: The Reflection Accuracy with Different LLM for Initial Response.Left: different LLMs provide initial responses when GPT3.5 is utilized for Evaluation and Revision.Center: different LLMs provide initial responses when Llama2-70B is utilized for Evaluation and Revision.Right: different LLMs provide initial responses when Llama2-13B is utilized for Evaluation and Revision.The results indicate that LLMs are easily influenced during reflection.LLM is predisposed to trust previous responses over diligently examining and correcting errors.</p>
<p>Table 1 :
1
Table1, we observe no significant accuracy changes before and after reflection.For We calculate the average accuracy of the ten experiments for pre-and post-reflection: Pre Acc.⇒ Post Acc.We also report the accuracy change's significance level (P-value) for ten trials, where ∆ = P ost − P re.A larger P indicates a less significant improvement.
Math ReasoningTranslationGSM8KSVAMPCommonMTGPT493.9⇒95.1 93.0⇒91.570.1⇒69.8P for ∆ &gt; 00.19330.58460.5426GPT3.576.6⇒75.8 79.8⇒80.569.1⇒69.3P for ∆ &gt; 00.66130.43060.4420davinci-00351.1⇒49.663⇒63.562.4⇒63.8P for ∆ &gt; 00.69880.47290.2009Llama2-70B 52.6⇒49.366⇒63.063.2⇒62.2P for ∆ &gt; 00.84160.95210.7723Llama2-13B 28.3⇒29.8 42.2⇒42.562.5⇒61.5P for ∆ &gt; 00.38550.25080.4690Llama2-7B19.8⇒17.0 37.5⇒36.153.7⇒48.8P for ∆ &gt; 00.95780.57700.7492</p>
<p>Table 2
2: We consider three reflection behaviors based onthe correctness of the pre-and post-reflection: Invalid,Valid, and Toxic. Besides, we summarize each sample'sfeedback into four categories when self-evaluation.StrategyGSM8K SVAMP CommonMTSelf-Evaluate w/ top-1-0.80.70.2P for ∆ &gt; 00.6613 0.43060.4420Self-Evaluate w/ top-20.120.80.16P for ∆ &gt; 00.4192 0.34570.3745Self-Contrast w/ top-20.92.50.45P for ∆ &gt; 00.0933 0.01180.0457</p>
<p>Table 3
3: We report the accuracy change (∆) betweenpost-and pre-reflection for 3 settings and t-test valuefor ∆&gt;0. Self-evaluate: Directly evaluate the initialresponse. Self-contrast: Contrast the difference betweentwo responses and generate a checklist for reflection.</p>
<p>Table 4 :
4
↑0.7 93.8 ↓0.1 21.6 ↑1.8 30.5 ↑2.2 53.1 ↑0.5 80.2 ↑0.4 93.3 ↑0.3 37.7 ↑0.2 41.9 ↑1.7 65.6 ↑0.4 2 Self-Reflection 75.8 ↓0.8 95.1 ↑1.2 17.0 ↓2.8 31.8 ↑3.5 49.3 ↓3.3 80.5 ↑0.7 91.5 ↓1.5 36.1 ↓1.4 42.5 ↑2.3 63.0 ↓3 The performance on mathematical reasoning.Self-Consistency (SC-Vote, -Select, -Reflect) samples eight responses and then performs voting, selecting, or reflection.For the Multi-Agent, we configure three agents to engage in a three-round debate.↑ and ↓ means accuracy changes over the CoT prompt.L-denotes Llama2-chat.↓0.5 52.1 ↓1.6 62.8 ↑0.3 63.0 ↓0.2 -SC-Reflect 69.0 ↓0.1 54.0 ↑0.3 62.2 ↓0.3 63.2 ↑0 Multi-Agent 69.9 ↑0.8 51.9 ↓1.8 63.1 ↑0.6 65.8 ↑2.6 Hint-Prompt 69.6 ↑0.5 54.2 ↑0.5 62.5 ↑0 64.6 ↑1.4 Self-Contrast 70.7 ↑1.6 52.1 ↓1.6 62.8 ↑0.3 66.7 ↑3.5
CoT Prompt76.693.919.828.352.679.893.037.540.2661ExpertPrompt77.3 3Self-Consistency-SC-Vote83.5 ↑6.9 94.2 ↑0.3 21.4 ↑1.6 37.6 ↑9.3 61.1 ↑8.5 84.6 ↑4.8 92.5 ↓0.5 45.2 ↑7.7 53.7 ↑13.5 72 ↑68-SC-Select76.3 ↓0.3 93.1 ↓0.8 16.2 ↓3.6 28.6 ↑0.3 54.6 ↑2.0 81.2 ↑1.4 93.2 ↑0.2 35.1 ↓2.4 38.9 ↓1.3 66.5 ↑0.59-SC-Reflect75.8 ↓0.8 93.3 ↓0.6 19.2 ↓0.6 29.1 ↓0.8 53.7 ↑1.1 81.1 ↑1.3 93.4 ↑0.4 32.5 ↓5 34.2 ↓6 67.5 ↑1.59Multi-Agent83.8 ↑7.2 93.5 ↓0.4 23.8 ↑4 34.9 ↑6.6 59.6 ↑7.0 84.1 ↑4.3 93.2 ↑0.2 42.5 ↑5 49.2 ↑9.0 70.1 ↑4.19Hint-Prompt78.8 ↑2.2 93.7 ↓0.2 18.3 ↓1.5 27.8 ↓0.5 59.6 ↑779.3 ↓0.5 93.1 ↑0.1 38.8 ↑1.3 40.6 ↑0.4 67.6 ↑1.6 6.7Math-Prompt79.6 ↑3.0 93.9 ↓0.0 19.5 ↓0.3 30.6 ↑2.3 59.8 ↑7.2 81.2 ↑1.4 93.6 ↑0.6 37.2 ↓0.3 41.5 ↑1.3 68.7 ↑0.5 4.5Self-Contrast84.4 ↑7.8 95.4 ↑1.5 20.5 ↑0.7 42.3 ↑9.2 64.2 ↑11.6 89.0 ↑9.2 94.0 ↑1 44.5 ↑7 54.6 ↑14.4 75.3 ↑9.3 7.8GPT3.5L-7BL-13BL-70BCoT Prompt69.153.762.563.2ExpertPrompt69.6 ↑0.5 53.8 ↑0.1 62.9 ↑0.4 63.4 ↑0.2Self-Reflection69.3 ↑0.2 48.8 ↓4.9 61.5 ↓1.0 62.2 ↓1.0Self-Consistency-SC-Vote-----SC-Select68.6</p>
<p>Table 5 :
5
The performance on Creative Translation.</p>
<p>Table 6 :
6
Self-Contrast is evaluated on two cases.</p>
<p>Figure A3: We compare the results of the Evaluating and Contrasting using two pie charts.It shows Contrasting is more accurate and stable than direct Evaluating.
Both Incorrect: 21 SamplesEvaluating Correct Only: 8 SamplesContrasting Result: 171(Correct) 29(Incorrect)Evaluating Result:140(Correct) 60(Incorrect)ContrastingCorrect Only:39 SamplesBoth Correct:132 Samples</p>
<p>Do you think the previous response is correct or not , and if not please point out where is wrong .
the problem and provide feedback .Evaluation: Please carefully examine theprevious responses for correctness , andRevision: Please refine your responseprovide detailed feedback .based on the feedback .10:Revision: Please refine the previousEvaluation: Please check your previousresponse based on the feedback .response for correctness and whether it2:can be further enhanced .Evaluation: Please review your previousresponses for any errors , and provideRevision: Please further refine yourdetailed feedback .response based on the feedback . If youdon ' t feel it is necessary then restateRevision: Please refine the previousthe previous responseresponse based on the feedback . If thereare no questions , you can repeat theD Our Promptprevious solution 3:D.1 Prompt for Self-Curated ProcessEvaluation: Revision: Please refine the previousresponse .4:Evaluation: Please carefully evaluate thequality of the previous response andpoint out if you feel something is notappropriateRevision: Please carefully consider thecomments in the feedback and re -generatethe answer .5:Evaluation: Please double -check theprevious response for any errors . Ifthere are any errors , please point themout .Revision: Please read the feedbackcarefully , and improve your answer .6:Evaluation: There may have been somemistakes with your previous response , soplease double -check and find out themistake . If you think there are noerrors at all , please just reply , "Exactly correct ".Revision: Please refine your response . Ifyou think it 's acceptable , then justrepeat your last response .7:Evaluation: Please check that yourprevious response matches the question .Math Reasoning: You are a math teacher .Please point out if it does not fitLet us solve the math question step bystep . The question is {input}.Revision: Please refine your responsebased on the feedback . If the feedbackCreative Translation: You are an expertpoints out something that is not perfecttranslator , please translate Chineseplease fix it !into English accurately . The Chinese8:sentence is {input}.Evaluation: Please consider whether yourresponse addresses the problem . If notor if there is an error please point itoutRevision: Please reflect based on thefeedback and improve your response .9:Evaluation: Please assess in detail1:whether your previous response solves
Reflection Prompts We designed 10 prompts for the self-reflection baseline.Each experiment follows Initial response-Evaluation-Revision pattern.The prompt for the Initial response remains consistent with previous experiments (Standard Prompt).</p>
<p>https://github.com/google-research/bleurt, 
https://docs.python.org/3/library/difflib. html
AcknowledgmentsThis work is supported by the National Natural Science Foundation of China (No. 62376245), the Key Research and Development Program of Zhejiang Province, China (No. 2024C01034), the Fundamental Research Funds for the Central Universities, and National Key Research and Development Project of China (No. 2018AAA0101900).Appendix A Complementary ExperimentsA.1 Detail For Manual Feedback EvaluationWe provide the details of manual evaluation in Table 2. Specifically, we categorize reflection results into four categories: Invalid (wrong -&gt; wrong), Valid (wrong -&gt; right), Toxic (right -&gt; wrong), and Other(right -&gt; right)based on their answer.Subsequently, we manually assess the quality of feedback within each reflection category.For instance, for a Toxic reflection case, we devise ten self-evaluation prompts, prompting LLMs to conduct a self-evaluation on their initial response, generating ten feedbacks.These feedbacks are manually checked for correctness and consistency.The criteria for classification are as follows:• If more than seven feedbacks accurately identify the errors in the initial response, we classify it into the first category: I. Accurately Identifies Errors.Please note that the feedback only needs to accurately identify the errors without necessarily correcting them.• If more than seven feedbacks indicate that the initial response has no errors, we categorize it into the fourth category: IV.Overconfidence -No Revision Required.• If, among the ten feedbacks, there are more than three different opinions, e.g., the first feedback suggesting there are no errors, another identifying error-1, and yet another pointing out error-2....For this scenario, we classify it into the third category: III.Cannot Output Consistent Feedback.In this scenario, self-evaluation exhibits significant randomness.• The remaining cases are classified into the second category: II.Stubbornly Offers Erroneous Feedback.The entire human evaluation process is conducted by two senior PhD students.One is responsible for categorization, while the other verifies the categorization again.A.2 LLM is More Likely to Trust Previous ResponseWe investigate whether LLMs are prone to uncritically trusting previous responses during reflection, rather than meticulously examining and rectifying errors.Typically, self-reflection often contains three stages, i.e., initial response, self-evaluate, and revision stage.We employ different LLMs to provide a poorer quality response as the initial response for the subsequent two stages.We observe whether this affects the results of the reflection, e.g., we replace gpt3.5→gpt3.5→gpt3.5 with Llama-2-70b→gpt3.5→gpt3.5.If LLMs tend to place undue trust in prior responses, the efficacy of the final reflective process will be adversely impacted.However, as shown in FigureA1, the reflection results are severely impacted by the quality of the initial response.E.g., compared with using gpt3.5 for three phases, Llama2-70b→gpt3.5→gpt3.5 exhibits a marked decrease (-8.4% for GSM8K).Furthermore, we also observe the weaker the LLM replaced, the poorer the performance after reflection.It suggests that LLMs tend to trust the initial solution rather than detect and revise the errors during the self-evaluate phase.
Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Language Models are Few-Shot Learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020NeurIPS</p>
<p>Chateval: Towards better llm-based evaluators through multi-agent debate. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shan Zhang, Jie Fu, Zhiyuan Liu, ArXiv, abs/2308.072012023</p>
<p>Autoagents: A framework for automatic agent generation. Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Jie Börje F Karlsson, Yemin Fu, Shi, arXiv:2309.172882023aarXiv preprint</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Cheng Qian, Chi-Min Chan, Yujia Qin, Ya-Ting Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou, ArXiv, abs/2308.108482023b</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, ArXiv, abs/2211.125882022</p>
<p>Universal self-consistency for large language model generation. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, Denny Zhou, 2023c</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, ArXiv, abs/2304.051282023d</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, ArXiv, abs/2204.02311Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, and others. 2022</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, ArXiv, abs/2110.141682021</p>
<p>Lm vs lm: Detecting factual errors via cross examination. Roi Cohen, May Hamri, Conference on Empirical Methods in Natural Language Processing. 2023Mor Geva, and Amir Globerson</p>
<p>Toxicity in chatgpt: Analyzing persona-assigned language models. A Deshpande, S Vishvak, Murahari, A Tanmay Rajpurohit, Karthik Kalyan, Narasimhan, ArXiv, abs/2304.053352023</p>
<p>Selfcollaboration code generation via chatgpt. Yihong Dong, Xue Jiang, Zhi Jin, Ge Li, ArXiv, abs/2304.075902023</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, ArXiv, abs/2305.143252023</p>
<p>Understanding back-translation at scale. Sergey Edunov, Myle Ott, Michael Auli, David Grangier, arXiv:1808.093812018arXiv preprint</p>
<p>Mathematical capabilities of chatgpt. Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, J J Berner, ArXiv, abs/2301.138672023</p>
<p>Improving language model negotiation with self-play and in-context learning from ai feedback. Yao Fu, Hao-Chun, Tushar Peng, Mirella Khot, Lapata, ArXiv, abs/2305.101422023</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao-Chun, Ashish Peng, Peter Sabharwal, Tushar Clark, Khot, ArXiv, abs/2210.007202022</p>
<p>The capacity for moral self-correction in large language models. Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil, Anna E Lukovsiut, Anna Chen, Azalia Goldie, Catherine Mirhoseini, Danny Olsson, Dawn Hernandez, Dustin Drain, Eli Li, Ethan Tran-Johnson, John Perez, Jamie Kernion, Jared Kerr, Joshua D Mueller, Landau, Karina Kamal Ndousse, Liane Nguyen, Michael Lovitt, Nelson Sellitto, Elhage, ' Noem, Saurav Mercado ; Kundu, Scott Kadavath, Shauna Johnston, Sheer El Kravec, Tamera Showk, Timothy Lanham, T J Telleen-Lawton, Tristan Henighan, Yuntao Hume, Zac Bai, Benjamin Hatfield-Dodds, Dario Mann, Amodei, ArXiv, abs/2302.07459Jared Kaplan2023Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan; Nicholas Joseph, Sam McCandlish, Tom B. Brown, Christopher Olah, Jack Clark, Sam Bowman</p>
<p>Pal: Program-aided Language Models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, ArXiv, abs/2211.104352022</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, ArXiv, abs/2309.174522023</p>
<p>The box is in the pen: Evaluating commonsense reasoning in neural machine translation. Jie He, Tao Wang, Deyi Xiong, Qun Liu, 10.18653/v1/2020.findings-emnlp.327Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, Jacob Steinhardt, ArXiv, abs/2103.038742021</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan P Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zi Hen Lin, Liyang Zhou, ArXiv, abs/2308.00352Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. 2023. Metagpt: Meta programming for multi-agent collaborative framework. </p>
<p>Enhancing large language models in coding through multi-perspective selfconsistency. Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, Nan Duan, ArXiv, abs/2309.172722023a</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, ArXiv, abs/2210.116102022</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 2023b</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, H Shrivastava, ArXiv, abs/2303.053982023</p>
<p>Self-consistency for open-ended generations. Siddhartha Jain, Xiaofei Ma, ArXiv, abs/2307.06857Anoop Deoras, and Bing Xiang. 2023</p>
<p>Learning to reason deductively: Math word problem solving as complex relation extraction. Zhanming Jie, Jierui Li, Wei Lu, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, T J Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zachary Dodds, Nova Dassarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, John Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, ArXiv, abs/2207.05221Jared Kaplan2022Catherine Olsson, Sam Ringer, Dario Amodei, Tom B. Brown, Jack Clark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, ArXiv, abs/2303.174912023</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Camel: Communicative agents for "mind" exploration of large language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, ArXiv, abs/23052023. 19118</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harrison Burda, John Edwards ; Leike, Schulman, ArXiv, abs/2305Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJan. 20050</p>
<p>Dynamic llm-agent network: An llmagent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, ArXiv, abs/2310.021702023</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, ArXiv, abs/2308.095832023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, ArXiv, abs/2303.176512023</p>
<p>Slobodan Jenko, and Martin T. Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. Niels Mündler, Jingxuan He, ArXiv, abs/2305.15852</p>
<p>Maf: Multi-aspect feedback for improving reasoning in large language models. Deepak Nathani, David Wang, Liangming Pan, William Yang, Wang , ArXiv, abs/2310.124262023</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Wenda Michael Stephen Saxon, OpenAI. 2022. Chatgpt. OpenAIDeepak Xu, OpenAI. 2022. Chatgpt. OpenAIXinyi Nathani, OpenAI. 2022. Chatgpt. OpenAIWilliam Wang, OpenAI. 2022. Chatgpt. OpenAIWang Yang, OpenAI. 2022. Chatgpt. OpenAIArXiv, abs/2308.03188Gpt-4 technical report. Liangming Pan. 2023. 2023</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, ArXiv, abs/2304.03442Generative agents: Interactive simulacra of human behavior. 2023</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, ArXiv, abs/2304.01904Refiner: Reasoning feedback on intermediate representations. 2023</p>
<p>Boosted prompt ensembles for large language models. Silviu Pitis, Michael Ruogu Zhang, Andrew Wang, Jimmy Ba, ArXiv, abs/2304.059702023</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, M Lomeli, Luke Zettlemoyer, ArXiv, abs/2302.04761Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. </p>
<p>Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, ArXiv, abs/2208.11663Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. Peer: A collaborative language model. </p>
<p>Synthetic prompting: Generating chain-of-thought demonstrations for large language models. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, ArXiv, abs/2302.006182023</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 2023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, ArXiv, abs/2303.113662023</p>
<p>Kumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu Wang, Ping Yu, Ram Pasunuru, Mrinmaya Sachan, Jason Weston, Asli Celikyilmaz, arXiv:2311.07961The art of llm refinement: Ask, refine, and trust. 2023arXiv preprint</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, Armand Aur'elien Rodriguez, Joulin, ArXiv, abs/2302.13971Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and Efficient Foundation Language Models. </p>
<p>Hugo Touvron, Louis Martin, Kevin R Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M Bikel, Lukas Blecher, Cantón Cristian, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony S Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel M Khabsa, A V Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Pushkar Mihaylov, Igor Mishra, Yixin Molybog, Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, R Smith, Xia Subramanian, Binh Tan, Ross Tang, Adina Taylor, Jian Williams, Puxin Xiang Kuan, Zhengxu Xu, Iliyan Yan, Zarov, ArXiv, abs/2307.09288Yuchen Zhang, Angela Fan. Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Can large language models really improve by self-critiquing their own plans?. Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati, ArXiv, abs/2310.081182023</p>
<p>Sequence-level certainty reduces hallucination in knowledge-grounded dialogue generation. Yixin Wan, Fanyou Wu, Weijie Xu, Srinivasan, Sengamedu, arXiv:2310.187942023arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, ArXiv, abs/2305.040912023a</p>
<p>Making large language models better reasoners with alignment. Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui, ArXiv, abs/2309.021442023b</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, R X Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, Zhifang Sui, Math-shepherd: Verify and reinforce llms step-by-step without human annotations. 2023c</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, 2023dICLR 2023 poster, abs/2203.11171</p>
<p>Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, 2023e</p>
<p>Chain of Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, ArXiv, abs/2211.000532022</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, ArXiv, abs/2212.095612022</p>
<p>Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.04671Visual chatgpt: Talking, drawing and editing with visual foundation models. 2023aarXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, ArXiv, abs/2308.081552023b</p>
<p>Self-polish: Enhance reasoning in large language models via problem refinement. Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, Xuanjing Huang, ArXiv, abs/2305.144972023</p>
<p>Decomposition enhances reasoning via selfevaluation guided decoding. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Mingsung Kan, Junxian He, Qizhe Xie, ArXiv, abs/2305.006332023</p>
<p>A goal-driven tree-structured neural model for math word problems. Zhipeng Xie, Shichao Sun, IJCAI. 2019</p>
<p>Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Bing Qin, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Expertprompting: Instructing large language models to be distinguished experts. Benfeng Xu, An Yang, Junyang Lin, Quang Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao, ArXiv, abs/2305.146882023a</p>
<p>Re-reading improves reasoning in language models. Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-Guang Lou, ArXiv, abs/2309.062752023b</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, ArXiv, abs/2305.106012023</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, ArXiv, abs/2210.036292022</p>
<p>Answering questions by meta-reasoning over multiple chains of thought. Tomer Ori Yoran, Ben Wolfson, Uri Bogin, Daniel Katz, Jonathan Deutch, Berant, ArXiv, abs/2304.130072023</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Long Long, Yu , Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zheng Li, Adrian Weller, Weiyang Liu, ArXiv, abs/2309.122842023</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Cheng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, ArXiv, abs/2308.018252023a</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, ArXiv, abs/2304.020152023b</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, ArXiv, abs/2309.056532023</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang, Glm-130b: An Open Bilingual Pre-trained Model. 2023ICLR 2023 poster</p>
<p>Opt: Open Pre-trained Transformer Language Models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, ArXiv, abs/2205.010682022a</p>
<p>Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang, arXiv:2306.07209Data-copilot: Bridging billions of data and humans with autonomous workflow. 2023aarXiv preprint</p>
<p>Multi-view reasoning: Consistent contrastive learning for math word problem. Wenqi Zhang, Yongliang Shen, Yanna Ma, Xiaoxia Cheng, Zeqi Tan, Qingpeng Nong, Weiming Lu, 10.18653/v1/2022.findings-emnlp.79Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022b</p>
<p>An expression tree decoding strategy for mathematical equation generation. Wenqi Zhang, Yongliang Shen, Qingpeng Nong, Zeqi Tan, Yanna Ma, Weiming Lu, 10.18653/v1/2023.emnlp-main.29Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023b</p>
<p>A closed-loop perception, decision-making and reasoning mechanism for human-like navigation. Wenqi Zhang, Kai Zhao, Peng Li, Xiao Zhu, Yongliang Shen, Yanna Ma, Yingfeng Chen, Weiming Lu, 10.24963/ijcai.2022/654Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-222022c</p>
<p>Learning to navigate in a vuca environment: Hierarchical multi-expert approach. Wenqi Zhang, Kai Zhao, Peng Li, Xiaochun Zhu, Faping Ye, Wei Jiang, Huiqiao Fu, Tao Wang, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021. 2021</p>
<p>Automatic chain of thought prompting in large language models. Mu Zhuosheng Aston Zhang, Alexander J Li, Smola, ArXiv, abs/2210.034932022d</p>
<p>Automatic model selection with large language models for reasoning. Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Qizhe Xie, ArXiv, abs/2305.143332023</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, ArXiv, abs/2304.097972023a</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed Cheng, Quoc V Huai Hsin Chi, Denny Le, Zhou, ArXiv, abs/2310.061172023b</p>
<p>Why does chatgpt fall short in answering questions faithfully?. Shen Zheng, Jie Huang, Kevin Chen, -Chuan Chang, ArXiv, abs/2304.105132023c</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, ArXiv, abs/2205.10625Quoc Le, and Ed Huai hsin Chi2022</p>
<p>Solving math word problems via cooperative reasoning induced language models. Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7210 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7210</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7210</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-258960634</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2301.05149v2.pdf" target="_blank">Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models</a></p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural theory-of-mind? on the limits of social intelligence in large LMs <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>Dissociating language and thought in large language models: a cognitive perspective <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7210",
    "paper_id": "paper-258960634",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural theory-of-mind? on the limits of social intelligence in large LMs",
            "rating": 2,
            "sanitized_title": "neural_theoryofmind_on_the_limits_of_social_intelligence_in_large_lms"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "Dissociating language and thought in large language models: a cognitive perspective",
            "rating": 1,
            "sanitized_title": "dissociating_language_and_thought_in_large_language_models_a_cognitive_perspective"
        }
    ],
    "cost": 0.00716375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models</p>
<p>Lingjun Zhao 
University of Maryland-College Park ♣ Princeton University ♢ Microsoft Research</p>
<p>Khanh Nguyen 
University of Maryland-College Park ♣ Princeton University ♢ Microsoft Research</p>
<p>♠ ♢ Hal 
University of Maryland-College Park ♣ Princeton University ♢ Microsoft Research</p>
<p>Daumé Iii 
University of Maryland-College Park ♣ Princeton University ♢ Microsoft Research</p>
<p>Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models</p>
<p>Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking. This insight leads us to augment them with better models of the listener and obtain a significant boost of 11% in success rate in guiding real humans. Our work advocates for having a principled procedure for aligning language models with humans that involves (i) formulating task-oriented capabilities, (ii) devising a method to quantify their deficiency, and (iii) iteratively improving them.</p>
<p>Introduction</p>
<p>To communicate successfully with humans, language models must possess cognitive capabilities similar to those that facilitate human communication. Examining the cognitive capabilities of language models is notoriously challenging because the operations of these models are largely unintelligible to humans. Psychologists faced similar challenges when investigating human cognition, and have devised various behavioral tests * The first two authors contribute equally.</p>
<p>to diagnose human cognitive capabilities (Premack and Woodruff, 1978;Wimmer and Perner, 1983;Baron-Cohen et al., 1985;Gopnik and Astington, 1988). Recent work (Sap et al., 2022;Kosinski, 2023;Ullman, 2023) applies these tests to evaluate large language models by inputting the tests to these models as prompts and verifying whether they behave like a normal human would.</p>
<p>While this approach is helpful for understanding the general limitations of language models, it has two potential drawbacks. First, it is applicable to only large language models that can comprehend human-written prompts, entangling linguistic capability with reasoning capability. Second, it shows that a language model can or cannot demonstrate certain mental skills, but does not imply that the model would employ those skills to perform a downstream task. For example, passing false-belief tests does not guarantee that a model will reason about the interpretation of the readers when generating summaries. In general, scoring high on psychological tests may not be sufficient to ensure language models would behave like humans in real-life scenarios.</p>
<p>In this work, we take a different approach to evaluating the cognitive capabilities of language models. We define and evaluate task-oriented cognitive capabilities, which are human-like capabilities that a model actually employs to perform the task it is designed for. Enhancing these capabilities thus warrants improved performance on the task. To identify these capabilities, we build on two lines of work from socio-cognitive science: Bayesian models of cooperative communication (Wang et al., 2020;Goodman and Frank, 2016;Shafto et al., 2014) and studies on how humans implement Bayesian reasoning (Sanborn and Chater, 2016;Sanborn et al., 2010;Vul et al., 2014;Mamassian et al., 2002). We propose a mathematical cognitive model called bounded pragmatic speaker, which can reasonably characterize the rea-soning processes of both humans and language models. Casting humans and language models in the same way enables us to juxtapose their cognitive capabilities. We mathematically formulate two capabilities that a bounded pragmatic agent must possess in order to generate optimally pragmatic utterances. These conditions correspond to wellknown cognitive capabilities of humans: (i) the ability to efficiently generate relevant utterances (the search capability) (Bloom and Fischler, 1980;Gold et al., 2000;Trosborg, 2010) and (ii) the ability to accurately simulate the listener's interpretations of their utterances (the pragmatic capability) (Premack and Woodruff, 1978;Gopnik and Astington, 1988;Tomasello, 2019;Call and Tomasello, 2011;Frank and Goodman, 2012). We design a simple procedure to quantitatively evaluate these capabilities of a language model. To evaluate each capability, we compute the task performance gap between the model and an oracle model, which is identical except that the evaluated capability of this model is at human level. Figure 1 illustrates our procedure, which theoretically can be applied to any language model.</p>
<p>We evaluate various language models on a navigation instruction generation problem (Anderson et al., 2018b), where a model generates English instructions to guide real humans in photo-realistic 3D environments. 1 Our evaluation reveals an interesting finding: all evaluated agents possess relatively efficient search capability but inadequate pragmatic capability. We improve the pragmatic capability of the evaluated models by enabling them to reason probabilistically about human listeners (Andreas and Klein, 2016;Fried et al., 2018a), employing state-of-the-art instructionfollowing agents (Magalhaes et al., 2019;Shen et al., 2022;Hong et al., 2021) as models of human listeners. We obtain significant improvement in success rate over the original agents, shrinking the gap with human performance on held-out data by 36%. Towards eliminating the remaining gap, we illustrate with empirical evidence a major challenge in developing better listener models. Specifically, when the instruction-following agents are employed as listener models for the instruction-generating agent, they are required to evaluate AI-generated instructions, which may be significantly different from human-generated 1 Our human-evaluation dataset and interface are publicly released at https://lingjunzhao.github.io/coop_ instruction.html.  Figure 1: We propose a framework called bounded pragmatic speaker which can characterize pragmatic reasoning in both humans and language models (a). A bounded pragmatic speaker is composed of a base speaker S base , representing prior knowledge that helps generate instructions efficiently, and a theory-of-mind (ToM) listener L ToM , a hypothetical model of how the real listener interprets instructions. Viewing language models and humans through this unifying lens enables comparing their cognitive capabilities (b). To evaluate a capability of a model, we compare it with an oracle model which is identical except that the evaluated capability is at human level. The outcome of our evaluation can better inform the future direction for improving the model (c).</p>
<p>instructions. Hence, a standard supervised-learning training scheme that only exposes these models to human-generated instructions would be inadequate for learning reliable models. We thus call for construction of novel datasets, algorithms, and evaluation methods for developing the pragmatic capability of language models.</p>
<p>Related Work</p>
<p>Navigation Instruction Generation. Instruction generation has been commonly studied in navigation settings (Anderson et al., 1991;Byron et al., 2009;Koller et al., 2010;Striegnitz et al., 2011;Goeddel and Olson, 2012;Fried et al., 2018a,b). The Matterport3D simulator and the accompanying datasets (R2R (Anderson et al., 2018b), R4R (Jain et al., 2019), andRxR (Ku et al., 2020)) offer more challenging settings by combining photo-realistic scenes with long, verbally rich instructions. Recent work on evaluating instruction generation agents (Zhao et al., 2021) reveals the ineffectiveness of standard learning and modeling approaches to this problem.  improve the accuracy and interpretability of instructions in the RxR setting. Kamath et al. (2023) leverage this model to synthesize additional data for training instructionfollowing agents. Our work aim to offer useful principles to further improve these models.</p>
<p>Mathematical Models of Human Communication. Different from communication within agents (Lazaridou et al., 2020;Roman Roman et al., 2020), human communication is a cooperative act (Grice, 1975;Scott-Phillips, 2014;Tomasello, 2019). Pragmatic communication in humans may involve different cognitive capabilities like basic understanding of language and social rules (Trosborg, 2010) and reasoning about the physical world (Bender and Koller, 2020) and human behavior (Enrici et al., 2019;Rubio-Fernandez, 2021). Our work describes similar capabilities but provides a formal mathematical description. Development of mathematical models of human communication have been greatly useful for understanding human behaviors (Ho et al., 2016;Sumers et al., 2022) and building communication agents (Andreas and Klein, 2016;Fried et al., 2018a,b;FAIR, 2022;Lin et al., 2022;Zhu et al., 2021;Bao et al., 2022). Wang et al. (2020) unify these models under an optimal-transport framework. The model we propose in this work is a generalized version capturing the essence of these models.</p>
<p>Evaluating Cognitive Capabilities of Neural Networks. Many benchmarks for evaluating the cognitive capabilities of AI-based agents have been created, focusing on theory-of-mind capabilities Nematzadeh et al., 2018), grounding (Lachmy et al., 2022Udagawa and Aizawa, 2019;Haber et al., 2019), or commonsense reasoning (Talmor et al., 2019;Levesque et al., 2012;Zellers et al., 2019;Sap et al., 2019). Large language models have demonstrated exceptional performance on following human instructions and solving complex reasoning tasks (Bubeck et al., 2023;Anil et al., 2023), raising the question of whether their cognitive capabilities are similar or as advanced as those of humans. Mahowald et al. (2023) advocate for separating formal competence (knowledge about linguistic rules and patterns) from their func-tional competence (knowledge about the world usage in the world) when assessing these models. Our bounded pragmatic speaker framework mathematically formalizes this description, allowing for quantitative evaluation of these competencies. Recent work (Sap et al., 2022;Kosinski, 2023;Ullman, 2023;Hu et al., 2023) examines cognitive capabilities of large language models through tests inspired by human psychological tests. The goal of these studies is to determine the limits of large language models, potentially calibrating the expectation on them. On the other hand, our focus is to devise a method that can be applied to language models of any size and benchmark cognitive capabilities that are relevant for accomplishing a specific task.</p>
<p>Problem Setting</p>
<p>We are concerned with instruction generation: learning a speaker agent r that generates language instructions to guide a human listener h to reach states in an environment.</p>
<p>Human Listener. We imagine a human listener h acting in a partially observed environment with states s. The human does not have access to states but only observations o h and takes actions a h . An instruction u ∈ U is a language utterance consisting of words. A trajectory e = (s 1 , o 1 , a 1 , · · · , s T , o T , a T ) is an execution of an instruction. The human can follow instructions to generate trajectories in the environment. For example, in an indoor navigation setting, upon hearing "go the kitchen and stop next to the oven", a human walks to the specified location. We define L h (e | u) as the probability that the human generates e upon hearing u.</p>
<p>Speaker Agent. In each task, the speaker agent first imagines an intended trajectory e ⋆ = (s 1 , o r 1 , a r 1 , · · · , s T , o r T , a r T ), which specifies a path to get to an intended goal state s T from the human's current state s 1 . Because the human's actions and perception may differ from those of the speaker, they may not be able to comprehend e ⋆ even if it is presented to them. Thus, the speaker needs to translate the trajectory into an instructionû that the human can understand and follow. To do so, it implements a language model S r (u | e), and an inference algorithm Gen(S r , e) to craft instructions based on the model (e.g., greedy or beam-search decoding). The speaker's objective is to generate instructions that maximize the expected chance of the listener reconstructing the intended trajectories arg max
Sr E e ⋆ [L h (e ⋆ | Gen(S r , e ⋆ ))]
(1)</p>
<p>Evaluation. The speaker is evaluated using a dataset D eval of held-out trajectories. For each trajectory e ⋆ k ∈ D eval , we generate an instruction u k = GEN(S r , e ⋆ k ). The instruction is then presented to a (real) human listener to follow, producing a trajectory e h k ∼ L h (· |û k ). The performance of the speaker, denoted by ρ(r), is the average similarity, Ψ, between the human-generated and the intended trajectories:
ρ(r) ≜ 1 |D eval | e ⋆ k ∈D eval Ψ(e h k , e ⋆ k )(2)
We will specify the choices for the metric Ψ in the experimental setup section ( §6).</p>
<p>Task-Oriented Cognitive Capabilities</p>
<p>Human have evolved highly effective cognitive capabilities for communication. How can we endow a speaker agent with similar capabilities and quantify the degree of resemblance between its capabilities and those of a human?</p>
<p>We propose a mathematical framework that reasonably characterizes the human cognitive process for instruction generation ( §4.1). We show that this model can also describe the operation of language models, which allows us to compare them with humans on specific cognitive capabilities. We identify two capabilities that are requisite for any agent implementing our framework to generate optimal instructions ( § 4.2), and introduce an evaluation scheme for collating these capabilities ( §4.3).</p>
<p>A Mathematical Cognitive Model of Instruction Generation</p>
<p>To formulate how humans generate instructions, we build on mathematical models of cooperative communication (Wang et al., 2020;Goodman and Frank, 2016;Shafto et al., 2014). We consider a general version where a speaker agent constructs a pragmatic speaker model S prag (u | e) based on two constituents: a base speaker model S base (u | e) and a theory-of-mind (ToM) listener model L ToM (e | u). The base speaker represents general knowledge of the agent about the world and the language it speaks. The ToM listener reflects situated knowledge about the listener, simulating how they would behave in the environment given an instruction. A pragmatic speaker aims to maximize the chance of the listener interpreting its instruction correctly, but it is still influenced by its general knowledge (e.g., social biases, language style). Formally, it is defined as:
S prag (u | e) ∝ L ToM (e | u)S base (u | e) (3)
To convey an intended trajectory e ⋆ , this speaker utters an instruction of maximum probability under its model:
u prag ≜ arg max u∈U S prag (u | e ⋆ ) = arg max u∈U L ToM (e ⋆ | u)S base (u | e ⋆ ) (4)
Humans as bounded pragmatic speakers. The pragmatic speaker model accounts for human behaviors highly accurately on problems where U is a small discrete space (Frank and Goodman, 2012). However, in problems like instruction generation where U is an unbounded set of linguistic expressions, it is unlikely that humans, which are known to be agents with bounded rationality (Simon, 1957), are able to compute the optimal utterance in Eq 4 exactly. A hypothesis, supported by empirical evidence, is that humans perform approximate inference via Monte-Carlo sampling (Sanborn and Chater, 2016;Sanborn et al., 2010;Vul et al., 2014;Mamassian et al., 2002). Applying this hypothesis to our setting, we derive a more practical model of how human generate instructions, in which they perform the search for the best utterance on only a subspace U sub of U defined by a set of candidates sampled from S basê
u bounded-prag ≜ arg max u ∈ U sub ⊂ U L ToM (e ⋆ | u) (5) where U sub = {u i ∼ S base (· | e ⋆ ) | 1 ≤ i ≤ N }.
We call an agent that generates instructions according to Eq 5 a bounded pragmatic speaker ( Figure 2). For such a speaker, instruction generation involves two tasks: candidate generation (performed by S base ) and candidate evaluation (performed by L ToM ). The former task ensures that the generation of an instruction is efficient, while the latter guarantees the generated instruction conveys the intended meaning to the human listener.</p>
<p>Formulating Task-Oriented Cognitive Capabilities</p>
<p>What cognitive capabilities enable humans to generate effective instructions? Viewing humans as bounded pragmatic speakers allows us to mathematically characterize those capabilities. Specifi-  Figure 2: The cognitive process of a bounded pragmatic speaker. In every task, the speaker first imagines a trajectory it wants to convey to the human listener. To reduce the search space, it then uses the base speaker to generate a small set of relevant candidate instructions. After that, it employs the theory-of-mind listener to simulate how the human listener would follow each instruction in the candidate set. The speaker finally elects the candidate instruction that causes the theory-of-mind listener to generate the trajectory most similar to the intended trajectory. The output instruction is finally sent to the human listener for a real execution in the environment.</p>
<p>cally, we require a bounded pragmatic speaker to be able to output the optimal utterance, i.e. satisfyinĝ
u bounded-prag = u ⋆ ≜ arg max u L h (e ⋆ | u) (6)
where L h is the human listener.</p>
<p>For this equation to hold, the constituent models S base and L ToM of the bounded pragmatic speaker must meet certain conditions. The condition for S base is that the candidate set it generates must contain the optimal instruction, i.e. u ⋆ ∈ U sub . This condition requires S base to be capable of quickly generating candidates and placing high probability on u ⋆ so the instruction can be found by sampling a few candidates. We refer to this capability as the search capability.</p>
<p>Meanwhile, the condition for L ToM is that it has to rank u ⋆ first among the candidates in U sub . Meeting this condition demands having the capability of constructing a mental emulation of the human listener and simulating the actions of the listener after receiving an instruction. We refer to this capability as the pragmatic capability.</p>
<p>The search and pragmatic capabilities are orthogonal and complementary. An agent with flawless pragmatic capability can evaluate the goodness of instructions given to it, but may not be able to efficiently generate good instructions by itself. In contrast, an agent with effective search capability can quickly bring to attention highly relevant utterances but cannot select the best one to output if its ToM model is erroneous.</p>
<p>Evaluating Task-Oriented Cognitive Capabilities</p>
<p>We have defined two cognitive capabilities that are requisite for humans in instruction generation. In this section, we will prove that a language model can also be cast as a bounded pragmatic speaker. Hence, we can compare it with a human on the two cognitive capabilities.</p>
<p>Language models as bounded pragmatic speakers. We consider a speaker agent r that learns a language model S r (u | e) and runs an inference algorithm to compute an outputû infer = GEN(S r , e ⋆ ) ≈ arg max u∈U S r (u | e ⋆ ). Generative LSTM-or Transformer-based models that implement greedy or beam-search decoding are examples of this agent. We make the following assumption about the inference algorithm. 2</p>
<p>Assumption (Better-than-sampling inference algorithm). We assume the inference algorithm is better at finding arg max u∈U S r (u | e ⋆ ) than drawing a small number of N samples from S r . Formally, let γ be the probability of drawing e ⋆ and a set of N instructions from S r such that
S r (û infer | e ⋆ ) &gt; max u∈U sub S r (u | e ⋆ ), wherê u infer = GEN(S r , e ⋆ ).
We assume that γ is large for a small integer N &gt; 0.</p>
<p>If this assumption holds, then with high probability, the agent r behaves identically to a bounded pragmatic speaker that computes its output as:
u ≜ arg max u∈U r sub S r (u | e ⋆ ) (7) U r sub ≜ {û infer } ∪ {u i ∼ S r | 1 ≤ i ≤ N } (8)
This agent uses S r as both the base speaker S base and ToM listener L ToM . Due to our assumption, on most inputs, the agent outputsû infer , similar to the original agent. We employ this bounded pragmatic speaker as the proxy for the original agent in comparisons with humans, and also refer to it as r.</p>
<p>Evaluation scheme. To evaluate a cognitive capability (search or pragmatic) of a speaker r, we compute the performance gap between it and an oracle agent that is at human level on the evaluated capability, but is equally good as it is at the other capability. Specifically, we define r ⋆ search to be an oracle speaker that employs S r as the ToM model but is given a "gold" candidate set U ⋆ cand that always contains a human-generated reference instruction u ⋆ . It selects its output as follows
u ⋆ search ≜ arg max u∈U ⋆ cand S r (u | e ⋆ )(9)
This agent has similar pragmatic capability as r but human-level search capability. Next, we construct r ⋆ pragmatic , an oracle that generates candidates using S r but employs a human L h to rank the candidates
u ⋆ pragmatic ≜ arg max u∈U r sub L h (e ⋆ | u)(10)
with U r sub from Eq 8. The search capability of r ⋆ pragmatic is as good as r but its pragmatic capability is that of a human.</p>
<p>We calculate the prospective performance gain (PPG) with respect to each capability as follows
PPG search (r) ≜ ρ(r ⋆ search ) − ρ(r)(11)PPG pragmatic (r) ≜ ρ(r ⋆ pragmatic ) − ρ(r)(12)
where ρ is the performance on held-out data (Eq 2 of § 3). Each metric computes the potential improvement if the corresponding capability is upgraded to match with that of a human. Comparing the two metrics reveals which of the two capabilities of r is currently more deficient and informs future development direction for the agent. For example, if PPG search (r) is large and PPG pragmatic (r) is small, it means that r is scoring the candidate instructions highly accurately but it is bad at finding high-score instructions. In this case, developers may want to focus on devising a more effective inference algorithm to improve the search capability of r. On the other hand, if r estimates poorly calibrated scores, signified by PPG pragmatic (r) being large, enhancing its inference algorithm is fruitless, but endowing it with a module that simulates the listener's behavior more accurately would boost its performance.</p>
<p>Improving Pragmatic Capability with Ensemble Instruction-Following Agents</p>
<p>In cases where our evaluation scheme indicates that the pragmatic capability of a language model is deficient, we improve it by installing a better ToM listener model. A common approach to learning this listener model is to use the same dataset used for learning the speaker model (Andreas and Klein, 2016;Fried et al., 2018a,b). We argue that this approach has a potential drawback. A ToM model learned in this way is only exposed to humangenerated input instructions. At deployment time, it would likely experience a covariate shift because as a ToM model, the model is then asked to score instructions generated by a speaker model, not by humans. These instructions may be incorrect, ungrammatical, or may simply have a different style than human-generated instructions. This covariate shift would hamper the model's judgement. Our preliminary experiments (Appendix §A.6) confirm that using a listener trained on only human-generated inputs as the ToM model hurts rather than improves the performance of various speakers. We show that this problem can be alleviated by employing ToM models that have calibrated uncertainty on unseen instructions. We obtain calibrated models through ensembling (Lakshminarayanan et al., 2017): we train listener modelsL (k) (e | u), k = 1 . . . K, each on a random 90% subset of the training data with different random initial seeds.</p>
<p>We also leverage access to a simulation of the environment to construct better ToM models. Note that the probability that a ToM model L ToM assigns to an instruction can be seen as an expectation of a binary metric: L ToM (e ⋆ | u) = E e∼L ToM (·|u) [1{e = e ⋆ }], which does not award credit if e overlaps only partially with e ⋆ . We propose two augmentations: (i) replace the binary metric with a soft metric Ψ(e, e ⋆ ) that can measure partial similarity between trajectories and (ii) approximate the expectation by executing listenerŝ L (k) in the simulated environment to sample trajec-tories. Our final model selects its instruction as:
u augment-ToM ≜ arg max u∈U r sub L ToM (e ⋆ | u) (13) L ToM (e ⋆ | u) ∝ 1 KM K k=1 M j=1 Ψ(e j (L (k) , u), e ⋆ ) U r sub ≜ {û infer } ∪ {u i ∼ S r | 1 ≤ i ≤ N }where
e(L, u) denotes a trajectory sampled from a listener model L conditioned on an instruction u, and M is the number of trajectories we sample from each listener. Essentially, the score L ToM (e ⋆ | u) of each candidate instruction is the average performance metric of K listeners, each of which attempts to follow the instruction M times.</p>
<p>Experimental Setup</p>
<p>Environment and Dataset. We employ Matter-port3D (Anderson et al., 2018b), a photo-realistic simulator of the visual perception of a person walking in an indoor environment. At any location, an agent is provided with RGB images capturing the 360-degree panoramic view when looking from that location.</p>
<p>We train and evaluate our models using the Room-to-Room (R2R) language-based navigation dataset. Each data point was collected by asking an English-speaking crowd-worker to write a verbal description of a path in an environment. The dataset is split into a training set (61 environments, 4,675 paths), a seen validation set (environments seen during training, 340 paths), and an unseen validation set (11 environments unseen during training, 783 paths). We train the models using the training set and perform model selection on the unseen validation set. Performance metrics are computed on the seen validation set.</p>
<p>Speaker Models. We evaluate three speaker architectures: (1) a decoder-only GPT-2 pre-trained on text (Radford et al., 2019); (2) an LSTM encoder-decoder (Shen et al., 2022); (3) a Transformer encoder-decoder (Vaswani et al., 2017). Parameters of the latter two models are randomly initialized. Details are in Appendix §A.2.</p>
<p>Human Evaluation. We evaluate each speaker model on 75 paths in the seen validation data split. In the end, we have annotated 1,200 instructions generated by 16 different systems (humans, 3 speaker models, and their ablated and augmented versions). To evaluate a speaker model, we present its generated instructions to a human annotator and ask them to follow the instructions to navigate in Matterport3D environments. We adapt the PanGEA tool 3 to setup a web navigation interface and create a task on Amazon Mechanical Turk (MTurk). We recruit 213 human evaluators in total. More details about the setting are given in Appendix §A.5.</p>
<p>Performance Metrics. The quality of a speaker is determined by the similarity between the intended trajectories and the actual trajectories that the human evaluators generate by following the speaker's instructions. We compute these similarity metrics: Success rate (SR) averages binary indicators of whether the final location of a humangenerated trajectory is within three meters of the final location of the intended trajectory; SPL (Anderson et al., 2018a) weights the success indicator with the ratio between the intended traveling distance and the actual one; and NDTW and SDTW are metrics based on dynamic time-warping alignment (Magalhaes et al., 2019), capturing the similarity between two point sequences. NDTW computes only a sequence similarity score while SDTW weights the score with the success indicator.</p>
<p>Experiments</p>
<p>We investigate the following questions:</p>
<p>(a) How well do the speakers perform on our problem? We find that, despite implementing advanced architectures, these speakers perform poorly compared to human speakers. (b) What causes their performance deficiency?</p>
<p>Using our evaluation scheme, we identify that the speakers possess decent search capability but inadequate pragmatic capability. (c) Can we improve the speakers by equipping them with better ToM listeners? We employ ensembles of state-of-the-art instructionfollowing agents as ToM listeners for the speakers, and obtain significant improvements. (d) What are the challenges in bridging the performance gap with human speakers? We show that instruction-following agents trained with only human-generated instructions are not optimal for serving as ToM listener models.</p>
<p>How well do the speakers perform on our problem? As seen in Figure 3, there is a wide margin between the agent speakers and the human </p>
<p>Fine-tuned GPT-2 EncDec-LSTM EncDec-Transformer</p>
<p>Humans (R2R annotators) Figure 3: Performance of different speakers on heldout evaluation data, grouped by performance metrics (NDTW, SR, SPL, SDTW). Human speakers are annotators of the R2R dataset. There is a considerable gap between model and human speakers. instructors. The best model speaker (EncDec-Transformer) lags behind the humans by 21.6 NDTW points. The encoder-decoder architecture with cross-attention of EncDec-Transformer outperforms the decoder-only self-attention architecture of GPT-2 (+11.7 NDTW), indicating that fusing the vision and language features too early in an architecture may be detrimental. On the other hand, EncDec-Transformer leads over EncDec-LSTM by 4.1 points NDTW, suggesting that the Transformer architecture is more effective than LSTM in this problem.</p>
<p>What causes the speakers' deficiency? Next, we investigate whether the lack of search or pragmatic capability is responsible for the deficiency of the speakers. The prospective performance gains presented in Figure 4 show that it is under-performed pragmatic capability that primarily causes the models to perform poorly. Specifically, while equipping the models with oracle search capability only improves their performance by 9.4% on average, granting them oracle pragmatic capability nearly doubles their performance metrics. In fact, the search capability of the models is already as good as that of the humans we employ, because the models with oracle pragmatic capabil-ity achieve even slightly higher NDTW scores than the human speakers.</p>
<p>Can we improve the speakers by equipping them with better ToM models? Following the procedure described in Section §5, we train state-of-theart instruction-following agents to serve as ToM listener models for the speakers. Performances of different combinations of speakers and listeners are given in Table 1. We see the largest improvement (+7.9 NDTW) over the best base speaker (EncDec-Transformer) by augmenting this speaker with an ensemble of 10 EnvDrop-CLIP listeners as the ToM model. In Figure 5, we show a qualitative example where having a ToM listener enables the speaker to generate a more accurate instruction. More examples are shown in Appendix §A.7. We observe that ensemble models consistently outperform single models. More results about the effectiveness of ensemble listeners compared to single listeners are given in Appendix §A.6.</p>
<p>What are the challenges in bridging the performance gap with human speakers? Despite the promising improvements, there remains a large gap of 17.9 NDTW points between our best speaker and the human speakers. As suggested by Figure 4, this gap can be closed by developing accurate ToM models. We argue that optimal ToM models cannot be simply obtained by learning optimal instructionfollowing agents, because the latter is learned to execute human-generated instructions while the former is asked to rank model-generated instructions. To illustrate the difference, we measure the agreement between human and model listeners on instructions generated by different speakers. We define the agreement score between a human L h and a modelL as
Agreement(L h ,L) = Average u∈D eval (NDTW(e h (u),ê(u))) (14)
where e h (u) andê(u) are the trajectories generated by L h andL given u, respectively, and D eval denotes the R2R seen validation set.</p>
<p>As seen in Table 2, the listener agents agree more with the humans on human-generated instructions than on model-generated ones. The results imply even an optimal instruction-following agent can fail to improve a base speaker in the presence of an input distribution mismatch. We thus advocate for developing ToM models that are robust or can adapt quickly against covariate shift, and for evaluating  (Majumdar et al., 2020) 38.9 (▲ 1.2) 39.8 (▼ 5.5) 46.2 (▼ 3.2) Ensemble of 10 EnvDrop-CLIP (Shen et al., 2022) 37.8 (▲ 0.   performance of these models on model-generated instructions.</p>
<p>Conclusion</p>
<p>This work introduces a framework for analyzing task-oriented cognitive capabilities of instructiongeneration language models. We show that insights from the analysis are helpful in directing development on these models. Our results highlight the necessity of constructing better ToM models for improving these models. We argue that learning accurate ToM listener models is met with novel, distinct challenges. We hope that our findings will motivate the community to focus more on evaluating task-oriented cognitive capabilities and to create datasets, training methods, and evaluation procedures for enhancing the pragmatic capability of language models.</p>
<p>Limitations</p>
<p>Our work is predicated on hypothetical models of human cognition. These models are still under development by cognitive scientists and need to be validated in more realistic domains. Our method assumes access to a simulation of the environment, which may be costly to construct in some domains. In general, instruction generation agents pose substantial risk to humans. Previous studies have shown that humans can become overly reliant on AI instructions and commit disastrous mistakes (Robinette et al., 2016). It is thus important for practitioners to comprehend the constraints of our experimental setting. Our experiments take place in a coarse simulator of real-world indoor environments, which restricts the action and perception of the human listeners. Due to the expensive cost and the large number of agent variants, our human evaluation remains limited in terms of population scale and diversity, and the comprehensiveness of the questionnaires. As each instruction is only evaluated by a single human, we have not investigated the variance of the interpretation of the same instruction among different humans. In addition, human evaluators may "guess" a path even if a part of the instruction is misleading or impossible to follow. Hence, the path-similarity metrics may not reflect faithfully the quality of the instructions. Nevertheless, results shown in Table 4 of §A.5 indicates that instructions generated by our agents are almost as easy to interpret as those generated by humans. But again, these results are still subject to the constraints of our annotator population. To deploy our method, practitioners should carefully re-evaluate its safety and effectiveness in conditions that closely emulate the deployment conditions.  </p>
<p>A Appendices</p>
<p>A.1 The Room-to-Room dataset</p>
<p>The R2R dataset (Anderson et al., 2018b) was originally created for training instruction-following agents. Each data point was collected by asking a crowd-worker to write a verbal description of a path in an environment. In the end, each path was annotated with three instructions. Each instruction contains 29 words on average. The dataset is split into a training set (61 environments, 4,675 paths), a seen validation set (340 paths) whose paths are sampled in the training environments, and an unseen validation set (11 environments unseen during training, 783 paths). We do not use the unseen test split because it does not provide ground-truth paths of the descriptions. We use the dataset consistent to their MIT License.</p>
<p>A.2 Implementation of Speaker Models</p>
<p>We train the speakers with a standard maximumlikelihood objective using the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 10 −4 . The speaker models take a sequence of visual observations and actions from the trajectory e ⋆ as input and output a text instruction u. The model is trained to estimate conditional probability S θ (u|e ⋆ ). We use grid search to select the model and training hyperparameters, and the best-found values are listed in Table 3.</p>
<p>Input. The input trajectory e ⋆ is a sequence of panoramic views and actions. Each panoramic view at time step t is represented by 36 vectors {o t,i } 36 i=1 , each of which is a visual feature vector extracted from a pre-trained vision model concatenated with orientation features describing the agent's current gaze direction. The image features of the GPT-2 model are extracted from a ResNet-152 model (He et al., 2016), whereas those of the encoder-decoder models are from a CLIP model (Radford et al., 2021). Each ground truth action a ⋆ t , which moves the agent to an adjacent location, is represented by image features from the gaze direction of the agent when looking towards that adjacent location, and orientation features capturing the direction of the adjacent location relative to the agent's current gaze direction.</p>
<p>Output. The output of a speaker model is a language instruction describing the input trajectory. At test time, the GPT-2 model employs beam search, and the encoder-decoder models generate instructions via greedy decoding (Shen et al., 2022).</p>
<p>Training Objective. We train the speakers with maximum-likelihood objective:
max θ (u ⋆ ,e ⋆ )∈D train |u ⋆ | t=1 log S θ (u ⋆ t | e ⋆ , u ⋆ &lt;t ) (15)
where θ is the speaker model parameters, u ⋆ t is t-th word of the ground-truth instruction, and u ⋆ &lt;t is the first t − 1 words of the instruction.</p>
<p>We select the best model based on the unseenvalidation BLEU score (Papineni et al., 2002) of the model-generated instructions with the respect to the ground-truth instructions.  </p>
<p>A.3 Fine-tuning GPT-2 Speaker Model</p>
<p>To represent the trajectory features as a sequence of feature vectors to feed into the GPT-2 model, we first average the view featuresō t for each time step:
o t = 1 36 36 i=1 o t,i(16)
We compute the input features e ⋆ t by concatenating the panoramic view features and ground truth action features:
e ⋆ t = <a href="17">ō t ; a ⋆ t </a>
The sequence of feature vectors e ⋆ representing a trajectory is calculated as follows
e ⋆ = <a href="18">tanh(e ⋆ 1 W ); · · · ; tanh(e ⋆ T W )</a>
where W is parameters of a linear layer. For the instruction u ⋆ , we perform an embedding look-up of its words. Then, we first prompt the model with e ⋆ and then train it to generate u ⋆ as a suffix.</p>
<p>A.4 Training Encoder-Decoder Speaker Models</p>
<p>Our EncDec-LSTM model follows the implementation of the speaker in Shen et al. (2022). We implement the EncDec-Transformer model by replacing the LSTM layers of the speaker model described in Tan et al. (2019) with Transformer layers (Vaswani et al., 2017).</p>
<p>A.5 Human Evaluation Interface and Data Collection</p>
<p>We pay the evaluator $5.20 per task which takes about 25 minutes, and the payment is decided by state minimum wage. For each task, we ask the evaluator to follow six instruction-following sessions. One of the six sessions, which appears in all tasks, is a quality-control test featuring an easy-tofollow human-written instruction. We only approve an evaluator if they navigate successfully to the goal destination in this test. Following Zhao et al. (2021), we instruct the judges to not explore the environments unnecessarily and not wander back and forth unless they are lost. We record the trajectories created by the human and use them to compute the performance metrics. Figure 6 shows the interface for our human evaluation to collect annotations, which we adapted from the PanGEA tool 4 consistent with their Apache License v2.0. After a human evaluator finishes following an instruction, we recorded the path they generate and compute similarity metrics with respect to the ground-truth path. After the instructionfollowing sessions, we ask each evaluator to assess the interpretability of the instructions by asking them how easy (or difficult) it was for them to follow the instruction. We provide four rating levels ranging from "1: I couldn't follow any part of the instruction" to "4: very easy, the instructions gave  accurate and sufficient information for me to follow". The answer of the evaluators is converted to a score between one and four. Table 4 shows the human evaluation results of the three speaker models we evaluated.</p>
<p>For the human evaluation survey, participants will be restricted to those fluent in English. There are no other restrictions for this study. Participants must be at least 18 years old. Before completing the survey, participants will be shown information about the task requirement: You are in a building, and are provided with a short set of instructions to navigate to a target location. Please follow the instructions as closely as possible. Do NOT explore the building unnecessarily and do NOT wander back and forth unless you are lost. Please read ALL of the instructions before you start moving.</p>
<p>We waive consent for this study for several reasons: 1) Minimal risk: The study collects minimal identifying information and there are no known risks for the subjects beyond everyday computer use. 2) Rights and welfare: All participants will be shown all information regarding task requirements before they complete our survey. They must consent to performing the task before they are shown the questions. 3) Practicality: Since the sessions are conducted online on a large scale, it would be infeasible to require all users to send a signed form. 4) Post participation information: We do not think there is any pertinent information that is not already shared with the participants before or during our experiments, so we do not feel it is necessary to provide any additional information after participation. PI information will be shared with the participants to enable them to obtain additional information about the study post completion.</p>
<p>For data anonymization, we removed the only identifying information, Amazon Mechanical Turk ID, after collecting the human annotation data. This information would also be removed for future dataset release. The dataset will be released under MIT license terms, which are compatible with those of the tools used to create it, and will be intended for research usage.</p>
<p>A.6 Single vs. Ensemble Listeners</p>
<p>As a preliminary experiment, we compare the effectiveness of a single and an ensemble of 10 VLN ⟳ BERT agents when serving as the ToM model of a speaker. Results in Figure 7 show that the ensemble listener is significantly better than the single listener for two different speakers.</p>
<p>A.7 Qualitative Examples</p>
<p>In Figure 8, we show additional qualitative examples where having a ToM listener enables the speaker to generate a more accurate instruction.  </p>
<p>Figure 4 :
4Performance (in NDTW) of the speakers and their human-augmented versions. Possessing humanlevel pragmatic capability improves performance of the speakers, showing that their original pragmatic capability is highly deficient compared to that of a human.</p>
<p>Figure 5 :
5A qualitative example where the pragmatic speaker (the last model) avoids missing information by simulating the interpretation of the human listener.</p>
<p>Figure 6 :
6Human evaluation interface.</p>
<p>Figure 7 :
7Comparison of using single and ensemble ToM listeners.</p>
<p>Human: Walk along the patio towards the couch. Stop next to the table that is in front of the couch. EncDec-Transformer: Walk straight down the walkway and stop next to the first chair on the left. [Correct destination is next to the couch and table] EncDec-Transformer + ToM Listener (Ensemble of 10 EnvDrop-CLIP): Go straight down the walkway. Go straight and pass the two chairs. Stop near the landing with the pillars.</p>
<p>Figure 8 :
8Turn around and exit out the door in the right corner. Enter the next room and walk straight ahead towards the outdoor area. Stop once you pass the columns and are in the middle facing all the chairs looking outside. EncDec-LSTM: Exit the bathroom and turn left. Walk past the bed and wait by the two chairs. [Correct destination is next to the chairs in the outdoor area] EncDec-LSTM + ToM Listener (Ensemble of 10 EnvDrop-CLIP): Walk out of the bathroom and make a left. Walk through the bedroom and continue straight towards the red chair. Stop at the chair before getting to the red front of the patio. Additional qualitative examples where the pragmatic speaker (the last model) avoids missing information by simulating the interpretation of the human listener.</p>
<p>Return argmax u∈D score(u), D = { u 1 ,..., u N } • Large search , small pragmatic ⇒ improve inference algorithm • Large pragmatic , small search ⇒ enhance model of listenerRepeat N times 
(i) Generate candidate (search capability) 
u i ~ S base ( ᐧ | e<em>) 
(ii) Evaluate candidate (pragmatic capability) 
score(u i ) = L ToM (e</em> | u i ) 
Pragmatic capability 
Search capability </p>
<p>search </p>
<p>pragmatic </p>
<p>Human </p>
<p>Evaluated 
agent </p>
<p>Human-level 
search 
capability </p>
<p>Human-level 
pragmatic 
capability </p>
<p>Bounded pragmatic speaker </p>
<p>Recommendation: </p>
<p>(a) </p>
<p>(b) </p>
<p>(c) </p>
<p>Space of bounded 
pragmatic speakers </p>
<p>Table 2 :
2Agreement (in NDTW) of human and model listeners on instructions generated by different speakers. The level of agreement decreases substantially when shifting from human-generated to model-generated instructions.‡ indicate results that are significantly lower than the human skyline (row 1) with p &lt; 0.05 (according to a two-related-sample t-test).Human: Turn around and walk down the stairs to the bottom. Walk into the kitchen and stand near the kitchen table.EncDec-Transformer + ToM Listener (Ensemble of 10 VLN↻ BERTs):Walk down the stairs and wait by the dining room table and chairs.EncDec-Transformer: Go down the stairs and stop at the bottom of 
the stairs. [correct destination is next to dining table] </p>
<p>Table 3 :
3Hyperparameters for training the GPT-2 
EncDec-Transformer speakers. </p>
<p>Tools. We use SacreBLEU 2.2.1 to compute BLEU scores. For preprocessing and implementing the speaker models, we use Pytorch 1.7.1, NLTK 3.6.7, SentencePiece 0.1.97, and Huggingface Transformers 4.5.1.Computation. The GPT-2 model has 124.4 million parameters, and was trained for 24 hours on single NVIDIA GEFORCE RTX 2080 Ti. The EncDec-LSTM model has 7.5 million parameters, taking 24 hours to train on single NVIDIA RTX A6000. The EncDec-Transformer model has 56.6 million parameters, trained on single NVIDIA RTX A6000 for 48 hours.SpeakerSR ↑ SPL ↑ NDTW ↑ SDTW ↑ Path Len ↓ Interpretability ↑Performance Metrics </p>
<p>Without ToM listener </p>
<p>Finetuned GPT-2 
36.0 
27.8 
37.7 
24.5 
20.9 
2.9 </p>
<p>EncDec-LSTM 
49.3 
37.6 
45.3 
33.8 
17.4 
3.3 </p>
<p>EncDec-Transformer 
54.7 
43.8 
49.4 
40.4 
15.8 
3.4 </p>
<p>With 10 VLN 
⟳ 
BERT as ToM listener </p>
<p>Finetuned GPT-2 
46.7 
30.9 
43.4 
28.1 
21.2 
3.0 </p>
<p>EncDec-LSTM 
54.7 
46.0 
56.4 
41.9 
14.0 
3.1 </p>
<p>EncDec-Transformer 
52.0 
44.0 
54.2 
41.6 
17.7 
3.2 </p>
<p>Humans (R2R dataset) 76.0 
67.6 
71.0 
64.8 
14.2 
3.6 </p>
<p>Table 4 :
4Humans evaluation results on instructions generated by the speaker models. The similarity metrics are defined in §6. Path Len measures the average length of the generated trajectories. Interpretability indicates how easy or difficult to follow the instructions according to human evaluators (without knowing the ground-truth trajectory).
We empirically verify that this assumption holds for the agents we evaluate with N = 10 and γ ranging from 0.7 to 0.9. We estimate γ by computing the fraction of evaluation examples where the agent's model ranksûinfer above N samples drawn from it.
https://github.com/google-research/pangea
https://github.com/google-research/pangea
AcknowledgmentsWe thank Jieyu Zhao, Trista Cao and our reviewers for providing helpful feedback to improve the manuscript. We are also grateful to Patrick Shafto and Ted Sumers for great discussions and references to mathematical models of human cognition.
The hcrc map task corpus. H Anne, Miles Anderson, Ellen Gurman Bader, Elizabeth Bard, Gwyneth Boyle, Simon Doherty, Stephen Garrod, Jacqueline Isard, Jan Kowtko, Jim Mcallister, Miller, Language and speech. 344Anne H Anderson, Miles Bader, Ellen Gurman Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller, et al. 1991. The hcrc map task corpus. Language and speech, 34(4):351-366.</p>
<p>Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, arXiv:1807.06757Manolis Savva, et al. 2018a. On evaluation of embodied navigation agents. arXiv preprintPeter Anderson, Angel Chang, Devendra Singh Chap- lot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mot- taghi, Manolis Savva, et al. 2018a. On evalua- tion of embodied navigation agents. arXiv preprint arXiv:1807.06757.</p>
<p>Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. 2018b. Vision- and-language navigation: Interpreting visually- grounded navigation instructions in real environ- ments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3674- 3683.</p>
<p>Reasoning about pragmatics with neural listeners and speakers. Jacob Andreas, Dan Klein, 10.18653/v1/D16-1125Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsJacob Andreas and Dan Klein. 2016. Reasoning about pragmatics with neural listeners and speakers. In Pro- ceedings of the 2016 Conference on Empirical Meth- ods in Natural Language Processing, pages 1173- 1182, Austin, Texas. Association for Computational Linguistics.</p>
<p>. Rohan Anil, M Andrew, Orhan Dai, Melvin Firat, Dmitry Johnson, Alexandre Lepikhin, Siamak Passos, Emanuel Shakeri, Paige Taropa, Zhifeng Bailey, Chen, arXiv:2305.10403arXiv preprintRohan Anil, Andrew M Dai, Orhan Firat, Melvin John- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Learning to mediate disparities towards pragmatic communication. Yuwei Bao, Sayan Ghosh, Joyce Chai, 10.18653/v1/2022.acl-long.202Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1Yuwei Bao, Sayan Ghosh, and Joyce Chai. 2022. Learn- ing to mediate disparities towards pragmatic commu- nication. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2829-2842, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Does the autistic child have a "theory of mind. Simon Baron-Cohen, Alan M Leslie, Uta Frith, Cognition. 211Simon Baron-Cohen, Alan M Leslie, and Uta Frith. 1985. Does the autistic child have a "theory of mind"? Cognition, 21(1):37-46.</p>
<p>Climbing towards nlu: On meaning, form, and understanding in the age of data. M Emily, Alexander Bender, Koller, Proceedings of the 58th annual meeting of the association for computational linguistics. the 58th annual meeting of the association for computational linguisticsEmily M Bender and Alexander Koller. 2020. Climbing towards nlu: On meaning, form, and understanding in the age of data. In Proceedings of the 58th an- nual meeting of the association for computational linguistics, pages 5185-5198.</p>
<p>Completion norms for 329 sentence contexts. A Paul, Ira Bloom, Fischler, Memory &amp; cognition. 86Paul A Bloom and Ira Fischler. 1980. Completion norms for 329 sentence contexts. Memory &amp; cog- nition, 8(6):631-642.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprintSébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE). Donna Byron, Alexander Koller, Kristina Striegnitz, Justine Cassell, Robert Dale, Johanna Moore, Jon Oberlander, Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009). the 12th European Workshop on Natural Language Generation (ENLG 2009)Athens, GreeceAssociation for Computational LinguisticsDonna Byron, Alexander Koller, Kristina Striegnitz, Jus- tine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2009. Report on the First NLG Chal- lenge on Generating Instructions in Virtual Environ- ments (GIVE). In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009), pages 165-173, Athens, Greece. Association for Computational Linguistics.</p>
<p>Does the chimpanzee have a theory of mind? 30 years later. Human Nature and Self Design. Josep Call, Michael Tomasello, Josep Call and Michael Tomasello. 2011. Does the chimpanzee have a theory of mind? 30 years later. Human Nature and Self Design, pages 83-96.</p>
<p>Theory of mind, pragmatics and the brain: Converging evidence for the role of intention processing as a core feature ofhuman communication. Ivan Enrici, G Bruno, Mauro Bara, Adenzato, Pragmatics &amp; Cognition. 261Ivan Enrici, Bruno G Bara, and Mauro Adenzato. 2019. Theory of mind, pragmatics and the brain: Converg- ing evidence for the role of intention processing as a core feature ofhuman communication. Pragmatics &amp; Cognition, 26(1):5-38.</p>
<p>Human-level play in the game of diplomacy by combining language models with strategic reasoning. FAIR. 2022Science. FAIR. 2022. Human-level play in the game of diplo- macy by combining language models with strategic reasoning. Science.</p>
<p>Annual Meeting of the Association for Computational Linguistics. Online. Association for Computational LinguisticsAnnual Meeting of the Association for Computational Linguistics, pages 7663-7674, Online. Association for Computational Linguistics.</p>
<p>Revisiting the evaluation of theory of mind through question answering. Matthew Le, Y-Lan Boureau, Maximilian Nickel, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Matthew Le, Y-Lan Boureau, and Maximilian Nickel. 2019. Revisiting the evaluation of theory of mind through question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5872-5877.</p>
<p>The winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, Thirteenth international conference on the principles of knowledge representation and reasoning. Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thir- teenth international conference on the principles of knowledge representation and reasoning.</p>
<p>Inferring rewards from language in context. Jessy Lin, Daniel Fried, Dan Klein, Anca Dragan, 10.18653/v1/2022.acl-long.585Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1Jessy Lin, Daniel Fried, Dan Klein, and Anca Dragan. 2022. Inferring rewards from language in context. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8546-8560, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Confer- ence on Learning Representations.</p>
<p>General evaluation for instruction conditioned navigation using dynamic time warping. Vihan Gabriel Ilharco Magalhaes, Alexander Jain, Eugene Ku, Jason Ie, Baldridge, NeurIPS Visually Grounded Interaction and Language (ViGIL) Workshop. Gabriel Ilharco Magalhaes, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. 2019. General evalu- ation for instruction conditioned navigation using dy- namic time warping. In NeurIPS Visually Grounded Interaction and Language (ViGIL) Workshop.</p>
<p>Kyle Mahowald, Anna A Ivanova, A Idan, Nancy Blank, Kanwisher, arXiv:2301.06627Joshua B Tenenbaum, and Evelina Fedorenko. 2023. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprintKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fe- dorenko. 2023. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627.</p>
<p>Improving vision-and-language navigation with imagetext pairs from the web. Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, Dhruv Batra, European Conference on Computer Vision. SpringerArjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. 2020. Im- proving vision-and-language navigation with image- text pairs from the web. In European Conference on Computer Vision, pages 259-274. Springer.</p>
<p>Bayesian modelling of visual perception. Probabilistic models of the brain. Pascal Mamassian, Michael Landy, Laurence T Maloney, 1336Pascal Mamassian, Michael Landy, and Laurence T Mal- oney. 2002. Bayesian modelling of visual perception. Probabilistic models of the brain, 13:36.</p>
<p>Evaluating theory of mind in question answering. Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, Tom Griffiths, 10.18653/v1/D18-1261Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsAida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Tom Griffiths. 2018. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2392-2400, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, Behavioral and brain sciences. 14David Premack and Guy Woodruff. 1978. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515-526.</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLRInternational Conference on Machine Learning. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Overtrust of robots in emergency evacuation scenarios. Paul Robinette, Wenchen Li, Robert Allen, Ayanna M Howard, Alan R Wagner, 11th ACM/IEEE international conference on humanrobot interaction (HRI). IEEEPaul Robinette, Wenchen Li, Robert Allen, Ayanna M Howard, and Alan R Wagner. 2016. Overtrust of robots in emergency evacuation scenarios. In 2016 11th ACM/IEEE international conference on human- robot interaction (HRI), pages 101-108. IEEE.</p>
<p>RMM: A recursive mental model for dialogue navigation. Yonatan Homero Roman Roman, Jesse Bisk, Asli Thomason, Jianfeng Celikyilmaz, Gao, 10.18653/v1/2020.findings-emnlp.157Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational LinguisticsHomero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, and Jianfeng Gao. 2020. RMM: A recursive mental model for dialogue navigation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1732-1745, Online. Association for Computational Linguistics.</p>
<p>Pragmatic markers: the missing link between language and theory of mind. Paula Rubio-Fernandez, Synthese. 1991Paula Rubio-Fernandez. 2021. Pragmatic markers: the missing link between language and theory of mind. Synthese, 199(1):1125-1158.</p>
<p>Bayesian brains without probabilities. N Adam, Nick Sanborn, Chater, Trends in cognitive sciences. 2012Adam N Sanborn and Nick Chater. 2016. Bayesian brains without probabilities. Trends in cognitive sci- ences, 20(12):883-893.</p>
<p>Rational approximations to rational models: alternative algorithms for category learning. N Adam, Thomas L Sanborn, Daniel J Griffiths, Navarro, Psychological review. 11741144Adam N Sanborn, Thomas L Griffiths, and Daniel J Navarro. 2010. Rational approximations to rational models: alternative algorithms for category learning. Psychological review, 117(4):1144.</p>
<p>Neural theory-of-mind? on the limits of social intelligence in large LMs. Maarten Sap, Le Ronan, Daniel Bras, Yejin Fried, Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. 2022. Neural theory-of-mind? on the limits of social intelligence in large LMs. In Proceedings of the 2022 Conference on Empirical Methods in Nat- ural Language Processing, pages 3762-3780, Abu Dhabi, United Arab Emirates. Association for Com- putational Linguistics.</p>
<p>Social IQa: Commonsense reasoning about social interactions. Maarten Sap, Hannah Rashkin, Derek Chen, Yejin Ronan Le Bras, Choi, 10.18653/v1/D19-1454Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Com- monsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 4463- 4473, Hong Kong, China. Association for Computa- tional Linguistics.</p>
<p>Speaking our minds: Why human communication is different, and how language evolved to make it special. Thom Scott-Phillips, Bloomsbury PublishingThom Scott-Phillips. 2014. Speaking our minds: Why human communication is different, and how language evolved to make it special. Bloomsbury Publishing.</p>
<p>A rational account of pedagogical reasoning: Teaching by, and learning from, examples. Patrick Shafto, D Noah, Thomas L Goodman, Griffiths, Cognitive psychology. 71Patrick Shafto, Noah D Goodman, and Thomas L Grif- fiths. 2014. A rational account of pedagogical rea- soning: Teaching by, and learning from, examples. Cognitive psychology, 71:55-89.</p>
<p>How much can clip benefit vision-and-language tasks?. Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsSheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. 2022. How much can clip benefit vision-and-language tasks? In Proceedings of the In- ternational Conference on Learning Representations.</p>
<p>Models of man; social and rational. A Herbert, Simon, wileyHerbert A Simon. 1957. Models of man; social and rational. wiley.</p>
<p>Report on the second second challenge on generating instructions in virtual environments (give-2.5). Kristina Striegnitz, A J Alexandre, Andrew Denis, Konstantina Gargett, Alexander Garoufi, Mariët Koller, Theune, 13th European workshop on natural language generation. Kristina Striegnitz, Alexandre AJ Denis, Andrew Gar- gett, Konstantina Garoufi, Alexander Koller, and Mar- iët Theune. 2011. Report on the second second chal- lenge on generating instructions in virtual environ- ments (give-2.5). In 13th European workshop on natural language generation.</p>
<p>How to talk so ai will learn: Instructions, descriptions, and autonomy. Theodore Sumers, D Robert, Hawkins, K Mark, Thomas L Ho, Dylan Griffiths, Hadfield-Menell, Advances in Neural Information Processing Systems. Theodore Sumers, Robert D Hawkins, Mark K Ho, Thomas L Griffiths, and Dylan Hadfield-Menell. 2022. How to talk so ai will learn: Instructions, descriptions, and autonomy. In Advances in Neural Information Processing Systems.</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A ques- tion answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. Licheng Hao Tan, Mohit Yu, Bansal, 10.18653/v1/N19-1268Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Learn- ing to navigate unseen environments: Back transla- tion with environmental dropout. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2610-2621, Minneapolis, Min- nesota. Association for Computational Linguistics.</p>
<p>Becoming human. Michael Tomasello, Becoming Human. Harvard University PressMichael Tomasello. 2019. Becoming human. In Be- coming Human. Harvard University Press.</p>
<p>Pragmatics across languages and cultures. Anna Trosborg, De Gruyter Mouton7Anna Trosborg. 2010. Pragmatics across languages and cultures, volume 7. De Gruyter Mouton.</p>
<p>A natural language corpus of common grounding under continuous and partially-observable context. Takuma Udagawa, Akiko Aizawa, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Takuma Udagawa and Akiko Aizawa. 2019. A natural language corpus of common grounding under contin- uous and partially-observable context. In Proceed- ings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7120-7127.</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. Tomer Ullman, arXiv:2302.08399arXiv preprintTomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>One and done? optimal decisions from very few samples. Edward Vul, Noah Goodman, L Thomas, Joshua B Griffiths, Tenenbaum, Cognitive science. 384Edward Vul, Noah Goodman, Thomas L Griffiths, and Joshua B Tenenbaum. 2014. One and done? optimal decisions from very few samples. Cognitive science, 38(4):599-637.</p>
<p>A mathematical theory of cooperative communication. Pei Wang, Junqi Wang, Pushpi Paranamana, Patrick Shafto, Advances in Neural Information Processing Systems. 33Pei Wang, Junqi Wang, Pushpi Paranamana, and Patrick Shafto. 2020. A mathematical theory of cooperative communication. Advances in Neural Information Processing Systems, 33:17582-17593.</p>
<p>Less is more: Generating grounded navigation instructions from landmarks. Su Wang, Ceslee Montgomery, Jordi Orbay, Vighnesh Birodkar, Aleksandra Faust, Izzeddin Gur, Natasha Jaques, Austin Waters, Jason Baldridge, Peter Anderson, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Su Wang, Ceslee Montgomery, Jordi Orbay, Vighnesh Birodkar, Aleksandra Faust, Izzeddin Gur, Natasha Jaques, Austin Waters, Jason Baldridge, and Pe- ter Anderson. 2021. Less is more: Generating grounded navigation instructions from landmarks. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15407-15417.</p>
<p>Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Heinz Wimmer, Josef Perner, Cognition. 131Heinz Wimmer and Josef Perner. 1983. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Cognition, 13(1):103-128.</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Vi- sual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pat- tern recognition, pages 6720-6731.</p>
<p>On the evaluation of vision-and-language navigation instructions. Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alexander Ku, Jason Baldridge, Eugene Ie, European Chapter of the Association for Computational Linguistics. Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alexander Ku, Jason Baldridge, and Eugene Ie. 2021. On the evaluation of vision-and-language navigation instructions. European Chapter of the Association for Computational Linguistics, pages 1302-1316.</p>
<p>Few-shot language coordination by modeling theory of mind. Hao Zhu, Graham Neubig, Yonatan Bisk, PMLRInternational Conference on Machine Learning. Hao Zhu, Graham Neubig, and Yonatan Bisk. 2021. Few-shot language coordination by modeling theory of mind. In International Conference on Machine Learning, pages 12901-12911. PMLR.</p>            </div>
        </div>

    </div>
</body>
</html>
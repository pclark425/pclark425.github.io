<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6877 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6877</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6877</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-270786925</p>
                <p><strong>Paper Title:</strong> Automation and machine learning augmented by large language models in a catalysis study</p>
                <p><strong>Paper Abstract:</strong> Recent advancements in artificial intelligence and automation are transforming catalyst discovery and design from traditional trial-and-error manual mode into intelligent, high-throughput digital methodologies. This transformation is driven by four key components, including high-throughput information extraction, automated robotic experimentation, real-time feedback for iterative optimization, and interpretable machine learning for generating new knowledge. These innovations have given rise to the development of self-driving labs and significantly accelerated materials research. Over the past two years, the emergence of large language models (LLMs) has added a new dimension to this field, providing unprecedented flexibility in information integration, decision-making, and interacting with human researchers. This review explores how LLMs are reshaping catalyst design, heralding a revolutionary change in the fields.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6877.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6877.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Reticular Chemist)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used in Reticular Chemist workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used as the central LLM in a human-in-the-loop workflow (ChemScope / ChemNavigator / ChemExecutor) to read literature, generate project blueprints and iterative experiment protocols for MOF discovery, update summaries with experimental feedback, and guide experimental optimization toward novel MOF synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A GPT-4 Reticular Chemist for Guiding MOF Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>multimodal/decoder-only large language model used with in-context learning and prompt-engineered workflows (human-in-the-loop agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-engineering + in-context learning to generate project blueprints, task lists, and step-by-step experimental instructions; iterative updating with experimental feedback (human executed) — i.e., LLM-directed procedural generation rather than direct molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Natural-language synthesis procedures and protocol templates (text descriptions of MOF synthesis steps); not direct SMILES generation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Design and synthesis of metal-organic frameworks (MOFs)</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Human-provided project goals and lab resource availability; prompt templates and stage-completion indicators; iterative human feedback used as constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Part of an integrated workflow (ChemScope → ChemNavigator → ChemExecutor) that interfaces with literature retrieval and human experiment execution; templates for recording experimental feedback used to close the loop. No specific quantum/docking tools reported in the review for this workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Primary literature (reticular chemistry literature) plus iterative experimental results recorded during the project; no public chemical dataset explicitly named in the review for this workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative project completion metrics (stage objectives), success in discovering and characterizing targeted MOFs (e.g., identification/characterization of isomorphic MOF-521s); no standard generative metrics (validity/uniqueness) reported in review for this use-case.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Successful use of GPT-4-guided iterative workflow to discover and characterize a series of isomorphic MOF-521s (experimental synthesis and characterization were completed with human execution). No numeric performance metrics reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Hallucination risk in LLM outputs; token-size/context window limits constrain amount of ICL examples; requirement for careful prompt engineering and consistent human feedback; models lack deep domain-specific expertise unless fine-tuned; reliability depends on human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6877.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6877.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (ChatGPT Chemistry Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (used as a chemistry assistant for MOF synthesis extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was applied to extract and format MOF synthesis information from the literature by segmenting text, embedding segments, classifying segments as 'synthesis' vs 'nonsynthesis' via in-context learning, and formatting extracted data into structured tables; used to build a chemistry chatbot for synthesis information retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model used with in-context learning (prompting) for information extraction and classification</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Text segmentation → embedding similarity retrieval → in-context learning classification of segments → text formatting into tables; prompt-engineered classification rather than molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Natural-language synthesis descriptions (text excerpts).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Extraction and structuring of MOF synthesis procedures from literature; enabling downstream synthesis planning/chatbot queries.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Careful prompt engineering and context provision to mitigate hallucination; selection of high-similarity text segments via vector similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Vector embedding retrieval for segment selection; used as part of a pipeline that converts raw text to structured synthesis tables and a chatbot interface. No direct integration with quantum chemistry or retrosynthesis tools reported in the review for this use.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Scientific literature (MOF synthesis descriptions); exact corpora not specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative extraction success; mitigation of hallucination via prompt engineering discussed. No formal numeric metrics reported in the review for the extraction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Extracted and formatted synthesis information from MOF literature and enabled a chemistry chatbot; hallucination was addressed through prompt engineering and context provision (no numerical performance values reported in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Hallucinations in generated context; need for careful prompt engineering and context; domain-specific inaccuracies if model not fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6877.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6877.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM surrogate for Bayesian optimization (White group)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM used as surrogate model inside Bayesian optimization to predict reaction outcomes from procedural text (White et al. approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative pre-trained transformer (various OpenAI models including text-curie-001, text-davinci-003 and GPT-4) was used as a surrogate model to predict C2 yield for oxidative coupling of methane from textual experimental procedures using either in-context learning or fine-tuning; the authors derived discrete output distributions (via multiple-choice or top-k completions) to estimate uncertainty for BO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian Optimization of Catalysts With In-context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI models (text-curie-001, text-davinci-003, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM used as surrogate model with in-context learning and optional fine-tuning; uncertainty approximated via multiple-choice or top-k completions</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context learning (ICL) and fine-tuning on examples; two uncertainty methods: (1) treat regression as multiple-choice with enumerated ranges (access probabilities), (2) query model k times to obtain top-k completions and form discrete probability distributions; used inside a Bayesian optimization loop.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Experimental procedure described as text (natural language representation of synthesis/experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Optimization of C2 yield in oxidative coupling of methane (catalyst/process optimization) via BO.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Limited ICL context size (only top relevant examples used due to token limits); discrete-output templates for uncertainty estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used within a Bayesian optimization framework; compared against Gaussian process regression baseline (GPR with text embeddings). No mention of quantum chemistry or retrosynthesis integration in this context.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Dataset of experimental procedures and associated C2 yields for oxidative coupling of methane (specific dataset size/details not provided in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Prediction error (mean absolute error, MAE); Bayesian optimization progress (quantiles reached, ability to find maxima in sample pool).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ICL GPT-4 model achieved MAE = 1.854 (units implicit with yield prediction) comparable to GPR baseline MAE = 1.893; fine-tuning reduced MAE to 1.325. In BO experiments, the ICL model reached the 99% quantile after 15 samples but failed to find the maximum in the sample pool; GPR performed slightly better in optimization efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Token-size limitations restricted number of ICL examples (possible reason for suboptimal BO performance), complexity of experimental descriptions, risk of hallucination, and lower optimization efficiency compared to some conventional surrogate models in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6877.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6877.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (fine-tuned predictive chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (fine-tuned for predictive chemistry tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 variants were fine-tuned on chemistry data cast as natural language (SMILES or IUPAC names in sentence form) and used for low-data property prediction, reaction yield prediction, and molecule generation; showed superior performance to some traditional models in data-scarce regimes and could generate molecules from textual requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging large language models for predictive chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM fine-tuned on chemistry-labelled sentences; also used with in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Chemistry data formatted as sentences (SMILES or IUPAC names embedded in natural language); exact corpora not specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning on labelled chemistry examples; in-context learning for few-shot prediction and conditional generation; used for classification/regression and conditional molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES or IUPAC names embedded in natural language sentences (text-based chemical representation).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Material property prediction, reaction yield prediction, and molecule generation (general molecule design tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not explicitly detailed in the review; typical low-data fine-tuning constraints and reliance on textual representation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not specified in the review for this work.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Chemistry-labelled sentences from curated chemistry data sources (specific datasets not enumerated in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Prediction accuracy metrics for classification/regression tasks; general comparison to traditional models under low-data conditions (specific metrics and numbers not provided in the review excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Fine-tuned GPT-3 achieved superior performance over traditional models in predicting material properties and reaction yields, especially in low-data regimes; the model could generate molecules given property requirements using SMILES or IUPAC textual inputs. No specific numeric metrics were included in the review text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Unclear whether LLM general chemical knowledge was exploited versus pattern learning; risk of hallucination and domain-specific misunderstandings unless fine-tuned; caution advised in interpretation and downstream use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6877.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6877.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Transformer (RT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based approach that frames regression as conditional sequence modelling to enable concurrent regression and molecular sequence generation; used as an architecture for property-conditioned molecule generation and prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regression Transformer (RT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>conditional transformer (sequence-to-sequence / sequence-regression hybrid) used for joint generation and regression tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional sequence modelling: regression task is cast as a conditioning input/output in sequence modelling, enabling generation of molecules conditioned on target properties.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Textual molecular sequences (SMILES/SELFIES or equivalent sequence tokens) used as sequence input/output.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Property-conditioned molecular generation and regression tasks (general molecule design).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned as an architectural approach; review does not provide experimental numbers here. General challenges for sequence-based molecular generation (validity, syntactic correctness, chemical plausibility) apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6877.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6877.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELFormer (SELFIES language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based chemical language model trained to produce and use SELFIES representations (SELF-referencIng Embedded Strings) for robust molecular representation learning and generation, improving chemical validity of generated sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SELFormer: molecular representation learning via SELFIES language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SELFormer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based chemical language model trained on SELFIES tokens</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct generation of SELFIES strings via language-model sampling, leveraging SELFIES' guaranteed syntactic validity</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule representation learning and de novo generation with enhanced validity</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Implicit validity guarantee from SELFIES representation; other constraints not specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>The review notes SELFIES-based language models help with syntactic validity but does not provide quantitative results here; other common limitations (property optimization, synthetic accessibility) remain to be addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6877.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6877.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language models for molecular distributions (Flam‑Shepherd / Aspuru‑Guzik)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models trained to learn complex molecular distributions (Flam-Shepherd, Zhu, Aspuru-Guzik et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple language-model architectures trained on SMILES can learn and sample complex molecular distributions; demonstrated by recovering distributions of high-scoring penalized logP molecules from ZINC15 and showing powerful generative capabilities for molecular design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can learn complex molecular distributions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Simple chemical language models (SMILES-based LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only sequence language models trained on SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>SMILES corpora such as ZINC15 (explicitly mentioned for experiments in which highest-scoring penalized logP molecules distribution was modelled).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES sequence generation by sampling from trained language models; unconditional or property-biased via dataset selection.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecule generation targeted at property distributions (e.g., high penalized logP), general molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC15 (used in the referenced experiments to learn distribution of high-scoring molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Distributional matching (ability to reproduce high-scoring molecule distributions), top-scoring molecule generation performance; exact metrics not enumerated in review excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Demonstrated ability to learn and generate distributions of highest-scoring penalized logP molecules from ZINC15; review cites this as evidence of generative power but does not give numeric metrics in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Generative validity and chemical plausibility depend on representation and training data; models trained solely on 1D/2D representations may lack deeper chemical understanding and can hallucinate unrealistic molecules or propose synthetically inaccessible structures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6877.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6877.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-augmented LLM agents (e.g., ChemCrow / RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-augmented large language model agents (Retrieval-Augmented Generation, ChemCrow-style agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmented LLM agents combine base LLMs with toolkits (literature retrieval, calculators, retrosynthesis planners, experiment controllers, and automated-experimentation interfaces) to reduce hallucination, access domain databases, and coordinate closed-loop experimental campaigns; proposed as a route toward self-evolving discovery agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmenting large language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs augmented with external toolset (general concept; implementations include ChemCrow and RAG agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>tool-using agent architecture (LLM + external tools/APIs/RAG retrieval systems)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) and tool-invocation: LLM decides when to call external tools (retrieval, calculators, planners) and integrates results into generated instructions or designs; can be chained inside iterative experimental workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Mixed: natural-language protocols, SMILES/SELFIES for molecules, graph descriptors, and structured data returned by external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Automated experimentation orchestration, synthesis planning, property prediction, closed-loop catalyst/material discovery, and general chemist assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Tool-based verification steps to reduce hallucination (e.g., database lookups, property calculators, retrosynthesis checks); human oversight for safety-critical steps.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Designed to integrate with literature retrieval, property predictors, retrosynthesis planners, simulation software, and automated experiment platforms (review explicitly mentions agents equipped for automated experimentation, information retrieval, and ML).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not standardized in the review; success criteria include reduction in hallucination, correctness of retrieved facts, and ability to orchestrate experiments. Specific metrics depend on the implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reliability of tool selection and invocation, need for robust tool interfaces, complexity of mapping ambiguous natural-language instructions to safe, executable robotic actions, verification of LLM-generated plans, and remaining hallucination risk if retrieval or tool verification is incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automation and machine learning augmented by large language models in a catalysis study', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A GPT-4 Reticular Chemist for Guiding MOF Discovery <em>(Rating: 2)</em></li>
                <li>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for predictive chemistry <em>(Rating: 2)</em></li>
                <li>Bayesian Optimization of Catalysts With In-context Learning <em>(Rating: 2)</em></li>
                <li>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling <em>(Rating: 2)</em></li>
                <li>SELFormer: molecular representation learning via SELFIES language models <em>(Rating: 1)</em></li>
                <li>Language models can learn complex molecular distributions <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6877",
    "paper_id": "paper-270786925",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "GPT-4 (Reticular Chemist)",
            "name_full": "Generative Pre-trained Transformer 4 (used in Reticular Chemist workflow)",
            "brief_description": "GPT-4 was used as the central LLM in a human-in-the-loop workflow (ChemScope / ChemNavigator / ChemExecutor) to read literature, generate project blueprints and iterative experiment protocols for MOF discovery, update summaries with experimental feedback, and guide experimental optimization toward novel MOF synthesis.",
            "citation_title": "A GPT-4 Reticular Chemist for Guiding MOF Discovery",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "multimodal/decoder-only large language model used with in-context learning and prompt-engineered workflows (human-in-the-loop agent)",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "Prompt-engineering + in-context learning to generate project blueprints, task lists, and step-by-step experimental instructions; iterative updating with experimental feedback (human executed) — i.e., LLM-directed procedural generation rather than direct molecule generation.",
            "chemical_representation": "Natural-language synthesis procedures and protocol templates (text descriptions of MOF synthesis steps); not direct SMILES generation.",
            "target_application": "Design and synthesis of metal-organic frameworks (MOFs)",
            "constraints_used": "Human-provided project goals and lab resource availability; prompt templates and stage-completion indicators; iterative human feedback used as constraints.",
            "integration_with_external_tools": "Part of an integrated workflow (ChemScope → ChemNavigator → ChemExecutor) that interfaces with literature retrieval and human experiment execution; templates for recording experimental feedback used to close the loop. No specific quantum/docking tools reported in the review for this workflow.",
            "dataset_used": "Primary literature (reticular chemistry literature) plus iterative experimental results recorded during the project; no public chemical dataset explicitly named in the review for this workflow.",
            "evaluation_metrics": "Qualitative project completion metrics (stage objectives), success in discovering and characterizing targeted MOFs (e.g., identification/characterization of isomorphic MOF-521s); no standard generative metrics (validity/uniqueness) reported in review for this use-case.",
            "reported_results": "Successful use of GPT-4-guided iterative workflow to discover and characterize a series of isomorphic MOF-521s (experimental synthesis and characterization were completed with human execution). No numeric performance metrics reported in the review.",
            "experimental_validation": true,
            "challenges_or_limitations": "Hallucination risk in LLM outputs; token-size/context window limits constrain amount of ICL examples; requirement for careful prompt engineering and consistent human feedback; models lack deep domain-specific expertise unless fine-tuned; reliability depends on human verification.",
            "uuid": "e6877.0",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5 (ChatGPT Chemistry Assistant)",
            "name_full": "OpenAI GPT-3.5 (used as a chemistry assistant for MOF synthesis extraction)",
            "brief_description": "GPT-3.5 was applied to extract and format MOF synthesis information from the literature by segmenting text, embedding segments, classifying segments as 'synthesis' vs 'nonsynthesis' via in-context learning, and formatting extracted data into structured tables; used to build a chemistry chatbot for synthesis information retrieval.",
            "citation_title": "ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_type": "decoder-only large language model used with in-context learning (prompting) for information extraction and classification",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "Text segmentation → embedding similarity retrieval → in-context learning classification of segments → text formatting into tables; prompt-engineered classification rather than molecule generation.",
            "chemical_representation": "Natural-language synthesis descriptions (text excerpts).",
            "target_application": "Extraction and structuring of MOF synthesis procedures from literature; enabling downstream synthesis planning/chatbot queries.",
            "constraints_used": "Careful prompt engineering and context provision to mitigate hallucination; selection of high-similarity text segments via vector similarity.",
            "integration_with_external_tools": "Vector embedding retrieval for segment selection; used as part of a pipeline that converts raw text to structured synthesis tables and a chatbot interface. No direct integration with quantum chemistry or retrosynthesis tools reported in the review for this use.",
            "dataset_used": "Scientific literature (MOF synthesis descriptions); exact corpora not specified in the review.",
            "evaluation_metrics": "Qualitative extraction success; mitigation of hallucination via prompt engineering discussed. No formal numeric metrics reported in the review for the extraction quality.",
            "reported_results": "Extracted and formatted synthesis information from MOF literature and enabled a chemistry chatbot; hallucination was addressed through prompt engineering and context provision (no numerical performance values reported in the review).",
            "experimental_validation": false,
            "challenges_or_limitations": "Hallucinations in generated context; need for careful prompt engineering and context; domain-specific inaccuracies if model not fine-tuned.",
            "uuid": "e6877.1",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM surrogate for Bayesian optimization (White group)",
            "name_full": "LLM used as surrogate model inside Bayesian optimization to predict reaction outcomes from procedural text (White et al. approach)",
            "brief_description": "A generative pre-trained transformer (various OpenAI models including text-curie-001, text-davinci-003 and GPT-4) was used as a surrogate model to predict C2 yield for oxidative coupling of methane from textual experimental procedures using either in-context learning or fine-tuning; the authors derived discrete output distributions (via multiple-choice or top-k completions) to estimate uncertainty for BO.",
            "citation_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "mention_or_use": "use",
            "model_name": "OpenAI models (text-curie-001, text-davinci-003, GPT-4)",
            "model_type": "decoder-only LLM used as surrogate model with in-context learning and optional fine-tuning; uncertainty approximated via multiple-choice or top-k completions",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "In-context learning (ICL) and fine-tuning on examples; two uncertainty methods: (1) treat regression as multiple-choice with enumerated ranges (access probabilities), (2) query model k times to obtain top-k completions and form discrete probability distributions; used inside a Bayesian optimization loop.",
            "chemical_representation": "Experimental procedure described as text (natural language representation of synthesis/experiment).",
            "target_application": "Optimization of C2 yield in oxidative coupling of methane (catalyst/process optimization) via BO.",
            "constraints_used": "Limited ICL context size (only top relevant examples used due to token limits); discrete-output templates for uncertainty estimation.",
            "integration_with_external_tools": "Used within a Bayesian optimization framework; compared against Gaussian process regression baseline (GPR with text embeddings). No mention of quantum chemistry or retrosynthesis integration in this context.",
            "dataset_used": "Dataset of experimental procedures and associated C2 yields for oxidative coupling of methane (specific dataset size/details not provided in the review).",
            "evaluation_metrics": "Prediction error (mean absolute error, MAE); Bayesian optimization progress (quantiles reached, ability to find maxima in sample pool).",
            "reported_results": "ICL GPT-4 model achieved MAE = 1.854 (units implicit with yield prediction) comparable to GPR baseline MAE = 1.893; fine-tuning reduced MAE to 1.325. In BO experiments, the ICL model reached the 99% quantile after 15 samples but failed to find the maximum in the sample pool; GPR performed slightly better in optimization efficiency.",
            "experimental_validation": false,
            "challenges_or_limitations": "Token-size limitations restricted number of ICL examples (possible reason for suboptimal BO performance), complexity of experimental descriptions, risk of hallucination, and lower optimization efficiency compared to some conventional surrogate models in this dataset.",
            "uuid": "e6877.2",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3 (fine-tuned predictive chemistry)",
            "name_full": "Generative Pre-trained Transformer 3 (fine-tuned for predictive chemistry tasks)",
            "brief_description": "GPT-3 variants were fine-tuned on chemistry data cast as natural language (SMILES or IUPAC names in sentence form) and used for low-data property prediction, reaction yield prediction, and molecule generation; showed superior performance to some traditional models in data-scarce regimes and could generate molecules from textual requirements.",
            "citation_title": "Leveraging large language models for predictive chemistry",
            "mention_or_use": "use",
            "model_name": "GPT-3 (fine-tuned variants)",
            "model_type": "decoder-only LLM fine-tuned on chemistry-labelled sentences; also used with in-context learning",
            "model_size": null,
            "training_data_description": "Chemistry data formatted as sentences (SMILES or IUPAC names embedded in natural language); exact corpora not specified in the review.",
            "generation_method": "Fine-tuning on labelled chemistry examples; in-context learning for few-shot prediction and conditional generation; used for classification/regression and conditional molecule generation.",
            "chemical_representation": "SMILES or IUPAC names embedded in natural language sentences (text-based chemical representation).",
            "target_application": "Material property prediction, reaction yield prediction, and molecule generation (general molecule design tasks).",
            "constraints_used": "Not explicitly detailed in the review; typical low-data fine-tuning constraints and reliance on textual representation.",
            "integration_with_external_tools": "Not specified in the review for this work.",
            "dataset_used": "Chemistry-labelled sentences from curated chemistry data sources (specific datasets not enumerated in the review).",
            "evaluation_metrics": "Prediction accuracy metrics for classification/regression tasks; general comparison to traditional models under low-data conditions (specific metrics and numbers not provided in the review excerpt).",
            "reported_results": "Fine-tuned GPT-3 achieved superior performance over traditional models in predicting material properties and reaction yields, especially in low-data regimes; the model could generate molecules given property requirements using SMILES or IUPAC textual inputs. No specific numeric metrics were included in the review text.",
            "experimental_validation": false,
            "challenges_or_limitations": "Unclear whether LLM general chemical knowledge was exploited versus pattern learning; risk of hallucination and domain-specific misunderstandings unless fine-tuned; caution advised in interpretation and downstream use.",
            "uuid": "e6877.3",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Regression Transformer",
            "name_full": "Regression Transformer (RT)",
            "brief_description": "A transformer-based approach that frames regression as conditional sequence modelling to enable concurrent regression and molecular sequence generation; used as an architecture for property-conditioned molecule generation and prediction.",
            "citation_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
            "mention_or_use": "mention",
            "model_name": "Regression Transformer (RT)",
            "model_type": "conditional transformer (sequence-to-sequence / sequence-regression hybrid) used for joint generation and regression tasks",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "Conditional sequence modelling: regression task is cast as a conditioning input/output in sequence modelling, enabling generation of molecules conditioned on target properties.",
            "chemical_representation": "Textual molecular sequences (SMILES/SELFIES or equivalent sequence tokens) used as sequence input/output.",
            "target_application": "Property-conditioned molecular generation and regression tasks (general molecule design).",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Mentioned as an architectural approach; review does not provide experimental numbers here. General challenges for sequence-based molecular generation (validity, syntactic correctness, chemical plausibility) apply.",
            "uuid": "e6877.4",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SELFormer",
            "name_full": "SELFormer (SELFIES language model)",
            "brief_description": "A transformer-based chemical language model trained to produce and use SELFIES representations (SELF-referencIng Embedded Strings) for robust molecular representation learning and generation, improving chemical validity of generated sequences.",
            "citation_title": "SELFormer: molecular representation learning via SELFIES language models",
            "mention_or_use": "mention",
            "model_name": "SELFormer",
            "model_type": "transformer-based chemical language model trained on SELFIES tokens",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "Direct generation of SELFIES strings via language-model sampling, leveraging SELFIES' guaranteed syntactic validity",
            "chemical_representation": "SELFIES",
            "target_application": "Molecule representation learning and de novo generation with enhanced validity",
            "constraints_used": "Implicit validity guarantee from SELFIES representation; other constraints not specified in the review.",
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "The review notes SELFIES-based language models help with syntactic validity but does not provide quantitative results here; other common limitations (property optimization, synthetic accessibility) remain to be addressed.",
            "uuid": "e6877.5",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Language models for molecular distributions (Flam‑Shepherd / Aspuru‑Guzik)",
            "name_full": "Language models trained to learn complex molecular distributions (Flam-Shepherd, Zhu, Aspuru-Guzik et al.)",
            "brief_description": "Simple language-model architectures trained on SMILES can learn and sample complex molecular distributions; demonstrated by recovering distributions of high-scoring penalized logP molecules from ZINC15 and showing powerful generative capabilities for molecular design tasks.",
            "citation_title": "Language models can learn complex molecular distributions",
            "mention_or_use": "mention",
            "model_name": "Simple chemical language models (SMILES-based LMs)",
            "model_type": "decoder-only sequence language models trained on SMILES",
            "model_size": null,
            "training_data_description": "SMILES corpora such as ZINC15 (explicitly mentioned for experiments in which highest-scoring penalized logP molecules distribution was modelled).",
            "generation_method": "Direct SMILES sequence generation by sampling from trained language models; unconditional or property-biased via dataset selection.",
            "chemical_representation": "SMILES",
            "target_application": "De novo molecule generation targeted at property distributions (e.g., high penalized logP), general molecular design",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": "ZINC15 (used in the referenced experiments to learn distribution of high-scoring molecules).",
            "evaluation_metrics": "Distributional matching (ability to reproduce high-scoring molecule distributions), top-scoring molecule generation performance; exact metrics not enumerated in review excerpt.",
            "reported_results": "Demonstrated ability to learn and generate distributions of highest-scoring penalized logP molecules from ZINC15; review cites this as evidence of generative power but does not give numeric metrics in the excerpt.",
            "experimental_validation": false,
            "challenges_or_limitations": "Generative validity and chemical plausibility depend on representation and training data; models trained solely on 1D/2D representations may lack deeper chemical understanding and can hallucinate unrealistic molecules or propose synthetically inaccessible structures.",
            "uuid": "e6877.6",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Tool-augmented LLM agents (e.g., ChemCrow / RAG)",
            "name_full": "Tool-augmented large language model agents (Retrieval-Augmented Generation, ChemCrow-style agents)",
            "brief_description": "Augmented LLM agents combine base LLMs with toolkits (literature retrieval, calculators, retrosynthesis planners, experiment controllers, and automated-experimentation interfaces) to reduce hallucination, access domain databases, and coordinate closed-loop experimental campaigns; proposed as a route toward self-evolving discovery agents.",
            "citation_title": "Augmenting large language models with chemistry tools",
            "mention_or_use": "mention",
            "model_name": "LLMs augmented with external toolset (general concept; implementations include ChemCrow and RAG agents)",
            "model_type": "tool-using agent architecture (LLM + external tools/APIs/RAG retrieval systems)",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "Retrieval-Augmented Generation (RAG) and tool-invocation: LLM decides when to call external tools (retrieval, calculators, planners) and integrates results into generated instructions or designs; can be chained inside iterative experimental workflows.",
            "chemical_representation": "Mixed: natural-language protocols, SMILES/SELFIES for molecules, graph descriptors, and structured data returned by external tools.",
            "target_application": "Automated experimentation orchestration, synthesis planning, property prediction, closed-loop catalyst/material discovery, and general chemist assistance.",
            "constraints_used": "Tool-based verification steps to reduce hallucination (e.g., database lookups, property calculators, retrosynthesis checks); human oversight for safety-critical steps.",
            "integration_with_external_tools": "Designed to integrate with literature retrieval, property predictors, retrosynthesis planners, simulation software, and automated experiment platforms (review explicitly mentions agents equipped for automated experimentation, information retrieval, and ML).",
            "dataset_used": null,
            "evaluation_metrics": "Not standardized in the review; success criteria include reduction in hallucination, correctness of retrieved facts, and ability to orchestrate experiments. Specific metrics depend on the implementation.",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Reliability of tool selection and invocation, need for robust tool interfaces, complexity of mapping ambiguous natural-language instructions to safe, executable robotic actions, verification of LLM-generated plans, and remaining hallucination risk if retrieval or tool verification is incomplete.",
            "uuid": "e6877.7",
            "source_info": {
                "paper_title": "Automation and machine learning augmented by large language models in a catalysis study",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A GPT-4 Reticular Chemist for Guiding MOF Discovery",
            "rating": 2,
            "sanitized_title": "a_gpt4_reticular_chemist_for_guiding_mof_discovery"
        },
        {
            "paper_title": "ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        },
        {
            "paper_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_of_catalysts_with_incontext_learning"
        },
        {
            "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
            "rating": 2,
            "sanitized_title": "regression_transformer_enables_concurrent_sequence_regression_and_generation_for_molecular_language_modelling"
        },
        {
            "paper_title": "SELFormer: molecular representation learning via SELFIES language models",
            "rating": 1,
            "sanitized_title": "selformer_molecular_representation_learning_via_selfies_language_models"
        },
        {
            "paper_title": "Language models can learn complex molecular distributions",
            "rating": 2,
            "sanitized_title": "language_models_can_learn_complex_molecular_distributions"
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "augmenting_large_language_models_with_chemistry_tools"
        }
    ],
    "cost": 0.02364675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automation and machine learning augmented by large language models in a catalysis study</p>
<p>Yuming Su 
Chemical</p>
<p>X Wang 0000-0002-7906-8061
Chemical</p>
<p>Yuanxiang Ye 
Chemical</p>
<p>Yibo Xie 
Chemical</p>
<p>Yujing Xu yujingxu@xmu.edu.cn 
Chemical</p>
<p>Yibin Jiang yibin_jiang@outlook.com 
Chemical</p>
<p>Cheng Wang wangchengxmu@xmu.edu.cn 
Chemical</p>
<p>Automation and machine learning augmented by large language models in a catalysis study
40BB4C14FBE0B80D0604DC669301071310.1039/d3sc07012cReceived 31st December 2023 Accepted 21st June 2024
Recent advancements in artificial intelligence and automation are transforming catalyst discovery and design from traditional trial-and-error manual mode into intelligent, high-throughput digital methodologies.This transformation is driven by four key components, including high-throughput information extraction, automated robotic experimentation, real-time feedback for iterative optimization, and interpretable machine learning for generating new knowledge.These innovations have given rise to the development of self-driving labs and significantly accelerated materials research.Over the past two years, the emergence of large language models (LLMs) has added a new dimension to this field, providing unprecedented flexibility in information integration, decision-making, and interacting with human researchers.This review explores how LLMs are reshaping catalyst design, heralding a revolutionary change in the fields.</p>
<p>Introduction</p>
<p>The eld of catalyst design and discovery is undergoing a profound transformation, facilitated by the convergence of articial intelligence (AI) [1][2][3] and automation systems, [4][5][6] as well as utilization of large data.This shi is propelled by advancements in four crucial areas: high-throughput information extraction, [7][8][9][10][11][12][13][14][15][16] automated robotic systems for chemical experimentation, [4][5][6][17][18][19] real-time active machine learning (ML) with on-line data processing and feedback for iterative optimization, 4,[20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35] and interpretable machine learning for generating knowledge, [36][37][38][39] each playing a pivotal role in evolving traditional methodologies. Centrl to this modern era are self-driving labs 40 that are further integrated with theoretical simulations and extensive databases, revolutionizing how catalysts are created and optimized.</p>
<p>Recently, large language models (LLMs) such as GPT-x, ERNIE Bot, Claude-x, and Lamma-x, 41 have begun to dramatically enhance these four technological pillars.By processing natural language, automating code generation and data analysis, optimizing design of experiment (DoE) algorithms, and facilitating human-computer interaction, 16,[42][43][44][45][46][47] LLMs are setting new standards for efficiency and innovation in catalysis research (Fig. 1).These capabilities allow for the extraction and utilization of data from diverse and unstructured sources such as scattered texts, videos, and images, previously inaccessible to more traditional ML technologies that relied on well-organized datasets.</p>
<p>Moreover, automated and intelligent robotic systems, which have seen signicant adoption over the last decade, spanning from ow systems 19,48,49 to desktops 50,51 and humanoid mobile robots, 4,5 now seamlessly integrate with advanced LLMs.This synergy is reshaping decision-making strategies within the eld, transitioning from traditional methods like Bayesian optimization 4 and active learning 32 to more sophisticated, LLM-enhanced approaches, 45,47 towards more talented self-driving labs for closed-loop discovery.This is only the beginning of a shiing paradigm to on-demand catalyst development and in silico performance scanning for catalyst design and optimization.</p>
<p>Despite these technological advances, the role of the human researcher remains indispensable.7][38][39] Articial neural networks (ANNs) 52 used to be regarded as black-box models that are hard to explain, but recent innovations such as SHapley Additive exPlanations (SHAP) 53 for graph neural networks (GNNs) and attention mechanisms in transformer models are enhancing the transparency of articial neural networks, which were previously considered opaque.In addition, LLMs have also showcased their capabilities in extracting data mapping and articulating them in a clear plain language format.</p>
<p>Given the rapid pace of these advancements, it is timely to review the revolutionary shi in AI applications for catalysis research and development.This review will delve into how the integration of LLMs is redening the four foundational ML technologies in catalysis, providing a historical perspective and discussing recent implementations that foreshadow the future of AI-assisted catalyst design.</p>
<p>High-throughput chemical information extraction</p>
<p>Traditionally, data extraction required manual efforts, which has successfully underpinned the establishment of chemical databases like Reaxys 54 and SciFinder. 55With the increasing demand to autonomously gather and standardize chemical information effectively, the development of automated data extraction methods has split into two primary directions: the extraction of chemical information from gures including optical chemical structure recognition (OCSR), [7][8][9][10] and text information extraction.Both avenues benet signicantly from enhancements provided by pre-trained LLMs. 15,16</p>
<p>Information extraction from gures</p>
<p>A considerable amount of chemical information resides in gures, rendering Optical Chemical Structure Recognition (OCSR) essential for converting these complex visual data into accessible and interpretable formats.The primary task of OCSR is to transform visual representations of chemical structures into formats ready for computer processing.We now list and briey discuss these different computer-ready formats.</p>
<p>2.1.1String representations.SMILES (Simplied Molecular Input Line Entry System): known for its human readability, SMILES translates chemical structures into linear text strings.SMARTS (SMILES Arbitrary Target Specication): an extension of SMILES, SMARTS allows for dening substructural patterns within molecules, enhancing search and analysis capabilities.</p>
<p>InChI (International Chemical Identier): provides a structured and layered representation of chemical data, facilitating interoperability across different data systems.SELFIES (Self-referencing Embedded Strings): designed to ensure the validity of molecules represented, enhancing data integrity.</p>
<p>These string representations, integral to systematic chemical naming, have become increasingly valuable with the advent of language models.The seamless integration of these formats into LLMs enhances their utility, making them more than just systematic nomenclature but a dynamic part of molecular data processing.Furthermore, the development of multi-modal large models allows for directly translating structural drawings to the string representations without prior conversion, marking a signicant advancement in the eld. 56g. 1 The workflow of catalyst design and discovery with information extraction, automated chemical experimentation, active machine learning, and interpretable machine learning.</p>
<p>2.1.2Graph-based representations.Transforming chemical drawings into graph-based representations views molecules as nodes (atoms) and connections as edges (bonds), aligning with computational analysis methods in machine learning and network theory.</p>
<p>2.1.3Evolution of OCSR technology.Initially, OCSR technology was predominantly rule-based, with the rst systems developed in the early 1990s. 57Today, state-of-the-art OCSR systems combine rule-based methods with machine learning techniques to improve accuracy and efficiency. 9,58,59,76This hybrid approach addresses the challenges of interpreting complex chemical drawings and converting them into machinereadable formats.We will delve into these technologies in more detail, particularly focusing on recent advancements with multimodal pre-trained large models.</p>
<p>2.1.4Rule-based OCSR.Rule-based OCSR systems are designed to automate the extraction of chemical data by emulating human perceptual abilities.These systems perform a range of tasks including character detection, shape recognition, and the identication of entity connections.][62][63][64] 2.1.4.1 Segmentation challenges.The initial and crucial step in rule-based OCSR is the segmentation of chemical structures from potentially complex images.This task is challenging and critical as it sets the foundation for all subsequent analyses.Early rule-based models such as optical recognition of chemical structures (OROCS), chemical literature data extraction (CLiDE), [65][66][67] the optical structure recognition application (OSRA) and Imago 68,69 faced signicant challenges in accurately segmenting chemical structures.These systems oen struggled with noisy data and the presence of fragmented characters or text lines adjacent to the chemical structures.</p>
<p>In 2014, Simone Marinai et al. 70 made an improvement by introducing a Markov logic-based probabilistic logic inference engine (Fig. 2).This development improved the ability to clean up noisy extractions, although challenges with fragmented elements persisted.More recently, in 2021, Yifei Wang et al. 59 advanced the eld further by employing a Single Shot MultiBox Detector (SSD) neural network combined with a Non-Maximum Area Suppression (NMAS) algorithm.This combination was specically designed to enhance object identication within a single frame, signicantly improving segmentation accuracy to 89.5% on a dataset of 2100 handwritten cyclic compound samples.</p>
<p>2.1.4.2 Inherent limitations.Despite these advancements, rule-based systems are oen limited by two major factors:</p>
<p>(1) Insufficient understanding of embedded rules: the complexity of the embedded rules can lead to misinterpretations and errors in data extraction.</p>
<p>(2) Susceptibility to noise: the intricate rules are prone to interference from noisy data, which can degrade the quality of the output.</p>
<p>MSE-DUDL combines a convolutional neural network (CNN) known for its prowess in visual pattern recognition, and a long short-term memory (LSTM) network equipped with an "attention" mechanism.This attention mechanism allows the model to focus selectively on different parts of the molecular structure, facilitating accurate SMILES prediction.While the method achieved an accuracy of 83% on a specialized test set, it faced limitations in recognizing certain complex chemical structures and stereochemical details, and struggled with images presented in inverted formats.The accuracy and reliability of OCSR continue to improve as newer models are developed and rened.The use of multiple models for cross-validation purposes enhances robustness, offering better performance than what could be achieved by a single model.This progress is vital as it addresses the signicant challenge of extracting organic reaction data on a large scale, a task that is increasingly crucial due to the exponential growth of available chemical data.</p>
<p>Other visual information extraction.</p>
<p>The extraction and analysis of experimental data, particularly data presented in gures, are critical yet challenging tasks in chemical research.Beyond the mere detection of chemical structures, there is a signicant need for advanced capabilities to analyze experimental data comprehensively.This task requires a multimodal approach that can integrate and cross-validate information from both gures and textual descriptions, an area that remains relatively underdeveloped.</p>
<p>2.1.6.1 Advancements in multimodal large models.Recent advancements in AI have introduced multimodal large models, such as GPT-4, Gemini, and Claude, which have demonstrated promising capabilities in summarizing information from diverse sources.These models can be adept at extracting and synthesizing comprehensive experimental data from the scien-tic literature on catalysis.</p>
<p>2.1.6.2Capabilities of multimodal large models in chemical data analysis 2.1.6.2.1 Graphical data analysis.Many of these advanced models are now capable of interpreting trends and patterns directly from graphical representations, although the variability in data presentation styles continues to challenge the accuracy and reliability of the extractions.more intuitive interfaces between researchers and computational systems.</p>
<p>2.1.6.2.3 Integration with OSRA.Efforts are ongoing to integrate systems like the Optical Structure Recognition Application (OSRA) with multimodal LLMs to enhance the extraction of chemical structures from the literature.For instance, DP Technology's introduction of the Uni-Finder module represents a step forward (still at a testing stage on May 5th 2024).This module is designed for the comprehensive reading of scientic documents, including journal papers and patents, which facilitates a deeper understanding and utilization of published research.</p>
<p>The continuous improvement of multimodal LLMs is expected to revolutionize how scientic results are communicated and utilized.As these models become more sophisticated, they will enable the scientic community to integrate vast amounts of data in unprecedented ways.This integration is anticipated to lead to the development of new tools that could dramatically enhance the efficiency and creativity of catalyst design processes.The ability to compile and analyze the extensive data generated globally by researchers represents a transformative shi towards data-driven science, promising signicant advancements in how we discover and develop new materials.</p>
<p>Text information extraction with language models</p>
<p>Before the advent of large language models (LLMs), there was signicant effort in natural language processing (NLP) dedicated to extracting chemical information from texts.This process involved several traditional NLP tasks such as named entity recognition, relation extraction, and the construction of knowledge graphs. 77,78In named entity recognition, entities (which could be single words or phrases) are identied and categorized within the text, facilitating the detection of reagents, products, catalysts, and other chemical entities.Relation extraction focuses on identifying the connections between these entities, while knowledge graphs organize these entities and their relationships into structured representations.This foundation has enabled the creation of catalysis datasets related to topics like hydrogen production, 12 CO 2 reduction, 13,14 and single-atom heterogeneous catalysis. 79.2.1 Evolution of tools and techniques 2.2.1.1ChemDataExtractor.The ChemDataExtractor tool, 80,81 developed as early as 2016, utilizes word tokenization, clustering, and traditional machine-learning models to extract chemical knowledge from the literature.This tool can identify compounds and their properties, setting a precedent for the integration of more sophisticated models.In 2021, Regina Barzilay et al. developed the ChemRxnExtractor, 82 a two-stage deep learning architecture based on transformer models.This system uses product extraction and reaction role labelling to structure chemical data. The tansformer architecture's attention mechanism allows the model to concentrate on relevant parts of the data for different tasks, and its adaptive pre-training on large-scale unlabelled text has signicantly improved its ability to identify and organize chemical information from textual sources (Fig. 4).It achieved notable F1 scores of 76.2% for product extraction and 78.7% for reaction role labelling on a specialized dataset.</p>
<p>2.2.1.2SciBERT.3][14][15][16] These models have effectively turned the extraction of text-based data from scientic papers into a nearly solved challenge.</p>
<p>2.2.1.3LLMs.Omar M. Yaghi et al. 16 utilized OpenAI's GPT-3.5 to extract and format synthesis information of metalorganic frameworks (MOFs) from the literature.They addressed the hallucination issue in LLMs through careful prompt engineering and context provision (Fig. 5).The process involved segmenting the text, creating numeric vectors to represent each segment, comparing vectors to the ones of predened synthesis descriptions, and choosing the segments with high similarity.GPT-3.5 then classied these segments as 'synthesis' or 'nonsynthesis' using in-context learning (ICL), before formatting the synthesis information into tables.This approach, which also led to the development of a chemistry chatbot, demonstrates a promising framework for using LLMs for extracting and organizing scientic information.</p>
<p>Summary</p>
<p>In the domain of chemical information extraction, advancements have been marked by the development and deployment of diverse methods and tools.These technologies are succinctly summarized in Table 1 and are broadly categorized into three primary types based on the underlying technology: rule-based OCSR, machine learning-based (ML-based) OCSR, and language model-based (LM-based) systems.</p>
<p>The rule-based OCSR systems, once dominant, are now increasingly complemented or surpassed by neural networkbased methods due to their exibility and growing accuracy.These machine learning-based systems are not only more adaptable but also continue to improve as they learn from more data.The incorporation of rule-based techniques as a supplementary approach provides a layered methodological depth that enhances the overall robustness and generalizability of these technologies.</p>
<p>Language model-based systems, particularly those utilizing advanced LLMs, represent the frontier of chemical information extraction.Although their full potential is yet to be realized, the rapid evolution into multimodal models suggests that transformative developments could emerge shortly.These models are particularly promising for handling the vast and complex data typical in catalysis research.The transition to open-source methods has also played a critical role in this eld.Beginning with systems like OSRA in the 1990s, the move towards open-source has not only facilitated wider access to advanced tools but has also spurred innovation and customization, enhancing the collective capability of the research community.</p>
<p>This evolving landscape of chemical information extraction methods underscores the importance of continual adaptation and development to harness the ever-increasing volumes of data in catalysis and other elds of chemistry.</p>
<p>Automated and intelligent chemical robotic system</p>
<p>Automation technologies have profoundly transformed modern manufacturing, yet their integration into chemical research remains limited.This is primarily due to the challenges in meeting the diverse and exible synthesis and characterization requirements of various chemical systems.Effective machine learning applications in this context demand a densely populated dataset within the search space to develop reliable models and derive meaningful insights.Consequently, the experimental systems employed must be both high-throughput and dependable.</p>
<p>4][85][86] The origins of chemical automation date back to the 1960s and 1970s with the development of automated devices like automated peptide synthesizers, 87 DNA synthesizers, 88 and organic synthesis modules. 891][92][93][94][95][96] More recently, the introduction of humanoid chemical robots [4][5][6] and autonomous ow-based synthesis platforms [17][18][19] has marked a new era of innovation in intelligent chemical synthesis.</p>
<p>A notable feature of this latest advancement is the interactive "ask and tell" process, such as active learning, where models are continuously trained on current observations and actively request additional data.This interactive approach can signicantly accelerate discovery efficiency compared to traditional screening strategies. 97Therefore, experimental processes must be designed to be not only high-throughput but also sufficiently exible to allow frequent access and modications.This is also the stage where LLMs can contribute, integrating crucial domain knowledge to enhance exploration and decision-making processes.</p>
<p>In this section, we will discuss how advancements in hardware design, coupled with LLMs, enhance operational exibility.Later, we will explore the promising potential of LLMdriven active learning in the subsequent section.</p>
<p>Automated and intelligent chemical experiment platform</p>
<p>To address diverse research tasks, various hardware design principles and methods were employed in building automation systems.This review will cover two categories of the systems:</p>
<p>(1) Humanoid robotic systems: this approach relies on the usage of multi-axis arms that provide a high degree of operation exibility, mimicking the behavior of human operators.</p>
<p>(2) Automated ow chemical systems: these systems are designed on the foundation of uid dynamics and transport pipelines to achieve precise chemical operations, which can be seamlessly interfaced with analytical instruments.</p>
<p>Humanoid robotic system</p>
<p>In a laboratory environment, a robotic arm coupled with automated guided vehicles (AGVs) and advanced computer vision systems 5 can robustly complete tasks such as sample preparation and handling, control of instruments, and integration of data recording, analysis, and experiment design.Key to this scheme is the exibility introduced by AGVs and robotic arms as compared to that of their predecessors.</p>
<p>The AGV-based autonomous mobile robot system launched by Andrew I. Cooper et al. 4 is a remarkable advance in chemical automation.The team found improved photocatalysts for producing hydrogen from water aer autonomous running for 8 days, completing 688 experiments in a design space of 10 variables.The robot (Fig. 6) can handle sample vials among eight workstations distributed around the lab, including a solid reagent dispensing system, a mixed liquid dispensing system and capping module, an ultrasound module, a photolysis module, a gas chromatography (GC) analysis module, and three separate sample storage modules to achieve a variety of experimental tasks.</p>
<p>Despite the great advances, the mobile robotic chemist from Cooper's group is purely driven by Bayesian algorithms and does not capture existing chemical knowledge or include theoretical or physical models.Later, a comprehensive articial intelligence chemistry laboratory (Fig. 7) was developed by Jun Jiang's team. 5This AI-Chemist consists of three modules, including a machine-reading module, a mobile robot module, and a computational module.The AI-Chemist system responds to scientic questions posed by researchers by tapping into vast amounts of literature.It digitizes and standardizes experimental protocols, enriching its knowledge base.The platform manages tasks, monitors the mobile robots, customizes experiment workows, and stores the data for future use.The research team used the platform to nd the best combinations of several Martian meteorite rocks to synthesize efficient water oxidation catalysts for future use in Martian exploration. 98he recent A-lab, developed by Gerbrand Ceder et al., 6 represents a signicant advancement in the eld of solid material synthesis.Despite some controversy on the actual phases of the fabricated materials, the hallmark of the A-lab is its high degree of automation, which encompasses the entire synthesis and characterization process, including critical steps such as powder dosing, sample heating, and X-ray diffraction (XRD) for product characterization.</p>
<p>One critical issue with the robotic arm system in laboratory settings is its moderate capacity to parallelize experimental tasks.While robotic arms bring automation and precision to the table, they still mimic human researchers to conduct multiple operations one by one.This constraint is particularly evident in high-throughput settings where speed and efficiency are paramount.To address this, integrating robotic systems with other automated solutions might be necessary.</p>
<p>Automated ow chemical system</p>
<p>01][102][103][104] The reactors used in the ow system can be categorized into two distinct types: batch reactors connected by pipelines and continuous ow reactors.The major advantage of the ow system comes from low-cost modularity, where the reaction module, product separation module, and detection module can all be connected to the same pipeline in sequence or parallel.</p>
<p>3.3.1 Batch reactors.An example of the batch reactor system connected by pipelines is the Chemputer developed by Leroy Cronin et al. in 2019. 17It is a general automated platform for organic synthesis (Fig. 8) with a uid backbone from a series of syringe pumps and six-way valves.The materials can be transported among modules.The modules support many operations including mixing, ltration, liquid-liquid separation, evaporation, and chromatographic separation.The same research team 18 has also introduced an autonomous workow to read the literature and execute experiments.A chemical description language (cDL) that aims to include all the synthesis operations in a standard format was proposed.Utilizing this system, the authors showcased the automated synthesis of 12 compounds from the literature, encompassing the painkiller lidocaine and several other pivotal molecules.]105 One drawback of many ow systems is the lack of exibility for different experiment tasks.One solution is to use general modules and their combination to support wider experiments.Alternatively, the modules can be reaction-specic as long as they can be designed and fabricated efficiently.Leroy Cronin et al. 49 showcased a portable, suitcase-sized chemical synthesis platform with automated on-demand 3D printing of groups of reactors for different reactions.Researchers demonstrated the broad applicability of this system by synthesizing ve organic small molecules, four oligopeptides, and four oligonucleotides, achieving good yields and purity.</p>
<p>The implementation of batch reactors with increased throughput has accelerated the search for catalysts in more complex systems that involve multiphase reactions.Cheng Wang et al. 106 developed a fast screening platform with a coherent implementation of automated ow cell assembly and GC characterization.It was used for parallel synthesis, electrochemical characterization, and catalytic performance evaluation of electrocatalysts for the reduction of CO 2 to C 2+ products, which led to the discovery of a Mg-Cu bimetallic catalyst with competitive CO 2 to C 2+ performance and good stability compared to the top catalysts from other literature reports (Fig. 9).</p>
<p>3.3.2Continuous ow reactors.Continuous ow reactors 107 provide a scalable solution for organic molecule synthesis, 103,108 inorganic material preparation, 109,110 colloidal nanomaterial synthesis, 111,112 and electrochemical synthesis, 113,114 and have gained wide applications in industry.</p>
<p>The reactants are rst pumped into a mixing device and then ow into temperature-controlled pipes or microstructured reactors until the reaction is complete.][117][118][119][120][121][122] Timothy F. Jamison et al. 115 developed a exible, manually recongurable benchtop ow chemistry platform (Fig. 10), including various reactor modules for heating/cooling, photochemical reaction, and packed bed reaction.In addition, the platform integrates liquid-liquid separation technology and is equipped with inline analysis tools such as high performance liquid chromatography (HPLC), Fourier transform infrared spectroscopy (FTIR), Raman spectroscopy, and mass spectrometry.</p>
<p>One issue of the continuous ow system is its high cost in paralleling and adaptation.To partly address this issue, Kerry Gilmore et al. 116   by Nathan Collins et al. 117 in an advanced automated continuous ow synthesizer called AutoSyn, which can access 3800 unique process combinations and up to seven consecutive reaction steps for efficiently preparing a variety of pharmaceutical small molecule compounds with a scale from milligrams to grams within hours.</p>
<p>To make the uidic system even more adaptive, Klavs F. Jensen et al. 123 combined the robotic arm and the ow system (Fig. 11): the robotic arm is responsible for assembling modular process units, including reactors and separators, into a continuous ow path.Aer the synthesis, the robotic arm can disconnect the reagent lines and move the processing module to the appropriate storage location.Pneumatic grippers are used to ensure tight connections between process chambers.In 2023, the same group introduced a prototype that further incorporates machine learning with robotics to autonomously design, synthesize, and analyze dye-like molecules with minimal human intervention. 124This system successfully synthesized and characterized 303 new dyes, advancing the efficiency of chemical discovery.</p>
<p>Flow chemistry systems, while revolutionizing chemical synthesis and processing, present several limitations in automation.The setup and maintenance of these systems are complex and resource-intensive.Establishing precise control over ow rates, temperature, and pressure requires specialized equipment and expertise.This complexity also extends to scalability issues; while ow systems excel in scaling up certain types of reactions, they may be less adaptable for reactions requiring long residence times or intricate synthesis steps.Additionally, the rigidity in altering reaction conditions can limit their exibility, making them less suitable for laboratories that frequently switch between diverse chemical processes.Material compatibility is another concern, as the construction materials of the ow reactors must withstand a wide range of chemicals and conditions, limiting their use with highly reactive or corrosive substances.Furthermore, while adept at handling large-scale production, ow chemistry systems can be less efficient for small-scale synthesis, oen leading to inefficiencies and wastage when dealing with minute quantities.</p>
<p>Large language models and robots</p>
<p>The introduction of LLMs to robotic systems denes a new frontier in automation.</p>
<p>First, LLMs have facilitated the development of robotics, including log information extraction, assisted robot design, 125 and task generation and planning. 42,43,126,127As pointed out by Francesco Stella et al., 125 LLMs can be the creator for designing the automating system, be the mentor and copilot for domain scientists who do not have the necessary educational background to implement automation in their research, and be an assistant to debugging, troubleshooting, and method selection during the technology implementation phase to accelerate the process.</p>
<p>Second, LLMs, especially the multimodal ones, can help develop next-generation robots with increased exibility.Vemprala and others from the Microso team 126 proposed a strategy that combines prompt engineering and a high-level feature library to enable ChatGPT to handle various robotic tasks and scenarios.An open-source tool called PromptCra was  introduced, which includes a collaboration platform and a ChatGPT-integrated sample robot simulator.However, the LLM-controlled robotic movement is not robust enough for direct use in chemistry experiments where safety and reliability are of primary concern.</p>
<p>Third, LLMs also offer solutions to program robots.Kourosh Darvish et al. introduced the CLAIRIFY method, 42 which combines automatic iterative prompting with program verication to ensure the syntactic accuracy of task plans and their alignment with environmental constraints.The system's objective is to produce a syntactically correct task plan suitable for robotic action as a prompt for LLMs to generate a program.However, the generated plan needs to be veried to detect any compilation error and pass the error messages as subsequent input prompts for iterative interaction with the LLMs.The capability of this method was demonstrated by translating natural language to an abstract and concise high-level chemical description language (cDL), which was originally developed and used in the control of Chemputers. 18ompared to high-level descriptive codes, generating lowlevel operational codes to interface directly with the robotic system can be more complicated.Genki N. Kanda et al. 43 demonstrated that GPT-4 can generate low-level operational Python scripts for automated robots like Opentrons-2 (OT-2) from natural language instructions.They designed a pipeline based on GPT-4 to automatically translate natural language experimental descriptions into Python scripts compatible with OT-2.Leveraging OpenAI, this approach iteratively queries the model, extracts, and validates scripts using a simulator of OT-2, and provides feedback on any errors for correction.This shi towards natural language instruction simplies the automation process, making it accessible to a broader range of researchers and promoting the automation of biological experiments.</p>
<p>Summary</p>
<p>Automated and intelligent chemical robotic systems are promising to signicantly enhance the efficiency, accuracy, and reproducibility of experiments.Table 2 summarizes various types of automated and intelligent chemical robotic systems, detailing their specic functions, supported operations, characterization techniques, and chemical spaces they explored.These systems range from humanoid robotic systems to batch reactors and continuous ow reactors, each with unique capabilities and applications to study different chemical systems.</p>
<p>We expect a much enhanced automation level in chemistry research.However, current automation in chemistry still faces challenges, particularly in the trade-offs between the exibility and throughput of automated systems.For instance, although capable of vast amounts of operations compared to ow systems, humanoid robotic systems are usually slower in operational speed to ensure accuracy and safety.On the other hand, ow chemistry systems can handle hundreds or thousands of experiments per day, but are more task-specic with limited exibility.New developments in these strategies are required to enhance exibility, throughput, and robustness at the same time.</p>
<p>Another challenge lies in the control part of the robotic systems.Although digital twins are very common for humanoid robotics and in industry, the development of digital twins for the whole automated chemistry system is still at its initial stage despite a few efforts. 4,18,128Ensuring the integrity and safety of experimental procedures remains paramount in automation labs.Therefore, greater attention must be directed toward enhancing the capability to simulate experimental procedures and detect any potential physical or chemical issues during the development of various robotic systems.Furthermore, despite the rapid advancements in novel algorithms, such as reinforcement learning, the control of robots in chemistry labs oen relies on hardcoded programming.This limitation restricts their ability to perform complex tasks and adds challenges to the maintenance, transferability, and future development of the systems.LLMs appear promising in introducing exibility to control systems.However, the reliability of LLMgenerated code must be veried either by human experts or through digital twins.It is foreseeable that digital twins and LLMs will soon be more cohesively integrated into the control of chemical robotic systems.</p>
<p>Design and discovery of catalysts with active machine learning</p>
<p>In the discovery of catalysts, the search space is oen vast and grows exponentially with the number of parameters.This inherent complexity makes the traditional trial-and-error approach for catalyst screening both labor and computationally intensive and time-consuming.The emergence of ML and LLMs has provided opportunities to address this problem.By utilizing ML and LLMs to guide experimental design with experimental or theoretical feedback, the search for catalysts can be signicantly accelerated.</p>
<p>Design of catalysts guided by machine learning</p>
<p>The implementation of ML in experimental design can lead to more efficient and cost-effective research.Olsson 129 denes active machine learning as a supervised machine learning technique in which the learner (i.e., the machine learning model) determines the sampling point from which it learns.Bayesian optimization (BO) 130 and active learning (AL) are two important branches of active machine learning that are applied in catalyst design.</p>
<p>BO is an optimization strategy that balances the exploration of uncertain regions and the exploitation of known regions with superior objective values.It is generally used to optimize a black-box function and consists of three key components:</p>
<p>(1) Surrogate model: this is a predictive model designed to approximate the underlying function.][135][136] (2) Acquisition function: an acquisition function is a scoring function used to rank sampling points within the input space based on the surrogate model's predictions.Examples of such</p>
<p>Chemical Science</p>
<p>Review functions include expected improvement (EI), 137,138 probability of improvement (PI), 139 and upper condence boundary (UCB). 140The acquisition function is instrumental in selecting the most promising candidates for further evaluation.</p>
<p>(3) Bayesian inference: 141 this is a foundational technique in Bayesian optimization, utilized for training the surrogate model.It uses Bayes' theorem to update the probability of a hypothesis or event based on observed evidence.</p>
<p>On the other hand, AL is a family of machine learning techniques that aims to minimize the number of labelled data points while obtaining a high-performance model.It can usually be achieved through an adaptive sampling strategy, which prioritizes the labelling of data points with the highest uncertainty and information gain for the model.</p>
<p>Both BO 4,20-28 and AL [29][30][31][32][33][34] have been applied in the design of and search for catalysts.BO can efficiently explore the vast parameter space of catalyst design and select experiments that are likely to yield the desired products.By iteratively updating the ML model and selecting new experiments based on the retrained model, BO can guide the search for optimal catalysts.AL, in the meantime, can assist in selecting the most informative data points for labelling, reducing labelling costs while improving model performance.It has been applied in many elds including materials design, 142,143 retrosynthesis, 144,145 and drug discovery. 146,147Besides the original purpose of AL, its application in catalyst design also demonstrated its capability for global optimization, presenting a remarkable analogy to the BO framework.The applications of BO and AL in the eld of catalysis will be discussed respectively below.</p>
<p>Bayesian optimization</p>
<p>BO effectively balances exploration and exploitation to identify the best candidates within the design space.The method can signicantly reduce the number of experiments required to nd the optimal reaction parameters or formulations.For example, in 2020, Yusuke Yamauchi and coworkers 20 employed BO to efficiently discover ternary PtPdAu alloy catalysts.They exhibited excellent catalytic activity in electrochemical methanol oxidation (Fig. 12).Remarkably, through only 47 experiments, which is less than 1% of the potential composition space, the authors successfully discovered the optimal composition with a high catalytic performance.More interestingly, the sampling scheme using current density as the performance metric yielded a precursor composition with minimal Au content, which would have been challenging for chemists to predict.Thus, the implementation of BO can not only accelerate the search for catalysts but also offer new insights into the design of catalysts.In 2020, Bayesian experiments for autonomous researchers (BEAR) 21 combined BO with high-throughput automated experiment systems to achieve self-driven material discoverya cycle of the design of experiments, automated experiment feedback, and retraining of machine learning models to design new experiments.As discussed before, Andrew I. Cooper et al. 4 developed an AI chemist to improve the catalytic performance for hydrogen production with BO (Fig. 13).It successfully discovered a mixture of photocatalysts that exhibited six times higher activity than the original formulation.Compared to manual operations, the experimental time cost is reduced by approximately 60 times.</p>
<p>In 2021, Jan Rossmeisl et al. 22 developed a computational framework that combines density functional theory (DFT) calculations, ML-driven kinetic modelling, and BO to explore a wide range of composition space to search for multi-component high entropy alloys for the oxygen reduction reaction (ORR).To accelerate catalyst discovery, the authors integrated kinetic modelling with BO, where a Gaussian-process-based surrogate model provided suggestions for alloy compositions.The proposed compositions were evaluated using the kinetic model, and the surrogate model was updated based on the ORR activity predicted by the kinetic model.BO effectively identied optimal compositions through 150 iterations, including Ag 18 Pd 82 , Ir z50 Pt z50 , and Ir z10 Pd z60 Ru z30 (Fig. 14).These compositions closely matched the optimal compositions found through grid search in the same chemical space.Experimental conrmation of the three optimized compositions by highthroughput thin-lm synthesis and ORR testing in the Ag-Pd, Ir-Pt, and Pd-Ru binary alloy spaces, reveals the best-performing compositions of Ag 14 Pd 86 , Ir 35 Pt 65 , and Pd 65 Ru 35 .The experimental results reasonably matched the results of BO, and BO can accelerate the discovery of optimal catalysts by up to 20 times.</p>
<p>Active learning</p>
<p>Active learning is a strategy that explores the design space to establish a precise and reliable mapping from it to an output space (e.g.various properties of compounds) and optimizes toward high-performance solutions.Active learning can be used to reduce the number of expensive DFT simulations for the design and screening of catalysts in a large space.</p>
<p>Yousung Jung et al. 30 proposed an active learning method in the discovery of catalysts for the CO 2 RR driven by uncertainty and prediction error.It utilizes cost-effective non-ab initio input features, i.e., LMTO d-band width and electronegativity, as chemisorption descriptors to predict adsorption energies on alloy surfaces.Screening of large-scale materials is carried out by combining these descriptors with two machine learning models: an ensemble of articial neural networks (ANNs) and kernel ridge regression (KRR).The catalytic performance of a set of 263 alloy systems was studied by predicting <em>CO binding energy using the models.During the active learning process, an ensemble consisting of ve neural networks with the same architecture but varied initial weights was trained on an initial dataset.The ensemble was used to predict the </em>CO binding energy on the rest of the dataset to nd candidates with the highest prediction variance, which will be included in the next training process.As an alternative machine learning model, the performance of KRR 148,149 was also elaborated.It involves the training of a KRR model on the initial dataset with <em>CO binding energy as the output.Then, an additional KRR model was trained on the prediction error from the previously trained model as an error predictor. 148,150Later, the KRR error predictor was used to estimate the error rate for the rest of the dataset, which helps select candidates for the next round of training.Both models (ensemble of ANNs and the KRR model) were used to predict the adsorption energy of CO on (100) crystalline surfaces.The best model gives an RMSE of only 0.05 eV without the d-band center as a descriptor.The authors discovered Cu 3 Y@Cu</em> to be a highly active and cost-effective catalyst for the CO 2 RR.</p>
<p>Besides the original purpose of using active learning to establish an accurate and reliable model, it can also be utilized for global optimization.In 2018, Zachary W. Ulissi et al. 31 proposed a cyclic workow with ideas from agent-based model optimization and active learning for screening electrocatalysts for the CO 2 RR and HER.This workow, illustrated in Fig. 15, involves machine learning screening, DFT validation, and machine learning retraining.To start, the researchers obtained a search space of intermetallic crystals and their corresponding surfaces from the Materials Project. 151They then selected a series of materials as optimal candidates for catalysis using a machine-learning model.DFT calculations for the selected candidates were performed, providing more accurate predictions of the catalytic properties.The DFT results were then used to retrain the machine learning model, creating an iterative process for continuously improving the catalyst database.In their study, the authors considered a total of 31 elements, composed of 50% d-block elements and 33% p-block elements.The search space consists of 1499 intermetallics for potential catalysis applications.131 possible surfaces from 54 alloys and 258 possible surfaces from 102 alloys were identied as valid candidates for the CO 2 RR and HER, respectively.The number of candidate alloy catalysts can be further reduced to 10 and 14 for the CO 2 RR and HER.This comprehensive screening approach allowed for the identication of theoretically promising catalysts for the CO 2 RR and HER.</p>
<p>In 2020, Edward H. Sargent et al. 32 developed a machine learning-accelerated, high-throughput DFT framework for rapid screening of CO 2 RR electrocatalysts (Fig. 16) similar to the one from Zachary W. Ulissi's group 31 described above.The researchers studied a dataset of 244 different copper-containing intermetallics, forming a search space of 12 229 surfaces and 228 969 adsorption sites.DFT simulations were performed on a subset of these sites to calculate the CO adsorption energies.These data were then used to train machine learning models to predict the CO adsorption energy on the adsorption sites.The researchers encoded each adsorption site as a numeric array and used a combination of random forest and boosted trees to enhance prediction performance.The framework combined the machine learning predicted CO adsorption energy with the volcano scaling relationship to identify sites with the highest catalytic activity.These optimal points were then simulated using DFT to provide additional training data for the machinelearning model.Thus, an active learning workow was established, cycling between DFT simulations, machine learning regression, and machine learning prioritization, to continuously query and construct a DFT database.This workow performed over 300 regressions, which guided DFT calculations for</p>
<p>Chemical Science Review</p>
<p>CO binding energies at approximately 4000 different adsorption sites to identify Cu-Al as the most promising material for the CO 2 RR in the search space.Furthermore, the authors synthesized de-alloyed nanoporous Cu-Al catalysts for validation, which achieved over 80% Faraday efficiency (compared to ∼66% for pure Cu) at a current density of 400 mA cm −2 (1.5 V vs. NHE).It showed a 2.8-fold improvement in cathodic power conversion efficiency (PCE) at 400 mA cm −2 compared to previous state-of-the-art results.This work demonstrated an effective method for high-throughput catalyst screening, combining machine learning and DFT calculations.</p>
<p>While BO and AL are initially different approaches, they tend to converge on the catalyst optimization task.BO usually uses a probabilistic model with the goal of optimization, while AL can adopt more diverse models with the goal of efficiently constructing a machine learning model.When AL also used a probabilistic model and assessed uncertainty in making the decision about which point to explore next, it is equivalent to exploration-oriented BO, but the ultimate goal of AL is to improve the model most efficiently, which is beyond the uncertainty strategy.</p>
<p>When all the obtainable information about the system comes from the previous experimental/calculation results, BO and AL are mathematically sound methods to most efficiently explore the space.However, when domain knowledge is available, it is possible to come up with a more efficient strategy by combining the testing information with domain knowledge.The addition of domain knowledge into the process can be achieved by using LLMs.</p>
<p>Design and synthesis of catalysts guided by large language models</p>
<p>The diverse and interdisciplinary knowledge spanning chemistry, materials science, computer science, and data science, which are needed for the data-driven design and discovery of catalysts, can present a formidable challenge for researchers.LLMs 152,153 offer a promising solution to overcome the knowledge gaps from multiple elds efficiently.LLMs have been used by chemists for tasks such as catalytic reaction prediction, 45 property prediction, [154][155][156][157] and synthesis condition design. 156,158n BO and AL, a machine learning model (or a surrogate model) is necessary to approximate a mapping.Traditional machine learning models can take continuous, discrete, or categorical variables as the input.In contrast, LLMs, with their inherent capabilities to process natural language descriptions and generate new content accordingly, can be potentially used as a surrogate model, which can support a versatile input format.To incorporate the training data into the models, incontext learning (ICL), a technique that includes training data as examples in the prompt for LLMs, can be used.Alternatively, ne-tuning the models using the existing dataset represents another viable approach.</p>
<p>Review</p>
<p>Chemical Science</p>
<p>Andrew D. White's group 45 demonstrated the usage of LLMs as the surrogate model in Bayesian optimization.The aim is to use a generative pre-trained transformer (GPT) as a surrogate model to predict the properties of the product according to the experimental procedure.Both ne-tuning and ICL were used for model training.To introduce prediction uncertainty when querying the LLMs, they designed two prompting strategies, a (1) multiple-choice option template and (2) top k completions template for regression.With the multiple-choice template, the LLM will treat the regression problem as a multi-option question to give a predicted value in one of the ve ranges.Furthermore, the probability of selecting each option can be accessed.In the top k completion template, the question will be queried k times to the LLM to generate k answers.Both strategies generated a discrete probability distribution of the output, which can be used in Bayesian optimization.The authors used a series of models from OpenAI (text-curie-001, text-davinci-003, GPT-4, etc.) with in-context learning or ne-tuning to predict the C 2 yield for oxidative coupling of methane based on synthesis procedures.Gaussian process regression was used as a baseline with text embedding to convert the synthesis description to a numeric input.Among the LLMs, GPT-4 is the best model in either ICL or ne-tuning.When GPT-4 and the top-k completion strategies were used, the ICL model showed comparable performance (mean absolute error, which is abbreviated as MAE, of 1.854) to the Gaussian process regression (MAE of 1.893).When the ne-tuning was implemented, the MAE of the model was further decreased to 1.325.Later, the authors implemented Bayesian optimization using the Gaussian process or LLMs with ICL as the surrogate model.The ICL model reached 99% quantile aer 15 samples, aer which the performance did not improve signicantly and failed to nd the maximum value in the sample pool.Although the GPR model also failed to nd the maximum in the sample pool, it was a little closer to the maximum and showed a higher efficiency in the optimization.Due to the token size limitation and the complexity of the C 2 data, the authors only selected the ve most relevant examples during ICL, which can be the reason</p>
<p>Chemical Science</p>
<p>Review why the ICL model did not perform as well as the GPR model in Bayesian optimization.However, as a proof-of-concept, it is enough to demonstrate that LLMs have the potential to guide researchers in decision-making.The in-context learning ability of the LLMs is promising for building an interactive workow where an AI agent iteratively assists and instructs human experts to increase search efficiency through experimental feedback.Recently, Omar M. Yaghi and his coworkers have built such a workow and demonstrated its capability in the synthesis of MOFs with prompt engineering and in-context learning. 47This innovative workow involves three components: ChemScope, ChemNavigator, and ChemExecutor (Fig. 17).With the usage of Chem-Scope, the human researchers offer GPT-4 the project goals and necessary information like the literature of reticular chemistry and availability of lab resources to generate a project blueprint.Here, GPT-4 reads the general concepts of reticular chemistry and constructs a scheme of the project with multiple stages, where each stage contains well-dened objectives and indicators for their completion.Then, ChemNavigator and ChemExecutor were used coherently to go through the stages and achieve the objectives dened by ChemScope.ChemNavigator was used to dene tasks to complete the objectives of the current stage.It takes the project scheme from ChemScope, previous trial-and-error summaries, human feedback, and current situations to update the summaries and generate three tasks accordingly.With the updated summary and tasks from ChemNavigator, the ChemExecutor outputs step-by-step instructions to complete the task.Additionally, ChemExecutor also denes a template to record the experimental feedback from the human researchers, which will be used later in the next iteration.At this point, the human researchers will perform the experiments and ll up the template.The interaction among ChemExecutor, ChemExecutor, and human researchers was iterated several times until the completion of the project.The recording of experimental feedback and consistent updating of the summary enabled GPT-4 to learn from experiment outcomes and optimize protocols to complete the complex tasks.Using this human-computer interactive workow, the researchers successfully discovered and characterized a series of isomorphic MOF-521s.This work highlights the advantages of the large language model in interacting with human experts in natural language without coding skills, making it easy to use for all chemists.Additionally, the in-context learning facilitated by GPT-4 can continuously optimize experimental protocols to complete complicated research tasks.When such a workow is integrated with automated robotic systems, it paves the way for a new paradigm of self-driving labs, where the design and discovery of catalysts go beyond a purely data-driven approach.</p>
<p>Despite the potential applications of LLMs in the design of and search for catalysts, there are still some problems to be addressed.The major problem is the well-known hallucinations in the context generated by LLMs.Although researchers have tried to mitigate this issue through methods such as prompt engineering, in-context learning, and ne-tuning, further improvements are needed to enable the accuracy and reliability of these models.Secondly, LLMs with direct domain expertise are still lacking.Thus, when dealing with domain-specic scientic problems, the models need to be ne-tuned; otherwise they can show low accuracy and misunderstanding.While LLMs hold promise in chemical research, further research and improvements are necessary to overcome the existing limitations and bring the application of articial intelligence in the research of catalysts into a new era.Review Chemical Science</p>
<p>Summary</p>
<p>Traditional trial-and-error methods require a signicant cost of time in screening and testing candidate catalysts, together with inference through expert knowledge and occasionally serendipity.Active machine learning can lower the knowledge barrier and greatly accelerate the discovery process by utilizing experimental data to build surrogate models, avoiding brute-force or uniform search of the entire chemical space.The implementation of active machine learning in the optimization of catalyst search is summarized in Table 3.Several challenges persist in implementing active machine learning, particularly related to surrogate models.These models excel in interpolation rather than extrapolation, making them prone to overtting and necessitating training data of a specic scale.Many efforts are made to improve the surrogate models for higher generality (e.g., Phoenics 135 ) and extend variables from simple continuous variables to discrete or categorical variables (e.g., Gryffin 136 ).Additionally, a crucial challenge lies in selecting relevant catalysis features compatible with surrogate models.Incorporating irrelevant descriptors can impede the effectiveness of active learning algorithms, reducing their performance to that of uniform random search.The difficulty in feature selection connes certain closed-loop searches to mere recipe optimization, treating the process as a black box and adjusting only continuous variables such as reagent ratios or concentrations (Table 3).However, catalytic reaction activity and selectivity are closely linked to explicit factors such as intermediate adsorption energy, d-band center, electronegativity, and steric hindrance, which inherently serve as valid features.These features can be assessed through ab initio theoretical calculations or in situ characterization.While the advent of automated laboratories has alleviated concerns regarding insufficient data acquisition, it remains a costly endeavor, especially considering the challenges in automating certain characterization techniques.Consequently, strategies for evaluating and selecting an appropriate subset from these explicit features require further renement.The subsequent section will delve into the detailed elaboration of chemical descriptors employed in machine learning algorithms.</p>
<p>Interpretable machine learning for catalysis</p>
<p>In catalysis research, the pursuit of knowledge extends beyond mere data collection; true understanding stems from interpretable models that can elucidate observations in ways that are comprehensible to human scientists. 39,159In this section, we explore the potential role of large language models (LLMs) in identifying suitable descriptors for catalysis systems and enhancing model-agnostic methods for interpretability.[38]</p>
<p>Descriptors for traditional machine learning</p>
<p>Understanding catalysis data begins with the identication of the correct descriptors of catalytic systems.Descriptors are crucial in interpretable machine learning because they must not only capture relevant information but also minimize redundancy.The range of available descriptors provides substantial exibility in modelling various aspects of catalytic processes.</p>
<p>5.1.1Experimental descriptors.Experimental descriptors are mainly the reaction conditions, normally including temperature, pH value, pressure, voltage, reactant concentration, and reaction time. 160,161.1.2Topological/structural descriptors.Topological descriptors are derived from molecular connectivity tables using graph theory to specify connectivity, paths, and structural features.A similar concept can be extended to crystalline materials for catalysis such as zeolites. 162,163Other structural descriptors include atomic/covalent radius, atomic number (mass number), atomic position, 164 group number, molar volume, lattice constants, rotational angle, bond length, coordination number, the number of protons and valence electrons, 165 active sites, and surface properties such as defects, microstructure, and facet characteristics. 166,167.1.3Molecular ngerprints.Fingerprints are a variety of molecular descriptors that encode a molecule based on the presence or absence of specic chemical substructures.These substructures range from simple functional groups to more complex molecular motifs.Some of the ngerprints are based on pre-dened fragments, such as Molecular ACCess System (MACCS), 168 the Daylight ngerprints, 169 and a more recent extension the Local Functional Group Fingerprint (LoFFi). 170ther ngerprints delve into the connectivity or topology of a molecule, exemplied by the Extended Connectivity Fingerprint (ECFP) 171 and its more interpretable simplication molecular fragment featurization (MFF). 172.1.4Trans-rotational-invariant 3D representations.While atomic coordinates in Cartesian axes can represent molecules or crystalline materials, these representations are not inherently invariant to translation and rotation-properties that many chemical properties of interest do possess.To address this, several strategies have been developed to make these representations operational-invariant.One approach involves augmenting the data through multiple translations and rotations, a method that is cumbersome but effective in some cases.Another method expands atomic coordinates around a central point using spherical harmonics and radial functions, exem-plied by the Smooth Overlap of Atomic Positions (SOAP) representation.A third strategy involves generating special auto-correlation functions of some function of interest, such as the revised autocorrelation functions (RACs), 173 which correlate atomic properties within a molecule or material for highly efficient encoding.</p>
<p>5.1.5Physicochemical descriptors.Physicochemical descriptors, rooted in organic physical chemistry, systematically describe the electronic and steric properties of molecules and substituent groups.A notable example is the Hammett parameters, which quantify the electronic effects of substituent groups on aromatic rings based on the linear free energy relationship.Various electronic descriptors are attributed to molecular properties at the atomic level, [174][175][176] including the lipid/water distribution coefficient log P, molar refractivity (MR), electronegativity, and atomic charges.8][179] Tools like PaDEL-Descriptor soware 180 and SPOC descriptors 181 package them into comprehensive descriptor suites for broader applications in research.</p>
<p>5.1.6Spectrum-based descriptors.</p>
<p>Spectrum-based descriptors 182,183 form a latent space reecting key physicochemical properties of molecules and materials, which are both measurable and calculable.Certain spectra can directly reveal interactions critical in catalysis, such as the vibrational spectra of CO adsorbed on metal surfaces, which provide insights into adsorption energy, charge transfer degree, bond energy, and the d-band center of the metal. 184.1.7Theory-based descriptors.In heterogeneous catalysis, the adsorption of a reactant on the catalyst's surface typically represents the initial step.Consequently, adsorption energy serves as a critical descriptor.Notably, the adsorption energies of various species are interconnected through the linear free energy relationship, or the scaling law, oen referred to as Brønsted-Evans-Polanyi (BEP) relations. 185A recent study by Lin Zhuang and coworkers applied principal component analysis to the adsorption energies of multiple species, 186 revealing just two independent components which correspond to covalent and ionic interactions, respectively.Beyond adsorption energy, the potential of zero charges on an electrocatalyst's surface adds another vital dimension to electrocatalyst design. 187[190][191][192][193][194] 5.1.8Graph-based representations.Graph-based representations have emerged as a potent tool for delineating the geometry and connectivity of catalytic materials.In these models, atoms are depicted as nodes and bonds as edges within molecular or crystal graphs.Graph convolution techniques allow for embedding these graphs into numeric vectors, making them suitable for analysis via machine learning models. 166Since the application of this approach to inorganic crystalline materials 195 in 2017 and to organic reactions 196 in 2018, graph-based machine learning models for molecules have rapidly developed.This methodology is now a mainstream approach for addressing the complex, high-dimensional, nonlinear relationships characteristic of catalysis.</p>
<p>Descriptor selection and machine learning</p>
<p>Descriptor selection is a crucial step in the machine learning process, involving the elimination of irrelevant and redundant descriptors.This task is particularly challenging in catalysis research, where data sets are oen limited.An overly large set of descriptors can lead to spurious correlations that do not reect underlying chemical phenomena.Traditional machine learning techniques vary in how they select descriptors: 5.2.1 Multivariate linear regression (MLR).][199][200][201] Methods like LASSO promote sparsity (encouraging most of the coefficients to be zero) in the model by penalizing the magnitude of the coefficients, which helps in reducing overtting and enhances interpretability by retaining only the most signicant features.</p>
<p>5.2.2</p>
<p>Tree-based models.Models such as random forests and gradient boosting trees 202 inherently select and rank features based on their importance, which helps in understanding which descriptors are more critical. 172.2.3 Symbolic regression (SR) and sparsifying operator (SISSO).These methods elegantly assemble descriptors into mathematical formulations that provide deeper insights into catalysis mechanisms. 203For example, SR identied a simple geometric parameter m/t to guide the design of oxide perovskite catalysts with enhanced oxygen evolution reaction activities. 204unhai Ouyang who developed SISSO 203 continued to rene the method, which has been widely implemented to nd the numerical relationship, ranging from predicting free energy 205,206 to reaction activity. 207.2.4 Dimensional reduction.Techniques like principal component analysis (PCA) 208 reduce the dimensionality of the data, although PCA's linearity is a limitation.Nonlinear dimension reduction methods, such as kernel PCA and manifold learning or autoencoders, have been developed to overcome these restrictions.</p>
<p>Articial Neural Networks (ANNs): these models automatically extract and continuously rene descriptors through the iterative updating of network weights.</p>
<p>Incorporating chemical knowledge through LLMs</p>
<p>All the above descriptor selection processes have neglected the physical meanings of descriptors, which can lead to models that lack interpretability or generalizability.Large language models (LLMs) have the potential to revolutionize this process by embedding chemical knowledge into the selection process.They can track the physical signicance of descriptors and, when data alone are insufficient, use embedded chemistry knowledge to guide the selection process.Recent advancements have demonstrated the utility of augmenting traditional models with LLMs to leverage the linguistic implications of descriptors, providing a novel perspective on model training and interpretability. 209.3.1 Pre-trained molecular models.Pre-trained molecular models, inspired by the success of pre-trained language models, utilize deep neural networks trained on vast unlabelled molecular databases.These models can be ne-tuned for specic downstream tasks, signicantly enhancing representation capabilities and improving prediction accuracy across a range of applications. 210The pre-training tasks typically involve reconstructing molecules from masked or perturbed structures, whether represented in 3D space, as 2D images or graphs, or as 1D symbolic sequences like SMILES.</p>
<p>One such pre-trained model, Uni-Mol, incorporates 3D information in its self-training reconstruction process and has outperformed state-of-the-art models in molecular property prediction.It demonstrates strong performance in tasks that require spatial information, such as predicting protein-ligand binding poses and generating molecular conformations. 211imilarly, Payel Das et al. showed that a motif-based transformer applied to 3D heterogeneous molecular graphs (MOL-FORMER) excels by utilizing attention mechanisms to capture spatial relationships within molecules. 157Another innovative approach, the Chemical Space Explorer (ChemSpacE), uses pretrained deep generative models for exploring chemical space in an interpretable and interactive manner. 212The ChemSpacE model has exhibited impressive capabilities in molecule optimization and manipulation tasks across both single-property and multi-property scenarios.This process not only enhances the interpretability of deep generative models by navigating through their latent spaces but also facilitates human-in-theloop exploration of chemical spaces and molecule design.</p>
<p>Despite these advancements, caution is necessary when considering the information used during pre-training.Unlike natural languages, which are imbued with rich contextual and cultural knowledge, pure chemical structures typically contain limited information, oen constrained to basic chemical rules such as the octet rule.Pre-training models solely on 2D chemical structures or 1D SMILES strings without incorporating additional chemical knowledge may lead to models that lack substantial chemical understanding.</p>
<p>Pre-trained models, with their capacity for insightful interpretations and enhancements in molecular predictions, hold signicant promise for transforming areas in catalyst design, molecular property prediction, and reaction optimization.</p>
<p>5.3.2Direct use of language models.Before the widespread adoption of ChatGPT, researchers in 2019 began exploring the potential of using extensive text from scientic literature to encode materials science knowledge within word embeddings, aiming to recommend materials for functional applications. 35his approach resembles the Retrieval-Augmented Generation (RAG) agent, which employs a foundational language model that dynamically retrieves and integrates information from external data sources.This method helps reduce hallucination and adapt to specic domains.Fine-tuning large models on domain-specic materials, while more resource-intensive, is also a viable strategy.</p>
<p>Beyond the RAG agent, there are several examples of using language model architectures to train on chemistry data using SMILES or other molecular representations.These molecular pre-training models, discussed in the previous section, are developed from scratch purely with chemistry data and, as such, do not inherit the broader knowledge typically embedded in LLMs.Notable examples include SELFormer, which utilizes a transformer-based chemical language model to learn highquality molecular representations called SELFIES. 213Born and Manica proposed the Regression Transformer (RT), a method that abstracts regression as a conditional sequence modelling problem. 214Alán Aspuru-Guzik and his team investigated the ability of simple language models to learn complex molecular distributions, demonstrating their powerful generative capabilities through the prediction of distributions of the highest scoring penalized log P molecules in ZINC15. 215Francesca Grisoni provided a comprehensive overview of the current state, challenges, and future prospects of chemical language models in drug discovery. 216ecent initiatives have leveraged the capabilities of pretrained language models like GPT-3, ne-tuning them with chemically curated data.In 2023, Berend Smit et al. published</p>
<p>Chemical Science Review</p>
<p>an inuential paper titled "Is GPT-3 all you need for low-data discovery in chemistry" 15 rst on preprint.The title was apparently inspired by the seminal Google paper on transformers.The paper was later published in Nature Machine Intelligence with a modied title "Leveraging large language models for predictive chemistry". 156They experimented with ne-tuning GPT-3 using chemistry data written in a sentence and used it as a general machine learning model for classication and regression.The chemicals are represented by either SMILES or IUPAC names in natural language, which makes no difference in the prediction performance.The ne-tuned GPT-3 model achieved superior performance over traditional models in predicting material properties and reaction yields, especially in data-scarce scenarios.Its ability to accept the IUPAC names of chemicals as inputs facilitates non-specialist use.The authors explored the model's potential in generating molecules based on specic requirements and tested its in-context learning capabilities, which also showed promising results.</p>
<p>It is interesting to discuss what aspect of the LLM's ability is used in the task of learning chemistry data.Most likely, the LLM's abilities to learn new patterns and apply basic chemical logic are critical in these tasks.It is not clear if the LLM's general knowledge about specic molecule or functional groups is used or not.It is important to recognize that these models may not fully "understand" the underlying chemistry and should be used with caution due to their potential for producing misleading results or hallucinations.Despite these limitations, this work introduces a novel paradigm in machine learning that utilizes language models to foster advancements in low-data learning within the eld of chemistry.</p>
<p>Interpreting machine learning results</p>
<p>For data-driven research in catalysis to be fully benecial, it's crucial for models to be understandable so that human scientists can actively participate and apply their ndings.LLMs introduce both new challenges and opportunities for achieving this goal.</p>
<p>5.4.1 Model-agnostic interpretation methods.One commonly employed approach for model interpretation is SHapley Additive exPlanations (SHAP), 202 which utilizes principles from game theory, specically Shapley values, to assign importance to each feature and provide local explanations.This method has been widely used in catalysis studies to quantitatively analyze features responsible for variations in adsorption energy across different species, 217 key process variables inuencing yields, 218 and molecular features determining catalytic activity. 170Similarly, Local Interpretable Model-Agnostic Explanations (LIME), 219 which locally models descriptors' effects via an interpretable linear model, and Partial Dependence Plots (PDPs) that visualize the effect of features on predicted outcomes by marginalizing over the values of all other features, are also extensively used. 220,221.4.2Challenges with interpreting in-context learning.Applying these model-agnostic methods to the in-context learning of LLMs presents difficulties.A fundamental challenge lies in identifying coherent prompts that accurately map input features (X) to their corresponding outputs (Y).For example, consider the prompt: "Given input SMILES of the molecular catalyst is C(CCN)CC(]O)O and output yield of the reaction is 40%, please derive the output from the input".If we only asked the LLM to give the answer, we have no way to know how the model actually works.We need to add some prompt to ask the LLM to explain how the answer is arrived at.It is yet to be tested what prompt can accurately achieve the purpose and eliminate any hallucination.The ideal prompt may also be model specic and fulll two critical criteria:</p>
<ol>
<li>
<p>Interpretability: ideally, prompts should be phrased in natural language to ensure they are easily understood by human users.</p>
</li>
<li>
<p>Accuracy: prompts must accurately map input features to outputs, providing a clear and logical explanation of the data.</p>
</li>
</ol>
<p>3][224] However, as a result of gradient descent, it is not guaranteed that these searched prompts are generally interpretable.Additionally, gradientdescent-based methods are usually computationally expensive.To address these two problems, Jianfeng Gao et al. 44 introduced an interpretable auto-prompting method (iPrompt) using LLMs to directly generate and modify the prompts.There are three steps to search for ideal prompts in this method:</p>
<p>(1) Prompt proposal: in this stage, a prex of data points is fed to the LLMs, requiring them to complete the prompts that map the input features to the output values.It generates a series of candidate prompts that will be evaluated further.</p>
<p>(2) Reranking: the performance of the candidate prompts from (1) is evaluated, and those that maximize the accuracy will be maintained.</p>
<p>(3) Iterate with exploration: the top candidate prompts from (2) will be truncated randomly.Then the truncated prompts will be fed to LLMs to regenerate new prompts while maintaining accuracy.</p>
<p>This iterative process continues until no further improvements are observed.The direct generation and modication of prompts by LLMs in steps 1 and 3 enhance interpretability, while accuracy is optimized in step 2. However, despite their impressive capabilities, LLMs may still lack depth in mathematical rigor, theoretical simulation, or specialized domain knowledge required for some catalysis applications.Incorporating AI agents equipped with a comprehensive toolkit could potentially address these limitations, enhancing both the interpretability and accuracy of machine learning models in catalysis.</p>
<p>Summary</p>
<p>Interpretable machine learning models are becoming indispensable in chemical research for exploring complex chemical processes and catalytic mechanisms.These models allow chemists to extract diverse chemical information from data and elucidate structure-activity relationships with precision and efficiency.The shi towards models that prioritize excellent interpretability and continuity, such as those employing physicochemical and theory-based descriptors, marks a signicant advance over traditional Boolean ngerprints, which oen lack intuitive insights and demonstrate poor extrapolative capabilities.The use of LLMs in the learning process to bring in domain knowledge and consider chemical meaning of different descriptors in the learning process is still limited.</p>
<p>Recent developments in graph-based and latent space descriptors of pre-trained models are attracting increasing attention, despite sometimes not providing direct insights.These descriptors are valued for their potential to bridge sophisticated computational models with practical chemical understanding, a connection that is strengthening due to ongoing algorithmic improvements.</p>
<p>Model-agnostic methods like SHAP, LIME, and PDP provide robust frameworks for interpreting machine learning models.However, the methods need a signicant update to meet the new challenge due to the involvement of LLMs.</p>
<p>As we look to the future, the enhancement of interpretable models and the expansion of model-agnostic methods are set to increase AI's utility beyond mere speed and accuracy.By integrating tailored, interpretable descriptors across different systems, this approach not only deepens chemical insights but also empowers the use of machine learning to quantitatively analyze structure-activity relationships, thus broadening AI's impact on scientic discovery.</p>
<p>Conclusions and perspectives</p>
<p>The design and discovery of optimal catalysts is a complex endeavor due to the inherent complexity of catalytic processes and the vast search space.Traditional trial-and-error approaches are laborious, time-consuming, and oen fail to provide sufficient insights.However, the recent advancements in high-throughput information extraction, automated chemical experimentation, active machine learning, and interpretable machine learning have revolutionized this eld.</p>
<p>Automated extraction of unstructured chemical data, facilitated by optical character recognition and large language models (LLMs), lays the groundwork for robust data-driven approaches.Automated robotic platforms streamline experimentation, enabling real-time decision-making and facilitating closed-loop optimizations.Active learning algorithms optimize experiment selection based on accumulated data to minimize trial numbers.Interpretable machine learning models disclose underlying structure-property relationships, providing critical insights for rational catalyst design.</p>
<p>Despite these advances, challenges persist.Information extraction needs to evolve to handle diverse unstructured data formats reliably.Current technologies like image segmentation tools 225,226 are still advancing towards fully autonomous capabilities for extracting and analyzing raw chemical data from gures.Moreover, the integration of text and gure data demands enhanced anaphora resolution and inference capabilities to support detailed analyses.Future developments in multimodal AI, capable of processing text, images, video, and voice, will be crucial in this aspect.</p>
<p>LLMs have demonstrated potential in comprehending complex data and have been applied successfully in projects like the one-pot synthesis conditions of MOFs.Yet, the full scope of their capabilities, especially in formatting conditions for multistep synthesis procedures, remains underexplored.The cost and operational speed of robotic systems also limits their widespread adoption in chemical laboratories, necessitating innovations in specialized post-synthesis processing and autosampling for diverse catalytic systems.</p>
<p>The variability in control interfaces across different laboratory equipment poses another challenge, limiting hardware transferability among research communities.Standardizing control languages or systems could enhance collaborative efforts.Although natural language is commonly used to instruct experiments, its ambiguity necessitates sophisticated mapping to specic robotic operations, a task where LLMs could play a transformative role if their reliability is proven in more complex scenarios.</p>
<p>Furthermore, the high-dimensional nature of catalysis design and the chemical consumption in high-throughput processes suggest that automated platforms should be capable of managing varied reaction scales, from small-scale synthesis and characterization to larger-scale production.</p>
<p>As machine learning approaches become more integrated into catalyst design, it is anticipated that they will address increasingly complex design problems.Incorporating scientic hypotheses into the discovery process requires an iterative approach, where hypotheses are generated and modied, and data are queried for validation.AI agents, 227 e.g., ChemCrow 228 equipped with tools for automated experimentation, information retrieval, and machine learning, show promise in bridging these capabilities to create a self-evolving, intelligent system.</p>
<p>Although human feedback should ideally not exist in the process, it can be used for safety checks or as alternative solutions if any of the functions (e.g., automated experimentation) are missing in the toolset, as demonstrated by Omar M. Yaghi et al. 47 In the iteration, the AI agents should be instructed to generate or modify hypotheses together with their validation procedures within the toolset.Later the toolset can be utilized to give feedback to the AI agents for further improvement of the hypotheses via LLMs directly or Bayesian inference.</p>
<p>In conclusion, the last decade's advances have shied the paradigm from traditional methods to a more efficient, systematic approach to experimental design in catalyst research.The integration of LLMs and AI agents promises to further enhance the capability, exibility, and efficiency of these systems, paving the way for a future where intelligent systems can autonomously explore vast chemical spaces and contribute to scientic discovery in unprecedented ways.</p>
<p>Chemical Science</p>
<p>Review</p>
<ol>
<li>1
1
Fig. 2 Scheme of the Markov logic OCSR with low-level image information extraction and probabilistic logic inference.Reproduced with permission from ref. 70 Copyright 2014, American Chemical Society.</li>
</ol>
<p>superior accuracy and speed by extracting chemical structures from PDFs and outputting them in standardized formats, showcasing its efficacy over other OCSR systems like MolVec, OSRA, and Imago.2.1.5.1.4MolScribe.Representing the cutting edge, Mol-Scribe is an image-to-graph generation model 76 that merges neural network capabilities with rule-based methods.It predicts atoms and bonds along with their geometric layouts to construct 2D molecular graphs, applying symbolic chemistry constraints to recognize complex chemical patterns, including chirality and abbreviations.Enhanced by data augmentation strategies, MolScribe effectively handles domain shis and various drawing styles found in chemical literature.Its robustness has been conrmed through testing, showing an accuracy of 76-93% on public benchmarks.</p>
<p>Fig. 3
3
Fig. 3 Overview of the integrated DECIMER workflow including image segmentation, classification, and translation to obtain SMILES.Reproduced with permission from ref. 74 under CC BY license.</p>
<p>Fig. 4
4
Fig. 4 Scheme of the automated chemical reaction extraction from scientific literature.Reproduced with permission from ref. 82 Copyright 2019, American Chemical Society.</p>
<p>Fig. 5
5
Fig. 5 Scheme of the ChatGPT chemistry assistant workflow to extract synthesis information of MOFs from the literature.Reproduced with permission from ref. 16 Copyright 2023, American Chemical Society.</p>
<p>Fig. 6
6
Fig. 6 Autonomous mobile robot and experimental stations.The mobile robotic chemist (a), the roadmap of the whole laboratory (b) and several workstations (c-e) are shown.Reproduced with permission from ref. 4 Copyright 2020, Springer Nature.</p>
<p>Fig. 7
7
Fig. 7 Design of the all-round AI-Chemist with a scientific mind.It includes three modules for chemistry knowledge, autonomous experimentation, and theoretical computation and machine learning (A).The workflow of the AI-Chemist to study various systems are shown in (B).Reproduced with permission from ref. 5 Copyright 2022, China Science Publishing &amp; Media Ltd.</p>
<p>reported a "radial synthesizer" based on a series of continuous ow modules arranged radially around a central switching station, which allows selective access to individual reactors and avoids equipment redundancies and reconguration among different reactions.Storing stable intermediates inside uidic pathways enables simultaneous optimization of subsequent steps during route development.Online monitoring via infrared (IR) and 1 H/ 19 F NMR spectroscopy enables fast post-reaction analysis and feedback.The performance of this system has been demonstrated in transition metal-catalyzed C-C and C-N cross-coupling, olenation, reductive amination, nucleophilic aromatic substitution reactions, lightdriven oxidation-reduction catalysis, and continuous multi-step reactions.In addition, ow selection valve technology can be used to create different process combinations, as demonstrated</p>
<p>Fig. 8
8
Fig. 8 Physical implementation of the synthesis platform Chemputer.The scheme (A) and the actual set-up (B) of the Chemputer are shown respectively.Reproduced with permission from ref. 17 Copyright 2019, AAAS.</p>
<p>Fig. 10
10
Fig. 10 Plug-and-play, reconfigurable, continuous-flow chemical synthesis system.The workflow (A), the design of the flow system (B) and its actual setup (C) with interchangeable modules (D) are shown in the figure.Reproduced with permission from ref. 115 Copyright 2018, AAAS.</p>
<p>Fig. 11 A
11
Fig. 11 A robotically reconfigurable flow chemistry platform.Reproduced with permission from ref. 123 Copyright 2019, AAAS.</p>
<p>Fig. 12
12
Fig. 12 Bayesian optimization of the methanol electro-oxidation process.(a) Peak current density of methanol electro-oxidation as a function of the number of BO rounds.(b) A contour plot showing the peak current density and a ternary plot depicting the chemical composition in the electrolyte solution.Reproduced with permission from ref. 20 Copyright 2020, Royal Chemical Society.</p>
<p>Fig. 15
15
Fig. 15 Workflow for automating theoretical materials discovery.(a) and (b) The experimental workflow for catalyst discovery is accelerated by the ab initio DFT workflow.(c) Scientists relied on their expertise and experimental results to screen data for DFT calculations traditionally.(d) This work uses ML to select DFT data automatically and systematically.Reproduced with permission from ref. 31 Copyright 2018, Springer Nature.</p>
<p>Fig. 16
16
Fig. 16 Screening of CO 2 RR electrocatalysts using an active learning algorithm based on the DFT framework.(a) A two-dimensional activity volcano plot of the CO 2 RR.(b) A two-dimensional selectivity volcano plot of the CO 2 RR.(c) DFT calculations were performed on approximately 4000 adsorption sites of Cu-containing alloys identified by t-SNE.On the right, the Cu-Al clusters are labeled numerically.(d) Representative coordination sites for each cluster are labeled in the t-SNE.Reproduced with permission from ref. 32 Copyright 2020, Springer Nature.</p>
<p>Fig. 17
17
Fig. 17Framework diagram for GPT-4-directed MOF synthesis.The workflow consists of three phases: Reticular ChemScope, Reticular ChemNavigator, and Reticular Executor.The ICL capability of GPT-4 is achieved by combining pre-designed prompt systems with continuous human feedback.Reproduced with permission from ref. 47 Copyright 2021, Wiley.</p>
<p>Fig. 17Framework diagram for GPT-4-directed MOF synthesis.The workflow consists of three phases: Reticular ChemScope, Reticular ChemNavigator, and Reticular Executor.The ICL capability of GPT-4 is achieved by combining pre-designed prompt systems with continuous human feedback.Reproduced with permission from ref. 47 Copyright 2021, Wiley.</p>
<p>Table 1 Comparison of methods for information extraction
1MethodTypeExtracted contentSupported modalityOpen sourceReferenceCLiDERule-basedMolecular structures and chargeText &amp; imageYes66OSRARule-basedMolecular structuresText &amp; imageYes68ImagoRule-basedDepicted molecules with up and downText &amp; imageYes69stereo bonds and pseudostemsMSE-DUDLML-basedStructures of natural products andImageNo71peptide sequencesDECIMERML-basedChemical classes, species, organismImageYes72parts, and spectral dataMolMinerML-basedMolecule structuresImageNo75ChemDataExtractorLM-basedIdentiers, spectroscopic attributes, andTextYes80 and 81chemical property attributes (e.g.,melting point, oxidation/reductionpotentials, photoluminescence lifetime,and quantum yield)SciBERTLM-basedIdentiers of chemicalsTextYes11ChemRxnExtractorLM-basedReactants, catalysts, and solvents forTextYes82reactionsGPT-3.5LM-basedMOF synthesisTextNo16GPT-4LM-basedText &amp; imageNo</p>
<p>Table 2
2
The comparison of methods for robotic systems in chemistry.The systems are categorized into three types: humanoid, flow, or a mixture of both.The supported operations, characterization and originally studied chemical systems are listed in the table
TypeDescriptionSynthesis operationsCharacterizationTarget compoundsReferenceHumanoidMobile roboticSolid dispensing, liquidGas chromatographyCatalysts for photolysis of4chemistdispensing, capping/water to produce hydrogenuncapping, heating, andsonicationAn all-round AI-Solid dispensing, liquidUV-vis, uorescence,Materials for5Chemistdispensing, magneticand Ramanelectrocatalysts,stirring, sonication, drying,spectroscopy, and gasphotocatalysts, andcentrifugation, and liquidchromatographyluminescenceextractionA-lab, anPowder dosing and sampleX-ray diffraction (XRD)Primarily oxides and6autonomousheatingphosphates identiedlaboratorythrough extensive ab initiophase-stability dataFlow: BatchModular roboticMixing, ltration, liquid--Organic molecules17reactorssynthesis systemliquid separation,evaporation, andchromatographicseparationA portable suitcase-Liquid transfer,-Organic molecules49sized chemicaltemperature control,synthesis platformevaporation, ltration, andseparationFast screeningLiquid handling, electricMicro-fast gasElectrocatalysts for the106platform for thecell preparation, andchromatographyCO 2 RRCO 2 RRelectrolysisFlow: continuousBenchtop owLiquid handling, heating,High-performanceRecongurable system for115ow reactorschemistry platformcooling, photoreaction,liquid chromatographyautomated optimization ofextraction and purication(HPLC), IRdiverse chemical reactionsspectroscopy, Ramanspectroscopy, and massspectrometryRadial synthesizerLiquid transfer, mixing,IR spectrometry andCross-coupling,116systemand dilutionnuclear magneticolenation, reductiveresonanceamination, nucleophilicaromatic substitutionreactions, light-drivenredox catalysis, andcontinuous multi-stepreactionsAn automatedHeating, liquid-liquidLiquidPharmaceutical small117multistep chemicalseparation, gas-liquidchromatography-massmoleculessynthesizerseparation, andspectrometry (LC-MS)heterogeneous catalysisHumanoid roboticA robotic platformLiquid handling,High-performanceOrganic molecules123system with owfor ow synthesis ofseparation, andliquid chromatographyreactorsorganic compoundstemperature adjustmentand nuclear magneticresonance</p>
<p>Table 3
3
The comparison of active machine learning algorithms in chemistry.The algorithms rely on many surrogate models from the Gaussian process to the recent LLMs.Targets of the surrogate models are listed corresponding to the different research systems
TypeSurrogate modelsVariables (input)Target (output)Research systemsReferenceBayesianRandom forest andRatio of a metal precursorCurrent densityElectrocatalytic oxidation20optimizationGaussian process(continuous)of methanolGaussian processReagent concentration forHydrogen evolutionPhotocatalytic hydrogen4catalyst synthesisrategeneration(continuous)Gaussian processAlloy compositionsCurrent densityElectrocatalytic O 222(continuous)reductionLarge language modelsExperimental procedure asC 2 yieldOxidative coupling of45from open AItextmethaneActiveArticial neuralElectronegativity and d-<em>CO binding energyElectrocatalytic CO 230learningnetworks and kernelband width of alloys(</em>CO refers to adsorbedreductionridge regression(continuous)CO on a solid surface)Extra tree regressor,Fingerprints of the surfaceAdsorption energies ofElectrocatalytic CO 231random forest,and sites of intermetallicsCO and Hreduction and H 2Gaussian process,etc.(discrete)evolutionRandom forest andFingerprints of adsorptionCO adsorption energyElectrocatalytic CO 232boosted treesites from copper-reductioncontaining metals(discrete)GPT-4Synthesis procedure as textSuccess or failure of theMOF synthesis47inputsynthesis
© 2024 The Author(s).Published by the Royal Society of Chemistry Chem.Sci., 2024, 15, 12200-12233 | 12219</p>
<p>© 2024 The Author(s). Published by the Royal Society of Chemistry
© 2024 The Author(s). Published by the Royal Society of Chemistry Chem. Sci., 2024, 15, 12200-12233 | 12217
AcknowledgementsWe acknowledge the nancial support from the National Key R&amp;D Program of China (2021YFA1502500), the National Natural Science Foundation of China (22125502, 22071207, and 22121001), and the Fundamental Research Funds for the Central Universities (No. 20720220011).Notes and referencesData availabilityNo primary research results, soware or code have been included and no new data were generated or analysed as part of this review.Author contributionsAll the authors wrote the review together.Chemical Science ReviewConflicts of interestThe authors declare no conict of interest.
J Gasteiger, Chemistry in times of articial intelligence. 202021</p>
<p>Machine learning in materials science: Recent progress and emerging applications. T Mueller, A G Kusne, R Ramprasad, Rev. Comput. Chem. 292016</p>
<p>Machine-learning-assisted materials discovery using failed experiments. P Raccuglia, K C Elbert, P D Adler, C Falk, M B Wenny, A Mollo, M Zeller, S A Friedler, J Schrier, A J Norquist, Nature. 5332016</p>
<p>B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, N Rankin, B Harris, R S Sprick, A I Cooper, A mobile robotic chemist. 2020583</p>
<p>An all-round AI-Chemist with a scientic mind. Q Zhu, F Zhang, Y Huang, H Xiao, L Zhao, X Zhang, T Song, X Tang, X Li, G He, B Chong, J Zhou, Y Zhang, B Zhang, J Cao, M Luo, S Wang, G Ye, W Zhang, X Chen, S Cong, D Zhou, H Li, J Li, G Zou, W Shang, J Jiang, Y Luo, Natl. Sci. Rev. 1902022</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. N J Szymanski, B Rendy, Y Fei, R E Kumar, T He, D Milsted, M J Mcdermott, M Gallant, E D Cubuk, A Merchant, H Kim, A Jain, C J Bartel, K Persson, Y Zeng, G Ceder, Nature. 6242023</p>
<p>A review of optical chemical structure recognition tools. K Rajan, H O Brinkhaus, A Zielesny, C Steinbeck, J. Cheminf. 602020</p>
<p>ImageDataExtractor: A Tool To Extract and Quantify Data from Microscopy Images. K T Mukaddem, E J Beard, B Yildirim, J M Cole, J. Chem. Inf. Model. 602020</p>
<p>ChemSchematicResolver: a toolkit to decode 2D chemical diagrams with labels and R-groups into annotated chemical named entities. E J Beard, J M Cole, J. Chem. Inf. Model. 602020</p>
<p>Review of techniques and models used in optical chemical structure recognition in images and scanned documents. F Musazade, N Jamalova, J Hasanov, J. Cheminf. 612022</p>
<p>I Beltagy, K Lo, A Cohan, 10.48550/arXiv.1903.10676arXiv:1903.10676SciBERT: A Pretrained Language Model for Scientic Text, arXiv, 2019, preprint. </p>
<p>A text mining framework for screening catalysts and critical process parameters from scientic literature -A study on Hydrogen production from alcohol. A Kumar, S Ganesh, D Gupta, H Kodamana, Chem. Eng. Res. Des. 1842022</p>
<p>A corpus of CO2 electrocatalytic reduction process extracted from the scientic literature. L Wang, Y Gao, X Chen, W Cui, Y Zhou, X Luo, S Xu, Y Du, B Wang, Sci. Data. 1752023</p>
<p>Revisiting Electrocatalyst Design by a Knowledge Graph of Cu-Based Catalysts for CO2 Reduction. Y Gao, L Wang, X Chen, Y Du, B Wang, ACS Catal. 132023</p>
<p>Is GPT-3 all you need for low-data discovery in chemistry?. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.26434/chemrxiv-2023-fw8n4ChemRxiv. 2023preprint</p>
<p>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, J. Am. Chem. Soc. 1452023</p>
<p>Organic synthesis in a modular robotic system driven by a chemical programming language. S Steiner, J Wolf, S Glatzel, A Andreou, J M Granda, G Keenan, T Hinkley, G Aragon-Camarasa, P J Kitson, D Angelone, L Cronin, Science. 22112019</p>
<p>A universal system for digitization and automatic execution of the chemical synthesis literature. S H M Mehr, M Craven, A I Leonov, G Keenan, L Cronin, Science. 3702020</p>
<p>Digitization and validation of a chemical synthesis literature database in the ChemPU. S Rohrbach, M Šiaučiulis, G Chisholm, P.-A Pirvan, M Saleeb, S H M Mehr, E Trushina, A I Leonov, G Keenan, A Khan, A Hammer, L Cronin, Science. 3772022</p>
<p>Mesoporous trimetallic PtPdAu alloy lms toward enhanced electrocatalytic activity in methanol oxidation: unexpected chemical compositions discovered by Bayesian optimization. A S Nugraha, G Lambard, J Na, M S A Hossain, T Asahi, W Chaikittisilp, Y Yamauchi, J. Mater. Chem. A. 82020</p>
<p>A Bayesian experimental autonomous researcher for mechanical design. A E Gongora, B Xu, W Perry, C Okoye, P Riley, K G Reyes, E F Morgan, K A Brown, Sci. Adv. 2020, 6, eaaz1708</p>
<p>Bayesian Optimization of High-Entropy Alloy Compositions for Electrocatalytic Oxygen Reduction. J K Pedersen, C M Clausen, O A Krysiak, B Xiao, T A A Batchelor, T Löffler, V A Mints, L Banko, M Arenz, A Savan, W Schuhmann, A Ludwig, J Rossmeisl, Angew. Chem., Int. Ed. 602021</p>
<p>Bayesian optimization of single-atom alloys and other bimetallics: © 2024 The Author(s). G O Kayode, A F Hill, M M Montemore, Royal Society of Chemistry Chem. Sci. 152024. 12225Published by the</p>
<p>Review Chemical Science efficient screening for alkane transformations, CO2 reduction, and hydrogen evolution. J. Mater. Chem. A. 112023</p>
<p>Bayesian-optimization-assisted discovery of stereoselective aluminum complexes for ring-opening polymerization of racemic lactide. X Wang, Y Huang, X Xie, Y Liu, Z Huo, M Lin, H Xin, R Tong, Nat. Commun. 36472023</p>
<p>Composition-Designed Multielement Perovskite Oxides for Oxygen Evolution Catalysis. Y Okazaki, Y Fujita, H Murata, N Masuyama, Y Nojima, H Ikeno, S Yagi, I Yamada, Chem. Mater. 342022</p>
<p>Descriptor-Free Design of Multicomponent Catalysts. Y Zhang, T C Peck, G K Reddy, D Banerjee, H Jia, C A Roberts, C Ling, ACS Catal. 122022</p>
<p>Exploring the Composition Space of High-Entropy Alloy Nanoparticles for the Electrocatalytic H2/CO Oxidation with Bayesian Optimization. V A Mints, J K Pedersen, A Bagger, J Quinson, A S Anker, K M Ø Jensen, J Rossmeisl, M Arenz, ACS Catal. 122022</p>
<p>Benchmarking the performance of Bayesian optimization across multiple experimental materials science domains. Q Liang, A E Gongora, Z Ren, A Tiihonen, Z Liu, S Sun, J R Deneault, D Bash, F Mekki-Berrada, S A Khan, K Hippalgaonkar, B Maruyama, K A Brown, J Fisher Iii, T Buonassisi, Comput. Mater. 1882021</p>
<p>Active learning-based exploration of the catalytic pyrolysis of plastic waste. Y Ureel, M R Dobbelaere, O Akin, R J Varghese, C G Pernalete, J W Thybaut, K M Van Geem, Fuel. 1253402022</p>
<p>Active learning with nonab initio input features toward efficient CO2 reduction catalysts. J Noh, S Back, J Kim, Y Jung, Chem. Sci. 92018</p>
<p>Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution. K Tran, Z W Ulissi, Nat. Catal. 12018</p>
<p>Accelerated discovery of CO2 electrocatalysts using active machine learning. M Zhong, K Tran, Y Min, C Wang, Z Wang, C.-T Dinh, P De Luna, Z Yu, A S Rasouli, P Brodersen, S Sun, O Voznyy, C.-S Tan, M Askerka, F Che, M Liu, A Seitokaldani, Y Pang, S.-C Lo, A Ip, Z Ulissi, E H Sargent, Nature. 5812020</p>
<p>Exploring Optimal Water Splitting Bifunctional Alloy Catalyst by Pareto Active Learning. M Kim, Y Kim, M Y Ha, E Shin, S J Kwak, M Park, I.-D Kim, W.-B Jung, W B Lee, Y Kim, H.-T Jung, Adv. Mater. 22114972023</p>
<p>Searching for an Optimal Multi-Metallic Alloy Catalyst by Active Learning Combined with Experiments. M Kim, M Y Ha, W.-B Jung, J Yoon, E Shin, I -D. Kim, W B Lee, Y Kim, H.-T Jung, Adv. Mater. 21089002022</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. V Tshitoyan, J Dagdelen, L Weston, A Dunn, Z Rong, O Kononova, K A Persson, G Ceder, A Jain, Nature. 5712019</p>
<p>Recent advances in knowledge discovery for heterogeneous catalysis using machine learning. M Erdem Günay, R Yıldırım, Catal. Rev. 632021</p>
<p>T Toyao, Z Maeno, S Takakusagi, T Kamachi, I Takigawa, K.-I Shimizu, Machine Learning for Catalysis Informatics: Recent Applications and Prospects. 201910</p>
<p>Bridging the complexity gap in computational heterogeneous catalysis with machine learning. T Mou, H S Pillai, S Wang, M Wan, X Han, N M Schweitzer, F Che, H Xin, Nat. Catal. 62023</p>
<p>Extracting Knowledge from Data through Catalysis Informatics. A J Medford, M R Kunz, S M Ewing, T Borders, R Fushimi, ACS Catal. 82018</p>
<p>The rise of self-driving labs in chemical and materials sciences. M Abolhasani, E Kumacheva, Nat. Synth. 22023</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J.-Y Nie, J.-R Wen, 10.48550/arXiv.2303.18223arXiv:2303.18223A Survey of Large Language Models. 2023preprint</p>
<p>Large language models for chemistry robotics. N Yoshikawa, M Skreta, K Darvish, S Arellano-Rubach, Z Ji, L Bjørn Kristensen, A Z Li, Y Zhao, H Xu, A Kuramshin, A Aspuru-Guzik, F Shkurti, A Garg, Auton. Robots. 472023</p>
<p>LLMs can generate robotic scripts from goal-oriented instructions in biological laboratory automation, arXiv. T Inagaki, A Kato, K Takahashi, H Ozaki, G N Kanda, 10.48550/arXiv.2304.10267arXiv:2304.102672023preprint</p>
<p>C Singh, J X Morris, J Aneja, A M Rush, J Gao, 10.48550/arXiv.2210.01848arXiv:2210.01848Explaining Patterns in Data with Language Models via Interpretable Autoprompting, arXiv, 2022, preprint. </p>
<p>M Caldas Ramos, S S Michtavy, M D Porosoff, A D White, 10.48550/arXiv.2304.05341arXiv:2304.05341Bayesian Optimization of Catalysts With Incontext Learning. arXiv, 2023, preprint</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>A GPT-4 Reticular Chemist for Guiding MOF Discovery. Z Zheng, Z Rong, N Rampal, C Borgs, J T Chayes, O M Yaghi, Angew. Chem., Int. Ed. e2023119832023</p>
<p>A modular programmable inorganic cluster discovery robot for the discovery and synthesis of polyoxometalates. D S Salley, G A Keenan, D.-L Long, N L Bell, L Cronin, ACS Cent. Sci. 62020</p>
<p>An autonomous portable platform for universal chemical synthesis. J S Manzano, W Hou, S S Zalesskiy, P Frei, H Wang, P J Kitson, L Cronin, Nat. Chem. 142022</p>
<p>Laboratory automation. 1: Syntheses via vinyl sulfones. 14. Robotic orchestration of organic reactions: Yield optimization via an automated system with operator-specied reaction sequences. A R Frisbee, M H Nantz, G W Kramer, P L Fuchs, J. Am. Chem. Soc. 1061984</p>
<p>Selfdriving laboratory for accelerated discovery of thin-lm materials. B P Macleod, F G L Parlane, T D Morrissey, F Hse, L M Roch, K E Dettelbach, R Moreira, L P E Yunker, M B Rooney, J R Deeth, V Lai, G J Ng, H Situ, R H Zhang, M S Elliott, T H Haley, D J Dvorak, A Aspuru-Guzik, J E Hein, C P Berlinguette, Sci. Adv. 2020, 6, eaaz8867</p>
<p>An Introductory Review of Deep Learning for Prediction Models With Big Data, Front. F Emmert-Streib, Z Yang, H Feng, S Tripathi, M Dehmer, 10.3389/frai.2020.00004Artif. Intell. 32020</p>
<p>S Lundberg, S.-I Lee, 10.48550/arXiv.1705.07874A Unied Approach to Interpreting Model Predictions. 2017preprint</p>
<p>Reaxys. May 12, 2021</p>
<p>SciFinder. May 12, 2021</p>
<p>Accurate prediction of molecular properties and drug targets using a self-supervised image representation learning framework. X Zeng, H Xiang, L Yu, J Wang, K Li, R Nussinov, F Cheng, Nat. Mach. Intell. 42022</p>
<p>Automatic processing of graphics for image databases in science. R Rozas, H Fernandez, J. Chem. Inf. Comput. Sci. 301990</p>
<p>Chemical structure recognition (CSR) system: automatic analysis of 2D chemical structures in document images. S S Bukhari, Z Iikhar, A Dengel, 2019 International Conference on Document Analysis and Recognition (ICDAR). 2019presented in part</p>
<p>A component-detection-based approach for interpreting off-line handwritten chemical cyclic compound structures. Y Wang, T Zhang, X Yu, 2021 IEEE International Conference on Engineering. 2021presented in part at the</p>
<p>Kekule: OCR-optical chemical (structure) recognition. J R Mcdaniel, J R Balmuth, J. Chem. Inf. Comput. Sci. 321992</p>
<p>Chemical structure recognition: a rule-based approach presented in part at the Document Recognition and Retrieval XIX. N M Sadawi, A P Sexton, V Sorge, 2012</p>
<p>Robust method of segmentation and recognition of chemical structure images in cheminy presented in part at the Pre-proceedings of the 9th IAPR international workshop on graphics recognition. A Fujiyoshi, K Nakagawa, M Suzuki, 2011GREC</p>
<p>Research on chemical expression images recognition. C Hong, X Du, L Zhang, Joint International Mechanical, Electronic and Information Technology Conference (JIMET-15). 2015. 2015presented in part at the</p>
<p>Automated extraction of chemical structure information from digital raster images. J Park, G R Rosania, K A Shedden, M Nguyen, N Lyu, K Saitou, Chem. Cent. J. 2009, 3, 4</p>
<p>Optical recognition of chemical graphics. R Casey, S Boyer, P Healey, A Miller, B Oudot, K Zilles, Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR'93). 2nd International Conference on Document Analysis and Recognition (ICDAR'93)1993presented in part at the</p>
<p>Chemical literature data extraction: the CLiDE Project. P Ibison, M Jacquot, F Kam, A Neville, R W Simpson, C Tonnelier, T Venczel, A P Johnson, J. Chem. Inf. Comput. Sci. 331993</p>
<p>CLiDE Pro: the latest generation of CLiDE, a tool for optical chemical structure recognition. A T Valko, A P Johnson, J. Chem. Inf. Model. 492009</p>
<p>Optical structure recognition soware to recover chemical information: OSRA, an open source solution. I V Filippov, M C Nicklaus, J. Chem. Inf. Model. 492009</p>
<p>V Smolov, F Zentsev, M Rybalkin, Imago: Open-Source Toolkit for 2D Chemical Structure Image. 2011Recognition presented in part at the TREC</p>
<p>Markov logic networks for optical chemical structure recognition. P Frasconi, F Gabbrielli, M Lippi, S Marinai, J. Chem. Inf. Model. 542014</p>
<p>Molecular Structure Extraction from Documents Using Deep Learning. J Staker, K Marshall, R Abel, C M Mcquaw, J. Chem. Inf. Model. 592019</p>
<p>DECIMER: towards deep learning for chemical image recognition. K Rajan, A Zielesny, C Steinbeck, J. Cheminf. 652020</p>
<p>DECIMER-Segmentation: Automated extraction of chemical structure depictions from scientic literature. K Rajan, H O Brinkhaus, M Sorokina, A Zielesny, C Steinbeck, J. Cheminf. 202021</p>
<p>DECIMER. ai-An open platform for automated optical chemical structure identication, segmentation and recognition in scientic publications. K Rajan, H O Brinkhaus, M I Agea, A Zielesny, C Steinbeck, Nat. Commun. 50452023</p>
<p>MolMiner: You only look once for chemical structure recognition. Y Xu, J Xiao, C.-H Chou, J Zhang, J Zhu, Q Hu, H Li, N Han, B Liu, S Zhang, J Han, Z Zhang, S Zhang, W Zhang, L Lai, J Pei, J. Chem. Inf. Model. 622022</p>
<p>MolScribe: Robust Molecular Structure Recognition with Image-to-Graph Generation. Y Qian, J Guo, Z Tu, Z Li, C W Coley, R Barzilay, J. Chem. Inf. Model. 632023</p>
<p>LSTMVoter: chemical named entity recognition using a conglomerate of sequence labeling tools. W Hemati, A Mehler, J. Cheminf. 32019</p>
<p>Graph Convolutional Networks for Chemical Relation Extraction. D Mahendran, C Tang, B T Mcinnes, Companion Proceedings of the Web Conference 2022, Virtual Event. Lyon, France2022</p>
<p>Language models and protocol standardization guidelines for accelerating synthesis planning in heterogeneous catalysis. M Suvarna, A C Vaucher, S Mitchell, T Laino, J Pérez-Ramírez, Nat. Commun. 79642023</p>
<p>ChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientic Literature. M C Swain, J M Cole, J. Chem. Inf. Model. 562016</p>
<p>ChemDataExtractor 2.0: Autopopulated Ontologies for Materials Science. J Mavračić, C J Court, T Isazawa, S R Elliott, J M Cole, J. Chem. Inf. Model. 612021</p>
<p>Automated chemical reaction extraction from scientic literature. J Guo, A S Ibanez-Lopez, H Gao, V Quach, C W Coley, K F Jensen, R Barzilay, J. Chem. Inf. Model. 622021</p>
<p>Automated peptide synthesis. R B Merrield, J M Stewart, Nature. 2071965</p>
<p>Automation and optimization by simplex methods of 6-chlorohexanol synthesis. C Porte, W Debreuille, F Draskovic, A Delacroix, Process Control Qual. 41996</p>
<p>Investigation of cocatalysis conditions using an automated microscale multireactor workstation: Synthesis of m esotetramesitylporphyrin. R W Wagner, F Li, H Du, J S Lindsey, Org. Process Res. Dev. 31999</p>
<p>A retrospective on the automation of laboratory synthetic chemistry. J S Lindsey, Chemom. Intell. Lab. Syst. 171992</p>
<p>Instrument for automated synthesis of peptides. R B Merrield, J M Stewart, N Jernberg, Anal. Chem. 381966</p>
<p>Automated synthesis of gene fragments. G Alvarado-Urbina, G M Sathe, W.-C Liu, M F Gillen, P D Duck, R Bender, K K Ogilvie, Science. 2141981</p>
<p>Automation on the laboratory bench. M Legrand, A Foucard, J. Chem. Educ. 7671978</p>
<p>Integration of virtual and high-throughput screening. J Bajorath, Nat. Rev. Drug Discovery. 12002</p>
<p>In silico design of porous polymer networks: highthroughput screening for methane storage materials. R L Martin, C M Simon, B Smit, M Haranczyk, J. Am. Chem. Soc. 1362014</p>
<p>Impact of high-throughput screening in biomedical research. R Macarron, M N Banks, D Bojanic, D J Burns, D A Cirovic, T Garyantes, D V S Green, R P Hertzberg, W P Janzen, J W Paslay, U Schopfer, G Sitta Sittampalam, Nat. Rev. Drug Discovery. 102011</p>
<p>High-throughput screening of solid-state catalyst libraries. S M Senkan, Nature. 3941998</p>
<p>. X D Xiang, X Sun, G Briceno, Y Lou, K.-A Wang, H Chang, W G Wallace-Freedman, S.-W Chen, P G Schultz, A combinatorial approach to materials discovery. 2681995Science</p>
<p>Combinatorial and highthroughput materials science. W F Maier, K Stoewe, S Sieg, Angew. Chem., Int. Ed. 462007</p>
<p>Accelerating electrolyte discovery for energy storage with high-throughput screening. L Cheng, R S Assary, X Qu, A Jain, S P Ong, N N Rajput, K Persson, L A Curtiss, J. Phys. Chem. Lett. 62015</p>
<p>Less is more: Sampling chemical space with active learning. J S Smith, B Nebgen, N Lubbers, O Isayev, A E Roitberg, 10.1063/1.5023802J. Chem. Phys. 1482018</p>
<p>Automated synthesis of oxygen-producing catalysts from Martian meteorites by a robotic AI chemist. Q Zhu, Y Huang, D Zhou, L Zhao, L Guo, R Yang, Z Sun, M Luo, F Zhang, H Xiao, X Tang, X Zhang, T Song, X Li, B Chong, J Zhou, Y Zhang, B Zhang, J Cao, G Zhang, S Wang, G Ye, W Zhang, H Zhao, S Cong, H Li, L.-L Ling, Z Zhang, W Shang, J Jiang, Y Luo, Nat. Synth. 32024</p>
<p>Ondemand continuous-ow production of pharmaceuticals in a compact, recongurable system. A Adamo, R L Beingessner, M Behnam, J Chen, T F Jamison, K F Jensen, J.-C M Monbaliu, A S Myerson, E M Revalor, D R Snead, T Stelzer, N Weeranoppanant, S Y Wong, P Zhang, Science. 3522016</p>
<p>The synthesis of active pharmaceutical ingredients (APIs) using continuous ow chemistry. M Baumann, I R Baxendale, Beilstein J. Org. Chem. 112015</p>
<p>Flow chemistrya key enabling technology for (multistep) organic synthesis. J Wegner, S Ceylan, A Kirschning, Adv. Synth. Catal. 3542012</p>
<p>Flow "ne" synthesis: high yielding and selective organic synthesis by ow methods. S Kobayashi, Chem.-Asian J. 112016</p>
<p>Dynamic ow synthesis of porous organic cages. M E Briggs, A G Slater, N Lunt, S Jiang, M A Little, R L Greenaway, T Hasell, C Battilocchio, S V Ley, A I Cooper, Chem. Commun. 512015</p>
<p>Continuous ow techniques in organic synthesis. G Jas, A Kirschning, Chem. -Eur. J. 92003</p>
<p>Convergence of multiple synthetic paradigms in a universally programmable chemical synthesis machine. D Angelone, A J S Hammer, S Rohrbach, S Krambeck, J M Granda, J Wolf, S Zalesskiy, G Chisholm, L Cronin, Nat. Chem. 132021</p>
<p>Fast Screening for Copper-Based Bimetallic Electrocatalysts: Efficient Electrocatalytic Reduction of CO 2 to C 2+ Products on Magnesium-Modied Copper. M Xie, Y Shen, W Ma, D Wei, B Zhang, Z Wang, Y Wang, Q Zhang, S Xie, C Wang, Y Wang, Angew. Chem., Int. Ed. e2022134232022</p>
<p>The hitchhiker's guide to ow chemistry. M B Plutschack, B U Pieber, K Gilmore, P H Seeberger, Chem. Rev. 1172017</p>
<p>A high-throughput synthesis of 1, 2, 4-oxadiazole and 1, 2, 4-triazole libraries in a continuous ow reactor. A R Bogdan, Y Wang, RSC Adv. 52015</p>
<p>A simple milliuidic benchtop reactor system for the high-throughput synthesis and functionalization of gold nanoparticles with different sizes and shapes. S E Lohse, J R Eller, S T Sivapalan, M R Plews, C J Murphy, ACS Nano. 72013</p>
<p>High-throughput continuous ow synthesis of nickel nanoparticles for the catalytic hydrodeoxygenation of guaiacol. E J Roberts, S E Habas, L Wang, D A Ruddy, E A White, F G Baddour, M B Griffin, J A Schaidle, N Malmstadt, R L Brutchey, ACS Sustain. Chem. Eng. 52017</p>
<p>High-throughput synthesis of lignin particles (∼30 nm to ∼2 mm) via aerosol ow reactor: Size fractionation and utilization in pickering emulsions. M Ago, S Huan, M Borghei, J Raula, E I Kauppinen, O J Rojas, ACS Appl. Mater. Interfaces. 82016</p>
<p>Reproducible, highthroughput synthesis of colloidal nanocrystals for optimization in multidimensional parameter space. E M Chan, C Xu, A W Mao, G Han, J S Owen, B E Cohen, D J Milliron, Nano Lett. 102010</p>
<p>Electrochemical aromatic C-H hydroxylation in continuous ow. H Long, T.-S Chen, J Song, S Zhu, H.-C Xu, Nat. Commun. 39452022</p>
<p>Flow electrochemistry: a safe tool for uorine chemistry. B Winterson, T Rennigholtz, T Wirth, Chem. Sci. 122021</p>
<p>Recongurable system for automated optimization of diverse chemical reactions. A.-C Bédard, A Adamo, K C Aroh, M G Russell, A A Bedermann, J Torosian, B Yue, K F Jensen, T F Jamison, Science. 3612018</p>
<p>Automated radial synthesis of organic molecules. S Chatterjee, M Guidi, P H Seeberger, K Gilmore, Nature. 5792020</p>
<p>Fully automated chemical synthesis: toward the universal synthesizer. N Collins, D Stout, J.-P Lim, J P Malerich, J D White, P B Madrid, M Latendresse, D Krieger, J Szeto, V.-A Vu, Org. Process Res. Dev. 242020</p>
<p>Integrated plug ow synthesis and crystallisation of pyrazinamide. C D Scott, R Labes, M Depardieu, C Battilocchio, M G Davidson, S V Ley, C C Wilson, K Robertson, React. Chem. Eng. 32018</p>
<p>A droplet-reactor system capable of automation for the continuous and scalable production of noble-metal nanocrystals. G Niu, L Zhang, A Ruditskiy, L Wang, Y Xia, Nano Lett. 182018</p>
<p>Continuous ow droplet-based crystallization platform for producing spherical drug microparticles. E W Yeap, D Z Ng, D Lai, D J Ertl, S Sharpe, S A Khan, Org. Process Res. Dev. 232018</p>
<p>The coupling of in-ow reaction with continuous ow seedless tubular crystallization. B Rimez, J Septavaux, B Scheid, React. Chem. Eng. 42019</p>
<p>O Okafor, K Robertson, R Goodridge, V Sans, Continuous-ow crystallisation in 3D-printed compact devices. 20194</p>
<p>A robotic platform for ow synthesis of organic compounds informed by AI planning. C W Coley, D A Thomas, Iii , J A M Lummiss, J N Jaworski, C P Breen, V Schultz, T Hart, J S Fishman, L Rogers, H Gao, R W Hicklin, P P Plehiers, J Byington, J S Piotti, W H Green, A J Hart, T F Jamison, K F Jensen, Science. 15662019</p>
<p>B A Koscher, R B Canty, M A Mcdonald, K P Greenman, C J Mcgill, C L Bilodeau, W Jin, H Wu, F H Vermeire, B Jin, T Hart, T Kulesza, S.-C Li, T S Jaakkola, R Barzilay, R Gómez-Bombarelli, W H Green, K F Jensen, Autonomous, multiproperty-driven molecular discovery: From predictions to measurements and back. 20231407</p>
<p>How can LLMs transform the robotic design process?. F Stella, C Della Santina, J Hughes, Nat. Mach. Intell. 2023</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, Microso Auton. Syst. Robot. Res. 2023, 2, 20</p>
<p>L Wang, Y Ling, Z Yuan, M Shridhar, C Bao, Y Qin, B Wang, H Xu, X Wang, 10.48550/arXiv.2310.01361arXiv:2310.01361Generating Robotic Simulation Tasks via Large Language Models, arXiv, 2023, preprint. GenSim</p>
<p>Organic synthesis in a modular robotic system driven by a chemical programming language. S Steiner, J Wolf, S Glatzel, A Andreou, J M Granda, G Keenan, T Hinkley, G Aragon-Camarasa, P J Kitson, D Angelone, L Cronin, Science. 22112019</p>
<p>A literature survey of active machine learning in the context of natural language processing. F Olsson, Report. )110031542009Swedish Institute of Computer Science</p>
<p>Y Ureel, M R Dobbelaere, Y Ouyang, K De Ras, M K Sabbe, G B Marin, K M Van Geem, Active Machine Learning for Chemical Engineers: A Bright Future Lies Ahead, Engineering. 202327</p>
<p>. X Wang, Y Jin, S Schmitt, M Olhofer, ACM Comput. Surv. 2872023Recent Advances in Bayesian Optimization</p>
<p>Optimisation of water demand forecasting by articial intelligence with short data sets. R González Perea, E Camacho Poyato, P Montesinos, J A Rodríguez Díaz, Biosyst. Eng. 1772019</p>
<p>Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks. J M Hernández-Lobato, R P Adams, International Conference on Machine Learning. 2015presented in part</p>
<p>Practical variational inference for neural networks. A Graves, Proceedings of the 24th International Conference on Neural Information Processing Systems. the 24th International Conference on Neural Information Processing SystemsGranada, Spain2011presented in part at the</p>
<p>. F Häse, L M Roch, C Kreisbeck, A Aspuru-Guzik, Phoenics: A Bayesian Optimizer for Chemistry, ACS Cent. Sci. 42018</p>
<p>Gryffin: An algorithm for Bayesian optimization of categorical variables informed by expert knowledge. F Häse, M Aldeghi, R J Hickman, L M Roch, A Aspuru-Guzik, 10.1063/5.0048164Applied Physics Reviews. 82021</p>
<p>J Močkus, 10.1007/978-3-662-38527-2_55Optimization Techniques IFIP Technical Conference: Novosibirsk. G I Marchuk, Berlin Heidelberg, Berlin, HeidelbergSpringerJuly 1-7, 1974. 1975</p>
<p>Optimization of one-dimensional multimodal functions. A Zilinskas, J. R. Stat. Soc., C: Appl. Stat. 271978</p>
<p>A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise. H J Kushner, J. Basic Eng. 861964</p>
<p>Using condence bounds for exploitationexploration trade-offs. P Auer, J. Mach. Learn. Res. 32003</p>
<p>Counting biomolecules with Bayesian inference. J.-B Masson, Nat. Comput. Sci. 22022</p>
<p>Active learning for accelerated design of layered materials. L Bassman Oelie, P Rajak, R K Kalia, A Nakano, F Sha, J Sun, D J Singh, M Aykol, P Huck, K Persson, P Vashishta, Comput. Mater. 742018</p>
<p>Bias free multiobjective active learning for materials design and discovery. K M Jablonka, G M Jothiappan, S Wang, B Smit, B Yoo, Nat. Commun. 23122021</p>
<p>Iterative experimental design based on active machine learning reduces the experimental burden associated with reaction screening. N S Eyke, W H Green, K F Jensen, React. Chem. Eng. 52020</p>
<p>Using Active Learning to Develop Machine Learning Models for Reaction Yield Prediction. S Viet Johansson, H Gummesson Svensson, E Bjerrum, A Schliep, M Haghir, C Chehreghani, O Tyrchan, Engkvist, Mol. Inf. e22000432022</p>
<p>Active learning for computational chemogenomics. D Reker, P Schneider, G Schneider, J B Brown, Future Med. Chem. 92017</p>
<p>Active-learning strategies in computer-assisted drug discovery. D Reker, G Schneider, Drug Discovery Today. 202015</p>
<p>Kernel ridge regression with active learning for wind speed prediction. F Douak, F Melgani, N Benoudjit, Appl. Energy. 1032013</p>
<p>Evidence-based uncertainty sampling for active learning. M Sharma, M Bilgic, Data Min. Knowl. Discov. 312017</p>
<p>A two-stage regression approach for spectroscopic quantitative analysis. F Douak, N Benoudjit, F Melgani, Chemom. Intell. Lab. Syst. 1092011</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, K A Persson, APL Mater. 2013, 1, 011002</p>
<p>What can large language models do in chemistry?. T Guo, K Guo, B Nan, Z Liang, Z Guo, N Chawla, O Wiest, X Zhang, 10.48550/arXiv.2305.18365arXiv:2305.18365a comprehensive benchmark on eight tasks, arXiv, 2023, preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Adv. Neural Inf. Process Syst. 332020</p>
<p>Language models for the prediction of SARS-CoV-2 inhibitors. A E Blanchard, J Gounley, D Bhowmik, M Chandra Shekar, I Lyngaas, S Gao, J Yin, A Tsaris, F Wang, J Glaser, Int. J. High Perform. Comput. 362022</p>
<p>TransPolymer: a Transformer-based language model for polymer property predictions. C Xu, Y Wang, A Barati, Farimani, Comput. Mater. 642023</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nat. Mach. Intell. 62024</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nat. Mach. Intell. 42022</p>
<p>. Z Yang, Y Wang, L Zhang, 10.1101/2023.04.19.537579AI becomes a masterbrain scientist. 2023preprintbioRxiv</p>
<p>J Kim, D Kang, S Kim, H W Jang, Catalyze Materials Science with Machine Learning. 20213</p>
<p>Molecular Machine Learning for Chemical Catalysis: Prospects and Challenges. S Singh, R B Sunoj, Acc. Chem. Res. 562023</p>
<p>Predictive modeling in homogeneous catalysis: a tutorial. A G Maldonado, G Rothenberg, Chem. Soc. Rev. 392010</p>
<p>Inverse design of porous materials using articial neural networks. B Kim, S Lee, J Kim, Sci. Adv. 2020, 6, eaax9324</p>
<p>Machine learning approach for structure-based zeolite classication. D A Carr, M Lach-Hab, S Yang, I I Vaisman, E Blaisten-Barojas, Microporous Mesoporous Mater. 1172009</p>
<p>Atomic-position independent descriptor for machine learning of material properties. A Jain, T Bligaard, Phys. Rev. B. 2141122018</p>
<p>Estimation of Electronegativity Values of Elements in Different Valence States. K Li, D Xue, J. Phys. Chem. A. 1102006</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chem. Sci. 92018</p>
<p>. P Schlexer Lamoureux, K T Winther, J A Garrido Torres, V Streibel, M Zhao, M Bajdich, F Abild-Pedersen, T Bligaard, Machine Learning for Computational Heterogeneous Catalysis. 112019ChemCatChem</p>
<p>Reoptimization of MDL Keys for Use in Drug Discovery. J L Durant, B A Leland, D R Henry, J G Nourse, J. Chem. Inf. Comput. Sci. 422002</p>
<p>Unsupervised Data Base Clustering Based on Daylight's Fingerprint and Tanimoto Similarity: A Fast and Automated Way To Cluster Small and Large Data Sets. D Butina, J. Chem. Inf. Comput. Sci. 391999</p>
<p>Uncovering the inuence of the modier redox potential on CO2 reduction through combined datadriven machine learning and hypothesis-driven experimentation. X He, Y Su, J Zhu, N Fang, Y Chen, H Liu, D Zhou, C Wang, J. Mater. Chem. A. 112023</p>
<p>Extended-Connectivity Fingerprints. D Rogers, M Hahn, J. Chem. Inf. Model. 502010</p>
<p>Machine-Learning-Guided Discovery and Optimization of Additives in Preparing Cu Catalysts for CO2 Reduction. Y Guo, X He, Y Su, Y Dai, M Xie, S Yang, J Chen, K Wang, D Zhou, C Wang, J. Am. Chem. Soc. 1432021</p>
<p>Resolving Transition Metal Chemical Space: Feature Selection for Machine Learning and Structure-Property Relationships. J P Janet, H J Kulik, J. Phys. Chem. A. 1212017</p>
<p>Predictive and mechanistic multivariate linear regression models for reaction development. C B Santiago, J.-Y Guo, M S Sigman, Chem. Sci. 92018</p>
<p>Machine-Learning-Augmented Chemisorption Model for CO2 Electroreduction Catalyst Screening. X Ma, Z Li, L E K Achenie, H Xin, J. Phys. Chem. Lett. 62015</p>
<p>. H Mai, T C Le, D Chen, D A Winkler, R A Caruso, Machine Learning for Electrocatalyst and Photocatalyst Design and Discovery. 1222022Chem. Rev.</p>
<p>The Development of Multidimensional Analysis Tools for Asymmetric Catalysis and Beyond. M S Sigman, K C Harper, E N Bess, A Milo, Acc. Chem. Res. 492016</p>
<p>Comparing quantitative prediction methods for the discovery of small-molecule chiral catalysts. J P Reid, M S Sigman, Nat. Rev. Chem. 22018</p>
<p>Three-Dimensional Correlation of Steric and Electronic Free Energy Relationships Guides Asymmetric Propargylation. K C Harper, M S Sigman, Science. 3332011</p>
<p>PaDEL-descriptor: an open source soware to calculate molecular descriptors and ngerprints. C W Yap, J. Comput. Chem. 322011</p>
<p>An Ensemble Structure and Physicochemical (SPOC) Descriptor for Machine-Learning Prediction of Chemical Reaction and Molecular Properties. Q Yang, Y Liu, J Cheng, Y Li, S Liu, Y Duan, L Zhang, S Luo, Chemphyschem. e2022002552022</p>
<p>E L Willighagen, H M G W Denissen, R Wehrens, L M C Buydens, On the Use of 1H and 13C 1D NMR Spectra as QSPR Descriptors. 200646</p>
<p>Machine learning models capable of chemical deduction for identifying reaction products. T Jin, Q Zhao, A B Schoeld, B M Savoie, 10.26434/chemrxiv-2023-l6lzpChemRxiv. 2023preprint</p>
<p>Quantitatively Determining Surface-Adsorbate Properties from Vibrational Spectroscopy with Interpretable Machine Learning. X Wang, S Jiang, W Hu, S Ye, T Wang, F Wu, L Yang, X Li, G Zhang, X Chen, J Jiang, Y Luo, J. Am. Chem. Soc. 1442022</p>
<p>Fast Prediction of Selectivity in Heterogeneous Catalysis from Extended Brønsted-Evans-Polanyi Relations: A Theoretical Insight. D Loffreda, F Delbecq, F Vigné, P Sautet, Angew. Chem., Int. Ed. 482009</p>
<p>Unsupervised machine learning reveals eigen reactivity of metal surfaces. F Wei, L Zhuang, Sci. Bull. 692024</p>
<p>The importance of a charge transfer descriptor for screening potential CO(2) reduction electrocatalysts. S Ringe, Nat. Commun. 25982023</p>
<p>Descriptors of Oxygen-Evolution Activity for Oxides: A Statistical Evaluation. W T Hong, R E Welsch, Y Shao-Horn, J. Phys. Chem. C. 1202015</p>
<p>A Universal Descriptor for Complicated Interfacial Effects on Electrochemical Reduction Reactions. C Ren, S Lu, Y Wu, Y Ouyang, Y Zhang, Q Li, C Ling, J Wang, J. Am. Chem. Soc. 1442022</p>
<p>Scaling-Relation-Based Analysis of Bifunctional Catalysis: The Case for Homogeneous Bimetallic Alloys. M Andersen, A J Medford, J K Nørskov, K Reuter, ACS Catal. 72017</p>
<p>Toward Excellence of Electrocatalyst Design by Emerging Descriptor-Oriented Machine Learning. J Liu, W Luo, L Wang, J Zhang, X.-Z Fu, J.-L Luo, Adv. Funct. Mater. 21107482022</p>
<p>High-Throughput Screening of Electrocatalysts for Nitrogen Reduction Reactions Accelerated by Interpretable Intrinsic Descriptor. X Lin, Y Wang, X Chang, S Zhen, Z J Zhao, J Gong, Angew Chem. Int. Ed. Engl. e2023001222023</p>
<p>Electric Dipole Descriptor for Machine Learning Prediction of Catalyst Surface-Molecular Adsorbate Interactions. X Wang, S Ye, W Hu, E Sharman, R Liu, Y Liu, Y Luo, J Jiang, J. Am. Chem. Soc. 1422020</p>
<p>L H Mou, T Han, P E S Smith, E Sharman, J Jiang, Machine Learning Descriptors for Data-Driven Catalysis Study. 2023, 10, e2301020</p>
<p>Universal fragment descriptors for predicting properties of inorganic crystals. O Isayev, C Oses, C Toher, E Gossett, S Curtarolo, A Tropsha, Nat. Commun. 2017, 8, 15679</p>
<p>A graphconvolutional neural network model for the prediction of chemical reactivity. C W Coley, W Jin, L Rogers, T F Jamison, T S Jaakkola, W H Green, R Barzilay, K F Jensen, Chem. Sci. 102019</p>
<p>Linear free energy relationships in rate and equilibrium phenomena. L P Hammett, Trans. Faraday Soc. 341938</p>
<p>Linear Free Energy Relationships from Rates of Esterication and Hydrolysis of Aliphatic and Orthosubstituted Benzoate Esters. R W TaJr, J. Am. Chem. Soc. 741952</p>
<p>Data Science Meets Physical Organic Chemistry. J M Crawford, C Kingston, F D Toste, M S Sigman, Acc. Chem. Res. 542021</p>
<p>The Evolution of Data-Driven Modeling in Organic Chemistry. W L Williams, L Zeng, T Gensch, M S Sigman, A G Doyle, E V Anslyn, ACS Cent. Sci. 72021</p>
<p>Holistic prediction of enantioselectivity in asymmetric catalysis. J P Reid, M S Sigman, Nature. 5712019</p>
<p>From local explanations to global understanding with explainable AI for trees. S M Lundberg, G Erion, H Chen, A Degrave, J M Prutkin, B Nair, R Katz, J Himmelfarb, N Bansal, S.-I Lee, Nat. Mach. Intell. 22020</p>
<p>SISSO: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates. R Ouyang, S Curtarolo, E Ahmetcik, M Scheffler, L M Ghiringhelli, Phys. Rev. Mater. 2838022018</p>
<p>Simple descriptor derived from symbolic regression accelerating the discovery of new perovskite catalysts. B Weng, Z Song, R Zhu, Q Yan, Q Sun, C G Grice, Y Yan, W J Yin, Nat. Commun. 35132020</p>
<p>Physical descriptor for the Gibbs energy of inorganic crystalline solids and temperature-dependent materials chemistry. C J Bartel, S L Millican, A M Deml, J R Rumptz, W Tumas, A W Weimer, S Lany, V Stevanović, C B Musgrave, A M Holder, Nat. Commun. 41682018</p>
<p>Data-Driven Descriptor Engineering and Rened Scaling Relations for Predicting Transition Metal Oxide Reactivity. W Xu, M Andersen, K Reuter, ACS Catal. 112020</p>
<p>Single-atom alloy catalysts designed by rst-principles calculations and articial intelligence. Z.-K Han, D Sarker, R Ouyang, A Mazheika, Y Gao, S V Levchenko, Nat. Commun. 18332021</p>
<p>Probabilistic principal component analysis. M E Tipping, C M Bishop, J. R. Stat. Soc., B: Stat. Methodol. 611999</p>
<p>Augmenting interpretable models with large language models during training. C Singh, A Askari, R Caruana, J Gao, Nat. Commun. 79132023</p>
<p>A Systematic Survey of Chemical Pre-trained Models. J Xia, Y Zhu, Y Du, Y Liu, S Z Li, IJCAI. 2023</p>
<p>Uni-Mol: A Universal 3D Molecular Representation Learning Framework. G Zhou, Z Gao, Q Ding, H Zheng, H Xu, Z Wei, L Zhang, G Ke, The Eleventh International Conference on Learning Representations. 2023</p>
<p>ChemSpacE: Interpretable and Interactive Chemical Space Exploration. Y Du, X Liu, N M Shah, S Liu, J Zhang, B Zhou, 10.26434/chemrxiv-2022-x49mh-v3ChemRxiv. 2022preprint</p>
<p>SELFormer: molecular representation learning via SELFIES language models. A Yüksel, E Ulusoy, A Ünlü, T Dogan, Mach. Learn.: Sci. Technol. 2023, 4, 025035</p>
<p>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, Nat. Mach. Intell. 52023</p>
<p>Language models can learn complex molecular distributions. D Flam-Shepherd, K Zhu, A Aspuru-Guzik, Nat. Commun. 32932022</p>
<p>Chemical language models for de novo drug design: Challenges and opportunities. F Grisoni, Curr. Opin. Struct. Biol. 1025272023</p>
<p>Combined High-Throughput DFT and ML Screening of Transition Metal Nitrides for Electrochemical CO2 Reduction. A G Yohannes, C Lee, P Talebi, D H Mok, M Karamad, S Back, S Siahrostami, ACS Catal. 132023</p>
<p>Machine learning based interpretation of microkinetic data: a Fischer-Tropsch synthesis case study. A Chakkingal, P Janssens, J Poissonnier, A J Barrios, M Virginie, A Y Khodakov, J W Thybaut, React. Chem. Eng. 72022</p>
<p>Why Should I Trust You?": Explaining the Predictions of Any Classier. M T Ribeiro, S Singh, C Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningSan Francisco, California, USA2016</p>
<p>Interpretable design of Ir-free trimetallic electrocatalysts for ammonia oxidation with graph neural networks. H S Pillai, Y Li, S H Wang, N Omidvar, Q Mu, L E K Achenie, F Abild-Pedersen, J Yang, G Wu, H Xin, Nat. Commun. 7922023</p>
<p>Impacts of catalyst and process parameters on Ni-catalyzed methane dry reforming via interpretable machine learning. K Vellayappan, Y Yue, K H Lim, K Cao, J Y Tan, S Cheng, T Wang, T Z H Gani, I A Karimi, S Kawi, 10.1016/j.apcatb.2023.122593Appl. Catal., B. 3302023</p>
<p>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts, arXiv. T Shin, Y Razeghi, R L Logan, I V , E Wallace, S Singh, 10.48550/arXiv.2101.00190arXiv:2010.15980DOI: 10.48550/ arXiv.2101.00190Continuous Prompts for Generation, arXiv, 2021, preprint. 2020preprint</p>
<p>WARP: Word-level Adversarial ReProgramming. K Hambardzumyan, H Khachatrian, J May, Online. August, 2021</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollár, R Girshick, 10.48550/arXiv.2304.02643arXiv:2304.02643Segment Anything, arXiv, 2023, preprint. </p>
<p>Validity and Reliability Analysis of the PlotDigitizer Soware Program for Data Extraction from Single-Case Graphs. O Aydin, M Y Yassikaya, Perspect. Behav. Sci. 452022</p>
<p>H Yang, S Yue, Y He, 10.48550/arXiv.2306.02224arXiv:2306.02224Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions, arXiv, 2023, preprint. </p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nat. Mach. Intell. 652024</p>            </div>
        </div>

    </div>
</body>
</html>
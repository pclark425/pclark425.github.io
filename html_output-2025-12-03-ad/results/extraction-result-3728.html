<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3728 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3728</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3728</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-cd1ad1d985ea9351c317e04b99c4d94b9fc9e951</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cd1ad1d985ea9351c317e04b99c4d94b9fc9e951" target="_blank">DeLLMa: Decision Making Under Uncertainty with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments, is proposed and validated on multiple realistic decision-making environments, demonstrating that it can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods.</p>
                <p><strong>Paper Abstract:</strong> The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of decision-making under uncertainty. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling inference-time reasoning, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3728.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3728.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeLLMa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decision-making Large Language Model assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision-theoretic, multi-step inference-time framework that uses LLMs to (1) enumerate latent states, (2) elicit probabilistic forecasts over those states, (3) elicit a utility function via LLM-ranked comparisons, and (4) compute Monte Carlo expected utilities to choose a Bayes-optimal action.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeLLMa (framework using LLM backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A modular inference-time procedure that calls an underlying LLM (e.g., GPT-4, Claude-3, Gemini 1.5) to produce discrete latent factors and verbal probability ratings; it converts verbal scores to numeric probabilities, assumes factor independence to form a proposal distribution π^LLM, samples states, elicits utilities by asking the LLM to rank state-action minibatches, fits a Bradley–Terry model, then computes Monte Carlo expected utilities analytically to select actions.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate probabilities of future real-world outcomes (discrete latent states affecting decisions) to forecast prices/yields for agricultural planning and short-horizon stock returns; then use those probabilities to choose an action (which crop to plant or which stock to invest in) that maximizes expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Prompt-based state forecasting: (a) enumerate k latent factors and ℓ plausible discrete values per factor, (b) prompt the LLM to assign a verbal probability category (very likely → very unlikely) to each value, (c) map verbal categories to numeric scores via a dictionary, normalize to obtain per-factor marginals, assume independence across factors to form π^LLM(f_1,...,f_k|C), and draw Monte Carlo samples from that distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Two in-paper decision datasets: (1) Agriculture: decisions over planting fruits using USDA September 2021 report summaries + California price/yield statistics (7 candidate fruits, 120 problem instances formed from combinations); (2) Stocks: choose a stock to buy Dec 1, 2023 and sell Dec 29, 2023 using 24 monthly historical prices (Dec 2021–Nov 2023) for selected tickers (AMD, DIS, GME, GOOGL, META, NVDA, SPY), 120 problem instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Metrics reported include decision accuracy (fraction of instances where chosen action equals ground-truth optimal action), normalized ground-truth utility (chosen utility / optimal utility), forecast calibration ECE and negative log-likelihood (NLL) for state forecasts, and human-LLM agreement % for pairwise preference elicitation. Representative reported values: DeLLMa achieves up to a 40% increase in decision accuracy vs competing methods (abstract); Table 1 state-forecast ECE/NLL: GPT-4 ECE=0.062, NLL=1.11; Claude 3 ECE=0.142, NLL=1.20; Gemini 1.5 ECE=0.064, NLL=1.09. Table 3 (comparison to o1): DeLLMa (n=64) accuracy: Agriculture 73.3%, Stocks 64.2%; o1-preview: Agriculture 33.3%, Stocks 35.0%. Table 4 human agreement on utility rankings: GPT-4 70.4%, Claude 3 65.3%, Gemini 1.5 69.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>DeLLMa compared experimentally to Zero-Shot prompting, Self-Consistency (SC), Chain-of-Thought (CoT), and OpenAI o1-preview. Across agriculture and stocks tasks, DeLLMa variants (Pairs/Top1/Naive) consistently outperform zero-shot, SC, and CoT, particularly as action-set size grows. Quantitatively, DeLLMa (per-action sample size 64) yields ~73.3% accuracy on Agriculture vs o1-preview 33.3% (Table 3) and 64.2% vs 35.0% on Stocks. Ablations show scaling sample size and minibatch overlap increases accuracy; Pairs typically outperforms Top1 (except for some high-volatility stock settings where Top1 can be better due to reduced hallucinated noise).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Discrete, curated state/action spaces (ℓ^k combinatorics) and small-action problems due to context and compute limits; independence assumption across latent factors may be unjustified; reliance on LLM verbal-to-numeric mapping and LLM rankings introduces calibration and hallucination risks (noted especially in high-volatility stocks where pairwise rankings can be noisy); cost and API query count scale linearly with sample size and overlap; reported that some baseline methods underperform or echo context sentiment rather than reason about counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>DeLLMa demonstrates that LLMs can be harnessed to produce useful probabilistic forecasts for downstream decision-making: (1) the paper shows LLM-produced state forecasts can be reasonably calibrated (ECE ~0.06 for GPT-4/Gemini on the agriculture task); (2) combining LLM forecasting with structured utility elicitation and expected-utility maximization yields large improvements in decision accuracy (up to ~40% relative increase claimed; absolute numbers in Table 3 show large margins vs o1-preview); (3) scaling inference-time compute (more samples, overlap) improves performance; (4) human evaluation shows substantial agreement (~65–70%) between LLM-elicited pairwise preferences and human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeLLMa: Decision Making Under Uncertainty with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3728.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3728.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt4-1106-preview checkpoint used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer-based language model from OpenAI used as the primary LLM backbone in DeLLMa experiments to generate state enumerations, verbal probability assessments, and ranked preference comparisons for utility elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 family model (used via checkpoint gpt4-1106-preview in experiments). The paper does not provide internal architecture/parameter counts; it is used as a black-box LLM to generate textual enumerations, verbal probability labels, and ranked comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Produce verbal probabilistic forecasts over discrete latent factors affecting agriculture yields/prices and stock returns, and rank sampled state-action pairs to support utility elicitation for decision selection.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Prompting to elicit verbal probability categories for each plausible value of each latent factor, mapping categories to numeric scores, normalizing per-factor marginals and assuming independence to form π^LLM; sample Monte Carlo states from this proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on the paper's Agriculture dataset (USDA reports + CA price/yield statistics; 120 instances) and Stocks dataset (24-month historical prices for selected tickers; 120 instances).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>State forecast calibration and likelihood: ECE=0.062, NLL=1.11 (Table 1). Decision accuracy and normalized utility via downstream decision task: DeLLMa with GPT-4 backbone achieves top reported results (e.g., DeLLMa (n=64) Agriculture accuracy 73.3% in Table 3). Human agreement on utility rankings: 70.4% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>When used within DeLLMa, GPT-4 outperforms direct prompting baselines (zero-shot, SC, CoT) on both tasks; DeLLMa-Pairs/Top1 with GPT-4 achieve substantially higher accuracy than zero-shot/SC/CoT and outperform o1-preview as reported in Table 3. Ablations show GPT-4's forecasts help produce well-calibrated probabilities and better decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Forecasts depend on coarse verbal-to-numeric mapping and independence assumption; forecasting quality and ranking reliability can degrade in high-volatility scenarios (stocks) and when prompts induce hallucination; cost scales with sample size and overlap parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>GPT-4 produced reasonably calibrated state forecasts (low ECE) and high downstream decision performance when combined with DeLLMa; human-LLM agreement on pairwise utility rankings ~70% indicates LLM rankings are useful for eliciting utilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeLLMa: Decision Making Under Uncertainty with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3728.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3728.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude 3 family LLM evaluated as an alternative backbone for DeLLMa; used to enumerate states, provide verbal probability labels, and produce ranked comparisons for utility elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's Claude 3 model family (Opus/Sonnet/Haiku variants referenced in the paper). Used as a black-box LLM backbone for DeLLMa experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Provide verbal probability assessments for latent factors in agriculture and stocks decision tasks and rank state-action minibatches for utility elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Same prompt-based verbal-probability mapping approach as other backbones: LLM returns verbal likelihood categories per factor-value; dictionary maps to numeric; normalize marginals and assume independence to sample state vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Agriculture and Stocks datasets described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>State forecast calibration: ECE=0.142, NLL=1.20 (Table 1). Decision accuracy for DeLLMa on Agriculture quoted in Table 2 and Figure 2: DeLLMa overall performance with Claude 3 is lower than GPT-4 and Gemini 1.5 (e.g., DeLLMa Agriculture reported 40.8% in Table 2 row DeLLMa for Claude 3). Human agreement on utility elicitation: 65.3% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Claude-3 within DeLLMa improved over zero-shot/SC/CoT baselines in some conditions but underperformed relative to GPT-4/Gemini 1.5 in several ablations and main results; DeLLMa still yielded improvements over naive baselines but gains were smaller for Claude-3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Higher ECE indicates weaker calibration in this setup compared to GPT-4/Gemini; ablations showed DeLLMa with Claude-3 is more sensitive to forecasting variations and sometimes closer to zero-shot baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Claude-3 can be used within the DeLLMa pipeline but produced less well-calibrated state forecasts and lower downstream decision accuracy compared to GPT-4 and Gemini 1.5 in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeLLMa: Decision Making Under Uncertainty with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3728.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3728.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini 1.5 model evaluated as an LLM backbone for DeLLMa; used for state enumeration, verbal probability forecasts, and ranking for utility elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google Gemini 1.5 multimodal LLM family (referenced in the paper). Used as a black-box LLM to produce the textual components of DeLLMa (state/value suggestion, verbal probability labels, ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Generate verbal probability labels for latent factor values in agriculture and stocks tasks and rank sampled state-action pairs for eliciting utilities.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Prompt the model to return verbal likelihood categories for each candidate value of each latent factor; convert to numeric via a mapping dictionary; normalize and assume factor independence to sample discrete state vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same in-paper Agriculture and Stocks datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>State forecast calibration: ECE=0.064, NLL=1.09 (Table 1). Decision performance: DeLLMa/Gemini reported in Table 2 and Figures—DeLLMa accuracy and normalized utility exceed baselines but are slightly lower than GPT-4 in some metrics (e.g., Table 2 DeLLMa accuracy for Gemini 1.5: 47.5%). Human agreement for utility elicitation: 69.6% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Gemini 1.5 within DeLLMa produced substantial improvements over zero-shot/SC/CoT and had similar calibration to GPT-4; DeLLMa using Gemini outperformed baselines though absolute performance sometimes varied by task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same procedural limitations as other backbones: discretization, independence assumption, potential for hallucinated rankings in noisy settings, and API/query cost scaling with sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Gemini 1.5 produced well-calibrated forecasts comparable to GPT-4 (low ECE, low NLL) and achieved high human-LLM agreement in utility elicitation; using it within DeLLMa yields consistent decision improvements over baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeLLMa: Decision Making Under Uncertainty with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3728.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3728.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art inference-time-reasoning model (OpenAI o1) used as a baseline comparison to evaluate whether general inference-time reasoning is sufficient for decision-making under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o1 (o1-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's o1 model family (preview checkpoint used in comparisons) noted for advanced inference-time reasoning and scaling repeated sampling; used as a baseline in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Used as a general inference-time reasoning baseline to produce decisions directly from prompts for the same Agriculture and Stocks decision instances (zero-shot prompt applied to o1-preview).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Applied here as a baseline using its internal inference-time reasoning capabilities with a zero-shot prompt (the paper did not integrate o1-preview into the DeLLMa expected-utility pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Compared on the paper's Agriculture and Stocks datasets (same 120 instances each).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported decision accuracy: o1-preview achieved substantially lower accuracies compared to DeLLMa: Agriculture 33.3%, Stocks 35.0% (Table 3). Cost per instance reported: o1 ~ $0.21 per instance vs DeLLMa $0.09–$0.37 depending on configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>DeLLMa substantially outperformed o1-preview in the evaluated decision tasks despite o1's advanced inference-time reasoning capabilities, suggesting specialized decision-theoretic scaffolding yields large benefits over general-purpose inference-time methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper notes inability to fully control the o1 backbone and that o1 was used with zero-shot prompting for baseline comparison; the baseline's poor performance suggests that general inference-time scaling alone may be insufficient for structured decision-making under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Specialized, decision-theory-aligned inference-time procedures (DeLLMa) can substantially outperform strong general inference-time reasoning models like o1-preview on single-step, high-cost decision tasks under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeLLMa: Decision Making Under Uncertainty with Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Approaching human-level forecasting with language models <em>(Rating: 2)</em></li>
                <li>Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy <em>(Rating: 2)</em></li>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 2)</em></li>
                <li>Language models (mostly) know what they know <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3728",
    "paper_id": "paper-cd1ad1d985ea9351c317e04b99c4d94b9fc9e951",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "DeLLMa",
            "name_full": "Decision-making Large Language Model assistant",
            "brief_description": "A decision-theoretic, multi-step inference-time framework that uses LLMs to (1) enumerate latent states, (2) elicit probabilistic forecasts over those states, (3) elicit a utility function via LLM-ranked comparisons, and (4) compute Monte Carlo expected utilities to choose a Bayes-optimal action.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeLLMa (framework using LLM backbones)",
            "model_description": "A modular inference-time procedure that calls an underlying LLM (e.g., GPT-4, Claude-3, Gemini 1.5) to produce discrete latent factors and verbal probability ratings; it converts verbal scores to numeric probabilities, assumes factor independence to form a proposal distribution π^LLM, samples states, elicits utilities by asking the LLM to rank state-action minibatches, fits a Bradley–Terry model, then computes Monte Carlo expected utilities analytically to select actions.",
            "prediction_task": "Estimate probabilities of future real-world outcomes (discrete latent states affecting decisions) to forecast prices/yields for agricultural planning and short-horizon stock returns; then use those probabilities to choose an action (which crop to plant or which stock to invest in) that maximizes expected utility.",
            "method_of_probability_estimation": "Prompt-based state forecasting: (a) enumerate k latent factors and ℓ plausible discrete values per factor, (b) prompt the LLM to assign a verbal probability category (very likely → very unlikely) to each value, (c) map verbal categories to numeric scores via a dictionary, normalize to obtain per-factor marginals, assume independence across factors to form π^LLM(f_1,...,f_k|C), and draw Monte Carlo samples from that distribution.",
            "dataset_or_benchmark": "Two in-paper decision datasets: (1) Agriculture: decisions over planting fruits using USDA September 2021 report summaries + California price/yield statistics (7 candidate fruits, 120 problem instances formed from combinations); (2) Stocks: choose a stock to buy Dec 1, 2023 and sell Dec 29, 2023 using 24 monthly historical prices (Dec 2021–Nov 2023) for selected tickers (AMD, DIS, GME, GOOGL, META, NVDA, SPY), 120 problem instances.",
            "performance_metrics": "Metrics reported include decision accuracy (fraction of instances where chosen action equals ground-truth optimal action), normalized ground-truth utility (chosen utility / optimal utility), forecast calibration ECE and negative log-likelihood (NLL) for state forecasts, and human-LLM agreement % for pairwise preference elicitation. Representative reported values: DeLLMa achieves up to a 40% increase in decision accuracy vs competing methods (abstract); Table 1 state-forecast ECE/NLL: GPT-4 ECE=0.062, NLL=1.11; Claude 3 ECE=0.142, NLL=1.20; Gemini 1.5 ECE=0.064, NLL=1.09. Table 3 (comparison to o1): DeLLMa (n=64) accuracy: Agriculture 73.3%, Stocks 64.2%; o1-preview: Agriculture 33.3%, Stocks 35.0%. Table 4 human agreement on utility rankings: GPT-4 70.4%, Claude 3 65.3%, Gemini 1.5 69.6%.",
            "comparison_to_baselines": "DeLLMa compared experimentally to Zero-Shot prompting, Self-Consistency (SC), Chain-of-Thought (CoT), and OpenAI o1-preview. Across agriculture and stocks tasks, DeLLMa variants (Pairs/Top1/Naive) consistently outperform zero-shot, SC, and CoT, particularly as action-set size grows. Quantitatively, DeLLMa (per-action sample size 64) yields ~73.3% accuracy on Agriculture vs o1-preview 33.3% (Table 3) and 64.2% vs 35.0% on Stocks. Ablations show scaling sample size and minibatch overlap increases accuracy; Pairs typically outperforms Top1 (except for some high-volatility stock settings where Top1 can be better due to reduced hallucinated noise).",
            "limitations_or_challenges": "Discrete, curated state/action spaces (ℓ^k combinatorics) and small-action problems due to context and compute limits; independence assumption across latent factors may be unjustified; reliance on LLM verbal-to-numeric mapping and LLM rankings introduces calibration and hallucination risks (noted especially in high-volatility stocks where pairwise rankings can be noisy); cost and API query count scale linearly with sample size and overlap; reported that some baseline methods underperform or echo context sentiment rather than reason about counterfactuals.",
            "notable_findings": "DeLLMa demonstrates that LLMs can be harnessed to produce useful probabilistic forecasts for downstream decision-making: (1) the paper shows LLM-produced state forecasts can be reasonably calibrated (ECE ~0.06 for GPT-4/Gemini on the agriculture task); (2) combining LLM forecasting with structured utility elicitation and expected-utility maximization yields large improvements in decision accuracy (up to ~40% relative increase claimed; absolute numbers in Table 3 show large margins vs o1-preview); (3) scaling inference-time compute (more samples, overlap) improves performance; (4) human evaluation shows substantial agreement (~65–70%) between LLM-elicited pairwise preferences and human annotators.",
            "uuid": "e3728.0",
            "source_info": {
                "paper_title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (gpt4-1106-preview checkpoint used in experiments)",
            "brief_description": "A large transformer-based language model from OpenAI used as the primary LLM backbone in DeLLMa experiments to generate state enumerations, verbal probability assessments, and ranked preference comparisons for utility elicitation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt4-1106-preview)",
            "model_description": "OpenAI's GPT-4 family model (used via checkpoint gpt4-1106-preview in experiments). The paper does not provide internal architecture/parameter counts; it is used as a black-box LLM to generate textual enumerations, verbal probability labels, and ranked comparisons.",
            "prediction_task": "Produce verbal probabilistic forecasts over discrete latent factors affecting agriculture yields/prices and stock returns, and rank sampled state-action pairs to support utility elicitation for decision selection.",
            "method_of_probability_estimation": "Prompting to elicit verbal probability categories for each plausible value of each latent factor, mapping categories to numeric scores, normalizing per-factor marginals and assuming independence to form π^LLM; sample Monte Carlo states from this proposal.",
            "dataset_or_benchmark": "Evaluated on the paper's Agriculture dataset (USDA reports + CA price/yield statistics; 120 instances) and Stocks dataset (24-month historical prices for selected tickers; 120 instances).",
            "performance_metrics": "State forecast calibration and likelihood: ECE=0.062, NLL=1.11 (Table 1). Decision accuracy and normalized utility via downstream decision task: DeLLMa with GPT-4 backbone achieves top reported results (e.g., DeLLMa (n=64) Agriculture accuracy 73.3% in Table 3). Human agreement on utility rankings: 70.4% (Table 4).",
            "comparison_to_baselines": "When used within DeLLMa, GPT-4 outperforms direct prompting baselines (zero-shot, SC, CoT) on both tasks; DeLLMa-Pairs/Top1 with GPT-4 achieve substantially higher accuracy than zero-shot/SC/CoT and outperform o1-preview as reported in Table 3. Ablations show GPT-4's forecasts help produce well-calibrated probabilities and better decisions.",
            "limitations_or_challenges": "Forecasts depend on coarse verbal-to-numeric mapping and independence assumption; forecasting quality and ranking reliability can degrade in high-volatility scenarios (stocks) and when prompts induce hallucination; cost scales with sample size and overlap parameters.",
            "notable_findings": "GPT-4 produced reasonably calibrated state forecasts (low ECE) and high downstream decision performance when combined with DeLLMa; human-LLM agreement on pairwise utility rankings ~70% indicates LLM rankings are useful for eliciting utilities.",
            "uuid": "e3728.1",
            "source_info": {
                "paper_title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude-3",
            "name_full": "Claude 3 (Anthropic)",
            "brief_description": "Anthropic's Claude 3 family LLM evaluated as an alternative backbone for DeLLMa; used to enumerate states, provide verbal probability labels, and produce ranked comparisons for utility elicitation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 3",
            "model_description": "Anthropic's Claude 3 model family (Opus/Sonnet/Haiku variants referenced in the paper). Used as a black-box LLM backbone for DeLLMa experiments.",
            "prediction_task": "Provide verbal probability assessments for latent factors in agriculture and stocks decision tasks and rank state-action minibatches for utility elicitation.",
            "method_of_probability_estimation": "Same prompt-based verbal-probability mapping approach as other backbones: LLM returns verbal likelihood categories per factor-value; dictionary maps to numeric; normalize marginals and assume independence to sample state vectors.",
            "dataset_or_benchmark": "Agriculture and Stocks datasets described in the paper.",
            "performance_metrics": "State forecast calibration: ECE=0.142, NLL=1.20 (Table 1). Decision accuracy for DeLLMa on Agriculture quoted in Table 2 and Figure 2: DeLLMa overall performance with Claude 3 is lower than GPT-4 and Gemini 1.5 (e.g., DeLLMa Agriculture reported 40.8% in Table 2 row DeLLMa for Claude 3). Human agreement on utility elicitation: 65.3% (Table 4).",
            "comparison_to_baselines": "Claude-3 within DeLLMa improved over zero-shot/SC/CoT baselines in some conditions but underperformed relative to GPT-4/Gemini 1.5 in several ablations and main results; DeLLMa still yielded improvements over naive baselines but gains were smaller for Claude-3.",
            "limitations_or_challenges": "Higher ECE indicates weaker calibration in this setup compared to GPT-4/Gemini; ablations showed DeLLMa with Claude-3 is more sensitive to forecasting variations and sometimes closer to zero-shot baseline performance.",
            "notable_findings": "Claude-3 can be used within the DeLLMa pipeline but produced less well-calibrated state forecasts and lower downstream decision accuracy compared to GPT-4 and Gemini 1.5 in the reported experiments.",
            "uuid": "e3728.2",
            "source_info": {
                "paper_title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini-1.5",
            "name_full": "Gemini 1.5",
            "brief_description": "Google's Gemini 1.5 model evaluated as an LLM backbone for DeLLMa; used for state enumeration, verbal probability forecasts, and ranking for utility elicitation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5",
            "model_description": "Google Gemini 1.5 multimodal LLM family (referenced in the paper). Used as a black-box LLM to produce the textual components of DeLLMa (state/value suggestion, verbal probability labels, ranking).",
            "prediction_task": "Generate verbal probability labels for latent factor values in agriculture and stocks tasks and rank sampled state-action pairs for eliciting utilities.",
            "method_of_probability_estimation": "Prompt the model to return verbal likelihood categories for each candidate value of each latent factor; convert to numeric via a mapping dictionary; normalize and assume factor independence to sample discrete state vectors.",
            "dataset_or_benchmark": "Same in-paper Agriculture and Stocks datasets.",
            "performance_metrics": "State forecast calibration: ECE=0.064, NLL=1.09 (Table 1). Decision performance: DeLLMa/Gemini reported in Table 2 and Figures—DeLLMa accuracy and normalized utility exceed baselines but are slightly lower than GPT-4 in some metrics (e.g., Table 2 DeLLMa accuracy for Gemini 1.5: 47.5%). Human agreement for utility elicitation: 69.6% (Table 4).",
            "comparison_to_baselines": "Gemini 1.5 within DeLLMa produced substantial improvements over zero-shot/SC/CoT and had similar calibration to GPT-4; DeLLMa using Gemini outperformed baselines though absolute performance sometimes varied by task.",
            "limitations_or_challenges": "Same procedural limitations as other backbones: discretization, independence assumption, potential for hallucinated rankings in noisy settings, and API/query cost scaling with sample size.",
            "notable_findings": "Gemini 1.5 produced well-calibrated forecasts comparable to GPT-4 (low ECE, low NLL) and achieved high human-LLM agreement in utility elicitation; using it within DeLLMa yields consistent decision improvements over baselines.",
            "uuid": "e3728.3",
            "source_info": {
                "paper_title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "o1-preview",
            "name_full": "OpenAI o1-preview",
            "brief_description": "A state-of-the-art inference-time-reasoning model (OpenAI o1) used as a baseline comparison to evaluate whether general inference-time reasoning is sufficient for decision-making under uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI o1 (o1-preview)",
            "model_description": "OpenAI's o1 model family (preview checkpoint used in comparisons) noted for advanced inference-time reasoning and scaling repeated sampling; used as a baseline in the paper's experiments.",
            "prediction_task": "Used as a general inference-time reasoning baseline to produce decisions directly from prompts for the same Agriculture and Stocks decision instances (zero-shot prompt applied to o1-preview).",
            "method_of_probability_estimation": "Applied here as a baseline using its internal inference-time reasoning capabilities with a zero-shot prompt (the paper did not integrate o1-preview into the DeLLMa expected-utility pipeline).",
            "dataset_or_benchmark": "Compared on the paper's Agriculture and Stocks datasets (same 120 instances each).",
            "performance_metrics": "Reported decision accuracy: o1-preview achieved substantially lower accuracies compared to DeLLMa: Agriculture 33.3%, Stocks 35.0% (Table 3). Cost per instance reported: o1 ~ $0.21 per instance vs DeLLMa $0.09–$0.37 depending on configuration.",
            "comparison_to_baselines": "DeLLMa substantially outperformed o1-preview in the evaluated decision tasks despite o1's advanced inference-time reasoning capabilities, suggesting specialized decision-theoretic scaffolding yields large benefits over general-purpose inference-time methods.",
            "limitations_or_challenges": "Paper notes inability to fully control the o1 backbone and that o1 was used with zero-shot prompting for baseline comparison; the baseline's poor performance suggests that general inference-time scaling alone may be insufficient for structured decision-making under uncertainty.",
            "notable_findings": "Specialized, decision-theory-aligned inference-time procedures (DeLLMa) can substantially outperform strong general inference-time reasoning models like o1-preview on single-step, high-cost decision tasks under uncertainty.",
            "uuid": "e3728.4",
            "source_info": {
                "paper_title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Approaching human-level forecasting with language models",
            "rating": 2
        },
        {
            "paper_title": "Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy",
            "rating": 2
        },
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 1
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        }
    ],
    "cost": 0.014340499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DeLLMa: Decision Making Under Uncertainty with Large Language Models</h1>
<p>Ollie Liu<em>, Deqing Fu</em>, Dani Yogatama, and Willie Neiswanger<br>University of Southern California<br>me@ollieliu.com, {deqingfu, yogatama, neiswang}@usc.edu</p>
<h4>Abstract</h4>
<p>The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of decision-making under uncertainty. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling inference-time reasoning, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a $40 \%$ increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) are rapidly gaining traction across many domains due to their potential for automating and enhancing a broad spectrum of tasks [7, 10]. One important potential use is in decision making under uncertainty, i.e., deciding which action to take given some set of possibilities, properly factoring in user goals and uncertainty about the world. The ability to make good decisions under uncertainty holds broad relevance across high-stakes tasks in fields such as business, marketing, medicine, aeronautics, and logistics [22, 36]-and its value is not limited to organization-level decisions but extends to aiding individuals in making informed choices as well. The ability of LLMs to analyze large quantities of data makes them potentially well-suited for sophisticated decision support tools, and ensuring that these models give accurate, context-aware recommendations could significantly augment human decision-making capabilities.</p>
<p>However, optimal decision making under uncertainty is often challenging. For humans, there exist frameworks from decision theory and utility theory (developed in fields such as economics, statistics, and philosophy) to provide a structured approach for better decision-making [6, 29, 49]. Research has consistently demonstrated that without these frameworks, human decision-making can often be highly irrational, swayed by biases and incomplete information [4]. Similarly, making decisions with LLMs faces its own set of challenges. Issues include the tendency to fixate on specific explanations or information without adequately balancing all evidence, and the inability to effectively handle uncertainty, manage biases, or align with a user's goals and utilities [5, 17]. Our paper presents experiments that exemplify these issues.</p>
<p>Furthermore, beyond merely making rational decisions, it is crucial to understand why an LLM made a particular decision. This aids in building trust in the decision, assessing its quality, and improving any components that may lead to suboptimal outcomes. The ability to explain decisions and verify decision-making quality-which we refer to as human auditability-is essential for the practical application of LLMs to aid decision making in many real problems [47].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In this paper, our goal is to develop a framework that enables LLMs to make better decisions under uncertainty. Our aim is not only to enhance decision-making accuracy but also to allow human users to understand the rationale behind each decision. Drawing inspiration from prior work on multi-step reasoning like Chain-of-Thought (CoT) [51] and Tree-of-Thoughts (ToT) [56], in which compute is scaled at inference time, we design a procedure based on classical decision theory, originally designed for rational decision making under uncertainty by humans. Our approach involves three key steps: first, identify and forecast pertinent unknown variables given in-context information; second, elicit a utility function that aligns with the user's goals; and finally, use this utility function to identify the decision that maximizes expected utility. We call our proposed framework DeLLMa, short for Decision-making Large Language Model assistant.</p>
<p>We evaluate DeLLMa on real decision-making scenarios in agriculture and finance, and compare it against existing strategies for LLM decision-making, including zero-shot (direct) prompting, self-consistency [50], and CoT approaches. We find that DeLLMa significantly enhances decision-making accuracy, with improvements of up to a $40 \%$ increase in accuracy, particularly as the complexity and number of potential actions increases. Additionally, DeLLMa can consistently enhance performance across a variety of leading language models, and its structure allows us to understand the rationale behind each decision. In full, our contributions are:</p>
<ul>
<li>We introduce DeLLMa, a method for human-auditable LLM decision making under uncertainty, employing a multi-step reasoning process at inference time based on classical decision theory.</li>
<li>We evaluate components of DeLLMa, including the calibration of its state forecasting approach and a human agreement study to assess its utility elicitation method.</li>
<li>On realistic decision-making environments, we show that DeLLMa gives up to a $40 \%$ improvement in decision-making accuracy over competing methods, and yields consistent improvements when deployed across multiple leading LLMs.</li>
</ul>
<h1>2 Related Work</h1>
<p>Exemplified by OpenAI o1 [35], a combination of recent advances in scaling inference-time reasoning - such as task decomposition and structured search - has brought forth superhuman performances on deterministic reasoning tasks [12, 20]; but we nonetheless find that they are insufficient for decision-making under uncertainty. DeLLMa offers a specialized inference-time solution that scales favorbly with parallel sampling [45], a key ingredient for performant utility elicitation and decision optimization. Below, we summarize prior works pertaining to the DeLLMa framework.</p>
<p>LLMs for Decision Making. Prior works have leveraged LLMs for optimizing blackbox functions [34, 43, 54]. These settings involve methods that make a substantial number of low-cost decisions (which do not incur a high price for suboptimality). Instead, we focus on single-step expensive decisions, particularly in the prescence of uncertainty, with a focus on the optimality of decisions. Additionally, a number of applied domains that involve decision making have started to explore LLM-based methods, such as in supply chain optimization [26], medicine and health [5], and automated driving [31].</p>
<p>Uncertainty in LLMs. LLMs, without proper calibration, can be overly confident in their responses [44]. Such pitfalls make them unlikely to make reliable decisions under uncertainty. Prior work has aimed to solve this issue; one line of research involves asking LLMs for their own confidence, with or without additional finetuning [11, 21, 27, 33, 48, 53, 59]. Referring to [3] for a detailed survey, many recent advances [14, 16, 28, 52] adopt a Bayesian inference framework to quantify and reason with uncertainties in LLMs. Other works have shown that tool usage [40], retrieval augmentation [19], and model ensemble [42] can improve calibration and forecasting capabilities of LLMs. Our framework can take advantage of these advances in LLM-based forecasting for improved decision making.</p>
<p>Inference-time Reasoning in LLMs. Many recent works have leveraged inference time compute to extend the computational power of language models [32, 46]. These can be categorized into three main approaches: (1) instructing the model to generate intermediate reasoning traces [13, 43, 51, 55, 57, 58, 61], (2) decomposing a complex reasoning problem into tangible components [35, 38, 56, 60], and most recently (3) scaling the number of parallel samples to be postprocessed into a final solution [9, 35, 45, 50].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given a decision problem and contextual information as a prompt, DeLLMa (decision-making LLM assistant) maximizes an expected utility to select an available action. We illustrate the key steps of DeLLMa on decision-making tasks in agriculture planning (top) and finance (bottom).</p>
<h1>3 Methods</h1>
<p>Preliminaries. Suppose that a decision maker needs to make a choice between a set of options to achieve some goal-i.e., has a decision problem. We begin by formalizing such a decision problem, and afterwards describe how we approach decision making with LLMs. There are three main components to the decision problems that we will describe: actions, states, and utilities.</p>
<p>First, the actions are the possible options that a decision maker wishes to choose between. We use $\mathcal{A}$ to denote the space of actions, and $a \in \mathcal{A}$ for a single action. Second, the set of unknown states of nature are denoted $\Theta$. In our formulation, we define a state $\theta \in \Theta$ to be any latent variable whose true value is unknown, yet affects outcomes relevant to the decision maker's goals. To perform optimal decision making, one must act while accounting for uncertainty over these unknown states.</p>
<p>The third component involves the decision maker's preferences for different possible outcomes. We formalize our framework for decision making under uncertainty using utility theory, which can be viewed as "modeling the preferences of an agent as a real-valued function over uncertain outcomes" [18, 23, 41]. A key element is the utility function (in some formulations, this is instead given in terms of a loss function $L$ ). The utility function, denoted $U: \Theta \times \mathcal{A} \rightarrow \mathbb{R}$, assigns a scalar value to any state and action $(\theta, a) \in \Theta \times \mathcal{A}$. Intuitively, a higher utility means that the state-action pair yields a more-preferable outcome for the user.</p>
<p>The goal of the decision maker will be to choose a final decision $a^{*} \in \mathcal{A}$, which yields the highest possible utility, while accounting for uncertainty in the unknown states $\theta$.</p>
<p>Decision Making with LLMs: Setup and Current Approaches. We first describe the setting in which we intend our framework to operate. Suppose a human wishes to use this LLM assistant to help make a decision. They begin by describing a decision problem via a user prompt $\mathcal{P}$. We formalize a user prompt as a triplet $\mathcal{P}=(\mathcal{G}, \mathcal{A}, \mathcal{C})$, which includes: a natural language description of the user's goal $\mathcal{G}$, a list of $n$ actions $\mathcal{A}=\left(a_{1}, \ldots, a_{n}\right)$, and a passage of contextual information $\mathcal{C}$, which might be, e.g., pages from a report, or a text-based representation of historical data.</p>
<p>Referring to the agriculture planning decision problem in Figure 1 as a running example, the goal $\mathcal{G}$ is for a farmer to maximize their revenue in the forthcoming year; the action set $\mathcal{A}$ lists the possible produce the farmer is considering planting (e.g., apples, avocados, pears); and the context $\mathcal{C}$ consists of historical summaries of agricultural yields or information about the climate around the farm.</p>
<p>It is tempting to delegate such decision-making to LLMs with direct prompting. However, we observe that responses from conventional approaches, such as Self-Consistency and CoT, do not adequately balance available evidence, handle uncertain information, or align with user preferences; we show in Sec. 4 that these methods perform poorly, especially with increasing numbers of actions.</p>
<p>DeLLMa: Decision-Making LLM Assistant. To help encourage improved decisions under uncertainty, we propose a framework that guides an LLM to follow the scaffolding of classical decision theory. By restricting</p>
<p>LLMs to this scaffold we can also explicitly see components of the decision-making process-e.g., predictions of unknown states and utility function values-which provides human-auditability, allowing a user to identify why a given decision was made by the model.</p>
<p>In our initial formalization of this framework, we restrict ourselves to a slightly curtailed class of problems, and thus make a few simplifying assumptions. For example, we have assumed above that there are a discrete, enumerable set of $n$ possible actions, i.e., $\mathcal{A}=\left(a_{1}, \ldots, a_{n}\right)$. We will also assume there is a discrete set of $m$ possible states, $\Theta=\left(\theta_{1}, \ldots, \theta_{m}\right)$, though $m$ may be quite large.</p>
<p>Our framework will use an LLM to produce a belief distribution over the unknown states, given the input context $\mathcal{C}$. We view this as a posterior belief distribution over the states, which we denote by $\pi(\theta \mid \mathcal{C})$. Implicitly, we are assuming that the LLM implies a prior belief distribution $\pi(\theta)$, given only the model weights or training data.</p>
<p>Our framework will also elicit a utility function, based in part on the description of the user's goals $\mathcal{G} \in \mathcal{P}$. This utility function assigns a scalar value to any state-action pair $(\theta, a)$. We denote this utility function as $U(\theta, a)$. Given these, the expected utility under our LLM of taking an action $a$, given some additional context C, can be written</p>
<p>$$
U_{\mathcal{C}}(a)=\mathbb{E}<em _Theta="\Theta" _in="\in" _theta="\theta">{\pi(\theta \mid \mathcal{C})}[U(\theta, a)]=\sum</em>) U(\theta, a)
$$} \pi(\theta \mid \mathcal{C</p>
<p>Then, following the expected utility principle for rational decision making [30, 36], we select the Bayes-optimal decision $a^{*}$, which maximizes the expected utility, written</p>
<p>$$
a^{*}=\operatorname{argmax}<em c="c">{a \in \mathcal{A}} U</em>(a)
$$</p>
<p>We call our framework DeLLMa, short for Decision-making Large Language Model assistant. DeLLMa carries out this sequence of four steps-state enumeration, state forecasting, utility elicitation, and expected utility maximization. A full description of DeLLMa is shown in the box below. In the following sections we give details on our specific implementation of each of these four steps.</p>
<h1>DeLLMA: an Assistant for LLM Decision Making Under Uncertainty</h1>
<p>Input: Prompt $\mathcal{P}=(\mathcal{G}, \mathcal{A}, \mathcal{C})$ consisting of a user's goal $\mathcal{G}$, actions $\mathcal{A}=\left(a_{1}, \ldots, a_{n}\right)$, and context $\mathcal{C}$.</p>
<ol>
<li>
<p>State Enumeration: Produce a list of $m$ states $\Theta=\left(\theta_{1}, \ldots, \theta_{m}\right)$, which are unknown quantities whose values are predicted to influence the user's goal $\mathcal{G}$.</p>
</li>
<li>
<p>Section 3.1.</p>
</li>
<li>
<p>State Forecasting: For each state $\theta_{j}$, produce a probabilistic forecast $\pi\left(\theta_{j} \mid \mathcal{C}\right)$, which describes the probability of different values of this state given context $\mathcal{C}$.</p>
</li>
<li>
<p>Section 3.2.</p>
</li>
<li>
<p>Utility Function Elicitation: Produce a utility function $U:\left(\theta_{j}, a_{i}\right) \rightarrow \mathbb{R}$, which assigns a scalar value to each state-action pair $\left(\theta_{j}, a_{i}\right)$, based on the user's goal $\mathcal{G}$.</p>
</li>
<li>
<p>Section 3.3.</p>
</li>
<li>
<p>Expected Utility Maximization: For each action $a_{i}$, compute the expected utility $U_{\mathcal{C}}\left(a_{i}\right)=\mathbb{E}<em i="i">{\pi\left(\theta \mid \mathcal{C}\right)}\left[U\left(\theta, a</em>}\right)\right]$, and return the decision $a^{*}=\operatorname{argmax<em _mathcal_C="\mathcal{C">{i \in{1, \ldots, n}} U</em>\right)$. - Section 3.4 .}}\left(a_{i</p>
</li>
</ol>
<h3>3.1 State Enumeration</h3>
<p>As an initial demonstration of DeLLMa, our goal is to develop a simple implementation that performs well empirically; however, each component in the framework can be extended or made more sophisticated. We first describe the strategy that we adopt for enumerating a space of relevant latent states $\Theta=\left(\theta_{1}, \ldots, \theta_{m}\right)$. Given $\mathcal{P}$ as context, we prompt an LLM to identify $k$ latent factors that are predicted to influence the user's goal $\mathcal{G}$ (see $\S$ C. 2 for details on this prompt). Each latent factor is a string (a word or phrase), which can be viewed as describing a dimension of our state space $\Theta$. We denote these $k$ latent factors as $\left(f_{1}, \ldots, f_{k}\right)$.</p>
<p>For each latent factor, we prompt our LLM to generate $\ell$ plausible values of the latent factor (empirically we find that it is sufficient to set $\ell$ to be small, e.g., $&lt;5$ ). For a latent factor $f_{j}$, we denote its plausible values</p>
<p>as $\tilde{f}_{j}^{1: \ell}$, where each plausible value is also a string (a word or phrase). This process discretizes the state space, where each of the $k$ dimensions has $\ell$ bins. We find that this strategy, while simple, yields an empirically effective method for forecasting states (described in $\S 3.2$ ).</p>
<p>A single state $\theta_{j}$ in this state space consists of one plausible value from each of the $k$ latent factors, which we can denote by $\theta_{j}=\theta_{j}^{1: k} \in \Theta$. In total, this produces a discretized state space of size $|\Theta|=m=\ell^{k}$. While this state space is too large to enumerate explicitly, we develop a procedure to forecast probabilities for these states in a scalable manner.</p>
<h1>3.2 State Forecasting</h1>
<p>In the next step of DeLLMa, we form a probabilistic forecast of the unknown states, given information contained in the context $\mathcal{C} \in \mathcal{P}$. A number of recent works have shown that LLMs are capable of returning well-calibrated forecasts with respect to some provided information [19, 42] (see §2). Here, we develop a relatively simple forecasting method that we find works well empirically; though DeLLMa allows us to flexibly use other forecasting methods in the future (potentially leveraging search and retrieval of information). This step yields a distribution over the state space, which we can sample from to get a Monte Carlo estimate of the expected utility.</p>
<p>For each of the $k$ latent factors, and each of their $\ell$ possible values $\tilde{f}_{j}^{1: \ell}$, we prompt our LLM to assign a verbalized probability score $\in{$ very likely, likely, somewhat likely, somewhat unlikely, unlikely, very unlikely $}$. In total, we must assign $k \times \ell$ scores. We provide all prompts for this probability score procedure in Figure 11. We then define a dictionary $\mathcal{V}$ that maps each verbalized probability score to a numerical value. Similar strategies converting from verbalized to numeric scores have been used with success in prior work [48, 53]. After normalization, this yields a distribution over the state space $\Theta$, assuming independence between the $k$ latent factors, which we posit for computational simplicity. We sample states from this distribution by iterating through each of the latent factors, sampling according to its approximate marginal probability, and concatenating the samples. In Sec. 4.1 we directly evaluate this state forecasting procedure to show that it yields well-calibrated forecasts in real-data scenarios, and we conduct an ablation study in Sec. 4.3.</p>
<p>The full procedure is shown in Algorithm 1. Here, the Normalize function simply scales the weights instantiated in the marginal distribution to a well-defined probability mass function (PMF). We consider the sampled states to be from an LLM-defined proposal distribution $\pi^{\mathrm{LLM}}(\theta \mid \mathcal{C})$, returned as output from Algorithm 1, which approximates the posterior belief distribution $\pi(\theta \mid \mathcal{C})$.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 StateForecast
    Input: LLM \(\mathcal{M}\), user prompt \(\mathcal{P}=(\mathcal{G}, \mathcal{A}, \mathcal{C})\), plausibilty score mapping \(\mathcal{V}\), latent factors \(\left\{f_{1}, \cdots, f_{k}\right\}\) and plausible
    values \(\left\{\tilde{f}_{1}^{1: \ell}, \cdots, \tilde{f}_{k}^{1: \ell}\right\}\).
    for \(i=1\) to \(k\) do
        \(\pi_{i}(\cdot \mid \mathcal{C}) \leftarrow\}
        \# Verbalized probability score
        \(\left[v_{1}, \cdots v_{\ell}\right] \leftarrow \mathcal{M}\left(\mathcal{P}, f_{i}, \tilde{f}_{i}^{1: \ell}\right)\)
        for \(j=1\) to \(\ell\) do
            \(\pi_{j}\left(\tilde{f}_{i}^{j} \mid \mathcal{C}\right) \leftarrow \mathcal{V}\left[v_{j}\right]\)
        end for
        \(\pi_{i}(\cdot \mid \mathcal{C}) \leftarrow \operatorname{Normalize}\left(\pi_{i}(\cdot \mid \mathcal{C})\right)\)
    end for
    return \(\pi^{\mathrm{LLM}}\left(f_{1}, \cdots, f_{k} \mid \mathcal{C}\right) \succ \prod_{i=1}^{k} \pi_{i}(\cdot \mid \mathcal{C})\)
</code></pre></div>

<h3>3.3 Utility Function Elicitation</h3>
<p>Next, we need a method to elicit (which is to say: construct) a utility function $U: \Theta \times \mathcal{A} \rightarrow \mathbb{R}$, which maps a state-action pair to a real value. An accurate utility function, which balances the preferences of a human user with respect to the goal that they describe, is difficult to define directly in a general-purpose manner. There is a long history of work on utility elicitation methods [15], which aim to construct a utility function</p>
<p>from, e.g., pairwise preference data. Here, we combine these methods with large language models to try and automatically elicit a utility function.</p>
<p>We conduct the following procedure. We first sample states from the forecast state distribution $\pi^{\mathrm{LLM}}(\theta \mid \mathcal{C})$, and from these form a set of state-action pairs. We group these pairs into minibatches, and prompt our LLM to rank the elements of each minibatch, given the user's goal $\mathcal{G} \in \mathcal{P}$. This LLM-based ranking of items-where each item consists of an action and a particular instantation of states - is a procedure that can be broadly applied, and LLMs have a history of being successfully used for similar comparisons [24, 37]. Based on these rankings, we are able to extract pairwise preferences, which we can use in classic utility elicitation algorithms.</p>
<p>We show this procedure in Algorithm 2 and discuss two implementations of the FormatRank step: Rank2Pairs and One-vs-All. Denoting $(\theta, a)<em _1_="(1)">{(i)}$ as the $i$-th preferred state-action pair of the minibatch, Rank2Pairs converts a ranking of decreasing preference $\mathcal{R}=\left{(\theta, a)</em>, \cdots,(\theta, a)<em _i_="(i)">{(b)}\right}$ to a list of pairwise comparisons by adding $(\theta, a)</em>&gt;(\theta, a)<em _i_="(i)">{(j)}$ whenever $i<j$. In contrast, One-vs-All assumes that the LLM is indifferent towards all but the top-ranked state-action pair, i.e., $\{(\theta, a)_{(1)}>(\theta, a)</em>$ with respect to the sampled state-action pairs. Finally, we find two additional ingredients are beneficial for scaling our inference-time reasoning, which help improve the accuracy and computational efficiency of utility elicitation: batching and variance reduction.} \mid \forall 2 \leq i \leq b}$. This implementation may be desirable when accurate comparisons of certain suboptimal state-actions is challenging. We then make use of these preferences as training data for a Bradley-Terry model [8] to elicit an approximate utility function $U: \Theta \times \mathcal{A} \rightarrow \mathbb{R</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nx">UtilityElicitation</span>
<span class="w">    </span><span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">P</span><span class="p">}=(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">C</span><span class="p">})</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">proposal</span><span class="w"> </span><span class="nx">distribution</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">LLM</span><span class="p">}}(</span><span class="err">\</span><span class="nx">theta</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">C</span><span class="p">})</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">size</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">minibatch</span><span class="w"> </span><span class="nx">size</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">b</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">and</span>
<span class="w">    </span><span class="nx">overlap</span><span class="w"> </span><span class="nx">proportion</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">q</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="err">\#</span><span class="w"> </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">fixed</span><span class="w"> </span><span class="nx">states</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="k">forall</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">times</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">LLM</span><span class="p">}},</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="err">\</span><span class="nx">leq</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="nx">leq</span><span class="err">\</span><span class="nx">lfloor</span><span class="w"> </span><span class="nx">s</span><span class="w"> </span><span class="o">/|</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="o">|</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Shuffle</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Omega</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">Pairwise</span><span class="w"> </span><span class="nx">comparisons</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">lfloor</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="err">\</span><span class="nx">times</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="nx">q</span><span class="p">)</span><span class="err">\</span><span class="nx">rfloor</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\#</span><span class="w"> </span><span class="nx">Rank</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">minibatch</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">R</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">P</span><span class="p">},</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">cdots</span><span class="p">,</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="nx">b</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="nx">b</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\#</span><span class="w"> </span><span class="nx">Format</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="nx">comparison</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Omega</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">Omega</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">FormatRank</span><span class="p">}(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">R</span><span class="p">})</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">U</span><span class="p">(</span><span class="err">\</span><span class="nx">cdot</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">cdot</span><span class="p">):=</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">BradleyTerry</span><span class="p">}(</span><span class="err">\</span><span class="nx">Omega</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="nx">R</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Batching. We implement a batched inference procedure that slices state-action samples $S_{A}={(\theta, a)}$ into overlapping minibatches for ranking. We ensure that $q \%$ of samples are shared between two consecutive minibatches drawn from $S_{A}$, where $q$ is a hyperparameter that modulates a minibatch's degree of exposure to the preference of the previous minibatch. Larger $q$ results in finer-grained preference at the cost of more queries. Effects of $q$ are ablated in Figure 3.</p>
<p>Variance Reduction. Directly sampling $\left|S_{A}\right|$ state values from the proposal distribution $\pi^{\mathrm{LLM}}$ may lead to high-variance estimates of utilities. We instead sample $\left|S_{A}\right| /|\mathcal{A}|$ independent state values from $\pi^{\mathrm{LLM}}$, create $|\mathcal{A}|$ duplicates, and pair them with each action $a$ (see Figure 5 in Appendix A).</p>
<p>In $\S 4.3$, we conduct ablation studies to validate our utility elicitation procedure, which shows high agreement rate between DeLLMa rankings and human preferences. We also study scaling laws of sample size and overlap percentage. We find that this type of specialized inference-time solution scales favorably against general-purpose systems such as OpenAI o1 [35].</p>
<h1>3.4 Expected Utility Maximization</h1>
<p>In the final step of DeLLMa, we compute the expected utility for each action, and then return the action that maximizes the expected utility. In particular, for each action, we compute a Monte Carlo estimate of the expected utility using state-action samples (drawn from the state forecast distribution $\pi^{\mathrm{LLM}}(\theta \mid \mathcal{C})$ ), as well as</p>
<p>the elicited utility function. Note that these calculations are all performed analytically (not via an LLM). We can then approximate the expected utility $U_{\mathcal{C}}(a)$ as</p>
<p>$$
U_{\mathcal{C}}(a)=\mathbb{E}<em S="S" _in="\in" _theta="\theta">{\pi(\theta \mid \mathcal{C})}[U(\theta, a)] \approx \frac{1}{|S|} \sum</em> U(\theta, a)
$$</p>
<p>given a set of state samples $S \subseteq \Theta$ drawn from our LLM-defined state forecast distribution, which is an approximation of the LLM's posterior belief distribution about states given context $\mathcal{C}$, i.e., $S \stackrel{\text { i.i.d. }}{=} \pi^{\mathrm{LLM}}(\theta \mid$ $\mathcal{C}) \approx \pi(\theta \mid \mathcal{C})$. After computing the expected utility $U_{\mathcal{C}}(a)$ for each action, DeLLMa returns the final decision: $a^{*}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} U_{\mathcal{C}}(a)$.</p>
<h1>4 Experiments</h1>
<p>We evaluate the performance of DeLLMa on two decision making under uncertainty environments sourced from different domains: agricultural planning (Agriculture) and finance investing (Stocks). Both involve sizable degrees of uncertainty from diverse sources, and are representative of distinct data modalities (natural language and tabular) involved in decision making. We propose the following DeLLMa variants designed to assess algorithmic improvements, as outlined in Section 3:</p>
<ul>
<li>DeLLMa-Pairs is the method using all techniques in $\S 3.3$ and Rank2Pairs for utility elicitation.</li>
<li>DeLLMa-Top1 is identical to DeLLMa-Pairs, but replaces Rank2Pairs with One-vs-All.</li>
<li>DeLLMa-Naive is a base version of DeLLMa-Pairs, where we sample multiple states per action and construct pairwise comparisons from a single batch (i.e., no batching or variance reduction).</li>
</ul>
<p>For DeLLMa-Pairs and Top1, we allocate a per action sample size of 64 and a minibatch size of 32 . We set the overlap proportion $q$ to $25 \%$ for the Agriculture dataset and $50 \%$ for the Stocks dataset due to budget constraints. For DeLLMa-Naive, we fix a total sample size of 50 . As a default LLM in experiments (except for comparisons across different LLMs), we use GPT-4 [1] ${ }^{1}$.</p>
<p>We compare DeLLMa against three baselines-zero-shot, self-consistency, and Chain-of-Thought (where example prompts for these baselines are given in Figures 9, 10, 19, and 20):</p>
<ul>
<li>Zero-Shot. Only the goal $\mathcal{G}$, the action space $\mathcal{A}$, and the context $\mathcal{C}$ is provided. We adopt a greedy decoding process by setting temperature $=0$.</li>
<li>Self-Consistency (SC) [50]. We use the same prompt as in zero-shot, but with temperature $=0.5$ to generate a set of $K$ responses. We take the majority vote of the $K$ responses.</li>
<li>Chain-of-Thought (CoT) [51]. For decision-making tasks, there is no standard CoT pipeline. Inspired by workflows from decision theory, we create a prompting chain consisting of three steps: (1) ask for unknown factors that impact the decision; (2) given these, ask for their possibility of occurence; (3) then ask for a final decision. Such a mechanism is similar to the DeLLMa pipeline (see $\S 3$ ) but only consists of prompting.</li>
</ul>
<p>Evaluation Metrics. For both datasets, our action spaces consist of a set of items, and we evaluate the performance of both DeLLMa and baseline methods by comparing the accuracy of their prediction from this set against the ground-truth optimal action (i.e., the action that maximizes ground-truth utility). We also report normalized utility-i.e., the ground-truth utility of the action chosen by a given method, normalized by the optimal ground-truth utility-in Appendix B.</p>
<p>In what follows, we report the performance of DeLLMa against baseline approaches, while controlling for the LLM backbone. In $\S 4.3$, we also evaluate OpenAI o1 [35] to study the difference between general inference-time reasoning and our specialized approach for decision making under uncertainty. This experiment is not included in our main result as we are unable to control the LLM backbone. We defer more involved decision-making under uncertainty problems, such as constructing a weighted combination of actions (i.e., a portfolio), to future works.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results on the Agriculture environment. Left: DeLLMa variants outperform baseline methods for varying numbers of actions. Right: We see that DeLLMa yields a consistent improvement in decision-making accuracy across three families of leading LLMs.</p>
<h1>4.1 Agriculture</h1>
<p>Data Acquisition. We collect bi-annual reports published by the United States Department of Agriculture (USDA) that provide analysis of supply-and-demand conditions in the U.S. fruit markets ${ }^{2}$. To emulate real-life farming timelines, we use the report published in September 2021 as context for planning the forthcoming agricultural year. We additionally supplement these natural language contexts with USDA-issued price and yield statistics in California ${ }^{3}$.</p>
<p>We define the utility of planting a fruit as its price $\times$ yield reported in the forthcoming year. We identify 7 fruits as our action set $\mathcal{A}$-apple, avocado, grape, grapefruit, lemon, peach, and pear-that are both studied in the September 2021 report, and endowed with these statistics in 2021 and 2022. We create decision making problems by enumerating all possible combinations of availble fruits, resulting in 120 decision problem instances. For each decision-making instance, we use related sections of the USDA report and current-year price and yield statistics as context. See Appendix C. 1 and Figure 8 for additional details on preprocessing, and Figure 12 for DeLLMa prompts.</p>
<p>Main Result. From Figure 2, we observe that all DeLLMa strategies consistently outperform baseline comparison methods, especially for larger action sets. We plot decision accuracy here, but include equivalent plots showing the utility of each strategy in Appendix B. Among DeLLMa variants, the performance of Pairs and Top1 are consistent across the board; both are significantly better than Naive. This observation shows the benefits of the algorithms outlined in $\S 3.3$. We give more detailed analyses in the ablation study below. We also show a comparison of DeLLMa deployed on multiple leading LLMs (GPT-4 [1], Claude-3 [2], Gemini 1.5 [39]), with consistent performance improvements across multiple model families. We refer readers to Appendix C and supplementary material for details on data, prompts, and responses.</p>
<p>How Good Are State Forecasts? We first evaluate the quality of forecast distributions. Towards this, we manually annotate a set of ground truth values for states that the LLMs forecast within DeLLMa, and then compare this with the forecast state distribution. We compare the average calibration (ECE), and the negative log-likelihood (NLL), two popular metrics for assessing the quality of a probabilistic forecast. As shown in Table 1, all models achieve reasonable calibration performances, which correlate with the overall DeLLMa prediction accuracy. We conduct additional ablation studies to assess each DeLLMa module contributions in $\S 4.3$.</p>
<p>Table 1: An evaluation of state forecasting, showing ECE and NLL. Lower is better.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ECE $\downarrow$</th>
<th style="text-align: center;">NLL $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">1.11</td>
</tr>
<tr>
<td style="text-align: left;">Claude 3</td>
<td style="text-align: center;">0.142</td>
<td style="text-align: center;">1.20</td>
</tr>
<tr>
<td style="text-align: left;">Gemini 1.5</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">1.09</td>
</tr>
</tbody>
</table>
<p>Failure Modes of Baseline Methods. One surprising observation is that, in the case of larger action sets, our baseline methods often underperform random guessing. A failure mode is that these methods elicit decisions that echo the sentiments presented in context, and they lack the ability to reason what-if scenarios</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left: Study on sample size and overlap percentage used by DeLLMa. Scaling the compute at test time produces better average accuracy. When ablating overlap percentage, we fix sample size at 16; when ablating sample size, we fix overlap percentage at 25%. Right: Illustration of the DeLLMa decision tree for the Agriculture dataset, showing two of the actions, and two of the sampled states per action. Each weight w denotes the posterior probability $\pi(\theta_{i}^{(j)} \mid \mathcal{C})$.</p>
<p>that lead to utility changes. Our experiments on SC and CoT indicate that neither sampling reasoning paths, prompting the model to imagine the alternatives, nor augmenting an LLM with a posterior belief substantially enhance the model's ability to reason with uncertainty. By conditioning on sampled states, DeLLMa can avoid this pitfall while leveraging in-context learning to decide the preferred state-action pair. See Appendix C.4 and Figures 13, 17 and 18 for discussions on failure cases of baseline methods that are addressed by DeLLMa.</p>
<p>In addition to performance improvements, our structured approach is endowed with human auditability. In Figure 3 (right), we show an abbreviated decision network, with actions, states, sampled latent factors, and derived utilities constructed from outputs of a DeLLMa agent. This modular approach to decision making can facilitate transparency and trust of LLMs in high-stake scenarios.</p>
<h3>4.2 Stocks</h3>
<p>Making decisions involving financial investing requires handling a variety of uncertainties; however, it is fundamentally different from the agriculture decision problems. Most evidently is the difference in input format—contexts for agriculture rely more on textual information (summarizations from USDA reports) whereas contexts for stocks involve tabular data (historical prices per share). Stocks decision problems are well suited to test LLMs capability in reasoning on dynamic tabular data.</p>
<p><strong>Data Acquisition.</strong> Similar to the setup in §4.1, the action space $\mathcal{A}$ consists of individual stocks and we curate a total of 120 decision problem instances of varying sizes. In our experiments, we choose popular stocks whose symbols are AMD, DIS, GME, GOOGL, META, NVDA and SPY. Unlike agriculture data where the context $\mathcal{C}$ are collected through USDA reports, we collect historical stock prices as the context for this problem. As illustrated in Figure 1, each stock is presented with 24 monthly price in history. In preventing possible data leakage and promoting LLMs to use their common-sense knowledge in making decisions, when using gpt4-1106-preview as the LLM checkpoint, historical price between December 2021 to November 2023 are provided as the context $\mathcal{C}$. These historical monthly prices are collected via Yahoo Finance<sup>4</sup> manually by the authors.</p>
<p>The goal of the LLM agent is to choose which stock to invest on December 1st, 2023, and sell on the last trading day of that month (December 29, 2023) so that the return is maximized. Detailed prompts are presented in Appendix D.1. We note that we only consider the simplistic setting—choosing one stock from a set of options ${a_1, \ldots, a_n}$.</p>
<p><strong>Main Results.</strong> Similarly, we compare three variants of DeLLMa with the three popular baselines. Shown in Figure 4, most of the observations are consistent with those in the agriculture experiments—DeLLMa's</p>
<p><sup>4</sup>finance.yahoo.com</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results on the Stocks environment. Left: We see that on average, DeLLMa-Top1 outperforms all baselines. Right: Illustration of the DeLLMa decision tree for the Stocks dataset, showing two of the actions and two of the sampled states per action.</p>
<p>variants outperform the baseline candidates. Unlike the agriculture setting, here DeLLMa-Naive only slightly improves over the baselines. We hypothesis this is due to the high volatility of stocks data, where inefficient sample size without variance reduction produces highly volatile predictors as well. This also validates the need for the design of the DeLLMa-Pairs and DeLLMa-Top1 methods.</p>
<p>Additionally, DeLLMa-Top1 performs better than DeLLMa-Pairs in stocks data. This is surprising at first glance since DeLLMa-Pairs have more observations in terms of pairwise preferences and ranking of sampled action-state pairs. However, upon further consideration, we hypothesize potential explanations: LLM hallucination is still an unsolved problem. For high-volatility data like stocks, LLMs can easily hallucinate with internal rankings if asked to perform difficult tasks (such as to rank a batch of options where a ground-truth ranking may not exist). On the other hand, the model may still have high confidence about its prediction of the top choice in the batch. In such scenarios, using the hallucinated rankings may only provide extra noise that hinders the model performance.</p>
<h3>4.3 Ablation Studies</h3>
<p><strong>Scaling Laws for Inference-Time Compute.</strong> Referring back to Figure 2, a potential explanation for the performance difference between DeLLMa-Naive and Pairs/Top1 is the discrepancy in sample size: for Naive, we allocate a fixed sample size (50) for all decision-making instances, and we allocate a fixed <em>per action sample size</em> (64) for Pairs and Top1. We eliminate this confounder in our ablation study, by noting that with <em>per action sample size</em> 8 (middle subfigure in Figure 3), DeLLMa-Pairs/Top1 achieve comparable performance to Naive, despite only receiving 16-48 samples in total for each problem instance.</p>
<p>In Figure 3 (left), we observe linear performance trends when scaling up overlap percentage and sample size. Intuitively, both higher overlap percentages (<em>i.e.</em>, more exposure between minibatches) and larger sample size lead to construction of finer-grained pairwise comparisons and thus high quality approximate utilities. Furthermore, DeLLMa-Pairs consistently outperforms Top1, implying that a nontrivial portion of the pairwise comparisons are meaningful. However, we note that the number of required API queries scales linearly with both parameters, and users are advised to choose these parameters that balance performance and cost. We report statistics for API queries and prompt lengths for our methods in Table 5 of Appendix B.</p>
<p><strong>State Forecasting Ablation.</strong> We now assess the quality of the state forecasting procedure by evaluating DeLLMa in the presence of low-quality or extraneous state information. On the agriculture dataset, we report in Table 2 the ablation results from 3 variants for the proposal distribution <em>π</em><sup>LLM</sup>: uniform, underspecified, and overspecified. We fix a <em>per action sample size</em> of 16 and an <em>overlap percentage</em> of 25%, and refer readers to Appendix E for additional details.</p>
<p>We observe that the performance of the modified forecasting methods in GPT-4 and Gemini 1.5 are similar to full DeLLMa performance, and are better than our baseline methods. With these models, DeLLMa appears to be robust against misspecified, insufficiently representative, and extraneous forecast states. However, with Claude 3, ablation performance is more akin to zero-shot baselines. This discrepancy may be due to Claude's weaker performance in DeLLMa (c.f. Figure 2). Overall, our state forecasting procedure appears to be</p>
<p>Table 2: Performance comparison across variations of our state forecasting procedure.</p>
<table>
<thead>
<tr>
<th></th>
<th>GPT-4</th>
<th>Claude 3</th>
<th>Gemini 1.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>Uniform</td>
<td>$58.3 \%$</td>
<td>$32.5 \%$</td>
<td>$45.8 \%$</td>
</tr>
<tr>
<td>Underspecified</td>
<td>$55.0 \%$</td>
<td>$34.2 \%$</td>
<td>$42.5 \%$</td>
</tr>
<tr>
<td>Overspecified</td>
<td>$56.7 \%$</td>
<td>$35.8 \%$</td>
<td>$45.8 \%$</td>
</tr>
<tr>
<td>DeLLMa</td>
<td>$60.0 \%$</td>
<td>$40.8 \%$</td>
<td>$47.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance comparison against SotA inference-time reasoning models.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>DeLLMa $(n=64)$</th>
<th>o1-preview</th>
</tr>
</thead>
<tbody>
<tr>
<td>Agriculture</td>
<td>$73.3 \%$</td>
<td>$33.3 \%$</td>
</tr>
<tr>
<td>Stock</td>
<td>$64.2 \%$</td>
<td>$35.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on a human evaluation of utility elicitation in DeLLMa. Details in $\S 4.3$.</p>
<table>
<thead>
<tr>
<th></th>
<th>GPT-4</th>
<th>Claude 3</th>
<th>Gemini 1.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>Agreement $\%$</td>
<td>$70.4 \%$</td>
<td>$65.3 \%$</td>
<td>$69.6 \%$</td>
</tr>
<tr>
<td>with Human</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>beneficial for DeLLMa performance.
Evaluation Against OpenAI o1. In $\S 4$, we have compared different prompting approaches while fixing the LLM backbone. We now evaluate OpenAI o1-a class of more advanced models adept in inference-time reasoning-against DeLLMa. Table 3 shows that our best performances (attained with a per action sample size of 64 and overlap percentage of $25 \%$ ) can outperform o1 with the zero-shot prompt (c.f. Figures 9 and 22) by a wide margin, while attaining similar costs (DeLLMa: $\$ 0.09$ to $\$ 0.37$ vs. o1: $\$ 0.21$ per instance) ${ }^{5}$. This study indicates the benefits of specialized inference-time reasoning for decision making under uncertainty.
Human Evaluation for Preference Ranking. Lastly, we perform a study to evaluate the quality of utility elicitation in DeLLMa. We curate a large set of state-action samples; human annotators (the paper authors) are then shown pairs of these state-action samples and asked to annotate a preference, based on the decision prompt $\mathcal{P}$. We then compute the agreement rate between the pairwise annotations given by DeLLMa and those given by the annotators. Although this step is noisy, we see a strong agreement between LLM and human annotations, as shown in Table 4, across multiple LLMs. We also find that our inter-annotator agreement rate is $77.5 \%$, which is on par with the human-LLM agreements. Additional details of our annotation procedure are reported in Appendix F.</p>
<h1>5 Conclusion</h1>
<p>We propose DeLLMa, a framework designed to harness LLMs for decision making under uncertainty in high-stakes settings. This is a structured approach in which we provide a process for inference-time reasoning in LLMs that follows the principles of classical decision theory. We then develop a feasible implementation with LLMs. Through experiments on real datasets, we highlight a systematic failure of popular prompting strategies when applied to this type of decision making task, and demonstrate the benefits of our approach to address these issues. The modularity of our framework avails many possibilities, most notably auditability and using LLMs for a broader spectrum of probablistic reasoning tasks.</p>
<h2>References</h2>
<p>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024.
[3] Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and Wilker Aziz. Uncertainty in natural language generation: From theory to applications. arXiv preprint arXiv:2307.15703, 2023.
[4] Max H Bazerman and Don A Moore. Judgment in managerial decision making. John Wiley \&amp; Sons, 2012.
[5] Manuela Benary, Xing David Wang, Max Schmidt, Dominik Soll, Georg Hilfenhaus, Mani Nassir, Christian Sigler, Maren Knödler, Ulrich Keller, Dieter Beule, et al. Leveraging large language models for decision support in personalized oncology. JAMA Network Open, 6(11):e2343689-e2343689, 2023.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>[6] James O Berger. Statistical decision theory and Bayesian analysis. Springer Science \&amp; Business Media, 2013 .
[7] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
[8] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952.
[9] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024.
[10] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[11] Jiuhai Chen and Jonas W. Mueller. Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment. ArXiv, abs/2308.16175, 2023. URL https://api. semanticscholar.org/CorpusID:261339369.
[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.
[13] Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the new autodiff-unlocking efficient optimization of computational workflows. arXiv preprint arXiv:2406.16218, 2024.
[14] Fabian Falck, Ziyu Wang, and Christopher C Holmes. Are large language models bayesian? a martingale perspective on in-context learning. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models.
[15] Peter H Farquhar. State of the art—utility assessment methods. Management science, 30(11):1283-1300, 1984 .
[16] Yu Feng, Ben Zhou, Weidong Lin, and Dan Roth. Bird: A trustworthy bayesian inference framework for large language models. arXiv preprint arXiv:2404.12494, 2024.
[17] Emilio Ferrara. Should chatgpt be biased? challenges and risks of bias in large language models. arXiv preprint arXiv:2304.03738, 2023.
[18] Peter C Fishburn. Utility theory. Management science, 14(5):335-378, 1968.
[19] Danny Halawi, Fred Zhang, Chen Yueh-Han, and Jacob Steinhardt. Approaching human-level forecasting with language models. arXiv preprint arXiv:2402.18563, 2024.
[20] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>[21] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022.
[22] Mykel J Kochenderfer. Decision making under uncertainty: theory and application. MIT press, 2015.
[23] Mykel J Kochenderfer, Tim A Wheeler, and Kyle H Wray. Algorithms for decision making. MIT press, 2022 .
[24] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF: Scaling reinforcement learning from human feedback with ai feedback, 2024. URL https://openreview.net/forum?id= AAxIs3D2ZZ.
[25] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474, 2020.
[26] Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language models for supply chain optimization, 2023.
[27] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words, 2022 .
[28] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantification for black-box large language models, 2023.
[29] R Duncan Luce and Howard Raiffa. Games and decisions: Introduction and critical survey. Courier Corporation, 1989.
[30] Mark J Machina. Choice under uncertainty: Problems solved and unsolved. Journal of Economic Perspectives, 1(1):121-154, 1987.
[31] Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. A language agent for autonomous driving, 2023.
[32] William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.
[33] Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents' overconfidence through linguistic calibration, 2022.
[34] Allen Nie, Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. Importance of directional feedback for llm-based optimizers. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.
[35] OpenAI. Learning to reason with llms. https://openai.com/index/learning-to-reason-with-llms/, 2024 .
[36] Martin Peterson. An introduction to decision theory. Cambridge University Press, 2017.
[37] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. Large language models are effective text rankers with pairwise ranking prompting, 2023.
[38] Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošǐǔtė, et al. Question decomposition improves the faithfulness of model-generated reasoning. arXiv preprint arXiv:2307.11768, 2023.</p>
<p>[39] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.
[40] Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, F. Xia, Jacob Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar. Robots that ask for help: Uncertainty alignment for large language model planners. ArXiv, abs/2307.01928, 2023. URL https://api.semanticscholar.org/CorpusID:259342058.
[41] Paul JH Schoemaker. The expected utility model: Its variants, purposes, evidence and limitations. Journal of economic literature, pp. 529-563, 1982.
[42] Philipp Schoenegger, Indre Tuminauskaite, Peter S Park, and Philip E Tetlock. Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy. arXiv preprint arXiv:2402.19379, 2024 .
[43] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024 .
[44] Chenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-Graber. Re-examining calibration: The case of question answering. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2814-2829, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. findings-emnlp.204. URL https://aclanthology.org/2022.findings-emnlp.204.
[45] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.
[46] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024.
[47] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):1930-1940, 2023 .
[48] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023.
[49] John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. 1944.
[50] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.
[51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.
[52] Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, Vikash K Mansinghka, Jacob Andreas, and Joshua B Tenenbaum. From word models to world models: Translating from natural language to the probabilistic language of thought. arXiv preprint arXiv:2306.12672, 2023.
[53] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023 .</p>
<p>[54] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.
[55] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[56] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.
[57] Yining Ye, Xin Cong, Yujia Qin, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Large language model as autonomous decision maker. arXiv preprint arXiv:2308.12519, 2023.
[58] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.
[59] Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, et al. Uncertainty is fragile: Manipulating uncertainty in large language models. arXiv preprint arXiv:2407.11282, 2024.
[60] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.
[61] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, and Chao Zhang. Toolchain<em>: Efficient action space navigation in large language models with a</em> search. arXiv preprint arXiv:2310.13227, 2023.</p>
<h1>Appendix</h1>
<h2>A Utility Elicitation Details</h2>
<p>In Figure 5, we show an illustration of the overlapped batching and variance reduction strategies that DeLLMa uses in its utility elicitation procedure (described in detail in Section 3.3).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Schematic diagram of overlapped batching and variance reduction.</p>
<h2>B Additional Results</h2>
<p>In Figures 6 and 7 we show the normalized utility (i.e., ground-truth utility of the chosen action, normalized by the maximum ground-truth utility) of each method. These results can be contrasted against the accuracy of each method's prediction of the optimal action in Figures 2 and 4.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Normalized utility for the Agriculture dataset. Across all action sizes, DeLLMa-Pairs/Top1 outperforms other methods, including DeLLMa-Naive. Similar to accuracy, DeLLMa-Pairs slightly outperforms Top1, which is consistent with our hypothesis that the complete ranking generated from GPT-4 is meaningful.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Normalized utility for the Stock dataset. DeLLMa-Top1 is competitive againt Pairs, suggesting that financial decision making may be a challenging scenario for GPT-4 to elicit preference unequivocally.</p>
<p>Aside from performance improvements, we also compare the cost measured in terms of prompt length and API calls in Table 5. For the Agriculture dataset, DeLLMa-Naive is comparable to SC in terms of prompt length and significantly outperform the latter, while only requiring $20 \%$ of API calls. DeLLMa-Pairs/Top1 can push the performance further, but incurs a higher cost, especially for large sample sizes.</p>
<p>Table 5: Number of GPT-4 API calls and word counts per decision-making instance (with action set size 4 for the Agriculture dataset) across all methods discussed in $\S 4.1$. For DeLLMa-Naive, we set the total sample size to 50 . For DeLLMa-Pairs, we fix the overlap percentage to $25 \%$ and vary the per action sample size from 16 to 64. DeLLMa-Top1 has the same statistics as DeLLMa-Pairs since they only differ in post processing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Zero-Shot</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">SC</th>
<th style="text-align: center;">D-Naive</th>
<th style="text-align: center;">D-Pairs (16)</th>
<th style="text-align: center;">D-Pairs (64)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">API Calls</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Token Counts</td>
<td style="text-align: center;">495</td>
<td style="text-align: center;">1946</td>
<td style="text-align: center;">2477</td>
<td style="text-align: center;">2629</td>
<td style="text-align: center;">5181</td>
<td style="text-align: center;">20639</td>
</tr>
</tbody>
</table>
<h1>C Additional Details for Agriculture</h1>
<h2>C. 1 Dataset Curation</h2>
<p>To reduce context length, we first extract executive and per-fruit summaries of the report, reducing the context length from 8,721 words to around $&lt;700$ words. These summaries are constructed on a per-model basis: each LLM (GPT-4, Claude-3, and Gemini 1.5) is prompted to generate its own summaries. We proof-read these summaries to ensure that they do not contain any factual errors. For each decision making instance, we use the executive summary, the summaries relevant to the fruits in consideration, and their current-year price and yield statistics as context. We provide these summaries in our supplementary material, and refer readers to the prompt for summarizing the report in Appendix C.2, and report concrete summaries and statistics in supplementary materials.</p>
<h2>C. 2 Prompt Used by Agriculture</h2>
<p>Summary Prompt In Figure 8 we present the prompt used for summarizing the context in our Agriculture experiments. This prompt is shared across all models tested.</p>
<p>Zero-Shot Prompt In Figure 9 we present an example prompt for zero-shot experiments consisting of $\mathcal{P}=(\mathcal{G}, \mathcal{C}, \mathcal{A})$, with GPT-4 summaries generated from Figure 8 as context. We fix the prompt format for all LLMs and select the corresponding summaries for each LLM.</p>
<p>Self-Consistency Prompt We adopt the same zero-shot prompt as the one shown in Figure 9, but take a majority vote from $K$ reasoning paths as the final prediction. Our preliminary study finds that LLMs tend to be very confident in their decisions, and even with much higher temperature than 0.5 , often all $K$ independent runs yield consistent decisions. We thus set $K=5$ to balance cost and performance.</p>
<p>Chain-of-Thought Prompt We design a multi-prompt CoT procedure that closely emulates our DeLLMa agent, consisting of three steps: (1) ask for unknown factors that impact the decision; (2) given these, ask for their possibility of occurence; (3) then ask for a final decision. This procedure differs from DeLLMa agents as it does not use an external program for utility elicitation and expected utility maximization, but delegates each step of the process to an LLM. Example prompt can be found in Figure 10.</p>
<p>DeLLMa Prompt Similar to the CoT Prompt, DeLLMa is a multi-prompt procedure (1) ask for unknown factors that impact the decision; (2) given these, ask for their possibility of occurence (i.e. a belief distribution). Instead of directly deciding on an action, DeLLMa samples state-action pairs (via an external program) from this belief distribution, and leverage an LLM to elicit a utility function $U: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ from pairwise comparisons. From here, we use another external program to search for an action that maximizes the expected utility.</p>
<p>In Figs. 11 and 14 we present the prompt for state enumeration and state forecasting (i.e. step (1) and (2)) for the Agriculture dataset. Our implementation combines these two modules, but in the future they should be separate as our framework continues to progress. For example, state enumeration may leverage retrieval-augmented generation [25] for generating high-quality unknown factors, and state forecasting may resort to tool usage to delibrate calibrated belief distributions from quanitative analysis.</p>
<p>In Figure 12, we provide a DeLLMa prompt to generate ranked comparisons from sampled state-action pairs. These state action pairs are sampled from the belief distribution via an external program, written in Python. We provide all our implementations in the supplementary material.</p>
<h1>C. 3 Agriculture Dataset Responses</h1>
<p>Summaries See Figure 14 for formatted GPT-4 summaries from the USDA agricultural report and statistics. Claude-3 and Gemini summaries follow the exact same format, but differ in LLM generated contents.</p>
<p>Zero-Shot Responses See Figures 13, 17 and 18 for sample GPT-4 responses to zero-shot prompts.
Self-Consistency Responses SC responses follow the exact same format as those in Figures 13, 17 and 18.
Chain-of-Thought Responses See Figure 15 for a curtailed version of GPT-4 CoT response.
DeLLMa Responses See Figure 16 for a curtailed version of GPT-4 DeLLMa response.</p>
<h2>C. 4 Failure Cases on the Agriculture Dataset</h2>
<p>In $\S 4.1$, we postulate that a failure mode of our baseline approaches is that they lack the ability to perform probablistic reasoning, but are rather following the sentiment presented in context. Here, we qualitatively test this hypothesis, by showcasing prompts and responses from our Zero-Shot experiments. We present instances in Figures 13, 17 and 18 that are not able to make optimal prediction with baseline methods, but are solved by DeLLMa agents.</p>
<p>Within each figure, we first present $\mathcal{P}$, followed by the (incorrect) decision and explanation generated from the Zero-Shot baseline, and then present the top-ranked state action pair generated by DeLLMa along with its explanation.</p>
<p>In the case of deciding between apple and grapefruit in Figure 13, GPT-4 presumes that a high price for grapefruit is sustainable due to the presence of extreme weather conditions in the previous year. With CoT, the model reasons with some counterfactual scenarios, such as more suitable weather for the forthcoming year, but not in a systematic way. With DeLLMa, we provide these a list of counterfactual scenarios as context, and leverage the model's ability for situational thinking and ranking to elicit an informative preference. We make similar observations for Figures 17 and 18.</p>
<h1>D Additional Details for Stocks</h1>
<h2>D. 1 Prompt Used by Stocks</h2>
<p>Zero-Shot Prompt Similar to the agriculture setup, we first present a sample zero-shot prompt in Figure 19. The zero-shot prompt consists of three main parts (see Figure 1 for visual illustrations): (1) an enumeration of action spaces (here AMD or GME), (2) a provided context (here, historical stock prices between December 2021 to November 2023), and (3) the goal of the user (choose only one stock to maximize their profit via investing in stocks).</p>
<p>Self-Consistency Prompt SC prompts are identical to zero-shot prompts.
Chain-of-Thought Prompt Similarly, we examplify a prompt for CoT. Notably, we break the chain into three parts in Figure 20: (1) Enumerating unknown factors, (2) Enumerating $\pi^{\mathrm{LLM}}(\cdot \mid \mathcal{C})$, and (3) making the final decision given the previous responses and context.</p>
<p>DeLLMa Prompt Finally, we showcase the DeLLMa ranking prompts where the LLM is asked to provide comprehensive rankings of given state-action pairs.</p>
<h2>D. 2 Stock Dataset Responses</h2>
<p>Zero-Shot Responses See Figure 22 for sample GPT-4 responses to zero-shot prompts.
Self-Consistency Responses SC responses follow the exact same format as those in Figure 22.
Chain-of-Thought Responses See Figure 23 for a curtailed version of GPT-4 CoT response.
DeLLMa Responses See Figure 24 for a curtailed version of GPT-4 DeLLMa response.</p>
<h2>E Additional Details for Ablation Studies</h2>
<ul>
<li>Uniform: we maintain the same set of states, but replace the forecast distribution $\pi^{\mathrm{LLM}}$ with a uniform distribution.</li>
<li>Underspecified: we reduce the diversity of our set of states by removing a latent factor $f_{k}$, chosen at random, and then perform state forecasting as usual.</li>
<li>Overspecified: in addition to the original set of latent factors, we add a small set of additional factors, which are intentionally unrepresentative or redundant. This represents a case where the latent factors contain extraneous information, which may confuse the LLM backbones.</li>
</ul>
<h1>F Details on Annotation Pipeline</h1>
<p>For each pair of state-action values, we ask human evaluators (paper authors) to annotate which state-action tuple is more preferred to the other, and compare our annotations against preferences elicited by LLMs. To reduce bias in this procedure, we present these pairs in shuffled orders, i.e. it is not always true that $(s, a)<em 2="2">{1}&gt;(s, a)</em>$ if $\left{(s, a)<em 2="2">{1},(s, a)</em>\right}$ is presented to the annotators. For each pair, we ask the annotators to either label them as 1 (state-action tuple 1 is preferred), 2 (state-action tuple 2 is preferred), or 0 (uncertain). We then evaluate annotator-LLM agreement after evaluation is concluded.</p>
<p>For inter-annotator agreement, we present to two annotators a fixed list of state-action pairs (also in shuffled order), while holding out LLM preferences. We then evaluate annotator agreement with exact match.</p>
<h2>G License for Existing Assets</h2>
<p>Large Language Models DeLLMa is built on top of frontier class LLMs such as GPT-4 [1] ${ }^{6}$, Claude-3 [2] ${ }^{7}$, and Gemini $1.5\left[39]^{8}\right.$. We are not aware of their licenses, but ascertain that we abide to their respective terms of service (ToS) for the usage of these models.</p>
<p>Agriculture Dataset The Agriculture dataset is curated from statistics and reports published by the U.S. Department of Agriculture. These assets are freely accessible under the Freedom of Information Act. We ascertain that we abide to the USDA ToS ${ }^{9}$.</p>
<p>Stock Dataset The Stock dataset is curated with the Yahoo Finance API ${ }^{10}$. We ascertain that we abide to the Yahoo Developer API ToS ${ }^{11}$.</p>
<h2>H Societal Impact and Limitations</h2>
<p>Rational decision making under uncertainty has been extensively studied across many disciplines, but remains elusive even for humans. Our work bears significant societal consequences as we live in a world where humans increasingly rely on intelligent assistants for diverse problems. Unfortunately, existing foundation models and LLMs are not capable of acting optimally under uncertainties yet, which may induce harm if the general public delegate these models for decision making. These risks are exacerbated when intelligent assistants operate as a black box without adequate transparency. DeLLMa serves as a first step towards an approach that builds trust for humans to rely on these systems to make important decisions under uncertainty.</p>
<p>While our approach sensibly improves the performance of LLMs for decision making under uncertainty, deploying a prototype system in the wild without additional guardrail could incur significant financial losses, in case of a suboptimal decision. We expect users to conduct extensive field tests for the feasibility of such systems, or leverage a hybrid approach that integrates analyst judgements for optimal decision making.</p>
<p>Finally, a limitation of our approach stems from constrained state and action spaces. Our framework currently operates on a small number of discrete state and action spaces, due to the context window size of state-of-the-art LLMs. This drawback may potentially limit the applicability of our system in more complex use cases, or scenarios that require sequential decision making.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Example Summarization Prompt for Agriculture</h1>
<p>Below is an agriculture report published by the USDA:
<USDA Fruits \& Nuts Report, omitted for brevity> ${ }^{\circ}$
Please write a detailed summary of the report. You should format your response as a JSON object. The JSON object should contain the following keys:</p>
<ul>
<li>'summary': a string that summarize, in detail, the overview of the report. Your summary should include price, yield, production, and other information relevant to a farmer making decisions about what to plant. You should also include key factors, such as weather, supply chain, and demand, that affect the market.</li>
<li>'apple': a string that describes, in detail, information pertaining to apple in the report. You should include information on apple prices and production, as well as factors that affect them.</li>
<li>'avocado': a string that describes, in detail, information pertaining to avocado in the report. You should include information on avocado prices and production, as well as factors that affect them.</li>
<li>'grape': a string that describes, in detail, information pertaining to grape in the report. You should include information on grape prices and production, as well as factors that affect them.</li>
<li>'grapefruit': a string that describes, in detail, information pertaining to grapefruit in the report. You should include information on grapefruit prices and production, as well as factors that affect them.</li>
<li>'lemon': a string that describes, in detail, information pertaining to lemon in the report. You should include information on lemon prices and production, as well as factors that affect them.</li>
<li>'peach': a string that describes, in detail, information pertaining to peach in the report. You should include information on peach prices and production, as well as factors that affect them.</li>
<li>'pear': a string that describes, in detail, information pertaining to pear in the report. You should include information on pear prices and production, as well as factors that affect them.</li>
<li>'factors': a list of strings that enumerates the factors that affect the market, based on the report. You should include at least 5 factors, ranked in decreasing order of importance.</li>
</ul>
<p>Figure 8: Example Summarization Prompt for Agriculture</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\circ}$ https://www.ers.usda.gov/publications/pub-details/?pubid=107539&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
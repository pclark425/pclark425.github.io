<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-255 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-255</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-255</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-272911333</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.17391v2.pdf" target="_blank">Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia</a></p>
                <p><strong>Paper Abstract:</strong> Though Large Language Models (LLMs) have shown remarkable abilities in mathematics reasoning, they are still struggling with performing numeric operations accurately, such as addition and multiplication. Numbers can be tokenized into tokens in various ways by different LLMs and affect the numeric operations performance. Currently, there are two representatives: 1) Tokenize into $1$-digit, and 2) Tokenize into $1\sim 3$ digit. The difference is roughly equivalent to using different numeral systems (namely base $10$ or base $10^{3}$). In light of this, we study the scaling behavior of different numeral systems in the context of transformer-based large language models. We empirically show that a base $10$ system is consistently more data-efficient than a base $10^{2}$ or $10^{3}$ system across training data scale, model sizes under from-scratch training settings, while different number systems have very similar fine-tuning performances. We attribute this to higher token frequencies of a base $10$ system. Additionally, we reveal extrapolation behavior patterns on addition and multiplication. We identify that base $100$ and base $1000$ systems struggle on token-level discernment and token-level operations. We also sheds light on the mechanism learnt by the models.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e255.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e255.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-Addition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia suite — Addition experiments (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct-addition experiments using decoder-only Pythia models (70M–12B) trained or fine-tuned on synthetic addition data tokenized into different numeral systems (base-10, base-100, base-1000); evaluated with exact-match, relative error, and normalized edit similarity across data scales (2^13–2^19) and extrapolation lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70M, 410M, 1.4B, 6.9B, 12B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition (direct output of a + b = )</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Training operands la, lb in digit-length ∈ [1,10] (numbers < 10^11); extrapolation tests include operands up to 10^16−1 (longer lengths beyond training)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>synthetic supervised training; two interventions compared: from-scratch random initialization training vs fine-tuning pre-trained Pythia; direct generation (no scratchpad); numeral systems tested: base-10 (single-digit tokens), base-100 (2-digit tokens), base-1000 (3-digit tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Across from-scratch trainings, base-10 models achieve substantially better data efficiency (lower relative error, higher normalized edit similarity, higher exact-match) than base-100 or base-1000 for addition at the same data scales; fine-tuning reduces but does not eliminate differences (base-10 on-par or better). Exact-match for in-domain addition can saturate (approaches 100% in easy regimes); extrapolation to longer lengths yields overall exact-match ≈ 0.0 on held-out very-long cases, but fine-tuned models show partial successful behaviors (truncated-addition outputs, occasional correct carries).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Evidence that models learn token-level addition operations and use positional alignment: (1) base-10 advantage attributed to much higher per-token frequencies during training (more frequent learning signal per token id); (2) fine-tuned models align tokens and positional information better, enabling 'truncated addition' behavior and occasional generation of carries at unseen positions; (3) multi-digit tokenizations (base-100/1000) suffer token-level discernment and token-level operation learning because larger vocabulary reduces token frequency and increases state space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Base-10 advantage holds across model sizes (70M→12B) and data scales; addition shows saturation behavior with large training data (performance plateaus), and differences among numeral systems persist rather than vanish as model size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Extrapolation failures include truncated addition (model ignores tokens beyond max seen length), misalignment of tokens leading to spurious additions across multiple positions (common in from-scratch models), inconsistent carry generation (carry sometimes missing or wrong), and overall exact-match collapse on out-of-distribution lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared numeral systems (base-10 vs base-100 vs base-1000), compared from-scratch training vs fine-tuning, compared multiple Pythia model sizes, and varied training data scales (2^13 to 2^19).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>For direct addition, single-digit tokenization (base-10) is consistently more data-efficient than multi-digit tokenizations when training from-scratch, fine-tuning reduces but does not remove this advantage, and models tend to learn token-level addition with positional alignment but fail to generalize length beyond training (truncated addition and intermittent carry extrapolation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e255.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e255.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-Multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia suite — Multiplication experiments (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct-multiplication experiments using the same Pythia decoder-only models and numeral-system conditions as the addition experiments; multiplication is harder and reveals stronger differences between numeral systems and training regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70M, 410M, 1.4B, 6.9B, 12B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multiplication (direct output of a × b = )</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Training operands la, lb in digit-length ∈ [1,10] (numbers < 10^11); evaluated in-domain and on extrapolation lengths up to 10^16−1</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>synthetic supervised training; from-scratch vs fine-tuning; numeral systems: base-10, base-100, base-1000; direct generation (no scratchpad).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy (exact-match) is very low for multiplication across settings (often near zero for many operand-length pairs). Normalized edit similarity and relative-error metrics show base-10 models outperform base-100/base-1000, and the base-10 advantage becomes more pronounced as training data scale increases. Fine-tuned models occasionally show short successful extrapolations; from-scratch models' extrapolation exact-match is consistently zero. Models often produce correct leading and trailing tokens of the product but produce gibberish/repetitive tokens in the middle for long/extrapolation cases.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Models appear to learn coarse magnitude estimation and some token-level multiplication patterns (start/end tokens), but fail to robustly learn full multi-token multiplication algorithms; multi-digit tokenization exacerbates token-level operation learning difficulty due to sparser token frequency. Observed behaviors suggest partial decomposition into token-level operations plus learned heuristics (e.g., magnitude estimation) rather than a reliable algorithmic multiplication implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance does not reliably improve dramatically with increasing model size within tested range; however, base-10 advantage widens with larger training data (data-scaling amplifies base-10 superiority for multiplication).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Exact-match collapse for many operand length pairs; when extrapolating, outputs often have correct prefix and suffix tokens but gibberish in middle; from-scratch models fail to extrapolate operand length; generally unable to perform token-level carries/propagations consistently for multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Same comparisons: numeral systems (base-10, base-100, base-1000), from-scratch vs fine-tune, multiple model sizes, different data scales.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Multiplication is substantially harder than addition; base-10 tokenization yields better data efficiency and generalization tendencies, but overall multiplication accuracy remains low and models often only partially approximate results (prefix/suffix correctness, magnitude estimation) rather than reliably computing full products.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e255.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e255.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumeralSystemEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of numeral/tokenization system (base-10 vs base-100 vs base-1000) on arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic comparison showing that tokenization granularity (single-digit vs 2- or 3-digit tokens) affects data efficiency, learning of token-level operations, extrapolation behavior, and failure modes in transformer arithmetic experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia (used as evaluation platform)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70M, 410M, 1.4B, 6.9B, 12B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition and multiplication (direct calculation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Training used numbers convertible to the three numeral systems; training numbers < 10^11 in base-10; experiments include extrapolation to much larger numbers (up to 10^16−1).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Synthetic generation of token ids for three numeral systems (10, 10^2, 10^3 bases); supervised training either from-scratch or fine-tuned from pre-trained Pythia; evaluation on in-domain interpolation and out-of-domain length extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Base-10 (single-digit tokenization) is consistently more data-efficient than base-100 and base-1000 when training from-scratch (lower relative error / higher normalized edit similarity). Fine-tuning reduces the gap but base-10 remains at least competitive. Larger base (100/1000) leads to sparser token frequency distributions and worse sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Primary mechanistic hypothesis: higher token occurrence frequency in base-10 yields stronger learning signals for token-level arithmetic operations; multi-digit tokenizations enlarge the token/state space causing poorer token-level discernment and harder-to-learn token-level operations. Observed algorithmic patterns: truncated addition (models ignore unseen trailing tokens), extrapolated base-10 carry generation (models sometimes generate carry at unseen positions), tokens generalize but sequence length does not.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>The advantage of base-10 persists across tested model sizes and data scales; for addition there is evidence of saturation with very large data, while for multiplication the base-10 advantage increases with more data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Base-100/1000 models struggle with token-level discernment (less reliable mapping from token id to digit substrings), token-level operation errors (bad multi-digit carry/propagation), increased misalignment on unseen lengths, and overall poorer extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct comparisons across numeral systems (10, 10^2, 10^3), across training regimes (from-scratch vs fine-tune), across model sizes and data scales.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Tokenization granularity strongly affects arithmetic learning: single-digit tokenization (base-10) yields higher data efficiency and more robust token-level arithmetic behavior because of greater per-token frequency; multi-digit tokenizations trade inference length/cost for harder token-level learning and worse extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>Transformers can do arithmetic with the right embeddings <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>The impact of positional encoding on length generalization in transformers <em>(Rating: 2)</em></li>
                <li>What algorithms can transformers learn? a study in length generalization. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 1)</em></li>
                <li>Pythia: A suite for analyzing large language models across training and scaling <em>(Rating: 1)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-255",
    "paper_id": "paper-272911333",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Pythia-Addition",
            "name_full": "Pythia suite — Addition experiments (this paper)",
            "brief_description": "Direct-addition experiments using decoder-only Pythia models (70M–12B) trained or fine-tuned on synthetic addition data tokenized into different numeral systems (base-10, base-100, base-1000); evaluated with exact-match, relative error, and normalized edit similarity across data scales (2^13–2^19) and extrapolation lengths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pythia",
            "model_size": "70M, 410M, 1.4B, 6.9B, 12B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition (direct output of a + b = )",
            "number_range_or_complexity": "Training operands la, lb in digit-length ∈ [1,10] (numbers &lt; 10^11); extrapolation tests include operands up to 10^16−1 (longer lengths beyond training)",
            "method_or_intervention": "synthetic supervised training; two interventions compared: from-scratch random initialization training vs fine-tuning pre-trained Pythia; direct generation (no scratchpad); numeral systems tested: base-10 (single-digit tokens), base-100 (2-digit tokens), base-1000 (3-digit tokens)",
            "performance_result": "Across from-scratch trainings, base-10 models achieve substantially better data efficiency (lower relative error, higher normalized edit similarity, higher exact-match) than base-100 or base-1000 for addition at the same data scales; fine-tuning reduces but does not eliminate differences (base-10 on-par or better). Exact-match for in-domain addition can saturate (approaches 100% in easy regimes); extrapolation to longer lengths yields overall exact-match ≈ 0.0 on held-out very-long cases, but fine-tuned models show partial successful behaviors (truncated-addition outputs, occasional correct carries).",
            "mechanistic_insight": "Evidence that models learn token-level addition operations and use positional alignment: (1) base-10 advantage attributed to much higher per-token frequencies during training (more frequent learning signal per token id); (2) fine-tuned models align tokens and positional information better, enabling 'truncated addition' behavior and occasional generation of carries at unseen positions; (3) multi-digit tokenizations (base-100/1000) suffer token-level discernment and token-level operation learning because larger vocabulary reduces token frequency and increases state space.",
            "performance_scaling": "Base-10 advantage holds across model sizes (70M→12B) and data scales; addition shows saturation behavior with large training data (performance plateaus), and differences among numeral systems persist rather than vanish as model size increases.",
            "failure_modes": "Extrapolation failures include truncated addition (model ignores tokens beyond max seen length), misalignment of tokens leading to spurious additions across multiple positions (common in from-scratch models), inconsistent carry generation (carry sometimes missing or wrong), and overall exact-match collapse on out-of-distribution lengths.",
            "comparison_baseline": "Compared numeral systems (base-10 vs base-100 vs base-1000), compared from-scratch training vs fine-tuning, compared multiple Pythia model sizes, and varied training data scales (2^13 to 2^19).",
            "key_finding": "For direct addition, single-digit tokenization (base-10) is consistently more data-efficient than multi-digit tokenizations when training from-scratch, fine-tuning reduces but does not remove this advantage, and models tend to learn token-level addition with positional alignment but fail to generalize length beyond training (truncated addition and intermittent carry extrapolation).",
            "uuid": "e255.0",
            "source_info": {
                "paper_title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Pythia-Multiplication",
            "name_full": "Pythia suite — Multiplication experiments (this paper)",
            "brief_description": "Direct-multiplication experiments using the same Pythia decoder-only models and numeral-system conditions as the addition experiments; multiplication is harder and reveals stronger differences between numeral systems and training regimes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pythia",
            "model_size": "70M, 410M, 1.4B, 6.9B, 12B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "multiplication (direct output of a × b = )",
            "number_range_or_complexity": "Training operands la, lb in digit-length ∈ [1,10] (numbers &lt; 10^11); evaluated in-domain and on extrapolation lengths up to 10^16−1",
            "method_or_intervention": "synthetic supervised training; from-scratch vs fine-tuning; numeral systems: base-10, base-100, base-1000; direct generation (no scratchpad).",
            "performance_result": "Overall accuracy (exact-match) is very low for multiplication across settings (often near zero for many operand-length pairs). Normalized edit similarity and relative-error metrics show base-10 models outperform base-100/base-1000, and the base-10 advantage becomes more pronounced as training data scale increases. Fine-tuned models occasionally show short successful extrapolations; from-scratch models' extrapolation exact-match is consistently zero. Models often produce correct leading and trailing tokens of the product but produce gibberish/repetitive tokens in the middle for long/extrapolation cases.",
            "mechanistic_insight": "Models appear to learn coarse magnitude estimation and some token-level multiplication patterns (start/end tokens), but fail to robustly learn full multi-token multiplication algorithms; multi-digit tokenization exacerbates token-level operation learning difficulty due to sparser token frequency. Observed behaviors suggest partial decomposition into token-level operations plus learned heuristics (e.g., magnitude estimation) rather than a reliable algorithmic multiplication implementation.",
            "performance_scaling": "Performance does not reliably improve dramatically with increasing model size within tested range; however, base-10 advantage widens with larger training data (data-scaling amplifies base-10 superiority for multiplication).",
            "failure_modes": "Exact-match collapse for many operand length pairs; when extrapolating, outputs often have correct prefix and suffix tokens but gibberish in middle; from-scratch models fail to extrapolate operand length; generally unable to perform token-level carries/propagations consistently for multiplication.",
            "comparison_baseline": "Same comparisons: numeral systems (base-10, base-100, base-1000), from-scratch vs fine-tune, multiple model sizes, different data scales.",
            "key_finding": "Multiplication is substantially harder than addition; base-10 tokenization yields better data efficiency and generalization tendencies, but overall multiplication accuracy remains low and models often only partially approximate results (prefix/suffix correctness, magnitude estimation) rather than reliably computing full products.",
            "uuid": "e255.1",
            "source_info": {
                "paper_title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "NumeralSystemEffect",
            "name_full": "Effect of numeral/tokenization system (base-10 vs base-100 vs base-1000) on arithmetic",
            "brief_description": "Systematic comparison showing that tokenization granularity (single-digit vs 2- or 3-digit tokens) affects data efficiency, learning of token-level operations, extrapolation behavior, and failure modes in transformer arithmetic experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pythia (used as evaluation platform)",
            "model_size": "70M, 410M, 1.4B, 6.9B, 12B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition and multiplication (direct calculation)",
            "number_range_or_complexity": "Training used numbers convertible to the three numeral systems; training numbers &lt; 10^11 in base-10; experiments include extrapolation to much larger numbers (up to 10^16−1).",
            "method_or_intervention": "Synthetic generation of token ids for three numeral systems (10, 10^2, 10^3 bases); supervised training either from-scratch or fine-tuned from pre-trained Pythia; evaluation on in-domain interpolation and out-of-domain length extrapolation.",
            "performance_result": "Base-10 (single-digit tokenization) is consistently more data-efficient than base-100 and base-1000 when training from-scratch (lower relative error / higher normalized edit similarity). Fine-tuning reduces the gap but base-10 remains at least competitive. Larger base (100/1000) leads to sparser token frequency distributions and worse sample efficiency.",
            "mechanistic_insight": "Primary mechanistic hypothesis: higher token occurrence frequency in base-10 yields stronger learning signals for token-level arithmetic operations; multi-digit tokenizations enlarge the token/state space causing poorer token-level discernment and harder-to-learn token-level operations. Observed algorithmic patterns: truncated addition (models ignore unseen trailing tokens), extrapolated base-10 carry generation (models sometimes generate carry at unseen positions), tokens generalize but sequence length does not.",
            "performance_scaling": "The advantage of base-10 persists across tested model sizes and data scales; for addition there is evidence of saturation with very large data, while for multiplication the base-10 advantage increases with more data.",
            "failure_modes": "Base-100/1000 models struggle with token-level discernment (less reliable mapping from token id to digit substrings), token-level operation errors (bad multi-digit carry/propagation), increased misalignment on unseen lengths, and overall poorer extrapolation.",
            "comparison_baseline": "Direct comparisons across numeral systems (10, 10^2, 10^3), across training regimes (from-scratch vs fine-tune), across model sizes and data scales.",
            "key_finding": "Tokenization granularity strongly affects arithmetic learning: single-digit tokenization (base-10) yields higher data efficiency and more robust token-level arithmetic behavior because of greater per-token frequency; multi-digit tokenizations trade inference length/cost for harder token-level learning and worse extrapolation.",
            "uuid": "e255.2",
            "source_info": {
                "paper_title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2,
            "sanitized_title": "teaching_arithmetic_to_small_transformers"
        },
        {
            "paper_title": "Transformers can do arithmetic with the right embeddings",
            "rating": 2,
            "sanitized_title": "transformers_can_do_arithmetic_with_the_right_embeddings"
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2,
            "sanitized_title": "investigating_the_limitations_of_transformers_with_simple_arithmetic_tasks"
        },
        {
            "paper_title": "The impact of positional encoding on length generalization in transformers",
            "rating": 2,
            "sanitized_title": "the_impact_of_positional_encoding_on_length_generalization_in_transformers"
        },
        {
            "paper_title": "What algorithms can transformers learn? a study in length generalization.",
            "rating": 2,
            "sanitized_title": "what_algorithms_can_transformers_learn_a_study_in_length_generalization"
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 1,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Pythia: A suite for analyzing large language models across training and scaling",
            "rating": 1,
            "sanitized_title": "pythia_a_suite_for_analyzing_large_language_models_across_training_and_scaling"
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.011847249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia
27 Sep 2024</p>
<p>Zhejian Zhou zhejianz@usc.edu 
University of Southern California ♠ Shanghai AI Laboratory
The Chinese University of Hong Kong</p>
<p>Jiayu Wang wangjiayu@pjlab.org.cn 
University of Southern California ♠ Shanghai AI Laboratory
The Chinese University of Hong Kong</p>
<p>Dahua Lin dhlin@ie.cuhk.edu.hk 
University of Southern California ♠ Shanghai AI Laboratory
The Chinese University of Hong Kong</p>
<p>Kai Chen chenkai@pjlab.org.cn 
University of Southern California ♠ Shanghai AI Laboratory
The Chinese University of Hong Kong</p>
<p>Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia
27 Sep 2024A4F206142EDB918995B483B5D763AAEFarXiv:2409.17391v2[cs.CL]
Though Large Language Models (LLMs) have shown remarkable abilities in mathematics reasoning, they are still struggling with performing numeric operations accurately, such as addition and multiplication.Numbers can be tokenized into tokens in various ways by different LLMs and affect the numeric operations performance.Currently, there are two representatives: 1) Tokenize into 1-digit, and 2) Tokenize into 1 ∼ 3 digit.The difference is roughly equivalent to using different numeral systems (namely base 10 or base 10 3 ).In light of this, we study the scaling behavior of different numeral systems in the context of transformer-based large language models.We empirically show that a base 10 system is consistently more data-efficient than a base 10 2 or 10 3 system across training data scale, model sizes under from-scratch training settings, while different number systems have very similar fine-tuning performances.We attribute this to higher token frequencies of a base 10 system.Additionally, we reveal extrapolation behavior patterns on addition and multiplication.We identify that base 100 and base 1000 systems struggle on token-level discernment and token-level operations.We also sheds light on the mechanism learnt by the models.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have stormed the world with their amazing reasoning abilities (Ope-nAI, 2023;Google, 2023;Touvron et al., 2023b).However, numeric operations remain challenging for LLMs to comprehend under the architecture of Transformer (Vaswani et al., 2017;Lee et al., 2023;Yuan et al., 2023;Zhou et al., 2024;McLeish et al., 2024).Several techniques have been proposed to improve the performance of numeric operations including improving positional embeddings (Kazemnejad et al., 2024;McLeish et al., 2024) and using scratchpad (Nye et al., 2021;Liu and Low, 2023).These works mostly focus on a random initialized Transformer with 1-digit tokenization.However, pre-trained LLMs have various tokenizers that can affect the numeric operations performances.Currently, there are two main tokenization schemes: 1) Tokenize into 1-digit (Touvron et al., 2023a,b;Jiang et al., 2023;Bai et al., 2023;Team et al., 2024;Shao et al., 2024), and 2) Tokenize into 1 ∼ 3 digit (Biderman et al., 2023;OpenAI, 2023;Cai et al., 2024).An example of different tokenization 1 is shown in Table 1.Abstracting away practical details of tokenizers, these two schemes can be viewed as using a base 10 numeral system versus a base 10 3 system.The former aligns better with human intuition and the prevalent base 10 system in daily usage.Yet, the latter encodes numbers into fewer tokens.Our question follows intuitively: What is the difference between these schemes in numeric operations?</p>
<p>We resort to data-scaling efficiency to answer this question.That is, there would be substantial differences in the scaling behavior of these numeral systems.Intuitively, a base 10 system has a smaller set of tokens that could appear at each position.However, it would take up more context length to represent the numbers.Out of practical considerations, we choose to restrict our study to the base 10, base 10 2 , and base 10 3 systems which adhere to tokenizers of existing large language models.</p>
<p>To design experiments for scaling behavior, we identify the following critical dimensions: 1) numeral system 2) data scale, and 3) model size.</p>
<p>To further corroborate the generalizability of our claim, we also test if our conclusion holds for different numeric operations.On the other hand, it is possible that pre-trained models have a bias towards 2 ∼ 3 digit tokens.To strengthen our claim, we test if our observed trend holds irrespective of whether our models are trained from-scratch or fine-tuned.</p>
<p>We observe that a base 10 system is consistently more data-efficient when trained from-scratch, and Type</p>
<p>Models</p>
<p>Tokenize 31415926535</p>
<p>1-digit Llama1-2/Mistral/QWen/Gemma ['3', '1', '4', '1', '5', '9', '2', '6', '5', '3', '5'] Multiple Pythia/GPT-4o/Llama3/InternLM ['314', '159', '265', '35'] Table 1: Tokenizing 31415926535 via different large language models.that fine-tuned models perform comparably.We attribute this to higher token frequencies in base 10 training data.We believe our observation could transfer to other tasks.For example, having a smaller state/action space could be favorable in terms of data efficiency for a sequential planning task.</p>
<p>We also study the length extrapolation behaviors of different numeral systems.We identify that base 100 and base 1000 systems struggle on token-level discernment, and on learning token-level operations.We further shed light on the mechanisms learnt by models.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We reveal that A base 10 system is consistently more data-efficient than a base 102 or base 10 3 system under different data scales, model sizes, and different operators, especially training from-scratch.</p>
<p>• We showcase through extrapolation experiments that base 100 and base 1000 systems struggle on token-level discernment and on learning token-level operations.</p>
<p>• We identify several calculation patterns in the extrapolation setting.Such patterns include truncated addition and extrapolation of base-10 carry.</p>
<p>Related Work</p>
<p>Numeral System A numeral system represents a number by a sequence of tokens within pre-defined sets.In order to perform numeric operations, the model would have learned to discern between the tokens precisely.Numeral systems are closely related to tokenizers.We first review prevalent tokenization conventions.Llama1/2 (Touvron et al., 2023a,b) tokenize numbers into 1-digit, enforcing a base-10 system.This design is also adopted by other general-domain LLMs (Jiang et al., 2023;Bai et al., 2023;Team et al., 2024) and mathspecialized model Deepseek-Math (Shao et al., 2024).On the other hand, the most capable model to date, GPT-4o, tokenizes numbers into 1 ∼ 3 digit, which is roughly equivalent to using a base 10 3 system.To the best of our knowledge, no one has systematically studied how the numeral system affects the transformers' arithmetic ability.</p>
<p>Arithmetic Operations in Transformers To improve the arithmetic abilities of the transformer (Wang et al., 2021;Nogueira et al., 2021), people have designed positional embeddings (Kazemnejad et al., 2024;McLeish et al., 2024), scratchpad (Nye et al., 2021), and special training procedures (Liu and Low, 2023;Deng et al., 2023).In this paper, we do not improve the performance of arithmetic operations, but aim to understand the scaling impact of choices of numeral systems.We focus on using decoder-only transformers to generate the results of arithmetic operations directly (i.e.without a scratchpad).</p>
<p>Scaling Laws in Large Language Models Scaling laws 2 have been widely studied in the context of LLMs (Kaplan et al., 2020;Hernandez et al., 2021;Gao et al., 2023;Bi et al., 2024) which aims to predict model losses based on different data scales and model parameters.Different from this line of research, we do not aim to accurately predict performances when we scale up computing.We leverage scaling behavior as a proxy to study the impact of numeral systems selection.</p>
<p>Scaling Behavior Experiment Designs</p>
<p>To understand how the numeral systems affect numeric operation in LLMs, we identify the following dimensions of interest for our experiments when training an LLM with numerical operation: 1) numeral system 2) training data scale 3) model size 4) from-scratch or fine-tuning 5) different operations.</p>
<p>For 1) and 2), we generate synthetic inputs according to the process explained in section 3.1.For 3), we make use of the Pythia scaling suite (Biderman et al., 2023) for ranging over different model sizes.For 4), we replicate experiments for both settings to the best of our effort.For 5), we choose to include results of addition and multiplication.</p>
<p>We list the complete configurations for our experiments.1) numeral system: base 10, base 10 2 , base 103 2) training data scale: 2 13∼19 training samples 3) model size: 70M, 410M, 1.4B, 6.9B, 12B from Pythia 4) random-initialized or fine-tuned from Pythia (i.e.from-scratch or fine-tune) 5) operations 3 : addition, multiplication.After choosing a configuration, we train our model using our generated data and evaluate the model on a nonoverlapping evaluation set.The training procedure is the same as instruction-tuning a language model which masks the prompt (for example 12 + 23 =) and only calculates the losses on the outputs (35).</p>
<p>Synthetic Data Generation</p>
<p>We generate synthetic data of scales 2 13∼19 for numeral systems base 10, base 10 2 , and base 10 3 .We abstract away the nitty-gritty details involved in practical tokenization schemes and generate synthetic input ids and labels directly.</p>
<p>We first illustrate our training distribution generation process using addition as an example.Let a and b be two operands, each row would be in the form of a + b = c.Let la and lb be the digit lengths of a and b in base 10.We fix la, lb ∈ [1, 10], and we attempt to evenly distribute generated data over la × lb.If the total number of pairs for la × lb is smaller than we request, we take all possible pairs.No pairs are repeated during our generation.</p>
<p>Based on our generated training distribution, we convert each number into the corresponding base 10, base 10 2 , and base 10 3 representations.Note that this could be easily done by grouping digits in the original base 10 representation.We then map the digit numbers onto their corresponding token ids.Intuitively, base 10 would have 10 ids (corresponding to 0 ∼ 9), base 10 2 would have 100 ids, and base 10 3 would have 1000 ids.In Figure 1, we demonstrate the answer token distribution for each numeral system.</p>
<p>Importantly, token frequencies of a base 10 system are at least an order of magnitude larger than those of a base 100 or base 1000 system.We believe that higher token frequencies lead to better trained models.This is the reason to the superior performances of a base 10 model.</p>
<p>We obtain the distribution by converting all answers into ids.We normalize the token values from each numeral system by dividing against the base.As our sampling result shows, the probability density gets more imbalanced as the base gets larger.For example, tokens 0 ∼ 9 are one magnitude more likely to appear, followed by two-digit tokens, then three digit tokens.Such a phenomenon could have deeper roots in number theory.In this paper, we accept this experiment fact and continue with our exploration.</p>
<p>Evaluation Setup</p>
<p>We sample non-overlapping operand pairs for evaluation.We attempt to evenly sample 1000 pairs for each la × lb.If half of the total number of pairs is smaller than 1000, then we reserve half for evaluation.Overall, we strive to make sure that the training and evaluation sets are from the same distribution and have no overlap.</p>
<p>To observe a clear trend, we report the following metrics 1) relative error 2) exact match accuracy 3) normalized edit similarity.Of these three metrics, exact match accuracy is the most intuitive, which is a hard match between model-generated tokens and ground truth tokens.Based on our initial experiments, this metric is not informative enough for a range of settings.We thus design two more metrics to reveal the underlying dynamics of our models.</p>
<p>Relative Error To generate a metric that is meaningful to practical settings, we calculate relative error as log o g , where g is the ground truth answer and o is the model output.We then compute the mean of the magnitude difference over all evalu- ation pairs.This metric is more informative than exact match accuracy since it captures the relative error made by the model.</p>
<p>Note that this metric has two inductive biases.First, this metric gives more weight to the length differences between model outputs and ground truth answers.Even if the output has a long common sub-sequence with the ground truth, it will still be penalized for not getting the output length right.</p>
<p>Second, this metric biases towards the accuracy of leading digits.If we make a connection between the numeral system and signal processing, this is equivalent to putting more weight on low frequency component of the number (trailing digits change rapidly while leading digits change slowly).</p>
<p>Normalized Edit Similarity Since numbers in a numeral system are sequences of tokens, we introduce a generalized and Normalized Edit Similarity metric, which would give credit to partially correct answers based on string similarity.Edit Distance is a powerful metric that can capture substring similarities.We extend this metric to our scenario using the following setup:</p>
<p>Each number could be represented as a sequence of chars, with each char representing a single digit from 0 ∼ 9. We define the generalized edit distance as the minimum number of insertions, deletions, and substitutions needed to transform one sequence into another.Suppose that the two se-
quences are A = a 1 a 2 ...a n and B = b 1 b 2 ...b m .
Let ed be the edit distance between A and B. We define the normalized edit similarity as ned
= max(m,n)−ed max(m,n) . This metric is normalized into [0, 1].
Compared with the Relative Error metric, this metric connects more closely to human perception.It prioritizes answers that would have the longest sub-sequences with the ground truth.Since human perception is largely visual for numbers, this metric aligns more with the visual similarity between the answer and the ground truth.</p>
<p>Note that Relative Error can be somewhat viewed as a revised version of the Normalized Edit Similarity we used, where insert and delete operations are penalized harder, and replace operation is reweighted by the magnitude of the difference.</p>
<p>Experiments and Results</p>
<p>In this section, we present the main results of our experiments that demonstrate the scaling efficiency of a base 10 system.4</p>
<p>Overall Trends</p>
<p>For each scenario, our main metrics of interest are Relative Error and Normalized Edit Similarity.For the addition operation, we also report Exact Match Accuracy.However, for multiplication, exact match accuracy is too low such that no information could be gained.</p>
<p>Overall, a base 10 system is consistently more data-efficient than a base 10 2 or a base 10 3 system when trained from scratch, as shown in Figure 2 and Figure 4.That is, to obtain a certain performance, a base 10 system would need data of a smaller scale to achieve it.</p>
<p>We highlight the fact that fine-tuning scenarios do not instead favor base 100 and base 1000.During pre-training, most tokenizers lean towards combining consecutive digits, which would have favored base 10 2 and base 10 3 over base 10.Considering this, the decent performances of base 10 fine-tuned models further corroborate the superiority of the base 10 system.</p>
<p>It is worth noting that the differences in data efficiency do not diminish just as we scale up model size.We do observe a saturation trend for addition when we put in more training data.However, for multiplication, the superiority of a base 10 system gets more pronounced as data scales up.</p>
<p>In-domain Interpolation Evaluation</p>
<p>First, we test whether our trained models could interpolate between the points identically sampled from the same training distribution (i.e.whether our models could generalize in-domain).</p>
<p>Addition</p>
<p>In Figure 2, we showcase the scaling behavior for addition.We first focus on from-scratch scenario.We can observe a clear trend that base 10 is consistently better than base 10 2 and base 10 3 for both metrics, which is a strong affirmation of our claim.A base 10 system is at least of a constant magnitude more data efficient than base 10 2 and base 10 3 systems, and this trend does not diminish as the model size gets larger.</p>
<p>For fine-tuning experiments, the difference between numeral systems is less profound.Pythia is pre-trained on tokenization with base 10 3 , which weakens the advantages of base 10.A base 10 system is at least on par with base 10 2 or base 10 3 , as we do not observe an exaggerated performance difference as we scale up data.</p>
<p>Multiplication</p>
<p>We plot the Normalized Edit Similarity for multiplication in Figure 4. We can also conclude that the base 10 number system is consistently more dataefficient than base 10 2 and base 10 3 .The trend is consistent for both from-scratch and supervised fine-tuning settings.</p>
<p>The superiority of a base 10 system is more pronounced and more consistent under the multiplication setting.First, for Relative Error of models trained from scratch, the advantage of a base 10 system is more perceivable than the addition setting.For the Normalized Edit Similarity metric, we observe a trend where the data efficiency of a base 10 system gains more advantage at large data scales.We relate this phenomenon to the differences between Figure 7 and Figure 6 in the Appendix.As shown, addition is a much simpler task when compared with multiplication.For a large range of operand length pairs, the exact match accuracy remains zero.Our hypothesis is that the sample efficiency of a base 10 system against base 100 and base 1000 systems is magnified by the difficulty of the task.</p>
<p>Out-of-domain Extrapolation Evaluation</p>
<p>We have tested whether our models could generalize in-domain.An equally important question is whether our models could extrapolate to unseen data points, especially in terms of length.</p>
<p>During training distribution generation, we only consider numbers that are less than 10 11 .Therefore, we generate cases where one operand lies in the range of 10 11 ∼ 10 16 − 1, and the other operand ranges from 1 ∼ 10 16 − 1.To this end, we perform 12 sets of case study experiments.Here we list the complete configurations: 1) numeral system: base 10, 10 2 , 10 3 2) data scale: 2 19 3) model size: 6.9B 4) if from-scratch: True, False 5) operations: addition, multiplication.We discover intriguing extrapolation behavior that could shed light on the mechanisms that the models have learned.</p>
<p>Addition</p>
<p>Note that although addition is an easy task to train, the models have only seen numbers less than 10 11 .We leverage addition pairs of length la and lb, where at least one of la or lb is greater than 10.To illustrate how the performance of our model decays, we plot the Relative Error matrices where one operand length is in [1,5] and the other in [11,15] in Figure 5.The results are obtained using a 1.4B fine-tuned model trained on 2 19 training samples.For each pair of la × lb, we randomly generate 100 samples, which results in a total of 5000 samples.Of all such samples, the extrapolation exact match accuracy is 0.0.</p>
<p>Yet, the models do not collapse completely on out-of-domain length distribution.We conduct case studies in Table 2. Our first discovery is that there is a consistent behavior of Truncated Addition across all numeral systems of fine-tuned models.Our second observation is that fine-tuned models are much better at aligning the tokens involved in extrapolated addition, as compared with models trained from-scratch.Truncated Addition Extrapolates While we are manually inspecting the extrapolation behavior of fine-tuned models, we discover consistently that models would try to perform the addition truncating the tokens that exceed training length it has seen.We elaborate on this behavior under two configurations.</p>
<p>For illustrative purposes, we add a comma to denote the max training length position the models have seen.First, take for example a model trained on a base 10 system, a fine-tuned model is given input a = 8318686348, 0 and b = 3, where the token representation of a is 11.Only the first 10 digits of a would participate in addition, yielding a result of 8318686348 + 3 = 8318686351.A fine-tuned model trained on the base 10 2 system displays very similar results.Given a = 734766443, 03 and b = 3, the model performs 734766643 + 3 = 734766446, ignoring two trailing digits, which is equivalent to ignoring the last token under base 10 2 .The phenomenon of truncated addition is hardly ob- served on models trained from-scratch.The main obstacle could arise from the inability to align corresponding tokens with unseen token lengths.For example, a base 10 model trained from-scratch would calculate 2635078980, 7+1 = 2635079091, where the 1 seems to have been added to multiple positions.This could also indicate that fine-tuned models have learned to utilize positional information better.</p>
<p>Base 10 Carry Extrapolates While we attempted to explain extrapolation behavior using truncated addition, we noticed some outliers where the answer is only 1 absolute value larger than the truncated addition result.Manual inspection quickly reveals that the models gener-ate carry for out-of-distribution positions.For a base 10 fine-tuned model, 3968299531, 8 + 2 = 3968299534 (= 3968299531 + 2 + 1), where a carry has been generated because 8 + 2 = 10.Note that the carry is not generated by aligning the ones digit since 1 + 2 = 3 &lt; 10, which is an ablation showcasing that calculating carry exhibits extrapolation behavior.</p>
<p>Tokens Generalize, Length Does Not For a base 10 3 system, two kinds of behavior have been observed.Before we describe the behaviors, we restate our experiment settings.Our training distribution only contains numbers that are less than 10 11 under the base 10 system.This creates two scenarios for extrapolation experiments of a model trained Table 3: Representative Cases for Multiplication Extrapolation.We list successful cases for fine-tuned models (i.e.SFT) and showcase the failure of from-scratch models.</p>
<p>1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 lb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 la 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.8 1.8 2.9 3.9 5.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.4 1.5 2.8 3.9 5.0 1.0 1.0 1.0 1.0 0.9 0.7 0.7 0.8 0.8 0.7 0.7 1.4 2.5 3.6 4.8 2.0 2.0 2.0 1.8 1.4 1.3 1.5 1.5 1.5 1.5 1.6 1.9 2.7 3.8 4.9 3.0 3.0 2.7 2.5 2.2 2.3 2.3 2.4 2.6 2.6 2.7 2.7 3.1 3.9 4.9 3.9 3.9 3.6 3.3 3.2 3.2 3.2 3.5 3.8 3.7 3.7 3.7 3.9 4.3 5.0 4.9 4.7 4.3 4.3 4.2 4.1 4.3 4.8 4.9 4.9 4.9 4.9 5.0 5.0 5.3 with base 10 3 .1) both operands are less than 10 13 2) at least one of the operands is no smaller than 10 13 .For 1), although the model has not seen any data points within the range of 10 11 ∼ 10 13 −1, the length of both operands does not exceed 4, which has been trained.For 2), the length of at least one operand has not been seen during training at all.</p>
<p>Relative Error a + b with Extrapolation</p>
<p>Out of a sample size of 100 for each la × lb pair, a base 10 3 fine-tuned model could achieve 90% exact match accuracy with la = 11, lb ∈ [1, 8].However, accuracy quickly drops to 0 if one of the operands has a token length greater than 4 under the base 10 3 system.</p>
<p>Multiplication</p>
<p>Different from addition, there is at least one successful example of extrapolation of operand length for fine-tuned models of all number systems shown in Table 3. Yet, the exact match accuracy on the extrapolation set of models tuned from-scratch is consistently zero.Moreover, a closer look at the generated results showcases that the model is only able to correctly generate the starting tokens and ending tokens of the answer, with gibberish and repetitive tokens in the middle.</p>
<p>Conclusion</p>
<p>In this paper, we study the selection of a numeral system for large language models.We compare the data-scaling efficiencies of base 10, 100, and 1000 systems.Through carefully designed experiments, we showcase the superiority of the base 10 system.</p>
<p>We offer an analysis of the extrapolation behavior of trained models on addition and multiplication.We reveal calculation patterns that successfully extrapolate, such as carry generation in addition and magnitude estimation in multiplication.Our work sheds light on tokenization designs and the mechanisms that models have learnt for arithmetic tasks.</p>
<p>Limitations</p>
<p>Scaling behavior analysis requires a huge amount of computational resources.Limited by this factor, we have not performed a thorough grid search for hyperparameters of every setting.It is possible that for every configuration that is of interest, we should use a unique set of hyperparameters to achieve optimal performance.In our experiments, we have witnessed instability issues regarding some data points where the training loss seemingly collapses.It is possible that such issues arose because of suboptimal hyperparameter choices.Research on numerical operation has few potential risks.</p>
<p>A Discussion on Multi-digit Tokenization</p>
<p>In this paper, we showcase the superior data efficiency of a base 10 system.Yet, in Table 1, popular models such as Llama3 and GPT-4o still adhere to multi-digit tokenization for numbers.We provide a discussion about this phenomenon here.</p>
<p>From our perspective, this design choice relates to user experience and cost management.During inference, using a larger numeral system reduces the total number of tokens, leading to shorter input and output lengths.This reduction boosts token throughput (due to smaller kv cache size) and increases the number of queries per second (because output is shorter), significantly improving user experience and reducing training/inference costs.</p>
<p>One plausible assumption is that for a base 100 or base 1000 system, one can devote more computer into training to trade-off for better inference experience.It is likely that the efficiency for training and inference could not be improved simultaneously.</p>
<p>B Discussion on Task Selection</p>
<p>We provide a detailed reasoning for choosing direct addition and multiplication as our tasks (or arithmetic operations) to investigate.</p>
<p>First, we strengthen that our setting targets direct calculation of addition and multiplication.For example, the model is directly prompted with 13 + 5 =, (or 13 × 5 =).The model is expected to directly output 18, (or 65).</p>
<p>We noticed that the accuracy of direct calculation upper bounds model's ability to carry out calculations in context.Under such settings, the model need to perform calculations mixing natural language and numbers.For example, the model might output "To solve the problem, we need to calculate the product of 13 × 5, which is 65".This motivates us to investigate direct calculations first.</p>
<p>Second, we address why we only presented results on addition and multiplication.In our initial small-scale experiments, we found out two things.First, for division, pre-trained models have trouble ending their outputs.Second, for subtraction, we found trends that are similar to addition.We also argue that arithmetic tasks have shared attributes with addition and multiplication: 1) For each task, a token-level operation should be learned (eg.adding single-digit numbers).2) For each task, the model would need to discern between the tokens.</p>
<p>We would like to state that our work has a potentially broader impact.Our generalized finding is that having fewer states could enhance sequential modeling.For other planning tasks, such as robotics or theorem proving, this conclusion may also hold true.</p>
<p>C Connection with Scaling Law</p>
<p>We intentionally avoid using the term Scaling Law.Generally, researchers fit a law to predict the performance of models when scaling up compute.However, for addition, we already observe saturation in Figure 2. It is hard to accurately fit a law for this curve.</p>
<p>Moreover, we are not interested in predicting performances.We are inspecting the difference in data efficiency as we scale up compute.Therefore, observing that a base 10 system is more efficient is eloquent enough for our purpose.</p>
<p>We also find that our performances do not improve significantly with model sizes.We have noticed that the 70M and 410M versions of Pythia models are particularly hard to train.We do not dive into this technical detail.</p>
<p>D Metric Matrices for Length Pairs</p>
<p>We take a 1.4B model trained from-scratch on addition and multiplication as an exemplar and plot matrices for both Exact Match Accuracy and Normalized Edit Similarity with respect to each pair of input lengths in Figure 6 and Figure 7.</p>
<p>E Overfitting Analysis</p>
<p>Alongside our main results, we also perform ablation studies on overfitting under addition settings, since the accuracy quickly saturates to 100.0%.First of all, we subsample a portion of our training set to forward through the model.We attempt to sample 1000 examples for each la × lb pair in 10 × 10.If the total number of training pairs is smaller, we take all training pairs for la × lb.</p>
<p>Generally, for all the metrics of interest, we observe nearly identical performance on our training and evaluation set.One example of accuracy on addition is shown in Figure 8. Furthermore, since our evaluation set is non-overlapping with the training set, it would be safe to conclude that no overfitting phenomenon has been observed.</p>
<p>F Hyper-parameters</p>
<p>We briefly discuss the hyperparameter search process that we have gone through for each configuration.Based on our initial experiments, models exhibit nearly identical behavior in both the training set and the evaluation set.We therefore use training set metrics for hyperparameter selection.Then, we first fix the learning rate magnitude and sweep for training epochs.We observed that fine-tuned models are insensitive for training epochs, while the model training from-scratch consistently improves with more epochs.We choose epochs where the performances of models begin to plateau.Generally, epoch performance trends only depend on the training setting (i.e.fine-tuning or from-scratch).Fixing the training epoch, we perform a grid search over learning rate magnitudes {2e − 3, 2e − 4, 2e − 5, 2e − 6, 2e − 7} for each configuration.Generally, we found that 70M and 410M models favor a larger learning rate of 2e − 4 while models larger than 1.4B use 2e − 5.There is no significant difference between fine-tuning learning rates and from-scratch learning rates.We provide our full hyperparameters in Table 4.The hyperparameters for addition is the same as multiplication, except that we used 2e − 5 for fine-tuning 410M Pythia.</p>
<p>To speed up training, we pack sequences to a maximum length of 2048, therefore fixing the batch size.All experiments are trained using 8xA100 Nvidia GPUs.</p>
<p>G Performance Differences in</p>
<p>Real-World Models</p>
<p>One reasonable question to ask is whether our finding is useful in training real-world scenario math models.We present our results in Figure 9.We test two versions of models: 1) Multi-Digit Tokenization, and 2) Single-Digit Tokenization.We test our conclusion under three scenarios: 1) directly calculating a × b, 2) solving an natural language application problem that involves solving a×b, and 3) solving a variant application problem that also involves solving a × b.We generate test data using a program and extract the answer using heuristics.We report accuracies using hard match.Two observations could be made.1) Single-Digit Tokenization consistently outperforms Multi-Digit Tokenization, and 2) the accuracy of directly calculating a × b gives a reasonable upper-bound of the model's performance when solving an application problem that involves multiplication.</p>
<p>H Results for Conventional Relative Error</p>
<p>One might argue that the Relative Error metric that we used in the paper diverges from conventional calculation.Furthermore, it has similarities with the Normalized Edit Similarity metric that we used.</p>
<p>Here, we plot the results where RE = |o−g| g in Figure 10.</p>
<p>For Addition, the advantage of a base 10 system is magnified using conventional calculations.For Multiplication, we witness significant issues of instability, hindering the discovery of underlying insights.Eval Acc for from_scratch Addition</p>
<p>Figure 1 :
1
Figure 1: Answer Token Distribution for Multiplication.We sample 2 13 addition samples to illustrate the distribution.Token values are normalized to [0, 1].</p>
<p>Figure 2 :
2
Figure2: Relative Error (lower is better) and Normalized Edit Similarity (higher is better) for addition operation with different data scales, model parameter sizes, from-scratch or fine-tune, and numeral systems.</p>
<p>Figure 3 :
3
Figure 3: Exact match accuracy for addition operation with different data scales, model parameter sizes, from-scratch or fine-tune, and numeral systems.</p>
<p>Figure 4 :
4
Figure 4: Relative Error and Normalized Edit Similarity for multiplication operation with different data scales, model parameter sizes, from-scratch or fine-tune, and numeral systems.</p>
<p>Figure 5 :
5
Figure 5: Relative Error Matrix for Extrapolation Behavior Analysis.The results are obtained using a 1.4B model fine-tuned on 2 19 training samples.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Exact Match Accuracy and Normalized Edit Similarity Matrices for Addition Eval Set.The results are obtained using a 1.4B model trained from scratch on 2 19 samples.</p>
<p>Figure 8 :
8
Figure 8: Exact Match Accuracy on the training set versus the eval set for addition operation with different models trained from scratch, on different data scale and numeral systems.</p>
<p>Figure 9 :
9
Figure9: Performances of difference tokenization schemes in three real-world scenarios: 1) directly calculating a × b, 2) solving an natural language application problem that involves solving a × b, and 3) solving a variant application problem that also involves solving a × b.</p>
<p>Table 2 :
2
Representative Cases for Addition Extrapolation.We add a comma to denote the maximum token length of a single number that the model has seen during training.
ModelPatternaba + bModel OutputSFT-Base 10w/o carry8318686348,038318686348,38318686351carry success 3968299531,823968299532,03968299534SFT-Base 10 2 w/o carry7 34 76 64 43, 0337 34 76 64 43, 067 34 76 64 46misaligned1 63 47 53 10, 8121 63 47 53 10, 831 63 47 53 12, 83carry failure7 28 37 46 59, 47947 28 37 46 60, 417 28 37 47 53SFT-Base 10 3 w/o carry2 929 747 175, 022 92 929 747 175, 031 2 929 747 184misaligned8 748 392 297, 087 28 748 392 297, 089 8 748 392 299, 089carry failure8 172 938 472, 837 494 8 172 938 473, 331 8 172 938 966Modelab a × bModel OutputSFT-Base 109298574444, 76 5579144666,825579144666,82SFT-Base 10 23 44 97 17 48, 09 8 27 59 77 39 84, 72 27 59 77 39 84, 72SFT-Base 10 318 419 335, 3844 73 677 341, 53673 677 341, 536Scratch-Base 10445275579238 356220463384358888899984Scratch-Base 10 2 2 45 57 14 10, 66 8 19 64 57 12 85, 28 20 10 88 12 12, 48Scratch-Base 10 3 17 709 751, 4955 88 548 757, 47587 229 700, 075</p>
<p>Model Version From Scratch Learning Rate Max Epochs Log10 Data Scale
pythia_70mTrue2e-4101-313-19pythia_70mFalse2e-521-313-19pythia_410mTrue2e-4101-313-19pythia_410mFalse3e-521-313-19pythia_1_4bTrue2e-5101-313-19pythia_1_4bFalse2e-521-313-19pythia_6_9bTrue2e-5101-313-19pythia_6_9bFalse2e-521-313-19pythia_12bTrue2e-5101-313-19pythia_12bFalse2e-521-313-19</p>
<p>Table 4 :
4
Hyperparameters for Multiplication</p>
<p>We provide a discussion on tokenization schemes in Appendix A.
We discuss the connections between our work and scaling laws in Appendix C.
We provide a more detailed discussion on task selection in Appendix B.
We provide full hyperparameters in Appendix F. We also provide small-scale evaluations in real-world scenarios in Appendix G.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, arXiv:2401.02954Deepseek llm: Scaling open-source language models with longtermism. 2024arXiv preprint</p>
<p>Pythia: A suite for analyzing large language models across training and scaling. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, O' Kyle, Eric Brien, Mohammad Hallahan, Shivanshu Aflah Khan, Purohit, Sai Usvsn, Edward Prashanth, Raff, International Conference on Machine Learning. PMLR2023</p>
<p>Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.17297Internlm2 technical report. 2024arXiv preprint</p>
<p>Implicit chain of thought reasoning via knowledge distillation. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber, ArXiv, abs/2311.014602023</p>
<p>Scaling laws for reward model overoptimization. Leo Gao, John Schulman, Jacob Hilton, International Conference on Machine Learning. PMLR2023</p>
<p>Gemini: A family of highly capable multimodal models. Google, arXiv:2312.118052023Preprint</p>
<p>Danny Hernandez, Jared Kaplan, Tom Henighan, Sam Mccandlish, arXiv:2102.01293Scaling laws for transfer. 2021arXiv preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Scaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.083612020Preprint</p>
<p>The impact of positional encoding on length generalization in transformers. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, Siva Reddy, Advances in Neural Information Processing Systems. 202436</p>
<p>Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, Dimitris Papailiopoulos, arXiv:2307.03381Teaching arithmetic to small transformers. 2023arXiv preprint</p>
<p>Tiedong Liu, Bryan Kian, Hsiang Low, arXiv:2305.14201Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. 2023arXiv preprint</p>
<p>Sean Mcleish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Bhavya Brian R Bartoldson, Abhinav Kailkhura, Jonas Bhatele, Avi Geiping, Schwarzschild, arXiv:2405.17399Transformers can do arithmetic with the right embeddings. 2024arXiv preprint</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin, arXiv:2102.130192021arXiv preprint</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIPreprint</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, arXiv:2402.033002024Preprint</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezPreprintand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Exploring generalization ability of pretrained language models on arithmetic and logical reasoning. Cunxiang Wang, Boyuan Zheng, Yuchen Niu, Yue Zhang, Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021. Qingdao, ChinaSpringer2021. October 13-17, 2021Proceedings, Part I 10</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, arXiv:2304.020152023Preprint</p>
<p>What algorithms can transformers learn? a study in length generalization. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, Preetum Nakkiran, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
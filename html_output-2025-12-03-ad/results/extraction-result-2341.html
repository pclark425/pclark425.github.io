<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2341 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2341</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2341</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-227131886</p>
                <p><strong>Paper Title:</strong> <a href="https://web.archive.org/web/20210717151344/https:/escholarship.org/content/qt9pm0x5mh/qt9pm0x5mh.pdf?t=qv78lu" target="_blank">Machine learning for metabolic engineering: A review.</a></p>
                <p><strong>Paper Abstract:</strong> Machine learning provides researchers a unique opportunity to make metabolic engineering more predictable. In this review, we offer an introduction to this discipline in terms that are relatable to metabolic engineers, as well as providing in-depth illustrative examples leveraging omics data and improving production. We also include practical advice for the practitioner in terms of data management, algorithm libraries, computational resources and important non-technical issues. A variety of applications ranging from pathway construction and optimization, to genetic editing optimization, cell factory testing and production scale-up are discussed. Moreover, the promising relationship between machine learning and mechanistic models is thoroughly reviewed. Finally, the future perspectives and most promising directions for this combination of disciplines are examined.</p>
                <p><strong>Cost:</strong> 0.044</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2341.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2341.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kinetic learning (Costello et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine learning approach to predict metabolic pathway dynamics from time-series multiomics data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised ML model trained on time-series protein and metabolite data to predict metabolite rate-of-change (pathway kinetics), using data augmentation and feature selection to overcome sparse experimental timepoints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A machine learning approach to predict metabolic pathway dynamics from time-series multiomics data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Metabolic pathway kinetics / metabolic engineering</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict instantaneous rates of change of metabolites (pathway dynamics) from measured protein and metabolite concentrations to enable pathway design without hand-crafted kinetic parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Limited/scarce: original experimental case used only three time series of 7 timepoints each (very small), labeled supervised data; the authors used data augmentation to expand to ~200 instances. Data are high-quality experimental multi-omic time series but costly to produce.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time series, multivariate numeric (protein concentrations, metabolite concentrations); relatively low number of instances but high dimensionality if host metabolism included; smooth/continuous temporal structure exploited for interpolation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-high: nonlinear dynamical mapping from protein/metabolite abundances to metabolite rate changes; low instance count relative to potential inputs (curse of dimensionality) required dimensionality reduction/feature selection; search/learning complexity addressed via augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging: kinetic modeling is well-established (Michaelis-Menten, mechanistic models) but limited by missing parameters; data-driven alternatives are nascent and exploratory.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium: predictive accuracy prioritized over mechanistic interpretability; mechanistic insight desirable but not required for effective predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised regression models with feature selection and data augmentation (ensemble/TPOT pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised regression framework where inputs are protein and metabolite concentrations at timepoints and outputs are metabolite rate-of-change; steps included Savitzky–Golay smoothing and interpolation to augment timepoints, feature selection/dimensionality reduction, and automated pipeline search (TPOT) over scikit-learn models to select preprocessing and model ensemble; trained on augmented time-series instances.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (regression) with automated model selection / ensemble; uses data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate for short, labeled time-series where hand-crafted kinetics are unreliable; requires assumptions (smoothness/continuity) for augmentation and careful feature selection to avoid curse of dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Outperformed a hand-crafted kinetic model in predictive power on held-out strain despite very limited raw data, enabled systematic application to any pathway with less manual curation; success depended critically on data augmentation and feature selection.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for enabling pathway dynamics prediction when kinetic parameters are unknown, reducing time spent building mechanistic models; can accelerate DBTL cycles by providing predictive screening of designs without exhaustive mechanistic parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively against classic kinetic (Michaelis–Menten) models; ML approach gave better predictive performance on the test strain and was faster to apply than manual kinetic model construction.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Data augmentation (interpolation/smoothing) to increase instance count, feature selection to reduce dimensionality, pipeline automation (TPOT) to pick suitable preprocessing and model, and careful experimental design to provide consistent high-quality time-series.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Data-driven supervised learning can recover useful kinetic predictions from very limited time-series when combined with principled augmentation and feature-selection, and can outperform handcrafted kinetic models for prediction tasks even without mechanistic representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2341.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cell-free butanol optimization (Karim et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In vitro prototyping and rapid optimization of biosynthetic enzymes for cell design (iPROBE + neural networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of neural networks to predict high-performing enzyme homolog/composition and concentration combinations for a six-step n-butanol pathway tested in a rapid cell-free prototyping system, substantially improving production metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In vitro prototyping and rapid optimization of biosynthetic enzymes for cell design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Pathway optimization / enzyme combinatorial design in metabolic engineering</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Search the combinatorial space of enzyme homologs and enzyme concentrations for a multi-step pathway to maximize combined objective (TREE score: titer, rate, enzyme expression) using limited experimental screening enabled by cell-free prototyping.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Limited but structured: initial training set of 120 pathway designs (instances) measured for TREE score; labeled supervised data; cell-free platform enables relatively rapid generation but full combinatorial space is huge (hundreds of thousands of combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular categorical + numeric: categorical features (which homolog used per step) and numeric features (enzyme concentrations); response is scalar TREE score combining multiple performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high combinatorial complexity: combinatorial explosion (e.g., example 314,928 combinations for partial choices), nonlinear interactions between enzymes and concentrations; unknown epistatic effects across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Applied/advancing: pathway optimization is established experimentally; data-driven ML-guided optimization is an emerging, practical approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: primary goal is predictive optimization (higher TRY); interpretability helpful but not required to realize improved production.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep neural networks (fully connected feedforward ensembles) with architecture search and subsequent nonlinear optimization (Nelder–Mead)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Ensembles of small fully connected neural networks (tested architectures with 5–15 hidden layers and 5–15 nodes per layer) trained on 120 labeled instances; genetic algorithms used to propose architectures; ten-fold cross-validation selected best models; trained models used in conjunction with a Nelder–Mead simplex nonlinear optimizer to propose new pathway designs maximizing the TREE score.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (deep learning) + optimization; ensemble modeling and hyperparameter/architecture search</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited to structured combinatorial design problems where inputs are actionable (homolog identities, concentrations) and labeled data can be generated; limited by amount of labeled instances which constrains network size and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Predicted designs produced >4-fold improvement in butanol production score relative to baseline (~2.5× higher titer and 58% increase in rate); 5 of the top 6 performing pathways were among ML recommendations and outperformed 18 expert-selected pathways.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Highly effective at navigating large combinatorial space with limited experiments; outperformed expert designs and found top-performing combinations; limited by the single reported DBTL cycle—more cycles would likely improve performance and allow larger networks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High practical impact by accelerating DBTL cycles, reducing experimental burden for multi-enzyme pathway optimization, and demonstrating ML can outperform expert selection in complex combinatorial design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to expert-determined pathways (18 designs) and baseline pathway; ML recommendations outperformed these baselines. Also contrasted against exhaustive search (impractical) and smaller NN architectures due to data limits.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of fast cell-free prototyping to cheaply generate labeled instances, selection of actionable inputs (enzyme homolog + concentrations), cross-validation and architecture search, and optimization of a composite response (TREE) that balanced multiple objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Neural-network based surrogate models trained on modest but high-quality cell-free data can efficiently guide exploration in huge combinatorial enzyme design spaces and outperform human expert selections when inputs are actionable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2341.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepRibo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep neural network that uses ribosome profiling coverage and candidate ORF sequence features to predict translated open reading frames and translation initiation sites with improved sensitivity, particularly for small ORFs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Genome annotation / ORF prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Accurate identification of expressed protein-coding open reading frames and translation initiation sites from genome sequence and ribosome profiling data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate-large: used ribo-seq and candidate ORFs from multiple species with large numbers of candidate ORFs (e.g., 626,708 candidate ORF DNA sequences and ribo-seq signals from 7 species), labeled training data derived from experimental ribosome profiling.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequence data (DNA/RNA), coverage timepoint/profile arrays (coverage signals) — multimodal (sequence + signal) and high-dimensional.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: sequence features and coverage patterns map nonlinearly to translational status; high class imbalance and species variation demands robust models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature bioinformatics field with established methods (HMMs, random forests) but improving with deep learning leveraging larger experimental datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium: accurate annotation is the priority; mechanistic interpretability less critical though biological plausibility and discovery of novel small ORFs are important.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep neural networks combining RNN and CNN architectures</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Model inputs include ribosome profiling coverage signals and candidate ORF sequences; architecture used recurrent networks and convolutional layers to capture local sequence patterns and coverage signal features; trained supervised on labeled ribo-seq data to predict translated ORFs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (sequence + signal multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for high-throughput ribo-seq backed annotation tasks where labeled coverage data are available; scales with more experimental data and generalizes across bacteria.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported more robust performance and greater sensitivity than REPARATION (random forest based) and improved identification of novel small ORFs compared to RefSeq annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables higher-confidence genome annotation, discovery of previously missed small ORFs, and faster pathway reconstruction for metabolic engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against REPARATION (random forest) and RefSeq annotations; DeepRibo showed improved performance and novel ORF detection.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of large ribosome profiling datasets, multimodal inputs (sequence + coverage), and expressive deep architectures able to capture complex sequence-signal relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep learning trained on large, high-throughput experimental ribosome profiling datasets can substantially improve ORF annotation sensitivity and reveal novel coding sequences missed by traditional methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2341.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepEC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convolutional neural network model that maps protein sequence to enzyme commission (EC) numbers using a very large curated training set, improving speed and accuracy of enzyme functional annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Protein function annotation / enzyme prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict enzyme commission numbers (functional enzyme classification) for protein sequences to aid pathway design and enzyme selection in metabolic engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large-scale: trained on 1,388,606 expert-curated protein sequences with 4,669 EC labels (Swiss-Prot and TrEMBL); labeled dataset abundant and curated.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequence data (amino acid strings) converted to numeric encodings suitable for CNNs; large labeled corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional sequence-to-function mapping with many classes (EC numbers) and subtle sequence determinants of function.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established field with many annotation tools; deep learning is a recent, high-performance approach for high-throughput annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: primary need is accurate functional annotation; detailed mechanistic interpretation less essential though domain knowledge useful.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional neural networks (deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>CNNs trained on a very large labeled protein sequence dataset to predict EC numbers; model learns sequence motifs and patterns predictive of enzyme function and outputs multi-class EC predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (sequence classification)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable where large labeled sequence datasets exist; particularly effective for high-throughput annotation and mutation effect sensitivity prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Trained on 1,388,606 sequences; reported improved EC prediction accuracy and speed relative to five alternative tools (Cat-Fam, DETECT v2, ECPred, EFICAz2.5, PRIAM) though specific metrics not reported in review.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>More sensitive in predicting the effects of sequence domain and binding site mutations and outperformed alternative EC prediction tools in accuracy and throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for scaling enzyme annotation in pathway design and selecting candidate sequences for metabolic engineering and protein engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against multiple existing EC prediction tools and reported superior accuracy and speed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Very large curated training dataset, expressive CNN architectures that capture sequence motifs, and evaluation versus established baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Training deep convolutional models on very large curated protein sequence datasets yields substantial gains in enzyme function prediction accuracy and throughput, facilitating pathway reconstruction tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2341.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RetroPath RL / Segler et al. retrosynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement learning / deep learning approaches for retrosynthetic pathway design (Segler et al.; RetroPath RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of deep neural networks combined with Monte Carlo Tree Search (MCTS) to discover synthetic (chemical and biological) retrosynthesis routes efficiently and in agreement with expert chemists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planning chemical syntheses with deep neural networks and symbolic AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Pathway retrosynthesis / pathway design</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Find sequences of chemical transformations (or biochemical reactions) that produce a target compound from available precursors—search in a combinatorially exploding reaction space.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large reaction corpora available: Segler et al. preprocessed 12.4 million reaction rules from Reaxys; RetroPath RL and related methods leverage reaction databases and generalized reaction rules. Labeled data abundant for chemical reactions, less so for enzymatic biotransformations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph-like representations of molecules/reactions, reaction rules; large symbolic/structured corpora; can be represented as graphs or sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Extremely high combinatorial complexity: branching tree search over reaction rule applications; requires heuristic search to be tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature in organic synthesis with growing application to biosynthesis (retrobiosynthesis); classical rule-based methods exist, deep learning + search is recent and high-impact.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: solutions need to adhere to chemical plausibility; mechanistic enzymology may be needed later for enzyme selection but search focuses on feasible transformation sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep neural networks guiding Monte Carlo Tree Search (MCTS); reinforcement learning extensions (RetroPath RL)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train deep neural networks on large reaction datasets to predict likely reaction steps given a molecule; integrate these priors into a Monte Carlo Tree Search to prioritize promising retrosynthesis branches, improving efficiency and chemical plausibility; RetroPath RL applies reinforcement learning for bioretrosynthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning + heuristic/Monte Carlo search; reinforcement learning for retrosynthesis variants</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to large retrosynthesis spaces where hand-crafted heuristics are insufficient; scaling relies on availability of reaction corpora and computational search.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Segler et al. found pathways for twice as many molecules and ~30× faster than traditional computer-aided searches (per review summary); RetroPath RL extended to biological pathway predictions (no exact numbers in review).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Deep-learning-guided search produced synthesis routes that adhered better to chemical principles and were indistinguishable from literature routes by expert chemists; improved coverage and speed over classical methods.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables automated design of synthetic and biosynthetic pathways for novel molecules, greatly reducing manual search time and expanding feasible targets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to traditional computer-aided retrosynthesis search algorithms; deep-learning + MCTS found more routes and was much faster, with better adherence to chemical intuition.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Huge curated reaction datasets, expressive deep models to provide priors, and efficient integration with heuristic search (MCTS) to manage combinatorial explosion.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Combining learned priors from large reaction corpora with tree search heuristics lets AI efficiently explore combinatorial retrosynthesis spaces and substantially outperforms classical rule-based search.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2341.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Enzyme-reaction prediction (Faulon / Mellor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine learning methods to predict enzyme-reaction pairs and kinetic constants (SVMs, Gaussian Processes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of SVMs and Gaussian process models to predict whether a given enzyme sequence catalyzes a particular reaction (including Km predictions), leveraging sequence k-mer features and reaction signatures; GP offers uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular signatures-based prediction of enzyme promiscuity</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Enzyme function prediction / promiscuity / kinetics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identify candidate enzymes for reactions (including promiscuous activities) and estimate kinetic parameters (e.g., Km) to support pathway design when no canonical enzyme is known.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Variable: datasets of enzyme-reaction pairs exist but smaller than sequence corpora; labeled examples created from known enzyme-reaction annotations; data may be noisy and incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Paired data: protein sequence features (k-mers) and reaction signatures (functional group transformations); structured/feature-engineered tabular data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: mapping from sequence/reaction signature to catalytic activity is complex, involves many-to-many relationships and context-dependence; predictive uncertainty important.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Developing: machine learning provides complementary, computationally efficient alternatives to expensive DFT/QM calculations for screening enzyme candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium: predictions used to prioritize candidates; mechanistic validation experimentally required; uncertainty quantification valuable.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Support Vector Machines and Gaussian Process models (semi-supervised GP for enzyme search)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Models use engineered features from protein sequences (k-mer counts) and reaction signatures (functional groups/transformations) to classify enzyme-reaction pairs as positive/negative and regress kinetic parameters; Gaussian processes provide probabilistic predictions with uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (classification/regression); probabilistic modeling (GP)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for screening candidate enzymes rapidly and prioritizing experimental validation, especially when detailed quantum chemistry is too costly.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Gaussian process models provided useful predictions and uncertainty measures for enzyme-reaction pairs and Km estimation; enabled prioritization of candidate enzymes for pathway design.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate-to-high: accelerates enzyme selection for novel reactions and reduces computational/experimental costs by prioritizing promising candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Presented as computationally efficient alternatives to QM/DFT; GP advantages include uncertainty quantification over SVM/classical classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Careful feature engineering (reaction signatures, sequence k-mers), probabilistic modeling for uncertainty, and integration with pathway design pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Machine learning classifiers and probabilistic regressors can screen enzyme–reaction possibilities and estimate kinetic parameters rapidly, with uncertainty estimates enabling better experimental prioritization versus expensive mechanistic computations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2341.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML-assisted directed evolution (Wu et al., Romero/Arnold work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning-guided directed protein evolution and sequence–function modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of supervised and ensemble ML models to learn protein sequence–function landscapes from screening data, enabling in silico evolution rounds and prioritized variant libraries that accelerate directed evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning-assisted directed protein evolution with combinatorial libraries</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Protein engineering / directed evolution</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Explore vast protein sequence space to discover variants with improved fitness (activity, stability, color, thermostability) while minimizing experimental rounds and screenings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Variable: experimental screening datasets (ranging from hundreds to hundreds of thousands of variants depending on study); labeled fitness measurements required for supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequence-to-fitness labeled data (tabular with encoded sequence features), sometimes large combinatorial libraries (hundreds-thousands of variants).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Extremely high-dimensional combinatorial optimization with rugged, epistatic fitness landscapes; many local optima and sparse sampling limits.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established experimental practice (directed evolution) augmented recently by ML to reduce experimental burden; ML-guided methods are maturing rapidly.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: ML aims to find higher-fitness variants; mechanistic interpretability helpful but not required for success.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Ensembles of supervised models (linear, kernel, neural networks, Gaussian processes, random forests), in silico evolution guided by top models</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train multiple supervised models on sequence–fitness data; select best-performing models to predict and rank new variants for experimental testing; use model-guided in silico evolution algorithms to propose candidate libraries with high predicted fitness, iteratively improving with new labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning + active learning / model-guided search</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to protein engineering problems where moderate screening data exists; reduces required experimental rounds and broadens exploration of sequence space.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated successful evolution of improved enzymes/proteins across multiple studies (e.g., productivity, color, thermostability); ML-guided strategies enabled deeper exploration than greedy experimental heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: reduces time and cost of protein engineering and enables discovery of variants that might be missed by standard directed evolution heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to traditional directed evolution (local search, single-site/saturation strategies), ML-guided in silico exploration provides more efficient search and often finds superior variants.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of quality labeled screening data, use of ensembles and model selection, iterative update with new experimental rounds, and combining multiple model types to hedge against model bias.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised ML trained on screening data can effectively model sequence–function landscapes and guide in silico evolution to accelerate directed evolution and find superior protein variants with fewer experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2341.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniRep / sequence representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified rational protein engineering with sequence-based deep representation learning (UniRep)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent neural network trained on tens of millions of protein sequences to learn internal representations (embeddings) that capture physicochemical and functional information useful for downstream protein property prediction and engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unified rational protein engineering with sequence-based deep representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Protein representation learning / rational protein design</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn general-purpose numeric representations of protein sequences that encode structural/functional information to improve tasks like stability prediction and design with limited labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Very large unlabeled sequence corpora used for pretraining (~24 million UniRef50 sequences); downstream labeled data for specific tasks can be limited but benefit from transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Long sequence data (amino acid sequences) used to train recurrent models; output is fixed-length numeric embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: sequences encode complex structural and functional relationships; transfer learning helps address scarcity of labeled data for specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging: transfer learning and deep sequence models are a rapidly advancing approach in protein engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: embeddings are used for predictive tasks; some interpretability possible via downstream models using representations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Recurrent neural networks for large-scale unsupervised/representation learning (UniRep)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train RNNs on massive protein sequence corpora to learn sequence representations capturing evolutionary, physicochemical, and functional signals; use these embeddings as input features to simpler predictive models (random forest, linear) for downstream tasks like stability or mutation effect prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / self-supervised representation learning + transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for low-N protein engineering tasks where labeled data are scarce; representations transfer across protein families and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Trained on ~24 million sequences; downstream applications showed improved prediction of stability and functional consequence of mutations (specific metrics not reproduced in review).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enabled prediction and optimization of protein stability and function across diverse proteins, demonstrating generalizability of learned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: representation learning reduces labeled-data requirements for many protein engineering tasks and improves model generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed models trained from scratch on limited labeled data and provided better generalization versus classical feature-engineering approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Very large unlabeled sequence corpus for pretraining, effective RNN architecture, and leveraging embeddings with simple downstream predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Pretrained sequence embeddings capture rich protein information that transfers to diverse engineering tasks, enabling accurate predictions with limited labeled data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2341.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ART (Automated Recommendation Tool)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A machine learning Automated Recommendation Tool for synthetic biology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool tailored for metabolic engineering that uses Bayesian ensemble modeling and uncertainty quantification to recommend experimental designs from small datasets and iteratively guide DBTL cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A machine learning Automated Recommendation Tool for synthetic biology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Design recommendation / experimental planning in metabolic engineering</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Recommend actionable strain designs (e.g., promoter combinations) to optimize chemical production using limited initial datasets, while quantifying prediction uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Targeted for small datasets: effective for problems with small training sets (<hundreds of instances); supports both labeled design–response data and multi-omics features when available.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular data: categorical design variables (e.g., promoter combinations) and numeric multi-omics features; responses are scalar production measures.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: small-data regime with potentially many features; requires uncertainty-aware models and ensemble strategies to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Tooling for ML-guided DBTL is emerging; ART represents applied tooling addressing domain needs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium: uncertainty quantification and explainability valued to build trust; mechanistic insight helpful but ART focuses on predictive recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Bayesian ensemble modeling with uncertainty quantification (probabilistic ensemble / ART)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Ensemble of models combined in a Bayesian framework to produce predictive point estimates and credible uncertainty intervals for responses; used to make recommendations balanced between predicted performance and uncertainty to guide iterative experimental cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (ensemble) with Bayesian uncertainty estimation; active recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for small-data experimental design in metabolic engineering where uncertainty matters; designed to work with limited labeled instances and provide principled exploration–exploitation trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides uncertainty-aware recommendations and has been demonstrated on synthetic and real datasets; helps choose exploratory versus exploitative experiments to build trust and improve models over DBTL cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for practical adoption in labs constrained by experimental budgets, by enabling principled, uncertainty-aware experimental planning and accelerating iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Addresses shortcomings of single-model recommendations by explicitly quantifying uncertainty versus plain-point-prediction ensemble approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Design tailored for small datasets, emphasis on uncertainty quantification, integration with scikit-learn pipelines and DBTL workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>In small-data, high-cost experimental settings, Bayesian ensembles that quantify uncertainty provide a practical and trustworthy way to recommend experiments and accelerate learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2341.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq-DeepCpf1 and DeepCpf1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity (Seq-DeepCpf1 / DeepCpf1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep convolutional neural networks trained on large-scale sgRNA activity datasets to predict on-target CRISPR-Cpf1 (AsCpf1) knockout efficacy; performance improves with dataset size and inclusion of epigenetic features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Genome editing guide design (CRISPR efficacy prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict on-target activity (indel frequency) of CRISPR guide RNAs to select highly effective guides and improve genome editing outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large-scale: the review cites training on >15,000 target sequences for DeepCpf1 and 16,292 sgRNA sequences for Seq-DeepCpf1; generally requires >10k high-quality labeled instances for deep models to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequence composition features of guides (nucleotide sequences), optionally epigenetic/contextual features (chromatin accessibility), labeled with measured activity rates (numeric).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-high: sequence and chromatin context nonlinearly determine activity; off-target and cell-type differences add complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Growing maturity: many computational tools exist; deep learning approaches are recent and show improved generalizability with large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium: practical guide efficacy prediction prioritized, but interpretability and understanding off-target mechanisms are important for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional neural networks (deep learning) with additional biological feature integration</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>CNNs trained on labeled sgRNA activity datasets to predict indel frequencies; models improved by expanding training data (>10k instances) and by adding orthogonal input features like chromatin accessibility to improve generalization across cell types.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (sequence regression/classification)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when large, high-quality sgRNA activity datasets are available; benefits from inclusion of epigenetic/contextual features.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Performance improved steadily as training data size increased; Seq-DeepCpf1 trained on 16,292 sgRNAs outperformed conventional ML algorithms (exact metrics not provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>DeepCpf1/Seq-DeepCpf1 outperformed classical ML approaches and generalized better to independent datasets; data quantity and richer feature sets improved accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for improving CRISPR guide selection, reducing failed edits, and improving on-target efficacy across cell types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed conventional ML models trained on smaller datasets; showed that deep models scale with larger (>10k) datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large labeled datasets, use of deep CNNs appropriate for sequence data, and augmentation with relevant biological context (chromatin accessibility).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep convolutional models trained on large, high-quality sgRNA activity datasets (and enriched with epigenetic features) substantially improve prediction and generalization of guide efficacy over classical ML.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2341.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepCRISPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepCRISPR: optimized CRISPR guide RNA design by deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach combining unsupervised representation learning on billions of unlabeled sgRNA sequences and supervised deep neural networks with epigenetic features to predict both on-target efficacy and off-target profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepCRISPR: optimized CRISPR guide RNA design by deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>CRISPR guide design (on-target and off-target prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Simultaneously maximize on-target knockout activity and minimize off-target effects for sgRNA design across different cell types.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Very large unlabeled pretraining corpus (~0.68 billion sgRNA sequences) used for unsupervised representation learning; labeled on-target (~200k) and off-target (~160k) datasets used for supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Large-scale sequence data for unsupervised pretraining; labeled sequence + epigenetic/context features for supervised prediction tasks (on/off-target activities).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: both sequence-dependent and epigenetic factors influence activity and off-targeting; cross-cell-type generalization is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Advanced computational development with increasing dataset availability enabling deep approaches; unsupervised pretraining is recent and powerful.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for safety-critical applications: off-target minimization and interpretability matter, but prediction accuracy is primary for guide selection.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Unsupervised representation learning + supervised deep convolutional denoising neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Unsupervised deep learning to learn sgRNA sequence representations from massive unlabeled data; use learned representations as inputs to supervised CNNs trained on labeled on-target/off-target datasets along with epigenetic features to predict activities with improved specificity and sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Self-supervised/unupervised pretraining + supervised deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable where massive unlabeled sequence pools and moderately large labeled datasets exist; addresses generalizability across cell types.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Outperformed classical ML methods and demonstrated high generalizability to other cell types; unsupervised pretraining automated feature discovery and improved downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High impact for robust, generalizable CRISPR guide design with improved safety (reduced off-targets) and efficacy across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed conventional ML and classical feature-engineered methods due to representation learning and large-scale pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of huge unlabeled sequences for pretraining, labeled activity datasets, integration of epigenetic/context features, and deep architectures capable of leveraging representations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Combining unsupervised representation learning on massive unlabeled genomic sequence corpora with supervised deep models and epigenetic features yields robust, generalizable CRISPR guide efficacy and off-target predictions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2341.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prosit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-directional RNN model that predicts peptide chromatographic retention time and tandem mass spectra from peptide sequences, improving proteomics identification workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Proteomics data processing / peptide identification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict peptide tandem mass spectra and retention times from sequence to improve peptide identification and throughput in mass spectrometry proteomics.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate-large: trained on 550,000 tryptic peptides; labeled experimental spectra available for supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequence data (peptides) mapped to spectral data (mass spectra) and retention times — multimodal numeric/time-series-like outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: mapping sequence to spectral fragmentation patterns is complex and influenced by many physicochemical factors; noisy experimental measurements add difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Maturing: proteomics has established pipelines; deep learning enhancements are recent and significantly improve identification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium: pragmatic improvements in spectral prediction are the goal; detailed mechanistic fragmentation theory less required for improved identification.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Bidirectional recurrent neural networks (RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train bi-directional RNNs on large peptide–spectrum datasets to predict expected tandem mass fragmentation patterns and retention times; outputs used to improve peptide identification in proteomics pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (sequence-to-spectrum regression)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for mass-spectrometry based proteomics when sizable labeled spectra datasets are available; improves database search and identification sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enabled improved peptide identification quality and throughput in proteomics workflows, facilitating downstream ML and metabolic engineering analyses that rely on proteomics.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for enhancing high-throughput proteomics data quality, which in turn improves ML models that depend on proteomic features for metabolic engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Reported improvements over classical spectrum prediction and heuristic methods, leveraging deep sequence-to-spectrum learning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large labeled peptide–spectrum training sets, expressive RNN architecture, and integration into proteomics pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep sequence-to-spectrum models can substantially improve proteomics identification, providing higher-quality input data for downstream ML-driven metabolic engineering tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2341.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bioprocess optimization (Coleman et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>An integrated approach to optimization of Escherichia coli fermentations using historical data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven pipeline using decision-tree-based feature selection, ANN ensembles, and genetic algorithms to model and optimize fermentation outputs from historical bioprocess data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An integrated approach to optimization of Escherichia coli fermentations using historical data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Bioprocess modeling and scale-up / fermentation optimization</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Use historical fermentation batch data to identify key process variables and optimize fermentation inputs (media, inoculum, operational parameters) to maximize biomass, product concentration, and productivity.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Limited-moderate historical datasets: example used 69 fed-batch fermentations with 13 process features; data heterogeneous and noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Heterogeneous tabular data: categorical and continuous process variables, time-series measurements aggregated per batch; preprocessed to reduce dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: multiparametric processes with temporal heterogeneity, batch effects, correlated inputs, and nonlinear relationships between process variables and outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established industrial practice but often artisanal; ML-driven data mining of historical records is an established augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium: mechanistic process understanding useful but data-driven models can reveal actionable variable selections; combining both is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Decision tree feature selection + ANN ensembles + genetic algorithm optimization</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Decision trees used to preselect important process inputs from historical datasets to avoid overfitting; selected features trained using ANN ensembles to predict outputs; genetic algorithms applied to model predictions to identify input settings that maximize outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (ensemble) + heuristic optimization; feature selection preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Suitable for exploiting historical process data to improve operational settings where mechanistic models are incomplete; requires careful preprocessing to remove redundant/correlated inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Applied to 69 fermentations; produced actionable optimized input settings; specific numerical performance gains not reproduced in review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated identification of important process variables and provided optimized settings that improved outputs in practice; highlighted need for preprocessing to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Practical impact on reducing trial-and-error in scale-up and process development, enabling extraction of insights from archival datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared favorably to naive modeling approaches; emphasized feature selection to outperform overfit models on small archival datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Historical dataset availability, robust feature selection to avoid overfitting, ensemble models for prediction, and optimization algorithms to convert models into actionable process settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Feature-selected ensemble models trained on curated historical process data can drive practical fermentation optimization and identify non-obvious influential process variables despite noisy heterogeneous data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2341.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement learning for bioprocess control (Treloar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep reinforcement learning for the control of microbial co-cultures in bioreactors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of reinforcement learning (fitted Q-learning / policy gradient variants) to learn control policies for bioreactor processes (e.g., species composition, temperature control) from online data to optimize production goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning for the control of microbial co-cultures in bioreactors</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Bioprocess control / bioreactor optimization</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Control dynamical bioreactor environments (e.g., co-culture composition, temperature, feed rates) to achieve desired production metrics or species abundances using sequential decision policies learned from interaction data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies: online continuous measurements can be abundant during an experiment (e.g., measurements every 5 min) but collecting many trials for RL can be costly; many RL demos are in silico or in simulated settings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-series control and measurement streams (continuous online signals) with discrete/continuous action spaces; high temporal resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: non-stationary, stochastic dynamics with delayed outcomes; large state and action spaces; sample inefficiency is a central challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Early-stage in practice for bioprocess control; traditional PID/MPC dominate but RL shows promise in research and simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-high: operational safety and stability demand interpretable and reliable control policies; integrating mechanistic models helps sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Model-free reinforcement learning algorithms (fitted Q-learning, deep deterministic policy gradients, policy gradient with RNNs, multi-step Q-learning variants)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>RL algorithms learn control policies mapping observed reactor states to control actions by maximizing cumulative reward (e.g., product yield) using trial-and-error interaction data; implementations include neural-network parameterized policies, recurrent policies for temporal dependencies, and transfer learning from mechanistic simulators to real online adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Reinforcement learning (model-free and hybrid with transfer learning)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable where online instrumentation provides high-frequency measurements and safe experimental iteration is possible; sample efficiency and safety constraints limit immediate industrial deployment without hybridization or transfer learning.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>RL controllers in simulations and some lab demonstrations achieved improved control (lower overshoot, faster tracking, smoother control) compared to PID/MPC baselines; however, RL needs many interactions and can be impractical without simulation pretraining or hybrid approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Potentially transformative for adaptive, data-driven bioprocess control, especially when combined with mechanistic simulators or transfer learning to reduce required real-world data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to advanced PID and MPC methods; RL achieved smoother and faster control in examples but required more data and benefited from model hybridization or pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of high-frequency online measurements, simulation environments for pretraining/transfer learning, and hybrid approaches combining mechanistic controllers to improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Reinforcement learning can learn superior control policies for complex bioprocesses but practical deployment needs improved sample efficiency via transfer learning or integration with mechanistic models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e2341.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mechanism-aware ML (Culley et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid approach incorporating mechanistic model-derived features (e.g., FBA/pFBA-predicted fluxes) alongside transcriptomic data into ML models to improve prediction of growth phenotypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Phenotype prediction / integration of mechanistic and data-driven models</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict yeast growth rate from transcriptomic profiles augmented with features derived from genome-scale metabolic models to improve predictive accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate: transcriptomics datasets and genome-scale model simulations available; labeled phenotype data (growth rates) needed.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional transcriptomic features combined with mechanistic model outputs (predicted fluxes) — multimodal fused feature set.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: mapping from expression to phenotype is nonlinear and context-dependent; high dimensionality risks overfitting without meaningful feature engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Maturing: integration of GSMs and ML is an active area; mechanistic models exist but have limited quantitative accuracy alone.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High: combining mechanistic features provides interpretability and biological plausibility; mechanistic constraints help ensure biologically meaningful ML predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Neural networks trained on combined omics and genome-scale model-derived flux features</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Compute flux predictions from parsimonious flux balance analysis (pFBA) and use these flux features together with transcriptomics as inputs to neural networks to predict growth rate; hybrid feature set improved predictive performance versus transcriptomics alone.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (hybrid mechanistic + data-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate where mechanistic GSMs exist and can generate informative derived features; helps small-data regimes by providing informative priors.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Using pFBA-predicted fluxes as additional features improved neural network predictive power relative to using transcriptomics alone, demonstrating benefit of hybridization.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Promising for improving phenotype prediction and making ML outputs more biologically interpretable and robust, aiding metabolic engineering decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Shown to outperform pure data-driven models on the studied task when mechanistic features were included.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality of mechanistic GSM, meaningful flux predictions from pFBA, and effective fusion of mechanistic and omics features in ML models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Augmenting ML models with mechanistic model–derived features can significantly improve predictive accuracy and biological plausibility in phenotype prediction tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e2341.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kcat prediction (Heckmann et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of ML to predict enzyme catalytic turnover numbers (kcat) from network, structural, and biochemical features, then using predicted kcat to parameterize genome-scale models and improve proteome/flux predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Kinetic parameter prediction / genome-scale metabolic modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict enzyme catalytic rates (kcat) where experimental measurements are missing and use these predictions to parametrize mechanistic models for improved flux/proteome predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Limited for kcat measurements but augmented by features: ML trained on datasets combining known kcat with enzyme structural, network, biochemical, and assay condition features.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular features aggregating enzyme structural properties, network context, biochemistry, and assay metadata; labeled with experimental kcat values.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: kcat depends on multiple structural and contextual factors; noisy labels and scarcity of measured kcat complicate regression.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Growing: GSMs are mature but lack reliable kinetic parameterization; ML approaches provide practical means to fill gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High: ultimate goal is mechanistically meaningful parameterization of models; ML predictions are used as inputs to mechanistic models and validated for plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Ensemble of supervised ML regressors (various models applied to predict kcat)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train machine learning models on enzyme turnover numbers using a wide set of features (structural correlates, network metrics, assay conditions) to predict kcat, then use predicted values to parameterize genome-scale metabolic or proteome-constrained models to improve predictions of proteome allocation and fluxes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised regression (hybrid ML-mechanistic pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when mechanistic models lack kinetic parameters; ML can impute missing kcat values to enable more quantitative simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Predicted turnover numbers captured structural correlates and improved proteome predictions when used to parametrize GSMs, demonstrating utility of ML-derived parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: enables more quantitative genome-scale and proteome-constrained modeling across organisms and conditions by filling kinetic data gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers data-driven parameter estimation as an efficient complement to laborious experimental kcat measurement and computationally intensive QM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Rich engineered feature set combining structural and network properties, and validation by improved mechanistic model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>ML can predict enzyme kinetic parameters from heterogeneous feature sets and meaningfully improve mechanistic metabolic model parameterization and downstream predictions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2341.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e2341.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metabolome-from-proteome (Zelezniak et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine learning predicts the yeast metabolome from the quantitative proteome of kinase knockouts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multilinear regression and ML approaches to predict metabolite concentrations from protein levels, leveraging network proximity (closest enzyme neighbors) as features to infer metabolome changes in mutants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning predicts the yeast metabolome from the quantitative proteome of kinase knockouts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Metabolomics prediction / genotype-to-metabolome mapping</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict intracellular metabolite concentrations (responses) from quantitative proteomics (inputs), aiming to infer metabolic consequences of genetic perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate: proteome measurements from kinase knockout panels and matched metabolomics available; labeled paired proteome–metabolome datasets used for supervised regression.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional numeric proteomics features paired with metabolite concentration responses; network-structured relationships used to select features (nearest enzyme neighbors).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: metabolite levels are determined by network fluxes, enzyme activities, regulation, and cellular state; many-to-many mappings and possible hidden variables.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging: integrated proteome–metabolome predictive modeling is an advancing area with increasing multi-omic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for interpretation: predictions aim to reflect biochemical network relationships and suggest mechanistic hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Multilinear regression augmented with network-constrained feature selection and other ML methods</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Express metabolite concentrations as functions of expression levels of proximate enzymes in the metabolic network (multilinear regression), optionally augmented by other ML techniques to capture nonlinearity; utilize network topology to choose meaningful input features.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (regression) with network-informed feature engineering</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when paired proteomics and metabolomics exist and network maps are available to constrain feature choices; helps generate mechanistic hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Able to predict metabolite concentration changes from proteome changes for yeast mutants and to propose candidate mechanistic links; demonstrates value of network-informed ML.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Medium-high: supports prioritization of follow-up experiments and helps extract mechanistic hypotheses from multi-omic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Network-constrained regression provides an interpretable alternative to black-box models and helps focus on biologically plausible predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality paired multi-omics data, curated metabolic network maps for feature selection, and emphasis on interpretable modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Incorporating metabolic network structure into supervised models enables more accurate and interpretable prediction of metabolite levels from proteomic data, bridging data-driven and mechanistic perspectives.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A machine learning approach to predict metabolic pathway dynamics from time-series multiomics data <em>(Rating: 2)</em></li>
                <li>In vitro prototyping and rapid optimization of biosynthetic enzymes for cell design <em>(Rating: 2)</em></li>
                <li>DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns <em>(Rating: 2)</em></li>
                <li>Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers <em>(Rating: 2)</em></li>
                <li>Planning chemical syntheses with deep neural networks and symbolic AI <em>(Rating: 2)</em></li>
                <li>Reinforcement learning for bioretrosynthesis <em>(Rating: 2)</em></li>
                <li>Machine learning-assisted directed protein evolution with combinatorial libraries <em>(Rating: 2)</em></li>
                <li>Unified rational protein engineering with sequence-based deep representation learning <em>(Rating: 2)</em></li>
                <li>A machine learning Automated Recommendation Tool for synthetic biology <em>(Rating: 2)</em></li>
                <li>Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity <em>(Rating: 2)</em></li>
                <li>DeepCRISPR: optimized CRISPR guide RNA design by deep learning <em>(Rating: 2)</em></li>
                <li>Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning <em>(Rating: 2)</em></li>
                <li>An integrated approach to optimization of Escherichia coli fermentations using historical data <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning for the control of microbial co-cultures in bioreactors <em>(Rating: 2)</em></li>
                <li>Machine learning predicts the yeast metabolome from the quantitative proteome of kinase knockouts <em>(Rating: 2)</em></li>
                <li>Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2341",
    "paper_id": "paper-227131886",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Kinetic learning (Costello et al.)",
            "name_full": "Machine learning approach to predict metabolic pathway dynamics from time-series multiomics data",
            "brief_description": "Supervised ML model trained on time-series protein and metabolite data to predict metabolite rate-of-change (pathway kinetics), using data augmentation and feature selection to overcome sparse experimental timepoints.",
            "citation_title": "A machine learning approach to predict metabolic pathway dynamics from time-series multiomics data",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Metabolic pathway kinetics / metabolic engineering",
            "problem_description": "Predict instantaneous rates of change of metabolites (pathway dynamics) from measured protein and metabolite concentrations to enable pathway design without hand-crafted kinetic parameterization.",
            "data_availability": "Limited/scarce: original experimental case used only three time series of 7 timepoints each (very small), labeled supervised data; the authors used data augmentation to expand to ~200 instances. Data are high-quality experimental multi-omic time series but costly to produce.",
            "data_structure": "Time series, multivariate numeric (protein concentrations, metabolite concentrations); relatively low number of instances but high dimensionality if host metabolism included; smooth/continuous temporal structure exploited for interpolation.",
            "problem_complexity": "Moderate-high: nonlinear dynamical mapping from protein/metabolite abundances to metabolite rate changes; low instance count relative to potential inputs (curse of dimensionality) required dimensionality reduction/feature selection; search/learning complexity addressed via augmentation.",
            "domain_maturity": "Emerging: kinetic modeling is well-established (Michaelis-Menten, mechanistic models) but limited by missing parameters; data-driven alternatives are nascent and exploratory.",
            "mechanistic_understanding_requirements": "Medium: predictive accuracy prioritized over mechanistic interpretability; mechanistic insight desirable but not required for effective predictions.",
            "ai_methodology_name": "Supervised regression models with feature selection and data augmentation (ensemble/TPOT pipeline)",
            "ai_methodology_description": "Supervised regression framework where inputs are protein and metabolite concentrations at timepoints and outputs are metabolite rate-of-change; steps included Savitzky–Golay smoothing and interpolation to augment timepoints, feature selection/dimensionality reduction, and automated pipeline search (TPOT) over scikit-learn models to select preprocessing and model ensemble; trained on augmented time-series instances.",
            "ai_methodology_category": "Supervised learning (regression) with automated model selection / ensemble; uses data augmentation",
            "applicability": "Applicable and appropriate for short, labeled time-series where hand-crafted kinetics are unreliable; requires assumptions (smoothness/continuity) for augmentation and careful feature selection to avoid curse of dimensionality.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Outperformed a hand-crafted kinetic model in predictive power on held-out strain despite very limited raw data, enabled systematic application to any pathway with less manual curation; success depended critically on data augmentation and feature selection.",
            "impact_potential": "High for enabling pathway dynamics prediction when kinetic parameters are unknown, reducing time spent building mechanistic models; can accelerate DBTL cycles by providing predictive screening of designs without exhaustive mechanistic parameterization.",
            "comparison_to_alternatives": "Compared qualitatively against classic kinetic (Michaelis–Menten) models; ML approach gave better predictive performance on the test strain and was faster to apply than manual kinetic model construction.",
            "success_factors": "Data augmentation (interpolation/smoothing) to increase instance count, feature selection to reduce dimensionality, pipeline automation (TPOT) to pick suitable preprocessing and model, and careful experimental design to provide consistent high-quality time-series.",
            "key_insight": "Data-driven supervised learning can recover useful kinetic predictions from very limited time-series when combined with principled augmentation and feature-selection, and can outperform handcrafted kinetic models for prediction tasks even without mechanistic representations.",
            "uuid": "e2341.0"
        },
        {
            "name_short": "Cell-free butanol optimization (Karim et al.)",
            "name_full": "In vitro prototyping and rapid optimization of biosynthetic enzymes for cell design (iPROBE + neural networks)",
            "brief_description": "Use of neural networks to predict high-performing enzyme homolog/composition and concentration combinations for a six-step n-butanol pathway tested in a rapid cell-free prototyping system, substantially improving production metrics.",
            "citation_title": "In vitro prototyping and rapid optimization of biosynthetic enzymes for cell design",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Pathway optimization / enzyme combinatorial design in metabolic engineering",
            "problem_description": "Search the combinatorial space of enzyme homologs and enzyme concentrations for a multi-step pathway to maximize combined objective (TREE score: titer, rate, enzyme expression) using limited experimental screening enabled by cell-free prototyping.",
            "data_availability": "Limited but structured: initial training set of 120 pathway designs (instances) measured for TREE score; labeled supervised data; cell-free platform enables relatively rapid generation but full combinatorial space is huge (hundreds of thousands of combinations).",
            "data_structure": "Structured tabular categorical + numeric: categorical features (which homolog used per step) and numeric features (enzyme concentrations); response is scalar TREE score combining multiple performance metrics.",
            "problem_complexity": "Very high combinatorial complexity: combinatorial explosion (e.g., example 314,928 combinations for partial choices), nonlinear interactions between enzymes and concentrations; unknown epistatic effects across steps.",
            "domain_maturity": "Applied/advancing: pathway optimization is established experimentally; data-driven ML-guided optimization is an emerging, practical approach.",
            "mechanistic_understanding_requirements": "Low-to-medium: primary goal is predictive optimization (higher TRY); interpretability helpful but not required to realize improved production.",
            "ai_methodology_name": "Deep neural networks (fully connected feedforward ensembles) with architecture search and subsequent nonlinear optimization (Nelder–Mead)",
            "ai_methodology_description": "Ensembles of small fully connected neural networks (tested architectures with 5–15 hidden layers and 5–15 nodes per layer) trained on 120 labeled instances; genetic algorithms used to propose architectures; ten-fold cross-validation selected best models; trained models used in conjunction with a Nelder–Mead simplex nonlinear optimizer to propose new pathway designs maximizing the TREE score.",
            "ai_methodology_category": "Supervised learning (deep learning) + optimization; ensemble modeling and hyperparameter/architecture search",
            "applicability": "Well-suited to structured combinatorial design problems where inputs are actionable (homolog identities, concentrations) and labeled data can be generated; limited by amount of labeled instances which constrains network size and depth.",
            "effectiveness_quantitative": "Predicted designs produced &gt;4-fold improvement in butanol production score relative to baseline (~2.5× higher titer and 58% increase in rate); 5 of the top 6 performing pathways were among ML recommendations and outperformed 18 expert-selected pathways.",
            "effectiveness_qualitative": "Highly effective at navigating large combinatorial space with limited experiments; outperformed expert designs and found top-performing combinations; limited by the single reported DBTL cycle—more cycles would likely improve performance and allow larger networks.",
            "impact_potential": "High practical impact by accelerating DBTL cycles, reducing experimental burden for multi-enzyme pathway optimization, and demonstrating ML can outperform expert selection in complex combinatorial design.",
            "comparison_to_alternatives": "Compared to expert-determined pathways (18 designs) and baseline pathway; ML recommendations outperformed these baselines. Also contrasted against exhaustive search (impractical) and smaller NN architectures due to data limits.",
            "success_factors": "Use of fast cell-free prototyping to cheaply generate labeled instances, selection of actionable inputs (enzyme homolog + concentrations), cross-validation and architecture search, and optimization of a composite response (TREE) that balanced multiple objectives.",
            "key_insight": "Neural-network based surrogate models trained on modest but high-quality cell-free data can efficiently guide exploration in huge combinatorial enzyme design spaces and outperform human expert selections when inputs are actionable.",
            "uuid": "e2341.1"
        },
        {
            "name_short": "DeepRibo",
            "name_full": "DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns",
            "brief_description": "Deep neural network that uses ribosome profiling coverage and candidate ORF sequence features to predict translated open reading frames and translation initiation sites with improved sensitivity, particularly for small ORFs.",
            "citation_title": "DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Genome annotation / ORF prediction",
            "problem_description": "Accurate identification of expressed protein-coding open reading frames and translation initiation sites from genome sequence and ribosome profiling data.",
            "data_availability": "Moderate-large: used ribo-seq and candidate ORFs from multiple species with large numbers of candidate ORFs (e.g., 626,708 candidate ORF DNA sequences and ribo-seq signals from 7 species), labeled training data derived from experimental ribosome profiling.",
            "data_structure": "Sequence data (DNA/RNA), coverage timepoint/profile arrays (coverage signals) — multimodal (sequence + signal) and high-dimensional.",
            "problem_complexity": "High: sequence features and coverage patterns map nonlinearly to translational status; high class imbalance and species variation demands robust models.",
            "domain_maturity": "Mature bioinformatics field with established methods (HMMs, random forests) but improving with deep learning leveraging larger experimental datasets.",
            "mechanistic_understanding_requirements": "Medium: accurate annotation is the priority; mechanistic interpretability less critical though biological plausibility and discovery of novel small ORFs are important.",
            "ai_methodology_name": "Deep neural networks combining RNN and CNN architectures",
            "ai_methodology_description": "Model inputs include ribosome profiling coverage signals and candidate ORF sequences; architecture used recurrent networks and convolutional layers to capture local sequence patterns and coverage signal features; trained supervised on labeled ribo-seq data to predict translated ORFs.",
            "ai_methodology_category": "Supervised deep learning (sequence + signal multimodal)",
            "applicability": "Appropriate for high-throughput ribo-seq backed annotation tasks where labeled coverage data are available; scales with more experimental data and generalizes across bacteria.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported more robust performance and greater sensitivity than REPARATION (random forest based) and improved identification of novel small ORFs compared to RefSeq annotations.",
            "impact_potential": "Enables higher-confidence genome annotation, discovery of previously missed small ORFs, and faster pathway reconstruction for metabolic engineering.",
            "comparison_to_alternatives": "Compared against REPARATION (random forest) and RefSeq annotations; DeepRibo showed improved performance and novel ORF detection.",
            "success_factors": "Use of large ribosome profiling datasets, multimodal inputs (sequence + coverage), and expressive deep architectures able to capture complex sequence-signal relationships.",
            "key_insight": "Deep learning trained on large, high-throughput experimental ribosome profiling datasets can substantially improve ORF annotation sensitivity and reveal novel coding sequences missed by traditional methods.",
            "uuid": "e2341.2"
        },
        {
            "name_short": "DeepEC",
            "name_full": "Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers",
            "brief_description": "Convolutional neural network model that maps protein sequence to enzyme commission (EC) numbers using a very large curated training set, improving speed and accuracy of enzyme functional annotation.",
            "citation_title": "Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Protein function annotation / enzyme prediction",
            "problem_description": "Predict enzyme commission numbers (functional enzyme classification) for protein sequences to aid pathway design and enzyme selection in metabolic engineering.",
            "data_availability": "Large-scale: trained on 1,388,606 expert-curated protein sequences with 4,669 EC labels (Swiss-Prot and TrEMBL); labeled dataset abundant and curated.",
            "data_structure": "Sequence data (amino acid strings) converted to numeric encodings suitable for CNNs; large labeled corpus.",
            "problem_complexity": "High-dimensional sequence-to-function mapping with many classes (EC numbers) and subtle sequence determinants of function.",
            "domain_maturity": "Established field with many annotation tools; deep learning is a recent, high-performance approach for high-throughput annotation.",
            "mechanistic_understanding_requirements": "Low-to-medium: primary need is accurate functional annotation; detailed mechanistic interpretation less essential though domain knowledge useful.",
            "ai_methodology_name": "Convolutional neural networks (deep learning)",
            "ai_methodology_description": "CNNs trained on a very large labeled protein sequence dataset to predict EC numbers; model learns sequence motifs and patterns predictive of enzyme function and outputs multi-class EC predictions.",
            "ai_methodology_category": "Supervised deep learning (sequence classification)",
            "applicability": "Highly applicable where large labeled sequence datasets exist; particularly effective for high-throughput annotation and mutation effect sensitivity prediction.",
            "effectiveness_quantitative": "Trained on 1,388,606 sequences; reported improved EC prediction accuracy and speed relative to five alternative tools (Cat-Fam, DETECT v2, ECPred, EFICAz2.5, PRIAM) though specific metrics not reported in review.",
            "effectiveness_qualitative": "More sensitive in predicting the effects of sequence domain and binding site mutations and outperformed alternative EC prediction tools in accuracy and throughput.",
            "impact_potential": "High for scaling enzyme annotation in pathway design and selecting candidate sequences for metabolic engineering and protein engineering.",
            "comparison_to_alternatives": "Compared against multiple existing EC prediction tools and reported superior accuracy and speed.",
            "success_factors": "Very large curated training dataset, expressive CNN architectures that capture sequence motifs, and evaluation versus established baselines.",
            "key_insight": "Training deep convolutional models on very large curated protein sequence datasets yields substantial gains in enzyme function prediction accuracy and throughput, facilitating pathway reconstruction tasks.",
            "uuid": "e2341.3"
        },
        {
            "name_short": "RetroPath RL / Segler et al. retrosynthesis",
            "name_full": "Reinforcement learning / deep learning approaches for retrosynthetic pathway design (Segler et al.; RetroPath RL)",
            "brief_description": "Application of deep neural networks combined with Monte Carlo Tree Search (MCTS) to discover synthetic (chemical and biological) retrosynthesis routes efficiently and in agreement with expert chemists.",
            "citation_title": "Planning chemical syntheses with deep neural networks and symbolic AI",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Pathway retrosynthesis / pathway design",
            "problem_description": "Find sequences of chemical transformations (or biochemical reactions) that produce a target compound from available precursors—search in a combinatorially exploding reaction space.",
            "data_availability": "Large reaction corpora available: Segler et al. preprocessed 12.4 million reaction rules from Reaxys; RetroPath RL and related methods leverage reaction databases and generalized reaction rules. Labeled data abundant for chemical reactions, less so for enzymatic biotransformations.",
            "data_structure": "Graph-like representations of molecules/reactions, reaction rules; large symbolic/structured corpora; can be represented as graphs or sequences.",
            "problem_complexity": "Extremely high combinatorial complexity: branching tree search over reaction rule applications; requires heuristic search to be tractable.",
            "domain_maturity": "Mature in organic synthesis with growing application to biosynthesis (retrobiosynthesis); classical rule-based methods exist, deep learning + search is recent and high-impact.",
            "mechanistic_understanding_requirements": "Low-to-medium: solutions need to adhere to chemical plausibility; mechanistic enzymology may be needed later for enzyme selection but search focuses on feasible transformation sequences.",
            "ai_methodology_name": "Deep neural networks guiding Monte Carlo Tree Search (MCTS); reinforcement learning extensions (RetroPath RL)",
            "ai_methodology_description": "Train deep neural networks on large reaction datasets to predict likely reaction steps given a molecule; integrate these priors into a Monte Carlo Tree Search to prioritize promising retrosynthesis branches, improving efficiency and chemical plausibility; RetroPath RL applies reinforcement learning for bioretrosynthesis.",
            "ai_methodology_category": "Supervised deep learning + heuristic/Monte Carlo search; reinforcement learning for retrosynthesis variants",
            "applicability": "Highly applicable to large retrosynthesis spaces where hand-crafted heuristics are insufficient; scaling relies on availability of reaction corpora and computational search.",
            "effectiveness_quantitative": "Segler et al. found pathways for twice as many molecules and ~30× faster than traditional computer-aided searches (per review summary); RetroPath RL extended to biological pathway predictions (no exact numbers in review).",
            "effectiveness_qualitative": "Deep-learning-guided search produced synthesis routes that adhered better to chemical principles and were indistinguishable from literature routes by expert chemists; improved coverage and speed over classical methods.",
            "impact_potential": "High — enables automated design of synthetic and biosynthetic pathways for novel molecules, greatly reducing manual search time and expanding feasible targets.",
            "comparison_to_alternatives": "Compared to traditional computer-aided retrosynthesis search algorithms; deep-learning + MCTS found more routes and was much faster, with better adherence to chemical intuition.",
            "success_factors": "Huge curated reaction datasets, expressive deep models to provide priors, and efficient integration with heuristic search (MCTS) to manage combinatorial explosion.",
            "key_insight": "Combining learned priors from large reaction corpora with tree search heuristics lets AI efficiently explore combinatorial retrosynthesis spaces and substantially outperforms classical rule-based search.",
            "uuid": "e2341.4"
        },
        {
            "name_short": "Enzyme-reaction prediction (Faulon / Mellor)",
            "name_full": "Machine learning methods to predict enzyme-reaction pairs and kinetic constants (SVMs, Gaussian Processes)",
            "brief_description": "Use of SVMs and Gaussian process models to predict whether a given enzyme sequence catalyzes a particular reaction (including Km predictions), leveraging sequence k-mer features and reaction signatures; GP offers uncertainty quantification.",
            "citation_title": "Molecular signatures-based prediction of enzyme promiscuity",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Enzyme function prediction / promiscuity / kinetics",
            "problem_description": "Identify candidate enzymes for reactions (including promiscuous activities) and estimate kinetic parameters (e.g., Km) to support pathway design when no canonical enzyme is known.",
            "data_availability": "Variable: datasets of enzyme-reaction pairs exist but smaller than sequence corpora; labeled examples created from known enzyme-reaction annotations; data may be noisy and incomplete.",
            "data_structure": "Paired data: protein sequence features (k-mers) and reaction signatures (functional group transformations); structured/feature-engineered tabular data.",
            "problem_complexity": "High: mapping from sequence/reaction signature to catalytic activity is complex, involves many-to-many relationships and context-dependence; predictive uncertainty important.",
            "domain_maturity": "Developing: machine learning provides complementary, computationally efficient alternatives to expensive DFT/QM calculations for screening enzyme candidates.",
            "mechanistic_understanding_requirements": "Medium: predictions used to prioritize candidates; mechanistic validation experimentally required; uncertainty quantification valuable.",
            "ai_methodology_name": "Support Vector Machines and Gaussian Process models (semi-supervised GP for enzyme search)",
            "ai_methodology_description": "Models use engineered features from protein sequences (k-mer counts) and reaction signatures (functional groups/transformations) to classify enzyme-reaction pairs as positive/negative and regress kinetic parameters; Gaussian processes provide probabilistic predictions with uncertainty estimates.",
            "ai_methodology_category": "Supervised learning (classification/regression); probabilistic modeling (GP)",
            "applicability": "Appropriate for screening candidate enzymes rapidly and prioritizing experimental validation, especially when detailed quantum chemistry is too costly.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Gaussian process models provided useful predictions and uncertainty measures for enzyme-reaction pairs and Km estimation; enabled prioritization of candidate enzymes for pathway design.",
            "impact_potential": "Moderate-to-high: accelerates enzyme selection for novel reactions and reduces computational/experimental costs by prioritizing promising candidates.",
            "comparison_to_alternatives": "Presented as computationally efficient alternatives to QM/DFT; GP advantages include uncertainty quantification over SVM/classical classifiers.",
            "success_factors": "Careful feature engineering (reaction signatures, sequence k-mers), probabilistic modeling for uncertainty, and integration with pathway design pipelines.",
            "key_insight": "Machine learning classifiers and probabilistic regressors can screen enzyme–reaction possibilities and estimate kinetic parameters rapidly, with uncertainty estimates enabling better experimental prioritization versus expensive mechanistic computations.",
            "uuid": "e2341.5"
        },
        {
            "name_short": "ML-assisted directed evolution (Wu et al., Romero/Arnold work)",
            "name_full": "Machine-learning-guided directed protein evolution and sequence–function modeling",
            "brief_description": "Use of supervised and ensemble ML models to learn protein sequence–function landscapes from screening data, enabling in silico evolution rounds and prioritized variant libraries that accelerate directed evolution.",
            "citation_title": "Machine learning-assisted directed protein evolution with combinatorial libraries",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Protein engineering / directed evolution",
            "problem_description": "Explore vast protein sequence space to discover variants with improved fitness (activity, stability, color, thermostability) while minimizing experimental rounds and screenings.",
            "data_availability": "Variable: experimental screening datasets (ranging from hundreds to hundreds of thousands of variants depending on study); labeled fitness measurements required for supervised learning.",
            "data_structure": "Sequence-to-fitness labeled data (tabular with encoded sequence features), sometimes large combinatorial libraries (hundreds-thousands of variants).",
            "problem_complexity": "Extremely high-dimensional combinatorial optimization with rugged, epistatic fitness landscapes; many local optima and sparse sampling limits.",
            "domain_maturity": "Established experimental practice (directed evolution) augmented recently by ML to reduce experimental burden; ML-guided methods are maturing rapidly.",
            "mechanistic_understanding_requirements": "Low-to-medium: ML aims to find higher-fitness variants; mechanistic interpretability helpful but not required for success.",
            "ai_methodology_name": "Ensembles of supervised models (linear, kernel, neural networks, Gaussian processes, random forests), in silico evolution guided by top models",
            "ai_methodology_description": "Train multiple supervised models on sequence–fitness data; select best-performing models to predict and rank new variants for experimental testing; use model-guided in silico evolution algorithms to propose candidate libraries with high predicted fitness, iteratively improving with new labeled data.",
            "ai_methodology_category": "Supervised learning + active learning / model-guided search",
            "applicability": "Highly applicable to protein engineering problems where moderate screening data exists; reduces required experimental rounds and broadens exploration of sequence space.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated successful evolution of improved enzymes/proteins across multiple studies (e.g., productivity, color, thermostability); ML-guided strategies enabled deeper exploration than greedy experimental heuristics.",
            "impact_potential": "High: reduces time and cost of protein engineering and enables discovery of variants that might be missed by standard directed evolution heuristics.",
            "comparison_to_alternatives": "Compared to traditional directed evolution (local search, single-site/saturation strategies), ML-guided in silico exploration provides more efficient search and often finds superior variants.",
            "success_factors": "Availability of quality labeled screening data, use of ensembles and model selection, iterative update with new experimental rounds, and combining multiple model types to hedge against model bias.",
            "key_insight": "Supervised ML trained on screening data can effectively model sequence–function landscapes and guide in silico evolution to accelerate directed evolution and find superior protein variants with fewer experiments.",
            "uuid": "e2341.6"
        },
        {
            "name_short": "UniRep / sequence representation",
            "name_full": "Unified rational protein engineering with sequence-based deep representation learning (UniRep)",
            "brief_description": "Recurrent neural network trained on tens of millions of protein sequences to learn internal representations (embeddings) that capture physicochemical and functional information useful for downstream protein property prediction and engineering.",
            "citation_title": "Unified rational protein engineering with sequence-based deep representation learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Protein representation learning / rational protein design",
            "problem_description": "Learn general-purpose numeric representations of protein sequences that encode structural/functional information to improve tasks like stability prediction and design with limited labeled data.",
            "data_availability": "Very large unlabeled sequence corpora used for pretraining (~24 million UniRef50 sequences); downstream labeled data for specific tasks can be limited but benefit from transfer.",
            "data_structure": "Long sequence data (amino acid sequences) used to train recurrent models; output is fixed-length numeric embeddings.",
            "problem_complexity": "High: sequences encode complex structural and functional relationships; transfer learning helps address scarcity of labeled data for specific tasks.",
            "domain_maturity": "Emerging: transfer learning and deep sequence models are a rapidly advancing approach in protein engineering.",
            "mechanistic_understanding_requirements": "Low-to-medium: embeddings are used for predictive tasks; some interpretability possible via downstream models using representations.",
            "ai_methodology_name": "Recurrent neural networks for large-scale unsupervised/representation learning (UniRep)",
            "ai_methodology_description": "Train RNNs on massive protein sequence corpora to learn sequence representations capturing evolutionary, physicochemical, and functional signals; use these embeddings as input features to simpler predictive models (random forest, linear) for downstream tasks like stability or mutation effect prediction.",
            "ai_methodology_category": "Unsupervised / self-supervised representation learning + transfer learning",
            "applicability": "Highly applicable for low-N protein engineering tasks where labeled data are scarce; representations transfer across protein families and tasks.",
            "effectiveness_quantitative": "Trained on ~24 million sequences; downstream applications showed improved prediction of stability and functional consequence of mutations (specific metrics not reproduced in review).",
            "effectiveness_qualitative": "Enabled prediction and optimization of protein stability and function across diverse proteins, demonstrating generalizability of learned representations.",
            "impact_potential": "High: representation learning reduces labeled-data requirements for many protein engineering tasks and improves model generalizability.",
            "comparison_to_alternatives": "Outperformed models trained from scratch on limited labeled data and provided better generalization versus classical feature-engineering approaches.",
            "success_factors": "Very large unlabeled sequence corpus for pretraining, effective RNN architecture, and leveraging embeddings with simple downstream predictors.",
            "key_insight": "Pretrained sequence embeddings capture rich protein information that transfers to diverse engineering tasks, enabling accurate predictions with limited labeled data.",
            "uuid": "e2341.7"
        },
        {
            "name_short": "ART (Automated Recommendation Tool)",
            "name_full": "A machine learning Automated Recommendation Tool for synthetic biology",
            "brief_description": "A tool tailored for metabolic engineering that uses Bayesian ensemble modeling and uncertainty quantification to recommend experimental designs from small datasets and iteratively guide DBTL cycles.",
            "citation_title": "A machine learning Automated Recommendation Tool for synthetic biology",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Design recommendation / experimental planning in metabolic engineering",
            "problem_description": "Recommend actionable strain designs (e.g., promoter combinations) to optimize chemical production using limited initial datasets, while quantifying prediction uncertainty.",
            "data_availability": "Targeted for small datasets: effective for problems with small training sets (&lt;hundreds of instances); supports both labeled design–response data and multi-omics features when available.",
            "data_structure": "Structured tabular data: categorical design variables (e.g., promoter combinations) and numeric multi-omics features; responses are scalar production measures.",
            "problem_complexity": "Moderate: small-data regime with potentially many features; requires uncertainty-aware models and ensemble strategies to avoid overfitting.",
            "domain_maturity": "Tooling for ML-guided DBTL is emerging; ART represents applied tooling addressing domain needs.",
            "mechanistic_understanding_requirements": "Medium: uncertainty quantification and explainability valued to build trust; mechanistic insight helpful but ART focuses on predictive recommendations.",
            "ai_methodology_name": "Bayesian ensemble modeling with uncertainty quantification (probabilistic ensemble / ART)",
            "ai_methodology_description": "Ensemble of models combined in a Bayesian framework to produce predictive point estimates and credible uncertainty intervals for responses; used to make recommendations balanced between predicted performance and uncertainty to guide iterative experimental cycles.",
            "ai_methodology_category": "Supervised learning (ensemble) with Bayesian uncertainty estimation; active recommendation",
            "applicability": "Highly applicable for small-data experimental design in metabolic engineering where uncertainty matters; designed to work with limited labeled instances and provide principled exploration–exploitation trade-offs.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Provides uncertainty-aware recommendations and has been demonstrated on synthetic and real datasets; helps choose exploratory versus exploitative experiments to build trust and improve models over DBTL cycles.",
            "impact_potential": "High for practical adoption in labs constrained by experimental budgets, by enabling principled, uncertainty-aware experimental planning and accelerating iterative improvement.",
            "comparison_to_alternatives": "Addresses shortcomings of single-model recommendations by explicitly quantifying uncertainty versus plain-point-prediction ensemble approaches.",
            "success_factors": "Design tailored for small datasets, emphasis on uncertainty quantification, integration with scikit-learn pipelines and DBTL workflows.",
            "key_insight": "In small-data, high-cost experimental settings, Bayesian ensembles that quantify uncertainty provide a practical and trustworthy way to recommend experiments and accelerate learning.",
            "uuid": "e2341.8"
        },
        {
            "name_short": "Seq-DeepCpf1 and DeepCpf1",
            "name_full": "Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity (Seq-DeepCpf1 / DeepCpf1)",
            "brief_description": "Deep convolutional neural networks trained on large-scale sgRNA activity datasets to predict on-target CRISPR-Cpf1 (AsCpf1) knockout efficacy; performance improves with dataset size and inclusion of epigenetic features.",
            "citation_title": "Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Genome editing guide design (CRISPR efficacy prediction)",
            "problem_description": "Predict on-target activity (indel frequency) of CRISPR guide RNAs to select highly effective guides and improve genome editing outcomes.",
            "data_availability": "Large-scale: the review cites training on &gt;15,000 target sequences for DeepCpf1 and 16,292 sgRNA sequences for Seq-DeepCpf1; generally requires &gt;10k high-quality labeled instances for deep models to improve performance.",
            "data_structure": "Sequence composition features of guides (nucleotide sequences), optionally epigenetic/contextual features (chromatin accessibility), labeled with measured activity rates (numeric).",
            "problem_complexity": "Moderate-high: sequence and chromatin context nonlinearly determine activity; off-target and cell-type differences add complexity.",
            "domain_maturity": "Growing maturity: many computational tools exist; deep learning approaches are recent and show improved generalizability with large datasets.",
            "mechanistic_understanding_requirements": "Medium: practical guide efficacy prediction prioritized, but interpretability and understanding off-target mechanisms are important for safety.",
            "ai_methodology_name": "Convolutional neural networks (deep learning) with additional biological feature integration",
            "ai_methodology_description": "CNNs trained on labeled sgRNA activity datasets to predict indel frequencies; models improved by expanding training data (&gt;10k instances) and by adding orthogonal input features like chromatin accessibility to improve generalization across cell types.",
            "ai_methodology_category": "Supervised deep learning (sequence regression/classification)",
            "applicability": "Appropriate when large, high-quality sgRNA activity datasets are available; benefits from inclusion of epigenetic/contextual features.",
            "effectiveness_quantitative": "Performance improved steadily as training data size increased; Seq-DeepCpf1 trained on 16,292 sgRNAs outperformed conventional ML algorithms (exact metrics not provided in review).",
            "effectiveness_qualitative": "DeepCpf1/Seq-DeepCpf1 outperformed classical ML approaches and generalized better to independent datasets; data quantity and richer feature sets improved accuracy.",
            "impact_potential": "High for improving CRISPR guide selection, reducing failed edits, and improving on-target efficacy across cell types.",
            "comparison_to_alternatives": "Outperformed conventional ML models trained on smaller datasets; showed that deep models scale with larger (&gt;10k) datasets.",
            "success_factors": "Large labeled datasets, use of deep CNNs appropriate for sequence data, and augmentation with relevant biological context (chromatin accessibility).",
            "key_insight": "Deep convolutional models trained on large, high-quality sgRNA activity datasets (and enriched with epigenetic features) substantially improve prediction and generalization of guide efficacy over classical ML.",
            "uuid": "e2341.9"
        },
        {
            "name_short": "DeepCRISPR",
            "name_full": "DeepCRISPR: optimized CRISPR guide RNA design by deep learning",
            "brief_description": "A hybrid approach combining unsupervised representation learning on billions of unlabeled sgRNA sequences and supervised deep neural networks with epigenetic features to predict both on-target efficacy and off-target profiles.",
            "citation_title": "DeepCRISPR: optimized CRISPR guide RNA design by deep learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "CRISPR guide design (on-target and off-target prediction)",
            "problem_description": "Simultaneously maximize on-target knockout activity and minimize off-target effects for sgRNA design across different cell types.",
            "data_availability": "Very large unlabeled pretraining corpus (~0.68 billion sgRNA sequences) used for unsupervised representation learning; labeled on-target (~200k) and off-target (~160k) datasets used for supervised training.",
            "data_structure": "Large-scale sequence data for unsupervised pretraining; labeled sequence + epigenetic/context features for supervised prediction tasks (on/off-target activities).",
            "problem_complexity": "High: both sequence-dependent and epigenetic factors influence activity and off-targeting; cross-cell-type generalization is challenging.",
            "domain_maturity": "Advanced computational development with increasing dataset availability enabling deep approaches; unsupervised pretraining is recent and powerful.",
            "mechanistic_understanding_requirements": "High for safety-critical applications: off-target minimization and interpretability matter, but prediction accuracy is primary for guide selection.",
            "ai_methodology_name": "Unsupervised representation learning + supervised deep convolutional denoising neural networks",
            "ai_methodology_description": "Unsupervised deep learning to learn sgRNA sequence representations from massive unlabeled data; use learned representations as inputs to supervised CNNs trained on labeled on-target/off-target datasets along with epigenetic features to predict activities with improved specificity and sensitivity.",
            "ai_methodology_category": "Self-supervised/unupervised pretraining + supervised deep learning",
            "applicability": "Highly applicable where massive unlabeled sequence pools and moderately large labeled datasets exist; addresses generalizability across cell types.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Outperformed classical ML methods and demonstrated high generalizability to other cell types; unsupervised pretraining automated feature discovery and improved downstream performance.",
            "impact_potential": "High impact for robust, generalizable CRISPR guide design with improved safety (reduced off-targets) and efficacy across contexts.",
            "comparison_to_alternatives": "Outperformed conventional ML and classical feature-engineered methods due to representation learning and large-scale pretraining.",
            "success_factors": "Availability of huge unlabeled sequences for pretraining, labeled activity datasets, integration of epigenetic/context features, and deep architectures capable of leveraging representations.",
            "key_insight": "Combining unsupervised representation learning on massive unlabeled genomic sequence corpora with supervised deep models and epigenetic features yields robust, generalizable CRISPR guide efficacy and off-target predictions.",
            "uuid": "e2341.10"
        },
        {
            "name_short": "Prosit",
            "name_full": "Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning",
            "brief_description": "A bi-directional RNN model that predicts peptide chromatographic retention time and tandem mass spectra from peptide sequences, improving proteomics identification workflows.",
            "citation_title": "Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Proteomics data processing / peptide identification",
            "problem_description": "Predict peptide tandem mass spectra and retention times from sequence to improve peptide identification and throughput in mass spectrometry proteomics.",
            "data_availability": "Moderate-large: trained on 550,000 tryptic peptides; labeled experimental spectra available for supervised training.",
            "data_structure": "Sequence data (peptides) mapped to spectral data (mass spectra) and retention times — multimodal numeric/time-series-like outputs.",
            "problem_complexity": "High: mapping sequence to spectral fragmentation patterns is complex and influenced by many physicochemical factors; noisy experimental measurements add difficulty.",
            "domain_maturity": "Maturing: proteomics has established pipelines; deep learning enhancements are recent and significantly improve identification accuracy.",
            "mechanistic_understanding_requirements": "Low-to-medium: pragmatic improvements in spectral prediction are the goal; detailed mechanistic fragmentation theory less required for improved identification.",
            "ai_methodology_name": "Bidirectional recurrent neural networks (RNN)",
            "ai_methodology_description": "Train bi-directional RNNs on large peptide–spectrum datasets to predict expected tandem mass fragmentation patterns and retention times; outputs used to improve peptide identification in proteomics pipelines.",
            "ai_methodology_category": "Supervised deep learning (sequence-to-spectrum regression)",
            "applicability": "Appropriate for mass-spectrometry based proteomics when sizable labeled spectra datasets are available; improves database search and identification sensitivity.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Enabled improved peptide identification quality and throughput in proteomics workflows, facilitating downstream ML and metabolic engineering analyses that rely on proteomics.",
            "impact_potential": "High for enhancing high-throughput proteomics data quality, which in turn improves ML models that depend on proteomic features for metabolic engineering.",
            "comparison_to_alternatives": "Reported improvements over classical spectrum prediction and heuristic methods, leveraging deep sequence-to-spectrum learning.",
            "success_factors": "Large labeled peptide–spectrum training sets, expressive RNN architecture, and integration into proteomics pipelines.",
            "key_insight": "Deep sequence-to-spectrum models can substantially improve proteomics identification, providing higher-quality input data for downstream ML-driven metabolic engineering tasks.",
            "uuid": "e2341.11"
        },
        {
            "name_short": "Bioprocess optimization (Coleman et al.)",
            "name_full": "An integrated approach to optimization of Escherichia coli fermentations using historical data",
            "brief_description": "A data-driven pipeline using decision-tree-based feature selection, ANN ensembles, and genetic algorithms to model and optimize fermentation outputs from historical bioprocess data.",
            "citation_title": "An integrated approach to optimization of Escherichia coli fermentations using historical data",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Bioprocess modeling and scale-up / fermentation optimization",
            "problem_description": "Use historical fermentation batch data to identify key process variables and optimize fermentation inputs (media, inoculum, operational parameters) to maximize biomass, product concentration, and productivity.",
            "data_availability": "Limited-moderate historical datasets: example used 69 fed-batch fermentations with 13 process features; data heterogeneous and noisy.",
            "data_structure": "Heterogeneous tabular data: categorical and continuous process variables, time-series measurements aggregated per batch; preprocessed to reduce dimensionality.",
            "problem_complexity": "High: multiparametric processes with temporal heterogeneity, batch effects, correlated inputs, and nonlinear relationships between process variables and outputs.",
            "domain_maturity": "Established industrial practice but often artisanal; ML-driven data mining of historical records is an established augmentation.",
            "mechanistic_understanding_requirements": "Medium: mechanistic process understanding useful but data-driven models can reveal actionable variable selections; combining both is beneficial.",
            "ai_methodology_name": "Decision tree feature selection + ANN ensembles + genetic algorithm optimization",
            "ai_methodology_description": "Decision trees used to preselect important process inputs from historical datasets to avoid overfitting; selected features trained using ANN ensembles to predict outputs; genetic algorithms applied to model predictions to identify input settings that maximize outputs.",
            "ai_methodology_category": "Supervised learning (ensemble) + heuristic optimization; feature selection preprocessing",
            "applicability": "Suitable for exploiting historical process data to improve operational settings where mechanistic models are incomplete; requires careful preprocessing to remove redundant/correlated inputs.",
            "effectiveness_quantitative": "Applied to 69 fermentations; produced actionable optimized input settings; specific numerical performance gains not reproduced in review summary.",
            "effectiveness_qualitative": "Demonstrated identification of important process variables and provided optimized settings that improved outputs in practice; highlighted need for preprocessing to avoid overfitting.",
            "impact_potential": "Practical impact on reducing trial-and-error in scale-up and process development, enabling extraction of insights from archival datasets.",
            "comparison_to_alternatives": "Compared favorably to naive modeling approaches; emphasized feature selection to outperform overfit models on small archival datasets.",
            "success_factors": "Historical dataset availability, robust feature selection to avoid overfitting, ensemble models for prediction, and optimization algorithms to convert models into actionable process settings.",
            "key_insight": "Feature-selected ensemble models trained on curated historical process data can drive practical fermentation optimization and identify non-obvious influential process variables despite noisy heterogeneous data.",
            "uuid": "e2341.12"
        },
        {
            "name_short": "Reinforcement learning for bioprocess control (Treloar et al.)",
            "name_full": "Deep reinforcement learning for the control of microbial co-cultures in bioreactors",
            "brief_description": "Application of reinforcement learning (fitted Q-learning / policy gradient variants) to learn control policies for bioreactor processes (e.g., species composition, temperature control) from online data to optimize production goals.",
            "citation_title": "Deep reinforcement learning for the control of microbial co-cultures in bioreactors",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Bioprocess control / bioreactor optimization",
            "problem_description": "Control dynamical bioreactor environments (e.g., co-culture composition, temperature, feed rates) to achieve desired production metrics or species abundances using sequential decision policies learned from interaction data.",
            "data_availability": "Varies: online continuous measurements can be abundant during an experiment (e.g., measurements every 5 min) but collecting many trials for RL can be costly; many RL demos are in silico or in simulated settings.",
            "data_structure": "Time-series control and measurement streams (continuous online signals) with discrete/continuous action spaces; high temporal resolution.",
            "problem_complexity": "High: non-stationary, stochastic dynamics with delayed outcomes; large state and action spaces; sample inefficiency is a central challenge.",
            "domain_maturity": "Early-stage in practice for bioprocess control; traditional PID/MPC dominate but RL shows promise in research and simulation.",
            "mechanistic_understanding_requirements": "Medium-high: operational safety and stability demand interpretable and reliable control policies; integrating mechanistic models helps sample efficiency.",
            "ai_methodology_name": "Model-free reinforcement learning algorithms (fitted Q-learning, deep deterministic policy gradients, policy gradient with RNNs, multi-step Q-learning variants)",
            "ai_methodology_description": "RL algorithms learn control policies mapping observed reactor states to control actions by maximizing cumulative reward (e.g., product yield) using trial-and-error interaction data; implementations include neural-network parameterized policies, recurrent policies for temporal dependencies, and transfer learning from mechanistic simulators to real online adaptation.",
            "ai_methodology_category": "Reinforcement learning (model-free and hybrid with transfer learning)",
            "applicability": "Applicable where online instrumentation provides high-frequency measurements and safe experimental iteration is possible; sample efficiency and safety constraints limit immediate industrial deployment without hybridization or transfer learning.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "RL controllers in simulations and some lab demonstrations achieved improved control (lower overshoot, faster tracking, smoother control) compared to PID/MPC baselines; however, RL needs many interactions and can be impractical without simulation pretraining or hybrid approaches.",
            "impact_potential": "Potentially transformative for adaptive, data-driven bioprocess control, especially when combined with mechanistic simulators or transfer learning to reduce required real-world data.",
            "comparison_to_alternatives": "Compared to advanced PID and MPC methods; RL achieved smoother and faster control in examples but required more data and benefited from model hybridization or pretraining.",
            "success_factors": "Availability of high-frequency online measurements, simulation environments for pretraining/transfer learning, and hybrid approaches combining mechanistic controllers to improve sample efficiency.",
            "key_insight": "Reinforcement learning can learn superior control policies for complex bioprocesses but practical deployment needs improved sample efficiency via transfer learning or integration with mechanistic models.",
            "uuid": "e2341.13"
        },
        {
            "name_short": "Mechanism-aware ML (Culley et al.)",
            "name_full": "A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",
            "brief_description": "Hybrid approach incorporating mechanistic model-derived features (e.g., FBA/pFBA-predicted fluxes) alongside transcriptomic data into ML models to improve prediction of growth phenotypes.",
            "citation_title": "A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Phenotype prediction / integration of mechanistic and data-driven models",
            "problem_description": "Predict yeast growth rate from transcriptomic profiles augmented with features derived from genome-scale metabolic models to improve predictive accuracy.",
            "data_availability": "Moderate: transcriptomics datasets and genome-scale model simulations available; labeled phenotype data (growth rates) needed.",
            "data_structure": "High-dimensional transcriptomic features combined with mechanistic model outputs (predicted fluxes) — multimodal fused feature set.",
            "problem_complexity": "High: mapping from expression to phenotype is nonlinear and context-dependent; high dimensionality risks overfitting without meaningful feature engineering.",
            "domain_maturity": "Maturing: integration of GSMs and ML is an active area; mechanistic models exist but have limited quantitative accuracy alone.",
            "mechanistic_understanding_requirements": "High: combining mechanistic features provides interpretability and biological plausibility; mechanistic constraints help ensure biologically meaningful ML predictions.",
            "ai_methodology_name": "Neural networks trained on combined omics and genome-scale model-derived flux features",
            "ai_methodology_description": "Compute flux predictions from parsimonious flux balance analysis (pFBA) and use these flux features together with transcriptomics as inputs to neural networks to predict growth rate; hybrid feature set improved predictive performance versus transcriptomics alone.",
            "ai_methodology_category": "Supervised learning (hybrid mechanistic + data-driven)",
            "applicability": "Appropriate where mechanistic GSMs exist and can generate informative derived features; helps small-data regimes by providing informative priors.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Using pFBA-predicted fluxes as additional features improved neural network predictive power relative to using transcriptomics alone, demonstrating benefit of hybridization.",
            "impact_potential": "Promising for improving phenotype prediction and making ML outputs more biologically interpretable and robust, aiding metabolic engineering decisions.",
            "comparison_to_alternatives": "Shown to outperform pure data-driven models on the studied task when mechanistic features were included.",
            "success_factors": "Quality of mechanistic GSM, meaningful flux predictions from pFBA, and effective fusion of mechanistic and omics features in ML models.",
            "key_insight": "Augmenting ML models with mechanistic model–derived features can significantly improve predictive accuracy and biological plausibility in phenotype prediction tasks.",
            "uuid": "e2341.14"
        },
        {
            "name_short": "kcat prediction (Heckmann et al.)",
            "name_full": "Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models",
            "brief_description": "Use of ML to predict enzyme catalytic turnover numbers (kcat) from network, structural, and biochemical features, then using predicted kcat to parameterize genome-scale models and improve proteome/flux predictions.",
            "citation_title": "Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Kinetic parameter prediction / genome-scale metabolic modeling",
            "problem_description": "Predict enzyme catalytic rates (kcat) where experimental measurements are missing and use these predictions to parametrize mechanistic models for improved flux/proteome predictions.",
            "data_availability": "Limited for kcat measurements but augmented by features: ML trained on datasets combining known kcat with enzyme structural, network, biochemical, and assay condition features.",
            "data_structure": "Structured tabular features aggregating enzyme structural properties, network context, biochemistry, and assay metadata; labeled with experimental kcat values.",
            "problem_complexity": "High: kcat depends on multiple structural and contextual factors; noisy labels and scarcity of measured kcat complicate regression.",
            "domain_maturity": "Growing: GSMs are mature but lack reliable kinetic parameterization; ML approaches provide practical means to fill gaps.",
            "mechanistic_understanding_requirements": "High: ultimate goal is mechanistically meaningful parameterization of models; ML predictions are used as inputs to mechanistic models and validated for plausibility.",
            "ai_methodology_name": "Ensemble of supervised ML regressors (various models applied to predict kcat)",
            "ai_methodology_description": "Train machine learning models on enzyme turnover numbers using a wide set of features (structural correlates, network metrics, assay conditions) to predict kcat, then use predicted values to parameterize genome-scale metabolic or proteome-constrained models to improve predictions of proteome allocation and fluxes.",
            "ai_methodology_category": "Supervised regression (hybrid ML-mechanistic pipeline)",
            "applicability": "Applicable when mechanistic models lack kinetic parameters; ML can impute missing kcat values to enable more quantitative simulations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Predicted turnover numbers captured structural correlates and improved proteome predictions when used to parametrize GSMs, demonstrating utility of ML-derived parameters.",
            "impact_potential": "High: enables more quantitative genome-scale and proteome-constrained modeling across organisms and conditions by filling kinetic data gaps.",
            "comparison_to_alternatives": "Offers data-driven parameter estimation as an efficient complement to laborious experimental kcat measurement and computationally intensive QM approaches.",
            "success_factors": "Rich engineered feature set combining structural and network properties, and validation by improved mechanistic model predictions.",
            "key_insight": "ML can predict enzyme kinetic parameters from heterogeneous feature sets and meaningfully improve mechanistic metabolic model parameterization and downstream predictions.",
            "uuid": "e2341.15"
        },
        {
            "name_short": "Metabolome-from-proteome (Zelezniak et al.)",
            "name_full": "Machine learning predicts the yeast metabolome from the quantitative proteome of kinase knockouts",
            "brief_description": "Multilinear regression and ML approaches to predict metabolite concentrations from protein levels, leveraging network proximity (closest enzyme neighbors) as features to infer metabolome changes in mutants.",
            "citation_title": "Machine learning predicts the yeast metabolome from the quantitative proteome of kinase knockouts",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Metabolomics prediction / genotype-to-metabolome mapping",
            "problem_description": "Predict intracellular metabolite concentrations (responses) from quantitative proteomics (inputs), aiming to infer metabolic consequences of genetic perturbations.",
            "data_availability": "Moderate: proteome measurements from kinase knockout panels and matched metabolomics available; labeled paired proteome–metabolome datasets used for supervised regression.",
            "data_structure": "High-dimensional numeric proteomics features paired with metabolite concentration responses; network-structured relationships used to select features (nearest enzyme neighbors).",
            "problem_complexity": "High: metabolite levels are determined by network fluxes, enzyme activities, regulation, and cellular state; many-to-many mappings and possible hidden variables.",
            "domain_maturity": "Emerging: integrated proteome–metabolome predictive modeling is an advancing area with increasing multi-omic datasets.",
            "mechanistic_understanding_requirements": "High for interpretation: predictions aim to reflect biochemical network relationships and suggest mechanistic hypotheses.",
            "ai_methodology_name": "Multilinear regression augmented with network-constrained feature selection and other ML methods",
            "ai_methodology_description": "Express metabolite concentrations as functions of expression levels of proximate enzymes in the metabolic network (multilinear regression), optionally augmented by other ML techniques to capture nonlinearity; utilize network topology to choose meaningful input features.",
            "ai_methodology_category": "Supervised learning (regression) with network-informed feature engineering",
            "applicability": "Appropriate when paired proteomics and metabolomics exist and network maps are available to constrain feature choices; helps generate mechanistic hypotheses.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Able to predict metabolite concentration changes from proteome changes for yeast mutants and to propose candidate mechanistic links; demonstrates value of network-informed ML.",
            "impact_potential": "Medium-high: supports prioritization of follow-up experiments and helps extract mechanistic hypotheses from multi-omic datasets.",
            "comparison_to_alternatives": "Network-constrained regression provides an interpretable alternative to black-box models and helps focus on biologically plausible predictors.",
            "success_factors": "High-quality paired multi-omics data, curated metabolic network maps for feature selection, and emphasis on interpretable modeling.",
            "key_insight": "Incorporating metabolic network structure into supervised models enables more accurate and interpretable prediction of metabolite levels from proteomic data, bridging data-driven and mechanistic perspectives.",
            "uuid": "e2341.16"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A machine learning approach to predict metabolic pathway dynamics from time-series multiomics data",
            "rating": 2,
            "sanitized_title": "a_machine_learning_approach_to_predict_metabolic_pathway_dynamics_from_timeseries_multiomics_data"
        },
        {
            "paper_title": "In vitro prototyping and rapid optimization of biosynthetic enzymes for cell design",
            "rating": 2,
            "sanitized_title": "in_vitro_prototyping_and_rapid_optimization_of_biosynthetic_enzymes_for_cell_design"
        },
        {
            "paper_title": "DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns",
            "rating": 2,
            "sanitized_title": "deepribo_a_neural_network_for_precise_gene_annotation_of_prokaryotes_by_combining_ribosome_profiling_signal_and_binding_site_patterns"
        },
        {
            "paper_title": "Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers",
            "rating": 2,
            "sanitized_title": "deep_learning_enables_highquality_and_highthroughput_prediction_of_enzyme_commission_numbers"
        },
        {
            "paper_title": "Planning chemical syntheses with deep neural networks and symbolic AI",
            "rating": 2,
            "sanitized_title": "planning_chemical_syntheses_with_deep_neural_networks_and_symbolic_ai"
        },
        {
            "paper_title": "Reinforcement learning for bioretrosynthesis",
            "rating": 2,
            "sanitized_title": "reinforcement_learning_for_bioretrosynthesis"
        },
        {
            "paper_title": "Machine learning-assisted directed protein evolution with combinatorial libraries",
            "rating": 2,
            "sanitized_title": "machine_learningassisted_directed_protein_evolution_with_combinatorial_libraries"
        },
        {
            "paper_title": "Unified rational protein engineering with sequence-based deep representation learning",
            "rating": 2,
            "sanitized_title": "unified_rational_protein_engineering_with_sequencebased_deep_representation_learning"
        },
        {
            "paper_title": "A machine learning Automated Recommendation Tool for synthetic biology",
            "rating": 2,
            "sanitized_title": "a_machine_learning_automated_recommendation_tool_for_synthetic_biology"
        },
        {
            "paper_title": "Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity",
            "rating": 2,
            "sanitized_title": "deep_learning_improves_prediction_of_crisprcpf1_guide_rna_activity"
        },
        {
            "paper_title": "DeepCRISPR: optimized CRISPR guide RNA design by deep learning",
            "rating": 2,
            "sanitized_title": "deepcrispr_optimized_crispr_guide_rna_design_by_deep_learning"
        },
        {
            "paper_title": "Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning",
            "rating": 2,
            "sanitized_title": "prosit_proteomewide_prediction_of_peptide_tandem_mass_spectra_by_deep_learning"
        },
        {
            "paper_title": "An integrated approach to optimization of Escherichia coli fermentations using historical data",
            "rating": 2,
            "sanitized_title": "an_integrated_approach_to_optimization_of_escherichia_coli_fermentations_using_historical_data"
        },
        {
            "paper_title": "Deep reinforcement learning for the control of microbial co-cultures in bioreactors",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_for_the_control_of_microbial_cocultures_in_bioreactors"
        },
        {
            "paper_title": "Machine learning predicts the yeast metabolome from the quantitative proteome of kinase knockouts",
            "rating": 2,
            "sanitized_title": "machine_learning_predicts_the_yeast_metabolome_from_the_quantitative_proteome_of_kinase_knockouts"
        },
        {
            "paper_title": "Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models",
            "rating": 2,
            "sanitized_title": "machine_learning_applied_to_enzyme_turnover_numbers_reveals_protein_structural_correlates_and_improves_metabolic_models"
        }
    ],
    "cost": 0.04446024999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Lawrence Berkeley National Laboratory Recent Work Title Machine learning for metabolic engineering: A review. Publication Date Machine learning for metabolic engineering: A review
2021</p>
<p>Christopher E Lawson 
Jose Manuel Martí 
Tijana Radivojevic 
Christopher E Lawson 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>Jose Manuel Martí 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>DOE Agile BioFoundry
94608EmeryvilleCAUSA</p>
<p>Tijana Radivojevic 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>DOE Agile BioFoundry
94608EmeryvilleCAUSA</p>
<p>Sai Vamshi 
R Jonnalagadda 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>DOE Agile BioFoundry
94608EmeryvilleCAUSA</p>
<p>Reinhard Gentz 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>Computational Research Division
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Nathan J Hillson 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>DOE Agile BioFoundry
94608EmeryvilleCAUSA</p>
<p>Sean Peisert 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Computational Research Division
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>University of California Davis
95616DavisCAUSA</p>
<p>Joonhoon Kim 
Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>Pacific Northwest National Laboratory
99354RichlandWAUSA</p>
<p>Blake A Simmons 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>DOE Agile BioFoundry
94608EmeryvilleCAUSA</p>
<p>Christopher J Petzold 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>DOE Agile BioFoundry
94608EmeryvilleCAUSA</p>
<p>Steven W Singer 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>Aindrila Mukhopadhyay 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>Environmental Genomics and Systems Biology Division
Lawrence Berkeley National Laboratory
USA</p>
<p>Deepti Tanjore 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Advanced Biofuels and Bioproducts Process Development Unit
94608EmeryvilleCAUSA</p>
<p>Joshua G Dunn 
Ginkgo Bioworks
02210BostonMAUSA</p>
<p>Hector Garcia Martin 
Biological Systems and Engineering
Lawrence Berkeley National Laboratory
94720BerkeleyCAUSA</p>
<p>Joint BioEnergy Institute
94608EmeryvilleCAUSA</p>
<p>DOE Agile BioFoundry
94608EmeryvilleCAUSA</p>
<p>Basque Center for Applied Mathematics
48009BilbaoSpain</p>
<p>Environmental Genomics and Systems Biology Division
Lawrence Berkeley National Laboratory
USA</p>
<p>Lawrence Berkeley National Laboratory Recent Work Title Machine learning for metabolic engineering: A review. Publication Date Machine learning for metabolic engineering: A review</p>
<p>Metabolic Engineering
63202110.1016/j.ymben.2020.10.005Permalink https://escholarship.org/uc/item/9pm0x5mh 2021 Peer reviewed eScholarship.org Powered by the California Digital Library University of California 1096-7176/ A R T I C L E I N F O Keywords: Machine Learning Metabolic Engineering Synthetic Biology Deep Learning A B S T R A C T
Machine learning provides researchers a unique opportunity to make metabolic engineering more predictable. In this review, we offer an introduction to this discipline in terms that are relatable to metabolic engineers, as well as providing in-depth illustrative examples leveraging omics data and improving production. We also include practical advice for the practitioner in terms of data management, algorithm libraries, computational resources, and important non-technical issues. A variety of applications ranging from pathway construction and optimization, to genetic editing optimization, cell factory testing, and production scale-up are discussed. Moreover, the promising relationship between machine learning and mechanistic models is thoroughly reviewed. Finally, the future perspectives and most promising directions for this combination of disciplines are examined.</p>
<p>Introduction</p>
<p>Metabolic engineering is enjoying an auspicious moment, when its potential is becoming evident in the form of many commercially available products with undeniable impact on society. This discipline has produced: synthetic silk for clothing (Hahn, 2019;Johansson et al., 2014), meatless burgers that taste like meat because of bioengineered heme ("Meat-free outsells beef," 2019), synthetic human collagen for cosmetic purposes ("Geltor unveils first biodesigned human collagen for skincare market", 2019), antimalarial and anticancer drugs (Ajikumar et al., 2010;Paddon and Keasling, 2014), the fragance of recovered extinct flowers (Kiedaisch, 2019), biofuels (Hanson, 2013;Peralta-Yahya et al., 2012), hoppy flavored beer produced without hops (Denby et al., 2018), and synthetic cannabinoids (Dolgin, 2019;Luo et al., 2019), among others. Since the number of possible metabolites is enormous, we can only expect these successes to significantly increase in number in the future. Traditional approaches, however, limit metabolic engineering to the usual 5-15 gene pathway, whereas full genome-scale engineering holds the promise of much more ambitious and rigorous biodesign of organisms. Genome-scale engineering involves multiplex DNA editing that is not limited to a single gene or pathway, but targets the full genome (Bao et al., 2018;Esvelt and Wang, 2013;Garst et al., 2017;Liu et al., 2015;Si et al., 2017). This approach can open the field of metabolic engineering to stunning new possibilities: engineering of microbiomes for therapeutic or bioremediation uses (Lawson et al., 2019), designing of multicellular organisms as biomaterials that match a specification (Islam et al., 2017), ecosystem engineering (Hastings et al., 2007), and perhaps even fusion of physical and biological systems. None of these examples are likely to become reality through a traditional trial-and-error approach: the number of genetic part combinations that could produce these outcomes is a vanishingly small fraction of the total possible. For example, engineering a microbiome to produce a medical drug involves not only introducing and balancing the corresponding pathway in one or more of the microbiome species, but also modifying internal regulatory networks so as to keep the community stable and robust to external perturbations. Even for the case of single pathways and teams of highly-trained experts, the trial-and-error approach is hardly sustainable, since it results in very long development times: for example, it took Amyris an estimated 150 person-years of effort to produce the immediate precursor of the antimalarial artemisinin, and Dupont 575 person-years to generate propanediol (Hodgman and Jewett, 2012). An approach that pinpoints the designs that match a desired specification is needed.</p>
<p>The main challenge in more sophisticated biodesign is, arguably, our inability to accurately predict the outcomes of bioengineering (Carbonell et al., 2019;Lopatkin and Collins, 2020). New technologies provide markedly easier ways to make the desired DNA changes, but the final result on cell behavior is usually unpredictable (Gardner, 2013). If metabolic engineering is "the science of rewiring the metabolism of cells to enhance production of native metabolites or to endow cells with the ability to produce new products" (Nielsen and Keasling, 2016), the ability to engineer a cell to a specification (e.g. a given titer, rate and yield of a desired product) is critical for this purpose. Only the ability to accurately predict the performance of a genetic design can avoid an arduous trial-and-error approach to reach that specification.</p>
<p>Moreover, while the flourishing offshoots of the genomic revolution provide powerful new capabilities to discover new DNA sequences, understand their function, and modify them, it is not trivial to harness these technologies productively. The genomic revolution has provided the DNA code as a condensed set of cell instructions that constitutes the main engineering target, and functional genomics to understand the cell behavior. Furthermore, the cost for these data is rapidly decreasing: sequencing cost decreases faster than Moore's law, transcriptomic data grow exponentially (Stephens et al., 2015), and high-throughput workflows for proteomics and metabolomics are slowly becoming a reality Zampieri et al., 2017). But many researchers find themselves buried in this "deluge of data": there seems to be more data than time to analyze them. Furthermore, data come in many different types (genomics, transcriptomics, proteomics, metabolomics, protein interaction maps, etc), complicating their analysis. As a result, analysis of functional genomics data often does not yield sufficient insights to infer actionable strategies to manipulate DNA for a desired phenotype. Moreover, CRISPR-based tools (Doudna and Charpentier, 2014;Knott and Doudna, 2018) provide easy DNA editing and metabolic perturbations (e.g. CRISPRi (Tian et al., 2019)). These tools provide the potential to perform genome-wide manipulations in model systems (Wang et al., 2018), and a growing number of hosts (Peters et al., 2019). However, it is not clear how to prioritize the possible targets. Rational engineering approaches have proven useful in the past (George et al., 2015;Kang et al., 2019;Tian et al., 2019), but the detailed knowledge of a pathway can produce on the order of tens of targets, whereas CRISPR-based tools can reach tens of thousands of genome sites (Bao et al., 2018;Bassalo et al., 2018;Garst et al., 2017;Gilbert et al., 2014).</p>
<p>Machine learning (ML) is a possible solution to these problems. Machine learning can systematically provide predictions and recommendations for the next steps to be implemented through CRISPR (or other methods (Paschon et al., 2019;Reyon et al., 2012;Wang et al., 2019)), and it can use the exponentially growing amounts of functional genomics data to systematically improve its performance. Machine learning has already proven its utility in many other fields: self-driving cars (Duarte and Ratti, 2018), automated translation  , face recognition (Voulodimos et al., 2018), natural language parsing (Kreimeyer et al., 2017), tumor detection (Paeng et al., 2016), and explicit content detection in music lyrics (Chin et al., 2018), among others. It has the potential to produce similar breakthroughs in metabolic engineering.</p>
<p>However, a change in perspective is required regarding the relative importance of molecular mechanisms. Whereas the machine learning paradigm concentrates on enabling predictive power, metabolic engineers typically define scientific value around the understanding of genetic or molecular mechanisms (see section 4.0). Nonetheless, the biological sciences (including computational biology) have been particularly challenged to make accurate quantitative predictions of complex systems from known and tested mechanisms. Hence, if accurate quantitative predictions are needed for a more transformative metabolic engineering, it may be desirable to shift some of the emphasis from identifying molecular mechanisms into enabling data-driven approaches. This apparent detour may, in the end, more efficiently produce mechanistic models, if we combine the predictive power of machine learning with the insight of molecular mechanisms (Heo and Feig, 2020).</p>
<p>In this review we provide an explanation of machine learning in metabolic engineering terms, in the hopes of providing a bridge between both disciplines. We explore the promises of machine learning, as well as its current pitfalls, provide examples of how it has been used so far, as well as auspicious future uses. In short, we will make the case that machine learning can take metabolic engineering to the next step in its maturation as a discipline, but it requires a conscious choice to understand its limitations and potential.</p>
<p>Demystifying machine learning for bioengineers</p>
<p>What is machine learning?</p>
<p>Machine learning is a subdiscipline of Artificial Intelligence (AI), which attempts to emulate how a human brain understands, and interacts with, the world (Fig. 1). A fully functioning AI would enable us to perform the same processes as human metabolic engineers: choose the best molecules to produce, suggest possible pathways to produce it, select the right pathway design to obtain the desired titers, rates and yield, and interpret the resulting experimental data to troubleshoot the metabolic engineering effort. A fully functioning AI would of course be useful for many other tasks such as: fully autonomous cars and planes, recommending medical treatments, directing agricultural practices, reading and summarizing texts like a human, automating translations from different human languages, and producing music and movies. Obviously, we do not yet have full functioning AIs (or strong AI or artificial general intelligence as it is often referred to (Pei et al., 2019;Walch, 2019 )), and it is a continuing debate whether we will ever have them (Melnyk, 1996), but AI approaches have been quite successful in some bounded tasks such as playing chess and Go better than humans (Silver et al, 2016(Silver et al, , 2018, or predicting protein structures from sequence (AlQuraishi, 2019). Since AI and machine learning are generally applicable tools, some of these partial successes can be very useful for metabolic engineering (see section 3 for examples).</p>
<p>Machine learning is the study of computer algorithms that seek to improve automatically through experience (i.e. learning), often by training on supervised examples (Fig. 2), also known as supervised machine learning. This works by statistically linking an input to its associated response for several different examples: e.g. promoter choice for a pathway and the corresponding final production, protein sequence and its function, etc (Figs. 2 and 3). It is important to realize that the emphasis is set in predicting the response, rather than produce mechanistic understanding. In fact, the algorithm linking input and response is not meant to represent a mechanistic understanding of the underlying processes: for example, modeling the full process of promoters causing the expression of proteins that code enzymes which then catalyze reactions that transform metabolites and result in a predicted production. Rather, the algorithm is chosen to be as expressive as possible to be able to learn any relationship between input and response. Hence, none of the biological information is encoded in the algorithm; all the biological information is provided by the training data, which must be carefully selected (supervised) so the algorithm can learn the desired relationship (promoters to production, protein sequence to function, etc.), generalize it, and be able to predict it for new inputs that were not in the training set (Fig. 3). This difference is crucial with respect to traditional metabolic engineering and microbiology, where understanding the mechanism is considered of paramount importance (see section 2.2.1 for a specific example). In machine learning, we can see the situation in which we can predict that, e.g., a given promoter choice will have the best production, but we cannot explain the metabolic mechanism that provides that optimal production . This state of affairs has its pros and cons, and efforts have been made to introduce biological prior knowledge in the algorithms (see section 4).</p>
<p>There is a continuous interplay between the complexity of a supervised machine learning algorithm and the amount of data available to train it (Fig. 4). If the model/algorithm is not expressive enough (not enough parameters), it will be unable to describe the data accurately (underfitting). If the model displays much more parameters than data instances are available, it will just "memorize" the training data set rather than grasp the underlying general patterns required to predict new inputs (overfitting). In this case, the algorithm will produce exceedingly good results for the training set, but very poor ones for any new input that is used as a test (Figs. 3 and 4). Cross validation (Fig. 3) provides an effective way to choose the number of parameters: both overfitting and underfitting result in very poor predictions.</p>
<p>There are many supervised machine learning algorithms available in the public domain: linear regressions, quadratic regressions, random forest, support vector machines, neural networks, Gaussian process regressors, gradient boosting regressors (the popular library scikit-learn provides a good starting point with an extensive list and explanations (Pedregosa et al., 2011)). To give a concrete example, a classic machine learning algorithm is the decision tree, that can be used, for example, to predict which protein expression levels result in high production (Fig. 5). As can be observed, this algorithm represents a high-level abstraction of how humans are believed to think. Because no single algorithm is best for every learning task (Wolpert, 1996), a significant endeavor when applying machine learning is choosing the optimal algorithm for your problem (and its hyperparameters, see Fig. 5). Ensemble modeling is an alternative approach that sidesteps the challenge of model selection (Radivojević et al., 2020). Ensemble modeling takes the input of various different models and has them "vote" for a particular prediction. Based on their performance, a different weight is assigned to each algorithm. The examples of the random forest algorithm (Ho, 1995) or the super learner algorithm (van der Laan et al., 2007) have demonstrated that even very simple models can increase their performance significantly by using an ensemble of them (e.g., several decision trees in a random forest algorithm).</p>
<p>Learning without supervision also constitutes an important part of Deep learning. Machine learning is a subdiscipline of Artificial Intelligence, which attempts to reproduce how human brains think. Symbolic AI (or Good Old Fashioned AI, or GOFAI), is a part of AI devoted to reproduce thought through symbolic representations of the world. In contrast, machine learning mimics thought using algorithms that learn a task (e.g., identify a dog) through learning from data. GOFAI was dominant in the early states of AI (50s-80s) but has now lost relative influence. Machine learning, however, is now the dominant branch of AI and focuses on improving performance through the acquisition of experience in terms of data. Among the many possible algorithmic approaches in machine learning, neural networks ( Fig. 7) have become most popular since ~2010 because their performance seems not to saturate as easily as other methods (Fig. 8). Neural networks with many layers (Fig. 8) are called deep neural networks, and constitute the basis for Deep Learning.</p>
<p>Fig. 2.</p>
<p>Machine learning basics. Supervised Machine learning algorithms define learning in a narrow way: the ability to predict a response (e.g. the target compound production) from a set of inputs (e.g. protein concentrations for a pathway). The inputs (or features) and response (or output) can be numbers (e. g. protein concentrations) or categories (e.g. different available promoters). All supervised machine learning algorithms follow this general architecture. Because the algorithm linking input and response does not include mechanistic information, but is rather chosen to be as expressive as possible, machine learning can predict relationships between really diverse inputs and outputs: e. g., production and enzyme choice (see section 2.2.2), metabolite rate change and multiomics measurements (section 2.2.1), or protein sequence and protein function. The supervision consists in providing training data consisting of the input and the associated response. This labeling of the input data to teach the algorithm the right associations is the step that is most arduous and costly, particularly for large data sets. This has prompted AI researchers to develop methods that do not require this step (Fig. 6).</p>
<p>Fig. 3.</p>
<p>Machine learning terminology. The standard workflow for supervised machine learning involves first using a training data set (including the inputs, or features, and the corresponding responses, or labels) to train the chosen algorithm. The training data set is composed of instances or examples of the inputs and response to be learnt. Instances depend on the problem to be learnt: they could be different strains and conditions (section 2.2.2 example), time points (section 2.2.1 example) or different proteins. The goal is for the algorithm to be able to predict the response for inputs that it has never seen before (i.e. were not in the training set), which is the ultimate test of its performance. A way to foresee how the algorithm will perform under such a test is to use only part of the training data set (all data except red overlay) for training, and then check the predictions for the remaining inputs (red overlay), to be compared with the known responses. This procedure is called validation and, if performed several times by randomly holding out a fraction of the training data set, it takes the name of cross validation. A 10-fold cross-fold validation, for example, randomly holds out 10% of the training set to test predictions for several draws. Cross validation is a good way to determine the needed algorithm complexity needed (Fig. 4). (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)</p>
<p>Fig. 4.</p>
<p>Model complexity vs data availability. The number of parameters (model characteristics that can be changed to fit data, see Fig. 5) in a model provides an idea of its complexity (more parameters → more complex). If the number of parameters is much smaller than the number of instances, the model cannot hope to describe the training data (underfit model). This can happen with "long and skinny" training data: few inputs and many instances. If the number of parameters is much bigger than the instances, the model but will be unable to generalize beyond the training set. The solution for the underfitting case is straightforward: increase the complexity of the model (number of parameters). The solution for the overfitting case is reducing the model parameters. However, if the number of inputs/features is high, it may be impossible to do so. This is often the case in metabolic engineering, where omics data sets displaying tens of thousands of features are available, but only for ~100 instances ("short and fat" training data). It becomes imperative then to choose the most informative features through the feature selection methods provided by unsupervised learning (Fig. 6). This feature selection is needed to avoid the "curse of dimensionality": i.e., the amount of data needed to support results in a statistically sound fashion often grows exponentially with the dimensionality. Poor cross validation scores (Fig. 3) can help identify both overfitting and underfitting.</p>
<p>machine learning, given the significant effort involved in creating labeled data sets. The areas of machine learning focused on this challenge are unsupervised learning and reinforcement learning. Unsupervised learning searches for patterns in a data set with no pre-existing labels, requires only minimal human supervision, and often attempts to create clusterings or representations that aid human understanding or reduce dimensionality (Fig. 6). Examples of unsupervised machine learning algorithms include Principal Component Analysis (PCA), Kmeans clustering (Sculley, 2010), and Single Value Decomposition (Manning et al., 2008). Familiar examples in metabolic engineering include identifying patterns in metabolomics profiles that distinguish between different types of cells: healthy vs. sick (Sajda, 2006), stressed vs. non-stressed (Luque de Castro and Priego-Capote, 2018; Mamas et al., 2011), or high-producing vs low-producing (Alonso-Gutierrez et al., 2015). Reinforcement learning represents a different paradigm regarding learning from experience that posits that humans learn not from properly labeled examples, but rather from interacting and probing their environment. Hence, the aim of reinforcement learning is to use experience and data to update an internal policy that optimizes a desired goal (Fig. 6). A prime example of this approach (Treloar et al., 2020) is controlling a bioreactor which contains a co-culture (environment), through manipulations of the concentration of auxotrophic nutrients flowing into the reactor (actions), and informed by the relative abundances (measurements), to ensure a specified co-culture composition (goal). Perhaps the most known example of reinforcement learning are the Hidden Markov Models (HMMs) that are commonly used to annotate genes and align sequences (Yoon, 2009). Reinforcement learning has also been applied to suggest pathways for specific molecules (Koch et al., 2020) or molecules that fit desired properties (Popova et al., 2018), as well as to optimize large-scale bioreactor fermentations using online Example of a supervised machine learning algorithm: a decision tree. Decision trees come from an abstracted view of how human learning works, rather than a mechanistic understanding. Decisions trees automatically build a decision "flowchart" that, in this case, predicts high or low production based on the protein expression levels. An example training data set and corresponding decision tree are shown in panels A and B, respectively, based on a set of strains (instances) and their production (response) depending on different protein expression levels (input features). Using the training data set, the algorithm decides on the optimal split points (x 1 , x 2 and y 1 ) to predict the production based on the input features. The split points are the parameters of the algorithm: more parameters will allow the algorithm to describe more instances. The algorithm also has a number of "hyperparameters" which are set before training, including the maximum tree depth, and the minimum number of instances required to split a node, among others (see scikit learn library for more details). Decision trees form the base for one of the most popular algorithms: the random forest. The random forest algorithm is just an ensemble of decision trees.</p>
<p>Fig. 6.</p>
<p>Other types of machine learning that do not require labeled data. Unsupervised methods and reinforcement learning were created to avoid the cumbersome process of labeling data for supervised methods. Unsupervised machine learning methods often search for patterns that aid human understanding or reduce dimensionality (e.g. PCA). For example, in this case the algorithm projected the five inputs (e.g. metabolomics data) into a two dimensional plane that groups them according to similarity. This type of dimensionality reduction can be very useful for feature selection (Fig. 4). Reinforcement learning methods attempt to achieve a goal through a continuous interaction with an environment from which they learn through a variety of measurements, and on which they can act through a menu of actions. The result of the actions as viewed by the measurements is used to iteratively update an internal policy that dictates future actions. Fig. 7. Artificial neural networks are a particular type of machine learning algorithms that loosely mimic how neurons work (Fig. 1). Neurons are modeled as having a set of inputs (dendrites) and a single long axon that serves as output (A). Artificial neural network cells mimic that: several outputs combined linearly and a non-linear output (B). The output is combined to other cell inputs, creating an artificial neural network (ANN). Here we see a fully connected network where all cells from each layer are connected to all cells in the next layer (C). This type of architecture results in many parameters (w ij and b j ), which requires large amounts of data to determine (Fig. 4). Deep neural networks are ANNs with many layers (Fig. 8). Given the original biological origin of ANNs, there is a significant interest in the AI field in obtaining further inspiration from biomimicry.</p>
<p>continuous process data (see section 3.3). However, there is still generally a dearth of reinforcement learning examples in metabolic engineering, which represents an opportunity for this type of machine learning. Deep learning (DL, Fig. 7 (LeCun et al., 2015)) is a specific type of machine learning algorithm that has been particularly successful in the past decade (Fig. 8). This algorithm has been shown to improve performance with the amount of training data when other methods plateau. Deep learning is based in Artificial Neural Networks, which attempt to mimic how neurons work (Fig. 7). In the last decade, deep learning has been the basis of the most celebrated AI achievements. However, compared to more classical machine learning methods, deep learning generally requires far larger amounts of data for training: 10,000s or millions of instances, as opposed to hundreds or thousands (although that depends on the number of inputs, see section 2.2.2). The reason for these large data sets hunger is that deep neural networks can include thousands to millions of parameters, which need to be determined from the data (see Fig. 4). In metabolic engineering, the use of deep learning has been sparse for this reason: the data sets tend to be small (&lt;100 instances), with the notable exception of sequence data. Deep learning has been most useful with sequence data: e.g., to predict protein function (Ryu et al., 2019), or translation initiation sites (Clauwaert et al., 2019) (see section 3.0). However, this is expected to change as more high-throughput methods to characterize cellular components become available, provided that data are structured consistently and stored appropriately (see section 2.4). Indeed, techniques to generate high quality omics data are improving rapidly and the cost per sample is decreasing (Stephens et al., 2015), so application of deep learning to metabolic engineering might become commonplace soon.</p>
<p>A couple of illustrative examples of machine learning in metabolic engineering</p>
<p>We will now illustrate how machine learning algorithms work through two different applications that elucidate particularly important points: predicting the kinetics of a metabolic network, and optimizing cell-free butanol production. We have focused on these examples because we believe they most relate to the day-to-day activities of metabolic engineers: leveraging omics data and improving production.</p>
<p>Kinetic learning: relearning Michaelis-Menten dynamics through machine learning</p>
<p>Our first example uses machine learning to tackle a commonly encountered problem in bioengineering: predicting the kinetics of a metabolic pathway. Predicting pathway dynamics can enable a much more efficient pathway design by allowing us to foresee in advance which pathway designs will meet our specifications (e.g., titers, rates and yields). Classic kinetic models predict the rate of change of a given metabolite based on an explicit functional relationship between substrate/product concentrations (metabolites) and enzymes (protein abundance, substrate affinity, maximum substrate turnover rate). Michaelis-Menten kinetic models (Costa et al., 2010;Heinrich and Schuster, 1996) have historically been the most common choice. In reality, the true functional relationship between metabolites and enzymes are typically unknown for most reactions due to gaps in our understanding of the mechanisms involved, resulting in poor prediction capabilities.</p>
<p>Costello et al.  showed that supervised machine learning (Fig. 2) can offer an alternative approach, where the relationship between metabolites and enzymes can be directly "learnt" from time series of protein and metabolite concentration data. In a sense, this approach involves relearning the equivalent of Michaelis-Menten based purely on data. This is a prime example of how a machine learning approach ignores mechanism in favor of predicting power: there is no intention that the function predicting metabolite change rate from proteins and metabolites describes a mechanism, but it offers the best prediction of the final limonene/isopentenol, which is what we require for our engineering. In this case, the inputs (Fig. 3) were the exogenous pathway protein and metabolite concentrations, and the response was the rate of change of the metabolite. The instances involved each of the time points for which the metabolite rate of changes was learnt.</p>
<p>This approach outperformed a classic kinetic model in predictive power using very little data: only three time series of protein and metabolite measurements of 7 time points each (for two different pathways). While it would be desirable to have hundreds of time point measurements, the high cost and time associated with performing multiomic experiments typically constrains data sets to less than 10 time points/samples, which is too sparse for training accurate models. Critical to its success, hence, was the use of data augmentation to increase the number of available instances from the initial 7 time points to the final 200 used for learning. Data augmentation simulates additional instances by modifying or interpolating actual data. In this case, data augmentation involved first smoothing the data (via a Savitzky-Golay filter) and then interpolating new data points from the fitted curve. This augmentation scheme only assumed continuity and smoothness between time points, but provided sufficient data to train a machine learning model using data from only 2 time series that accurately predict pathway dynamics of the "unseen" third strain. The final predictions of metabolite concentrations for the exogenous pathways, although not perfect by any measure, were more accurate than equivalent predictions by a hand-crafted kinetic model. More importantly, while the kinetic model took weeks to produce through arduous literature search, the kinetic learning approach can be systematically applied to any pathway, product and host with no extra overhead.</p>
<p>An opportunity to improve the machine learning model predictions of Costello et al. would of course be to collect more data, but deciding which data to collect is not always clear. For example, instead of using protein and metabolite data only from the exogenous pathways as input features, protein and metabolite measurements from the full host metabolism could be added (surely, host metabolic effects like ATP supply must be relevant). However, using these extra data would not necessarily improve machine learning predictions. This is because many machine learning algorithms suffer from the "curse of dimensionality": that is, the amount of data needed to support results in a statistically sound fashion often grows exponentially with the dimensionality of the input (Fig. 4). Hence, machine learning algorithms may struggle to learn from data sets that have many measurements or "input features" (columns), but few instances (rows). Adding host proteins and metabolites will increase the number of inputs without increasing the number of instances. Unfortunately, most multi-omic data sets used in metabolic engineering fit this description, containing more than 5000 measurements (e.g. proteins or metabolites abundances), but only tens to hundreds of instances (e.g. different time points, strains, or growth conditions, depending on what your algorithm is attempting to learn) (Fig. 4). Therefore, collecting as many instances as possible should be emphasized early on during experimental design (see section 2.4).</p>
<p>In the absence of being able to generate more data, algorithms that reduce the number of input features to the most important ones can be performed, a process known as feature selection. Feature selection (Pedregosa et al., 2011) was used in Costello et al. (Costello and Martin, 2018) to identify a subset of the input features based on their contribution to the model's error. This, more limited, curated set of features was then used to predict metabolite dynamics. The idea behind this is to remove non-informative or redundant input features from the model. An additional approach used was dimensionality reduction, where "synthetic features" are created that transform the original input features into fewer ones (or "lower dimensions") based on their contribution to explaining the data's variability (for example, via principal component analysis). Similar to feature selection, these algorithms simplify the data set in order to better fit a machine learning model. These approaches were integrated into a machine learning pipeline using the tree-based pipeline optimization tool (TPOT) (Olson et al., 2016;Olson and Moore, 2019), which automatically selected the best combination of feature preprocessing steps and machine learning models from the scikit-learn library (Pedregosa et al., 2011) to maximize prediction performance.</p>
<p>Artificial neural networks to improve butanol production in cell-free systems</p>
<p>Our second example involves using deep neural networks to optimize cell-free butanol production (Karim et al., 2020). Here, the authors provide an example of how machine learning can accelerate the design-build-test-learn (DBTL) cycles used in metabolic engineering (Nielsen and Keasling, 2016), by effectively guiding pathway design. In this study, the authors optimized a six-step pathway for producing n-butanol, an important solvent and drop-in biofuel, using a cell-free prototyping approach (iPROBE). iPROBE reduces the overall time to build pathways from weeks or months to a few days (around five in this case), providing the quick turnaround and large numbers of enzyme combinations that can enable successful use of machine learning. Several pathway variants were constructed in vitro and scored based on their measured butanol production through a TREE score which combines titer, rate, and enzyme expression. The challenge, however, lies in analyzing the sheer number of pathway combination possibilities. Testing only six homologs for the first four pathway steps at 3 different enzyme concentrations would result in 314,928 pathway combinations (strain genotypes). Even with the increased turnover provided by the cell-free approach, it would take years for typical analytical pipelines to exhaustively test the landscape of possible combinations. Therefore, a data-driven design-of-experiments approach was implemented using neural networks to predict optimized pathway designs (homolog sets and enzyme ratios) from an initial data set that could subsequently be tested. In this case the input for the neural network was the enzyme homologs used for each of the reaction steps and their corresponding concentrations. The response was the TREE score, and each instance was a pathway design.</p>
<p>The pathways predicted from the neural network model were able to improve butanol production scores over fourfold (~2.5 times higher titer, 58% increase in rate) compared to the base-case pathway. An initial data set of 120 instances (pathway designs) was used to train and test different neural network architectures consisting of 5-15 fully connected hidden layers and 5 to 15 nodes per layer. Genetic algorithms were used to suggest combinations of network architectures, and tenfold cross validation was used to select the best. Once the model was built, the authors used a nonlinear optimization algorithm (Nelder-Mead simplex) to recommend pathway designs that optimized butanol production through the maximization of the TREE score. These machine learning recommendations resulted in 5 of the 6 top performing pathways, and outperformed 18 expert determined pathways selected based on prior knowledge, demonstrating the power of a data-driven design approach for cases in which design choices are numerous.</p>
<p>While the study by Karim et al. only reported 1 DBTL cycle, multiple cycles would have likely resulted in even better production pathways, and also provided more data instances for model training. Indeed, the neural network of 5-15 hidden layers developed by Karim et al. was relatively small compared to state-of-the-art deep neural networks, but this design was limited by having only 120 instances (pathway designs) to train on. If more data were to become available through more DBTL cycles, the neural network could have been made more complex by expanding its depth (hundreds of hidden layers), which would improve prediction performance (Fig. 4). This improved performance, however, comes at a cost: as the number of layers increases, the time to train the network (i.e. learning model weights and parameters) increases considerably. Moreover, the dense hidden layers of deep neural networks render them very difficult to interpret and infer possible mechanisms from. Hence a significant research thrust in machine learning involves new approaches to make models "explainable" (see Section 5.3) (Gunning, 2016;. The use of only 1-2 DBTL cycles seems to be the most common case in published projects (Denby et al., 2018;Alonso-Gutierrez et al., 2015;Opgenorth et al., 2019;Zhang et al., 2020). In our experience, this happens not because more DBTL cycles are not expected to be useful, but because results from a single DBTL cycle are often enough for a publication. Often, in the academic world, there is little incentive (or resources) to continue further.</p>
<p>Requirements for machine learning in metabolic engineering</p>
<p>Here we provide a practical guide on the immediate prerequisites to applying machine learning to metabolic engineering, in the next section we will discuss some practical considerations for experimental design once the machine learning project is in progress, and, in section 5.1, we discuss long term hurdles for the development of the discipline as a whole. In essence, four requirements need to be aligned for a successful application: data, algorithms, computing power and an interdisciplinary environment. Each of them is critical for a real impact.</p>
<p>Data needs to be abundant, non-sparse, high quality, and well organized. Training data needs to be abundant because machine learning algorithms depend critically on training data to be predictive. There is no prior biological knowledge embedded in them. In general, the more training data, the more accurate the algorithm predictions will be. Data augmentation (see section 2.2.1) can certainly help, and should be routinely used in metabolic engineering due to the scarcity of large data sets, but it is no substitute for experimental data. There is, however, no way to know a priori how much data will be enough. Different problems present different difficulty levels to being "learnt" (Radivojević et al., 2020), and this difficulty level can only be assessed empirically. A scaling plot of predictive accuracy vs. instances can be very helpful in this regard. Training data can be abundant but still sparse, depending on the phase space ( Fig. 9) considered. A total of a hundred instances can be enough if only two input features are considered, or completely insufficient if a thousand input features are considered. The "curse of dimensionality" implies that the amount of data needed to support results in a statistically sound and reliable fashion often grows exponentially with the dimensionality (Fig. 4). The data must be high-quality in the sense that it must avoid biases due to inconsistent protocols and provide quantification for repeatability (see section 2.4). Both goals can be systematically achieved through automation (see section 5.2). Data needs to be well organized, following standards and ontologies, and must include the corresponding metadata (see section 2.4). The alternative is that data analysts will spend 50-80% of their effort organizing the data and metadata for analysis, mining their efforts (Lohr, 2014). Since data analysts might be the most effective effort multiplier in your team (Nielsen and Keasling, 2016), and possibly the most expensive (Metz, 2018), it is very useful to optimize their effort.</p>
<p>While there are many machine learning algorithms to choose from ( Fig. 10), there is no clear best algorithm for every situation. Indeed there is a famous theorem (the no free lunch theorem, NFLT) that proves (under some conditions) that no single algorithm is most effective for every type of problem (Wolpert, 1996). While the utility of the NFLT for machine learning has been cast in doubt (Giraud-Carrier and Provost, 2005 ), the standard approach remains to try as many algorithms as possible and compare their results. In this effort, it is very useful to count on libraries that collect a large variety of algorithms and have standardized input, output and other standard procedures (e.g. cross-validation). The most popular among them is, without a doubt, scikit-learn (Pedregosa et al., 2011), a python library that comprises a very wide selection of machine learning methods, is well documented, and easy to use (Fig. 10). These features combined with its open source nature, and its compatibility with Jupyter notebooks (Kluyver, 2016), which facilitate reproducibility and communication, make it our top recommendation for beginners. Furthermore, the open source nature and wide use of scikit-learn means that there are several tools that leverage it to combine and test methods. Tree-based pipeline optimization tool (TPOT), for example, automatically combines all the available algorithms and preprocessing steps in scikit-learn to choose the best option (Olson et al., 2016). Another example is the Automated Recommendation Tool (ART), which leverages scikit-learn, ensemble modeling, and bayesian inference to provide uncertainty quantification for predictions (Radivojević et al., 2020). A proprietary alternative is to use Matlab, for which a machine learning toolbox is available (Ciaburro, 2017), with possible educational discounts. For artificial neural networks, the best supported (and free) frameworks are TensorFlow and Pytorch, backed by Google and Facebook respectively. Keras, a framework focused on providing a simple interface for neural networks, is now the official high-level front-end for TensorFlow (Géron, 2019). Keras has its own hyperparameter tuner, Keras Tuner , and an extremely simple interface for DL with Keras and Ten-sorFlow, AutoKeras .</p>
<p>Computation is another key element, particularly for large amounts of data. Whereas the libraries above (Scikit-learn, Matlab toolbox, Tensorflow, Pytorch) can be run on a standard laptop (e.g. 2018 Macbook Pro, 3.5 Ghz Intel Core i7, 16 GB RAM), as more training data is added this may be insufficient. This is particularly the case for deep neural networks using Tensorflow or Pytorch, which will benefit from the parallelization obtained through Graphics Processing Units (GPUs). The need to scale up all these Python frameworks for high performance computing (HPC) or deployment on cloud computing environments (e.g. Amazon EC2, Microsoft Azure, and Google's Cloud Platform) has promoted the development of several parallel and distributed computing backends for data analysis and machine learning, such as Ray, Spark, and Dask (Rocklin, 2015). Furthermore, as the general applicability of AI has become more evident, new processor architectures are being created specifically for neural network machine learning, including Google's Tensor Processing Unit (TPU), Nvidia's V100 and A100, Graphcore's Intelligence Processing Unit (IPU), and a variety of FPGA-based solutions.</p>
<p>Since very few people master both machine learning and metabolic engineering, interdisciplinary collaborations are truly necessary. Machine learning practitioners and metabolic engineers are trained very differently, however, and this can produce significant friction (see section 5.1). Both disciplines profess different cultures, which are reflected in how they solve problems, but also which problems are prioritized. It is, hence, very important to foster an inclusive work environment that integrates and values contributors with very different skills, and does not penalize knowledge gaps. It is also important to be very clear about the interfaces: which exchanges (e.g., data, designs, predictions) are expected, and when, in order for both sides to be effective. This is Alexnet, one of the first ANNs that leveraged the network depth to improve performance and win the ImageNet image classification contest in 2012. Deep networks lower the amount of parameters by sparsely using fully connected layers, which require many parameters. The first five layers in Alexnet are convolutional layers (Rawat and , which only take input from a limited number of cells in the previous layer. Many architectures are possible for deep learning, and finding the optimal one is more of an art than a science. See Lecun et al. (LeCun et al., 2015) for more details.</p>
<p>Practical considerations for implementing machine learning</p>
<p>As in the case of a genetic selection or screen, machine learning requires careful experimental planning to make it effective. An experimental design that ignores its basic assumptions (e.g., instances are independent and identically distributed) will result in a random walk over possible designs with the same (or even worse) results as a trialand-error approach.</p>
<p>Here, we offer a succinct list of recommendations to consider when planning to use machine learning to guide bioengineering:</p>
<p>• Choose the right objective/response. When a response for the algorithm is chosen, you are entering a Faustian bargain with your algorithm: it will try to optimize it to the detriment of everything else (Riley 2019) . For example, setting final titer as the response might provide high titers in the end for a production strain, but at rates so slow that the result is of little practical use. In the case of Karim et al., (see section 2.2.2), the response was a carefully selected mixture of titer, rate, and enzyme expression precisely for this reason. Deciding on the right response is a bit of an art, and less trivial than often assumed. Be careful what you ask the algorithm for, because you may get it! • Choose inputs that truly predict your response. Performing small, directed experiments in the lab to verify that the response of interest (e.g. a phenotype) is affected by a given input (e.g. a treatment) can save a significant amount of time and headaches later in the DBTL cycle, by limiting the number of inputs (and the overall complexity of the model) to terms that matter. Omitting this step might give rise to a frustrating chase of a red herring in the form of statistical noise, or cause serious challenges to the interpretability of the model. • Choose actionable inputs that can be measured. The machine learning process will require you to change your inputs in order to achieve the desired goal (e.g. increase production). Hence, these inputs need to be experiment variables that can be easily manipulated. Since you will need to assess whether you indeed reached the recommended targets, it is highly desirable that these inputs can be easily measured. For example, it is generally better to use as inputs promoter  or enzyme choices (Karim et al., 2020), rather than protein levels (Opgenorth et al., 2019). Promoter or enzyme choices are entirely under the metabolic engineer's control, and their effects on expression may be verified via sequencing; whereas certain target protein levels may be difficult to reach, and usually require specialized mass spectrometry methods to verify. • Choose very carefully how many experiment variables you would like to explore. Choosing too many variables (i.e. input features, Fig. 3) can make the corresponding phase space too large for machine learning to explore in a reasonable amount of DBTL cycles. Choosing too few variables might mean missing important system configurations (e.g. if protein X is not chosen and it needs to be downregulated to improve production, it will be impossible to find the optimum). As a very crude rule of thumb, you should budget for around at least 100 instances per 5-10 variables. This, of course, depends on the difficulty presented by the problem being learnt: more difficult problems will need more instances per variable, whereas easier problems will require less instances per variable. • Verify that your experiment variables can be independently acted upon. Whole-operonic effects can make this unexpectedly difficult (Opgenorth et al., 2019). For example, if recommendations require protein A concentration to be increased three-fold and protein B to be decreased by a factor of two to improve production, but a strong promoter for protein A also produces an increase in protein B, it will be difficult to reach the target protein profile. Hence, modular pathway designs (Boock et al., 2015) that ensure that the full input phase space can be fully explored are highly recommended. Systematic part characterization involving large promoter libraries with a variety of tested relative strengths are a fundamental tool in this endeavor. • Design your experiment to start with ~100 instances for the initial DBTL cycle. Although there are examples of success stories with less than a hundred instances as starting points (Radivojević et al., 2020), this outcome cannot be guaranteed. Actual success depends on the complexity of the problem (Radivojević et al., 2020), and this complexity can only be gauged by testing predictive accuracy as data sets increase. By starting with ~100 instances, one ensures some progress even if predictions are not accurate: this amount of instances goes a long way to ensure statistical convergence. The alternative is a non-predictive model and little understanding whether the problem is lack of data (instances), or other design problems (Opgenorth et al., 2019). Consider automating as much of your process as possible so as to guarantee enough instances. This automation may seem an unnecessary hassle, but it will pay off in the long run. • Sample the initial phase space as widely as possible. Ensure that you cover wide ranges for both input and response variables. Strive to include both bad (e.g. low production) and intermediate results as well as good ones (e.g. high production), since this is the only way that the algorithms can learn to distinguish the inputs needed to reach any of these regimes. The Latin Hypercube (McKay et al., 1979) is a good choice to choose starting points, but other options are also available. • Consider uncertainty, as well as predicted response, when choosing next steps. As the need to quantify prediction uncertainty becomes more recognized in the biological sciences (Begoli et al., 2019), more algorithms provide it along with response predictions (Radivojević et al., 2020). Using this information can improve the whole process. Choose some recommendations with the lowest possible uncertainty even if the predicted outcome is not so desirable (e.g. low production), so as to establish trust in the approach (see sociological hurdles in section 5.1). Choose some recommendations with large uncertainty even if the predicted outcome is not desirable so as not to miss unexpected opportunities. In addition, to obtain an empirical view of how uncertainty in the data affects the accuracy of predictions, it may be instructive to create simulated, in silico "ground truth" data sets displaying different levels of noise in order to test the performance of the machine learning algorithm. • Avoid biases created through inconsistent protocols and beware of hidden variables. Machine learning algorithms learn to map an input to a response (Fig. 3). If different DBTL cycles produce different results for reasons that are not reflected in the input (hidden variables (Riley 2019)), the algorithms will provide poor predictions. Such uncontrolled variables can easily arise in biological data due to lab temperature or climate fluctuations, reagent batch differences, undetected culture mutations, "edge effects" in plate-based assays, and equipment drift. These effects should be assessed and eliminated as part of the experimental design, and is one of the key topics of communication for bench and computational scientists to empower downstream data analysis and predictions. Machine learning can also help by performing simple checks: if an algorithm can predict which well or batch sample the data came from, that means they unduly influence the response. Lack of repeatability is the main stumbling block of machine learning. • Add experimental controls to test for repeatability. Since ensuring repeatability is among the top requirements for machine learning to be successful, it is important to test and quantify it often. Batch, instrument, and operator effects are often the first principal component of data. These effects can be detected by including a few controls of known response in every experiment (e.g., 2-3 base strains in every DBTL cycle). While this approach consumes valuable analytical resources, it ensures that the data can be trusted and does not need to be discarded, saving substantial labor during modeling and analysis. • Plan for several DBTL cycles. Machine learning algorithms shine when they can dynamically probe your system, since they are designed to learn from data interactively. While results can be obtained using two DBTL cycles, they are not comparable to what &gt;5 cycles can provide (Radivojević et al., 2020). If only a limited budget of, e.g. 100 instances, is available, it is better to start with a strong first cycle and several weaker ones (e.g. 40 instances for cycle 1, then six 10 instance cycles) than the usual two DBTL cycle study (e.g. 60 instances for first cycle, 40 instances for the second one). • Standardize your data and metadata. Taking machine learning for metabolic engineering seriously requires large amounts of high quality data. Hence, it is advisable to store it in a standardized manner. There are a variety of data repositories available for this purpose: e.g., the Experiment Data Depot (Morrell et al., 2017), , the Nature Scientific Data journal ("Open for business," 2017), to name a few. Moreover, a labeled data set of high quality is a significant resource for the community, and is more likely to be cited. • Be careful about how you split your data for cross-validation.</p>
<p>Cross-validation of your model (Fig. 3), assumes data sets are independent and identically distributed (iid). This assumption is basic for machine learning, and presumes that both validation and training sets stem from the same generative processes and have no memory of past generated samples. However, it can be violated in practice due to temporal effects on biological systems or group effects during sample processing (Riley 2019). In these cases, alternatives to random splitting need to be considered. Sheridan (2013), for example, showed that randomly splitting compound libraries used for drug discovery overestimated their model's ability to successfully predict drug candidates. The reason for this difference is that compounds added to the public record at particular dates shared higher structural similarity, resulting in models that had already "seen" compounds in the test set when randomly split. Similar considerations need to be made when sample generation occurs in a biased manner, which is quite common in biological experiments. For example, "batch effects" can be avoided by splitting the data first by group (e.g. each batch) to ensure the same group is not represented in both testing and training sets (see scikit-learn group k-fold). Do only worry about this effect if you have a large data set (&gt;100 instances).</p>
<p>Perhaps the best way to get familiar with machine learning, and its potential and limitations, is to experiment with it in a tutorial. The recently published Automated Recommendation Tool (Radivojević et al., 2020) includes three synthetic data sets, three real data sets and a software package that can be used for this purpose. Furthermore, some of these cases are explained in detail in several Jupyter notebooks contained in the github repository (https://github.com/JBEI/ART/tree/ master/notebooks), and can be used as tutorials.</p>
<p>Applications of machine learning to metabolic engineering</p>
<p>Although application of machine learning in metabolic engineering is nascent, early studies have already shown its potential use for accelerating bioengineering. Here, we highlight examples where machine learning is being used to improve different stages of the metabolic engineering development cycle: gene annotation and pathway design, pathway optimization, pathway building, performance testing, and production scale-up (Table 1). We focus on prime examples that best epitomize the potential of machine learning in metabolic engineering, rather than an exhaustive list of applications. The reason for this decision is that this list is quickly growing and might be outdated soon, and there are recent reviews on the topic that provide that information Presnell and Alper, 2019;Volk et al., 2020). We also discuss key challenges and opportunities when applying machine learning for metabolic engineering, with particular focus on practices that could formalize data-driven approaches.</p>
<p>Machine learning for design</p>
<p>The goal of metabolic engineering design is to develop DNA parts and assembly instructions to synthesize metabolic pathways and produce a desired molecule (Nielsen and Keasling, 2016;Woolston et al., 2013). This requires completion of several tasks, including gene annotation, pathway reconstruction and design, as well as metabolic flux optimization, which currently rely heavily on domain expertise and enjoy little standardization (Nielsen and Keasling, 2016). Application of machine learning can improve the accuracy and speed of these tasks, offering a standardized approach that fully leverages experimental data.</p>
<p>Pathway reconstruction and design</p>
<p>Locating and annotating protein encoding genes in a genome sequence is essential for metabolic pathway reconstruction and design. This is conventionally done bioinformatically, for example using Hidden Markov Models (HMMs) (Finn et al., 2011;Kelley et al., 2012;Yoon, 2009). Initially, genes are identified in a genome by searching for known protein coding signatures (e.g. Shine-Dalgarno sequences), and this is followed by annotation based on sequence homology searches against a database of previously characterized proteins. More recently, however, deep learning approaches have been used to identify and functionally annotate protein sequences in genomes by leveraging large high-quality experimental data sets (Armenteros et al., 2019;Clauwaert et al., 2019;Ryu et al., 2019). DeepRibo, for example, uses high-throughput ribosome profiling coverage signals and candidate open reading frame sequences (input features) to train deep neural networks to delineate expressed open reading frames (response is part of predicted ORF or not for every nucleotide) (Clauwaert et al., 2019). This approach showed more robust performance compared to a similar tool, REPARATION (Ndah et al., 2017), that uses a random forest classifier instead of deep neural networks. DeepRibo also improved prediction of protein coding sequences in different bacteria (e.g. Escherichia coli and Streptomyces coelicolor) compared to RefSeq annotations, including higher identification of novel small open reading frames commonly missed by sequence alignment algorithms. Another example is DeepEC, which takes a protein sequence as input and predicts enzyme commission (EC) numbers as output with high precision and throughput using deep neural networks (Ryu et al., 2019). A data set containing 1,388,606 expert curated reference protein sequences and 4669 enzyme commission numbers (Swiss-Prot (Bairoch and Apweiler, 2000) and TrEMBL (UniProt Consortium, 2015) data sets) was used to train the deep neural networks, which improved EC number prediction accuracy and speed compared to 5 alternative EC number predictions tools, including Cat-Fam (Yu et al., 2009), DETECT v2 (Nursimulu et al., 2018), ECPred (Dalkiran et al., 2018), EFICAz2.5 (Kumar and Skolnick, 2012), and PRIAM (Claudel-Renard et al., 2003). DeepEC was also shown to be more sensitive in predicting the effects of protein sequence domain and binding site mutations compared to these tools, which could improve the accuracy of annotating homologous proteins that have mutations with previously unknown effects on function (e.g. from metagenomic data sets).</p>
<p>The design of metabolic pathways involves identifying a series of chemical reactions that produce a desired product from a starting substrate, and selecting different enzymes that catalyze each reaction. While nature has evolved many pathways for producing diverse molecules, the known and characterized biochemical pathways can still be insufficient to produce certain molecules of interest, especially nonnatural compounds or secondary metabolites. Therefore, retrosynthesis methods that start with a desired chemical and suggest a set of chemical reactions that could produce it from cellular metabolite precursors are being pursued to design new metabolic pathways Lee et al., 2019). The latest and most sophisticated of these methods use generalized reaction rules to describe possible biochemical transformations (Delépine et al., 2018;Kumar et al., 2018). However, the number of possible reaction combinations is intractable since it grows combinatorially with the number of reactions. Choosing the right reaction combination is a non-trivial problem, which is typically tackled via optimization or heuristic methods. A possible solution to this search problem comes from solving the same problem in organic synthesis, through the use of deep neural networks (Segler et al., 2018). Segler et al. preprocessed 12.4 million reaction rules from the Reaxys chemistry database to train three deep neural networks implemented within a Monte Carlo tree search (heuristic search algorithm used in decision making) to discover retrosynthesis routes for small molecules. This deep learning approach found pathways for twice as many molecules, thirty times faster than traditional computer-aided searches (Segler et al., 2018). The predicted synthesis routes better adhered to known chemical principles than traditional computer-aided searches and could not be differentiated by expert organic chemists compared to synthesis routes taken from the literature, highlighting the potential of deep learning to be applied for metabolic retrosynthesis (or retrobiosynthesis). Indeed, a similar Monte Carlo Tree Search method has recently been extended to predict synthetic pathways within biological systems (RetroPath RL), enabling systematic pathways design for metabolic engineering (Koch et al., 2020).</p>
<p>Pathways designed via retrosynthesis still face the difficult challenge of finding enzymes for novel biochemical reactions, for which no enzyme is known. In this case, the solution involves enzymes that may catalyze the novel reaction through enzyme promiscuity, or new enzyme functions must be designed or evolved that perform the desired chemistry. While chemoinformatic techniques (e.g. density functional theory, DFT, and partitioned quantum mechanics and molecular mechanics, QM/QM) can be used to predict the interaction between metabolites and proteins in silico (Alderson et al., 2012), these techniques are computationally intensive and require substantial domain expertise. Therefore, the task of searching for promiscuous enzymes is increasingly being performed using more general and computationally efficient techniques from machine learning. For example, given a reaction and enzyme pair instance, Support Vector Machines (Faulon et al., 2008) and Gaussian Processes (Mellor et al., 2016) have been developed to predict whether the enzyme catalyzes the reaction, with the latter model having the added benefit of providing uncertainty quantification. These models predict positive or negative enzyme reaction pairs from protein sequences (e.g. K-mers) and reaction signatures (e.g. functional groups, chemical transformation properties) (Carbonell and Faulon, 2010) by learning patterns about promiscuous enzyme activities through training. They can also be applied to predict substrate affinity for proteins (K m values) (Mellor et al., 2016), an important kinetic parameter for determining enzyme activity, which is difficult and time consuming to measure experimentally. This is critical for pathway design as sequences with the most desirable kinetic properties can be selected when multiple candidates catalyzing a given reaction are available.</p>
<p>In the case that no enzyme can be found for a target reaction, new enzymes may be designed or discovered through protein engineering. A common laboratory method for protein engineering is directed evolution, where beneficial mutations accumulate in a protein through iterative experimental rounds of mutation and selection until the desired protein function is achieved (Yang et al., 2019). In essence, a series of local searches (via sequence mutation and screening) are performed on an enormous and highly complex functional landscape with the hope of finding a local optima (i.e. protein variant with desired properties). However, experimental approaches can only explore an infinitesimal part of this landscape and computational approaches are needed to guide experimental efforts. Machine learning can be used to guide directed evolution and decrease the number of experimental iterations needed to obtain a protein with the desired function. This is achieved by leveraging previous screening data to learn a protein's sequence-function landscape and predict new sequence libraries that contain variants with higher fitness. For example, instead of experimentally performing sequential single point mutations or recombining mutations found in best variants (common directed evolution approaches), Wu et al. (2019) trained a machine learning model to perform in silico evolution rounds that ranked new protein variants by predicted fitness for experimental testing. Instead of relying on a single machine learning method, multiple models (linear, kernel, neural network, and ensemble) were trained in parallel, and the ones showing the highest accuracy were used to perform in silico evolution rounds . This enabled deeper exploration of the possible variant functional landscape, resulting in the successful evolution of an Notes: RNN = recurrent neural network; CNN = convolutional neural network.</p>
<p>C.E. Lawson et al. immunoglobulin-binding protein and a putative nitric oxide dioxygenase from Rhodothermus marinus. ML-assisted directed evolution has also been used to maximize enzyme productivity (Fox et al., 2007), change the color of fluorescent proteins (Saito et al., 2018), and optimize protein thermostability (Romero et al., 2013) making it a promising approach for searching large sequence-function spaces in an efficient manner for proteins variants with desired properties. In addition to directed evolution, deep learning has also recently been applied for the rational design of proteins (Alley et al., 2019;Biswas et al., 2020;Costello and Garcia Martin, 2019). For example, Alley et al. (2019) developed UniRep, which uses recurrent neural networks to learn an internal statistical representation of proteins that contained physicochemical, organism, secondary structure, evolutionary and functional information, by training on 24 million UniRef50 (Suzek et al., 2015) amino acid sequences (instances). The resulting representation was applied to train models (random forest or sparse linear model) using UniRep encoded proteins that predicted the stability of a large collection of de novo designed proteins and also the functional consequence of single point mutations on wild-type proteins. UniRep encoding was also used to optimize the function of two fundamentally different proteins (to wild-type), a eukaryotic green fluorescent protein from Aequorea victoria, and a prokaryotic β-lactam hydrolyzing enzyme from Escherichia coli, highlighting the generalizability of this approach for rational protein engineering (Biswas et al., 2020). Other generative models based on deep learning have been used to suggest protein sequences with desired functionality and location .</p>
<p>Pathway optimization</p>
<p>Following pathway design, metabolic flux optimization is required to maximize product titers, rates, and yields (TRY). In this endeavor, machine learning provides an orthogonal approach to computational approaches leveraging flux analysis and genome-scale models, which have been successfully used in the past to increase TRY (Maia et al., 2016). The combination of both approaches has the potential to be more effective than each of them separately (see section 4 for a discussion).</p>
<p>A common approach to increase TRY involves fine tuning gene expression through the modification of promoter and ribosome binding site (RBS) sequences. Despite decades of progress in understanding the regulatory mechanisms controlling gene expression (Snyder et al., 2014), quantitative prediction of gene expression based on sequence information remains challenging. While computational models do exist to predict gene expression (Leveau and Lindow, 2001;Salis et al., 2009;Rhodius and Mutalik, 2010), they rely on a comprehensive understanding of transcription and translation processes. This knowledge is often unavailable, especially for non-model organisms. Therefore, many gene expression optimization efforts rely on trial-and-error experimental approaches based on promoter and RBS library screening , that also suffer from the large combinatorial space of possible sequences.</p>
<p>Machine learning has also guided the design of promoter and RBS sequences in a data-driven manner for improved control of gene expression. In particular, neural networks have been used to predict gene expression output from input promoter sequences or coding regions (Kotopka and Smolke, 2020;Meng et al., 2013;Tunney et al., 2018). Meng et al. (2013) used a simple neural network trained with 100 mutated promoter and RBS sequences as inputs to predict promoter strength (response). This machine learning model outperformed mechanistic models based on position weight matrix or thermodynamics methods (Leveau and Lindow, 2001;Salis et al., 2009;Rhodius and Mutalik, 2010), and was able to optimize heterologous expression of a small peptide BmK1 (used in traditional Chinese medicine) and the dxs gene involved in the isoprenoid production pathway (Meng et al., 2013). Additionally, optimization of promoter strength and inducer concentration/time has been achieved using partial least squares regression (Jervis et al., 2019a), whereas prediction of riboswitch dynamic range from aptamer sequence biophysical properties has been achieved using a combination of random forests and neural networks (Groher et al., 2019). In this latter riboswitch design example, instead of directly using sequence information to train the random forest, the authors calculated known riboswitch biophysical properties from aptamer sequences (entropy, stem melting temperature, GC content, length, free energy, etc.) and used these as input features for model training, in order to predict switching behavior. This allowed for the interpretation of which input features were most important to the model prediction using variable importance (e.g. melting temperature was more important than free energy), enabling inferences on possible mechanisms.</p>
<p>More recently, machine learning models have been used to optimize multi-step pathways for chemical production (Zhou et al., 2020). For example, Zhou et al. (2018) used neural network ensembles to improve a 5-step pathway for violacein production (pharmaceutical) by selecting promoter combinations to tune gene expression. Using an initial training set of only 24 strains (out of a possible 500) containing different promoters for each gene, the model predicted a new strain that improved violacein titer by 2.42-fold after only 1 DBTL iteration. Their ensemble approach allowed top producing strains to be predicted from a combination of over 1000 ANN, which improved model accuracy and also allowed optimization of violacein based on both titer and purity. In another example, Opgenorth et al., (2019) used an ensemble of four different models (random forest, polynomial, multilayer perceptron, TPOT meta-learner) to optimize a 3-step pathway for dodecanol production from two DBTL cycles. The model was trained using data generated from 12 strains (48 data points total) with different RBS sequences for each gene, where an optimization step was used to recommend improved strain designs to build and test in the second cycle. Additional machine learning models have guided the optimization of multi-gene pathways, including limonene production in E. coli using support vector regression (Jervis et al., 2019b), lycopene synthesis in E. coli using gaussian processes (HamediRad et al., 2019), and tryptophan production in S. cerevisiae using ensemble models . Together, these examples highlight the potential of systemically leveraging high-throughput strain construction, testing, and machine learning to optimize multi-step pathway expression for improving product TRY.</p>
<p>To enable broader use of ML-driven pathway optimization and design by the metabolic engineering community, Radivojevic et al. (Radivojević et al., 2020) developed the Automated Recommendation Tool (ART). ART is specifically tailored to the needs of the metabolic engineering field: effective methods for small training data sets and uncertainty quantification. ART's ability to quantify uncertainty enables a principled way to explore areas of the phase space that are least known, and is of critical importance to gauge the reliability of the recommendations. We expect that further development of tools tailored to the specific needs of the field will enable broader application of machine learning.</p>
<p>Machine learning for building and testing cellular factories</p>
<p>Machine learning can also be used to improve the tools that build and test cellular factories. A major challenge in gene editing using CRISPR-Cas technologies, for example, is predicting the on-target knockout efficacy and off-target profile of single-guide RNA (sgRNA) designs. Several approaches exist to make these predictions, including alignment-based methods (Aach et al., 2014), hypothesis-driven methods (Heigwer et al., 2014;Hsu et al., 2013), and classic machine learning algorithms (i.e. non-deep learning) (Chari et al., 2017;Doench et al., 2016). However, their generalizability has been limited by the small size and low quality (high noise) of the training data. Higher-throughput screening methods combined with deep learning have recently improved the accuracy and generalizability of sgRNA activity prediction tools. For example , developed DeepCpf1, which predicts on-target knockout efficacy (indel frequencies) using deep neural networks trained on large-scale sgRNA (AsCpf1) activity data sets. While previous machine learning tools had been trained on medium-scale data (1251 target sequences), the authors high-throughput experimental approach generated a data set of indel frequencies for over 15,000 target sequence compositions, which was sufficient to train deep neural networks. Seq-DeepCpf1 was shown to outperform conventional ML-based algorithms, and steadily increased in performance as training data size increased, highlighting the value of data sets with &gt;10,000 high-quality training instances. Seq-DeepCpf1 was also extended by considering input features other than target sequence composition known to affect sgRNA activity (in this example, chromatin accessibility (Jensen et al., 2017)) that further improved prediction accuracy and performance on independently collected data sets from other cell types (a metric of model generalizability). This highlights the value of expanding input features beyond the obvious choice.</p>
<p>In addition to predicting on-target knockout efficacy, the off-target profile of sgRNA activity is also important to forecast, in order to prevent undesirable perturbations that result in genomic instability or functional disruption of otherwise normal genes. This has been performed using both regressive models and deep neural networks (Listgarten et al., 2018,Lin and. To combine on-target knockout efficacy and off-target profile predictions into one tool Chuai et al (Chuai et al., 2018), developed DeepCRISPR. DeepCRISPR uses both an unsupervised deep representation learning technique and deep neural networks to maximize on-target efficacy (high sensitivity), while minimizing off-target effects (high specificity). Unsupervised representation learning allows DeepCRISPR to automatically discover the best representation of input features from billions of genome-wide unlabeled sgRNA sequences, instead of specifying what input features should look like (e.g. target sequence composition). This sgRNA representation was then used when training a deep neural network using labeled data consisting of target sequences and epigenetic information (input features) to predict both on-target and off-target activities (responses). Overall, DeepCRISPR outperformed classic machine learning methods and exhibited high generalizability to other cell types, highlighting the value of unsupervised representation learning to automate feature identification.</p>
<p>Machine learning methods could also be used to optimize the DNA assembly and transformation protocols critical for building engineered strains. Although DNA and strain construction has traditionally been accomplished empirically (Chan et al., 2013) or guided by rule-of-thumb approaches (Engler and Marillonnet, 2014), the ability to assemble and test DNA constructs and their transformation efficiencies under different conditions in high-throughput could enable data-driven optimization. For example, machine learning could leverage comprehensive overhang ligase fidelity data sets (Potapov et al., 2018) to expand the identification of high-fidelity overhang sets for gibson assembly, potentially allowing more DNA fragments to be assembled in a single reaction. Machine learning could also leverage large data sets that examine transformation efficiency under a range of different conditions (e.g. media compositions, temperatures, incubation times, electroporation conditions, plasmid designs) to improve plasmid delivery and expression. This would be particularly useful for expanding genetic systems to a broader range of host organisms that have potential for industrial applications (Brophy et al., 2018;Wang et al., 2019).</p>
<p>Once cell factories are built their performance needs to be tested. Cell factories can be assayed for various components such as target molecules, transcripts, proteins, and metabolites. The throughput of these assays varies greatly from over 10,000 samples per day to fewer than 20 samples per day . Together, the data from these assays provide a comprehensive picture of how the engineered cells function. However, constructing large numbers of strains followed by high-throughput screening often produces noisy data sets arising from several factors, including small plate-based formats (e.g. edge effects), analytical measurement errors, and laboratory handling errors and biases. One way to reduce these errors is manual inspection, but this approach is not scalable for large data sets and often not reproducible due to person-to-person variability. Therefore, machine learning methods that predict outliers and biases from data and perform data processing in a standardized and reproducible manner are desirable. For this, the use of unsupervised learning algorithms that do not depend on "good" and "bad" labeled data examples have been used, such as clustering analysis methods (Fig. 6). The sci-kit learn library implemented in python has a set of machine learning tools available to perform outlier detection, including Isolation Forest, Local Outlier Factor, One-Class SVM, and Elliptic Envelope, that can be integrated into workflows to provide rapid and robust data quality processing. Additionally, supervised learning approaches based on deep neural networks have been applied to improve multi-omics data processing, for example protein identification from tandem mass spectra (Gessulat et al., 2019) and peak detection during metabolomic data processing (Melnikov et al., 2020). Given the large volume of data generated overtime from lab workflows and analytical instruments, further efforts to standardized data processing using machine learning should result in improved data sets for cellular factory design and analysis.</p>
<p>Machine learning for scaling up cellular factories</p>
<p>One of the largest challenges in metabolic engineering is maintaining the performance of laboratory strains when scaling up to commercial production plants (Chubukov et al., 2016;Wehrs et al., 2019). The typical procedure consists of cultivating lab strains in successively larger fermentation systems from bench-scale (~250mL-5L), to pilot-scale (~20-200L), to full-scale processes (&gt;1000L). Critical to successful scale up is understanding how process variables (feed rate, pH, temperature, fermentation time, mixing regime, media composition, aeration rate, etc.) impact host physiology, cell growth, and product TRY. Accordingly, a central task of bioprocess scale-up is to identify and fine tune these process variables to maintain robust and stable production of the desired chemical. This process is often heuristic, and scale-up process development is often seen as more of an art than a science (Crater and Lievense, 2018;Humphrey, 1998). The fundamental reasons for this, is that large scale fermentations are expensive and difficult to predict. A fermentation is a massively multiparametric process that can be affected by the slightest change in any of the number of factors involved in bioreactor conditions. For example, a change in feedstock or water source, inoculation volume, or even altitude of the bioreactor can impact the progress of the fermentation process. Performing thorough fermentation optimization studies in bioreactors is not only expensive, but also time consuming. Each 2L bioreactor test can cost over 1000 USD and last over a week. Hence, scientific methods are needed to accelerate fermentation process development in bioreactors, beyond the current artisanal procedure. Fortunately, modern fermentation systems used during scale up and at commercial plants contain sophisticated process controls, comprehensive data collection and archiving systems, and automation, which can be leveraged for training machine learning algorithms.</p>
<p>The use of machine learning to mine the wealth of online and offline bioprocess data to shed light on the cause of scale-up process failures, and to improve process outcomes, is common (Charaniya et al., 2008;Baughman and Liu, 2014). For example, Coleman et al. (2003) used historical process data to develop a three-step optimization method using decision trees, an ANN ensemble, and a genetic algorithm to identify which process input variables were most important for fermentation modeling, and to select input values that increased product output. To avoid overfitting, process inputs (different fermentation, media, and inoculum conditions -13 total) were sub-selected using decision tree analysis on a data set of 69 fed-batch fermentations, which identified inputs that best corresponded with each process output (biomass density, product concentration, and productivity). This feature selection preprocessing step is common for bioprocess data sets to remove highly correlated or redundant process inputs prior to model training to prevent overfitting (Melcher et al., 2015;Coleman et al., 2003;Charaniya et al., 2008). The subsetted inputs were then used to train ANN ensembles to quantitatively predict each process output. This resulted in a data-driven process model that was used to identify novel input conditions that maximized process outputs via optimization (genetic algorithm). A similar approach combining ANN modeling followed by optimization using a genetic algorithm was taken by Pappu et al. (Pappu and Gummadi, 2017) to optimize fermentation parameters for producing xylitol. The model accurately predicted xylose consumption, biomass density, and xylitol production following training on 27 fermentation batches with multiple inputs, and was used to select new process inputs (pH, agitation speed, and aeration rate) that increased xylitol titers from 59.4 to 65.7 g/L. These examples highlight the ability to generate predictive process models in a data-driven fashion, providing an alternative to more traditional physical-based kinetic models (e.g. Monod or Droop model) that often fail to capture poorly understood relationships between microbial growth and multiple process variables (Kovárová-Kovar and Egli, 1998).</p>
<p>Bioprocess data is highly heterogeneous and requires appropriate data pre-processing to be used for machine learning. Many bioprocess parameters are collected online as continuous measurements (optical (cell) density, pH, dissolved oxygen, oxygen uptake rate, flow rate, offgas production, etc.) while others (e.g., chemical concentrations, substrate consumption rates) are measured offline at discrete time intervals. Additionally, some parameters, such as product concentrations, are only measured at the final time point, while others are categorical or binary (e.g. ON/OFF nutrient feed setting). This results in highly heterogeneous data sets with respect to time and between fermentation runs that require pre-processing to extract temporal trends that compactly and smoothly represent the data, preventing model overfitting (i.e. many more features than instances). For example, instead of using each time point measurement for model training, first and second order derivatives can be used to more compactly represent temporal trends (Cheung and Stephanopoulos, 1990a); (Cheung and Stephanopoulos, 1990b), as can wavelet decomposition methods (Bakshi and Stephanopoulos, 1994), which outperforms more classical smoothing approaches such as Savitzky-Golay. For low and very low signal-to-noise ratios, more recent methods of denoising can be applied, such as mean envelope filter (Merino et al., 2015) or spectral noise reduction by vector casting (Gebrekidan et al., 2020). Other approaches, including discrete Fourier transform and symbolic aggregate approximation (SAX) can be applied, which represent temporal trends as representative segments (e. g. mean over time window) instead of the entire time-series (Charaniya et al., 2008). In addition to reducing the number of timepoints used for model training, temporal offsets between data sets can arise, for example, due to lag phases in growth between fermentation batches. This can be corrected using dynamic time warping strategies that align time profiles between data sets to avoid incorrect comparisons (Chakrabarti et al., 2002); (Keogh and Ratanamahatana, 2005).</p>
<p>The availability of continuous online bioreactor data has also enabled control and optimization of bioprocesses through reinforcement learning. Currently, bioprocesses are controlled manually or using proportional-integral-derivative (PID) controller or model predictive control (MPC) (Qin and Badgwell, 2003) methods that automatically modulate one or more process variables (e.g. feeding rate) to control an output (e.g. temperature, production concentration). While these techniques have been widely used for complex multivariable control applications, they are built upon fixed models of the environment that do not get continuously updated and improved as they see more data. Therefore, there is growing interest in using model-free reinforcement learning methods to learn, through trial and error, the best control algorithm from large online data, and to optimize process operations (for a detailed overview see ). For example, a control policy was learned from online ethanol data to control final ethanol titers during yeast fermentations that had a lower overshoot, faster tracking, shorter transition, and smoother control signal than an advanced PID controller (Li et al., 2011). Reinforcement learning methods have also been demonstrated in simulated systems to control co-culture species biomass abundances and optimize product yields (Treloar et al., 2020), control reactor temperatures (Xie et al., 2020), and to optimize a downstream product separation unit (Hwangbo and Sin, 2020). However, current reinforcement learning methods alone still suffer from requiring large amounts of data for complex multivariable processes, and are often impractical or too costly to implement in real world applications (Shin et al., 2019). Therefore, approaches to improve the sample efficiency of reinforcement learning methods are needed; promising examples include combining them with model-based controllers (Xie et al., 2020) or through transfer learning, where offline model simulations are initially used to train control policies followed by the efficient adaptation of these policies with real online data (Petsagkourakis et al., 2020).</p>
<p>In sum, despite the challenges of high experimental cost and unpredictable nature of fermentations, the wealth of data generated in a single fermentation makes application of machine learning to scale-up an appealing proposal. Machine learning can be used to identify optimal fermentation parameters (i.e. selecting the most appropriate process conditions) and recommend appropriate responses during process upsets (via adaptive process monitoring and control) using the large amount of data that is available. This area may benefit significantly from coupling machine learning with mechanistic modelling (see next section) such as computational fluid dynamics simulations (Haringa et al., 2016(Haringa et al., , 2017.</p>
<p>Machine learning and mechanism</p>
<p>Two paradigms at odds</p>
<p>Whereas the machine paradigm concentrates on enabling predictive power, metabolic engineers typically define scientific value around the understanding of mechanism, because it is perceived to be the road to better performance. Mechanisms are defined as the causally related set of processes and parts that result in the observed phenomena. Understanding these mechanisms has been crucial in the history of microbiology because it results in knowledge that can indeed be leveraged to predict the behavior of a biological system (pathways, strains, products, etc.) and can also be transferred to different systems where the same mechanism is involved. For example, if fosmidomycin is toxic and inhibits 1-deoxy-d-xylulose-5-phosphate reductoisomerase (DXR) in the mevalonate pathway in E. coli, you would expect fosmidomycin to inhibit DXR in another host (Murkin et al., 2014). The kinetics of this inhibition mechanism can also be used to quantitatively predict the corresponding changes in mevalonate pathway flux, based on a Michaelis Menten equation that relates fosmidomycin concentration and DXR reaction rate (i.e. inhibitory dissociation constant, K i ).</p>
<p>While there are a variety of different mechanistic mathematical models that are useful for guiding design, including gene expression models (Ay and Arnosti, 2011), genome-scale models (GSM) (King et al., 2016,Thiele andPalsson, 2010), kinetic models (O. D. O.D. , whole cell models (Karr et al., 2012;Macklin et al., 2020), and process models (Koutinas et al., 2012), many of them fail to provide the accurate quantitative predictions needed to systematically drive metabolic engineering projects in practice. For example, predicting metabolic flux changes due to gene knockouts with GSM remains challenging (O'Brien et al., 2015), even after attempts to improve prediction accuracy by deriving constraints or objective functions from experimental data such as transcriptomics (Machado and Herrgård, 2014). Moreover, kinetic model predictions based on assumed quantitative relationship between inputs (e.g. fosmidomycin concentration) and outputs (e.g. DXR reaction flux) often do not hold in reality (Costa et al., 2010;Heijnen, 2005) and are nearly impossible to parameterize for every enzyme across all growth conditions. A key reason why these models fail is because their mathematical relationships between inputs and outputs are based on ideal conditions (e.g. in vitro for Michaelis Menten equation) that do not capture the complexity of the intracellular environment (e.g. regulation). They also lack the ability to automatically leverage more data to learn and improve prediction performance. If the model predictions fail, it takes a human head to creatively figure out how to correct the model, which often happens too slowly, leaving design to rely on trial-and-error experimental approaches. Therefore, new quantitative prediction frameworks are needed to drive the commercial success of metabolic engineering projects in industry, and bring about the field's full potential (as discussed in the introduction).</p>
<p>Machine learning's flexible data-driven framework can help overcome the challenges facing predictive biology. Machine learning links inputs and outputs (Fig. 2) without needing to understand what happens in between (i.e. the mechanism). Instead of using knowledge-derived mathematical relationships, machine learning models empirically derive input/output relationships (equations) through training on data that can be collected in a higher throughput manner (titers, rates, yields, expression levels, etc.) and can automatically improve prediction performance as more data becomes available. Of course, machine learning approaches have their own limits. They require a large amount of data that is expensive to collect, and which constitutes currently the largest practical bottleneck (see Section 5.1). Moreover, most machine learning algorithms, particularly deep neural networks, are black boxes and difficult to interpret, although this is also improving (see Section 5.3). Therefore, troubleshooting machine learning models to try and achieve further predictive power once performance has plateaued is challenging, especially since a clear connection to mechanism is not available. Accordingly, the preferred type of model is both predictive and mechanistic, and it is by leveraging machine learning with mechanistic models that these types of models can be created.</p>
<p>Integrating biological knowledge and machine learning</p>
<p>It is by integrating machine learning and mechanistic models that the benefits of both approaches can be combined: predictability that systematically increases as more data is available, and mechanistic insight. It is not entirely clear how to proceed about reaching this goal, but there are some budding attempts (Fig. 11). A more comprehensive list of approaches that integrate data-and knowledge-based models can be found in the review by Zampieri et al. (2019).</p>
<p>One interesting avenue to explore is whether machine learning can be used to parameterize mechanistic models. A couple of studies (Andreozzi et al., 2016;Heckmann et al., 2018) demonstrated the potential for this by leveraging a set of machine learning models to predict enzyme catalytic turnover numbers from input features composed of network properties, enzyme structural properties, biochemistry, and assay conditions. Enzyme turnover numbers were then used to parametrize genome-scale models which improved proteome predictions. Similarly, Chakrabarti et al. (2013) used a machine learning approach to identify feasible kinetic parameters for an ORACLE (optimization and risk analysis of complex living entities) kinetic model of metabolism. More generally, deriving biological knowledge from machine learning methods would enable an efficient way to advance scientific understanding from the increasing data deluge coming from multi-omic approaches. While it is not obvious how an actual mechanism can be learnt from purely data-driven machine learning approaches that are based on correlations rather than causation, some recent examples have demonstrated promising results in identifying relationships that are candidates for follow-up experiments to distill mechanisms. For example, Ma et al. (2018) developed a visible neural network (VNN), which couples the model's inner workings to those of a real system, by incorporating knowledge from gene ontologies into a VNN to simultaneously simulate cell hierarchical structure and function. The resulting VNN was optimized for functional prediction (e.g., growth rate) while respecting biological structure (subsystem hierarchy) and was capable of identifying subsystem activity patterns. Another study (Zelezniak et al., 2018) leveraged metabolic network information to predict metabolite concentrations (response) from protein levels (input) in S. cerevisiae mutants through a multilinear regression: metabolite concentrations were expressed as a function of expression levels of the closest enzyme neighbors in the metabolic network. A more general approach called explainable artificial intelligence, XAI (see Section 5.3), presents enourmous potential for providing mechanistic insights within data-driven machine learning models. An algorithm of this type was able to detect enhancer activity in the Drosophila embryo and alternative splicing in human-derived cell lines by systematically capturing high-order interactions between features that are predictive of the response (Basu et al., 2018).</p>
<p>Another possible approach is to incorporate input features derived from mechanistic models into machine learning models to improve their predictive power. For example, Culley et al. (2020) developed a machine learning pipeline for predicting S. cerevisiae growth rate that leveraged transcriptomic data and genome-scale model predicted fluxes as input features. They show that using fluxes predicted from parsimonious flux balance analysis (pFBA) as features combined with transcriptomics data improved the predictive power of neural networks over using transcriptomics data alone. In a similar direction, it would be worthwhile exploring whether synthetic data augmentation based on mechanistic simulations can increase predictive accuracy of machine learning models while learning hypothesized mechanisms underlying the data. Also, mechanistic models can be a useful tool for feature selection for machine learning models. It has been shown that GSMs can be fruitfully leveraged to identify a subset of reactions to then be optimized through machine learning methods .</p>
<p>Finally, incorporating known physical or biological constraints on the solution space of machine learning algorithms can ensure biologically meaningful solutions or rule out possible machine learning solutions that are known to be biologically infeasible. In a study by  various machine learning algorithms were used to predict central carbon metabolic fluxes measured through 13 C Metabolic Flux Analysis (response) from culture and genetic information (input). The best performing machine learning model flux predictions were then changed as minimally as possible to satisfy the stoichiometric constraints provided by a GSM. Similarly, machine learning was used to reconcile empirical genetic interaction data with FBA model predictions (Szappanos et al., 2011).</p>
<p>Inspiring new machine learning from metabolic engineering</p>
<p>Metabolic engineering can also provide inspiration for new machine learning and AI algorithms. Biomimicry was the inspiration for neural networks (Fig. 7), so it is not unreasonable to think that biology can be the inspiration for more and better algorithms. Gene regulatory networks, for example, involve a sophisticated network of molecular interactions that regulate and determine the cell behaviour to sense and react to environmental cues and optimize survival. A full mechanistic understanding of the general principles of how this is achieved for different cells, environments, and threats, could provide valuable insights for new machine learning approaches.</p>
<p>Indeed, metabolic engineering is in a better situation to inspire new machine learning algorithms than other disciplines. While there is no hope that understanding how a neural network identifies a cat will reveal physiologically meaningful information on the brain identification processes, in metabolic engineering we are quite close to the mechanism. Indeed, some of the mechanistic models provide predictions that may not be completely accurate, but are qualitatively acceptable (Lerman et al., 2012;Karr et al., 2012;Macklin et al., 2020). We believe using machine learning to complement the parts of mechanistic models that are less tested can significantly increase their accuracy. These hybrid models can lead to new inspiration for new machine learning architectures and general approaches.</p>
<p>Perspectives for machine learning in metabolic engineering</p>
<p>Major bottlenecks for further application</p>
<p>While the need for improved predictive power fosters the further application of machine learning in metabolic engineering, there are some fundamental obstacles to a wider application. These obstacles are both technical (data and algorithmic challenges) and sociological.</p>
<p>The foremost challenge is undoubtedly the scarcity of the large data sets needed to train machine learning algorithms. The majority of metabolic engineering projects typically involve much less than 100 strains/conditions. Whereas training instances can be multiplied by shrewd data augmentation (see section 2.2.1), it seems unlikely that the current status quo will be able to provide the amount of data routinely found in other fields (several million instances/images in ImageNet (Deng et al., 2009)). This will undoubtedly limit the benefit that metabolic engineering can leverage from machine learning. Another challenge is data quality, which is as important as quantity. High repeatability and low uncertainty are critical characteristics of high-quality data: an experiment must produce similar responses under identical inputs, or there is little hope that an algorithm can be predictive. Furthermore, data sharing is often hampered by the lack of biological data standards needed for this exchange. For example, in the case of multiomics data, there are databases for genes (e.g. Genbank IDs (Benson et al., 2011)), proteins (e.g. Uniprot IDs (The UniProt Consortium, 2017)), metabolites (pubchem IDs (Kim et al., 2016)), and reactions (e.g. BIGG database (King et al., 2016)), but these databases are often not comprehensive (e.g. not all proteins are submitted to Uniprot) and are not fully interlinked (e.g. BIGG metabolites not always have a pubchem entry). While there are efforts to solve this problem (e. g. Metanet X (Moretti et al., 2016), or BioCyc (Karp et al., 2019)), this issue rarely reaches the high profile needed to attract the investment required to completely solve it. Moreover, if the state of data standardization is not good, metadata standardization is in an even worse state. Without an investment in this piece of infrastructure, there is little hope for a disruptive impact of machine learning in metabolic engineering (Fig. 12). A possible solution to several of these problems involves automation (see section 5.2).</p>
<p>A second hurdle involves the adaptation of machine learning algorithms to the special needs of metabolic engineering. Uncertainty quantification is one of the needs of a discipline with small training data sets that is beginning to be met (Radivojević et al., 2020). Explainable AI (XAI) involves creating models such that the reasons for their predictions can be understood by humans (see section 5.3). This is particularly important in metabolic engineering, where we often have, or can easily investigate, the mechanism responsible for a given response. This investigation is much more complicated for other fields like, e.g., artificial vision or astrophysics. The integration of prior biological knowledge into machine learning algorithms, and its extraction from machine learning results is also an area that could provide significant advances in both metabolic engineering and machine learning, as discussed in section 4.</p>
<p>Another, often overlooked, obstacle involves the sociological challenge of having two very different groups working together: machine learning researchers and metabolic engineers. These two crowds are typically trained very differently and there is little intersection among them. Communication is, hence, often complicated by these differences. Furthermore, they are different not only in their skill toolbox, but also in which problems arouse their interest. This creates problems in aligning interests and managing projects. Interaction is, however, necessary: it is becoming impossible even for machine learning researchers to keep abreast of the literature on their field, and the new metabolic engineering tools (e.g., CRISPR-based gene editing, cell-free engineering) are posing a similar challenge in this field. Only through an interdisciplinary effort can the best of both disciplines be combined to create something bigger than the sum of the parts.</p>
<p>Integrating machine learning and synthetic biology with automation</p>
<p>As indicated above, the training data for machine learning must be high-quality, in the sense that it must avoid biases due to inconsistent protocols and provide quantification for repeatability (see section 2.4). Both goals can be systematically achieved through automation, which is one of the main reasons the intersection of machine learning, synthetic biology, and automation is thriving (Carbonell et al., 2019). Biological and chemical sciences data are nowadays growing at an unprecedented pace, but the databases aggregating biological and chemical findings are usually biased (Rodrigues, 2020). To avoid this bias, it is highly desirable to start veering away from the traditional approach of one entire PhD per molecule or one scientist performing the full metabolic engineering process, in order to adopt the creation and maintenance of integrated engineering pipelines (Fig. 13). This is the path embodied by biofoundries Hillson et al., 2019). This goal can be achieved by extending current automation pipelines for machine learning (Olson and Moore, 2018). Pipelines are fully or semi-automated infrastructure that realize a procedure in a systematic manner: e.g., phenotyping through proteomics, strain construction, fermentation. Automated pipelines facilitate consistent protocols and reproducibility in synthetic biology (Jessop-Fabre and Sonnenschein, 2019), and have the capability to produce the amount of data required by machine learning. Fully automated and integrated DBTL pipelines have already been successfully adopted for the identification and optimization of biosynthetic pathways . In general, we expect machine learning, biochemical analytical techniques and automation to follow a path of parallel development and keep symbiotically interacting in pipelines so that machine learning will be a pillar in every step of biosystems design (Volk et al., 2020).</p>
<p>Automating metabolic engineering often involves multiplexing the bioengineering efforts to parallelize a set of combinatorial experiments. For example, digital microfluidics is a high-throughput liquid handling technique able to quickly automate diverse biological experiments at micro and nanoscales, thus accelerating the DBTL cycles and making synthetic biology programmable (Gach et al., 2017;Kothamachu et al., 2020). The combination of microfluidics with nanofluidics and optoelectronics has been used for the automated growth and analysis of thousands of cell lines in parallel on a single chip . Another implementation of these technologies enables the parallel construction and optical screening of tens of thousands of synthetic microbial communities per day (Kehe et al., 2019). Other efforts focus on parallelizing experiments while maintaining their potential to efficiently scale up. That is the case of automated workflows for media optimization, induction profiling, or microbial bioprocess optimization leveraging the Biolector, a microtiter plate-based cultivation device (Rohe et al., 2012). Another example of productive scale up involves the Automated Microscale Bioreactor (Ambr 250), which can generate comparable cell growth and protein production profiles comparable to those obtained in 1000-L bioreactor industrial scale fermentations .</p>
<p>Some automation technologies focus on the human-to-system interface and embrace AI to further accelerate the experimentation processes. Robotic Process Automation (RPA) is an alternative approach that provides agents (bots) that operate on user interfaces in the way a human would do (van der Aalst et al., 2018). RPA is meant to replace humans in repetitive work that is frequent enough to make fully automation economically feasible. Intelligent RPA (IRPA) is the current effort to fuse RPA with advanced AI methods to drastically extend its scope (Syed et al., 2020). Combining experimentation platforms with AI to accelerate experimental research is at the core of the so-called self-driving laboratories (Häse et al., 2019), which typically use multi-objective optimization techniques (Häse et al., 2018) and iterate over the design, execution, and learning steps of the experiments with complete autonomy (MacLeod et al., 2020). The use of AI-driven automation Fig. 12. The hierarchy of needs for leveraging machine learning in metabolic engineering. It is futile to rely on machine learning to guide metabolic engineering without first establishing the basic infrastructure that it depends on. The very base consists on creating the infrastructure to physically collect large amounts of high quality data. The next step is to have the databases, standard and ontologies to structure and store the data appropriately. Data cleaning and outlier detection follow. The base for simple machine learning algorithms (linear regression), feature selection and algorithm training is at this point set. It is only at this stage that sophisticated machine learning and deep learning can significantly improve the metabolic engineering practice. Adapted from Rogati (Rogati, 2017).</p>
<p>Fig. 13. Traditional metabolic engineering vs pipeline.</p>
<p>The traditional metabolic engineering process involves a single researcher doing all phases of the project from pathway choice to strain building, fermentation, and data analysis. The pipeline approach instead focuses resources on creating a single, flexible, semi automated, pipeline consisting of different connected services supported by specialized teams. The pipeline approach favors repeatability, data quality and the stream of data required by machine learning. Furthermore, the pipeline allows for simultaneous development of multiple strains, so knowledge obtained from one design can immediately be leveraged for all others. BioCAD: Biological Computer-Aided Design; BioCAM: Biological Computer-Aided Manufacturing of Synthetic DNA (Oberortner et al., 2020). technologies with hardware robotics represents a step further. In that sense, a very recent automation effort has used state of the art robotics to completely move the focus from automating the instruments to automate the researcher (Burger et al., 2020).</p>
<p>Cloud labs are tools based on cloud technologies (Xu, 2012) that allow a scientist to remotely conduct biological research through robotic control, by using a high-level interface to ease the requirement for any programming knowledge. As an added benefit, researchers usually get all the intermediate and final results stored on the cloud in digital formats prepared for downstream analysis by local or cloud computing (Mell and Grance, 2011). Past years have seen an emergence of cloud labs (Check Hayden, 2014) and tools (Bates et al., 2017), which has been recently boosted by the social distancing requirements of the SARS-CoV2 pandemic. Thus, a remote or distributed manner of experimentation is arising as an alternative to the local or centralized classic model.</p>
<p>Interestingly, COVID-19 has also promoted the do-it-yourself (DIY) approach to lab automation. In 1981, IBM introduced the personal computer (PC), democratizing computing with an open architecture model (Miller, 1989;O'Regan, 2012), and producing a paradigm shift. An equivalent shift for automation seems to be in motion, due to the combination of the maturity of the open source model with the rise of free open scientific hardware (FOSH), now accelerated by the SARS-CoV2 pandemic (Maia Chagas et al., 2020). This trend in automation emerged from the use of 3D printing for a growing number of scientific and engineering applications in the laboratory (Silver, 2019). This pure DIY approach has already produced successful high-throughput automation platforms for bioengineering  and is susceptible to improvement by machine learning techniques such as deep reinforcement learning (Treloar et al., 2020). Some companies, such as Opentrons, are taking advantage of this new market niche and are providing open automation solutions halfway between the extreme DIY and the classical automation (May, 2019) based on proprietary and expensive equipment and consumables (Maia Chagas, 2018). There are already some open automation systems built on top of Opentrons liquid handling robots and devoted to synthetic biology applications, such as the DNA-BOT for automated DNA assembly (Storch et al., 2020). On the other hand, a low-cost modular FOSH liquid handler has been recently combined with machine learning for automatizing droplet experiments with AI-enabled computer vision (Faiña et al., 2020). Considering all of the above, it seems that there are technological developments quickly converging towards open hardware and software automation solutions based on machine learning and specific for synthetic biology.</p>
<p>Novel machine learning techniques to watch</p>
<p>Deep learning, with applications using several interconnected layers of ANNs (see Figs. 7 and 8), has been the subfield of machine learning driving the recent boost of AI. The number of such layers of ANNs is the depth of the neural network. With increasing depths, deep neural networks often have a large number of parameters. For example, a state-ofthe-art system for natural language processing (NLP) (Manning, 1999), the autoregressive language model GPT-3 , has almost one hundred layers and 175 billion parameters. These DL systems are intricate black boxes making decisions that are not easily interpretable from a human perspective. If a prediction deviates from the expected answer, it is generally not easy to understand why it failed, or how to correct the issue. These algorithms are only as good as the data they are trained with, so biases in the data have a significant impact on the predictions (Rodrigues, 2020), with a growing need for developing bias quantification metrics along with methods for overfitting detection and data debiasing (Ellingson et al., 2020).</p>
<p>The lack of interpretability has prevented machine learning in general and DL in particular from expanding in some fields that require trust in the underlying technology, such as in defense, healthcare, and other sensitive applications. Different novel approaches are under active research to overcome this critical drawback. Some of these try to make classic machine learning methods such as random forests more interpretable without a loss of efficacy (Basu et al., 2018). Another technique is even able to extract explicit physical relations by applying symbolic regression to components of a Graph Neural Network (GNN) trained by encouraging sparse latent representations in a supervised setting (Cranmer et al., 2020). In drug discovery, the lack of transparent and reproducible workflows has hindered widespread adoption of machine learning models, but this is being solved by novel scalable pipelines with traceable models stressing uncertainty quantification (Minnich et al., 2020).</p>
<p>In 2017, DARPA launched its explainable artificial intelligence (XAI) program as a comprehensive strategy to tackle the machine learning interpretability problem. DARPA's XAI aims at developing superior AI systems able to have a symbiotic relationship with humans (Gunning and Aha, 2019). A recent evolution on top of the XAI paradigm is the concept of Responsible AI (Barredo Arrieta et al., 2020), which imposes further constraints on the implementation of AI systems, like transparency, accountability, and ethics. However, the movement towards greater interpretability involves significant trade-offs in terms of performance, with a toll on fidelity and accuracy . Ultimately, that compromise could be rendered unnecessary by advances in high performance computing (HPC), since AI and HPC are converging in approaching the exascale era (Gwynne, 2019). Indeed, the joint effort of XAI developments with exascale computing, by bridging the gaps between cutting-edge research and sustainable policies, could pave the way for designing practical solutions to global challenges such as climate change (Streich et al., 2020).</p>
<p>XAI has numerous applications in unraveling the profound mechanics of natural or artificial systems, such as the molecular mechanisms underlying genome biology (Basu et al., 2018). A related DL framework is the use of physics-informed neural networks (PINN), which are trained to solve supervised forward and inverse problems involving nonlinear partial differential equations (PDE), thus supporting the union of data-driven and mathematical models (Raissi et al. 2019(Raissi et al. , 2020. In the case of very noisy data, Bayesian Neural Networks can be combined with PINNs (called then B-PINNs) to both avoid overfitting and quantify uncertainty (Yang et al., 2020).</p>
<p>Transfer learning (TL) (Ando and Zhang, 2005;Caruana, 1997;Pan and Yang, 2010) is the technique of knowledge transfer from a domain with enough training data to another related domain of interest that lacks such data. This transfer considerably enhances the learning performance by avoiding costly data-labeling efforts. This area is under rapid expansion but already offers many consolidated models from which to choose carefully depending on the type of application and its data (Zhuang et al., 2020). For example, TL has been used to tackle the problem of predicting associations between genotype and phenotype (Petegrosso et al., 2017). Clearly, TL could be key for different metabolic engineering projects if used to transfer predictive capabilities from one organism to another, avoiding the cost and time expenses of getting large multiomics data sets from scratch. Finally, TL can be combined with XAI methods, for instance, for gathering pathway and metabolic information in model organisms and translate it to others so as to get comprehensive genome-scale metabolic models in an efficient manner.</p>
<p>Conclusion</p>
<p>Machine learning provides an opportunity to make metabolic engineering more predictable and efficient. In this review, we have attempted to provide an introduction to this discipline in terms that are relatable to metabolic engineers, as well as providing illustrative examples along the traditional phases of metabolic engineering (from pathway choice and construction to scaling). We have also included practical advice including library suggestions and experimental design recommendations. Finally, we have examined the perspectives for this combination of disciplines, which are particularly relevant and difficult to predict, given the current explosive growth of both machine learning and synthetic biology.</p>
<p>In our opinion, metabolic engineering could take two courses in the future: incremental or disruptive. In one, traditional methods prevail, progress is incremental, and more molecules are arduously brought into commercial use at an increasing rate. In another one, metabolic engineering fully embraces and integrates the possibilities afforded by automation and machine learning. This choice leads to a disruptive change that makes production of new molecules a relatively easy task dwarfed by the more ambitious goals enabled by the new predictive capabilities. Metabolic engineering is used to engineer microbiomes, create new biomaterials, provide fundamental understanding of emergent properties and evolution, and suggest new artificial intelligence approaches.</p>
<p>The fundamental challenges for the disruptive path involve enabling streams of high-quality data, developing new algorithms to integrate the advantages of data-driven and mechanistic approaches, and fully leveraging novel tools in machine learning and synthetic biology. In our view, solving these challenges is only possible through a multidisciplinary collaboration of scientists including metabolic engineers, biochemists, microbiologists, computer scientists, electrical engineers, chemical engineers, mathematicians, statisticians, and physicists, among others. We hope to have provided in this review a helpful resource for that multidisciplinary collaboration. </p>
<p>Fig. 1 .
1Machine learning vs Artificial intelligence vs.</p>
<p>Fig. 5 .
5Fig. 5. Example of a supervised machine learning algorithm: a decision tree. Decision trees come from an abstracted view of how human learning works, rather than a mechanistic understanding. Decisions trees automatically build a decision "flowchart" that, in this case, predicts high or low production based on the protein expression levels. An example training data set and corresponding decision tree are shown in panels A and B, respectively, based on a set of strains (instances) and their production (response) depending on different protein expression levels (input features). Using the training data set, the algorithm decides on the optimal split points (x 1 , x 2 and y 1 ) to predict the production based on the input features. The split points are the parameters of the algorithm: more parameters will allow the algorithm to describe more instances. The algorithm also has a number of "hyperparameters" which are set before training, including the maximum tree depth, and the minimum number of instances required to split a node, among others (see scikit learn library for more details). Decision trees form the base for one of the most popular algorithms: the random forest. The random forest algorithm is just an ensemble of decision trees.</p>
<p>Fig. 8 .
8Deep Learning involves artificial neural networks many layers deep. (A) Deep learning methods have been shown to improve performance with the amount of training data when other methods plateau (B)</p>
<p>Fig. 9 .
9The input phase space. It is a multidimensional space composed by all possible configurations of a system. Each axis represents, for example, the expression level for a protein p i (or any other variable such as nucleobase for each position, transcription level, promoter, fermentation condition etc.) required to specify the input state of a system. Hence, a point in the space (blue) corresponds to a unique possible state of the system, consisting of e.g. expression levels for each protein in the pathway considered. The volume of this space, representing all possible states, grows exponentially with the number of variables. (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)Fig. 10. The scikit-learn library is our top recommendation for machine learning beginners. This library provides a wide range of supervised and unsupervised algorithms, as well as practical advice on how to choose among them. Image obtained from scikit-learn github repository (https://github.com/scikit-learn/scikit -learn,Pedregosa et al., 2011).</p>
<p>the Inventory of Composable Elements (ICE) (Ham et al., 2012), DICOM-SB (Sainz de Murieta et al., 2016), SynBioHub (McLaughlin et al., 2018), ProteomeXchange(Vizcaíno et al., 2014), MetaboLights(Haug et al., 2013), BioGraph-IIn(Gonzalez-Beltran et al., 2013)</p>
<p>Fig. 11 .
11Integrating machine learning and mechanistic models. (A) Parametrizing GSMs using machine learning predictions; (B) Using GSMs to derive input features for machine learning; (C) constraining the machine learning solution space with mechanism.</p>
<p>CRediT authorship contribution statement Christopher E. Lawson: Writing -review &amp; editing, Writingoriginal draft, All authors contributed to the writing of this review. Jose Manuel Martí: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Tijana Radivojevic: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Sai Vamshi R. Jonnalagadda: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Reinhard Gentz: Writingreview &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Nathan J. Hillson: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Sean Peisert: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Joonhoon Kim: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Blake A. Simmons: Writingreview &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Christopher J. Petzold: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Steven W. Singer: Writing -review &amp; editing, Writingoriginal draft, All authors contributed to the writing of this review. Aindrila Mukhopadhyay: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Deepti Tanjore: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Joshua G. Dunn: Writingreview &amp; editing, Writing -original draft, All authors contributed to the writing of this review. Hector Garcia Martin: Writing -review &amp; editing, Writing -original draft, All authors contributed to the writing of this review.</p>
<p>Table 1
1Machine learning applications for metabolic engineering.Task 
Application 
Input features 
Algorithm 
Response 
Ref. </p>
<p>Identify ORF 
Identify signal peptide 
(SignalP 5.0) </p>
<p>20,758 protein amino acid sequence 
deep RNN 
presence or absence of 
signal peptide </p>
<p>Armenteros et al. 
(2019) 
ORF prediction 
(DeepRibo) </p>
<p>626,708 candidate ORF DNA 
sequences and ribo-seq signal 
(alignment file) from 7 species </p>
<p>RNN and CNN 
translation initiation site 
and translated open 
reading frames </p>
<p>Clauwaert et al. 
(2019) </p>
<p>ORF prediction 
(REPARATION) </p>
<p>67,158 candidate ORF DNA sequences 
and ribo-seq signal (alignment file) 
from 4 species </p>
<p>random forest 
translation initiation site 
and translated open 
reading frames </p>
<p>Ndah et al. (2017) </p>
<p>Annotate ORF 
Annotate enzyme 
(DeepEC) </p>
<p>1,388,606 protein sequences and 
4669 EC numbers </p>
<p>CNN 
enzyme commission (EC) 
numbers </p>
<p>Ryu et al. (2019) </p>
<p>Annotate enzyme 
(ECPred) </p>
<p>11,018 protein amino acid sequences, 
subsequence extraction and peptide 
physicochemical properties </p>
<p>ensemble classifier (BLAST-kNN, 
Pepstats-SVM and SPMap) </p>
<p>EC numbers 
Dalkiran et al. (2018) </p>
<p>Annotate enzyme 
(DEEPre) </p>
<p>22,168 protein amino acid sequence 
CNN and RNN 
EC numbers 
Le et al., 2018 </p>
<p>Annotate enzyme 
(EnzyNet) </p>
<p>63,558 protein sequences represented 
as voxel-based protein spatial structure </p>
<p>CNN 
EC numbers 
Amidi et al., 2018 </p>
<p>Enzyme &amp; 
Pathway 
design </p>
<p>Automated enzyme 
search </p>
<p>10,951 compounds and 6556 
reactions. Features represented as 
reaction signature and enzyme amino 
acid sequence </p>
<p>support vector machines 
positive or negative 
enzyme-reaction pairs </p>
<p>Faulon et al. (2008) </p>
<p>Automated enzyme 
search </p>
<p>7318 reactions and 9001 enzymes. 
Features represented as reaction 
signature and enzyme amino acid 
sequence pair </p>
<p>gaussian process model 
positive or negative 
enzyme-reaction pairs, 
and </p>
<p>Mellor et al. (2016) </p>
<p>Michaelis constant KM 
(substrate affinity) 
Directed evolution 
805 protein amino acid sequence 
variants </p>
<p>linear, kernel, neural network, and 
ensemble methods </p>
<p>protein fitness 
Wu et al. (2019) </p>
<p>Directed evolution 
585,199 protein amino acid sequence 
variants </p>
<p>partial least-squares linear 
regression </p>
<p>bacterial halohydrin 
dehalogenase 
productivity </p>
<p>Fox et al. (2007) </p>
<p>and multivariate optimization 
Directed evolution 
218 protein amino acid sequence 
variants </p>
<p>gaussian process 
fluorescent protein color 
Saito et al. (2018) </p>
<p>Directed evolution 
4716 protein amino acid sequence 
variants </p>
<p>gaussian process 
protein thermostability 
Romero et al. (2013) </p>
<p>Rational protein 
design (UniRep) </p>
<p>~24 millon protein amino acid 
sequence </p>
<p>RNN 
protein feature 
representation </p>
<p>Alley et al. (2019) </p>
<p>Rational protein 
design </p>
<p>96 protein amino acid sequence 
variants (UniRep encoding) </p>
<p>UniRep pretraining + linear 
regression (ridge, lasso-lars, 
ensemble) </p>
<p>protein fitness 
Biswas et al. (2020) </p>
<p>Rational protein 
design (BioSeqVAE) </p>
<p>protein amino acid sequence 
residual neural networks 
protein representation 
Costello and Garcia 
Martin, 2019 
Synthetic pathway 
design (RetroPath RL) </p>
<p>N/A 
Monte Carlo Tree Search 
reinforcement learning </p>
<p>metabolic pathway 
Koch et al. (2020) </p>
<p>Pathway 
optimization </p>
<p>Promoter design 
675,000 constitutive and 327,000 
inducible promoter sequences </p>
<p>CNN 
gene expression activity 
Kotopka and Smolke, 
2020 
Promoter design 
100 mutated promoter and RBS 
sequences </p>
<p>neural network 
promoter strength 
Meng et al. (2013) </p>
<p>Promoter design 
promoter amino acid sequence 
neural network 
promoter strength 
Tunney et al., 2018 
Riboswitch design 
biophysical properties (entropy, stem 
melting temperature, GC content, 
length, free energy, etc) from 96 
riboswitch aptamer sequences </p>
<p>Random forest and CNN 
dynamic range of gene 
expression between ON/ 
OFF states </p>
<p>Groher et al. (2019) </p>
<p>Plasmid design 
(SelProm) </p>
<p>120 plasmid sequences 
partial least-squares regression 
promoter strength, 
induction time, inducer 
concentration </p>
<p>Jervis et al. (2019a) </p>
<p>multi-gene pathway 
optimization (MiYA) </p>
<p>24 strains, different promoter 
combinations </p>
<p>neural network 
β-carotene and violacein 
production </p>
<p>Zhou et al. (2018) </p>
<p>multi-gene pathway 
optimization </p>
<p>156 strains, different RBS sequences 
support vector machines and 
neural network </p>
<p>limonene production titer 
Jervis et al. (2019b) </p>
<p>multi-gene pathway 
optimization 
(BioAutomata) </p>
<p>136 strains, different promoter and 
RBS sequences </p>
<p>gaussian process and Bayesian 
optimization </p>
<p>lycopene production titer 
HamediRad et al. 
(2019) </p>
<p>(Automated 
Recommendation Tool 
(ART) </p>
<p>promoter combinations, multi-omics 
data, etc </p>
<p>Bayesian ensemble model 
chemical production titer, 
rate, yield </p>
<p>Radivojević et al. 
(2020) </p>
<p>multi-gene pathway 
optimization </p>
<p>250 strains, different promoter 
combinations </p>
<p>probabilistic ensemble model 
(ART) and Bayesian optimization 
(EVOLVE algorithm) </p>
<p>tryptophan production 
titer </p>
<p>Zhang et al. (2020) </p>
<p>Multi-gene Pathway 
optimization </p>
<p>12 strains, 4 biological replicates per 
strain </p>
<p>Ensemble model (random forest, 
polynomial, multilayer 
perceptron, TPOT meta-learner) </p>
<p>Dodecanol production 
Opgenorth et al. 
(2019) </p>
<p>CRISPR 
sgRNA Scorer 2.0, 
CRISPR activity </p>
<p>430 sgRNA sequences 
support vector machine 
sgRNA on-target activity 
Chari et al. (2017) </p>
<p>(continued on next page) </p>
<p>C.E. Lawson et al. </p>
<p>Table 1 (continued )
1Task 
Application 
Input features 
Algorithm 
Response 
Ref. </p>
<p>Azimuth, CRISPR 
activity </p>
<p>4390 sgRNA sequences 
support vector machine with 
logistical regression </p>
<p>sgRNA on-target and off-
target activity </p>
<p>Doench et al. (2016) </p>
<p>Seq-DeepCpf1, 
CRISPR activity </p>
<p>16,292 sgRNA sequences 
CNN 
sgRNA on-target activity 
Kim et al. (2018) </p>
<p>Elevation, CRISPR 
activity </p>
<p>299,387 sgRNA-target pairs 
gradient boosted regression trees 
sgRNA off-target activity 
Listgarten et al. (2018) </p>
<p>CRISPR activity 
294,534 sgRNA-target pairs 
CNN and deep feedforward neural 
network </p>
<p>sgRNA off-target activity 
Lin and Wong (2018) </p>
<p>DeepCRISPR, CRISPR 
activity </p>
<p>0.68 billion sgRNA sequences 
(unlabeled pre-training data) 
~160,000 sgRNA-target pairs (off-
target data sets) ~200,000 sgRNA seq 
uences (on-target data set) </p>
<p>deep convolutional denoising 
neural network and CNN </p>
<p>sgRNA on-target and off-
target activity </p>
<p>Chuai et al. (2018) </p>
<p>Outlier 
Detection </p>
<p>Novelty and Outlier 
Detection </p>
<p>Any training data set 
isolation forest, local outlier 
factor, one-class SVM, and elliptic 
envelope </p>
<p>outlier identification 
https://scikit-learn.or 
g/stable/modules/out 
lier_detection.html 
Omics Data 
Processing </p>
<p>Prosit, peptide 
identification </p>
<p>550,000 tryptic peptides 
bi-directional RNN 
peptide chromatographic 
retention time and 
tandem mass spectra </p>
<p>Gessulat et al. (2019) </p>
<p>Peakonly, metabolite 
peak detection </p>
<p>4000 regions of interest, labeled as 
noise, one or more peaks, or uncertain 
peak </p>
<p>CNN 
peak detection + 
integration (peak area) </p>
<p>Melnikov et al. (2020) </p>
<p>Bioprocess 
Control &amp; 
optimization </p>
<p>Bioprocess 
optimization </p>
<p>69 fed-batch fermentations, 13 process 
features (fermentation conditions, 
inoculum conditions, media variables) </p>
<p>three-step optimization method 
using decision trees, neural 
network, and hybrid genetic 
algorithm </p>
<p>maximum cell 
concentration, product 
concentration, and 
productivity </p>
<p>Coleman et al. (2003) </p>
<p>Bioprocess 
optimization </p>
<p>25 fed-batch fermentations, 11 process 
features (temperature, induction 
strength, growth rate, process 
variables) + spectroscopic information </p>
<p>data preprocessing, random forest 
and neural network </p>
<p>cell dry mass, 
recombinant soluble 
protein conc., inclusion 
bodies conc. </p>
<p>Melcher (2015) </p>
<p>Bioprocess 
optimization </p>
<p>27 batch fermentations, 7 process 
features (time, pH, temperature, kLa, 
biomass, xylose, glycerol) </p>
<p>regression and neural network 
coupled to genetic algorithm for 
optimization </p>
<p>xylitol production 
Pappu and Gummadi 
(2017) </p>
<p>Bioprocess control &amp; 
real-time optimization </p>
<p>continuous bioreactor, 24 h duration 
with measurement/action every 5 min </p>
<p>Neural network fitted Q-learning 
algorithm (reinforcement 
learning) </p>
<p>control species biomass 
ratio; maximize product 
yield </p>
<p>Treloar et al. (2020) </p>
<p>Process control 
600 temperature measurement/action 
timesteps (episodes) </p>
<p>Model Predictive Control (MPC) 
guided deep deterministic policy 
gradient (reinforcement learning). 
Policy parameterized by neural 
network </p>
<p>Reactor (CSTR) 
temperature control </p>
<p>Xie et al., 2020 </p>
<p>Real-time process 
optimization </p>
<p>500 measurement/action episodes 
Policy gradient parameterized by a 
recurrent neural network. Transfer 
learning from offline training on 
mechanistic model </p>
<p>Maximize product yield 
(phycocyanin) </p>
<p>Petsagkourakis et al., 
2020 </p>
<p>Process control 
21 measurement/action episodes 
multi-step action Q-learning 
controller based on fuzzy k 
selector </p>
<p>ethanol concentration 
control </p>
<p>Li et al., 2011 </p>
<p>C.E.Lawson et al. <br />
AcknowledgementsThis work was part of the the Agile BioFoundry (http://agilebiofo undry.org) and the DOE Joint BioEnergy Institute (http://www.jbei. org), supported by the U. S. Department of Energy, Energy Efficiency and Renewable Energy, Bioenergy Technologies Office, and the Office of Science, through contract DE-AC02-05CH11231 between Lawrence Berkeley National Laboratory and the U. S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy. gov/downloads/doe-public-access-plan). This research is also supported by the Basque Government through the BERC 2014-2017 program and by the Spanish Ministry of Economy and Competitiveness MINECO: BCAM Severo Ochoa excellence accreditation SEV-2013-0323.
CasFinder: flexible algorithm for identifying specific Cas9 targets in genomes. J Aach, P Mali, G M Church, 10.1101/005074Aach, J., Mali, P., Church, G.M., 2014. CasFinder: flexible algorithm for identifying specific Cas9 targets in genomes. BioRxiv. https://doi.org/10.1101/005074.</p>
<p>Isoprenoid pathway optimization for Taxol precursor overproduction in Escherichia coli. P K Ajikumar, W.-H Xiao, K E J Tyo, Y Wang, F Simeon, E Leonard, O Mucha, T H Phon, B Pfeifer, G Stephanopoulos, 10.1126/science.1191652Science. 330Ajikumar, P.K., Xiao, W.-H., Tyo, K.E.J., Wang, Y., Simeon, F., Leonard, E., Mucha, O., Phon, T.H., Pfeifer, B., Stephanopoulos, G., 2010. Isoprenoid pathway optimization for Taxol precursor overproduction in Escherichia coli. Science 330, 70-74. https:// doi.org/10.1126/science.1191652.</p>
<p>Enzyme informatics. R G Alderson, L De Ferrari, L Mavridis, J L Mcdonagh, J B O Mitchell, N Nath, 10.2174/156802612804547353Curr. Top. Med. Chem. 12Alderson, R.G., De Ferrari, L., Mavridis, L., McDonagh, J.L., Mitchell, J.B.O., Nath, N., 2012. Enzyme informatics. Curr. Top. Med. Chem. 12, 1911-1923. https://doi.org/ 10.2174/156802612804547353.</p>
<p>Unified rational protein engineering with sequence-based deep representation learning. E C Alley, G Khimulya, S Biswas, M Alquraishi, G M Church, 10.1038/s41592-019-0598-1Nat. Methods. 16Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., Church, G.M., 2019. Unified rational protein engineering with sequence-based deep representation learning. Nat. Methods 16, 1315-1322. https://doi.org/10.1038/s41592-019-0598-1.</p>
<p>Principal component analysis of proteomics (PCAP) as a tool to direct metabolic engineering. J Alonso-Gutierrez, E.-M Kim, T S Batth, N Cho, Q Hu, L J G Chan, C J Petzold, N J Hillson, P D Adams, J D Keasling, H Garcia Martin, T S Lee, 10.1016/j.ymben.2014.11.011Metab. Eng. 28Alonso-Gutierrez, J., Kim, E.-M., Batth, T.S., Cho, N., Hu, Q., Chan, L.J.G., Petzold, C.J., Hillson, N.J., Adams, P.D., Keasling, J.D., Garcia Martin, H., Lee, T.S., 2015. Principal component analysis of proteomics (PCAP) as a tool to direct metabolic engineering. Metab. Eng. 28, 123-133. https://doi.org/10.1016/j. ymben.2014.11.011.</p>
<p>AlphaFold at CASP13. M Alquraishi, 10.1093/bioinformatics/btz422Bioinformatics. 35AlQuraishi, M., 2019. AlphaFold at CASP13. Bioinformatics 35, 4862-4865. https://doi. org/10.1093/bioinformatics/btz422.</p>
<p>EnzyNet: enzyme classification using 3D convolutional neural networks on spatial representation. A Amidi, S Amidi, D Vlachakis, V Megalooikonomou, N Paragios, E I Zacharaki, PeerJ. 64750Amidi, A., Amidi, S., Vlachakis, D., Megalooikonomou, V., Paragios, N., Zacharaki, E.I., 2018. EnzyNet: enzyme classification using 3D convolutional neural networks on spatial representation. PeerJ 6, e4750.</p>
<p>A framework for learning predictive structures from multiple tasks and unlabeled data. R K Ando, T Zhang, J. Mach. Learn. Res. 6Ando, R.K., Zhang, T., 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res. 6, 1817-1853.</p>
<p>iSCHRUNK-In silico approach to characterization and reduction of uncertainty in the kinetic models of genome-scale metabolic networks. S Andreozzi, L Miskovic, V Hatzimanikatis, 10.1016/j.ymben.2015.10.002Metab. Eng. 33Andreozzi, S., Miskovic, L., Hatzimanikatis, V., 2016. iSCHRUNK-In silico approach to characterization and reduction of uncertainty in the kinetic models of genome-scale metabolic networks. Metab. Eng. 33, 158-168. https://doi.org/10.1016/j. ymben.2015.10.002.</p>
<p>SignalP 5.0 improves signal peptide predictions using deep neural networks. J J A Armenteros, K D Tsirigos, C K Sønderby, T N Petersen, O Winther, S Brunak, G Von Heijne, H Nielsen, 10.1038/s41587-019-0036-zNat. Biotechnol. 37Armenteros, J.J.A., Tsirigos, K.D., Sønderby, C.K., Petersen, T.N., Winther, O., Brunak, S., von Heijne, G., Nielsen, H., 2019. SignalP 5.0 improves signal peptide predictions using deep neural networks. Nat. Biotechnol. 37, 420-423. https://doi.org/10.1038/ s41587-019-0036-z.</p>
<p>Mathematical modeling of gene expression: a guide for the perplexed biologist. A Ay, D N Arnosti, 10.3109/10409238.2011.556597Crit. Rev. Biochem. Mol. Biol. 46Ay, A., Arnosti, D.N., 2011. Mathematical modeling of gene expression: a guide for the perplexed biologist. Crit. Rev. Biochem. Mol. Biol. 46, 137-151. https://doi.org/ 10.3109/10409238.2011.556597.</p>
<p>The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000. A Bairoch, R Apweiler, 10.1093/nar/28.1.45Nucleic Acids Res. 28Bairoch, A., Apweiler, R., 2000. The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000. Nucleic Acids Res. 28, 45-48. https://doi.org/ 10.1093/nar/28.1.45.</p>
<p>Representation of process trends-III. Multiscale extraction of trends from process data. B R Bakshi, G Stephanopoulos, 10.1016/0098-1354(94)85028-3Comput. Chem. Eng. 18Bakshi, B.R., Stephanopoulos, G., 1994. Representation of process trends-III. Multiscale extraction of trends from process data. Comput. Chem. Eng. 18, 267-302. https:// doi.org/10.1016/0098-1354(94)85028-3.</p>
<p>Genome-scale engineering of Saccharomyces cerevisiae with single-nucleotide precision. Z Bao, M Hamedirad, P Xue, H Xiao, I Tasan, R Chao, J Liang, H Zhao, 10.1038/nbt.4132Nat. Biotechnol. 36Bao, Z., HamediRad, M., Xue, P., Xiao, H., Tasan, I., Chao, R., Liang, J., Zhao, H., 2018. Genome-scale engineering of Saccharomyces cerevisiae with single-nucleotide precision. Nat. Biotechnol. 36, 505-508. https://doi.org/10.1038/nbt.4132.</p>
<p>Explainable Artificial Intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. A Barredo Arrieta, N Díaz-Rodríguez, J Del Ser, A Bennetot, S Tabik, A Barbado, S Garcia, S Gil-Lopez, D Molina, R Benjamins, R Chatila, F Herrera, 10.1016/j.inffus.2019.12.012Inf. Fusion. 58Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garcia, S., Gil-Lopez, S., Molina, D., Benjamins, R., Chatila, R., Herrera, F., 2020. Explainable Artificial Intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 58, 82-115. https://doi.org/10.1016/j. inffus.2019.12.012.</p>
<p>Deep scanning lysine metabolism in Escherichia coli. M C Bassalo, A D Garst, A Choudhury, W C Grau, E J Oh, E Spindler, T Lipscomb, R T Gill, 10.15252/msb.20188371Mol. Syst. Biol. 14Bassalo, M.C., Garst, A.D., Choudhury, A., Grau, W.C., Oh, E.J., Spindler, E., Lipscomb, T., Gill, R.T., 2018. Deep scanning lysine metabolism in Escherichia coli. Mol. Syst. Biol. 14, e8371 https://doi.org/10.15252/msb.20188371.</p>
<p>Iterative random forests to discover predictive and stable high-order interactions. S Basu, K Kumbier, J B Brown, B Yu, 10.1073/pnas.1711236115Proc. Natl. Acad. Sci. U.S.A. 115Basu, S., Kumbier, K., Brown, J.B., Yu, B., 2018. Iterative random forests to discover predictive and stable high-order interactions. Proc. Natl. Acad. Sci. U.S.A. 115, 1943-1948. https://doi.org/10.1073/pnas.1711236115.</p>
<p>Wet lab accelerator: a web-based application democratizing laboratory automation for synthetic biology. M Bates, A J Berliner, J Lachoff, P R Jaschke, E S Groban, 10.1021/acssynbio.6b00108ACS Synth. Biol. 6Bates, M., Berliner, A.J., Lachoff, J., Jaschke, P.R., Groban, E.S., 2017. Wet lab accelerator: a web-based application democratizing laboratory automation for synthetic biology. ACS Synth. Biol. 6, 167-171. https://doi.org/10.1021/ acssynbio.6b00108.</p>
<p>The need for uncertainty quantification in machine-assisted medical decision making. E Begoli, T Bhattacharya, D Kusnezov, 10.1038/s42256-018-0004-1Nat. Mach. Intell. 1Begoli, E., Bhattacharya, T., Kusnezov, D., 2019. The need for uncertainty quantification in machine-assisted medical decision making. Nat. Mach. Intell. 1, 20-23. https:// doi.org/10.1038/s42256-018-0004-1.</p>
<p>. D A Benson, I Karsch-Mizrachi, D J Lipman, J Ostell, E W Sayers, 10.1093/nar/gkq1079GenBank. Nucleic Acids Res. 39Benson, D.A., Karsch-Mizrachi, I., Lipman, D.J., Ostell, J., Sayers, E.W., 2011. GenBank. Nucleic Acids Res. 39, D32-D37. https://doi.org/10.1093/nar/gkq1079.</p>
<p>Low-N protein engineering with data-efficient deep learning. S Biswas, G Khimulya, E C Alley, K M Esvelt, G M Church, 10.1101/2020.01.23.917682Biswas, S., Khimulya, G., Alley, E.C., Esvelt, K.M., Church, G.M., 2020. Low-N protein engineering with data-efficient deep learning. BioRxiv. https://doi.org/10.1101/ 2020.01.23.917682.</p>
<p>Screening and modular design for metabolic pathway optimization. J T Boock, A Gupta, K L Prather, Current Opinion in Biotechnology. 36Boock, J.T., Gupta, A., Prather, K.L., 2015. Screening and modular design for metabolic pathway optimization. Current Opinion in Biotechnology 36, 189-198.</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, D Amodei, Language Models Are Few-Shot Learners. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Amodei, D., 2020. Language Models Are Few-Shot Learners. arXiv. https://arxiv. org/abs/2005.14165.</p>
<p>A mobile robotic chemist. B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, N Rankin, B Harris, R S Sprick, A I Cooper, 10.1038/s41586-020-2442-2Nature. 583Burger, B., Maffettone, P.M., Gusev, V.V., Aitchison, C.M., Bai, Y., Wang, X., Li, X., Alston, B.M., Li, B., Clowes, R., Rankin, N., Harris, B., Sprick, R.S., Cooper, A.I., 2020. A mobile robotic chemist. Nature 583, 237-241. https://doi.org/10.1038/ s41586-020-2442-2.</p>
<p>Molecular signatures-based prediction of enzyme promiscuity. P Carbonell, J.-L Faulon, 10.1093/bioinformatics/btq317Bioinformatics. 26Carbonell, P., Faulon, J.-L., 2010. Molecular signatures-based prediction of enzyme promiscuity. Bioinformatics 26, 2012-2019. https://doi.org/10.1093/ bioinformatics/btq317.</p>
<p>An automated Design-Build-Test-Learn pipeline for enhanced microbial production of fine chemicals. P Carbonell, A J Jervis, C J Robinson, C Yan, M Dunstan, N Swainston, M Vinaixa, K A Hollywood, A Currin, N J W Rattray, S Taylor, R Spiess, R Sung, A R Williams, D Fellows, N J Stanford, P Mulherin, R Le Feuvre, P Barran, R Goodacre, N S Scrutton, 10.1038/s42003-018-0076-9Commun. Biol. 166Carbonell, P., Jervis, A.J., Robinson, C.J., Yan, C., Dunstan, M., Swainston, N., Vinaixa, M., Hollywood, K.A., Currin, A., Rattray, N.J.W., Taylor, S., Spiess, R., Sung, R., Williams, A.R., Fellows, D., Stanford, N.J., Mulherin, P., Le Feuvre, R., Barran, P., Goodacre, R., Scrutton, N.S., 2018. An automated Design-Build-Test- Learn pipeline for enhanced microbial production of fine chemicals. Commun. Biol. 1, 66. https://doi.org/10.1038/s42003-018-0076-9.</p>
<p>Opportunities at the intersection of synthetic biology, machine learning, and automation. P Carbonell, T Radivojevic, H García Martín, 10.1021/acssynbio.8b00540ACS Synth. Biol. 8Carbonell, P., Radivojevic, T., García Martín, H., 2019. Opportunities at the intersection of synthetic biology, machine learning, and automation. ACS Synth. Biol. 8, 1474-1477. https://doi.org/10.1021/acssynbio.8b00540.</p>
<p>Multitask Learning. R Caruana, 10.1023/a:1007379606734Springer Science and Business Media LLC. Caruana, R., 1997. Multitask Learning. Springer Science and Business Media LLC. https://doi.org/10.1023/a:1007379606734.</p>
<p>Towards kinetic modeling of genome-scale metabolic networks without sacrificing stoichiometric, thermodynamic and physiological constraints. A Chakrabarti, L Miskovic, K C Soh, V Hatzimanikatis, 10.1002/biot.201300091Biotechnol. J. 8Chakrabarti, A., Miskovic, L., Soh, K.C., Hatzimanikatis, V., 2013. Towards kinetic modeling of genome-scale metabolic networks without sacrificing stoichiometric, thermodynamic and physiological constraints. Biotechnol. J. 8, 1043-1057. https:// doi.org/10.1002/biot.201300091.</p>
<p>Locally adaptive dimensionality reduction for indexing large time series databases. K Chakrabarti, E Keogh, S Mehrotra, M Pazzani, 10.1145/568518.568520ACM Trans. Database Syst. 27Chakrabarti, K., Keogh, E., Mehrotra, S., Pazzani, M., 2002. Locally adaptive dimensionality reduction for indexing large time series databases. ACM Trans. Database Syst. 27, 188-228. https://doi.org/10.1145/568518.568520.</p>
<p>Engineering biological systems using automated biofoundries. R Chao, S Mishra, T Si, H Zhao, 10.1016/j.ymben.2017.06.003Metab. Eng. 42Chao, R., Mishra, S., Si, T., Zhao, H., 2017. Engineering biological systems using automated biofoundries. Metab. Eng. 42, 98-108. https://doi.org/10.1016/j. ymben.2017.06.003.</p>
<p>Mining bioprocess data: opportunities and challenges. S Charaniya, W.-S Hu, G Karypis, 10.1016/j.tibtech.2008.09.003Trends Biotechnol. 26Charaniya, S., Hu, W.-S., Karypis, G., 2008. Mining bioprocess data: opportunities and challenges. Trends Biotechnol. 26, 690-699. https://doi.org/10.1016/j. tibtech.2008.09.003.</p>
<p>sgRNA scorer 2.0: a speciesindependent model to predict CRISPR/cas9 activity. R Chari, N C Yeo, A Chavez, G M Church, 10.1021/acssynbio.6b00343ACS Synth. Biol. 6Chari, R., Yeo, N.C., Chavez, A., Church, G.M., 2017. sgRNA scorer 2.0: a species- independent model to predict CRISPR/cas9 activity. ACS Synth. Biol. 6, 902-904. https://doi.org/10.1021/acssynbio.6b00343.</p>
<p>The automated lab. Check Hayden, E , 10.1038/516131aNature. 516Check Hayden, E., 2014. The automated lab. Nature 516, 131-132. https://doi.org/ 10.1038/516131a.</p>
<p>Automated "cells-to-peptides" sample preparation workflow for high-throughput, quantitative proteomic assays of microbes. Y Chen, J M Guenther, J W Gin, L J G Chan, Z Costello, T L Ogorzalek, H M Tran, J M Blake-Hedges, J D Keasling, P D Adams, H García Martín, N J Hillson, C J Petzold, 10.1021/acs.jproteome.9b00455J. Proteome Res. 18Chen, Y., Guenther, J.M., Gin, J.W., Chan, L.J.G., Costello, Z., Ogorzalek, T.L., Tran, H. M., Blake-Hedges, J.M., Keasling, J.D., Adams, P.D., García Martín, H., Hillson, N.J., Petzold, C.J., 2019. Automated "cells-to-peptides" sample preparation workflow for high-throughput, quantitative proteomic assays of microbes. J. Proteome Res. 18, 3752-3761. https://doi.org/10.1021/acs.jproteome.9b00455.</p>
<p>Representation of process trends-Part I. A formal representation framework. J T Y Cheung, G Stephanopoulos, 10.1016/0098-1354(90)87023-I1016/0098-1354(90)87023-IComput. Chem. Eng. 14Cheung, J.T.Y., Stephanopoulos, G., 1990a. Representation of process trends-Part I. A formal representation framework. Comput. Chem. Eng. 14, 495-510. https://doi. org/10.1016/0098-1354(90)87023-I.</p>
<p>Representation of process trends-Part II. The problem of scale and qualitative scaling. J T Y Cheung, G Stephanopoulos, 10.1016/0098-1354(90)87024-JComput. Chem. Eng. 14Cheung, J.T.Y., Stephanopoulos, G., 1990b. Representation of process trends-Part II. The problem of scale and qualitative scaling. Comput. Chem. Eng. 14, 511-539. https://doi.org/10.1016/0098-1354(90)87024-J.</p>
<p>Explicit content detection in music lyrics using machine learning. H Chin, J Kim, Y Kim, J Shin, M Y Yi, 10.1109/BigComp.2018.000852018 IEEE International Conference on Big Data and Smart Computing (BigComp). Presented at the 2018 IEEE International Conference on Big Data and Smart Computing. BigComp). IEEEChin, H., Kim, J., Kim, Y., Shin, J., Yi, M.Y., 2018. Explicit content detection in music lyrics using machine learning. In: 2018 IEEE International Conference on Big Data and Smart Computing (BigComp). Presented at the 2018 IEEE International Conference on Big Data and Smart Computing. BigComp), IEEE, pp. 517-521. https://doi.org/10.1109/BigComp.2018.00085.</p>
<p>Systems metabolic engineering strategies: integrating systems and synthetic biology with metabolic engineering. K R Choi, W D Jang, D Yang, J S Cho, D Park, S Y Lee, 10.1016/j.tibtech.2019.01.003Trends Biotechnol. 37Choi, K.R., Jang, W.D., Yang, D., Cho, J.S., Park, D., Lee, S.Y., 2019. Systems metabolic engineering strategies: integrating systems and synthetic biology with metabolic engineering. Trends Biotechnol. 37, 817-837. https://doi.org/10.1016/j. tibtech.2019.01.003.</p>
<p>DeepCRISPR: optimized CRISPR guide RNA design by deep learning. G Chuai, H Ma, J Yan, M Chen, N Hong, D Xue, C Zhou, C Zhu, K Chen, B Duan, F Gu, S Qu, D Huang, J Wei, Q Liu, 10.1186/s13059-018-1459-4Genome Biol. 19Chuai, G., Ma, H., Yan, J., Chen, M., Hong, N., Xue, D., Zhou, C., Zhu, C., Chen, K., Duan, B., Gu, F., Qu, S., Huang, D., Wei, J., Liu, Q., 2018. DeepCRISPR: optimized CRISPR guide RNA design by deep learning. Genome Biol. 19, 80. https://doi.org/ 10.1186/s13059-018-1459-4.</p>
<p>Synthetic and systems biology for microbial production of commodity chemicals. V Chubukov, A Mukhopadhyay, C J Petzold, J D Keasling, H G Martín, 10.1038/npjsba.2016.9NPJ Syst. Biol. Appl. 2Chubukov, V., Mukhopadhyay, A., Petzold, C.J., Keasling, J.D., Martín, H.G., 2016. Synthetic and systems biology for microbial production of commodity chemicals. NPJ Syst. Biol. Appl. 2, 16009. https://doi.org/10.1038/npjsba.2016.9.</p>
<p>Matlab for Machine Learning. G Ciaburro, Packt Publishing LtdCiaburro, G., 2017. Matlab for Machine Learning. Packt Publishing Ltd.</p>
<p>Enzyme-specific profiles for genome annotation: PRIAM. C Claudel-Renard, C Chevalet, T Faraut, D Kahn, 10.1093/nar/gkg847Nucleic Acids Res. 31Claudel-Renard, C., Chevalet, C., Faraut, T., Kahn, D., 2003. Enzyme-specific profiles for genome annotation: PRIAM. Nucleic Acids Res. 31, 6633-6639. https://doi.org/ 10.1093/nar/gkg847.</p>
<p>DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns. J Clauwaert, G Menschaert, W Waegeman, 10.1093/nar/gkz061Nucleic Acids Res. 47Clauwaert, J., Menschaert, G., Waegeman, W., 2019. DeepRibo: a neural network for precise gene annotation of prokaryotes by combining ribosome profiling signal and binding site patterns. Nucleic Acids Res. 47, e36. https://doi.org/10.1093/nar/ gkz061.</p>
<p>An integrated approach to optimization of Escherichia coli fermentations using historical data. M C Coleman, K K S Buck, D E Block, 10.1002/bit.10719Biotechnol. Bioeng. 84Coleman, M.C., Buck, K.K.S., Block, D.E., 2003. An integrated approach to optimization of Escherichia coli fermentations using historical data. Biotechnol. Bioeng. 84, 274-285. https://doi.org/10.1002/bit.10719.</p>
<p>Hybrid dynamic modeling of Escherichia coli central metabolic network combining Michaelis-Menten and approximate kinetic equations. R S Costa, D Machado, I Rocha, E C Ferreira, 10.1016/j.biosystems.2010.03.001Biosystems. 100Costa, R.S., Machado, D., Rocha, I., Ferreira, E.C., 2010. Hybrid dynamic modeling of Escherichia coli central metabolic network combining Michaelis-Menten and approximate kinetic equations. Biosystems 100, 150-157. https://doi.org/10.1016/ j.biosystems.2010.03.001.</p>
<p>How to Hallucinate Functional Proteins. Z Costello, H Garcia Martin, org/abs/1903.00458Costello, Z., Garcia Martin, H., 2019. How to Hallucinate Functional Proteins. arXiv. htt ps://arxiv.org/abs/1903.00458.</p>
<p>A machine learning approach to predict metabolic pathway dynamics from time-series multiomics data. Z Costello, H G Martin, 10.1038/s41540-018-0054-3NPJ Syst. Biol. Appl. 4Costello, Z., Martin, H.G., 2018. A machine learning approach to predict metabolic pathway dynamics from time-series multiomics data. NPJ Syst. Biol. Appl. 4, 19. https://doi.org/10.1038/s41540-018-0054-3.</p>
<p>M Cranmer, A Sanchez-Gonzalez, P Battaglia, R Xu, K Cranmer, D Spergel, S Ho, Discovering Symbolic Models from Deep Learning with Inductive Biases. Cranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., Ho, S., 2020. Discovering Symbolic Models from Deep Learning with Inductive Biases. arXiv. https://arxiv.org/abs/2006.11287.</p>
<p>Scale-up of industrial microbial processes. J S Crater, J C Lievense, 10.1093/femsle/fny138FEMS Microbiol. Lett. 365Crater, J.S., Lievense, J.C., 2018. Scale-up of industrial microbial processes. FEMS Microbiol. Lett. 365 https://doi.org/10.1093/femsle/fny138.</p>
<p>A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth. C Culley, S Vijayakumar, G Zampieri, C Angione, 10.1073/pnas.2002959117Proc. Natl. Acad. Sci. U.S.A. 117Culley, C., Vijayakumar, S., Zampieri, G., Angione, C., 2020. A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth. Proc. Natl. Acad. Sci. U.S.A. 117, 18869-18879. https://doi.org/10.1073/pnas.2002959117.</p>
<p>ECPred: a tool for the prediction of the enzymatic functions of protein sequences based on the EC nomenclature. A Dalkiran, A S Rifaioglu, M J Martin, R Cetin-Atalay, V Atalay, T Dogan, 10.1186/s12859-018-2368-yBMC Bioinf. 19Dalkiran, A., Rifaioglu, A.S., Martin, M.J., Cetin-Atalay, R., Atalay, V., Dogan, T., 2018. ECPred: a tool for the prediction of the enzymatic functions of protein sequences based on the EC nomenclature. BMC Bioinf. 19, 334. https://doi.org/10.1186/ s12859-018-2368-y.</p>
<p>RetroPath2.0: a retrosynthesis workflow for metabolic engineers. B Delépine, T Duigou, P Carbonell, J.-L Faulon, 10.1016/j.ymben.2017.12.002Metab. Eng. 45Delépine, B., Duigou, T., Carbonell, P., Faulon, J.-L., 2018. RetroPath2.0: a retrosynthesis workflow for metabolic engineers. Metab. Eng. 45, 158-170. https://doi.org/ 10.1016/j.ymben.2017.12.002.</p>
<p>Industrial brewing yeast engineered for the production of primary flavor determinants in hopped beer. C M Denby, R A Li, V T Vu, Z Costello, W Lin, L J G Chan, J Williams, B Donaldson, C W Bamforth, C J Petzold, H V Scheller, H G Martin, J D Keasling, 10.1038/s41467-018-03293-xNat. Commun. 9Denby, C.M., Li, R.A., Vu, V.T., Costello, Z., Lin, W., Chan, L.J.G., Williams, J., Donaldson, B., Bamforth, C.W., Petzold, C.J., Scheller, H.V., Martin, H.G., Keasling, J.D., 2018. Industrial brewing yeast engineered for the production of primary flavor determinants in hopped beer. Nat. Commun. 9, 965. https://doi.org/ 10.1038/s41467-018-03293-x.</p>
<p>ImageNet: a large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 10.1109/CVPR.2009.52068482009 IEEE Conference on Computer Vision and Pattern Recognition. Presented at the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops. IEEEDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., 2009. ImageNet: a large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. Presented at the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops). IEEE, pp. 248-255. https://doi.org/10.1109/CVPR.2009.5206848.</p>
<p>Optimized sgRNA design to maximize activity and minimize off-target effects of CRISPR-Cas9. J G Doench, N Fusi, M Sullender, M Hegde, E W Vaimberg, K F Donovan, I Smith, Z Tothova, C Wilen, R Orchard, H W Virgin, J Listgarten, D E Root, 10.1038/nbt.3437Nat. Biotechnol. 34Doench, J.G., Fusi, N., Sullender, M., Hegde, M., Vaimberg, E.W., Donovan, K.F., Smith, I., Tothova, Z., Wilen, C., Orchard, R., Virgin, H.W., Listgarten, J., Root, D.E., 2016. Optimized sgRNA design to maximize activity and minimize off-target effects of CRISPR-Cas9. Nat. Biotechnol. 34, 184-191. https://doi.org/10.1038/nbt.3437.</p>
<p>Scientists brew cannabis using hacked beer yeast. E Dolgin, 10.1038/d41586-019-00714-9Dolgin, E., 2019. Scientists brew cannabis using hacked beer yeast. Nature. https://doi. org/10.1038/d41586-019-00714-9.</p>
<p>Genome editing. The new frontier of genome engineering with CRISPR-Cas9. J A Doudna, E Charpentier, 10.1126/science.1258096Science. 3461258096Doudna, J.A., Charpentier, E., 2014. Genome editing. The new frontier of genome engineering with CRISPR-Cas9. Science 346, 1258096. https://doi.org/10.1126/ science.1258096.</p>
<p>The impact of autonomous vehicles on cities: a review. F Duarte, C Ratti, 10.1080/10630732.2018.1493883J. Urban Technol. 25Duarte, F., Ratti, C., 2018. The impact of autonomous vehicles on cities: a review. J. Urban Technol. 25, 3-18. https://doi.org/10.1080/10630732.2018.1493883.</p>
<p>Machine learning and ligand binding predictions: a review of data, methods, and obstacles. S R Ellingson, B Davis, J Allen, 10.1016/j.bbagen.2020.129545Biochim. Biophys. Acta Gen. Subj. 1864Ellingson, S.R., Davis, B., Allen, J., 2020. Machine learning and ligand binding predictions: a review of data, methods, and obstacles. Biochim. Biophys. Acta Gen. Subj. 1864, 129545. https://doi.org/10.1016/j.bbagen.2020.129545.</p>
<p>Genome-scale engineering for systems and synthetic biology. K M Esvelt, H H Wang, 10.1038/msb.2012.66Mol. Syst. Biol. 9Esvelt, K.M., Wang, H.H., 2013. Genome-scale engineering for systems and synthetic biology. Mol. Syst. Biol. 9, 641. https://doi.org/10.1038/msb.2012.66.</p>
<p>EvoBot: an open-source, modular, liquid handling robot for scientific experiments. A Faiña, B Nejati, K Stoy, 10.3390/app10030814Appl. Sci. 10Faiña, A., Nejati, B., Stoy, K., 2020. EvoBot: an open-source, modular, liquid handling robot for scientific experiments. Appl. Sci. 10, 814. https://doi.org/10.3390/ app10030814.</p>
<p>Genome scale enzymemetabolite and drug-target interaction predictions using the signature molecular descriptor. J.-L Faulon, M Misra, S Martin, K Sale, R Sapra, 10.1093/bioinformatics/btm580Bioinformatics. 24Faulon, J.-L., Misra, M., Martin, S., Sale, K., Sapra, R., 2008. Genome scale enzyme- metabolite and drug-target interaction predictions using the signature molecular descriptor. Bioinformatics 24, 225-233. https://doi.org/10.1093/bioinformatics/ btm580.</p>
<p>HMMER web server: interactive sequence similarity searching. R D Finn, J Clements, S R Eddy, 10.1093/nar/gkr367Nucleic Acids Res. 39Finn, R.D., Clements, J., Eddy, S.R., 2011. HMMER web server: interactive sequence similarity searching. Nucleic Acids Res. 39, W29-W37. https://doi.org/10.1093/ nar/gkr367.</p>
<p>Improving catalytic function by ProSAR-driven enzyme evolution. R J Fox, S C Davis, E C Mundorff, L M Newman, V Gavrilovic, S K Ma, L M Chung, C Ching, S Tam, S Muley, J Grate, J Gruber, J C Whitman, R A Sheldon, G W Huisman, 10.1038/nbt1286Nat. Biotechnol. 25Fox, R.J., Davis, S.C., Mundorff, E.C., Newman, L.M., Gavrilovic, V., Ma, S.K., Chung, L. M., Ching, C., Tam, S., Muley, S., Grate, J., Gruber, J., Whitman, J.C., Sheldon, R.A., Huisman, G.W., 2007. Improving catalytic function by ProSAR-driven enzyme evolution. Nat. Biotechnol. 25, 338-344. https://doi.org/10.1038/nbt1286.</p>
<p>Droplet microfluidics for synthetic biology. P C Gach, K Iwai, P W Kim, N J Hillson, A K Singh, 10.1039/c7lc00576hLab Chip. 17Gach, P.C., Iwai, K., Kim, P.W., Hillson, N.J., Singh, A.K., 2017. Droplet microfluidics for synthetic biology. Lab Chip 17, 3388-3400. https://doi.org/10.1039/c7lc00576h.</p>
<p>Synthetic biology: from hype to impact. T S Gardner, 10.1016/j.tibtech.2013.01.018Trends Biotechnol. 31Gardner, T.S., 2013. Synthetic biology: from hype to impact. Trends Biotechnol. 31, 123-125. https://doi.org/10.1016/j.tibtech.2013.01.018.</p>
<p>Genome-wide mapping of mutations at single-nucleotide resolution for protein, metabolic and genome engineering. A D Garst, M C Bassalo, G Pines, S A Lynch, A L Halweg-Edwards, R Liu, L Liang, Z Wang, R Zeitoun, W G Alexander, R T Gill, 10.1038/nbt.3718Nat. Biotechnol. 35Garst, A.D., Bassalo, M.C., Pines, G., Lynch, S.A., Halweg-Edwards, A.L., Liu, R., Liang, L., Wang, Z., Zeitoun, R., Alexander, W.G., Gill, R.T., 2017. Genome-wide mapping of mutations at single-nucleotide resolution for protein, metabolic and genome engineering. Nat. Biotechnol. 35, 48-55. https://doi.org/10.1038/ nbt.3718.</p>
<p>Geltor unveils first biodesigned human collagen for skincare market. M T Gebrekidan, C Knipfer, A S Braeuer, 10.1002/jrs.5835J. Raman Spectrosc. 51Vector casting for noise reductionGebrekidan, M.T., Knipfer, C., Braeuer, A.S., 2020. Vector casting for noise reduction. J. Raman Spectrosc. 51, 731-743. https://doi.org/10.1002/jrs.5835. "Geltor unveils first biodesigned human collagen for skincare market", 2019. PRnewswire. https://www.prnewswire.com/news-releases/geltor-unveils-first-bi odesigned-human-collagen-for-skincare-market-300819885.html.</p>
<p>Metabolic engineering for the highyield production of isoprenoid-based. K W George, M G Thompson, A Kang, E Baidoo, G Wang, L J G Chan, P D Adams, C J Petzold, J D Keasling, T S Lee, 10.1038/srep11128C₅ alcohols in E. coli. Sci. Rep. 5George, K.W., Thompson, M.G., Kang, A., Baidoo, E., Wang, G., Chan, L.J.G., Adams, P. D., Petzold, C.J., Keasling, J.D., Lee, T.S., 2015. Metabolic engineering for the high- yield production of isoprenoid-based C₅ alcohols in E. coli. Sci. Rep. 5, 11128. https://doi.org/10.1038/srep11128.</p>
<p>A Géron, Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems. second ed. O'reilly MediaGéron, A., 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems, second ed. O'reilly Media.</p>
<p>Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning. S Gessulat, T Schmidt, D P Zolg, P Samaras, K Schnatbaum, J Zerweck, T Knaute, J Rechenberger, B Delanghe, A Huhmer, U Reimer, H.-C Ehrlich, S Aiche, B Kuster, M Wilhelm, 10.1038/s41592-019-0426-7Nat. Methods. 16Gessulat, S., Schmidt, T., Zolg, D.P., Samaras, P., Schnatbaum, K., Zerweck, J., Knaute, T., Rechenberger, J., Delanghe, B., Huhmer, A., Reimer, U., Ehrlich, H.-C., Aiche, S., Kuster, B., Wilhelm, M., 2019. Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning. Nat. Methods 16, 509-518. https://doi.org/ 10.1038/s41592-019-0426-7.</p>
<p>. L A Gilbert, M A Horlbeck, B Adamson, J E Villalta, Y Chen, E H Whitehead, C Guimaraes, B Panning, H L Ploegh, M C Bassik, L S Qi, M Kampmann, Gilbert, L.A., Horlbeck, M.A., Adamson, B., Villalta, J.E., Chen, Y., Whitehead, E.H., Guimaraes, C., Panning, B., Ploegh, H.L., Bassik, M.C., Qi, L.S., Kampmann, M.,</p>
<p>Bio-GraphIIn: a graph-based, integrative and semantically-enabled repository for life science experimental data. J S Weissman, A Gonzalez-Beltran, E Maguire, P Georgiou, S.-A Sansone, P Rocca-Serra, 10.14806/ej.19.B.728EMBnet j. 159CellWeissman, J.S., 2014. Genome-scale CRISPR-mediated control of gene repression and activation. Cell 159, 647-661. https://doi.org/10.1016/j.cell.2014.09.029. Gonzalez-Beltran, A., Maguire, E., Georgiou, P., Sansone, S.-A., Rocca-Serra, P., 2013. Bio-GraphIIn: a graph-based, integrative and semantically-enabled repository for life science experimental data. EMBnet j 19, 46. https://doi.org/10.14806/ej.19.B.728.</p>
<p>Tuning the performance of synthetic riboswitches using machine learning. A.-C Groher, S Jager, C Schneider, F Groher, K Hamacher, B Suess, 10.1021/acssynbio.8b00207ACS Synth. Biol. 8Groher, A.-C., Jager, S., Schneider, C., Groher, F., Hamacher, K., Suess, B., 2019. Tuning the performance of synthetic riboswitches using machine learning. ACS Synth. Biol. 8, 34-44. https://doi.org/10.1021/acssynbio.8b00207.</p>
<p>Explainable Artificial Intelligence (XAI) (No. DARPA-BAA-16-53). Defense Advanced Research Projects Agency. D Gunning, Gunning, D., 2016. Explainable Artificial Intelligence (XAI) (No. DARPA-BAA-16-53). Defense Advanced Research Projects Agency.</p>
<p>Darpa's explainable artificial intelligence (XAI) program. AI Mag. D Gunning, D Aha, 10.1609/aimag.v40i2.285040Gunning, D., Aha, D., 2019. Darpa's explainable artificial intelligence (XAI) program. AI Mag. 40, 44-58. https://doi.org/10.1609/aimag.v40i2.2850.</p>
<p>XAI-explainable artificial intelligence. D Gunning, M Stefik, J Choi, T Miller, S Stumpf, G.-Z Yang, 10.1126/scirobotics.aay7120Sci. Robot. 4Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S., Yang, G.-Z., 2019. XAI-explainable artificial intelligence. Sci. Robot. 4, eaay7120 https://doi.org/ 10.1126/scirobotics.aay7120.</p>
<p>Exascale supercomputer intiative launched. P Gwynne, 10.1088/2058-7058/32/5/12Phys. World. 32Gwynne, P., 2019. Exascale supercomputer intiative launched. Phys. World 32. https:// doi.org/10.1088/2058-7058/32/5/12, 11-11.</p>
<p>Spiber and North Face Japan create first readily-available spider silk jacket. J Hahn, Hahn, J., 2019. Spiber and North Face Japan create first readily-available spider silk jacket. https://www.dezeen.com/2019/10/24/spiber-moon-parka-spider-silkthe-n orth-face-japan/ accessed 2.19.20.</p>
<p>Towards a fully automated algorithm driven platform for biosystems design. M Hamedirad, R Chao, S Weisberg, J Lian, S Sinha, H Zhao, 10.1038/s41467-019-13189-zNat. Commun. 10HamediRad, M., Chao, R., Weisberg, S., Lian, J., Sinha, S., Zhao, H., 2019. Towards a fully automated algorithm driven platform for biosystems design. Nat. Commun. 10, 5150. https://doi.org/10.1038/s41467-019-13189-z.</p>
<p>Design, implementation and practice of JBEI-ICE: an open source biological part registry platform and tools. T S Ham, Z Dmytriv, H Plahar, J Chen, N J Hillson, J D Keasling, 10.1093/nar/gks531Nucleic Acids Res. 40Ham, T.S., Dmytriv, Z., Plahar, H., Chen, J., Hillson, N.J., Keasling, J.D., 2012. Design, implementation and practice of JBEI-ICE: an open source biological part registry platform and tools. Nucleic Acids Res. 40, e141. https://doi.org/10.1093/nar/ gks531.</p>
<p>Amyris ships first commerical order of Biofene from Brazil plant. Chris Hanson, Hanson, Chris, 2013. "Amyris ships first commerical order of Biofene from Brazil plant". Biomass Magazine. http://biomassmagazine.com/articles/8610/amyris-ships-fir st-commerical-order-of-biofene-from-brazil-plant.</p>
<p>Euler-Lagrange computational fluid dynamics for (bio)reactor scale down: an analysis of organism lifelines. Eng. C Haringa, W Tang, A T Deshmukh, J Xia, M Reuss, J J Heijnen, R F Mudde, H J Noorman, 10.1002/elsc.201600061Life Sci. 16Haringa, C., Tang, W., Deshmukh, A.T., Xia, J., Reuss, M., Heijnen, J.J., Mudde, R.F., Noorman, H.J., 2016. Euler-Lagrange computational fluid dynamics for (bio)reactor scale down: an analysis of organism lifelines. Eng. Life Sci. 16, 652-663. https://doi. org/10.1002/elsc.201600061.</p>
<p>Computational fluid dynamics simulation of an industrial P. chrysogenum fermentation with a coupled 9-pool metabolic model: towards rational scale-down and design optimization. C Haringa, W Tang, G Wang, A T Deshmukh, W A Van Winden, J Chu, W M Van Gulik, J J Heijnen, R F Mudde, H J Noorman, 10.1016/j.ces.2017.09.020Chem. Eng. Sci. 175Haringa, C., Tang, W., Wang, G., Deshmukh, A.T., van Winden, W.A., Chu, J., van Gulik, W.M., Heijnen, J.J., Mudde, R.F., Noorman, H.J., 2017. Computational fluid dynamics simulation of an industrial P. chrysogenum fermentation with a coupled 9- pool metabolic model: towards rational scale-down and design optimization. Chem. Eng. Sci. 175, 12-24. https://doi.org/10.1016/j.ces.2017.09.020.</p>
<p>Chimera: enabling hierarchy based multiobjective optimization for self-driving laboratories. F Häse, L M Roch, A Aspuru-Guzik, 10.1039/c8sc02239aChem. Sci. 9Häse, F., Roch, L.M., Aspuru-Guzik, A., 2018. Chimera: enabling hierarchy based multi- objective optimization for self-driving laboratories. Chem. Sci. 9, 7642-7655. https://doi.org/10.1039/c8sc02239a.</p>
<p>Next-generation experimentation with selfdriving laboratories. F Häse, L M Roch, A Aspuru-Guzik, 10.1016/j.trechm.2019.02.007Trends in Chemistry. 1Häse, F., Roch, L.M., Aspuru-Guzik, A., 2019. Next-generation experimentation with self- driving laboratories. Trends in Chemistry 1, 282-291. https://doi.org/10.1016/j. trechm.2019.02.007.</p>
<p>Ecosystem engineering in space and time. A Hastings, J E Byers, J A Crooks, K Cuddington, C G Jones, J G Lambrinos, T S Talley, W G Wilson, 10.1111/j.1461-0248.2006.00997.xEcol. Lett. 10Hastings, A., Byers, J.E., Crooks, J.A., Cuddington, K., Jones, C.G., Lambrinos, J.G., Talley, T.S., Wilson, W.G., 2007. Ecosystem engineering in space and time. Ecol. Lett. 10, 153-164. https://doi.org/10.1111/j.1461-0248.2006.00997.x.</p>
<p>MetaboLights-an openaccess general-purpose repository for metabolomics studies and associated metadata. K Haug, R M Salek, P Conesa, J Hastings, P De Matos, M Rijnbeek, T Mahendraker, M Williams, S Neumann, P Rocca-Serra, E Maguire, A González-Beltrán, S.-A Sansone, J L Griffin, C Steinbeck, 10.1093/nar/gks1004Nucleic Acids Res. 41Haug, K., Salek, R.M., Conesa, P., Hastings, J., de Matos, P., Rijnbeek, M., Mahendraker, T., Williams, M., Neumann, S., Rocca-Serra, P., Maguire, E., González- Beltrán, A., Sansone, S.-A., Griffin, J.L., Steinbeck, C., 2013. MetaboLights-an open- access general-purpose repository for metabolomics studies and associated meta- data. Nucleic Acids Res. 41, D781-D786. https://doi.org/10.1093/nar/gks1004.</p>
<p>Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models. D Heckmann, C J Lloyd, N Mih, Y Ha, D C Zielinski, Z B Haiman, A A Desouki, M J Lercher, B O Palsson, 10.1038/s41467-018-07652-6Nat. Commun. 9Heckmann, D., Lloyd, C.J., Mih, N., Ha, Y., Zielinski, D.C., Haiman, Z.B., Desouki, A.A., Lercher, M.J., Palsson, B.O., 2018. Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models. Nat. Commun. 9, 5252. https://doi.org/10.1038/s41467-018-07652-6.</p>
<p>E-CRISP: fast CRISPR target site identification. F Heigwer, G Kerr, M Boutros, 10.1038/nmeth.2812Nat. Methods. 11Heigwer, F., Kerr, G., Boutros, M., 2014. E-CRISP: fast CRISPR target site identification. Nat. Methods 11, 122-123. https://doi.org/10.1038/nmeth.2812.</p>
<p>Approximative kinetic formats used in metabolic network modeling. J J Heijnen, 10.1002/bit.20558Biotechnol. Bioeng. 91Heijnen, J.J., 2005. Approximative kinetic formats used in metabolic network modeling. Biotechnol. Bioeng. 91, 534-545. https://doi.org/10.1002/bit.20558.</p>
<p>The Regulation of Cellular Systems. R Heinrich, S Schuster, 10.1007/978-1-4613-1161-4Springer USBoston, MAHeinrich, R., Schuster, S., 1996. The Regulation of Cellular Systems. Springer US, Boston, MA. https://doi.org/10.1007/978-1-4613-1161-4.</p>
<p>High-accuracy protein structures by combining machine-learning with physics-based refinement. L Heo, M Feig, 10.1002/prot.25847Proteins. 88Heo, L., Feig, M., 2020. High-accuracy protein structures by combining machine-learning with physics-based refinement. Proteins 88, 637-642. https://doi.org/10.1002/ prot.25847.</p>
<p>Building a global alliance of biofoundries. N Hillson, M Caddick, Y Cai, J A Carrasco, M W Chang, N C Curach, D J Bell, R Le Feuvre, D C Friedman, X Fu, N D Gold, M J Herrgård, M B Holowko, J R Johnson, R A Johnson, J D Keasling, R I Kitney, A Kondo, C Liu, V J J Martin, P S Freemont, 10.1038/s41467-019-10079-2Nat. Commun. 10Hillson, N., Caddick, M., Cai, Y., Carrasco, J.A., Chang, M.W., Curach, N.C., Bell, D.J., Le Feuvre, R., Friedman, D.C., Fu, X., Gold, N.D., Herrgård, M.J., Holowko, M.B., Johnson, J.R., Johnson, R.A., Keasling, J.D., Kitney, R.I., Kondo, A., Liu, C., Martin, V.J.J., Freemont, P.S., 2019. Building a global alliance of biofoundries. Nat. Commun. 10, 2040. https://doi.org/10.1038/s41467-019-10079-2.</p>
<p>Cell-free synthetic biology: thinking outside the cell. C E Hodgman, M C Jewett, 10.1016/j.ymben.2011.09.002Metab. Eng. 14Hodgman, C.E., Jewett, M.C., 2012. Cell-free synthetic biology: thinking outside the cell. Metab. Eng. 14, 261-269. https://doi.org/10.1016/j.ymben.2011.09.002.</p>
<p>Random decision forests. T K Ho, Proceedings of 3rd international conference on document analysis and recognition 1. 3rd international conference on document analysis and recognition 1278Ho, T.K., 1995. Random decision forests. Proceedings of 3rd international conference on document analysis and recognition 1, 278.</p>
<p>DNA targeting specificity of RNA-guided Cas9 nucleases. P D Hsu, D A Scott, J A Weinstein, F A Ran, S Konermann, V Agarwala, Y Li, E J Fine, X Wu, O Shalem, T J Cradick, L A Marraffini, G Bao, F Zhang, 10.1038/nbt.2647Nat. Biotechnol. 31Hsu, P.D., Scott, D.A., Weinstein, J.A., Ran, F.A., Konermann, S., Agarwala, V., Li, Y., Fine, E.J., Wu, X., Shalem, O., Cradick, T.J., Marraffini, L.A., Bao, G., Zhang, F., 2013. DNA targeting specificity of RNA-guided Cas9 nucleases. Nat. Biotechnol. 31, 827-832. https://doi.org/10.1038/nbt.2647.</p>
<p>Shake flask to fermentor: what have we learned?. A Humphrey, 10.1021/bp970130kBiotechnol. Prog. 14Humphrey, A., 1998. Shake flask to fermentor: what have we learned? Biotechnol. Prog. 14, 3-7. https://doi.org/10.1021/bp970130k.</p>
<p>Morphology and mechanics of fungal mycelium. M R Islam, G Tudryn, R Bucinell, L Schadler, R C Picu, 10.1038/s41598-017-13295-2Sci. Rep. 7Islam, M.R., Tudryn, G., Bucinell, R., Schadler, L., Picu, R.C., 2017. Morphology and mechanics of fungal mycelium. Sci. Rep. 7, 13070. https://doi.org/10.1038/s41598- 017-13295-2.</p>
<p>Chromatin accessibility and guide sequence secondary structure affect CRISPR-Cas9 gene editing efficiency. K T Jensen, L Fløe, T S Petersen, J Huang, F Xu, L Bolund, Y Luo, L Lin, 10.1002/1873-3468.12707FEBS Lett. 591Jensen, K.T., Fløe, L., Petersen, T.S., Huang, J., Xu, F., Bolund, L., Luo, Y., Lin, L., 2017. Chromatin accessibility and guide sequence secondary structure affect CRISPR-Cas9 gene editing efficiency. FEBS Lett. 591, 1892-1901. https://doi.org/10.1002/1873- 3468.12707.</p>
<p>SelProm: a queryable and predictive expression vector selection tool for Escherichia coli. A J Jervis, P Carbonell, S Taylor, R Sung, M S Dunstan, C J Robinson, R Breitling, E Takano, N S Scrutton, 10.1021/acssynbio.8b00399ACS Synth. Biol. 8Jervis, A.J., Carbonell, P., Taylor, S., Sung, R., Dunstan, M.S., Robinson, C.J., Breitling, R., Takano, E., Scrutton, N.S., 2019a. SelProm: a queryable and predictive expression vector selection tool for Escherichia coli. ACS Synth. Biol. 8, 1478-1483. https://doi.org/10.1021/acssynbio.8b00399.</p>
<p>Machine learning of designed translational control allows predictive pathway optimization in Escherichia coli. A J Jervis, P Carbonell, M Vinaixa, M S Dunstan, K A Hollywood, C J Robinson, N J W Rattray, C Yan, N Swainston, A Currin, R Sung, H Toogood, S Taylor, J.-L Faulon, R Breitling, E Takano, N S Scrutton, 10.1021/acssynbio.8b00398ACS Synth. Biol. 8Jervis, A.J., Carbonell, P., Vinaixa, M., Dunstan, M.S., Hollywood, K.A., Robinson, C.J., Rattray, N.J.W., Yan, C., Swainston, N., Currin, A., Sung, R., Toogood, H., Taylor, S., Faulon, J.-L., Breitling, R., Takano, E., Scrutton, N.S., 2019b. Machine learning of designed translational control allows predictive pathway optimization in Escherichia coli. ACS Synth. Biol. 8, 127-136. https://doi.org/10.1021/acssynbio.8b00398.</p>
<p>Improving reproducibility in synthetic biology. M M Jessop-Fabre, N Sonnenschein, 10.3389/fbioe.2019.00018Front. Bioeng. Biotechnol. 7Jessop-Fabre, M.M., Sonnenschein, N., 2019. Improving reproducibility in synthetic biology. Front. Bioeng. Biotechnol. 7, 18. https://doi.org/10.3389/ fbioe.2019.00018.</p>
<p>Auto-keras: an efficient neural architecture search system. H Jin, Q Song, X Hu, 10.1145/3292500.3330648Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. Presented at the KDD '19: the 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. Presented at the KDD '19: the 25th ACM SIGKDD Conference on Knowledge Discovery and Data MiningNew York, NY, USAACMJin, H., Song, Q., Hu, X., 2019. Auto-keras: an efficient neural architecture search system. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. Presented at the KDD '19: the 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, USA, pp. 1946-1956. https://doi.org/10.1145/3292500.3330648.</p>
<p>Method of producing polymers of spider silk proteins 2010. US Patent 8642734B2, filed 2010. J Johansson, M Hedhammar, A Rising, K Nordling, Johansson, J., Hedhammar, M., Rising, A., Nordling, K., 2014. Method of producing polymers of spider silk proteins 2010. US Patent 8642734B2, filed 2010. and issued. https://patents.google.com/patent/US20120041177.</p>
<p>Optimization of the IPP-bypass mevalonate pathway and fed-batch fermentation for the production of isoprenol in Escherichia coli. A Kang, D Mendez-Perez, E.-B Goh, E E K Baidoo, V T Benites, H R Beller, J D Keasling, P D Adams, A Mukhopadhyay, T S Lee, 10.1016/j.ymben.2019.09.003Metab. Eng. 56Kang, A., Mendez-Perez, D., Goh, E.-B., Baidoo, E.E.K., Benites, V.T., Beller, H.R., Keasling, J.D., Adams, P.D., Mukhopadhyay, A., Lee, T.S., 2019. Optimization of the IPP-bypass mevalonate pathway and fed-batch fermentation for the production of isoprenol in Escherichia coli. Metab. Eng. 56, 85-96. https://doi.org/10.1016/j. ymben.2019.09.003.</p>
<p>In vitro prototyping and rapid optimization of biosynthetic enzymes for cell design. A S Karim, Q M Dudley, A Juminaga, Y Yuan, S A Crowe, J T Heggestad, S Garg, T Abdalla, W S Grubbe, B J Rasor, D N Coar, M Torculas, M Krein, F E Liew, A Quattlebaum, R O Jensen, J A Stuart, S D Simpson, M Köpke, M C Jewett, 10.1038/s41589-020-0559-0Nat. Chem. Biol. 16Karim, A.S., Dudley, Q.M., Juminaga, A., Yuan, Y., Crowe, S.A., Heggestad, J.T., Garg, S., Abdalla, T., Grubbe, W.S., Rasor, B.J., Coar, D.N., Torculas, M., Krein, M., Liew, F.E., Quattlebaum, A., Jensen, R.O., Stuart, J.A., Simpson, S.D., Köpke, M., Jewett, M.C., 2020. In vitro prototyping and rapid optimization of biosynthetic enzymes for cell design. Nat. Chem. Biol. 16, 912-919. https://doi.org/10.1038/s41589-020-0559-0.</p>
<p>The BioCyc collection of microbial genomes and metabolic pathways. P D Karp, R Billington, R Caspi, C A Fulcher, M Latendresse, A Kothari, I M Keseler, M Krummenacker, P E Midford, Q Ong, W K Ong, S M Paley, P Subhraveti, 10.1093/bib/bbx085Briefings Bioinf. 20Karp, P.D., Billington, R., Caspi, R., Fulcher, C.A., Latendresse, M., Kothari, A., Keseler, I. M., Krummenacker, M., Midford, P.E., Ong, Q., Ong, W.K., Paley, S.M., Subhraveti, P., 2019. The BioCyc collection of microbial genomes and metabolic pathways. Briefings Bioinf. 20, 1085-1093. https://doi.org/10.1093/bib/bbx085.</p>
<p>A whole-cell computational model predicts phenotype from genotype. J R Karr, J C Sanghvi, D N Macklin, M V Gutschow, J M Jacobs, B Bolival, N Assad-Garcia, J I Glass, M W Covert, 10.1016/j.cell.2012.05.044Cell. 150Karr, J.R., Sanghvi, J.C., Macklin, D.N., Gutschow, M.V., Jacobs, J.M., Bolival, B., Assad- Garcia, N., Glass, J.I., Covert, M.W., 2012. A whole-cell computational model predicts phenotype from genotype. Cell 150, 389-401. https://doi.org/10.1016/j. cell.2012.05.044.</p>
<p>Massively parallel screening of synthetic microbial communities. J Kehe, A Kulesa, A Ortiz, C M Ackerman, S G Thakku, D Sellers, S Kuehn, J Gore, J Friedman, P C Blainey, 10.1073/pnas.1900102116Proc. Natl. Acad. Sci. U.S.A. 116Kehe, J., Kulesa, A., Ortiz, A., Ackerman, C.M., Thakku, S.G., Sellers, D., Kuehn, S., Gore, J., Friedman, J., Blainey, P.C., 2019. Massively parallel screening of synthetic microbial communities. Proc. Natl. Acad. Sci. U.S.A. 116, 12804-12809. https://doi. org/10.1073/pnas.1900102116.</p>
<p>Gene prediction with Glimmer for metagenomic sequences augmented by classification and clustering. D R Kelley, B Liu, A L Delcher, M Pop, S L Salzberg, 10.1093/nar/gkr1067Nucleic Acids Res. 40Kelley, D.R., Liu, B., Delcher, A.L., Pop, M., Salzberg, S.L., 2012. Gene prediction with Glimmer for metagenomic sequences augmented by classification and clustering. Nucleic Acids Res. 40, e9. https://doi.org/10.1093/nar/gkr1067.</p>
<p>Exact indexing of dynamic time warping. E Keogh, C A Ratanamahatana, 10.1007/s10115-004-0154-9Knowl. Inf. Syst. 7Keogh, E., Ratanamahatana, C.A., 2005. Exact indexing of dynamic time warping. Knowl. Inf. Syst. 7, 358-386. https://doi.org/10.1007/s10115-004-0154-9.</p>
<p>You Can Now Smell a Flower That Went Extinct a Century Ago. Jill Kiedaisch, Kiedaisch, Jill, 2019. "You Can Now Smell a Flower That Went Extinct a Century Ago". Popular Mechanics. https://www.popularmechanics.com/science/environment/a2 7155735/smell-flower-extinct/.</p>
<p>Machine learning applications in systems metabolic engineering. G B Kim, W J Kim, H U Kim, S Y Lee, 10.1016/j.copbio.2019.08.010Curr. Opin. Biotechnol. 64Kim, G.B., Kim, W.J., Kim, H.U., Lee, S.Y., 2019. Machine learning applications in systems metabolic engineering. Curr. Opin. Biotechnol. 64, 1-9. https://doi.org/ 10.1016/j.copbio.2019.08.010.</p>
<p>Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity. H K Kim, S Min, M Song, S Jung, J W Choi, Y Kim, S Lee, S Yoon, H H Kim, 10.1038/nbt.4061Nat. Biotechnol. 36Kim, H.K., Min, S., Song, M., Jung, S., Choi, J.W., Kim, Y., Lee, S., Yoon, S., Kim, H.H., 2018. Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity. Nat. Biotechnol. 36, 239-241. https://doi.org/10.1038/nbt.4061.</p>
<p>A review of dynamic modeling approaches and their application in computational strain optimization for metabolic engineering. O D Kim, M Rocha, P Maia, 10.3389/fmicb.2018.01690Front. Microbiol. 9Kim, O.D., Rocha, M., Maia, P., 2018. A review of dynamic modeling approaches and their application in computational strain optimization for metabolic engineering. Front. Microbiol. 9, 1690. https://doi.org/10.3389/fmicb.2018.01690.</p>
<p>PubChem substance and compound databases. S Kim, P A Thiessen, E E Bolton, J Chen, G Fu, A Gindulyte, L Han, J He, S He, B A Shoemaker, J Wang, B Yu, J Zhang, S H Bryant, 10.1093/nar/gkv951Nucleic Acids Res. 44Kim, S., Thiessen, P.A., Bolton, E.E., Chen, J., Fu, G., Gindulyte, A., Han, L., He, J., He, S., Shoemaker, B.A., Wang, J., Yu, B., Zhang, J., Bryant, S.H., 2016. PubChem substance and compound databases. Nucleic Acids Res. 44, D1202-D1213. https://doi.org/ 10.1093/nar/gkv951.</p>
<p>BiGG Models: a platform for integrating, standardizing and sharing genome-scale models. Z A King, J Lu, A Dräger, P Miller, S Federowicz, J A Lerman, A Ebrahim, B O Palsson, N E Lewis, 10.1093/nar/gkv1049Nucleic Acids Res. 44King, Z.A., Lu, J., Dräger, A., Miller, P., Federowicz, S., Lerman, J.A., Ebrahim, A., Palsson, B.O., Lewis, N.E., 2016. BiGG Models: a platform for integrating, standardizing and sharing genome-scale models. Nucleic Acids Res. 44, D515-D522. https://doi.org/10.1093/nar/gkv1049.</p>
<p>Jupyter Notebooks-a publishing format for reproducible computational workflows. Positioning and Power in Academic Publishing: Players, Agents and Agendas. Thomas Kluyver, IOS Press EbooksKluyver, Thomas, 2016. Jupyter Notebooks-a publishing format for reproducible computational workflows. Positioning and Power in Academic Publishing: Players, Agents and Agendas. IOS Press Ebooks. http://ebooks.iospress.nl/publication /42900.</p>
<p>CRISPR-Cas guides the future of genetic engineering. G J Knott, J A Doudna, 10.1126/science.aat5011Science. 361Knott, G.J., Doudna, J.A., 2018. CRISPR-Cas guides the future of genetic engineering. Science 361, 866-869. https://doi.org/10.1126/science.aat5011.</p>
<p>Reinforcement learning for bioretrosynthesis. M Koch, T Duigou, J.-L Faulon, 10.1021/acssynbio.9b00447ACS Synth. Biol. 9Koch, M., Duigou, T., Faulon, J.-L., 2020. Reinforcement learning for bioretrosynthesis. ACS Synth. Biol. 9, 157-168. https://doi.org/10.1021/acssynbio.9b00447.</p>
<p>Role of digital microfluidics in enabling access to laboratory automation and making biology programmable. V B Kothamachu, S Zaini, F Muffatto, 10.1177/2472630320931794SLAS Technol. Kothamachu, V.B., Zaini, S., Muffatto, F., 2020. Role of digital microfluidics in enabling access to laboratory automation and making biology programmable. SLAS Technol, 2472630320931794. https://doi.org/10.1177/2472630320931794.</p>
<p>Model-driven generation of artificial yeast promoters. B J Kotopka, C D Smolke, Nature Communications. 111Kotopka, B.J., Smolke, C.D., 2020. Model-driven generation of artificial yeast promoters. Nature Communications 11 (1), 1-13.</p>
<p>Bioprocess systems engineering: transferring traditional process engineering principles to industrial biotechnology. M Koutinas, A Kiparissides, E N Pistikopoulos, A Mantalaris, 10.5936/csbj.201210022Comput. Struct. Biotechnol. J. 3Koutinas, M., Kiparissides, A., Pistikopoulos, E.N., Mantalaris, A., 2012. Bioprocess systems engineering: transferring traditional process engineering principles to industrial biotechnology. Comput. Struct. Biotechnol. J. 3, e201210022 https://doi. org/10.5936/csbj.201210022.</p>
<p>Growth kinetics of suspended microbial cells: from single-substrate-controlled growth to mixed-substrate kinetics. Microbiol. K Kovárová-Kovar, T Egli, Mol. Biol. Rev. 62Kovárová-Kovar, K., Egli, T., 1998. Growth kinetics of suspended microbial cells: from single-substrate-controlled growth to mixed-substrate kinetics. Microbiol. Mol. Biol. Rev. 62, 646-666.</p>
<p>Natural language processing systems for capturing and standardizing unstructured clinical information: a systematic review. K Kreimeyer, M Foster, A Pandey, N Arya, G Halford, S F Jones, R Forshee, M Walderhaug, T Botsis, 10.1016/j.jbi.2017.07.012J. Biomed. Inf. 73Kreimeyer, K., Foster, M., Pandey, A., Arya, N., Halford, G., Jones, S.F., Forshee, R., Walderhaug, M., Botsis, T., 2017. Natural language processing systems for capturing and standardizing unstructured clinical information: a systematic review. J. Biomed. Inf. 73, 14-29. https://doi.org/10.1016/j.jbi.2017.07.012.</p>
<p>Pathway design using de novo steps through uncharted biochemical spaces. A Kumar, L Wang, C Y Ng, C D Maranas, 10.1038/s41467-017-02362-xNat. Commun. 9Kumar, A., Wang, L., Ng, C.Y., Maranas, C.D., 2018. Pathway design using de novo steps through uncharted biochemical spaces. Nat. Commun. 9, 184. https://doi.org/ 10.1038/s41467-017-02362-x.</p>
<p>EFICAz2.5: application of a high-precision enzyme function predictor to 396 proteomes. N Kumar, J Skolnick, 10.1093/bioinformatics/bts510Bioinformatics. 28Kumar, N., Skolnick, J., 2012. EFICAz2.5: application of a high-precision enzyme function predictor to 396 proteomes. Bioinformatics 28, 2687-2688. https://doi. org/10.1093/bioinformatics/bts510.</p>
<p>Common principles and best practices for engineering microbiomes. C E Lawson, W R Harcombe, R Hatzenpichler, S R Lindemann, F E Löffler, M A O&apos;malley, H García Martín, B F Pfleger, L Raskin, O S Venturelli, D G Weissbrodt, D R Noguera, K D Mcmahon, 10.1038/s41579-019-0255-9Nat. Rev. Microbiol. 17Lawson, C.E., Harcombe, W.R., Hatzenpichler, R., Lindemann, S.R., Löffler, F.E., O'Malley, M.A., García Martín, H., Pfleger, B.F., Raskin, L., Venturelli, O.S., Weissbrodt, D.G., Noguera, D.R., McMahon, K.D., 2019. Common principles and best practices for engineering microbiomes. Nat. Rev. Microbiol. 17, 725-741. https:// doi.org/10.1038/s41579-019-0255-9.</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, 10.1038/nature14539Nature. 521LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521, 436-444. https:// doi.org/10.1038/nature14539.</p>
<p>A comprehensive metabolic map for production of biobased chemicals. S Y Lee, H U Kim, T U Chae, J S Cho, J W Kim, J H Shin, D I Kim, Y.-S Ko, W D Jang, Y.-S Jang, 10.1038/s41929-018-0212-4Nat. Catal. 2Lee, S.Y., Kim, H.U., Chae, T.U., Cho, J.S., Kim, J.W., Shin, J.H., Kim, D.I., Ko, Y.-S., Jang, W.D., Jang, Y.-S., 2019. A comprehensive metabolic map for production of bio- based chemicals. Nat. Catal. 2, 18-33. https://doi.org/10.1038/s41929-018-0212-4.</p>
<p>In silico method for modelling metabolism and gene product expression at genome scale. J A Lerman, D R Hyduke, H Latif, V A Portnoy, N E Lewis, J D Orth, A C Schrimpe-Rutledge, R D Smith, J N Adkins, K Zengler, B O Palsson, 10.1038/ncomms1928Nat. Commun. 3Lerman, J.A., Hyduke, D.R., Latif, H., Portnoy, V.A., Lewis, N.E., Orth, J.D., Schrimpe- Rutledge, A.C., Smith, R.D., Adkins, J.N., Zengler, K., Palsson, B.O., 2012. In silico method for modelling metabolism and gene product expression at genome scale. Nat. Commun. 3, 929. https://doi.org/10.1038/ncomms1928.</p>
<p>Predictive and interpretive simulation of green fluorescent protein expression in reporter bacteria. J H Leveau, S E Lindow, 10.1128/JB.183.23.6752-6762.2001J. Bacteriol. 183Leveau, J.H., Lindow, S.E., 2001. Predictive and interpretive simulation of green fluorescent protein expression in reporter bacteria. J. Bacteriol. 183, 6752-6762. https://doi.org/10.1128/JB.183.23.6752-6762.2001.</p>
<p>A novel mammalian cell line development platform utilizing nanofluidics and optoelectro positioning technology. K Le, C Tan, S Gupta, T Guhan, H Barkhordarian, J Lull, J Stevens, T Munro, 10.1002/btpr.2690Biotechnol. Prog. 34Le, K., Tan, C., Gupta, S., Guhan, T., Barkhordarian, H., Lull, J., Stevens, J., Munro, T., 2018. A novel mammalian cell line development platform utilizing nanofluidics and optoelectro positioning technology. Biotechnol. Prog. 34, 1438-1446. https://doi. org/10.1002/btpr.2690.</p>
<p>Retrosynthetic design of metabolic pathways to chemicals not found in nature. G.-M Lin, R Warden-Rothman, C A Voigt, 10.1016/j.coisb.2019.04.004Curr. Opin. Struct. Biol. Lin, G.-M., Warden-Rothman, R., Voigt, C.A., 2019. Retrosynthetic design of metabolic pathways to chemicals not found in nature. Curr. Opin. Struct. Biol. https://doi.org/ 10.1016/j.coisb.2019.04.004.</p>
<p>Off-target predictions in CRISPR-Cas9 gene editing using deep learning. J Lin, K.-C Wong, 10.1093/bioinformatics/bty554Bioinformatics. 34Lin, J., Wong, K.-C., 2018. Off-target predictions in CRISPR-Cas9 gene editing using deep learning. Bioinformatics 34, i656-i663. https://doi.org/10.1093/bioinformatics/ bty554.</p>
<p>Prediction of off-target activities for the end-to-end design of CRISPR guide RNAs. J Listgarten, M Weinstein, B P Kleinstiver, A A Sousa, J K Joung, J Crawford, K Gao, L Hoang, M Elibol, J G Doench, N Fusi, 10.1038/s41551-017-0178-6Nat. Biomed. Eng. 2Listgarten, J., Weinstein, M., Kleinstiver, B.P., Sousa, A.A., Joung, J.K., Crawford, J., Gao, K., Hoang, L., Elibol, M., Doench, J.G., Fusi, N., 2018. Prediction of off-target activities for the end-to-end design of CRISPR guide RNAs. Nat. Biomed. Eng. 2, 38-47. https://doi.org/10.1038/s41551-017-0178-6.</p>
<p>Genome scale engineering techniques for metabolic engineering. R Liu, M C Bassalo, R I Zeitoun, R T Gill, 10.1016/j.ymben.2015.09.013Metab. Eng. 32Liu, R., Bassalo, M.C., Zeitoun, R.I., Gill, R.T., 2015. Genome scale engineering techniques for metabolic engineering. Metab. Eng. 32, 143-154. https://doi.org/ 10.1016/j.ymben.2015.09.013.</p>
<p>The New York Times. Steve Lohr, Lohr, Steve, 2014. The New York Times. https://www.nytimes.com/2014/08/18/techn ology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html.</p>
<p>Predictive biology: modelling, understanding and harnessing microbial complexity. A J Lopatkin, J J Collins, 10.1038/s41579-020-0372-5Nat. Rev. Microbiol. Lopatkin, A.J., Collins, J.J., 2020. Predictive biology: modelling, understanding and harnessing microbial complexity. Nat. Rev. Microbiol. https://doi.org/10.1038/ s41579-020-0372-5.</p>
<p>Complete biosynthesis of cannabinoids and their unnatural analogues in yeast. X Luo, M A Reiter, L Espaux, J Wong, C M Denby, A Lechner, Y Zhang, A T Grzybowski, S Harth, W Lin, H Lee, C Yu, J Shin, K Deng, V T Benites, G Wang, E E K Baidoo, Y Chen, I Dev, C J Petzold, J D Keasling, 10.1038/s41586-019-0978-9Nature. 567Luo, X., Reiter, M.A., d'Espaux, L., Wong, J., Denby, C.M., Lechner, A., Zhang, Y., Grzybowski, A.T., Harth, S., Lin, W., Lee, H., Yu, C., Shin, J., Deng, K., Benites, V.T., Wang, G., Baidoo, E.E.K., Chen, Y., Dev, I., Petzold, C.J., Keasling, J.D., 2019. Complete biosynthesis of cannabinoids and their unnatural analogues in yeast. Nature 567, 123-126. https://doi.org/10.1038/s41586-019-0978-9.</p>
<p>The analytical process to search for metabolomics biomarkers. M D Luque De Castro, F Priego-Capote, 10.1016/j.jpba.2017.06.073J. Pharmaceut. Biomed. Anal. 147Luque de Castro, M.D., Priego-Capote, F., 2018. The analytical process to search for metabolomics biomarkers. J. Pharmaceut. Biomed. Anal. 147, 341-349. https://doi. org/10.1016/j.jpba.2017.06.073.</p>
<p>Using deep learning to model the hierarchical structure and function of a cell. J Ma, M K Yu, S Fong, K Ono, E Sage, B Demchak, R Sharan, T Ideker, 10.1038/nmeth.4627Nat. Methods. 15Ma, J., Yu, M.K., Fong, S., Ono, K., Sage, E., Demchak, B., Sharan, R., Ideker, T., 2018. Using deep learning to model the hierarchical structure and function of a cell. Nat. Methods 15, 290-298. https://doi.org/10.1038/nmeth.4627.</p>
<p>Systematic evaluation of methods for integration of transcriptomic data into constraint-based models of metabolism. D Machado, M Herrgård, 10.1371/journal.pcbi.1003580PLoS Comput. Biol. 10Machado, D., Herrgård, M., 2014. Systematic evaluation of methods for integration of transcriptomic data into constraint-based models of metabolism. PLoS Comput. Biol. 10, e1003580 https://doi.org/10.1371/journal.pcbi.1003580.</p>
<p>Simultaneous cross-evaluation of heterogeneous E. coli datasets via mechanistic simulation. D N Macklin, T A Ahn-Horst, H Choi, N A Ruggero, J Carrera, J C Mason, G Sun, E Agmon, M M Defelice, I Maayan, K Lane, R K Spangler, T E Gillies, M L Paull, S Akhter, S R Bray, D S Weaver, I M Keseler, P D Karp, J H Morrison, M W Covert, 10.1126/science.aav3751Science. 369Macklin, D.N., Ahn-Horst, T.A., Choi, H., Ruggero, N.A., Carrera, J., Mason, J.C., Sun, G., Agmon, E., DeFelice, M.M., Maayan, I., Lane, K., Spangler, R.K., Gillies, T.E., Paull, M.L., Akhter, S., Bray, S.R., Weaver, D.S., Keseler, I.M., Karp, P.D., Morrison, J.H., Covert, M.W., 2020. Simultaneous cross-evaluation of heterogeneous E. coli datasets via mechanistic simulation. Science 369. https://doi.org/10.1126/ science.aav3751.</p>
<p>Self-driving laboratory for accelerated discovery of thinfilm materials. B P Macleod, F G L Parlane, T D Morrissey, F Häse, L M Roch, K E Dettelbach, R Moreira, L P E Yunker, M B Rooney, J R Deeth, V Lai, G J Ng, H Situ, R H Zhang, M S Elliott, T H Haley, D J Dvorak, A Aspuru-Guzik, J E Hein, C P Berlinguette, 10.1126/sciadv.aaz8867Sci. Adv. 6MacLeod, B.P., Parlane, F.G.L., Morrissey, T.D., Häse, F., Roch, L.M., Dettelbach, K.E., Moreira, R., Yunker, L.P.E., Rooney, M.B., Deeth, J.R., Lai, V., Ng, G.J., Situ, H., Zhang, R.H., Elliott, M.S., Haley, T.H., Dvorak, D.J., Aspuru-Guzik, A., Hein, J.E., Berlinguette, C.P., 2020. Self-driving laboratory for accelerated discovery of thin- film materials. Sci. Adv. 6, eaaz8867 https://doi.org/10.1126/sciadv.aaz8867.</p>
<p>Haves and have nots must find a better way: the case for open scientific hardware. Maia Chagas, A , 10.1371/journal.pbio.3000014PLoS Biol. 16Maia Chagas, A., 2018. Haves and have nots must find a better way: the case for open scientific hardware. PLoS Biol. 16, e3000014 https://doi.org/10.1371/journal. pbio.3000014.</p>
<p>Leveraging open hardware to alleviate the burden of COVID-19 on global health systems. Maia Chagas, A Molloy, J C Prieto-Godino, L L Baden, T , 10.1371/journal.pbio.3000730PLoS Biol. 18Maia Chagas, A., Molloy, J.C., Prieto-Godino, L.L., Baden, T., 2020. Leveraging open hardware to alleviate the burden of COVID-19 on global health systems. PLoS Biol. 18, e3000730 https://doi.org/10.1371/journal.pbio.3000730.</p>
<p>The role of metabolites and metabolomics in clinically applicable biomarkers of disease. M Mamas, W B Dunn, L Neyses, R Goodacre, 10.1007/s00204-010-0609-6Arch. Toxicol. 85Mamas, M., Dunn, W.B., Neyses, L., Goodacre, R., 2011. The role of metabolites and metabolomics in clinically applicable biomarkers of disease. Arch. Toxicol. 85, 5-17. https://doi.org/10.1007/s00204-010-0609-6.</p>
<p>C D Manning, Foundations of Statistical Natural Language Processing. Cambridge, MassMit Pressfirst edManning, C.D., 1999. Foundations of Statistical Natural Language Processing, first ed. Mit Press, Cambridge, Mass.</p>
<p>Introduction to Information Retrieval. C D Manning, P Raghavan, H Schutze, 10.1017/CBO9780511809071Cambridge University PressCambridgeManning, C.D., Raghavan, P., Schutze, H., 2008. Introduction to Information Retrieval. Cambridge University Press, Cambridge. https://doi.org/10.1017/ CBO9780511809071.</p>
<p>A DIY approach to automating your lab. M May, 10.1038/d41586-019-01590-zNature. 569May, M., 2019. A DIY approach to automating your lab. Nature 569, 587-588. https:// doi.org/10.1038/d41586-019-01590-z.</p>
<p>Comparison of three methods for selecting values of input variables in the analysis of output from a computer code. M D Mckay, R J Beckman, W J Conover, 10.1080/00401706.1979.10489755Technometrics. 21McKay, M.D., Beckman, R.J., Conover, W.J., 1979. Comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics 21, 239-245. https://doi.org/10.1080/00401706.1979.10489755.</p>
<p>SynBioHub: a standards-enabled design repository for synthetic biology. J A Mclaughlin, C J Myers, Z Zundel, G Mısırlı, M Zhang, I D Ofiteru, A Goñi-Moreno, A Wipat, 10.1021/acssynbio.7b00403ACS Synth. Biol. 7McLaughlin, J.A., Myers, C.J., Zundel, Z., Mısırlı, G., Zhang, M., Ofiteru, I.D., Goñi- Moreno, A., Wipat, A., 2018. SynBioHub: a standards-enabled design repository for synthetic biology. ACS Synth. Biol. 7, 682-688. https://doi.org/10.1021/ acssynbio.7b00403.</p>
<p>Meat-free outsells beef. 10.1038/s41587-019-0313-xNat. Biotechnol. 37Meat-free outsells beef. Nat. Biotechnol. 37, 2019. https://doi.org/10.1038/s41587-019- 0313-x, 1250-1250.</p>
<p>The potential of random forest and neural networks for biomass and recombinant protein modeling in Escherichia coli fed-batch fermentations. M Melcher, T Scharl, B Spangl, M Luchner, M Cserjan, K Bayer, F Leisch, G Striedner, 10.1002/biot.201400790Biotechnol. J. 10Melcher, M., Scharl, T., Spangl, B., Luchner, M., Cserjan, M., Bayer, K., Leisch, F., Striedner, G., 2015. The potential of random forest and neural networks for biomass and recombinant protein modeling in Escherichia coli fed-batch fermentations. Biotechnol. J. 10, 1770-1782. https://doi.org/10.1002/biot.201400790.</p>
<p>Semisupervised Gaussian process for automated enzyme search. J Mellor, I Grigoras, P Carbonell, J.-L Faulon, 10.1021/acssynbio.5b00294ACS Synth. Biol. 5Mellor, J., Grigoras, I., Carbonell, P., Faulon, J.-L., 2016. Semisupervised Gaussian process for automated enzyme search. ACS Synth. Biol. 5, 518-528. https://doi.org/ 10.1021/acssynbio.5b00294.</p>
<p>The NIST Definition of Cloud Computing. P Mell, T Grance, 10.6028/NIST.SP.800-145Gaithersburg, MDNational Institute of Standards and TechnologyMell, P., Grance, T., 2011. The NIST Definition of Cloud Computing. National Institute of Standards and Technology, Gaithersburg, MD. https://doi.org/10.6028/NIST. SP.800-145.</p>
<p>Deep learning for the precise peak detection in high-resolution LC-MS data. A D Melnikov, Y P Tsentalovich, V V Yanshole, 10.1021/acs.analchem.9b04811Anal. Chem. 92Melnikov, A.D., Tsentalovich, Y.P., Yanshole, V.V., 2020. Deep learning for the precise peak detection in high-resolution LC-MS data. Anal. Chem. 92, 588-592. https://doi. org/10.1021/acs.analchem.9b04811.</p>
<p>Searle's abstract argument against strong AI. A Melnyk, 10.1007/BF00413696Synthese. 108Melnyk, A., 1996. Searle's abstract argument against strong AI. Synthese 108, 391-419. https://doi.org/10.1007/BF00413696.</p>
<p>Quantitative design of regulatory elements based on high-precision strength prediction using artificial neural network. H Meng, J Wang, Z Xiong, F Xu, G Zhao, Y Wang, 10.1371/journal.pone.0060288PloS One. 8Meng, H., Wang, J., Xiong, Z., Xu, F., Zhao, G., Wang, Y., 2013. Quantitative design of regulatory elements based on high-precision strength prediction using artificial neural network. PloS One 8, e60288. https://doi.org/10.1371/journal. pone.0060288.</p>
<p>Envelope filter sequence to delete blinks and overshoots. M Merino, I M Gómez, A J Molina, 10.1186/s12938-015-0046-0Biomed. Eng. Online. 14Merino, M., Gómez, I.M., Molina, A.J., 2015. Envelope filter sequence to delete blinks and overshoots. Biomed. Eng. Online 14, 48. https://doi.org/10.1186/s12938-015- 0046-0.</p>
<p>A.I. Researchers Are Making More Than $1 Million, Even at a Nonprofit. The New York Times. Cade Metz, Metz, Cade, 2018. A.I. Researchers Are Making More Than $1 Million, Even at a Nonprofit. The New York Times. https://www.nytimes.com/2018/04/19/technolo gy/artificial-intelligence-salaries-openai.html.</p>
<p>The computer revolution. E K Miller, 10.1109/45.31594IEEE Potentials. 8Miller, E.K., 1989. The computer revolution. IEEE Potentials 8, 27-31. https://doi.org/ 10.1109/45.31594.</p>
<p>AMPL: a data-driven modeling pipeline for drug discovery. A J Minnich, K Mcloughlin, M Tse, J Deng, A Weber, N Murad, B D Madej, B Ramsundar, T Rush, S Calad-Thomson, J Brase, J E Allen, 10.1021/acs.jcim.9b01053J. Chem. Inf. Model. 60Minnich, A.J., McLoughlin, K., Tse, M., Deng, J., Weber, A., Murad, N., Madej, B.D., Ramsundar, B., Rush, T., Calad-Thomson, S., Brase, J., Allen, J.E., 2020. AMPL: a data-driven modeling pipeline for drug discovery. J. Chem. Inf. Model. 60, 1955-1968. https://doi.org/10.1021/acs.jcim.9b01053.</p>
<p>MetaNetX/MNXref-reconciliation of metabolites and biochemical reactions to bring together genome-scale metabolic networks. S Moretti, O Martin, T Van Du Tran, A Bridge, A Morgat, M Pagni, 10.1093/nar/gkv1117Nucleic Acids Res. 44Moretti, S., Martin, O., Van Du Tran, T., Bridge, A., Morgat, A., Pagni, M., 2016. MetaNetX/MNXref-reconciliation of metabolites and biochemical reactions to bring together genome-scale metabolic networks. Nucleic Acids Res. 44, D523-D526. https://doi.org/10.1093/nar/gkv1117.</p>
<p>The experiment data Depot: a web-based software tool for biological experimental data storage, sharing, and visualization. W C Morrell, G W Birkel, M Forrer, T Lopez, T W H Backman, M Dussault, C J Petzold, E E K Baidoo, Z Costello, D Ando, J Alonso-Gutierrez, K W George, A Mukhopadhyay, I Vaino, J D Keasling, P D Adams, N J Hillson, Garcia Martin, 10.1021/acssynbio.7b00204ACS Synth. Biol. 6Morrell, W.C., Birkel, G.W., Forrer, M., Lopez, T., Backman, T.W.H., Dussault, M., Petzold, C.J., Baidoo, E.E.K., Costello, Z., Ando, D., Alonso-Gutierrez, J., George, K. W., Mukhopadhyay, A., Vaino, I., Keasling, J.D., Adams, P.D., Hillson, N.J., Garcia Martin, H., 2017. The experiment data Depot: a web-based software tool for biological experimental data storage, sharing, and visualization. ACS Synth. Biol. 6, 2248-2259. https://doi.org/10.1021/acssynbio.7b00204.</p>
<p>Mechanism and inhibition of 1-deoxy-D-xylulose-5-phosphate reductoisomerase. A S Murkin, K A Manning, S A Kholodar, 10.1016/j.bioorg.2014.06.001Bioorg. Chem. 57Murkin, A.S., Manning, K.A., Kholodar, S.A., 2014. Mechanism and inhibition of 1- deoxy-D-xylulose-5-phosphate reductoisomerase. Bioorg. Chem. 57, 171-185. https://doi.org/10.1016/j.bioorg.2014.06.001.</p>
<p>REPARATION: ribosome profiling assisted (re-)annotation of bacterial genomes. E Ndah, V Jonckheere, A Giess, E Valen, G Menschaert, P Van Damme, 10.1093/nar/gkx758Nucleic Acids Res. 45Ndah, E., Jonckheere, V., Giess, A., Valen, E., Menschaert, G., Van Damme, P., 2017. REPARATION: ribosome profiling assisted (re-)annotation of bacterial genomes. Nucleic Acids Res. 45, e168. https://doi.org/10.1093/nar/gkx758.</p>
<p>Engineering cellular metabolism. J Nielsen, J D Keasling, 10.1016/j.cell.2016.02.004Cell. 164Nielsen, J., Keasling, J.D., 2016. Engineering cellular metabolism. Cell 164, 1185-1197. https://doi.org/10.1016/j.cell.2016.02.004.</p>
<p>Improved enzyme annotation with EC-specific cutoffs using DETECT v2. N Nursimulu, L L Xu, J D Wasmuth, I Krukov, J Parkinson, 10.1093/bioinformatics/bty368Bioinformatics. 34Nursimulu, N., Xu, L.L., Wasmuth, J.D., Krukov, I., Parkinson, J., 2018. Improved enzyme annotation with EC-specific cutoffs using DETECT v2. Bioinformatics 34, 3393-3395. https://doi.org/10.1093/bioinformatics/bty368.</p>
<p>Using genome-scale models to predict biological capabilities. E J O&apos;brien, J M Monk, B O Palsson, 10.1016/j.cell.2015.05.019Cell. 161O'Brien, E.J., Monk, J.M., Palsson, B.O., 2015. Using genome-scale models to predict biological capabilities. Cell 161, 971-987. https://doi.org/10.1016/j. cell.2015.05.019.</p>
<p>. O&apos; Others, T Malley, E Bursztein, J Long, F Chollet, H Jin, L Invernizzi, Keras TunerOthers O'Malley, T., Bursztein, E., Long, J., Chollet, F., Jin, H., Invernizzi, L., 2019. Keras Tuner.</p>
<p>Revolutions in the 1980s and 1990s. G O&apos;regan, 10.1007/978-1-4471-2359-0_5A Brief History of Computing. London, LondonSpringerO'Regan, G., 2012. Revolutions in the 1980s and 1990s. In: A Brief History of Computing. Springer London, London, pp. 63-69. https://doi.org/10.1007/978-1- 4471-2359-0_5.</p>
<p>Identifying and harnessing the building blocks of machine learning pipelines for sensible initialization of a data science automation tool. R S Olson, J H Moore, 10.1007/978-3-319-97088-2_14Riolo, R., Worzel, B., Goldman, B., Tozier, B.Springer International PublishingChamGenetic Programming Theory and Practice XIV, Genetic and Evolutionary ComputationOlson, R.S., Moore, J.H., 2018. Identifying and harnessing the building blocks of machine learning pipelines for sensible initialization of a data science automation tool. In: Riolo, R., Worzel, B., Goldman, B., Tozier, B. (Eds.), Genetic Programming Theory and Practice XIV, Genetic and Evolutionary Computation. Springer International Publishing, Cham, pp. 211-223. https://doi.org/10.1007/978-3-319- 97088-2_14.</p>
<p>TPOT: a tree-based pipeline optimization tool for automating machine learning. R S Olson, J H Moore, 10.1007/978-3-030-05318-5_8Automated Machine Learning: Methods, Systems, Challenges, the Springer Series on Challenges in Machine Learning. Hutter, F., Kotthoff, L., Vanschoren, J.ChamSpringer International PublishingOlson, R.S., Moore, J.H., 2019. TPOT: a tree-based pipeline optimization tool for automating machine learning. In: Hutter, F., Kotthoff, L., Vanschoren, J. (Eds.), Automated Machine Learning: Methods, Systems, Challenges, the Springer Series on Challenges in Machine Learning. Springer International Publishing, Cham, pp. 151-160. https://doi.org/10.1007/978-3-030-05318-5_8.</p>
<p>Automating biomedical data science through tree-based pipeline optimization. R S Olson, R J Urbanowicz, P C Andrews, N A Lavender, L C Kidd, J H Moore, 10.1007/978-3-319-31204-0_9Squillero, G., Burelli, P.Springer International PublishingChamApplications of Evolutionary Computation, Lecture Notes in Computer ScienceOlson, R.S., Urbanowicz, R.J., Andrews, P.C., Lavender, N.A., Kidd, L.C., Moore, J.H., 2016. Automating biomedical data science through tree-based pipeline optimization. In: Squillero, G., Burelli, P. (Eds.), Applications of Evolutionary Computation, Lecture Notes in Computer Science. Springer International Publishing, Cham, pp. 123-137. https://doi.org/10.1007/978-3-319-31204-0_9.</p>
<p>Lessons from two design-buildtest-learn cycles of dodecanol production in Escherichia coli aided by machine learning. P Opgenorth, Z Costello, T Okada, G Goyal, Y Chen, J Gin, V Benites, M De Raad, T R Northen, K Deng, S Deutsch, E E K Baidoo, C J Petzold, N J Hillson, H Garcia Martin, H R Beller, 10.1021/acssynbio.9b00020ACS Synth. Biol. 4Open for business, 2017. Sci. Data 4, 170058. https://doi.org/10.1038/sdata.2017.58. Opgenorth, P., Costello, Z., Okada, T., Goyal, G., Chen, Y., Gin, J., Benites, V., de Raad, M., Northen, T.R., Deng, K., Deutsch, S., Baidoo, E.E.K., Petzold, C.J., Hillson, N.J., Garcia Martin, H., Beller, H.R., 2019. Lessons from two design-build- test-learn cycles of dodecanol production in Escherichia coli aided by machine learning. ACS Synth. Biol. 8, 1337-1351. https://doi.org/10.1021/ acssynbio.9b00020.</p>
<p>Semi-synthetic artemisinin: a model for the use of synthetic biology in pharmaceutical development. C J Paddon, J D Keasling, 10.1038/nrmicro3240Nat. Rev. Microbiol. 12Paddon, C.J., Keasling, J.D., 2014. Semi-synthetic artemisinin: a model for the use of synthetic biology in pharmaceutical development. Nat. Rev. Microbiol. 12, 355-367. https://doi.org/10.1038/nrmicro3240.</p>
<p>A Unified Framework for Tumor Proliferation Score Prediction in Breast Histopathology. K Paeng, S Hwang, S Park, M Kim, S Kim, Paeng, K., Hwang, S., Park, S., Kim, M., Kim, S., 2016. A Unified Framework for Tumor Proliferation Score Prediction in Breast Histopathology.</p>
<p>A survey on transfer learning. S J Pan, Q Yang, 10.1109/TKDE.2009.191IEEE Trans. Knowl. Data Eng. 22Pan, S.J., Yang, Q., 2010. A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 22, 1345-1359. https://doi.org/10.1109/TKDE.2009.191.</p>
<p>Artificial neural network and regression coupled genetic algorithm to optimize parameters for enhanced xylitol production by Debaryomyces nepalensis in bioreactor. S M J Pappu, S N Gummadi, 10.1016/j.bej.2017.01.010Biochem. Eng. J. 120Pappu, S.M.J., Gummadi, S.N., 2017. Artificial neural network and regression coupled genetic algorithm to optimize parameters for enhanced xylitol production by Debaryomyces nepalensis in bioreactor. Biochem. Eng. J. 120, 136-145. https://doi. org/10.1016/j.bej.2017.01.010.</p>
<p>Diversifying the structure of zinc finger nucleases for high-precision genome editing. D E Paschon, S Lussier, T Wangzor, D F Xia, P W Li, S J Hinkley, N A Scarlott, S C Lam, A J Waite, L N Truong, N Gandhi, B N Kadam, D P Patil, D A Shivak, G K Lee, M C Holmes, L Zhang, J C Miller, E J Rebar, 10.1038/s41467-019-08867-xNat. Commun. 101133Paschon, D.E., Lussier, S., Wangzor, T., Xia, D.F., Li, P.W., Hinkley, S.J., Scarlott, N.A., Lam, S.C., Waite, A.J., Truong, L.N., Gandhi, N., Kadam, B.N., Patil, D.P., Shivak, D. A., Lee, G.K., Holmes, M.C., Zhang, L., Miller, J.C., Rebar, E.J., 2019. Diversifying the structure of zinc finger nucleases for high-precision genome editing. Nat. Commun. 10, 1133. https://doi.org/10.1038/s41467-019-08867-x.</p>
<p>Scikit-learn: machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, J. Mach. Learn. Res. 12Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E., 2011. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, 2825-2830.</p>
<p>Towards artificial general intelligence with hybrid Tianjic chip architecture. J Pei, L Deng, S Song, M Zhao, Y Zhang, Wu, Shuang, G Wang, Z Zou, Z Wu, W He, F Chen, N Deng, Wu, Si, Y Wang, Y Wu, Z Yang, C Ma, G Li, W Han, H Li, L Shi, 10.1038/s41586-019-1424-8Nature. 572Pei, J., Deng, L., Song, S., Zhao, M., Zhang, Y., Wu, Shuang, Wang, G., Zou, Z., Wu, Z., He, W., Chen, F., Deng, N., Wu, Si, Wang, Y., Wu, Y., Yang, Z., Ma, C., Li, G., Han, W., Li, H., Shi, L., 2019. Towards artificial general intelligence with hybrid Tianjic chip architecture. Nature 572, 106-111. https://doi.org/10.1038/s41586- 019-1424-8.</p>
<p>Microbial engineering for the production of advanced biofuels. P P Peralta-Yahya, F Zhang, S B Del Cardayre, J D Keasling, 10.1038/nature11478Nature. 488Peralta-Yahya, P.P., Zhang, F., del Cardayre, S.B., Keasling, J.D., 2012. Microbial engineering for the production of advanced biofuels. Nature 488, 320-328. https:// doi.org/10.1038/nature11478.</p>
<p>Transfer learning across ontologies for phenome-genome association prediction. R Petegrosso, S Park, T H Hwang, R Kuang, 10.1093/bioinformatics/btw649Bioinformatics. 33Petegrosso, R., Park, S., Hwang, T.H., Kuang, R., 2017. Transfer learning across ontologies for phenome-genome association prediction. Bioinformatics 33, 529-536. https://doi.org/10.1093/bioinformatics/btw649.</p>
<p>Analytics for metabolic engineering. C J Petzold, L J G Chan, M Nhan, P D Adams, 10.3389/fbioe.2015.00135Front. Bioeng. Biotechnol. 3Petzold, C.J., Chan, L.J.G., Nhan, M., Adams, P.D., 2015. Analytics for metabolic engineering. Front. Bioeng. Biotechnol. 3, 135. https://doi.org/10.3389/ fbioe.2015.00135.</p>
<p>Deep reinforcement learning for de novo drug design. M Popova, O Isayev, A Tropsha, 10.1126/sciadv.aap7885Sci. Adv. 4Popova, M., Isayev, O., Tropsha, A., 2018. Deep reinforcement learning for de novo drug design. Sci. Adv. 4, eaap7885 https://doi.org/10.1126/sciadv.aap7885.</p>
<p>Systems metabolic engineering meets machine learning: a new era for data-driven metabolic engineering. K V Presnell, H S Alper, 10.1002/biot.201800416Biotechnol. J. 14Presnell, K.V., Alper, H.S., 2019. Systems metabolic engineering meets machine learning: a new era for data-driven metabolic engineering. Biotechnol. J. 14, e1800416 https://doi.org/10.1002/biot.201800416.</p>
<p>A machine learning Automated Recommendation Tool for synthetic biology. T Radivojević, Z Costello, K Workman, H Garcia Martin, 10.1038/s41467-020-18008-4Nat. Commun. 114879Radivojević, T., Costello, Z., Workman, K., Garcia Martin, H., 2020. A machine learning Automated Recommendation Tool for synthetic biology. Nat. Commun. 11, 4879. https://doi.org/10.1038/s41467-020-18008-4.</p>
<p>Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, 10.1016/j.jcp.2018.10.045J. Comput. Phys. 378Raissi, M., Perdikaris, P., Karniadakis, G.E., 2019. Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys. 378, 686-707. https://doi. org/10.1016/j.jcp.2018.10.045.</p>
<p>FLASH assembly of TALENs for high-throughput genome editing. D Reyon, S Q Tsai, C Khayter, J A Foden, J D Sander, J K Joung, 10.1038/nbt.2170Nat. Biotechnol. 30Reyon, D., Tsai, S.Q., Khayter, C., Foden, J.A., Sander, J.D., Joung, J.K., 2012. FLASH assembly of TALENs for high-throughput genome editing. Nat. Biotechnol. 30, 460-465. https://doi.org/10.1038/nbt.2170.</p>
<p>Predicting strength and function for promoters of the Escherichia coli alternative sigma factor, sigmaE. V A Rhodius, V K Mutalik, 10.1073/pnas.0915066107Proc. Natl. Acad. Sci. U.S.A. 107Rhodius, V.A., Mutalik, V.K., 2010. Predicting strength and function for promoters of the Escherichia coli alternative sigma factor, sigmaE. Proc. Natl. Acad. Sci. U.S.A. 107, 2854-2859. https://doi.org/10.1073/pnas.0915066107.</p>
<p>Three pitfalls to avoid in machine learning. P Riley, 10.1038/d41586-019-02307-yNature. 572Riley, P., 2019. Three pitfalls to avoid in machine learning. Nature 572, 27-29. https:// doi.org/10.1038/d41586-019-02307-y.</p>
<p>Dask: parallel computation with blocked algorithms and task scheduling. M Rocklin, Proceedings of the 14th Python in Science Conference. Presented at the SciPy. Huff, K., Bergstra, J.the 14th Python in Science Conference. Presented at the SciPyRocklin, M., 2015. Dask: parallel computation with blocked algorithms and task scheduling. In: Huff, K., Bergstra, J. (Eds.), Proceedings of the 14th Python in Science Conference. Presented at the SciPy 2015, pp. 130-136.</p>
<p>The good, the bad, and the ugly in chemical and biological data for machine learning. T Rodrigues, 10.1016/j.ddtec.2020.07.001Drug Discov. Today Technol. Rodrigues, T., 2020. The good, the bad, and the ugly in chemical and biological data for machine learning. Drug Discov. Today Technol. https://doi.org/10.1016/j. ddtec.2020.07.001.</p>
<p>The AI Hierarchy of Needs. Monica Rogati, Rogati, Monica, 2017. The AI Hierarchy of Needs. Hackernoon. https://hackernoon.com /the-ai-hierarchy-of-needs-18f111fcc007.</p>
<p>An automated workflow for enhancing microbial bioprocess optimization on a novel microbioreactor platform. Microb. Cell Factories 11. P Rohe, D Venkanna, B Kleine, R Freudl, M Oldiges, 10.1186/1475-2859-11-144144Rohe, P., Venkanna, D., Kleine, B., Freudl, R., Oldiges, M., 2012. An automated workflow for enhancing microbial bioprocess optimization on a novel microbioreactor platform. Microb. Cell Factories 11, 144. https://doi.org/10.1186/1475-2859-11- 144.</p>
<p>Navigating the protein fitness landscape with Gaussian processes. P A Romero, A Krause, F H Arnold, 10.1073/pnas.1215251110Proc. Natl. Acad. Sci. U.S.A. 110Romero, P.A., Krause, A., Arnold, F.H., 2013. Navigating the protein fitness landscape with Gaussian processes. Proc. Natl. Acad. Sci. U.S.A. 110, E193-E201. https://doi. org/10.1073/pnas.1215251110.</p>
<p>Deep learning enables high-quality and highthroughput prediction of enzyme commission numbers. J Y Ryu, H U Kim, S Y Lee, 10.1073/pnas.1821905116Proc. Natl. Acad. Sci. U.S.A. 116Ryu, J.Y., Kim, H.U., Lee, S.Y., 2019. Deep learning enables high-quality and high- throughput prediction of enzyme commission numbers. Proc. Natl. Acad. Sci. U.S.A. 116, 13996-14001. https://doi.org/10.1073/pnas.1821905116.</p>
<p>Toward the first data acquisition standard in synthetic biology. I Sainz De Murieta, M Bultelle, R I Kitney, 10.1021/acssynbio.5b00222ACS Synth. Biol. 5Sainz de Murieta, I., Bultelle, M., Kitney, R.I., 2016. Toward the first data acquisition standard in synthetic biology. ACS Synth. Biol. 5, 817-826. https://doi.org/ 10.1021/acssynbio.5b00222.</p>
<p>Machine-learning-guided mutagenesis for directed evolution of fluorescent proteins. Y Saito, M Oikawa, H Nakazawa, T Niide, T Kameda, K Tsuda, M Umetsu, 10.1021/acssynbio.8b00155ACS Synth. Biol. 7Saito, Y., Oikawa, M., Nakazawa, H., Niide, T., Kameda, T., Tsuda, K., Umetsu, M., 2018. Machine-learning-guided mutagenesis for directed evolution of fluorescent proteins. ACS Synth. Biol. 7, 2014-2022. https://doi.org/10.1021/acssynbio.8b00155.</p>
<p>Machine learning for detection and diagnosis of disease. P Sajda, 10.1146/annurev.bioeng.8.061505.095802Annu. Rev. Biomed. Eng. 8Sajda, P., 2006. Machine learning for detection and diagnosis of disease. Annu. Rev. Biomed. Eng. 8, 537-565. https://doi.org/10.1146/annurev. bioeng.8.061505.095802.</p>
<p>Automated design of synthetic ribosome binding sites to control protein expression. H M Salis, E A Mirsky, C A Voigt, 10.1038/nbt.1568Nat. Biotechnol. 27Salis, H.M., Mirsky, E.A., Voigt, C.A., 2009. Automated design of synthetic ribosome binding sites to control protein expression. Nat. Biotechnol. 27, 946-950. https:// doi.org/10.1038/nbt.1568.</p>
<p>Web-scale k-means clustering. D Sculley, 10.1145/1772690.1772862Proceedings of the 19th International Conference on World Wide Web -WWW '10. Presented at the the 19th International Conference. the 19th International Conference on World Wide Web -WWW '10. Presented at the the 19th International ConferenceNew York, New York, USAACM Press1177Sculley, D., 2010. Web-scale k-means clustering. In: Proceedings of the 19th International Conference on World Wide Web -WWW '10. Presented at the the 19th International Conference. ACM Press, New York, New York, USA, p. 1177. https:// doi.org/10.1145/1772690.1772862.</p>
<p>Planning chemical syntheses with deep neural networks and symbolic AI. M H S Segler, M Preuss, M P Waller, 10.1038/nature25978Nature. 555Segler, M.H.S., Preuss, M., Waller, M.P., 2018. Planning chemical syntheses with deep neural networks and symbolic AI. Nature 555, 604-610. https://doi.org/10.1038/ nature25978.</p>
<p>Time-split cross-validation as a method for estimating the goodness of prospective prediction. R P Sheridan, 10.1021/ci400084kJ. Chem. Inf. Model. 53Sheridan, R.P., 2013. Time-split cross-validation as a method for estimating the goodness of prospective prediction. J. Chem. Inf. Model. 53, 783-790. https://doi.org/ 10.1021/ci400084k.</p>
<p>Five innovative ways to use 3D printing in the laboratory. A Silver, 10.1038/d41586-018-07853-5Nature. 565Silver, A., 2019. Five innovative ways to use 3D printing in the laboratory. Nature 565, 123-124. https://doi.org/10.1038/d41586-018-07853-5.</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, 10.1038/nature16961Nature. 529Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., Hassabis, D., 2016. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484-489. https://doi.org/ 10.1038/nature16961.</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, T Lillicrap, K Simonyan, D Hassabis, 10.1126/science.aar6404Science. 362Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., Hassabis, D., 2018. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science 362, 1140-1144. https://doi.org/10.1126/science. aar6404.</p>
<p>Automated multiplex genomescale engineering in yeast. T Si, R Chao, Y Min, Y Wu, W Ren, H Zhao, 10.1038/ncomms15187Nat. Commun. 8Si, T., Chao, R., Min, Y., Wu, Y., Ren, W., Zhao, H., 2017. Automated multiplex genome- scale engineering in yeast. Nat. Commun. 8, 15187. https://doi.org/10.1038/ ncomms15187.</p>
<p>Molecular Genetics of Bacteria. L R Snyder, J E Peters, T M Henkin, W Champness, ASM Pressfourth ed.fourth ed.Snyder, L.R., Peters, J.E., Henkin, T.M., Champness, W., 2014. Molecular Genetics of Bacteria, fourth ed.fourth ed. ASM Press.</p>
<p>Big data: astronomical or genomical?. Z D Stephens, S Y Lee, F Faghri, R H Campbell, C Zhai, M J Efron, R Iyer, M C Schatz, S Sinha, G E Robinson, 10.1371/journal.pbio.1002195PLoS Biol. 13Stephens, Z.D., Lee, S.Y., Faghri, F., Campbell, R.H., Zhai, C., Efron, M.J., Iyer, R., Schatz, M.C., Sinha, S., Robinson, G.E., 2015. Big data: astronomical or genomical? PLoS Biol. 13, e1002195 https://doi.org/10.1371/journal.pbio.1002195.</p>
<p>DNA-BOT: a low-cost, automated DNA assembly platform for synthetic biology. M Storch, M C Haines, G S Baldwin, 10.1093/synbio/ysaa010Synth. Biol. Storch, M., Haines, M.C., Baldwin, G.S., 2020. DNA-BOT: a low-cost, automated DNA assembly platform for synthetic biology. Synth. Biol. https://doi.org/10.1093/ synbio/ysaa010.</p>
<p>Can exascale computing and explainable artificial intelligence applied to plant biology deliver on the United Nations sustainable development goals?. J Streich, J Romero, J G F M Gazolla, D Kainer, A Cliff, E T Prates, J B Brown, S Khoury, G A Tuskan, M Garvin, D Jacobson, A L Harfouche, 10.1016/j.copbio.2020.01.010Curr. Opin. Biotechnol. 61Streich, J., Romero, J., Gazolla, J.G.F.M., Kainer, D., Cliff, A., Prates, E.T., Brown, J.B., Khoury, S., Tuskan, G.A., Garvin, M., Jacobson, D., Harfouche, A.L., 2020. Can exascale computing and explainable artificial intelligence applied to plant biology deliver on the United Nations sustainable development goals? Curr. Opin. Biotechnol. 61, 217-225. https://doi.org/10.1016/j.copbio.2020.01.010.</p>
<p>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches. B E Suzek, Y Wang, H Huang, P B Mcgarvey, C H Wu, Uniprot Consortium, 10.1093/bioinformatics/btu739Bioinformatics. 31Suzek, B.E., Wang, Y., Huang, H., McGarvey, P.B., Wu, C.H., Consortium, UniProt, 2015. UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics 31, 926-932. https://doi.org/10.1093/ bioinformatics/btu739.</p>
<p>Robotic process automation: contemporary themes and challenges. R Syed, S Suriadi, M Adams, W Bandara, S J J Leemans, C Ouyang, A H M Ter Hofstede, I Van De Weerd, M T Wynn, H A Reijers, 10.1016/j.compind.2019.103162Comput. Ind. 115Syed, R., Suriadi, S., Adams, M., Bandara, W., Leemans, S.J.J., Ouyang, C., ter Hofstede, A.H.M., van de Weerd, I., Wynn, M.T., Reijers, H.A., 2020. Robotic process automation: contemporary themes and challenges. Comput. Ind. 115, 103162. https://doi.org/10.1016/j.compind.2019.103162.</p>
<p>An integrated approach to characterize genetic interaction networks in yeast metabolism. B Szappanos, K Kovács, B Szamecz, F Honti, M Costanzo, A Baryshnikova, G Gelius-Dietrich, M J Lercher, M Jelasity, C L Myers, B J Andrews, C Boone, S G Oliver, C Pál, B Papp, 10.1038/ng.846Nat. Genet. 43Szappanos, B., Kovács, K., Szamecz, B., Honti, F., Costanzo, M., Baryshnikova, A., Gelius- Dietrich, G., Lercher, M.J., Jelasity, M., Myers, C.L., Andrews, B.J., Boone, C., Oliver, S.G., Pál, C., Papp, B., 2011. An integrated approach to characterize genetic interaction networks in yeast metabolism. Nat. Genet. 43, 656-662. https://doi.org/ 10.1038/ng.846.</p>
<p>UniProt: the universal protein knowledgebase. 10.1093/nar/gkw1099The UniProt Consortium. 45The UniProt Consortium, 2017. UniProt: the universal protein knowledgebase. Nucleic Acids Res. 45, D158-D169. https://doi.org/10.1093/nar/gkw1099.</p>
<p>A protocol for generating a high-quality genome-scale metabolic reconstruction. I Thiele, B Ø Palsson, 10.1038/nprot.2009.203Nat. Protoc. 5Thiele, I., Palsson, B.Ø., 2010. A protocol for generating a high-quality genome-scale metabolic reconstruction. Nat. Protoc. 5, 93-121. https://doi.org/10.1038/ nprot.2009.203.</p>
<p>Redirecting metabolic flux via combinatorial multiplex CRISPRi-mediated repression for isopentenol production in Escherichia coli. T Tian, J W Kang, A Kang, T S Lee, 10.1021/acssynbio.8b00429ACS Synth. Biol. 8Tian, T., Kang, J.W., Kang, A., Lee, T.S., 2019. Redirecting metabolic flux via combinatorial multiplex CRISPRi-mediated repression for isopentenol production in Escherichia coli. ACS Synth. Biol. 8, 391-402. https://doi.org/10.1021/ acssynbio.8b00429.</p>
<p>Deep reinforcement learning for the control of microbial co-cultures in bioreactors. N J Treloar, A J H Fedorec, B Ingalls, C P Barnes, 10.1371/journal.pcbi.1007783PLoS Comput. Biol. 16Treloar, N.J., Fedorec, A.J.H., Ingalls, B., Barnes, C.P., 2020. Deep reinforcement learning for the control of microbial co-cultures in bioreactors. PLoS Comput. Biol. 16, e1007783 https://doi.org/10.1371/journal.pcbi.1007783.</p>
<p>UniProt: a hub for protein information. ; W M P Uniprot Consortium, M Bichler, A Heinzl, 10.1007/s12599-018-0542-4Bus. Inf. Syst. Eng. 43Nucleic Acids Res.UniProt Consortium, 2015. UniProt: a hub for protein information. Nucleic Acids Res. 43, D204-D212. https://doi.org/10.1093/nar/gku989. van der Aalst, W.M.P., Bichler, M., Heinzl, A., 2018. Robotic process automation. Bus. Inf. Syst. Eng. 60, 269-272. https://doi.org/10.1007/s12599-018-0542-4.</p>
<p>Super learner. M J Van Der Laan, E C Polley, A E Hubbard, 10.2202/1544-6115.1309Stat. Appl. Genet. Mol. Biol. 6van der Laan, M.J., Polley, E.C., Hubbard, A.E., 2007. Super learner. Stat. Appl. Genet. Mol. Biol. 6, Article25 https://doi.org/10.2202/1544-6115.1309.</p>
<p>ProteomeXchange provides globally coordinated proteomics data submission and dissemination. J A Vizcaíno, E W Deutsch, R Wang, A Csordas, F Reisinger, D Ríos, J A Dianes, Z Sun, T Farrah, N Bandeira, P.-A Binz, I Xenarios, M Eisenacher, G Mayer, L Gatto, A Campos, R J Chalkley, H.-J Kraus, J P Albar, S Martinez-Bartolomé, H Hermjakob, 10.1038/nbt.2839Nat. Biotechnol. 32Vizcaíno, J.A., Deutsch, E.W., Wang, R., Csordas, A., Reisinger, F., Ríos, D., Dianes, J.A., Sun, Z., Farrah, T., Bandeira, N., Binz, P.-A., Xenarios, I., Eisenacher, M., Mayer, G., Gatto, L., Campos, A., Chalkley, R.J., Kraus, H.-J., Albar, J.P., Martinez- Bartolomé, S., Hermjakob, H., 2014. ProteomeXchange provides globally coordinated proteomics data submission and dissemination. Nat. Biotechnol. 32, 223-226. https://doi.org/10.1038/nbt.2839.</p>
<p>Biosystems design by machine learning. M J Volk, I Lourentzou, S Mishra, L T Vo, C Zhai, H Zhao, 10.1021/acssynbio.0c00129ACS Synth. Biol. 9Volk, M.J., Lourentzou, I., Mishra, S., Vo, L.T., Zhai, C., Zhao, H., 2020. Biosystems design by machine learning. ACS Synth. Biol. 9, 1514-1533. https://doi.org/ 10.1021/acssynbio.0c00129.</p>
<p>Deep learning for computer vision: a brief review. A Voulodimos, N Doulamis, A Doulamis, E Protopapadakis, 10.1155/2018/7068349Comput. Intell. Neurosci. 7068349Voulodimos, A., Doulamis, N., Doulamis, A., Protopapadakis, E., 2018. Deep learning for computer vision: a brief review. Comput. Intell. Neurosci., 7068349 https://doi.org/ 10.1155/2018/7068349.</p>
<p>Rethinking Weak Vs. Strong AI. Kathleen Walch, Walch, Kathleen, 2019. Rethinking Weak Vs. Strong AI. Forbes. https://www.forbes.co m/sites/cognitiveworld/2019/10/04/rethinking-weak-vs-strong-ai/?sh=193 057d76da3.</p>
<p>RNAi expression tuning, microfluidic screening, and genome recombineering for improved protein production in Saccharomyces cerevisiae. G Wang, S M Björk, M Huang, Q Liu, K Campbell, J Nielsen, H N Joensson, D Petranovic, 10.1073/pnas.1820561116Proc. Natl. Acad. Sci. U.S.A. 116Wang, G., Björk, S.M., Huang, M., Liu, Q., Campbell, K., Nielsen, J., Joensson, H.N., Petranovic, D., 2019. RNAi expression tuning, microfluidic screening, and genome recombineering for improved protein production in Saccharomyces cerevisiae. Proc. Natl. Acad. Sci. U.S.A. 116, 9324-9332. https://doi.org/10.1073/pnas.1820561116.</p>
<p>Toward a Justification of Meta-Learning: Is the No Free Lunch Theorem a Show-Stopper. C Giraud-Carrier, F Provost, Proceedings of the ICML-2005 Workshop on Meta-Learning 12. the ICML-2005 Workshop on Meta-Learning 12Giraud-Carrier, C., Provost, F., 2005. Toward a Justification of Meta-Learning: Is the No Free Lunch Theorem a Show-Stopper. Proceedings of the ICML-2005 Workshop on Meta-Learning 12.</p>
<p>Engineering robust production microbes for large-scale cultivation. M Wehrs, D Tanjore, T Eng, J Lievense, T R Pray, A Mukhopadhyay, Trends in microbiology. 276Wehrs, M., Tanjore, D., Eng, T., Lievense, J., Pray, T.R., Mukhopadhyay, A., 2019. Engineering robust production microbes for large-scale cultivation. Trends in microbiology 27 (6), 524-537.</p>
<p>The lack of A priori distinctions between learning algorithms. D H Wolpert, 10.1162/neco.1996.8.7.1341Neural Comput. 8Wolpert, D.H., 1996. The lack of A priori distinctions between learning algorithms. Neural Comput. 8, 1341-1390. https://doi.org/10.1162/neco.1996.8.7.1341.</p>
<p>Precise, automated control of conditions for high-throughput growth of yeast and bacteria with eVOLVER. B G Wong, C P Mancuso, S Kiriakov, C J Bashor, A S Khalil, 10.1038/nbt.4151Nat. Biotechnol. 36Wong, B.G., Mancuso, C.P., Kiriakov, S., Bashor, C.J., Khalil, A.S., 2018. Precise, automated control of conditions for high-throughput growth of yeast and bacteria with eVOLVER. Nat. Biotechnol. 36, 614-623. https://doi.org/10.1038/nbt.4151.</p>
<p>Metabolic engineering: past and future. B M Woolston, S Edgar, G Stephanopoulos, 10.1146/annurev-chembioeng-061312-103312Annu. Rev. Chem. Biomol. Eng. 4Woolston, B.M., Edgar, S., Stephanopoulos, G., 2013. Metabolic engineering: past and future. Annu. Rev. Chem. Biomol. Eng. 4, 259-288. https://doi.org/10.1146/ annurev-chembioeng-061312-103312.</p>
<p>Rapid prediction of bacterial heterotrophic fluxomics using machine learning and constraint programming. S G Wu, Y Wang, W Jiang, T Oyetunde, R Yao, X Zhang, K Shimizu, Y J Tang, F S Bao, 10.1371/journal.pcbi.1004838PLoS Comput. Biol. 12Wu, S.G., Wang, Y., Jiang, W., Oyetunde, T., Yao, R., Zhang, X., Shimizu, K., Tang, Y.J., Bao, F.S., 2016. Rapid prediction of bacterial heterotrophic fluxomics using machine learning and constraint programming. PLoS Comput. Biol. 12, e1004838 https://doi. org/10.1371/journal.pcbi.1004838.</p>
<p>Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, J Klingner, A Shah, M Johnson, X Liu, Ł Kaiser, S Gouws, Y Kato, T Kudo, H Kazawa, K Stevens, J Dean, Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Dean, J., 2016. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.</p>
<p>Machine learningassisted directed protein evolution with combinatorial libraries. Z Wu, S B J Kan, R D Lewis, B J Wittmann, F H Arnold, 10.1073/pnas.1901979116Proc. Natl. Acad. Sci. U.S.A. 116Wu, Z., Kan, S.B.J., Lewis, R.D., Wittmann, B.J., Arnold, F.H., 2019. Machine learning- assisted directed protein evolution with combinatorial libraries. Proc. Natl. Acad. Sci. U.S.A. 116, 8852-8858. https://doi.org/10.1073/pnas.1901979116.</p>
<p>Characterization of TAP Ambr 250 disposable bioreactors, as a reliable scale-down model for biologics process development. P Xu, C Clark, T Ryder, C Sparks, J Zhou, M Wang, R Russell, C Scott, 10.1002/btpr.2417Biotechnol. Prog. 33Xu, P., Clark, C., Ryder, T., Sparks, C., Zhou, J., Wang, M., Russell, R., Scott, C., 2017. Characterization of TAP Ambr 250 disposable bioreactors, as a reliable scale-down model for biologics process development. Biotechnol. Prog. 33, 478-489. https:// doi.org/10.1002/btpr.2417.</p>
<p>From cloud computing to cloud manufacturing. X Xu, 10.1016/j.rcim.2011.07.002Robot. Comput. Integrated Manuf. 28Xu, X., 2012. From cloud computing to cloud manufacturing. Robot. Comput. Integrated Manuf. 28, 75-86. https://doi.org/10.1016/j.rcim.2011.07.002.</p>
<p>Machine-learning-guided directed evolution for protein engineering. K K Yang, Z Wu, F H Arnold, 10.1038/s41592-019-0496-6Nat. Methods. 16Yang, K.K., Wu, Z., Arnold, F.H., 2019. Machine-learning-guided directed evolution for protein engineering. Nat. Methods 16, 687-694. https://doi.org/10.1038/s41592- 019-0496-6.</p>
<p>L Yang, X Meng, G E Karniadakis, org/abs/2003.060972020. B-PINNs: Bayesian Physics-Informed Neural Networks for Forward and Inverse PDE Problems with Noisy Data. Yang, L., Meng, X., Karniadakis, G.E., 2020. B-PINNs: Bayesian Physics-Informed Neural Networks for Forward and Inverse PDE Problems with Noisy Data. arXiv. htt ps://arxiv.org/abs/2003.06097.</p>
<p>Hidden Markov models and their applications in biological sequence analysis. B.-J Yoon, 10.2174/138920209789177575Curr. Genom. 10Yoon, B.-J., 2009. Hidden Markov models and their applications in biological sequence analysis. Curr. Genom. 10, 402-415. https://doi.org/10.2174/ 138920209789177575.</p>
<p>Genome-wide enzyme annotation with precision control: catalytic families (CatFam) databases. C Yu, N Zavaljevski, V Desai, J Reifman, 10.1002/prot.22167Proteins. 74Yu, C., Zavaljevski, N., Desai, V., Reifman, J., 2009. Genome-wide enzyme annotation with precision control: catalytic families (CatFam) databases. Proteins 74, 449-460. https://doi.org/10.1002/prot.22167.</p>
<p>Machine and deep learning meet genome-scale metabolic modeling. G Zampieri, S Vijayakumar, E Yaneske, C Angione, 10.1371/journal.pcbi.1007084PLoS Comput. Biol. 15Zampieri, G., Vijayakumar, S., Yaneske, E., Angione, C., 2019. Machine and deep learning meet genome-scale metabolic modeling. PLoS Comput. Biol. 15, e1007084 https://doi.org/10.1371/journal.pcbi.1007084.</p>
<p>Frontiers of high-throughput metabolomics. M Zampieri, K Sekar, N Zamboni, U Sauer, 10.1016/j.cbpa.2016.12.006Curr. Opin. Chem. Biol. 36Zampieri, M., Sekar, K., Zamboni, N., Sauer, U., 2017. Frontiers of high-throughput metabolomics. Curr. Opin. Chem. Biol. 36, 15-23. https://doi.org/10.1016/j. cbpa.2016.12.006.</p>
<p>Machine learning predicts the yeast metabolome from the quantitative proteome of kinase knockouts. A Zelezniak, J Vowinckel, F Capuano, C B Messner, V Demichev, N Polowsky, M Mülleder, S Kamrad, B Klaus, M A Keller, M Ralser, 10.1016/j.cels.2018.08.001Cell Syst. 7Zelezniak, A., Vowinckel, J., Capuano, F., Messner, C.B., Demichev, V., Polowsky, N., Mülleder, M., Kamrad, S., Klaus, B., Keller, M.A., Ralser, M., 2018. Machine learning predicts the yeast metabolome from the quantitative proteome of kinase knockouts. Cell Syst 7, 269-283.e6. https://doi.org/10.1016/j.cels.2018.08.001.</p>
<p>Combining mechanistic and machine learning models for predictive engineering and optimization of tryptophan metabolism. J Zhang, S D Petersen, T Radivojevic, A Ramirez, A Pérez-Manríquez, E Abeliuk, B J Sánchez, Z Costello, Y Chen, M J Fero, H G Martin, J Nielsen, J D Keasling, M K Jensen, 10.1038/s41467-020-17910-1Nat. Commun. 114880Zhang, J., Petersen, S.D., Radivojevic, T., Ramirez, A., Pérez-Manríquez, A., Abeliuk, E., Sánchez, B.J., Costello, Z., Chen, Y., Fero, M.J., Martin, H.G., Nielsen, J., Keasling, J. D., Jensen, M.K., 2020. Combining mechanistic and machine learning models for predictive engineering and optimization of tryptophan metabolism. Nat. Commun. 11, 4880. https://doi.org/10.1038/s41467-020-17910-1.</p>
<p>MiYA, an efficient machine-learning workflow in conjunction with the YeastFab assembly strategy for combinatorial optimization of heterologous metabolic pathways in Saccharomyces cerevisiae. Y Zhou, G Li, J Dong, X.-H Xing, J Dai, C Zhang, 10.1016/j.ymben.2018.03.020Metab. Eng. 47Zhou, Y., Li, G., Dong, J., Xing, X.-H., Dai, J., Zhang, C., 2018. MiYA, an efficient machine-learning workflow in conjunction with the YeastFab assembly strategy for combinatorial optimization of heterologous metabolic pathways in Saccharomyces cerevisiae. Metab. Eng. 47, 294-302. https://doi.org/10.1016/j. ymben.2018.03.020.</p>
<p>A comprehensive survey on transfer learning. F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, H Zhu, H Xiong, Q He, Proc. IEEE. IEEEZhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., He, Q., 2020. A comprehensive survey on transfer learning. Proc. IEEE.</p>            </div>
        </div>

    </div>
</body>
</html>
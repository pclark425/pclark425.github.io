<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6591 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6591</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6591</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-81881d2589a34df12d5e2fd192d5354dda1f81a8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/81881d2589a34df12d5e2fd192d5354dda1f81a8" target="_blank">Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents</a></p>
                <p><strong>Paper Venue:</strong> 2024 IEEE Conference on Games (CoG)</p>
                <p><strong>Paper TL;DR:</strong> NetPlay is presented, the first LLM powered zero-shot agent for the challenging roguelike NetHack and demonstrates the necessity for dynamic methods in supplying context information for complex games such as NetHack.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown great success as high-level planners for zero-shot game-playing agents However, these agents are primarily evaluated on games where long-term planning is relatively straightforward. In contrast agents tested in more dynamic environments face limitations due to simplistic environments with only a few objects and interactions. To fill this gap in the literature, we present NetPlay, the first LLM powered zero-shot agent for the challenging roguelike NetHack. NetHack is a particularly challenging environment due to its diverse set of items and monsters, complex interactions, and many ways to die. NetPlay uses an architecture designed for dynamic robot environments, modified for NetHack. Like previous approaches, it prompts the LLM to choose from predefined skills and tracks past interactions to enhance decision-making. Given NetHack’s unpredictable nature, NetPlay detects important game events to interrupt running skills, enabling it to react to unforeseen circumstances. While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack’s mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback. Our findings demonstrate that NetPlay performs best with detailed context information, indicating the necessity for dynamic methods in supplying context information for complex games such as NetHack.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6591.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6591.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NetPlay</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NetPlay (LLM-powered NetHack agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-4–powered zero-shot NetHack agent that prompts an LLM to choose from predefined skills and maintains a short-term, chat-style memory of recent messages (system/AI/human) to guide sequential skill selection and re-planning in response to detected in-game events.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NetPlay</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Closed-loop architecture using GPT-4-Turbo to select parametrized skills from a predefined skill set. Interaction history (LLM thoughts, chosen skills, and system/human messages) is stored as a short-term message list and included in subsequent prompts; a separate data tracker records events and enriches observations and can interrupt running skills to trigger re-prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term chat-like message buffer (episodic buffer included in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text messages (categorized as system / AI / human) containing LLM thoughts, selected skills, and environment feedback (game messages, detected events); explicit observation descriptions are NOT stored in the memory due to size</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>memory messages are concatenated into the LLM prompt (token-limited); writing: LLM responses and system environment messages appended; older messages deleted when the memory exceeds a 500-token cap (FIFO eviction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetHack (full game runs and many small des-file scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>sequential decision-making / embodied planning / game-playing (complex dynamic environment)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Unguided NetPlay (with described short-term memory and tracker): Score = 284.85 ± 222.10; Depth = 2.60 ± 1.39; Level = 2.40 ± 1.23; Time = 1292.10 ± 942.74 (mean ± standard error) — (metrics from Table II for the unguided agent).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Score, Depth, Level, Time (mean ± standard error); success rates reported for small scenarios (see Table III) as counts / runs</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Memory is token-limited (500 tokens) causing older events to be dropped; observations are excluded from memory to save tokens which limits retention of spatial/large context; expanding memory or adding external knowledge (e.g., NetHack wiki) would increase LLM call cost and prompt length; more context helps performance but increases latency/cost and requires methods to find relevant context automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Memory helped with following specific instructions and sequential tasks but did not make NetPlay competitive with strong handcrafted symbolic agents for the ambiguous long-term task 'Win the Game'; failures arose from (1) limited memory capacity (older relevant events lost), (2) omission of large observation descriptions from memory, (3) lack of explicit feedback leading to repeated menu/navigation errors, (4) confusion loops (e.g., failing to attack passive monsters, repeatedly opening menus), and (5) the agent fixating on simple strategies and failing on ambiguous/long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow, Duygu Çakmak, James Kwan, "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents" (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6591.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6591.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner Monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied-agent approach that models interaction history as a chat containing the LLM's actions and thoughts, human feedback, and environment feedback; re-plans by conditioning on a substantial portion of recent interaction history to handle dynamic environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inner Monologue (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Models recent interaction history (actions, thoughts, environment feedback) as a chat-like transcript appended to prompts so the LLM can reason about prior steps when re-planning; designed for dynamic embodied tasks where single-plan execution is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>chat-like interaction history (short-term textual memory included in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text chat messages summarizing past actions, thoughts, and environment feedback</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>memory is appended to the LLM prompt (explicit inclusion of recent history) and used for re-planning; no vector retrieval or external datastore described in this paper's summary</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Robot/environment tasks (rearranging tabletop objects, kitchen tasks, simulated household activities) — referenced generally</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>embodied planning / dynamic decision-making / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes that using larger slices of recent history helps dynamic decision-making but implicitly increases prompt size and LLM token usage; referenced as an approach better suited for dynamic tasks than single-plan methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper beyond the general observation that dynamic environments require substantial recent history for robust re-planning and that observation complexity in robotics is lower than in NetHack.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>W. Huang et al., "Inner monologue: Embodied reasoning through planning with language models" (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6591.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6591.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DEPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Describe, Explain, Plan and Select (DEPS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-planning approach for embodied agents that re-plans by considering a substantial portion of recent interaction history rather than relying only on the last failed plan.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DEPS (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Interactive closed-loop planner where the LLM uses recent interaction history (descriptions/explanations/plans) to select next actions; emphasizes using more of the recent trace for re-planning in dynamic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recent interaction history / chat-like buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual summaries of recent interactions (actions, environment feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>history appended to prompt for LLM re-planning; no external retrieval mechanism described in this paper's summary</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied tasks / open-world multi-task agents (referenced generally; used in robot-like environments and open-world benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>embodied planning / open-world multi-tasking</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Using more of the recent history improves responsiveness in dynamic settings but increases prompt length and token use; implied increased compute/latency and need to manage what history to include.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not specified in detail in this paper; discussed as contrasting single-plan strategies which may fail in dynamic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents" (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6591.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6591.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jarvis-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as an example of agents that use memory augmentation to improve open-world multi-task performance, particularly in Minecraft-like settings where storage and reuse of successful plans improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Jarvis-1 (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory-augmented multimodal LLM agent that stores and reuses successful plans/knowledge to perform multi-task open-world activities; cited as part of prior work on Minecraft-style agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory-augmented store for plans/knowledge (likely long-term plan cache or textual memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored successful plans and text-based knowledge (not detailed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>reuse of stored successful plans when relevant tasks arise (described at a high level in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Minecraft open-world tasks (item acquisition, building, multi-step crafting) — referenced generally</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>open-world planning / long-horizon multi-step tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes that storage and reuse of successful plans significantly enhance performance but implies increased engineering for plan storage/retrieval and hierarchical plan management; specific trade-offs not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper; referenced to motivate that plan storage helps in hierarchical tasks but may be less applicable to highly unpredictable environments like NetHack without careful context selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Z. Wang et al., "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models" (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6591.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6591.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ghost / 'Ghost in the Minecraft'</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ghost in the Minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited Minecraft agent that leverages text-based knowledge and memory (e.g., storing and reusing successful plans) to improve multi-step item acquisition and open-world behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Ghost (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agent for open-world Minecraft tasks that uses memory or stored plans/knowledge to speed and improve hierarchical multi-step behaviors (storage and reuse of successful plans highlighted).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>text-based plan/knowledge memory (longer-term storage of successful plans)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual descriptions of successful plans, past actions and outcomes (not specified in detail in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>reuse of successful plans when indicated by task hierarchy; retrieval likely via prompt augmentation but not detailed here</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Minecraft item acquisition and open-world tasks (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>open-world multi-step planning / hierarchical tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Storage and reuse of successful plans improve performance for hierarchical tasks but require mechanisms to manage and retrieve relevant past plans; increased prompt/context and retrieval overhead implied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not specified in detail in this paper; authors contrast these successes with NetHack where environment unpredictability and observation complexity create different challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>X. Zhu et al., "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory" (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6591.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6591.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited example of an LLM agent that demonstrates emergent capabilities including constructing structures and leveraging stored knowledge/plans; noted as using memory-like mechanisms (storing and reusing successful plans) to improve performance in an open-ended environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An open-ended embodied LLM agent that stores successful plans/experiences to bootstrap future behavior and to enable complex, multi-step building and interaction behaviors in Minecraft-like environments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>stored plans / persistent textual memory of successful behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual records of successful plans and behaviors (not detailed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval and reuse of stored plans to guide future decision-making (high-level description in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-ended Minecraft tasks (building, multi-step objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>open-ended embodied planning / lifelong learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Storing and reusing plans improves long-horizon task success but incurs overhead in storing/searching relevant examples and increases prompt/context complexity; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed here; paper uses Voyager as a positive example of plan storage benefits in less unpredictable environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>G. Wang et al., "Voyager: An open-ended embodied agent with large language models" (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6591.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6591.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Motif</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Motif: Intrinsic motivation from artificial intelligence feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as an LLM-based method that uses the language model to learn reward functions by preferring specific in-game messages, applied to NetHack to capture playstyles; not primarily described as using an explicit memory store but uses message preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Motif: Intrinsic motivation from artificial intelligence feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Motif (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an LLM to assign preferences to game messages to construct reward functions capturing playstyles; leverages LLM judgments over recent messages but is not described here as employing an external memory store for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetHack playstyle preference learning / reward design</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>reward learning / preference modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper suggests LLM-based preference signals can be used to shape rewards, but the current paper notes Motif as separate work and does not detail memory trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper; cited to show other uses of LLMs in NetHack (reward design) rather than memory-centric control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>M. Klissarov et al., "Motif: Intrinsic motivation from artificial intelligence feedback" (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents <em>(Rating: 2)</em></li>
                <li>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models <em>(Rating: 2)</em></li>
                <li>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Motif: Intrinsic motivation from artificial intelligence feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6591",
    "paper_id": "paper-81881d2589a34df12d5e2fd192d5354dda1f81a8",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "NetPlay",
            "name_full": "NetPlay (LLM-powered NetHack agent)",
            "brief_description": "A GPT-4–powered zero-shot NetHack agent that prompts an LLM to choose from predefined skills and maintains a short-term, chat-style memory of recent messages (system/AI/human) to guide sequential skill selection and re-planning in response to detected in-game events.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NetPlay",
            "agent_description": "Closed-loop architecture using GPT-4-Turbo to select parametrized skills from a predefined skill set. Interaction history (LLM thoughts, chosen skills, and system/human messages) is stored as a short-term message list and included in subsequent prompts; a separate data tracker records events and enriches observations and can interrupt running skills to trigger re-prompting.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "short-term chat-like message buffer (episodic buffer included in prompt)",
            "memory_representation": "raw text messages (categorized as system / AI / human) containing LLM thoughts, selected skills, and environment feedback (game messages, detected events); explicit observation descriptions are NOT stored in the memory due to size",
            "memory_access_mechanism": "memory messages are concatenated into the LLM prompt (token-limited); writing: LLM responses and system environment messages appended; older messages deleted when the memory exceeds a 500-token cap (FIFO eviction)",
            "task_name": "NetHack (full game runs and many small des-file scenarios)",
            "task_category": "sequential decision-making / embodied planning / game-playing (complex dynamic environment)",
            "performance_with_memory": "Unguided NetPlay (with described short-term memory and tracker): Score = 284.85 ± 222.10; Depth = 2.60 ± 1.39; Level = 2.40 ± 1.23; Time = 1292.10 ± 942.74 (mean ± standard error) — (metrics from Table II for the unguided agent).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Score, Depth, Level, Time (mean ± standard error); success rates reported for small scenarios (see Table III) as counts / runs",
            "tradeoffs_reported": "Memory is token-limited (500 tokens) causing older events to be dropped; observations are excluded from memory to save tokens which limits retention of spatial/large context; expanding memory or adding external knowledge (e.g., NetHack wiki) would increase LLM call cost and prompt length; more context helps performance but increases latency/cost and requires methods to find relevant context automatically.",
            "limitations_or_failure_cases": "Memory helped with following specific instructions and sequential tasks but did not make NetPlay competitive with strong handcrafted symbolic agents for the ambiguous long-term task 'Win the Game'; failures arose from (1) limited memory capacity (older relevant events lost), (2) omission of large observation descriptions from memory, (3) lack of explicit feedback leading to repeated menu/navigation errors, (4) confusion loops (e.g., failing to attack passive monsters, repeatedly opening menus), and (5) the agent fixating on simple strategies and failing on ambiguous/long-horizon planning.",
            "citation": "Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow, Duygu Çakmak, James Kwan, \"Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents\" (this paper)",
            "uuid": "e6591.0",
            "source_info": {
                "paper_title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Inner Monologue",
            "name_full": "Inner Monologue: Embodied reasoning through planning with language models",
            "brief_description": "An embodied-agent approach that models interaction history as a chat containing the LLM's actions and thoughts, human feedback, and environment feedback; re-plans by conditioning on a substantial portion of recent interaction history to handle dynamic environments.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models",
            "mention_or_use": "mention",
            "agent_name": "Inner Monologue (as cited)",
            "agent_description": "Models recent interaction history (actions, thoughts, environment feedback) as a chat-like transcript appended to prompts so the LLM can reason about prior steps when re-planning; designed for dynamic embodied tasks where single-plan execution is insufficient.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "chat-like interaction history (short-term textual memory included in prompt)",
            "memory_representation": "raw text chat messages summarizing past actions, thoughts, and environment feedback",
            "memory_access_mechanism": "memory is appended to the LLM prompt (explicit inclusion of recent history) and used for re-planning; no vector retrieval or external datastore described in this paper's summary",
            "task_name": "Robot/environment tasks (rearranging tabletop objects, kitchen tasks, simulated household activities) — referenced generally",
            "task_category": "embodied planning / dynamic decision-making / robotics",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper notes that using larger slices of recent history helps dynamic decision-making but implicitly increases prompt size and LLM token usage; referenced as an approach better suited for dynamic tasks than single-plan methods.",
            "limitations_or_failure_cases": "Not detailed in this paper beyond the general observation that dynamic environments require substantial recent history for robust re-planning and that observation complexity in robotics is lower than in NetHack.",
            "citation": "W. Huang et al., \"Inner monologue: Embodied reasoning through planning with language models\" (cited in the paper)",
            "uuid": "e6591.1",
            "source_info": {
                "paper_title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "DEPS",
            "name_full": "Describe, Explain, Plan and Select (DEPS)",
            "brief_description": "An LLM-planning approach for embodied agents that re-plans by considering a substantial portion of recent interaction history rather than relying only on the last failed plan.",
            "citation_title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
            "mention_or_use": "mention",
            "agent_name": "DEPS (as cited)",
            "agent_description": "Interactive closed-loop planner where the LLM uses recent interaction history (descriptions/explanations/plans) to select next actions; emphasizes using more of the recent trace for re-planning in dynamic tasks.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "recent interaction history / chat-like buffer",
            "memory_representation": "textual summaries of recent interactions (actions, environment feedback)",
            "memory_access_mechanism": "history appended to prompt for LLM re-planning; no external retrieval mechanism described in this paper's summary",
            "task_name": "Embodied tasks / open-world multi-task agents (referenced generally; used in robot-like environments and open-world benchmarks)",
            "task_category": "embodied planning / open-world multi-tasking",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Using more of the recent history improves responsiveness in dynamic settings but increases prompt length and token use; implied increased compute/latency and need to manage what history to include.",
            "limitations_or_failure_cases": "Not specified in detail in this paper; discussed as contrasting single-plan strategies which may fail in dynamic tasks.",
            "citation": "Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, \"Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents\" (cited in the paper)",
            "uuid": "e6591.2",
            "source_info": {
                "paper_title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Jarvis-1",
            "name_full": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models",
            "brief_description": "Referenced as an example of agents that use memory augmentation to improve open-world multi-task performance, particularly in Minecraft-like settings where storage and reuse of successful plans improves results.",
            "citation_title": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models",
            "mention_or_use": "mention",
            "agent_name": "Jarvis-1 (as cited)",
            "agent_description": "Memory-augmented multimodal LLM agent that stores and reuses successful plans/knowledge to perform multi-task open-world activities; cited as part of prior work on Minecraft-style agents.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "memory-augmented store for plans/knowledge (likely long-term plan cache or textual memory)",
            "memory_representation": "stored successful plans and text-based knowledge (not detailed in this paper)",
            "memory_access_mechanism": "reuse of stored successful plans when relevant tasks arise (described at a high level in related work)",
            "task_name": "Minecraft open-world tasks (item acquisition, building, multi-step crafting) — referenced generally",
            "task_category": "open-world planning / long-horizon multi-step tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper notes that storage and reuse of successful plans significantly enhance performance but implies increased engineering for plan storage/retrieval and hierarchical plan management; specific trade-offs not detailed in this paper.",
            "limitations_or_failure_cases": "Not detailed in this paper; referenced to motivate that plan storage helps in hierarchical tasks but may be less applicable to highly unpredictable environments like NetHack without careful context selection.",
            "citation": "Z. Wang et al., \"Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models\" (cited in the paper)",
            "uuid": "e6591.3",
            "source_info": {
                "paper_title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Ghost / 'Ghost in the Minecraft'",
            "name_full": "Ghost in the Minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "brief_description": "A cited Minecraft agent that leverages text-based knowledge and memory (e.g., storing and reusing successful plans) to improve multi-step item acquisition and open-world behavior.",
            "citation_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "mention_or_use": "mention",
            "agent_name": "Ghost (as cited)",
            "agent_description": "LLM-based agent for open-world Minecraft tasks that uses memory or stored plans/knowledge to speed and improve hierarchical multi-step behaviors (storage and reuse of successful plans highlighted).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "text-based plan/knowledge memory (longer-term storage of successful plans)",
            "memory_representation": "textual descriptions of successful plans, past actions and outcomes (not specified in detail in this paper)",
            "memory_access_mechanism": "reuse of successful plans when indicated by task hierarchy; retrieval likely via prompt augmentation but not detailed here",
            "task_name": "Minecraft item acquisition and open-world tasks (referenced)",
            "task_category": "open-world multi-step planning / hierarchical tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Storage and reuse of successful plans improve performance for hierarchical tasks but require mechanisms to manage and retrieve relevant past plans; increased prompt/context and retrieval overhead implied.",
            "limitations_or_failure_cases": "Not specified in detail in this paper; authors contrast these successes with NetHack where environment unpredictability and observation complexity create different challenges.",
            "citation": "X. Zhu et al., \"Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory\" (cited in the paper)",
            "uuid": "e6591.4",
            "source_info": {
                "paper_title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "Cited example of an LLM agent that demonstrates emergent capabilities including constructing structures and leveraging stored knowledge/plans; noted as using memory-like mechanisms (storing and reusing successful plans) to improve performance in an open-ended environment.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager (as cited)",
            "agent_description": "An open-ended embodied LLM agent that stores successful plans/experiences to bootstrap future behavior and to enable complex, multi-step building and interaction behaviors in Minecraft-like environments.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "stored plans / persistent textual memory of successful behaviors",
            "memory_representation": "textual records of successful plans and behaviors (not detailed in this paper)",
            "memory_access_mechanism": "retrieval and reuse of stored plans to guide future decision-making (high-level description in related work)",
            "task_name": "Open-ended Minecraft tasks (building, multi-step objectives)",
            "task_category": "open-ended embodied planning / lifelong learning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Storing and reusing plans improves long-horizon task success but incurs overhead in storing/searching relevant examples and increases prompt/context complexity; not quantified in this paper.",
            "limitations_or_failure_cases": "Not detailed here; paper uses Voyager as a positive example of plan storage benefits in less unpredictable environments.",
            "citation": "G. Wang et al., \"Voyager: An open-ended embodied agent with large language models\" (cited in the paper)",
            "uuid": "e6591.5",
            "source_info": {
                "paper_title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Motif",
            "name_full": "Motif: Intrinsic motivation from artificial intelligence feedback",
            "brief_description": "Referenced as an LLM-based method that uses the language model to learn reward functions by preferring specific in-game messages, applied to NetHack to capture playstyles; not primarily described as using an explicit memory store but uses message preferences.",
            "citation_title": "Motif: Intrinsic motivation from artificial intelligence feedback",
            "mention_or_use": "mention",
            "agent_name": "Motif (as cited)",
            "agent_description": "Uses an LLM to assign preferences to game messages to construct reward functions capturing playstyles; leverages LLM judgments over recent messages but is not described here as employing an external memory store for retrieval.",
            "model_size": null,
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "NetHack playstyle preference learning / reward design",
            "task_category": "reward learning / preference modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper suggests LLM-based preference signals can be used to shape rewards, but the current paper notes Motif as separate work and does not detail memory trade-offs.",
            "limitations_or_failure_cases": "Not detailed in this paper; cited to show other uses of LLMs in NetHack (reward design) rather than memory-centric control.",
            "citation": "M. Klissarov et al., \"Motif: Intrinsic motivation from artificial intelligence feedback\" (cited in the paper)",
            "uuid": "e6591.6",
            "source_info": {
                "paper_title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
            "rating": 2
        },
        {
            "paper_title": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models",
            "rating": 2
        },
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Motif: Intrinsic motivation from artificial intelligence feedback",
            "rating": 1
        }
    ],
    "cost": 0.014090249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Playing NetHack with LLMs: Potential \&amp; Limitations as Zero-Shot Agents</h1>
<p>Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow<br>Queen Mary University of London<br>{d.jeurissen, diego.perez, jeremy.gow}@qmul.ac.uk</p>
<p>Duygu Çakmak, James Kwan<br>Creative Assembly<br>{duygu.cakmak, james.kwan}@creative-assembly.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have shown great success as high-level planners for zero-shot game-playing agents. However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions. To fill this gap in the literature, we present NetPlay, the first LLMpowered zero-shot agent for the challenging roguelike NetHack. NetHack is a particularly challenging environment due to its diverse set of items and monsters, complex interactions, and many ways to die.</p>
<p>NetPlay uses an architecture designed for dynamic robot environments, modified for NetHack. Like previous approaches, it prompts the LLM to choose from predefined skills and tracks past interactions to enhance decision-making. Given NetHack's unpredictable nature, NetPlay detects important game events to interrupt running skills, enabling it to react to unforeseen circumstances. While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack's mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback. Our findings demonstrate that NetPlay performs best with detailed context information, indicating the necessity for dynamic methods in supplying context information for complex games such as NetHack.</p>
<p>Index Terms-NetHack, Large Language Models, Zero-Shot Agent.</p>
<h2>I. INTRODUCTION</h2>
<p>Recently, agents based on Large Language Models (LLMs) [1] have been successfully applied to robot environments [2], [3] and Minecraft [4]-[6], among others. These agents do not require pre-training and typically involve prompting an LLM to solve tasks by choosing from predefined skills. They have proven effective for tasks demanding extensive knowledge, like crafting a diamond pickaxe in Minecraft. Additionally, they can understand a wide range of task descriptions.</p>
<p>LLM agents utilizing predefined skills are particularly promising for game development as developing a set of simple skills is often more feasible than designing an entire agent. However, existing studies predominantly focus on the capabilities of LLMs for game-playing, neglecting to address their limitations. Evaluations typically focus on predictable tasks, for example, finding a diamond in Minecraft, which can consistently be achieved through strip mining. Many games require more dynamic decision-making, where longterm planning is challenging, and the correct course of action is more ambiguous. While evaluations have been done on more
dynamic robot environments, these environments often contain only a handful of objects and lack complex interactions.</p>
<p>We build upon existing literature by evaluating an LLM agent in the context of the complex and unpredictable roguelike NetHack [7]. NetHack is a challenging game with many monsters, items, interactions, partial observability, and an intricate goal condition. The sheer size of NetHack, paired with the many sub-systems the player has to understand, make it an excellent candidate for evaluating the limitations of LLM agents. NetHack's description files also allow us to define levels, enabling us to evaluate the agent's abilities in isolation.</p>
<p>In the following, we present (NetPlay), a GPT-4 powered agent designed to tackle a wide range of tasks in NetHack. NetPlay is inspired by autoascend [8] a handcrafted agent that won the NetHack Challenge 2021 [9]. While autoascend relied on a large network of handcrafted rules to handle the complexity of NetHack, NetPlay only requires a set of isolated skills. Our experiments show that NetPlay can interact with most of NetHack's game mechanics and that it excels in following detailed instructions. Additionally, the agent exhibits creative behavior when focusing its attention on a specific problem. However, when tasked to play autonomously, NetPlay is far outperformed by autoascend. Consequently, this paper delves into reasons for this, such as the agent's struggles to handle ambiguous instructions, confusing observations, and a lack of explicit feedback.</p>
<p>We begin in section II with an overview of NetHack and a review of existing work on LLM-powered agents. Section III discusses the architecture of NetPlay, including many of the design decisions we had to make due to limitations caused by the LLM. In section IV, we first evaluate NetPlay's ability to autonomously play the game and compare its performance with a simple handcrafted agent and autoascend. We follow this up with an in-depth analysis of the agent's behavior across various isolated scenarios. Subsequently, we analyze the experiment results in section V and conclude this study in section VI. The source code can be found on GitHub ${ }^{1}$.</p>
<h2>II. BACKGROUND</h2>
<h2>A. NetHack</h2>
<p>NetHack [7], released in 1987, is an extremely challenging turn-based roguelike that continues to receive updates to this</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: The terminal view of the game NetHack. The left image presents an annotated view of the in-game screen, featuring the game's map, an example of a game message, and the agent's stats. The right image showcases a menu for picking up items from a tile containing multiple objects. Image source: alt.org/nethack</p>
<p>date. The objective is to traverse 50 procedurally generated levels, retrieving the Amulet of Yendor and successfully returning to the surface. Doing so unlocks the final challenge of the game: the four elemental planes, followed by the astral plane, where players must present the Amulet to their deity. See fig. 1 for a screenshot of the game.</p>
<p>Most aspects of the game are generated, such as level layouts, the player's starting class, and the inventory. The levels follow a somewhat linear structure with many branches and sub-dungeons in between. For instance, the entrance to the gnomish mines always spawns somewhere between depth 2 and 4, giving the player the option to explore them immediately or to postpone exploration until they are stronger.</p>
<p>NetHack encompasses a diverse array of monsters, items, and interactions. Players must skillfully utilize their resources while avoiding many of the game's lethal threats. Even for seasoned players possessing extensive knowledge of the game, victory is far from guaranteed. The game's inherent complexity requires players to continuously re-assess their situation to adapt to the unpredictability of the elements at play.</p>
<p>Nethack uses description files (<strong>des-files</strong>) to describe special levels like the oracle level that always contains a room with an oracle monster, centaur statues, and four fountains. Des-files offer extensive control over the level-generation process, allowing entirely handcrafted levels or a slightly constrained level-generation process.</p>
<h3><em>B. NetHack Learning Environment</em></h3>
<p>The NetHack Learning Environment <strong>NLE</strong> [10] serves as a reinforcement learning environment for playing NetHack 3.6.6. NLE offers easy access to most aspects of the game, such as the map, the agent's inventory, game messages, and the player's stats. While NLE provides simplified environments for learning purposes, it also allows users to play the entire game without any restrictions.</p>
<p>MiniHack [11] utilizes NLE alongside des-files to construct small-scale environments that isolate specific challenges that agents will encounter in NetHack. Although MiniHack provides a list of challenges, its primary purpose is to streamline the process of designing new challenges.</p>
<h3><em>C. autoascend</em></h3>
<p>In the 2021 NeurIPS NetHack Challenge [9], participants tackled the symbolic and neural tracks, where solutions were either handcrafted or designed using machine learning. Notably, the top-performing agents were exclusively symbolic, with the autoascend agent emerging as the frontrunner [12].</p>
<p>The autoascend agent [13] succeeded by meticulously parsing observations and creating an internal state representation to track essential information. The agent utilized the enriched data to implement a behavior tree by hierarchically combining strategies representing specific behaviors, like fighting, picking up objects, or exploring levels. Overall, autoascend's strategy consists of staying on the first dungeon level until reaching experience level 8, after which it will rapidly progress deeper into the dungeon. While following this general strategy, autoascend uses many sub-strategies to improve its chance of success, such as a solver for solving the Sokoban levels, using altars for farming or identifying items, or dipping a long sword into a fountain to gain Excalibur.</p>
<p>Despite its victory, autoascend's performance depended heavily on its starting class, demonstrating optimal results with the Valkyrie class. The agent occasionally descended to depth 10 and reached experience level 10. However, it is crucial to highlight that reaching around depth 50 is only one of the</p>
<p>objectives to beat NetHack, emphasizing how challenging the environment still is.</p>
<h2>D. LLM Agents</h2>
<p>Recently, a plethora of LLM-based agents have emerged, aiming to leverage the planning capabilities of these models. A prominent testbed for these agents is Minecraft [4]-[6], primarily focusing on the agent’s ability to obtain the various items in the game. While the details vary, most approaches implement a closed-loop planning system in which the LLM generates a plan consisting of a sequence of predefined skills. The plan is then executed and, in case of failure, the agent will re-plan using only feedback from the previous plan. A noteworthy aspect of these agents is the storage and reuse of successful plans, significantly enhancing overall performance due to the hierarchical nature of obtaining items like a diamond pickaxe. The agents primarily utilize an LLM for their knowledge of how to acquire items. However, one agent has demonstrated the ability to construct structures with human feedback [6].</p>
<p>Other popular applications are robot environments, where tasks include rearranging objects on a tabletop, interacting within a kitchen, or engaging in simulated household activities [2], [3], [14], [15]. Because these environments require more dynamic decision-making compared to acquiring items in Minecraft, agents like DEPS [14] and Inner Monologue [2] adopt a distinctive approach. Instead of relying solely on feedback from the last failed plan, they re-plan by considering a substantial portion of their recent interaction history. Similar to our approach, Inner Monologue models the interaction history as a chat containing the LLM’s actions and thoughts, human feedback, and feedback from the environment, such as scene descriptions and if an action was successful. While the robot environments require more dynamic decision-making, the complexity of the observations is limited, usually consisting of a list of visible objects with spatial information being omitted as the low-level skills are handling it.</p>
<p>An alternative use of LLMs involves employing them to design reward functions, which are then used to train reinforcement learning agents [16]-[18]. Most relevant to our work, Motif employs an LLM to learn various playstyles in NetHack. It achieves this by tasking the LLM to decide which of NetHack’s game messages it prefers. Motif can leverage these preferences to learn reward functions for different playstyles by conditioning the LLM to prefer game messages associated with a specific playstyle, such as fighting monsters.</p>
<h2>III. NetPlay</h2>
<p>This section discusses our LLM-powered Nethack agent NetPlay. See fig. 2 for an overview of the architecture. Long-term planning in NetHack proves challenging due to its unpredictability, as we cannot know when, where, or what will appear as we explore. Consequently, our agent shares many similarities with Inner Monologue, which is designed for dynamic environments. It implements a closed-loop system where the LLM selects skills sequentially while accumulating feedback in the form of game messages, errors, or manually detected events. Although we avoid constructing entire plans, the LLM’s thoughts are included for future prompts, allowing for strategic planning if deemed necessary by the LLM.</p>
<h2>A. Prompting</h2>
<p>We prompt the LLM to choose a skill from a predefined list. The prompt comprises three components: (a) the agent’s short-term memory, (b) a description of the observation, and (c) a task description alongside the output format.
(a) The observation description primarily focuses on the current level alongside additional data like context, inventory, and the agent’s stats. Because we do not use a multi-modal LLM, we attempt to convey spatial information by dividing the level into structures like rooms and corridors. Each structure is described using a unique identifier, the number of steps to reach it, the objects it contains with their respective positions, and the number of steps to reach each object. Monsters are described separately from the structures by categorizing them as close or distant, indicating their potential threat level. Each monster is described using its name, position, and number of steps to reach it. For close monsters, we also include compass coordinates. The LLM is also informed about which structures can be further explored alongside the positions of boulders and doors that block exploration progress. Note that despite our emphasis on providing spatial information, navigating the environment proved challenging for the LLM. Consequently, we automated a large portion of the exploration process using a single skill, potentially rendering certain aspects of this observation description obsolete.
(b) The short-term memory is implemented using a list of messages representing the timeline of events. Each message is either categorized as system, AI, or human. System messages convey feedback from the environment like game messages or errors, AI messages capture the LLM’s responses, and new tasks are indicated by human messages. Note that while it is possible for a human to provide continual feedback, we only study the case where the agent is given a task at the start of the game. The memory size is capped at 500 tokens, with older messages being deleted first. Observation descriptions are not stored in the memory due to their size.
(c) The task description includes details about the current task, available skills, and a JSON output format. We employ chain-of-thought prompting [19] to guide the LLM to a skill choice.</p>
<h2>B. Skills</h2>
<p>Skills, similar to strategies in autoascend, implement specific behaviors by returning a sequence of actions. They accept both mandatory and optional parameters as input. Skills can generate messages as feedback, which are stored in the agent’s memory. Messages are often used, for example, to report why a skill failed. An excerpt of skills can be found in table I.</p>
<p>Navigation is automated through skills like “move_to x y” or “go_to room_id”. However, exploring levels with only these</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Illustration of NetPlay playing NetHack. The process involves constructing a prompt using messages representing past events, the current observation, and a task description containing available skills and the desired output format. The response is parsed to retrieve the next skill. While executing the selected skill, a tracker enriches the given observations and detects important events, such as when a new monster appears. When the skill is done, or events interrupt the skill execution, the agent will restart the prompting process.
skills proved challenging for the LLM. To address this, we introduced the "explore_level" skill, which uses the exploration strategy from autoascend. This skill explores the current level by uncovering tiles, opening doors, and searching for hidden corridors. We removed the ability to kick open doors to avoid potential issues such as aggravating shopkeepers. Note that the agent can still decide to kick open doors using a separate "kick" skill. All movement-related skills will attack monsters that are in the way. The LLM can turn off this behavior using the "set_avoid_monster_flag" skill.</p>
<p>To indicate when the agent is done with a given task, it has access to the "finish_task" skill. Additionally, the LLM is equipped with the "press_key" and "type_text" skills for navigating NetHack's various game menus. While a menu is open, only the "finish_task" and text input skills remain available.</p>
<p>The remaining skills are thin wrappers around NetHack commands, such as drink or pickup. However, these commands
often involve multiple steps, such as confirming which item to drink or first positioning the agent correctly to then pick up an item. Consequently, the LLM often assumed that the "drink" command accepts an item parameter or that "pickup" works seamlessly regardless of the agent's current position. To mitigate these issues, we implemented four types of command skills. Base commands only invoke the command. Position commands offer the option to first move to the desired location. Inventory commands accept an item parameter to resolve the following popup menu. Finally, direction commands like "kick" move the agent close to a desired position before executing the command in the correct direction.</p>
<h2>C. Agent Loop</h2>
<p>Upon receiving a new task, the agent prompts the LLM to select the first skill to execute. The LLM's thoughts and the selected skill are stored in the agent's memory as feedback. While executing the chosen skill, a data tracker observes and</p>
<p>TABLE I: Skill Examples: Skills represent parametrizable behaviors that the LLM uses to play the game. The name, parameters, and descriptions help to understand what each skill does. For some skills, the LLM can omit optional parameters marked in [square brackets]. Note that the skill type is only used internally and does not matter for the final agent.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Name</th>
<th>Parameters</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Special</td>
<td>explore_level</td>
<td></td>
<td>Explores the level to find new rooms, as well as hidden doors and corridors.</td>
</tr>
<tr>
<td>Special</td>
<td>set_avoid_monster_flag</td>
<td>value: bool</td>
<td>If set to true skills will try to avoid monsters.</td>
</tr>
<tr>
<td>Special</td>
<td>press_key</td>
<td>key: string</td>
<td>Presses the given letter. For special keys only ESC, SPACE, and ENTER are supported.</td>
</tr>
<tr>
<td>Position</td>
<td>pickup</td>
<td>[x: int, y: int]</td>
<td>Pickup things at your location or specify where you want to pickup an item.</td>
</tr>
<tr>
<td>Position</td>
<td>up</td>
<td>[x: int, y: int]</td>
<td>Go up a staircase at your location or specify the position of the staircase you want to use.</td>
</tr>
<tr>
<td>Inventory</td>
<td>drop</td>
<td>item_letter: string</td>
<td>Drop an item.</td>
</tr>
<tr>
<td>Inventory</td>
<td>wield</td>
<td>item_letter: string</td>
<td>Wield a weapon.</td>
</tr>
<tr>
<td>Direction</td>
<td>kick</td>
<td>x: int, y: int</td>
<td>Kick something.</td>
</tr>
<tr>
<td>Basic</td>
<td>cast</td>
<td></td>
<td>Opens your spellbook to cast a spell.</td>
</tr>
<tr>
<td>Basic</td>
<td>pay</td>
<td></td>
<td>Pay your shopping bill.</td>
</tr>
</tbody>
</table>
<p>records details such as found structures, features hidden by monsters or items, which tiles the agent has already seen or searched, and events. The information collected by the data tracker is used by skills to make decisions.</p>
<p>The data tracker also looks for specific events in the game to provide additional feedback to the LLM. Events include new in-game messages, newly discovered structures, level changes or teleports, stat changes, low health, and the discovery of new monsters, items, and some map features such as fountains or altars.</p>
<p>A skill continues to run until completion or interruption. Skills are interrupted when specific events occur, such as changing the level, teleporting, discovering new objects, and reaching low health. In addition to events, many skills are interrupted when a menu shows up due to their inability to handle them. Regardless of why a skill stopped, the agent then prompts the LLM to select the next skill. The sole exception is when the "finish_task" skill is selected, or the game has ended, at which point the agent will stop until it receives a new task.</p>
<h2>D. Handcrafted Agent</h2>
<p>To assess the impact of the LLM in contrast to the predefined skills, we implemented a handcrafted agent that aims to replicate the behavior of NetPlay with the task set to "Win the Game". The following list shows a breakdown of the agent's decision-making process.</p>
<ol>
<li>Abort any open menu, as we did not implement a way to navigate them.</li>
<li>If there are hostile monsters nearby, fight them.</li>
<li>If health is below $60 \%$, try healing with potions or by praying.</li>
<li>Eat food from the inventory when hungry.</li>
<li>Pick up items, which in this case are potions and food.</li>
<li>If nothing to explore, move to the next level if possible.</li>
<li>If nothing else to do, explore the level and try kicking open doors.
All the conditions are evaluated in sequence. Once a condition is met, a corresponding skill is executed. The selected skill will be interrupted in the same way as NetPlay. Once a skill is interrupted, the agent will choose the next skill by again checking all conditions in order starting from the first.</li>
</ol>
<p>Note that although we aimed to imitate NetPlay's behavior, the provided rules are too simplistic to capture all the nuances.</p>
<h2>IV. EXPERIMENTS</h2>
<p>Our goals for the experiments were two-fold. First, to evaluate the ability of NetPlay to play NetHack. Second, to provide an analysis of the agent's strengths and weaknesses, focusing on identifying which aspects are influenced by the LLM.</p>
<h2>A. Setup</h2>
<p>All of our experiments used OpenAI's GPT-4-Turbo (GPT-4-1106-PREVIEW) API as LLM with the temperature set to 0 and the response format set to JSON. Other models were not considered as initial tests revealed that models like GPT-3.5 and a 70B parameter instruct version of LLAMA 2 [20] could not correctly utilize our skills. The agent's memory size was set to 500 tokens.</p>
<p>The agent had access to most commands that interact with the game directly, except for some rarely relevant commands, like turning undead or using a monster's special ability. All control and system commands, like opening the help menu or hiding icons on the map, were excluded. We also implemented a time limit of 10 LLM calls, at which point the experiment would terminate if the in-game time did not advance.</p>
<h2>B. Full Runs</h2>
<p>We started evaluating NetPlay by letting it play NetHack without any constraints, tasking it to win the game. We will refer to this agent as the "unguided agent." Although the task was to play the entire game, the agent occasionally confused its own objectives with the assigned task, resulting in the agent marking the task as done too early. To address this issue, we disabled the "finish_task" skill for this experiment.</p>
<p>Due to budget limitations, we evaluated all agents using only the Valkyrie role, as most agents performed best with this class during the NetHack 2021 challenge. We conducted 20 runs with the unguided agent. Additionally, we performed 100 runs each with autoascend and the handcrafted agent for comparison. After evaluating the unguided agent, we carried out an additional 10 runs employing a "guided agent" who was informed on how to play better. A detailed description of</p>
<p>TABLE II: Results summary of the mean and standard error for the agents achieved score, depth, experience level, and game time.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>NetPlay (Unguided)</th>
<th>NetPlay (Guided)</th>
<th>autoascend</th>
<th>handcrafted</th>
</tr>
</thead>
<tbody>
<tr>
<td>Score</td>
<td>$284.85 \pm 222.10$</td>
<td>$405.00 \pm 216.38$</td>
<td>$\mathbf{1 1 3 4 1 . 9 4} \pm \mathbf{1 1 6 2 5 . 3 9}$</td>
<td>$250.24 \pm 159.17$</td>
</tr>
<tr>
<td>Depth</td>
<td>$2.60 \pm 1.39$</td>
<td>$2.00 \pm 1.05$</td>
<td>$\mathbf{4 . 0 1} \pm \mathbf{3 . 0 4}$</td>
<td>$2.35 \pm 0.93$</td>
</tr>
<tr>
<td>Level</td>
<td>$2.40 \pm 1.23$</td>
<td>$3.30 \pm 0.95$</td>
<td>$\mathbf{3 . 3 4} \pm \mathbf{7 . 6 9}$</td>
<td>$2.39 \pm 1.05$</td>
</tr>
<tr>
<td>Time</td>
<td>$1292.10 \pm 942.74$</td>
<td>$2627.40 \pm 1545.12$</td>
<td>$\mathbf{2 1 1 6 9 . 8 1} \pm \mathbf{9 1 5 5 . 5 9}$</td>
<td>$1306 \pm 924.17$</td>
</tr>
</tbody>
</table>
<p>the guided agent will be provided below. For now, a summary of the results can be found in table II.</p>
<p>Table II shows that autoascend far outperforms both NetPlay and the handcrafted agent. While NetPlay managed to beat the handcrafted agent by a small margin, it is likely that with a few tweaks, the handcrafted agent can also outperform NetPlay.</p>
<p>The unguided agent primarily failed due to timeouts, followed by deaths caused by eating rotten corpses, fighting with low health, or being overwhelmed by enemies. Many timeouts were caused by the agent attempting to move past friendly monsters, such as a shopkeeper. By default, bumping into monsters attacks them, but for passive monsters, the game prompts the player before initiating an attack. The agent's refusal to attack these monsters often leads to a loop of canceling the prompt and moving, resulting in eventual timeouts. A similar loop took place when the agent attempted to pick up an item with a generic name on the map but a detailed name in the game's menu. This confusion led the agent to repeatedly close and reopen the menu, unable to locate the desired item.</p>
<p>Based on the results of the unguided agent, we constructed a guide that included strategies from autoascend, such as staying on the first two dungeon levels until reaching experience level 8, consuming only freshly slain corpses to avoid eating rotten ones, and leveraging altars to acquire items. Furthermore, we provided tips for common mistakes by the unguided agent, such as avoiding getting stuck behind passive monsters and informing the agent about the time limit to avoid timeouts.</p>
<p>The guided agent often managed to stay alive longer by consuming freshly killed corpses and praying when hungry or at low health. Its causes of death have been a mixture of timeouts, starvation, and dying in combat. Most of the timeouts stemmed from a bug with our tracker, which fails to detect when an object disappears while being obscured by a monster. For example, the agent repeatedly attempted to pick up a dagger already taken by its pet due to the tracker's misleading observation. Despite receiving game messages indicating the absence of the item, the agent failed to recognize the situation accurately.</p>
<p>Because we tasked the guided agent to stay on the first two dungeon levels, its average depth is lower than that of the unguided agent. However, because monsters keep spawning over time, staying on the first levels is an excellent way to grind experience. This results in the guided agent gaining more experience than the unguided agent. Nevertheless, the agent's tendency to stay on the first dungeon levels frequently caused it to die of starvation due to not finding enough monster corpses
to eat. Note that autoascend had a similar starvation issue.</p>
<h2>C. Scenarios</h2>
<p>After conducting the full runs, we hypothesized that although NetPlay can be creative and interact with most mechanics in the game, it tends to fixate on the most straightforward approach for a given task. To confirm this hypothesis, we constructed various small-scale scenarios using des-files and a corresponding task description. Note that we excluded the handcrafted agent and autoascend for this experiment as they cannot easily alter their behavior.</p>
<p>The tested scenarios evaluated NetPlay's ability to interact with game mechanics, follow instructions, and its creativity. We conducted five runs for each scenario, with all roles and the "finish_task" skill enabled. We also repeated some scenarios where the agent performed poorly with additional guidelines. We censored the word NetHack for the scenarios to evaluate the agent's ability independently of its knowledge about the game. To avoid the agent never using the "finish_task" skill, we set a time limit of 500 timesteps for creative scenarios and 200 for the others. See table III for a summary of the tested scenarios and their results.</p>
<p>The tested scenarios show that NetPlay performs best when provided with concrete instructions. The FOCUSED BOULDER task and both ESCAPE tasks, in particular, highlight how the agent can act creative if we focus its attention on a specific problem. However, without very detailed instructions, the agent often fails to do what it wants due to incorrect actions and a lack of explicit feedback.</p>
<p>The agent's struggle with explicit feedback is particularly evident in the BAG and MULTIPICKUP scenarios, where the agent often failed to navigate the menus correctly. While it understood the menus and often chose the correct course of action, it often failed by forgetting a crucial step, such as closing the menu.</p>
<h2>V. Potential and Limitations</h2>
<p>NetPlay uses a similar architecture to Inner Monologue and DEPS, which have shown promising results for simple dynamic environments. Our experiments show that despite the immense complexity of NetHack, the agent can fulfill a wide range of tasks given enough context information. To our knowledge, this is the first NetHack agent to exhibit such flexible behavior. However, the benefits of the presented approach seem to diminish the more ambiguous a given task is, making tasks such as "Win the Game" impossible.</p>
<p>A promising use case of the presented architecture is regression testing during game development. Game developers</p>
<p>TABLE III: Scenarios: A detailed description of all the tested scenarios, their results, and the agent's success rate. Note that in some scenarios, the agent did not use the "finish_task" skill, even after completing it. We still count these as success.</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Success</th>
<th>Description</th>
<th>Results</th>
</tr>
</thead>
<tbody>
<tr>
<td>Game Mechanics</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BAG</td>
<td>$1 / 5$</td>
<td>A room with four random objects and a bag of holding with the task of stuffing all objects into the bag.</td>
<td>The bag of holding menu is quite complex. The agent was only successful when using the option that automatically stuffs all items into the bag. In the other cases, the agent forgot to mark an item or to confirm its selection.</td>
</tr>
<tr>
<td>GUIDED BAG</td>
<td>$3 / 5$</td>
<td>Same as BAG, but we told the agent the quickest way to pick up items and to navigate the bag's menu.</td>
<td>The agent used the automatic option three times. In the other cases, the agent marked the task done too early, stating that it would pick up the remaining items next.</td>
</tr>
<tr>
<td>MULTIPICKUP</td>
<td>$3 / 5$</td>
<td>A room with 2-5 objects on the same spot, challenging the agent to navigate the multipickup menu.</td>
<td>The agent often picked up items inefficiently by opening the pickup menu multiple times. It failed twice by forgetting to confirm its item selection.</td>
</tr>
<tr>
<td>WAND</td>
<td>$1 / 5$</td>
<td>A room with a statue and a wand with the task of hitting the statue with the wand.</td>
<td>The agent often failed by standing atop the statue and casting the wand onto itself. Only once did the wand spawn next to the statue, causing the agent to cast the wand towards the statue.</td>
</tr>
<tr>
<td>GUIDED WAND</td>
<td>$5 / 5$</td>
<td>Same task as WAND, but we asked the agent to stand next to the statue instead of on top of it and fire in the statue's direction.</td>
<td>Most of the time, the agent succeeded on the first try, except once when he got it on the second try after repositioning himself.</td>
</tr>
<tr>
<td>Instructions</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ORDERED</td>
<td>$5 / 5$</td>
<td>A room with the task to pick up two wands, then a scroll of identification, and finally to identify one wand.</td>
<td>The agent executed the tasks accurately in the given order.</td>
</tr>
<tr>
<td>UNORDERED</td>
<td>$3 / 5$</td>
<td>A room with the task to drink from a fountain, open a locked and a closed door, and kill a monster in any order.</td>
<td>The agent completed the tasks in no particular order. One fail stemmed from high-level mobs spawning from the fountain, and one from incorrectly using the lockpick.</td>
</tr>
<tr>
<td>ALTERNATIVE</td>
<td>$5 / 5$</td>
<td>Three rooms with a fountain and a potion somewhere. The task was to drink from a fountain or a potion.</td>
<td>The agent always drank from the fountain, which in all cases was found first or was closest to the agent.</td>
</tr>
<tr>
<td>CONDITIONAL</td>
<td>$4 / 5$</td>
<td>Three rooms, with only a single potion hidden in one of the rooms. The task was to drink from a fountain, or if unavailable a potion.</td>
<td>The agent always drinks the first potion it finds without exploring further. In one case, it deemed the task impossible due to spawning with no fountain or potion in sight.</td>
</tr>
<tr>
<td>Creativity</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CARRY</td>
<td>$1 / 5$</td>
<td>The agent has to carry two very heavy objects through a monster-filled room. We also provided tools such as a bag of holding, a teleportation wand, and an invisibility cloak.</td>
<td>The agent often refused to play because it could not see the required items or it dropped them in the wrong room.</td>
</tr>
<tr>
<td>GUIDED CARRY</td>
<td>$4 / 5$</td>
<td>Same task as CARRY, but we told the agent to prioritize killing monsters first, to carry only one of the heavy items at a time, and to use the teleportation wand for easier travel.</td>
<td>Most of the time, the agent carried only one item, and it often used the wand to teleport. It failed once by dropping one item in the incorrect room.</td>
</tr>
<tr>
<td>BOULDER</td>
<td>$1 / 5$</td>
<td>Two rooms connected by a corridor with a boulder. The agent starts either with pickaxes or wands to remove the boulder.</td>
<td>When given only wands, the agent only used explore_level and ignored the boulder. Only once did it start with a pickaxe that it used to mine the boulder.</td>
</tr>
<tr>
<td>FOCUSED BOULDER</td>
<td>$3 / 5$</td>
<td>Same task as BOULDER, but the agent was told to remove any boulders blocking its path.</td>
<td>The agent often tried kicking the boulder, which failed, after which it then used a pickaxe or a wand. It failed twice due to not correctly utilizing the available tools.</td>
</tr>
<tr>
<td>GUIDED BOULDER</td>
<td>$5 / 5$</td>
<td>Same task as FOCUSED BOULDER, but the agent was told explicitly to remove the boulder with the wands or pickaxes. We also provided directions on how to utilize the tools.</td>
<td>In all cases, the agent quickly used the pickaxe or a wand to remove the boulder.</td>
</tr>
<tr>
<td>ESCAPE</td>
<td>$3 / 5$</td>
<td>The agent must escape from a stone-walled room. Escape methods: Digging with a wand through a wall, teleporting with a wand, or morphing into a wall-phasing monster using a polymorph control ring with a polymorph wand.</td>
<td>The agent escaped twice by teleporting, despite initial teleport failure. It also experimented with the wand of digging, casting it in all directions to find an exit. It failed twice due to incorrectly using the wands.</td>
</tr>
<tr>
<td>HINT ESCAPE</td>
<td>$5 / 5$</td>
<td>Same as ESCAPE, with a hint engraved on the floor. The hint either reveals which wall is brittle and leads to an escape or hints at the name of the wall-phasing monster.</td>
<td>After finding the hint, the agent often used the suggested escape method, except for one occasion when it teleported instead. In one instance, the initial attempts to dig through the wall failed, so it resorted to exploring other methods.</td>
</tr>
</tbody>
</table>
<p>could test specific aspects of their game by providing NetPlay with detailed instructions on what to test. This approach could not only streamline the testing process, but it would also benefit from NetPlay's flexibility, enabling the tests to adapt dynamically as the game evolves.</p>
<p>Given NetPlay's proficiency when given detailed context information, an obvious extension to our approach would be granting the agent access to the NetHack Wikipedia. This could be done using a skill that accepts a query and adds the resulting information to the agent's short-term memory. While we think this can improve the results at the cost of more LLM calls, finding the most relevant information for a given situation would be tricky. Instead, we recommend investing future research into automated methods for finding relevant context information, with a particular focus on finding the most successful past interactions as guidelines on how to play.</p>
<p>A significant limitation of our approach lies in the predefined skills and the observation descriptions, which struggle to encompass the vast complexity of NetHack. Designing the agent to handle all potential edge cases proved challenging, as it is difficult to anticipate every scenario. While the premise of this approach is that the LLM can handle these edge cases, this is only true as long as we have a comprehensive description of the environment and flexible skills. In practice, achieving such a well-designed agent requires an ever-growing repertoire of skills and an observation description that grows infinitely. As such, another promising research direction is to use machine learning to replace the handcrafted components of the agent.</p>
<h2>VI. CONCLUSION</h2>
<p>In this work, we introduce NetPlay, the first LLM-powered zero-shot agent for the challenging roguelike NetHack. Building upon an existing approach tailored for simpler dynamic environments, we extended its capabilities to address the complexities of NetHack. We evaluated the agent's performance on the whole game and analyzed its behavior using various isolated scenarios.</p>
<p>NetPlay demonstrates proficiency in executing detailed instructions but struggles with more ambiguous tasks, such as winning the game. Notably, a simple rule-based agent can achieve comparable performance in playing the game. NetPlay's strength lies in its flexibility and creativity. Our experiments show that, given enough context information, NetPlay can perform a wide range of tasks. Moreover, by focusing its attention on a particular problem, NetPlay is adept at exploring a wide range of potential solutions but often with limited success due to a lack of explicit feedback guiding it.</p>
<h2>VII. ACKNOWLEDGEMENTS</h2>
<p>This work was supported by the EPSRC Centre for Doctoral Training in Intelligent Games \&amp; Games Intelligence (IGGI) (EP/S022325/1).</p>
<p>This work was supported by Creative Assembly.</p>
<h2>REFERENCES</h2>
<p>[1] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes, and A. Mian, "A comprehensive overview of large language models," 2023.
[2] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter, "Inner monologue: Embodied reasoning through planning with language models," 2022.
[3] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su, "Llm-planner: Few-shot grounded planning for embodied agents with large language models," 2023.
[4] Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang, H. Lin, Z. He, Z. Zheng, Y. Yang, X. Ma, and Y. Liang, "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models," 2023.
[5] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, Y. Qiao, Z. Zhang, and J. Dai, "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory," 2023.
[6] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, "Voyager: An open-ended embodied agent with large language models," 2023.
[7] K. Lorber, "Nethack home page." [Online]. Available: https://nethack.org/
[8] "autoascend," GitHub, 102023 . [Online]. Available: https://github.com/ maciej-sypetkowski/autoascend
[9] "Neurips 2021 - nethack challenge," 10 2023. [Online]. Available: https://nethackchallenge.com/report.html
[10] H. Küttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette, and T. Rocktäschel, "The nethack learning environment," CoRR, vol. abs/2006.13760, 2020. [Online]. Available: https://arxiv.org/ abs/2006.13760
[11] M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro, F. Petroni, H. Küttler, E. Grefenstette, and T. Rocktäschel, "Minihack the planet: A sandbox for open-ended reinforcement learning research," CoRR, vol. abs/2109.13202, 2021. [Online]. Available: https://arxiv.org/abs/2109.13202
[12] E. Hambro, S. Mohanty, D. Babaev, M. Byeon, D. Chakraborty, E. Grefenstette, M. Jiang, D. Jo, A. Kanervisto, J. Kim, S. Kim, R. Kirk, V. Kurin, H. Küttler, T. Kwon, D. Lee, V. Mella, N. Nardelli, I. Nazarov, N. Ovsov, J. Parker-Holder, R. Raileanu, K. Ramanauskas, T. Rocktäschel, D. Rothermel, M. Samvelyan, D. Sorokin, M. Sypetkowski, and M. Sypetkowski, "Insights from the neurips 2021 nethack challenge," 2022.
[13] maciej sypetkowski, "Autoascend - 1st place nethack agent for the nethack challenge at neurips 2021," GitHub, 012024 . [Online]. Available: https://github.com/maciej-sypetkowski/autoascend
[14] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents," 2023.
[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, "Code as policies: Language model programs for embodied control," 2023.
[16] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, and A. Anandkumar, "Eureka: Human-level reward design via coding large language models," 2023.
[17] M. Klissarov, P. D’Oro, S. Sodhani, R. Raileanu, P.-L. Bacon, P. Vincent, A. Zhang, and M. Henaff, "Motif: Intrinsic motivation from artificial intelligence feedback," 2023.
[18] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, "Reward design with language models," 2023.
[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou, "Chain of thought prompting elicits reasoning in large language models," CoRR, vol. abs/2201.11903, 2022. [Online]. Available: https://arxiv.org/abs/2201.11903
[20] H. Touvron and et al., "Llama 2: Open foundation and fine-tuned chat models," 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/CommanderCero/NetPlay&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
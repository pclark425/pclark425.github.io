<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5746 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5746</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5746</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-5466371997e97d107d462378ce1279e8c7e36a28</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5466371997e97d107d462378ce1279e8c7e36a28" target="_blank">Anomaly Detecting within Dynamic Chinese Chat Text</a></p>
                <p><strong>Paper TL;DR:</strong> This paper first model standard language text using standard Chinese corpora and apply these models to detect anomalous chat text, and construct anomalous chat language model using one static chat text corpus and incorporate this model into the standard language models.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5746.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5746.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confidence-trigram</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Confidence-based anomaly detection using trigram language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detects anomalous Chinese chat text by scoring input word/POS trigrams with trigram language models built from standard corpora; decision is made by comparing a geometric-mean 'confidence' score to an empirically chosen threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Trigram language model (word and POS-tag variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classical n-gram (n=3) statistical language model estimating p(w_i | w_{i-2}, w_{i-1}) (or class-based p(c_i | c_{i-2}, c_{i-1})). Probabilities are estimated from standard Chinese corpora (People's Daily / PKU, Chinese Gigaword / CNGIGA, Chinese Penn Treebank / CNTB); unseen trigrams handled via linear interpolation smoothing. Word-based models use ICTCLAS segmentation; POS-tag models use POS classes (39 POS tags in ICTCLAS).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Scoring: compute trigram probabilities for each trigram in input, combine via geometric mean to form confidence C(W) = (∏_i p(T_i))^(1/K); compare to threshold (empirically chosen) to label anomaly. When chat corpus is incorporated, trigram-specific weight ω_i = exp(p_n(T_i) - p_c(T_i)) is multiplied to reduce confidence when chat trigrams are found.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences of Chinese words (token sequences) represented as trigrams; also POS-tag sequences (class sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Lexical and sequence anomalies (non-standard chat terms, ill-edited terms, anomalous word sequences / neologisms), i.e., syntactic/lexical anomalies manifesting as low-probability trigrams under standard corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Standard corpora: People's Daily (PKU), Chinese Gigaword (CNGIGA), Chinese Penn Treebank (CNTB); chat corpus: NIL corpus (annotated network informal language); test sets #1-#4 (including YESKY BBS chat data for test sets #2-#4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, recall, F1 focused on detection of chat/anomalous sentences. Example results (test set #2, standard corpora only): word-trigram confidence on CNGIGA: p=0.685, r=0.737, F1=0.710. With NIL corpus incorporated (test set #2): word-trigram confidence on CNGIGA: p=0.821, r=0.836, F1=0.828. Thresholds were empirically chosen on test set #1.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to an SVM classifier reported in Xia et al. (2005a). The best F1 reported in this paper (entropy method, see other entry) is 0.853 which is close to the best SVM F1 of 0.871; the authors claim comparable performance and greater stability over time (less degradation across test periods) when incorporating standard corpora + NIL. Confidence method generally slightly underperforms the entropy method.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires empirically tuned thresholds (sensitivity to threshold selection); performance depends on corpus coverage (CNGIGA better for word trigrams); word segmentation (ICTCLAS) may split unknown chat terms into elements, which is tolerated but could affect trigram evidence; POS-tag trigram variant performs worse due to limited POS-class granularity; within-domain evaluation only (no cross-domain tests), so generalization to other domains/time periods not fully validated without more chat corpora.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5746.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5746.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entropy-trigram</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy-based anomaly detection using trigram language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detects anomalous Chinese chat text by computing per-trigram entropy under trigram LMs; higher entropy indicates poorer fit to standard language and signals anomaly, optionally augmented by chat-corpus entropy to amplify anomaly scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Trigram language model (word and POS-tag variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same trigram statistical models as above (word-level and POS-tag class-level), estimating trigram probabilities from standard Chinese corpora; per-trigram probabilities are used to compute average entropy across the input trigrams.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Scoring: compute average entropy per trigram H~_K = -(1/K) ∑_i p̃_n(T_i) ln p̃_n(T_i). If chat corpus incorporated, compute chat-corpus entropy term H~*_K and combine (H~_K = H~_K + H~*_K) to increase anomaly score when chat trigrams are present; compare to an empirically chosen entropy threshold to label anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences of Chinese words (token sequences) represented as trigrams; POS-tag trigrams also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Lexical/sequence anomalies (rare/unexpected trigram patterns, neologisms and informal chat constructions) — i.e., increased uncertainty/perplexity relative to standard language models.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PKU (People's Daily), CNGIGA (Chinese Gigaword), CNTB (Chinese Penn Treebank), NIL corpus (chat); test sets #1-#4; YESKY BBS for test sets #2-#4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, recall, F1. Best reported result: entropy with CNGIGA word-trigram incorporating NIL on test set #2: p=0.857, r=0.849, F1=0.853 (the single best F1 reported). Similar stable F1 values (~0.848–0.853) across test sets #2–#4 when NIL incorporated. Authors also report that entropy-based method slightly outperforms the confidence-based method in most configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Authors compare to prior SVM-based approach (Xia et al. 2005a): their best F1 (0.853) is close to the SVM best (0.871), and they argue their method yields more stable performance across time without frequent retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires reliable probability estimates; small/low-quality corpora (especially for POS-tag trigrams) yield poorer entropy calibration; thresholds chosen empirically on a small labeled set (test set #1); within-domain only; combining entropies by summation (standard + chat) is heuristic; does not localize anomalous tokens (only classifies entire sentences as chat vs standard).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5746.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5746.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NIL-incorporation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Refined LM scoring by incorporating a static chat (NIL) corpus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Improves detection by combining standard-corpus trigram probabilities with trigram statistics from a static chat (NIL) corpus: confidence scores are downweighted for chat trigrams via ω_i = exp(p_n - p_c), and entropy scoring is augmented by adding chat-corpus entropy to amplify anomaly signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Trigram language model (with dual-corpus combination: standard corpora + NIL chat corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two trigram probability distributions are maintained: p_n(T) from standard corpora and p_c(T) from NIL chat corpus (word and POS-tag trigrams). For confidence, each trigram confidence is multiplied by ω_i = exp(p_n(T_i) - p_c(T_i)). For entropy, the average entropy from chat corpus is computed and summed with the standard-corpus entropy to produce a combined entropy score.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Scoring + model-combination: modify confidence by trigram-level multiplicative weights derived from p_n and p_c; modify entropy by summing cross-entropy/entropy contributions from both corpora; decisions by comparing refined scores to empirical thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences of Chinese words (trigrams) and POS-tag trigrams drawn from chat and standard text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Chat-specific lexical/sequence anomalies; the NIL corpus models canonical patterns of chat so that presence of such patterns lowers confidence or raises combined entropy, improving detection of chat phenomena (neologisms, emotive numerics, nonstandard tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>NIL corpus (annotated network informal language), PKU, CNGIGA, CNTB, test sets #1-#4, YESKY BBS chat data used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision/recall/F1. Incorporation of NIL corpus improved F1 by ~0.10 on average for word-trigram models versus using standard corpora only. Example: CNGIGA word-trigram confidence F1 improved from 0.710 (no NIL) to 0.828 (with NIL) on test set #2; entropy method improved to F1=0.853 (best). Results across test sets #2–#4 remain close, indicating stability.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against same trigram methods without NIL incorporation (substantial improvement in F1) and against previous SVM approach; authors claim NIL incorporation gives both higher and more stable performance than static SVM trained on earlier chat collections which degrade over time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>NIL corpus is static and limited in time; because chats evolve fast, a single static chat corpus may not capture future/new chat terms; thresholds remain empirical; method requires reliable p_c estimates which depend on chat corpus size and quality (authors plan to study optimal chat corpus size); approach does not handle cross-domain tests in this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>NIL is not Nothing: Recognition of Chinese Network Informal Language Expressions <em>(Rating: 2)</em></li>
                <li>A Two-Stage Incremental Annotation Approach to Constructing A Network Informal Language Corpus <em>(Rating: 2)</em></li>
                <li>Class-based n-gram models of natural language <em>(Rating: 1)</em></li>
                <li>HMM-based Chinese Lexical Analyzer ICTCLAS <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5746",
    "paper_id": "paper-5466371997e97d107d462378ce1279e8c7e36a28",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "Confidence-trigram",
            "name_full": "Confidence-based anomaly detection using trigram language models",
            "brief_description": "Detects anomalous Chinese chat text by scoring input word/POS trigrams with trigram language models built from standard corpora; decision is made by comparing a geometric-mean 'confidence' score to an empirically chosen threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Trigram language model (word and POS-tag variants)",
            "model_description": "Classical n-gram (n=3) statistical language model estimating p(w_i | w_{i-2}, w_{i-1}) (or class-based p(c_i | c_{i-2}, c_{i-1})). Probabilities are estimated from standard Chinese corpora (People's Daily / PKU, Chinese Gigaword / CNGIGA, Chinese Penn Treebank / CNTB); unseen trigrams handled via linear interpolation smoothing. Word-based models use ICTCLAS segmentation; POS-tag models use POS classes (39 POS tags in ICTCLAS).",
            "model_size": null,
            "anomaly_detection_method": "Scoring: compute trigram probabilities for each trigram in input, combine via geometric mean to form confidence C(W) = (∏_i p(T_i))^(1/K); compare to threshold (empirically chosen) to label anomaly. When chat corpus is incorporated, trigram-specific weight ω_i = exp(p_n(T_i) - p_c(T_i)) is multiplied to reduce confidence when chat trigrams are found.",
            "data_type": "Sequences of Chinese words (token sequences) represented as trigrams; also POS-tag sequences (class sequences).",
            "anomaly_type": "Lexical and sequence anomalies (non-standard chat terms, ill-edited terms, anomalous word sequences / neologisms), i.e., syntactic/lexical anomalies manifesting as low-probability trigrams under standard corpora.",
            "dataset_name": "Standard corpora: People's Daily (PKU), Chinese Gigaword (CNGIGA), Chinese Penn Treebank (CNTB); chat corpus: NIL corpus (annotated network informal language); test sets #1-#4 (including YESKY BBS chat data for test sets #2-#4).",
            "performance_metrics": "Precision, recall, F1 focused on detection of chat/anomalous sentences. Example results (test set #2, standard corpora only): word-trigram confidence on CNGIGA: p=0.685, r=0.737, F1=0.710. With NIL corpus incorporated (test set #2): word-trigram confidence on CNGIGA: p=0.821, r=0.836, F1=0.828. Thresholds were empirically chosen on test set #1.",
            "baseline_comparison": "Compared to an SVM classifier reported in Xia et al. (2005a). The best F1 reported in this paper (entropy method, see other entry) is 0.853 which is close to the best SVM F1 of 0.871; the authors claim comparable performance and greater stability over time (less degradation across test periods) when incorporating standard corpora + NIL. Confidence method generally slightly underperforms the entropy method.",
            "limitations_or_failure_cases": "Requires empirically tuned thresholds (sensitivity to threshold selection); performance depends on corpus coverage (CNGIGA better for word trigrams); word segmentation (ICTCLAS) may split unknown chat terms into elements, which is tolerated but could affect trigram evidence; POS-tag trigram variant performs worse due to limited POS-class granularity; within-domain evaluation only (no cross-domain tests), so generalization to other domains/time periods not fully validated without more chat corpora.",
            "uuid": "e5746.0"
        },
        {
            "name_short": "Entropy-trigram",
            "name_full": "Entropy-based anomaly detection using trigram language models",
            "brief_description": "Detects anomalous Chinese chat text by computing per-trigram entropy under trigram LMs; higher entropy indicates poorer fit to standard language and signals anomaly, optionally augmented by chat-corpus entropy to amplify anomaly scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Trigram language model (word and POS-tag variants)",
            "model_description": "Same trigram statistical models as above (word-level and POS-tag class-level), estimating trigram probabilities from standard Chinese corpora; per-trigram probabilities are used to compute average entropy across the input trigrams.",
            "model_size": null,
            "anomaly_detection_method": "Scoring: compute average entropy per trigram H~_K = -(1/K) ∑_i p̃_n(T_i) ln p̃_n(T_i). If chat corpus incorporated, compute chat-corpus entropy term H~*_K and combine (H~_K = H~_K + H~*_K) to increase anomaly score when chat trigrams are present; compare to an empirically chosen entropy threshold to label anomalies.",
            "data_type": "Sequences of Chinese words (token sequences) represented as trigrams; POS-tag trigrams also evaluated.",
            "anomaly_type": "Lexical/sequence anomalies (rare/unexpected trigram patterns, neologisms and informal chat constructions) — i.e., increased uncertainty/perplexity relative to standard language models.",
            "dataset_name": "PKU (People's Daily), CNGIGA (Chinese Gigaword), CNTB (Chinese Penn Treebank), NIL corpus (chat); test sets #1-#4; YESKY BBS for test sets #2-#4.",
            "performance_metrics": "Precision, recall, F1. Best reported result: entropy with CNGIGA word-trigram incorporating NIL on test set #2: p=0.857, r=0.849, F1=0.853 (the single best F1 reported). Similar stable F1 values (~0.848–0.853) across test sets #2–#4 when NIL incorporated. Authors also report that entropy-based method slightly outperforms the confidence-based method in most configurations.",
            "baseline_comparison": "Authors compare to prior SVM-based approach (Xia et al. 2005a): their best F1 (0.853) is close to the SVM best (0.871), and they argue their method yields more stable performance across time without frequent retraining.",
            "limitations_or_failure_cases": "Requires reliable probability estimates; small/low-quality corpora (especially for POS-tag trigrams) yield poorer entropy calibration; thresholds chosen empirically on a small labeled set (test set #1); within-domain only; combining entropies by summation (standard + chat) is heuristic; does not localize anomalous tokens (only classifies entire sentences as chat vs standard).",
            "uuid": "e5746.1"
        },
        {
            "name_short": "NIL-incorporation",
            "name_full": "Refined LM scoring by incorporating a static chat (NIL) corpus",
            "brief_description": "Improves detection by combining standard-corpus trigram probabilities with trigram statistics from a static chat (NIL) corpus: confidence scores are downweighted for chat trigrams via ω_i = exp(p_n - p_c), and entropy scoring is augmented by adding chat-corpus entropy to amplify anomaly signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Trigram language model (with dual-corpus combination: standard corpora + NIL chat corpus)",
            "model_description": "Two trigram probability distributions are maintained: p_n(T) from standard corpora and p_c(T) from NIL chat corpus (word and POS-tag trigrams). For confidence, each trigram confidence is multiplied by ω_i = exp(p_n(T_i) - p_c(T_i)). For entropy, the average entropy from chat corpus is computed and summed with the standard-corpus entropy to produce a combined entropy score.",
            "model_size": null,
            "anomaly_detection_method": "Scoring + model-combination: modify confidence by trigram-level multiplicative weights derived from p_n and p_c; modify entropy by summing cross-entropy/entropy contributions from both corpora; decisions by comparing refined scores to empirical thresholds.",
            "data_type": "Sequences of Chinese words (trigrams) and POS-tag trigrams drawn from chat and standard text corpora.",
            "anomaly_type": "Chat-specific lexical/sequence anomalies; the NIL corpus models canonical patterns of chat so that presence of such patterns lowers confidence or raises combined entropy, improving detection of chat phenomena (neologisms, emotive numerics, nonstandard tokens).",
            "dataset_name": "NIL corpus (annotated network informal language), PKU, CNGIGA, CNTB, test sets #1-#4, YESKY BBS chat data used for evaluation.",
            "performance_metrics": "Precision/recall/F1. Incorporation of NIL corpus improved F1 by ~0.10 on average for word-trigram models versus using standard corpora only. Example: CNGIGA word-trigram confidence F1 improved from 0.710 (no NIL) to 0.828 (with NIL) on test set #2; entropy method improved to F1=0.853 (best). Results across test sets #2–#4 remain close, indicating stability.",
            "baseline_comparison": "Compared against same trigram methods without NIL incorporation (substantial improvement in F1) and against previous SVM approach; authors claim NIL incorporation gives both higher and more stable performance than static SVM trained on earlier chat collections which degrade over time.",
            "limitations_or_failure_cases": "NIL corpus is static and limited in time; because chats evolve fast, a single static chat corpus may not capture future/new chat terms; thresholds remain empirical; method requires reliable p_c estimates which depend on chat corpus size and quality (authors plan to study optimal chat corpus size); approach does not handle cross-domain tests in this work.",
            "uuid": "e5746.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "NIL is not Nothing: Recognition of Chinese Network Informal Language Expressions",
            "rating": 2
        },
        {
            "paper_title": "A Two-Stage Incremental Annotation Approach to Constructing A Network Informal Language Corpus",
            "rating": 2
        },
        {
            "paper_title": "Class-based n-gram models of natural language",
            "rating": 1
        },
        {
            "paper_title": "HMM-based Chinese Lexical Analyzer ICTCLAS",
            "rating": 1
        }
    ],
    "cost": 0.009674,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Anomaly Detecting within Dynamic Chinese Chat Text</h1>
<p>Yunqing Xia<br>Department of S.E.E.M.<br>The Chinese University of Hong Kong<br>Shatin, Hong Kong<br>yqxia@se.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>The problem in processing Chinese chat text originates from the anomalous characteristics and dynamic nature of such a text genre. That is, it uses ill-edited terms and anomalous writing styles in chat text, and the anomaly is created and discarded very quickly. To handle this problem, one solution is to re-train the recognizer periodically. This costs a lot of manpower in producing the timely chat text corpus. The new approaches are proposed in this paper to detect the anomaly within dynamic Chinese chat text by incorporating standard Chinese corpora and chat corpus. We first model standard language text using standard Chinese corpora and apply these models to detect anomalous chat text. To improve detection quality, we construct anomalous chat language model using one static chat text corpus and incorporate this model into the standard language models. Our approaches calculate confidence and entropy for the input text and apply threshold values to help make the decisions. The experiments prove that performance equivalent to the best ones produced by the approaches in existence can be achieved stably with our approaches.</p>
<h2>1 Introduction</h2>
<p>Network Informal Language (NIL) refers to the special human language widely used in the community of network communication via platforms such as chat rooms/tools, mobile phone short message services (SMS), bulletin board systems (BBS), emails, blogs, etc. NIL is ubiquitous due in special to the rapid proliferation of Internet applications. As one important type of NIL text, chat text appears frequently within in-</p>
<h2>Kam-Fai Wong <br> Department of S.E.E.M. <br> The Chinese University of Hong Kong <br> Shatin, Hong Kong <br> kfwong@se.cuhk.edu.hk</h2>
<p>creasing volume of chat logs of online education (Heard-White, 2004) and customer relationship management (Gianforte, 2003) via chat rooms/tools. In wed-based chat rooms and BBS a large volume of NIL text is abused by (McCullagh, 2004). A survey by the Global System for Mobile Communication (GSM) showed that Germans send 200 million messages a year (German News, 2004). All the facts disclose the growing importance in processing NIL text.</p>
<p>Chat text holds anomalous characteristics in forming non-alphabetical characters, words, and phrases. It uses ill-edited terms and anomalous writing styles. Typical examples of anomalous Chinese chat terms can be found in (Xia et. al., 2005a). Besides the anomalous characteristics, our observations reveal remarkable dynamic nature of the chat text. The anomaly is created and discarded very quickly. Although there is no idea how tomorrow's chat text would look like, the changing will never stop. Instead, the changing gets faster and faster.</p>
<p>The challenging issues originates from the dynamic nature are two-fold. On the one hand, anomalous chat terms and writing styles are frequently found in chat text. Knowledge about chat text is urgently required to understand the anomaly. On the other hand, the dynamic nature of the chat text makes it nearly impossible to maintain a timely chat text knowledge base. This claim has been proved by (Xia et. al., 2005a) in which experiments are conducted with an SVM classifier. The classifier is trained on chat text created in an earlier period and tested on chat text created in a later period. In their experiments, performance of the SVM classifier becomes lower when the two periods are farther. This reveals that chat text is written in such a style that changes constantly along with time. A straightforward solution to this problem is to re-train the SVM classifier periodically with timely chat text collections. Unfortunately, this solution costs a lot of manpower in producing new chat text corpora. The super-</p>
<p>vised learning technique becomes ineffective in processing chat text.</p>
<p>This paper proposes approaches to detecting anomaly in dynamic Chinese chat text by incorporating standard Chinese corpora and a static chat corpus. The idea is basically error-driven. That is, we first create standard language models using trigram on standard Chinese corpora. These corpora provide negative training samples. We then construct anomalous chat language model using one static chat text corpus which provides positive training samples. We incorporate the chat language model with the standard language models and calculate confidence and entropy to help make decisions whether input text is anomalous chat text. We investigate two types of trigram, i.e. word trigram and part-ofspeech (POS) tag trigram in this work.</p>
<p>The remaining sections of this paper are organized as follow. In Section 2, the works related to this paper are addressed. In Section 3, approaches of anomaly detection in dynamic Chinese chat text with standard Chinese corpora are presented. In Section 4, we incorporate the NIL corpus into our approaches. In section 5, experiments are described to estimate threshold values and to evaluate performance of the two approaches with various configurations. Comparisons and discussions are also reported. We conclude this paper and address future works in Section 6.</p>
<h2>2 Related Works</h2>
<p>Some works had been carried out in (Xia et. al., 2005a) in which an SVM classifier is implemented to recognize anomalous chat text terms. A within-domain open test is conducted on chat text posted in March 2005. The SVM classifier is trained on five training sets which contain chat text posted from December 2004 to February 2005. The experiments show that performance of the SVM classifier increases when the training period and test period are closer. This reveals that chat text is written in a style that changes quickly with time. Many anomalous popular chat terms in last year are forgotten today and new ones replace them. This makes SVM based pattern learning technique ineffective to reflect the changes.</p>
<p>The solution to this problem in (Xia et. al., 2005b) is to re-train the SVM classifier periodically. This costs a lot of manpower in producing the timely chat text corpora, in which each piece
of anomalous chat text should be annotated with several attributes manually.</p>
<p>We argue that the anomalous chat text can be identified using negative training samples in static Chinese corpora. Our proposal is that we model the standard natural language using standard Chinese corpora. We incorporate a static chat text corpus to provide positive training samples to reflect fundamental characteristics of anomalous chat text. We then apply the models to detect the anomalous chat text by calculating confidence and entropy.</p>
<p>Regarding the approaches proposed in this paper, our arguments are, 1) the approaches can achieve performance equivalent to the best ones produced by the approaches in existence; and 2) the good performance can be achieved stably. We prove these arguments in the following sections.</p>
<h2>3 Anomaly Detection with Standard Chinese Corpora</h2>
<p>Chat text exhibits anomalous characteristics in using or forming words. We argue that the anomalous chat text, which is referred as anomaly in this article, can be identified with language models constructed on standard Chinese corpora with some statistical language modeling (SLM) techniques, e.g. trigram model.</p>
<p>The problem of anomaly detection can be addressed as follows. Given a piece of anomalous chat text, i.e. $W=\left{w_{1}, w_{2}, \ldots, w_{n}\right}$, and a language model $L M={p(x)}$, we attempt to recognize $W$ as anomaly by the language model. We propose two approaches to tackle this problem. We design a confidence-based approach to calculate how likely that $W$ fits into the language model. Another approach is designed based on entropy calculation. Entropy method was originally proposed to estimate how good a language model is. In our work we apply this method to estimate how much the constructed language models are able to reflect the corpora properly based on the assumption that the corpora are sound and complete.</p>
<p>Although there exist numerous statistical methods to construct a natural language model, the objective of them is one: to construct a probabilistic distribution model $p(x)$ which fits to the most extent into the observed language data in the corpus. We implement the trigram model and create language models with three Chinese corpora, i.e. People's Daily corpus, Chinese Gigaword and Chinese Pen Treebank. We investigate</p>
<p>quality of the language models produced with these corpora.</p>
<h3>3.1 The N-gram Language Models</h3>
<p>N -gram model is the most widely used in statistical language modeling nowadays. Without loss of generality we express the probability of a word sequence $W=\left{w_{1}, \ldots, w_{n}\right}$ of $n$ words, i.e. $p(W)$ as</p>
<p>$$
p(W)=p\left(w_{1}, \ldots, w_{n}\right)=\prod_{i=1}^{n} p\left(w_{i} \mid w_{0}, w_{1}, \ldots, w_{i-1}\right)
$$</p>
<p>where $w_{0}$ is chosen appropriately to handle the initial condition. The probability of the next word $w_{i}$ depends on the history $h_{i}$ of words that have been given so far. With this factorization the complexity of the model grows exponentially with the length of the history.</p>
<p>One of the most successful models of the past two decades is the trigram model $(\mathrm{n}=3)$ where only the most recent two words of the history are used to condition the probability of the next word.</p>
<p>Instead of using the actual words, one can use a set of word classes. Classes based on the POS tags, or the morphological analysis of words, or the semantic information have been tried. Also, automatically derived classes based on some statistical models of co-occurrence have been tried (Brown et. al., 1990). The class model can be generally described as</p>
<p>$$
p(W)=\prod_{i=1}^{n} p\left(w_{i} \mid c_{i}\right) p\left(c_{i} \mid c_{i-2}, c_{i-1}\right)
$$</p>
<p>if the classes are non-overlapping. These tri-class models have had higher perplexities than the corresponding trigram model. However, they have led to a reduction in perplexity when linearly combined with the trigram model.</p>
<h3>3.2 The Confidence-based Approach</h3>
<p>Given a piece of chat text $W=\left{w_{1}, w_{2}, \ldots, w_{n}\right}$ where each word $w_{i}$ is obtained with a standard Chinese word segmentation tool, e.g. ICTCLAS. As ICTCLAS is a segmentation tool based on standard vocabulary, it means that some unknown chat terms (e.g., "手" $\left.\int_{1}^{\infty} " \prime\right)$ would be broken into several element Chinese words (i.e., "手" and "手" in the above case). This does not hurt the algorithm because we use trigram in this method. A chat term may produce some anoma-
lous word trigrams which are evidences for anomaly detection.</p>
<p>We use non-zero probability for each trigram in this calculation. This is very simple but naïve. The calculation seeks to produce a so-called confidence, which reflects how much the given text fits into the training corpus in arranging its element Chinese words. This is enlightened by the observation that the chat terms use element words in anomalous manners which can not be simulated by the training corpus.</p>
<p>The confidence-based value is defined as</p>
<p>$$
C(W)=\left(\prod_{i=1}^{K} C\left(T_{i}\right)\right)^{\frac{1}{K}}
$$</p>
<p>where $K$ denotes the number of trigrams in chat text $W$ and $T_{i}$ is the $i$-th order trigram. $C\left(T_{i}\right)$ is confidence of trigram $T_{i}$. Generally $C\left(T_{i}\right)$ is assigned probability of the trigram $T_{i}$ in training corpus, i.e. $p\left(T_{i}\right)$. When a trigram is missing, linear interpolation is applied to estimate its probability.</p>
<p>We empirically setup a confidence threshold value to determine whether the input text contains chat terms, namely, it is a piece of chat text. The input is concluded to be stand text if its confidence is bigger than the confidence threshold value. Otherwise, the input is concluded to be chat text. The confidence threshold value can be estimated with a training chat text collection.</p>
<h3>3.3 The Entropy-based Approach</h3>
<p>The idea beneath this approach comes from entropy based language modeling. Given a language model, one can use the quantity of entropy to get an estimation of how good the language model (LM) might be. Denote by $p$ the true distribution, which is unknown to us, of a segment of new text $x$ of $k$ words. Then the entropy on a per word basis is defined as</p>
<p>$$
H=\lim <em x="x">{k \rightarrow \infty}-\frac{1}{k} \sum</em> p(x) \ln p(x)
$$</p>
<p>If every word in a vocabulary of size $|V|$ is equally likely then the entropy would be $\log _{2}|V| ; H \leq \ln |V|$ for other distributions of the words.</p>
<p>Enlightened by the estimation method, we compute the entropy-based value on a per trigram basis for the input chat text. Given a standard LM denoted by $\hat{p}$ which is modeled by trigram, the entropy-value is calculate as</p>
<p>$$
\tilde{H}<em i="1">{K}=-\frac{1}{K} \sum</em>\right)
$$}^{K} \tilde{p}\left(T_{i}\right) \ln \tilde{p}\left(T_{i</p>
<p>where $K$ denotes number of trigrams the input text contains. Our goal is to find how much difference the input text is compared against the LM. Obviously, bigger entropy discloses a piece of more anomalous chat text. An empirical entropy threshold is again estimated on a training chat text collection. The input is concluded to be stand text if its entropy is smaller than the entropy threshold value. Otherwise, the input is concluded to be chat text.</p>
<h2>4 Incorporating the Chat Text Corpus</h2>
<p>We argue performance of the approaches can be improved when an initial static chat text corpus is incorporated. The chat text corpus provides some basic forms of the anomalous chat text. These forms we observe provide valuable heuristics in the trigram models. Within the chat text corpus, we only consider the word trigrams and POS tag trigrams in which anomalous chat text appears. We thus construct two trigram lists. Probabilities are produced for each trigram according to its occurrence. One chat text example EXP1 is given below.</p>
<p>EXP1: 介个故事听起来 8 错。
SEG1: 介 个 故事 听 起来 8 错。
SEG1 presents the word segments produced by ICTCLAS. We generate chat text word trigrams based on SEG1 as follow.</p>
<p>TRIGRAM1: (1) /介 个 故事/
(2) /个 起来 8 /
(3)/起来 8 错/
(4) / 8 错。/</p>
<p>For each input trigram $T_{i}$, if it appears in the chat text corpus, we adjust the confidence and entropy values by incorporating its probability in chat text corpus.</p>
<h3>4.1 The Refined Confidence</h3>
<p>For each $C\left(T_{i}\right)$, we assign a weight $\varpi_{i}$, which is calculated as</p>
<p>$$
\varpi_{i}=e^{p_{n}\left(T_{i}\right)-p_{c}\left(T_{i}\right)}
$$</p>
<p>where $p_{n}\left(T_{i}\right)$ is probability of the trigram $T_{i}$ in standard corpus and $p_{c}\left(T_{i}\right)$ probability in chat text corpus. Equation (3) therefore is re-written as</p>
<p>$$
\begin{aligned}
C^{\prime}(W) &amp; =\left(\prod_{i=1}^{K} \varpi_{i} C\left(T_{i}\right)\right)^{\frac{1}{K}} \
&amp; =\left(\prod_{i=1}^{K} e^{p_{n}\left(T_{i}\right)-p_{c}\left(T_{i}\right)} p_{n}\left(T_{i}\right)\right)^{\frac{1}{K}}
\end{aligned}
$$</p>
<p>The intention of inserting $\varpi_{i}$ into confidence calculation is to decrease confidence of input chat text when chat text trigrams are found. Normally, when a trigram $T_{i}$ is found in chat text trigram lists, $p_{n}\left(T_{i}\right)$ will be much lower than $p_{c}\left(T_{i}\right)$; therefore $\varpi_{i}$ will be much lower than 1 . By multiplying such a weight, confidence of input chat text can be decreased so that the text can be easily detected.</p>
<h3>4.2 The Refined Entropy</h3>
<p>Instead of assigning a weight, we introduce the entropy-based value of the input chat text on the chat text corpus, i.e. $\tilde{H}<em K="K">{K}^{<em>}$, to produce a new equation. We denote $\tilde{H}_{K}^{</em>}$ the entropy calculated with equation (5). Similar to $\tilde{H}</em>^{<em>}, \tilde{H}_{K}^{</em>}$ is calculated with equation (8).</p>
<p>$$
\tilde{H}<em i="1">{K}^{*}=-\frac{1}{K} \sum</em>}^{K} \tilde{p<em i="i">{c}\left(T</em>}\right) \ln \tilde{p<em i="i">{c}\left(T</em>\right)
$$</p>
<p>We therefore re-write the entropy-based value calculation as follows.</p>
<p>$$
\begin{aligned}
&amp; \tilde{H}<em K="K">{K}=\tilde{H}</em>^{<em>}+\tilde{H}_{K}^{</em>} \
&amp; =-\frac{1}{K} \sum_{i=1}^{K}\left(\tilde{p}<em i="i">{n}\left(T</em>}\right) \ln \tilde{p<em i="i">{n}\left(T</em>}\right)+\tilde{p<em i="i">{c}\left(T</em>}\right) \ln \tilde{p<em i="i">{c}\left(T</em>\right)\right)
\end{aligned}
$$</p>
<p>The intention of introducing $\tilde{H}_{K}^{<em>}$ in entropy calculation is to increase the entropy of input chat text when chat text trigrams are found. It can be easily proved that $\tilde{H}<em K="K">{K}$ is never smaller than $H</em>^{</em>}$. As bigger entropy discloses a piece of more anomalous chat text, we believe more anomalous chat texts can be correctly detected with equation (9).</p>
<h2>5 Evaluations</h2>
<p>Three experiments are conducted in this work. The first experiment aims to estimate threshold values from a real text collection. The remaining experiments seek to evaluate performance of the approaches with various configurations.</p>
<h3>5.1 Data Description</h3>
<p>We use two types of text corpora to train our approaches in the experiments. The first type is</p>
<p>standard Chinese corpus which is used to construct standard language models. We use People's Daily corpus, also know as Peking University Corpus (PKU), the Chinese Gigaword (CNGIGA) and the Chinese Penn Treebank (CNTB) in this work. Considering coverage, CNGIGA is the most excellent one. However, PKU and CPT provide more syntactic information in their annotations. Another type of training corpus is chat text corpus. We use NIL corpus described in (Xia et. al., 2005b). In NIL corpus each anomalous chat text is annotated with their attributes.</p>
<p>We create four test sets in our experiments. We use the test set #1 to estimate the threshold values of confidence and entropy for our approaches. The values are estimated on two types of trigrams in three corpora. Test set #1 contains 89 pieces of typical Chinese chat text selected from the NIL corpus and 49 pieces of standard Chinese sentences selected from online Chinese news by hand. There is no special consideration that we select different number of chat texts and standard sentences in this test set.</p>
<p>The remaining three test sets are used to compare performance of our approaches on test data created in different time periods. The test set #2 is the earliest one and #4 the latest one according to their time stamp. There are 10 K sentences in total in test set $# 2, # 3$ and $# 4$. In this collection, chat texts are selected from YESKY BBS system (http://bbs.yesky.com/bbs/) which cover BBS text in March and April 2005 (later than the chat text in the NIL corpus), and standard texts are extracted from online Chinese news randomly. We describe the four test sets in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"># of standard <br> sentences</th>
<th style="text-align: center;"># of chat <br> sentences</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$# 1$</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">89</td>
</tr>
<tr>
<td style="text-align: center;">$# 2$</td>
<td style="text-align: center;">1013</td>
<td style="text-align: center;">2320</td>
</tr>
<tr>
<td style="text-align: center;">$# 3$</td>
<td style="text-align: center;">1013</td>
<td style="text-align: center;">2320</td>
</tr>
<tr>
<td style="text-align: center;">$# 4$</td>
<td style="text-align: center;">1014</td>
<td style="text-align: center;">2320</td>
</tr>
</tbody>
</table>
<p>Table 1: Number of sentences in the four test sets.</p>
<h3>5.2 Experiment I: Threshold Values Estimation</h3>
<h3>5.2.1 Experiment Description</h3>
<p>This experiment seeks to estimate the threshold values of confidence and entropy for two types of trigrams in three Chinese corpora.</p>
<p>We first run the two approaches using only standard Chinese corpora on the 138 sentences in the first test set. We put the calculated values
(confidence or entropy) into two arrays. Note that we already know type of each sentence in the first test set. So we are able to select in each array a value that produces the lowest error rate. In this way we obtain the first group of threshold values for our approaches.</p>
<p>We incorporate the NIL corpus to the two approaches and run them again. We then produce the second group of threshold values in the same way to produce the first group of values.</p>
<h3>5.2.2 Results</h3>
<p>The selected threshold values and corresponding error rates are presented in Table 2 5 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Trigram option</th>
<th style="text-align: center;">Threshold</th>
<th style="text-align: center;">Err rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">word of CNGIGA</td>
<td style="text-align: center;">$1.58 \mathrm{E}-07$</td>
<td style="text-align: center;">0.092</td>
</tr>
<tr>
<td style="text-align: left;">word of PKU</td>
<td style="text-align: center;">$7.06 \mathrm{E}-07$</td>
<td style="text-align: center;">0.098</td>
</tr>
<tr>
<td style="text-align: left;">word of CNTB</td>
<td style="text-align: center;">$2.09 \mathrm{E}-06$</td>
<td style="text-align: center;">0.085</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of CNGIGA</td>
<td style="text-align: center;">0.0278</td>
<td style="text-align: center;">0.248</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of PKU</td>
<td style="text-align: center;">0.0143</td>
<td style="text-align: center;">0.263</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of CNTB</td>
<td style="text-align: center;">0.0235</td>
<td style="text-align: center;">0.255</td>
</tr>
</tbody>
</table>
<p>Table 2: Selected threshold values of confidence for the approach using standard Chinese corpora and error rates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Trigram option</th>
<th style="text-align: center;">Threshold</th>
<th style="text-align: center;">Err rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">word of CNGIGA</td>
<td style="text-align: center;">$3.762 \mathrm{E}-056$</td>
<td style="text-align: center;">0.099</td>
</tr>
<tr>
<td style="text-align: left;">word of PKU</td>
<td style="text-align: center;">$5.683 \mathrm{E}-048$</td>
<td style="text-align: center;">0.112</td>
</tr>
<tr>
<td style="text-align: left;">word of CNTB</td>
<td style="text-align: center;">$2.167 \mathrm{E}-037$</td>
<td style="text-align: center;">0.169</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of CNGIGA</td>
<td style="text-align: center;">0.00295</td>
<td style="text-align: center;">0.234</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of PKU</td>
<td style="text-align: center;">0.00150</td>
<td style="text-align: center;">0.253</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of CNTB</td>
<td style="text-align: center;">0.00239</td>
<td style="text-align: center;">0.299</td>
</tr>
</tbody>
</table>
<p>Table 3: Selected threshold values of entropy for the approach using standard Chinese corpora and error rates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Trigram option</th>
<th style="text-align: center;">Threshold</th>
<th style="text-align: center;">Err rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">word of CNGIGA</td>
<td style="text-align: center;">$4.26 \mathrm{E}-05$</td>
<td style="text-align: center;">0.089</td>
</tr>
<tr>
<td style="text-align: left;">word of PKU</td>
<td style="text-align: center;">$3.75 \mathrm{E}-05$</td>
<td style="text-align: center;">0.102</td>
</tr>
<tr>
<td style="text-align: left;">word of CNTB</td>
<td style="text-align: center;">$6.85 \mathrm{E}-05$</td>
<td style="text-align: center;">0.092</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of CNGIGA</td>
<td style="text-align: center;">0.0398</td>
<td style="text-align: center;">0.257</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of PKU</td>
<td style="text-align: center;">0.0354</td>
<td style="text-align: center;">0.266</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of CNTB</td>
<td style="text-align: center;">0.0451</td>
<td style="text-align: center;">0.249</td>
</tr>
</tbody>
</table>
<p>Table 4: Selected threshold values of confidence for the approach incorporating the NIL corpus and error rates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Trigram option</th>
<th style="text-align: center;">Threshold</th>
<th style="text-align: center;">Err rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">word of CNGIGA</td>
<td style="text-align: center;">$8.368 \mathrm{E}-027$</td>
<td style="text-align: center;">0.102</td>
</tr>
<tr>
<td style="text-align: left;">word of PKU</td>
<td style="text-align: center;">$3.134 \mathrm{E}-019$</td>
<td style="text-align: center;">0.096</td>
</tr>
<tr>
<td style="text-align: left;">word of CNTB</td>
<td style="text-align: center;">$5.528 \mathrm{E}-021$</td>
<td style="text-align: center;">0.172</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of CNGIGA</td>
<td style="text-align: center;">0.00465</td>
<td style="text-align: center;">0.241</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of PKU</td>
<td style="text-align: center;">0.00341</td>
<td style="text-align: center;">0.251</td>
</tr>
<tr>
<td style="text-align: left;">POS tag of CNTB</td>
<td style="text-align: center;">0.00532</td>
<td style="text-align: center;">0.282</td>
</tr>
</tbody>
</table>
<p>Table 5: Selected thresholds values of entropy for the approach incorporating the NIL corpus and error rates.</p>
<p>We use the selected threshold values in experiment II and III to detect anomalous chat text within test set #2, #3 and #4.</p>
<h3>5.3 Experiment II: Anomaly Detection with Three Standard Chinese Corpora</h3>
<h3>5.3.1 Experiment Description</h3>
<p>In this experiment, we run the two approaches using the standard Chinese corpora on test set #2. The threshold values estimated in experiment I are applied to help make decisions.</p>
<p>Input text can be detected as either standard text or chat text. But we are only interested in how correctly the anomalous chat text is detected. Thus we calculate precision (p), recall (r) and $F_{1}$ measure (f) only for chat text.</p>
<p>$$
p=\frac{a}{a+c} \quad r=\frac{a}{a+b} \quad f=\frac{2 \times p \times r}{p+r}
$$</p>
<p>where $a$ is the number of true positives, $b$ the false negatives and $c$ the false positives.</p>
<h3>5.3.2 Results</h3>
<p>The experiment results for the approaches using the standard Chinese corpora on test set #2 are presented in Table 6.</p>
<h3>5.3.3 Discussions</h3>
<p>Table 4 shows that, in most cases, the entropybased approach outperforms the confidencebased approach slightly. It can thus be conclude that the entropy-based approach is more effective in anomaly detection.</p>
<p>It is also revealed that both approaches perform better with word trigrams than that with POS tag trigrams. This is natural for class based trigram model when number of class is small. Thirty-nine classes are used in ICTCLAS in POS tagging Chinese words.</p>
<p>When the three Chinese corpora are compared, the CNGIGA performs best in the confidencebased approach with word trigram model. However, it is not the case with POS tag trigram model. Results of two approaches on CNTB are best amongst the three corpora. Although we are able to draw the conclusion that bigger corpora yields better performance with word trigram, the same conclusion, however, does not work for POS tag trigram. This is very interesting. The reason we can address on this issue is that CNTB probably provides highest quality POS tag trigrams and other corpora contain more noisy POS tag trigrams, which eventually decreases the performance. An observation on word/POS tag lists
for three Chinese corpora verifies such a claim. Text in CNTB is best-edited amongst the three.</p>
<h3>5.4 Experiment III: Anomaly Detection with NIL Corpus Incorporated</h3>
<h3>5.4.1 Experiment Description</h3>
<p>In this experiment, we incorporate one chat text corpus, i.e. NIL corpus, to the two approaches. We run them on test set #2, #3 and #4 with the estimated threshold values. We use precision, recall and $F_{1}$ measure again to evaluate performance of the two approaches.</p>
<h3>5.4.2 Results</h3>
<p>The experiment results are presented in Table 7 Table 9 on test set #2, #3 and #4 respectively.</p>
<h3>5.4.3 Discussions</h3>
<p>We first compare the two approaches with different running configurations. All conclusions made in experiment II still work for experiment III. They are, i) the entropy-based approach outperforms the confidence-based approach slightly in most cases; ii) both approach perform better with word trigram than POS tag trigram; iii) both approaches perform best on CNGIGA with word trigram model. But with POS tag trigram model, CNTB produces the best results.</p>
<p>An interesting comparison is conducted on $F_{1}$ measure between the approaches in experiment II and experiment III on test set #2 in Figure 1 (the left two columns). Generally, $F_{1}$ measure of anomaly detection with both approaches with word trigram model is improved when the NIL corpus is incorporated. It is revealed in Table $7 \sim 9$ that same observation is found with POS tag trigram model.</p>
<p>We compare $F_{1}$ measure of the approaches with word trigram model in experiment III on test set #2, #3 and #4 in Figure 1 (the right three columns). The graph in Figure 1 shows that $F_{1}$ measure on three test sets are very close to each other. This is also true the approaches with POS tag trigram model as showed in Table 7 9. This provides evidences for the argument that the approaches can produce stable performance with the NIL corpus. Differently, as reported in (Xia et. al., 2005a), performance achieved in SVM classifier is rather unstable. It performs poorly with training set C#1 which contains BBS text posted several months ago, but much better with training set C#5 which contains the latest chat text.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus</th>
<th style="text-align: center;">Word trigram</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">POS tag trigram</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">confidence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">entropy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">confidence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">entropy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
</tr>
<tr>
<td style="text-align: center;">CNGIGA</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.650</td>
</tr>
<tr>
<td style="text-align: center;">PKU</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.648</td>
<td style="text-align: center;">0.636</td>
</tr>
<tr>
<td style="text-align: center;">CNTB</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.673</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.681</td>
</tr>
</tbody>
</table>
<p>Table 6: Results of anomaly detection using standard Chinese corpora on test set #2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus</th>
<th style="text-align: center;">Word trigram</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">POS tag trigram</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">confidence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">entropy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">confidence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">entropy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
</tr>
<tr>
<td style="text-align: center;">CNGIGA</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.675</td>
</tr>
<tr>
<td style="text-align: center;">PKU</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.839</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.683</td>
</tr>
<tr>
<td style="text-align: center;">CNTB</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.700</td>
</tr>
</tbody>
</table>
<p>Table 7: Results of anomaly detection incorporating NIL corpus on test set #2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus</th>
<th style="text-align: center;">Word trigram</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">POS tag trigram</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">confidence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">entropy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">confidence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">entropy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
</tr>
<tr>
<td style="text-align: center;">CNGIGA</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.830</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.674</td>
</tr>
<tr>
<td style="text-align: center;">PKU</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.684</td>
</tr>
<tr>
<td style="text-align: center;">CNTB</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.703</td>
</tr>
</tbody>
</table>
<p>Table 8: Results of anomaly detection incorporating NIL corpus on test set #3</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus</th>
<th style="text-align: center;">Word trigram</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">POS tag trigram</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">confidence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">entropy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">confidence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">entropy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">f</td>
</tr>
<tr>
<td style="text-align: center;">CNGIGA</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.839</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.674</td>
</tr>
<tr>
<td style="text-align: center;">PKU</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.825</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.687</td>
</tr>
<tr>
<td style="text-align: center;">CNTB</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.699</td>
</tr>
</tbody>
</table>
<p>Table 9: Results of anomaly detection incorporating NIL corpus on test set #4
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparisons on $F_{1}$ measure of the approaches with word trigram on test set #2, #3 and #4 in experiment II and experiment III.</p>
<p>We finally compare performance of our approaches against the one described in (Xia, et. al., 2005a). The best $F_{1}$ measure achieved in our work, i.e. 0.853 , is close to the best one in their work, i.e. 0.871 with training corpus $\mathrm{C} # 5$. This proves another argument that our approaches can produce equivalent performance to the best ones achieved by the approaches in existence.</p>
<h2>6 Conclusions</h2>
<p>The new approaches to detecting anomalous Chinese chat text are proposed in this paper. The approaches calculate confidence and entropy values with the language models constructed on negative training samples in three standard Chi-</p>
<p>nese corpora. To improve detection quality, we incorporate positive training samples in NIL corpus in our approaches. Two conclusions can be made based on this work. Firstly, $F_{1}$ measure of anomaly detection can be improved by around 0.10 when NIL corpus is incorporated into the approaches. Secondly, performance equivalent to the best ones produced by the approaches in existence can be achieved stably by incorporating the standard Chinese corpora and the NIL corpus.</p>
<p>We believe some strong evidences for our claims can be obtained by training our approaches with more chat text corpora which contain chat text created in different time periods. We are conducting this experiment seeks to find out whether and how our approaches are independent of time. This work is still progressing. A report on this issue will be available shortly. We also plan to investigate how size of chat text corpus influences performance of our approaches. The goal is to find the optimal size of chat text corpus which can achieve the best performance. The readers should also be noted that evaluation in this work is a within-domain test. Due to shortage of chat text resources, no cross-domain test is conducted. In the future cross-domain test, we will investigate how our approaches are independent of domain.</p>
<p>Eventual goal of chat text processing is to normalize the anomalous chat text, namely, convert it to standard text holding the same meaning. So the work carried out in this paper is the first step leading to this goal. Approaches will be designed to locate the anomalous terms in chat text and map them to standard words.</p>
<h2>Acknowledgement</h2>
<p>Research described in this paper is partially supported by the Chinese University of Hong Kong under the Direct Grant Scheme (No: 2050330) and Strategic Grant Scheme project (No: 4410001) respectively.</p>
<h2>Reference</h2>
<p>Brown, P. F., V. J. Della Pietra, P. V. de Souza, J. C. Lai, and R. L. Mercer. 1990. Class-based n-gram models of natural language. In Proceedings of the IBM Natural Language ITL, Paris, France.</p>
<p>Finkelhor, D., K. J. Mitchell, and J. Wolak. 2000. Online Victimization: A Report on the Nation's Youth. Alexandria, Virginia: National Center for Missing \&amp; Ex-ploited Children, page ix.</p>
<p>German News. 2004. Germans are world SMS champions, 8 April 2004, http://www.expatica.com/ source/site_article.asp?subchannel_id=52\&amp;story_i $\mathrm{d}=6469$.</p>
<p>Gianforte, G.. 2003. From Call Center to Contact Center: How to Successfully Blend Phone, Email, Web and Chat to Deliver Great Service and Slash Costs. RightNow Technologies.</p>
<p>Heard-White, M., Gunter Saunders and Anita Pincas. 2004. Report into the use of CHAT in education. Final report for project of Effective use of CHAT in Online Learning, Institute of Education, University of London.</p>
<p>McCullagh, D.. 2004. Security officials to spy on chat rooms. News provided by CNET Networks. November 24, 2004.</p>
<p>Xia, Y., K.-F. Wong and W. Gao. 2005a. NIL is not Nothing: Recognition of Chinese Network Informal Language Expressions, 4th SIGHAN Workshop on Chinese Language Processing at IJCNLP'05, pp95-102.</p>
<p>Xia, Y., K.-F. Wong and R. Luk. 2005b. A Two-Stage Incremental Annotation Approach to Constructing A Network Informal Language Corpus. In Proc. of NTCIR-5 Meeting, pp. 529-536.</p>
<p>Zhang, Z., H. Yu, D. Xiong and Q. Liu. 2003. HMMbased Chinese Lexical Analyzer ICTCLAS. SIGHAN'03 within ACL'03, pp. 184-187.</p>            </div>
        </div>

    </div>
</body>
</html>
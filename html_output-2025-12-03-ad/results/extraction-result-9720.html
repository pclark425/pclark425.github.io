<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9720 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9720</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9720</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-281891451</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.15087v2.pdf" target="_blank">HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions</a></p>
                <p><strong>Paper Abstract:</strong> Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's capability to integrate information from diverse sources. However, creating extensive and high-quality MHQA datasets is challenging: (i) manual annotation is expensive, and (ii) current synthesis methods often produce simplistic questions or require extensive manual guidance. This paper introduces HopWeaver, the first cross-document framework synthesizing authentic multi-hop questions without human intervention. HopWeaver synthesizes bridge and comparison questions through an innovative pipeline that identifies complementary documents and constructs authentic reasoning paths to ensure true multi-hop reasoning. We further present a comprehensive system for evaluating the synthesized multi-hop questions. Empirical evaluations demonstrate that the synthesized questions achieve comparable or superior quality to human-annotated datasets at a lower cost. Our framework provides a valuable tool for the research community: it can automatically generate challenging benchmarks from any raw corpus, which opens new avenues for both evaluation and targeted training to improve the reasoning capabilities of advanced QA models, especially in domains with scarce resources.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9720.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9720.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge (ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-judge evaluation ensemble (claude-3-7-sonnet, gpt-4o, gemini-2.0-flash, google/gemma-3-27b-it, meta-llama/llama-3.3-70b-instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble of proprietary and open-source LLMs used as automated, pointwise judges to score synthesized multi-hop QA pairs along multiple linguistic and task-oriented dimensions, with outputs averaged across models for final quality assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-hop question answering (MHQA) dataset quality evaluation / question generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>claude-3-7-sonnet-20250219; gpt-4o-2024-11-20; gemini-2.0-flash; google/gemma-3-27b-it; meta-llama/llama-3.3-70b-instruct (ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pointwise Likert-style scoring across multiple dimensions (Multi-hop validity, Fluency, Clarity, Conciseness, Relevance, Consistency, Answerability, Information integration, Reasoning guidance, Logical sophistication). Judges run N=5 independent runs at temperature=0 (deterministic sampling) to measure stability; the ensemble's average score is used as final evaluation. Judges instructed to be skeptical and strict, with domain-specific prompts and structured output format (see Appendix F, Polisher and scoring prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Pairwise human validation: 100 sampled questions turned into 50 pairwise comparisons (selected to be non-trivial by LLM score difference > 0.3). Three Master's-level Computer Science students served as human evaluators; each pair was judged and majority voting used to determine human consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Human-LLM alignment: Majority Agreement 94%; Complete (unanimous) Agreement 62%. Inter-human reliability (Fleiss' Kappa among 3 raters) = 0.46 (reported for the human validation subset). (See Appendix C.3, Table 9.)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>The paper identifies a principal trade-off when replacing human raters with LLM judges: by prioritizing LLM self-consistency and reproducibility, alignment with human subjective judgments can be reduced or deprioritized. Specifically, the authors note that optimizing LLM judges to match human scores can cause the LLM judges to inherit human biases and inconsistencies; conversely, privileging self-consistency may lose aspects of human subjectivity and nuanced preferences that humans exhibit.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper does not present detailed qualitative case studies of specific errors unique to LLM judges, but quantifies divergence: in a controlled pairwise study of 50 pairs (selected for non-trivial LLM score gaps) the LLM ensemble disagreed with human majority in ~6% of cases (implied by 94% majority agreement). The authors also note the conceptual example that LLM judges optimized for human alignment risk replicating human systematic biases (fatigue, cultural preferences, contextual misunderstandings), citing this as a potential loss when switching to LLM-based evaluation unless carefully handled (Appendix F.2).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Despite the concerns, the paper reports strong practical alignment and benefits: the LLM ensemble achieved 94% agreement with human majority on the validation pairs, indicating high concordance in practice; LLM judges provided scalability, reproducibility (low Avg.Intra-item SD at T=0), and were chosen for reliability metrics (Krippendorff's α, Fleiss' κ). The authors also mitigate self-enhancement bias by ensuring judges do not score outputs from their own close variants and by using an ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Main text Section 'Data Quality Evaluation' (LLM-as-Judge Evaluation); Appendix F (F.1 pointwise scoring, F.2 discussion 'Beyond Human Alignment', F.3 reliability & model selection); Appendix C.3 (Human Validation of LLM-as-Judge; Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9720.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9720.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency criterion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritizing LLM self-consistency over direct human alignment for judge selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selection principle used in the paper: choose judge models based primarily on their internal stability and reproducibility (self-consistency across repeated runs) rather than purely on alignment with potentially noisy human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation methodology for NLG / MHQA quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Criterion applied across candidate judge models (e.g., gemma-3-27b-it, gpt-4o etc.); not a specific model itself</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Reliability-driven selection: sample N=5 deterministic runs (temperature=0) per item to compute Avg.Intra-item SD; compute Krippendorff's α and Fleiss' Kappa across runs; select models with low AvgSD and high α/κ. Use ensemble averaging of selected judges for final scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Used human evaluation only as a validation signal (pairwise study described in Appendix C.3) rather than as the primary selection criterion for judge models. The paper also references literature documenting low human inter-rater reliability (examples: Krippendorff's α=0.33 on PersonaChat, α=0.08 on WMT Zh–En, α=0.49 on QAGS) to motivate this choice (Appendix F.2).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Reliability metrics for judge selection: Avg.Intra-item SD (lower is better), Krippendorff's α (higher better), Fleiss' Kappa (higher better). The authors report selecting judges based on these metrics (detailed scores in Appendix F.3 / Table 12). For human alignment validation they report 94% majority agreement (Appendix C.3).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>By prioritizing self-consistency over human alignment, the evaluation framework may lose sensitivity to certain subjective human preferences and nuanced judgments that humans provide; in other words, some aspects of human-centric evaluation (nuance, cultural/contextual interpretation, possibly tolerances/preferences) may be deemphasized or not captured. The paper also warns that forcing LLM judges to mimic noisy human judgments can propagate human biases into automated judges.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>No detailed qualitative failure cases are shown; the loss is argued conceptually: (1) human judgments are noisy and biased, so aligning to them can import those problems into the judge; (2) selecting for self-consistency intentionally risks diverging from human subjectivity—e.g., an LLM judge may rate items more consistently but slightly differently than human consensus in a minority of cases (empirically ~6% divergence in the paper's validation sample).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The authors argue this trade-off is beneficial for scalability and reproducibility: LLM judges chosen for self-consistency still achieved very high agreement with human majority (94%), and produced stable, reproducible ratings (low AvgSD). They further recommend combining multiple assessment strategies (LLM judges, solver Q-Only vs Q+Docs tests, evidence-accessibility metrics) to offset any single-method blind spots.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Appendix F.2 ('Beyond Human Alignment: The Case for LLM Self-Consistency'); Appendix F.3 (reliability metrics and judge selection); Appendix C.3 (human validation results).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llms instead of human judges? A large scale empirical study across 20 NLP evaluation tasks <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9720",
    "paper_id": "paper-281891451",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-judge (ensemble)",
            "name_full": "LLM-as-judge evaluation ensemble (claude-3-7-sonnet, gpt-4o, gemini-2.0-flash, google/gemma-3-27b-it, meta-llama/llama-3.3-70b-instruct)",
            "brief_description": "An ensemble of proprietary and open-source LLMs used as automated, pointwise judges to score synthesized multi-hop QA pairs along multiple linguistic and task-oriented dimensions, with outputs averaged across models for final quality assessment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Multi-hop question answering (MHQA) dataset quality evaluation / question generation evaluation",
            "llm_judge_model": "claude-3-7-sonnet-20250219; gpt-4o-2024-11-20; gemini-2.0-flash; google/gemma-3-27b-it; meta-llama/llama-3.3-70b-instruct (ensemble)",
            "llm_judge_setup": "Pointwise Likert-style scoring across multiple dimensions (Multi-hop validity, Fluency, Clarity, Conciseness, Relevance, Consistency, Answerability, Information integration, Reasoning guidance, Logical sophistication). Judges run N=5 independent runs at temperature=0 (deterministic sampling) to measure stability; the ensemble's average score is used as final evaluation. Judges instructed to be skeptical and strict, with domain-specific prompts and structured output format (see Appendix F, Polisher and scoring prompts).",
            "human_evaluation_setup": "Pairwise human validation: 100 sampled questions turned into 50 pairwise comparisons (selected to be non-trivial by LLM score difference &gt; 0.3). Three Master's-level Computer Science students served as human evaluators; each pair was judged and majority voting used to determine human consensus.",
            "agreement_metric": "Human-LLM alignment: Majority Agreement 94%; Complete (unanimous) Agreement 62%. Inter-human reliability (Fleiss' Kappa among 3 raters) = 0.46 (reported for the human validation subset). (See Appendix C.3, Table 9.)",
            "losses_identified": "The paper identifies a principal trade-off when replacing human raters with LLM judges: by prioritizing LLM self-consistency and reproducibility, alignment with human subjective judgments can be reduced or deprioritized. Specifically, the authors note that optimizing LLM judges to match human scores can cause the LLM judges to inherit human biases and inconsistencies; conversely, privileging self-consistency may lose aspects of human subjectivity and nuanced preferences that humans exhibit.",
            "examples_of_loss": "The paper does not present detailed qualitative case studies of specific errors unique to LLM judges, but quantifies divergence: in a controlled pairwise study of 50 pairs (selected for non-trivial LLM score gaps) the LLM ensemble disagreed with human majority in ~6% of cases (implied by 94% majority agreement). The authors also note the conceptual example that LLM judges optimized for human alignment risk replicating human systematic biases (fatigue, cultural preferences, contextual misunderstandings), citing this as a potential loss when switching to LLM-based evaluation unless carefully handled (Appendix F.2).",
            "counterexamples_or_caveats": "Despite the concerns, the paper reports strong practical alignment and benefits: the LLM ensemble achieved 94% agreement with human majority on the validation pairs, indicating high concordance in practice; LLM judges provided scalability, reproducibility (low Avg.Intra-item SD at T=0), and were chosen for reliability metrics (Krippendorff's α, Fleiss' κ). The authors also mitigate self-enhancement bias by ensuring judges do not score outputs from their own close variants and by using an ensemble.",
            "paper_reference": "Main text Section 'Data Quality Evaluation' (LLM-as-Judge Evaluation); Appendix F (F.1 pointwise scoring, F.2 discussion 'Beyond Human Alignment', F.3 reliability & model selection); Appendix C.3 (Human Validation of LLM-as-Judge; Table 9).",
            "uuid": "e9720.0",
            "source_info": {
                "paper_title": "HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-consistency criterion",
            "name_full": "Prioritizing LLM self-consistency over direct human alignment for judge selection",
            "brief_description": "A selection principle used in the paper: choose judge models based primarily on their internal stability and reproducibility (self-consistency across repeated runs) rather than purely on alignment with potentially noisy human ratings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Evaluation methodology for NLG / MHQA quality assessment",
            "llm_judge_model": "Criterion applied across candidate judge models (e.g., gemma-3-27b-it, gpt-4o etc.); not a specific model itself",
            "llm_judge_setup": "Reliability-driven selection: sample N=5 deterministic runs (temperature=0) per item to compute Avg.Intra-item SD; compute Krippendorff's α and Fleiss' Kappa across runs; select models with low AvgSD and high α/κ. Use ensemble averaging of selected judges for final scores.",
            "human_evaluation_setup": "Used human evaluation only as a validation signal (pairwise study described in Appendix C.3) rather than as the primary selection criterion for judge models. The paper also references literature documenting low human inter-rater reliability (examples: Krippendorff's α=0.33 on PersonaChat, α=0.08 on WMT Zh–En, α=0.49 on QAGS) to motivate this choice (Appendix F.2).",
            "agreement_metric": "Reliability metrics for judge selection: Avg.Intra-item SD (lower is better), Krippendorff's α (higher better), Fleiss' Kappa (higher better). The authors report selecting judges based on these metrics (detailed scores in Appendix F.3 / Table 12). For human alignment validation they report 94% majority agreement (Appendix C.3).",
            "losses_identified": "By prioritizing self-consistency over human alignment, the evaluation framework may lose sensitivity to certain subjective human preferences and nuanced judgments that humans provide; in other words, some aspects of human-centric evaluation (nuance, cultural/contextual interpretation, possibly tolerances/preferences) may be deemphasized or not captured. The paper also warns that forcing LLM judges to mimic noisy human judgments can propagate human biases into automated judges.",
            "examples_of_loss": "No detailed qualitative failure cases are shown; the loss is argued conceptually: (1) human judgments are noisy and biased, so aligning to them can import those problems into the judge; (2) selecting for self-consistency intentionally risks diverging from human subjectivity—e.g., an LLM judge may rate items more consistently but slightly differently than human consensus in a minority of cases (empirically ~6% divergence in the paper's validation sample).",
            "counterexamples_or_caveats": "The authors argue this trade-off is beneficial for scalability and reproducibility: LLM judges chosen for self-consistency still achieved very high agreement with human majority (94%), and produced stable, reproducible ratings (low AvgSD). They further recommend combining multiple assessment strategies (LLM judges, solver Q-Only vs Q+Docs tests, evidence-accessibility metrics) to offset any single-method blind spots.",
            "paper_reference": "Appendix F.2 ('Beyond Human Alignment: The Case for LLM Self-Consistency'); Appendix F.3 (reliability metrics and judge selection); Appendix C.3 (human validation results).",
            "uuid": "e9720.1",
            "source_info": {
                "paper_title": "HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llms instead of human judges? A large scale empirical study across 20 NLP evaluation tasks",
            "rating": 2,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        }
    ],
    "cost": 0.0130825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions</p>
<p>Zhiyu Shen 
School of Computer Science and Engineering Sun Yat-sen University</p>
<p>Jiyuan Liu 
School of Computer Science and Engineering Sun Yat-sen University</p>
<p>Yunhe Pang 
School of Computer Science and Engineering Sun Yat-sen University</p>
<p>Yanghui Rao raoyangh@mail.sysu.edu.cn 
School of Computer Science and Engineering Sun Yat-sen University</p>
<p>HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions
9A589CD621EB0FD627F0BE549B5E96DA
Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's capability to integrate information from diverse sources.However, creating extensive and high-quality MHQA datasets is challenging: (i) manual annotation is expensive, and (ii) current synthesis methods often produce simplistic questions or require extensive manual guidance.This paper introduces HopWeaver, the first cross-document framework synthesizing authentic multi-hop questions without human intervention.HopWeaver synthesizes bridge and comparison questions through an innovative pipeline that identifies complementary documents and constructs authentic reasoning paths to ensure true multi-hop reasoning.We further present a comprehensive system for evaluating the synthesized multi-hop questions.Empirical evaluations demonstrate that the synthesized questions achieve comparable or superior quality to human-annotated datasets at a lower cost.Our framework provides a valuable tool for the research community: it can automatically generate challenging benchmarks from any raw corpus, which opens new avenues for both evaluation and targeted training to improve the reasoning capabilities of advanced QA models, especially in domains with scarce resources.The code for HopWeaver is publicly available. 11 The code can be found at https://github.com/</p>
<p>Introduction</p>
<p>Integrating information from different sources shows the intelligence of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems (Huang and Huang, 2024;Hu et al., 2024).Multi-Hop Question Answering (MHQA), as a critical benchmark for this ability, requires models to integrate information distributed across documents (Guo et al., 2024;Mavi et al., 2024).MHQA requires a model to connect intermediate entities or concepts across documents to infer answers.However, constructing extensive and highquality MHQA datasets remains costly because manual annotation (Yang et al., 2018;Ho et al., 2020;Trivedi et al., 2022) struggles to cover diverse reasoning paths at scale and often introduces annotation bias (Klie et al., 2024;Wich et al., 2021).Furthermore, existing benchmarks, while valuable, may not fully expose the limitations of sophisticated RAG systems, which can sometimes overfit to the prevalent reasoning patterns within these datasets (Tang and Yang, 2024;Liu et al., 2025).</p>
<p>Recent studies have established synthesized data generation as a new paradigm for model training and evaluation (Lu et al., 2023;Guo and Chen, 2024).However, automatically synthesizing authentic multi-hop questions remains particularly challenging (Mavi et al., 2022;Guo et al., 2024).Existing approaches either require substantial manual intervention or produce "pseudo multi-hop" 1 arXiv:2505.15087v2[cs.CL] 8 Oct 2025 questions answerable from single documents.They face two fundamental limitations: (i) inability to identify bridge entities across complementary contexts, and (ii) failure to retrieve documents that are truly complementary rather than redundant.</p>
<p>We introduce HopWeaver, the first crossdocument framework that synthesizes multi-hop questions without any manual intervention.Building on established MHQA research (Yang et al., 2018;Ho et al., 2020;Trivedi et al., 2022;Yang et al., 2018;Ho et al., 2020), HopWeaver focuses on synthesizing two predominant question types: bridge questions (connecting facts across documents through intermediate entities) and comparison questions (contrasting attributes between entities).It employs an innovative retrieval mechanism that identifies authentic complementary documents and constructs reasoning paths that necessitate cross-document information integration (as shown in Figure 1).</p>
<p>We further develop a comprehensive evaluation system using (i) LLM-as-judge, (ii) answerability and difficulty, and (iii) evidence-accessibility.In summary, HopWeaver enables the cost-effective synthesis of high-quality MHQA data, making it especially valuable in specialized domains where it can generate multi-hop questions directly from raw corpora without relying on human intervention or structured knowledge bases.This work makes the following key contributions:</p>
<ol>
<li>
<p>We propose HopWeaver, the first crossdocument framework that synthesizes multihop questions without any manual intervention.</p>
</li>
<li>
<p>Through a novel, multi-dimensional evaluation system of our own design, we demonstrate that questions synthesized by Hop-Weaver meet or exceed the quality of humanannotated benchmarks.</p>
</li>
<li>
<p>We demonstrate that HopWeaver unlocks new multi-hop reasoning patterns from raw corpora to create diverse datasets.These datasets expose limitations in advanced RAG systems, opening new possibilities for robust model evaluation and targeted training, especially in specialized domains lacking annotated data.</p>
</li>
</ol>
<p>2 Related Works</p>
<p>MHQA Datasets and Evaluation</p>
<p>The field of multi-hop question answering has been driven by the development of challenging datasets that require reasoning across multiple sources.Yang et al. (2018) marked a significant advancement by introducing HotpotQA with 113k Wikipedia-based QA pairs requiring reasoning over supporting documents, featuring both bridge and comparison question types with sentence-level supporting facts for explainability.Ho et al. (2020) enhanced this approach by constructing datasets using both structured and unstructured data, introducing evidence information as comprehensive reasoning paths and utilizing logical rules to ensure multihop requirements.Trivedi et al. ( 2022) took a systematic bottom-up approach by composing singlehop questions into multi-hop ones with MuSiQue.However, critical evaluation has revealed fundamental limitations in dataset construction.Min et al. (2019) demonstrated that many supposedly multi-hop questions in HotpotQA can be solved through single-hop reasoning due to weak distractor paragraphs and entity type matching, underscoring the difficulty of guaranteeing truly multi-hop questions.</p>
<p>Multi-Hop Question Synthesis</p>
<p>From rule-based to neural question generation.</p>
<p>Early work transformed declarative sentences into questions with hand-crafted rules or templates (e.g., Heilman and Smith, 2010;Wyse and Piwek, 2009).</p>
<p>Neural sequence-to-sequence models later enabled data-driven question generation (Du et al., 2017;Zhou et al., 2017;Sun et al., 2018).However, these methods fundamentally rely on human-provided source documents (Fei et al., 2022;Xia et al., 2023)-they generate questions from given evidence rather than discovering related evidence pairs, making them unsuitable for automatic multihop question synthesis.</p>
<p>Structured knowledge supervision.To address the reliance on manually provided evidence documents, several studies leverage external structured knowledge.Knowledge-graph-aware pipelines (KGAST;Vuth et al., 2024, LLM+KG;Chen et al., 2024a) and template-driven systems such as MINTQA (He et al., 2024) assembles questions from manually curated triples or predefined templates.Others compose multi-hop questions from large pre-constructed pools of single-hop questions (Chen et al., 2024b).However, they inherit the limitations of structured resources: coverage gaps, fixed schemas, and considerable human curation effort, which severely constrain scalability.</p>
<p>Recent LLM-based attempts.Recent work leverages LLMs to reduce manual effort but lacks rigorous validation.Source2Synth (Lupidi et al., 2024) generates synthetic data grounded in real sources but requires pre-selected documents; Wu et al. (2024) extend the methodology to multimodal question generation, yet still rely on human-curated image-text pairs; while Luo et al. (2024) propose chain-of-exemplar approaches for educational distractor generation using carefully crafted prompts.</p>
<p>Despite their advances, these methods still cannot automatically discover complementary document pairs from raw corpora.</p>
<p>In short, existing methods either depend on structured resources or fall back to single-hop settings, leaving the problem of fully automatic, corpus-only multi-hop question synthesis largely unsolved.</p>
<p>Dataset Quality Evaluation</p>
<p>Traditional evaluation methods face fundamental limitations in ensuring authentic multi-hop requirements.Min et al. (2019) demonstrated that many "multi-hop" questions in HotpotQA are solvable through single-hop reasoning, exposing critical gaps in existing evaluation approaches.Moreover, human annotation exhibits significant reliability issues- Klie et al. (2024) and Wich et al. (2021) revealed systematic annotation problems and biases, with inter-rater agreement often unacceptably low (Bavaresco et al., 2024).</p>
<p>Recent work explores LLM-based evaluation as a scalable alternative.Liu et al. (2023) introduced G-Eval for automated assessment, while Fu et al. (2024b) proposed multi-dimensional frameworks for question generation evaluation.However, LLM judges require careful validation-Lee et al. ( 2025) emphasized self-consistency over human alignment as a reliability criterion, and Zheng et al. ( 2023) developed systematic benchmarks for judge evaluation.These converging insights underscore the need for comprehensive evaluation frameworks that combine multiple assessment strategies, particularly for synthesized MHQA datasets where traditional human-only evaluation proves both costly and unreliable.</p>
<p>Methodology</p>
<p>We introduce a framework for synthesizing two types of multi-hop questions: bridge questions and comparison questions (as shown in Figure 2).The formal preliminaries are provided in Appendix A.</p>
<p>Bridge Question Synthesis</p>
<p>Step 1: Bridge Entity Identification.First, a source document d s is randomly sampled from the corpus.An LLM then processes d s .The primary goal is to identify a reasonable bridge entity (e b ) that links disparate information contexts.The LLM achieves this goal by selecting a text segment (s p ) from d s that provides concentrated textual context and then identifying e b within s p .Finally, based on e b and s p , the LLM formulates an optimized query (q), enabling the targeted retrieval of complementary documents in the next phase.</p>
<p>Step 2: Two-Stage Coarse-to-Fine Retrieval.This step takes the query q and source document d s (from Step 1) as input.Its objective is to output a ranked list of k complementary documents, D t = {d 1 t , . . ., d k t }, which are identified through a two-stage retrieval strategy:</p>
<p>First, the Coarse Retrieval stage generates an initial candidate list.An initial set of documents is retrieved using q.Up to k candidates are greedily selected from this set using a modified Maximum Marginal Relevance (MMR) approach (Carbonell and Goldstein, 1998).As defined in Equation 1, this method balances query relevance (sim(q, d i )) with dissimilarity to the source document d s (via −sim(d i , d s )) and diversity among already selected documents in set S (via − max d j ∈S sim(d i , d j )), thereby promoting the selection of diverse and complementary contexts.The parameters λ 1 , λ 2 , λ 3 control this trade-off (as shown in Appendix H).</p>
<p>Score(d
i ) = λ 1 sim(q, d i ) − λ 2 sim(d i , d s ) − λ 3 max d j ∈S sim(d i , d j )(1)
Next, the Fine-grained Reranking stage refines this candidate list.Candidate documents d i are paired with q and re-scored by a fine-tuned reranker, yielding the final set D t composed of the top k documents according to these new scores.The details of the reranker model and its fine-tuning methodology are provided in Section 3.3.</p>
<p>Step 3: Multi-Hop Question Construction.This step constructs a verifiable multi-hop question by integrating information from the source document d s and a selected complementary document d t (from D t identified in Step 2), using the The module generates a QA pair, a reasoning path (d s → e b → d t ), and sub-questions to ensure the interpretability and verifiability of each question.See Appendix E for an example of a synthesized bridge question and its generation details.</p>
<p>Step 4: Question Polishing and Validation.With the QA pair and its supporting segments gen-erated in Step 3, we further enhance their quality by processing them through the Question Polishing and Validation module, detailed in Section 3.4.</p>
<p>Comparison Question Synthesis</p>
<p>Step 1: Entity and Attribute Identification.For each randomly sampled document d a , the module:</p>
<p>• Identifies the primary subject entity e a and its type (e.g., person, location, organization).</p>
<p>• Extracts 3-5 concise, factual attribute-value pairs (a, v a ) suitable for comparison (e.g., numeric, date, category).</p>
<p>Step 2: Filtering.To ensure concreteness and comparability, each entity and attribute is scored on a 1-5 scale (see Appendix G for detailed criteria), with only those meeting the threshold included in further steps.</p>
<p>Step 3: Query Generation and Retrieval.</p>
<p>Based on its understanding of the source entity e a and the filtered attributes from Step 2, the LLM generates queries and retrieves documents.The strategy selection is governed by a set of instructions provided to the LLM.The model defaults to Diversified Search unless it can confidently identify a comparable entity for Direct Recommendation.</p>
<p>• Direct Recommendation: The LLM selects a representative attribute of e a , recommends a comparable entity e c , and generates a verification query to retrieve documents containing e c with the same attribute.</p>
<p>• Diversified Search: The LLM generates three diverse retrieval queries to find other entities of the same type as e a .These queries are used to retrieve documents from the corpus, and their top-k results are merged to discover documents containing comparable entities.</p>
<p>This step outputs a list of retrieved documents for the subsequent stage (Step 4).</p>
<p>Step 4: Question Construction.The system first identifies entity e c within the retrieved document(s) and searches for a comparable attribute pair (a, v a , v b ) where both entities have specific, factual values for the attribute a.The approach to finding this pair follows the strategy from Step 3:
• Guided Comparison: Following a Direct
Recommendation, where a specific entity e c and attribute a are specified, the system focuses on retrieving this exact pair.</p>
<p>• Open Discovery: Following a Diversified Search, the system iterates through the attributes of entity e a to find the first valid comparable pair with any attribute of a discovered entity e c .</p>
<p>When finding a comparable pair, the module generates a comparison QA pair (e.g., "Which has the higher a: e a or e c ?" "e a "), along with the two document segments containing information of v a and v b , and a corresponding reasoning path.See Appendix E for an illustrative example of a comparison question.</p>
<p>Step 5: Question Polishing and Validation.The generated QA pair and its supporting segments are processed by the Question Polishing and Validation module (in Section 3.4) to ensure quality.To enhance the reranking stage (in Section 3.1, Step 2), we fine-tune the reranker using contrastive triples generated through simulating key steps of the bridge question synthesis process (Figure 3).</p>
<p>Fine-Tuning Reranker via Simulated Feedback</p>
<p>Generating Supervision Signals through Simulation.Our fine-tuning process begins with creating a labeled dataset directly from the bridge question synthesis process.We simulate this synthesis by retrieving a list of documents {d i } using our coarse retrieval methods.Each candidate d i attempts to generate the connecting sub-questions required to link it with d s through e b .The success or failure of multi-hop question generation provides a supervision signal, with successful attempts yielding positive example (d + ) and failed attempts producing negative example (d − ).</p>
<p>Contrastive Learning Fine-Tuning of the Reranker.These positive and negative examples form the dataset for reranker fine-tuning.We construct contrastive training triples, typically (query = e b , d + , d − ), and train the reranker to distinguish complementary documents.This optimization is guided by a cross-entropy loss:
L = − 1 N N i=1 log exp(f (e b , d + i )) k j=1 exp(f (e b , d ij ))(2)
where f (e b , d) is the score produced by the reranker for a query-document pair, N is the batch size, d + i is the positive document in the i-th group, and d ij represents all documents (one positive and k − 1 negatives) in the j-th position within the i-th group for calculating the sum in the denominator.</p>
<p>This supervision signal, derived directly from the downstream task's success, assists the reranker to learn what constitutes a truly complementary document.And an ablation study confirming the efficiency gains from our fine-tuned reranker is detailed in Appendix C.1.1.</p>
<p>Question Polishing and Validation</p>
<p>The polisher module assesses and refines each multi-hop question (both Bridge and Comparison types) via structured prompts, generating one of four outcomes:</p>
<p>(i) PASS: accepts the question.</p>
<p>(ii) ADJUST: applies minor wording or fluency improvements; outputs revised question and reasoning path.(iii) REWORKED: performs substantial restructuring; outputs new question, reasoning path, and answer.(iv) REJECTED: discards questions with irreparable flaws.This step guarantees that each question (i) involves cross-document reasoning, (ii) hides the bridge entity (for Bridge questions), and (iii) maintains fluency without exposing intermediate steps.</p>
<p>We conducted an ablation study to validate the effectiveness of the Polisher module, the results of which are presented in Appendix C.1.2.</p>
<p>Data Quality Evaluation System</p>
<p>Evaluating the quality of multi-hop questions requires a comprehensive evaluation system that extends beyond traditional metrics.We introduce a three-dimensional evaluation system designed to capture the critical attributes of high-quality MHQA datasets.</p>
<p>LLM-as-Judge Evaluation</p>
<p>Our evaluation employs an LLM-as-judge approach with a Likert scale to evaluate each synthesized MHQA pair.Building on recent advances in question generation evaluation metrics and LLMbased assessment (Fu et al., 2024b;Liu et al., 2023), we establish a novel scoring framework tailored for multi-hop questions (detailed in Appendix F).</p>
<p>While LLM judges offer scalability, they require careful validation, as discussed in Section 2.3.We therefore prioritize self-consistency over human alignment as our primary reliability criterion, ensuring stable and reproducible evaluations (see Appendix F.2 for detailed rationale).</p>
<p>To identify suitable LLM judges, we evaluated multiple models based on output stability.Results show proprietary LLMs like GPT-4o demonstrate strong performance across metrics, while opensource models such as Gemma-3-27b offer stable, cost-effective evaluation with better reproducibility.See Appendix F.3 for complete metrics, model specifications, selection rationale, and results with visualizations.We adopted the average score across selected judges as our final evaluation standard.To validate this approach, a pairwise human study confirmed a 92% agreement with our judges' rankings (detailed in Appendix C.3).</p>
<p>Answerability and Difficulty Evaluation</p>
<p>To evaluate the answerability and difficulty of our synthesized questions and their reliance on contextual evidence, we use multiple LLM solvers under two distinct conditions: • Q-Only: The solver sees only the question.This setting primarily gauges the baseline answerability using the solver's internal knowledge and reasoning capabilities.</p>
<p>• Q+Docs: The solver receives all supporting documents for the question, simulating a golden retrieval scenario.This setting evaluates the question's answerability when all necessary evidence is available.The performance improvement from the Q-Only to the Q+Docs indicates that: (i) the question is challenging and requires contextual evidence rather than just pre-existing knowledge or superficial cues; and (ii) the LLM-annotated golden evidence effectively supports a correct answer, confirming the question is answerable.These features are key signs of well-constructed multi-hop questions that test evidence integration.</p>
<p>Evidence-Accessiblity Evaluation</p>
<p>We examine whether annotated evidence for our synthesized question evidence is accessible in the corpus and evaluate the difficulty of its complete retrieval.Distinct retrieval methods are employed to fetch the top-k documents for each question and record retrieval metrics: (i) MAP (mean average precision), (ii) RECALL@K (proportion of golden evidence retrieved in top-k), (iii) NDCG@K (normalized discounted cumulative gain at k), and (iv) SUPPORT F1 (overlap between retrieved and golden evidence).</p>
<p>This comprehensive evaluation approach uses the recorded metrics to achieve two primary objectives: (i) to gauge the accessibility of individual evidence documents (using RECALL@K, e.g., @20), which is crucial for verifying corpus grounding and evaluating retrieval ranking quality (via MAP and NDCG@K); and (ii) to identify specific difficulties in multi-source evidence assembly, indicated by SUPPORT F1 (complete-set retrieval accuracy) relative to individual document recall (RECALL@K).This detailed analysis provides a vital retrieval baseline for our dataset, enabling a clearer interpretation of MHQA performance by clearly distinguishing retrieval challenges from reasoning demands.</p>
<p>Experiments</p>
<p>HopWeaver is evaluated using the most popular English Wikipedia corpus and four LLM generators with different scales and performances for synthesis.We compare the synthesized question with three human-annotated MHQA datasets, providing a comprehensive statistical comparison in Appendix B. See Appendix H for the complete ex-perimental setup and Appendix D for cost analysis.Our quality evaluation results in Table 1 contain the proportion of authentic multi-hop questions and their average scores.When employing the proprietary LLM Gemini-2.5-flash,HopWeaver achieves exceptional performance (98.6% multihop rate and 4.47 average score for Comparison questions; 96.4% and 4.27 for Bridge questions), surpassing all evaluated human-annotated datasets.This demonstrates HopWeaver's capacity to produce data that advance MHQA research.</p>
<p>Main Quality Evaluation</p>
<p>To evaluate HopWeaver's performance with more accessible and reproducible setups, we also tested three leading open-source LLMs of varying scales (QwQ-32B, Qwen-14B, GLM-4-9B-0414).These models also enable HopWeaver to synthesize high-quality questions that rival or exceed human datasets; the authenticity of this multi-hop nature is substantiated by a manual evaluation showing that the vast majority of the reasoning paths are correct (Appendix C.4).For instance, QwQ-32B achieves a 98.9% multi-hop rate and a 4.23 average score for Bridge questions, and the statistical stability of these quantitative results is confirmed by a largerscale evaluation on 500 samples (Appendix C.2).Even the smaller GLM-4-9B-0414 yields a high percentage of valid multi-hop questions (89.8% for Bridge, 93.9% for Comparison), offering a costeffective solution for large-scale synthesis.</p>
<p>A multi-dimensional analysis is conducted to compare the average scores across key quality dimensions for HopWeaver-synthesized questions (using different LLMs) against human datasets.Figure 4 shows that the question synthesized by Hopweaver surpasses the benchmark of human datasets in most dimensions, especially in logical sophistication and information integration, although the top human dataset holds a marginal advantage in conciseness.</p>
<p>Fluency</p>
<p>Answerability and Difficulty Evaluation</p>
<p>We evaluate various LLMs in both "Question-Only" (Q-Only) and "Question + Golden Documents" (Q+Docs) settings, as described in Section 4.2.The results, presented in Table 2, demonstrate a significant performance improvement across all models when the golden supporting documents are provided.For instance, GPT-4o's Exact Match (EM) score rose from 29.0% to 51.0% on Bridge questions and jumped from 69.0% to 94.0% on Compare questions (Q-Only to Q+Docs).</p>
<p>This substantial gain confirms two critical aspects aligned with our evaluation goals: (i) the questions involve multi-hop reasoning and are difficult to answer based solely on the models' internal knowledge; and (ii) the golden documents synthesized by HopWeaver are effective and contain the necessary information for models to deduce the correct answers, confirming the questions' answerability given appropriate evidence.This performance gap between the Q+Docs setting and ground truth reflects the task's inherent reasoning complexity, a conclusion supported by our detailed error analysis (Appendix C.5), which shows that failures stem mostly from model reasoning limitations, not question quality flaws.</p>
<p>Most notably, smaller models like Qwen3-8B</p>
<p>show substantial performance gains, with its F1 score increasing dramatically from 22.3% to 60.2% on bridge questions when gold documents are included.This clearly indicates that models with less extensive internal knowledge can still perform robust reasoning when relevant contextual evidence is properly provided by HopWeaver.Furthermore, the consistent ranking of model performance suggests that our dataset serves as a reliable benchmark for evaluating and comparing the reasoning capabilities of different LLMs in multi-hop scenarios.</p>
<p>Model Bridge</p>
<p>Compare
Q-Only (%) Q+Docs (%) Q-Only (%) Q+Docs (%) EM F1 EM F1 EM F1 EM F1 GPT-4o29</p>
<p>Evidence-Accessiblity Evaluation</p>
<p>We evaluated the synthesized dataset (by Gemini-2.5-flash)using three retrievers.Our evaluation requires retrievers to identify the exact document sources used to synthesize each question.</p>
<p>The evidence-accessibility evaluation (Table 3) highlights several key findings regarding the dataset: (i) evidence documents are largely accessible by retriever within the corpus, as demonstrated by Recall@k (e.g., BM25 Recall@20 of 0.7050), affirming the questions' strong corpus-grounding; and (ii) retrieving the complete set of necessary evidence documents simultaneously remains challenging (e.g., low Support F1: 0.17-0.22),despite individual document accessibility.This disparity underscores the genuine multi-hop nature of the questions, which necessitates integrating information from multiple, distinct sources; and (iii) the dataset effectively discriminates between retrieval strategies (e.g., BM25 &gt; GTE &gt; E5), demonstrating its utility as a benchmark for evaluating multi-hop retrieval systems.</p>
<p>Method MAP Recall@5 Recall@10 Recall@20 NDCG@5 NDCG@10 Support F1</p>
<p>End-to-End Evaluation on RAG Systems</p>
<p>To further validate the complexity of our synthesized dataset, we conduct an end-to-end performance evaluation on several mainstream RAG systems.For a fair and direct comparison, we benchmark five representative RAG methods on our dataset using the standardized pipeline from the FlashRAG toolkit (Jin et al., 2025) and compare our results against its published scores on HotpotQA and 2WikiMultiHopQA.</p>
<p>The results, presented in Table 4, reveal several key findings.First, consistently low F1 scores, on par with established benchmarks, confirm our dataset presents a significant challenge to RAG systems.More importantly, advanced RAG methods often failed to outperform the standard RAG, sometimes showing significant performance drops on our dataset.This outcome suggests that while these sophisticated retrieval strategies are highly effective on established benchmarks, their performance may be contingent on the specific reasoning patterns prevalent in those datasets but not generalize.Herein lies the value of HopWeaver: its ability to synthesize benchmarks with a broader diversity of question structures and contexts.The failure of advanced RAG methods on our dataset is direct evidence that such diversity is crucial for identifying their limitations and guiding the development of more robust, generalizable systems.</p>
<p>Conclusion</p>
<p>We presented HopWeaver, a fully automatic framework for synthesizing authentic multi-hop questions (bridge and comparison) from raw corpora.</p>
<p>Our experiments demonstrate that HopWeaver meets or exceeds human-level benchmarks across multiple evaluation dimensions and scales effectively with various LLMs.These capabilities make HopWeaver a practical solution for constructing complex MHQA datasets in domains where human annotation is limited.Furthermore, our work provides a valuable tool for the research community; the synthesized questions serve as a challenging benchmark that reveals limitations in prevalent RAG systems, guiding future improvements.</p>
<p>Limitations</p>
<p>The performance of HopWeaver, like other generative frameworks, is inherently linked to the capabilities of its underlying LLM and the quality of the source corpus.While our approach demonstrates robustness across several models, the nuance and complexity of the synthesized questions are ultimately bounded by the reasoning and language generation abilities of the backbone LLM.Similarly, the breadth of topics and the factual accuracy of the generated questions are dependent on the comprehensiveness and cleanliness of the text corpora from which they are derived.Furthermore, as the synthesis process is automated, it may inadvertently inherit and amplify subtle biases present in the source data.Finally, the experiments in this study are confined to the English language, and the framework's effectiveness across different languages has yet to be explored.</p>
<p>A Preliminaries</p>
<p>While MHQA exhibits various patterns (e.g., bridge, comparison, intersection, commonsense), analysis of major benchmarks (Yang et al., 2018;Ho et al., 2020;Trivedi et al., 2022) indicates that two types are fundamental and challenging: bridge questions and comparison questions.Building on existing research in MHQA, we formalize the key concepts used throughout this paper as follows:</p>
<p>A</p>
<p>A.3 Comparison Question</p>
<p>Comparison questions contrast two entities (similar entities of the same category) by identifying their values for a specific relation, shared attribute, denoted as r c ∈ R.This involves establishing attribute triplets for each entity:
t 1 = (e 1 , r c , v 1 ) and t 2 = (e 2 , r c , v 2 ) (7)
Each triplet can be represented by a logical reasoning path:
P = ⟨(e 1 , r 1 , e 2 ), . . . , (e n−1 , r n−1 , e n )⟩ (8)
The paths may differ but must lead to attribute heads associated with r c for effective comparison.</p>
<p>Distributed Source Constraint: A comparison requires multi-hop reasoning if no single document contains both facts t 1 and t 2 .Let D(t) be the set of documents stating triplet t.This means the sets of supporting documents must be disjoint:
D(t 1 ) ∩ D(t 2 ) = ∅ (9)</p>
<p>B Detailed Dataset Statistics</p>
<p>To provide a comprehensive context for our evaluation, this section presents a detailed statistical comparison between questions synthesized by Hop-Weaver and those from three widely-used humanannotated multi-hop QA benchmarks: HotpotQA, 2WikiMultiHopQA, and MuSiQue.The following table (Table 5) summarizes key characteristics, including dataset size, question type distribution, and linguistic properties.This comparison is intended to situate our work within the landscape of existing datasets and to clarify the rationale behind our experimental design for ensuring a fair comparison.</p>
<p>Dataset Interpretation and Evaluation Configuration.The statistics presented in Table 5 highlight several key aspects pertinent to the main evaluation in our paper.HopWeaver is a flexible framework capable of generating high-quality multi-hop questions at scale, offering a significant cost and time advantage over the manual annotation processes that produce static datasets.</p>
<p>For a fair and meaningful comparison in our main quality evaluation (Table 1), we did not compare datasets in their entirety but instead selected specific, comparable question types from each benchmark.This ensures that we evaluate genuinely similar reasoning patterns.The specific configuration was as follows:</p>
<p>• From HotpotQA, we used their designated Bridge and Comparison type questions, which directly align with the two question types synthesized by HopWeaver.</p>
<p>• From 2WikiMultiHopQA, we selected their Comparison and Compositional questions.</p>
<p>Their "Compositional" questions, which require chaining facts, serve as a strong analogue to our "Bridge" questions, particularly in their emphasis on avoiding reasoning shortcuts.</p>
<p>• From MuSiQue, which is structured by hop count rather than explicit reasoning type, we used their 2-hop questions as a proxy for the "Bridge" type, as it is the most common form of bridge reasoning.MuSiQue does not offer a distinct set of comparison questions.</p>
<p>Hop Distribution.All questions evaluated in our study are 2-hop, with the exception of MuSiQue, which also includes a smaller number of 3-hop and 4-hop variants.Our focus on the prevalent 2-hop structure allows for a robust validation of our core synthesis methodology across established datasets.</p>
<p>C Additional Experimental Validation</p>
<p>This appendix provides supplementary experimental results that substantiate the claims made in the main paper.We first present an ablation study justifying our key architectural choices.We then provide a large-scale validation of our primary findings, a human-centric evaluation of our LLM-asjudge framework, a manual assessment of the synthesized reasoning paths, and a detailed error analysis of QA failures.</p>
<p>C.1 Ablation Study</p>
<p>C.1.1 Reranker Efficiency</p>
<p>To evaluate our fine-tuned reranker's efficiency (in Section 3.3), we compare retrieval strategies for Bridge question synthesis using two metrics: Success Rate (percentage of documents yielding valid questions) and Average Attempts for Success (attempts needed to find successful pairs).Table 6 reveals progressively stronger results from standard dense retrieval ('standard') and MMR diversity ('diverse') to zero-shot reranking ('diverse + rerank (ZS)').The optimal performance comes from our fine-tuned reranker ('diverse + rerank (FT)'), delivering both superior success rates and requiring fewer document retrieval attempts.This demonstrates that fine-tuning substantially improves question synthesis efficiency and reduces computational costs.</p>
<p>C.1.2 Polisher Module Effectiveness</p>
<p>We investigate the contribution of the Polisher module (Section 3.4) to the final question quality.We   use LLM-as-judge to compare the quality assessment of questions between directly synthesized by LLMs ('Original') and the questions after being processed by the Polishing module ('Polished').</p>
<p>Table 7 shows that the Polisher module improves both the multi-hop validity rate and the average quality score for both Bridge and Comparison questions.This demonstrates the importance of the refinement and validation step in ensuring highquality synthesized data.Notably, our analysis reveals a differentiated impact based on the capabilities of the generator LLMs.For stronger models like Gemini-2.5-flash, the Polisher's improvements are modest, while smaller models such as GLM-4-9B exhibit more substantial gains (e.g., bridge question score rising from 3.71 to 3.87)</p>
<p>This highlights the Polisher module's value in a resource-optimized pipeline, enabling smaller generator models to achieve high-quality outputs through targeted refinement rather than scaling up model parameters.</p>
<p>C.2 Large-Scale Validation</p>
<p>To address potential concerns about the sample size in our main evaluation and to verify the statistical stability of our results, we conducted a large-scale validation.We generated 500 additional samples using our QwQ-32B model and sampled 500 corresponding questions from the HotpotQA dataset for comparison.The evaluation was performed using the same LLM-as-judge framework described in Section 4.1.</p>
<p>The results, presented in Table 8, closely mirror the findings from our main evaluation (Table 1).This consistency across a larger sample size demonstrates the statistical reliability of our evaluation methodology and reinforces our conclusion that HopWeaver consistently generates high-quality multi-hop questions.</p>
<p>C.3 Human Validation of LLM-as-Judge</p>
<p>To substantiate the reliability of our LLM-as-judge framework, we conducted a pairwise human validation study to measure its alignment with human expert judgment.We selected 100 questions from our evaluation set and created 50 pairwise comparisons, ensuring that the LLM score difference between the paired questions was greater than 0.3 to make the comparison non-trivial.Three Master's students in Computer Science, serving as human evaluators, were asked to choose the better multi-hop question in each pair based on our established evaluation criteria.The final agreement metrics from this study are detailed in Table 9.</p>
<p>Metric Value</p>
<p>Human-LLM Alignment Majority Agreement (LLM vs. Human Majority) 94% Complete Agreement (LLM vs. Human Unanimity) 62%</p>
<p>Inter-Human Reliability</p>
<p>Fleiss' Kappa (Among 3 Human Raters) 0.46</p>
<p>Table 9: Human validation metrics for the LLM-asjudge framework across 50 pairwise comparisons.</p>
<p>Using majority voting to determine the final human preference, we found a high level of agreement, with the LLM's ranking aligning with the human consensus in 94% of cases.This robust alignment between our automated assessment and human expert judgment validates the reliability of our evaluation methodology for comparing multihop question quality.</p>
<p>C.4 Manual Evaluation of Reasoning Paths</p>
<p>To directly verify that our synthesized questions necessitate authentic multi-hop reasoning, we performed a manual evaluation of the reasoning path correctness.We randomly sampled 100 questions generated by QwQ-32B and manually inspected their reasoning paths, which connect the source documents via the bridge entity.</p>
<p>The analysis, detailed in Table 10, reveals that 92% of the generated reasoning paths are correct and logically sound.The primary source of minor errors was "Information Gap," where the path was factually accurate but relied on information just outside the explicitly provided evidence segments.This manual verification confirms that Hop-Weaver's synthesis process generates high-quality questions that are structurally sound and genuinely require multi-hop reasoning.</p>
<p>Error Type Count Percentage</p>
<p>Correct 92 92% Information Gap Error 6 6% Ambiguity Issue 1 1% Factual Reasoning Error 1 1%</p>
<p>C.5 Error Analysis of QA Failures</p>
<p>To understand the nature of the task difficulty presented by our dataset, we conducted a detailed case-by-case analysis of QA failures.We examined all 49 cases where the GPT-4o model failed to produce an exact match (EM=0) for our generated bridge questions (from a set of 100, where the overall EM was 0.51 in table 2. Our comprehensive examination reveals three categories of failures:</p>
<p>• Category 1: Correct Answers with Format Variations (18 cases, 36.7%).In these instances, the model provided a factually correct answer that did not achieve a perfect string match due to minor variations in phrasing, punctuation, or completeness (e.g., providing the full name when only the last name was required).</p>
<p>• Category 2: Logical Reasoning Errors (29 cases, 59.2%).This was the largest category of failures, representing instances where the model's logical reasoning was flawed.This highlights the complexity of the questions and the inherent challenges of multi-hop reasoning for current LLMs.</p>
<p>• Category 3: Insufficient Evidence Segments (2 cases, 4.1%).In these rare cases, the provided evidence snippets were not comprehensive enough to fully answer the questions, although the questions themselves were factually correct and answerable with complete context.</p>
<p>This analysis demonstrates that the vast majority of failures are attributable to either minor format variations or inherent model reasoning limitations, rather than flaws in the synthesized questions.Therefore, the observed 50% EM performance reflects the genuine challenge of the multi-hop reasoning task rather than indicating issues with question answerability or dataset quality.</p>
<p>D Cost Analysis</p>
<p>The tests were conducted on gemini-2.5-flashpreview-04-17(the most expensive model we have tested).The following Table 11 summarizes the token consumption for question generation and LLMas-Judge evaluation (on GPT-4o).For closed-source models, API calls can be made via their respective platforms.For example, based on the consumption data in this table, synthesizing 1000 multi-hop questions (7600 times requests) using Gemini 2.5 Flash (assuming API pricing of $0.15 per million input tokens and $3.50 per million output tokens) would cost approximately $7.90.Similarly, using GPT-4o (assuming API pricing of $2.5 per million input tokens and $10 per million output tokens) as a single evaluation model to evaluate 1000 synthesized questions would cost approximately $27.72.For open-source models, we utilized a setup with 4x A100 (40GB VRAM) GPUs for deployment and inference with bf16 precision.For some open-source models, due to considerations of complex deployment engineering efforts, we used APIs via OpenRouter while still ensuring high reproducibility.</p>
<p>The result implies that the cost of synthesizing a dataset with thousands of entries is significantly lower than manual annotation; if small-scale opensource models are used, these generation costs can be almost negligible.Furthermore, if multiple LLMs are required for large-scale evaluation of dataset quality, the corresponding computational power or financial costs must also be taken into consideration.</p>
<p>E Examples of Synthesized Questions E.1 Bridge Question Example</p>
<p>Figure 5 shows an example of a synthesized bridge question detailing the involved reasoning path and source information.</p>
<p>E.2 Comparison Question Example</p>
<p>An example of a comparison question is presented in Figure 6, highlighting the entities and attributes under comparison.</p>
<p>F Evaluation Criteria for LLM-as-Judge F.1 Pointwise Scoring Framework</p>
<p>To assign an absolute quality score to each synthesized question-answer pair and ensure a rigorous, multi-faceted evaluation, we employ a pointwise scoring approach.This method allows for the independent evaluation of each item against a predefined set of criteria by an LLM judge (Liu et al., 2023;Fu et al., 2024a).We opted for pointwise scoring over pairwise comparison (Liusie et al., 2024) because our synthesized questions are nonparallel and vary significantly in difficulty, making it challenging to establish fair comparative benchmarks necessary for pairwise approaches.The detailed criteria for our pointwise evaluation are organized into three main categories:</p>
<p>• Multi-Hop QA Rule Dimension: This is a binary (Yes/No) evaluation determining if the question authentically involves reasoning across multiple documents, where information from one document is necessary to understand or utilize information in another, and the answer cannot be derived from any single document.This dimension is paramount; a "No" indicates a fundamental failure.</p>
<p>• Linguistic Dimensions: These evaluate the quality of question presentation, ensuring understandability and precision.Criteria include:</p>
<p>-Fluency: Grammatical correctness and coherence.-Clarity: Unambiguous and precise expression.</p>
<p>-Conciseness: Absence of redundant information.</p>
<p>• Task-Oriented Dimensions: These evaluate the functional and logical aspects of the Particularly, dimensions such as Consistency, Information Integration Ability, and Logical Sophistication are critical.Flaws in these areas are heavily penalized, reflecting their significance in ensuring the quality of authentic multi-hop questions.</p>
<p>For scoring the Linguistic and Task-oriented dimensions, a Likert-like scale (Very Poor, Poor, Fair, Good, Very Good) is employed.However, the LLM judge is instructed to adopt a skeptical default stance and interpret these scale points with heightened strictness, as summarized below, to minimize subjective bias and ensure only high-quality items receive favorable scores (Liu et al., 2023):</p>
<p>• Very Poor (Unacceptable): Fundamentally flawed (e.g., not truly multi-hop, severe contradictions, unanswerable).</p>
<p>• Poor (Weak/Barely Usable): Obvious, major flaws requiring significant revision (e.g., weak/forced logic, inconsistencies).</p>
<p>• Fair (Acceptable/Passable): Basic requirements met but with notable flaws or room for improvement; signifies minimum adequacy only, not a positive endorsement.</p>
<p>• Good: Well-designed, logically clear, fluent, and meets multi-hop criteria without obvious flaws.</p>
<p>• Very Good (Excellent/Outstanding): Exemplary design with deep logic, precision, and rigor.</p>
<p>This stringent evaluation mechanism, with a directive to assign lower ratings ('Poor' or 'Very Poor') when significant flaws are present (especially logical ones), effectively filters out low-quality or trivially multi-hop questions.A question with significant logical flaws cannot achieve a 'Good' or 'Very Good' rating overall, even if linguistically sound.</p>
<p>F.2 Beyond Human Alignment: The Case for LLM Self-Consistency</p>
<p>Human judgments, while valuable, exhibit limitations that challenge their role as the sole benchmark for aligning LLMs.First, inter-rater reliability in subjective tasks is often low; for example, human evaluations of dialogue quality show poor agreement (e.g., Krippendorf's α = 0.33 on Per-sonaChat, indicating fair agreement; α = 0.08 on WMT 2020 Zh-En, indicating slight agreement; α = 0.49 on QAGS, indicating moderate agreement, (Bavaresco et al., 2024)).Similarly, in MHQA tasks, which often span multiple domains and require complex reasoning, human judgments are likely to be inconsistent due to the subjective nature of criteria like question clarity, relevance, and logical sophistication.Second, human ratings are prone to systematic biases-such as fatigue, cultural preferences, or contextual misunderstandings-which introduce variability and undermine evaluation reliability.Indeed, research into human perceptions of LLM outputs reveals that factors unrelated to intrinsic quality, such as perceived LLM sentience or anthropomorphism, can influence human evaluations (Lee et al., 2025;Zheng et al., 2023;Chiang and Lee, 2023).</p>
<p>Furthermore, much of the current work on LLMas-judge focuses on achieving high correlation with human preferences or ratings (Ye et al., 2024).While aligning with human intuition is a desirable goal, an overemphasis on mimicking human scores can inadvertently lead LLM judges to replicate the aforementioned human biases and inconsistencies.If human agreement itself is a noisy or unstable signal, then LLM judges optimized solely for human-LLM consistency may inherit these limitations rather than serving as a more objective or stable evaluation instrument.This is particularly problematic when the goal is to create a scalable and reliable evaluation framework (Lee et al., 2025;Zheng et al., 2023).</p>
<p>Given these constraints, we contend that while human feedback remains a crucial component in the broader LLM development lifecycle, pursuing perfect alignment between LLM judge scores and raw human judgments as the primary evaluation metric for the judge itself is neither always feasible nor universally desirable for all evaluation tasks.Instead, we propose prioritizing the selfconsistency of LLMs-defined as their ability to deliver stable, reproducible outputs for identical inputs under controlled conditions-as a foundational criterion for selecting qualified judge models.This shift towards emphasizing demonstrable reliability in the LLM judge's own behavior ensures a more standardized and robust evaluation framework, mitigating the risk of amplifying the inherent shortcomings of human-based assessments when seeking fine-grained, repeatable quality scores.</p>
<p>F.3 LLM Reliability: Metrics and Evaluation Results</p>
<p>We posit that a trustworthy judge must produce stable outputs under identical conditions.Given each item, we sample N = 5 independent runs at temperature T = 0 to ensure output stability, as lower temperatures are suitable for evaluation tasks.</p>
<p>Metrics To provide a comprehensive evaluation of LLM judge reliability from multiple perspectives, we employ three complementary metrics: Avg.Intra-item SD measures the direct stability of scores, while Krippendorff's Alpha and Fleiss' Kappa evaluate the statistical significance of interrun agreement corrected for chance.</p>
<p>(i) Avg.Intra-item SD measures score volatility for each item across N repeated runs:
SD i = std({s (1) i , . . . , s (N ) i }) (10) AvgSD = 1 M M i=1 SD i(11) where s (j)
i is the score for item i in run j, and M is the total number of items.</p>
<p>(ii) Krippendorff's α (Krippendorff, 2018) treats the N runs as raters and measures agreement beyond chance for various data types:
α = 1 − D o D e(12)
where D o is the observed disagreement and D e is the expected disagreement by chance.</p>
<p>(iii) Fleiss' Kappa (κ) (Fleiss, 1971) measures inter-rater agreement for categorical ratings, evaluating reliability among multiple raters (our N runs):
κ = P − Pe 1 − Pe (13)
where P is the mean proportion of observed agreement among raters, and Pe is the mean proportion of agreement expected by chance.</p>
<p>Based on these reliability metrics, we evaluate several open-source and proprietary LLMs on a held-out subset (M = 100).The model with higher agreement metrics (α, κ) and lower AvgSD values is selected as the LLM judge.To avoid self-enhancement bias (Ferrara, 2023), the chosen judge never scores its own generations or those from close variants.Final LLM Judge Ensemble Considering a balance of evaluation performance (as indicated by the reliability metrics defined earlier in this appendix and further detailed in Table 12 presented below) and operational costs, we selected an ensemble of LLMs to serve as our final judges:</p>
<p>• claude-3-7-sonnet-20250219</p>
<p>• gpt-4o-2024-11-20</p>
<p>• gemini-2.0-flash
• google/gemma-3-27b-it • meta-llama/llama-3.3-70b-instruct
The use of this diverse set of models, including both proprietary and open-source options, aims to provide a robust and comprehensive evaluation, while also considering the reproducibility and accessibility of the evaluation process.The average scores   from this ensemble are used for the final quality evaluation of the synthesized multi-hop questions.</p>
<p>G Entity and Attribute Filtering Mechanism</p>
<p>To ensure the quality and suitability of entities and attributes for synthesizing comparison multihop questions, we employ a filtering mechanism based on evaluating the concreteness of subject entities and the comparability of their attribute values.This process assigns numerical scores on a 1-5 scale, facilitating downstream filtering of less ideal candidates.The detailed criteria, as defined in our COMPARE_ENTITY_FILTER_PROMPT, are summarized below:</p>
<p>G.1 Subject Entity Concreteness evaluation</p>
<p>The concreteness_score evaluates how specific, tangible, and suitable an entity is for direct attribute comparison.The scale is defined as:</p>
<p>• 5 (Highly Concrete): Specific person, place, organization, tangible object, work, or clearly defined historical event (e.g., "Paris", "IBM").Excellent candidate.</p>
<p>• 4 (Concrete): Specific but less common entity types, like a specific named award or law (e.g., "Nobel Prize in Physics").Good candidate.</p>
<p>• 3 (Borderline/Slightly Abstract): Broader but well-defined categories or specific complex relationships (e.g., "Mammal", "World War II").Use with caution.</p>
<p>• 2 (Abstract): General relationships, abstract concepts, fields of study (e.g., "US-China relations", "Democracy").Poor candidate.</p>
<p>• 1 (Highly Abstract): Vague concepts, general feelings, ambiguous terms (e.g., "Happiness").Unsuitable candidate.</p>
<p>G.2 Attribute Comparability Evaluation</p>
<p>The comparability_score evaluates each attribute value based on its suitability for direct and unambiguous comparison.The scale is defined as:</p>
<p>• 5 (Excellent): Precise Dates (YYYY-MM-DD), specific Years, specific Numbers, exact Locations, specific Names, well-defined unambiguous Categories (e.g., Nationality).</p>
<p>• 4 (Good): Specific but slightly less precise numbers (e.g., "1.2 million"), specific office/rank titles.</p>
<p>• 3 (Fair): Broader categories, precise year ranges, specific event names.Potential candidate but less ideal.</p>
<p>• 2 (Poor): Imprecise time (e.g., "Before 1960s"), descriptive reasons, lists.Unlikely suitable.</p>
<p>• 1 (Very Poor):Vague statements, subjective opinions, long text.Unsuitable.</p>
<p>The filtering module processes each entity and its attributes based on these scoring criteria.Entities and attributes that do not meet a predefined threshold (default setting: a minimum score of 5 for subject entities and 4 for attributes) are filtered out before proceeding to the comparison query generation step (see Section 3.2 for their position in the pipeline).This specific 5/4 threshold was adopted based on the evaluation standards of a particular open-source model (Gemma-3-27B-it, as detailed in Appendix H), making it a reasonable choice for our study.Researchers can adjust it to fit different models or specific task requirements.This ensures that only entities with a sufficient level of concreteness and attributes with high comparability are used for synthesizing comparison questions, thereby enhancing the quality and relevance of the synthesized questions.The output format for these scores is a delimited string, with the first part being the entity's concreteness score and subsequent parts detailing each attribute's name, value, and comparability score.</p>
<p>H Experimental Settings</p>
<p>Corpus.We use the widely-adopted English Wikipedia dump from December 20, 2018.This date was chosen to align closely with the Wikipedia snapshots used in the baseline datasets (Hot-potQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), and MusiQue (Trivedi et al., 2022)), ensuring a fair comparison of synthesized question quality under consistent data conditions.We preprocess the corpus by removing articles with more than 4096 words and store the remaining articles in JSONL format.</p>
<p>Generator</p>
<p>LLMs.We employ four LLMs with varying capabilities and parameter sizes for question synthesis pipeline: Gemini-2.5-flash-preview-04-17,QwQ-32B, Qwen-14B, and GLM-4-9B-0414.Preliminary experiments showed that older or smaller models often lacked the capability to perform the complex generation steps.To prevent potential self-enhancement bias (Ferrara, 2023), these generator LLMs are deliberately kept distinct from those LLMs that constitute our LLM-as-judge evaluation system.</p>
<p>We also note that the outputs of closed-source models can fluctuate over time, potentially impacting reproducibility, an effect we observed with the Gemini model during our experimental period.</p>
<p>LLM-as-judge Models.The details are shown in Appendix F.3.</p>
<p>Embedding and Reranker Models.For initial retrieval (coarse retrieval and MMR calculation) during the synthesis pipeline, we uniformly use the gte-multilingual-base embedding model.For the reranking stage (Step 2 in Section 3.1), we use BAAI/bge-reranker-v2-m3.For the retrievalbased dataset Evaluation (Section 4.3), we evaluate using gte-multilingual-base, E5-base-4k, and BM25.Our retrieval implementation is based on the FlashRAG (Jin et al., 2024).</p>
<p>Parameters for Coarse Retrieval In the coarse retrieval stage for bridge question synthesis (Section 3.1, Step 2), the MMR-like scoring function utilizes three trade-off parameters.We empirically set these parameters as follows: λ 1 = 0.87 (emphasizing query relevance), λ 2 = 0.03 (penalizing similarity to the source document), and λ 3 = 0.1 (promoting diversity among selected documents).These values are determined through preliminary experiments to balance the objectives of relevance, novelty, and diversity in the retrieved complementary documents.</p>
<p>Comparison Datasets.We compare the quality of HopWeaver-synthesized questions against three established human-annotated multi-hop QA datasets: HotpotQA (Yang et al., 2018), 2WikiMul-tiHopQA (Ho et al., 2020), and MusiQue (Trivedi et al., 2022) Evaluation LLM.For the QA-based dataset Evaluation (Section 4.2), we use GPT-4o, Gemini-2.0-flash,Qwen3-9B, Llama-3.3-70B to obtain answers from LLMs with different performance levels.</p>
<p>Polisher LLM.For the Polisher module experiments (Section 4.2), we use DeepSeek-R1 to get an effective supervision signal.</p>
<p>Filter LLM.For the Filter module experiments (Step 2 in Section 3.2), we use Gemma-3-27B-it to get a stable rating threshold, which avoids fluctuations in question quality caused by different LLMs' standards when picking up entities and attributes.</p>
<p>Default Generation Parameters.For all LLMs in our experiments, we use deterministic generation settings (temperature=0, do_sample=False) with top_p=0.9 and max_tokens=8192 to ensure reproducibility while accommodating complex reasoning chains required for multi-hop question synthesis.</p>
<p>RAG System Settings.For the end-to-end RAG system evaluation (Section 5.4), we adopt a unified configuration to ensure a fair comparison.The generator component utilizes the LLAMA3-8B-Instruct model with a maximum input length of 2048 tokens.The retriever employs the e5-base-v2 embedding model, configured to retrieve the top 5 documents for each query.To maintain consistency, all experiments use a default prompt that aligns with the standard practices of the FlashRAG benchmark.</p>
<p>I Prompt Settings</p>
<p>Considering that this work involves a substantial number of prompts, each with a relatively lengthy original format, we have made appropriate simplifications while retaining the essential information from the original prompts.Readers interested in the complete original prompts could refer to the provided code file.</p>
<p>Bridge Entity Extraction Prompt (ENTITY_EXTRACTION_PROMPT)</p>
<p>Goal</p>
<p>Given a text document, select a single segment with high potential to contain a bridge entity for multi-hop question generation, identify one bridge entity from that segment, extract relevant text segments, and generate an expanded query statement for this bridge entity to retrieve related documents from a vector database.Instructions 1. Select a Segment and Identify a Bridge Entity -Select a text segment with high potential for containing a bridge entity, then identify one bridge entity.-Principles: -High Connectivity: The entity has multiple associations with other entities.-Uniqueness and Clarity: The entity is clearly defined within the segment.-Attribute Richness: The entity has multiple queryable attributes.</p>
<p>-Cross-Document Distribution: Information likely spread across documents.-Distinct from Title: The bridge entity must not be identical to the document title.-Format: ("bridge_entity" &lt;|&gt; "entity_name" &lt;|&gt; "entity_type") 2. Extract Relevant Text Segments -Extract a single part of the document that directly mentions or describes the entity.-Provide a brief introductory sentence followed by the extracted segment.-Format: ("relevant_segments" &lt;|&gt; "entity_name" &lt;|&gt; "entity_introduction + extracted_part") 3. Generate an Expanded Query Statement -Generate a query to find COMPLEMENTARY information about the entity.-Use semantic direction shifting phrases like "instead of," "beyond," etc. -Format: ("query" &lt;|&gt; "entity_name" &lt;|&gt; "entity_query") 4. Return Output -Return a single list with the bridge entity, relevant segments, and expanded query.</p>
<p>Goal</p>
<p>Conduct a rigorous and critical evaluation of multi-hop questions and their answers across multiple quality dimensions.Focus on ensuring questions require genuine cross-document reasoning and are free from logical flaws.A high-quality multi-hop question necessitates reasoning that flows between documents, where information from one document provides context for another, and the answer must be impossible to determine using any single document in isolation.Instructions You are a strict and discerning Multi-Hop Question Answering (MHQA) dataset quality assessment expert.Evaluate the given multi-hop question and its answer across key dimensions in three categories.Apply rigorous scrutiny and do not hesitate to assign lower ratings if flaws are present, especially logical ones.1. Multi-Hop QA Rule Dimension -Multi-Hop Reasoning Requirement: Does the question genuinely require reasoning across multiple documents?(Yes/No) 2. Linguistic Dimensions (Rate as: Very Poor, Poor, Fair, Good, Very Good) -Fluency: Is the question grammatically correct, coherent, and easy to understand? -Clarity: Is the question clearly and precisely expressed without ambiguity?-Conciseness: Is the question concise without redundant information?3. Task-Oriented Dimensions (Rate as: Very Poor, Poor, Fair, Good, Very Good) -Relevance: Is the question relevant to the given passages and asking for key information?-Consistency: Is the information in the question completely and strictly consistent with the provided passages?-Question Answerability: Can the question be unambiguously answered based solely on the given passages?-Answer-Question Consistency: Does the provided answer completely and accurately address the question?-Information Integration Ability: Does the question coherently integrate information from multiple documents without forcing unnatural connections?-Reasoning Path Guidance: Does the question guide the answerer through a multi-step reasoning process?-Logical Sophistication: Does the question demonstrate non-trivial and sound design that requires multi-step thinking and is free from logical gaps or fallacies?Critical Scoring Guidance: -Penalize Logical Flaws Heavily: Pay close attention to Consistency, Logical Sophistication, and Information Integration Ability.-Multi-Hop Requirement is Paramount: If this requirement is "No," the question fundamentally fails.-Clarification on 'Fair': A 'Fair' rating signifies only basic adequacy and is not a positive endorsement.Rating Scale Interpretation: -Very Poor: Unacceptable quality with serious functional/logical errors -Poor: Weak/Barely Usable quality with obvious, major flaws -Fair: Acceptable/Passable quality meeting basic requirements with clear flaws -Good: Standard good quality, well-designed without obvious flaws -Very Good: Excellent/Outstanding quality with clever, rigorous design Output Format:</p>
<p>-Multi-Hop Reasoning Requirement: {yes/no} -Fluency: {rating} -Clarity: {rating} -Conciseness: {rating} -Relevance: {rating} -Consistency: {rating} -Question Answerability: {rating} -Answer-Question Consistency: {rating} -Information Integration Ability: {rating} -Reasoning Path Guidance: {rating} -Logical Sophistication: {rating} &lt;|COMPLETE|&gt; Compare Entity Extraction Prompt (COMPARE_ENTITY_EXTRACTION_PROMPT)</p>
<p>Goal</p>
<p>Given a text document, identify its primary subject entity and extract multiple key attributes associated with this entity, along with their corresponding values.For each extracted attribute, generate an expanded query statement designed to retrieve documents about other similar entities that also possess this attribute, facilitating subsequent comparison.Instructions 1. Identify the Primary Subject Entity -Determine the main person, place, organization, event, concept, or work that the document is primarily about.</p>
<p>-Determine the subject_entity_name (capitalized) and its general subject_entity_type. 2. Extract Comparable Attributes, Values, and Generate Queries -Identify multiple (aim for 3-5 if possible) distinct attributes associated with the primary subject entity.</p>
<p>-Focus strictly on attributes whose VALUES are suitable for comparison.Prioritize attributes that meet these criteria: -Concise &amp; Factual Value: Short value (e.g., name, number, date, category, location).</p>
<p>-Common Data Types: Prefer Numbers, Dates, Locations, Specific Names, Defined Categories.</p>
<p>-Likely Commonality: Prefer attributes likely to exist for other similar entities.</p>
<p>-For each identified comparable attribute: -Determine the attribute_name (e.g., "Population", "Date of Birth").</p>
<p>-Extract the attribute_value (e.g., "1.2 million", "1990-05-15").</p>
<p>-Generate an entity_b_query: A concise query to find other entities with the same attribute.3. Output Format Specification -Subject Entity Part: ("subject_entity"&lt;|&gt;"subject_entity_name"&lt;|&gt; "subject_entity_type") -Attribute Parts: ("attribute"&lt;|&gt;"attribute_name"&lt;|&gt;"attribute_value" &lt;|&gt;"entity_b_query") -Use ## as delimiter between parts.</p>
<p>-Append &lt;|COMPLETE|&gt; at the end.</p>
<p>Figure 1 :
1
Figure 1: Examples of two multi-hop questions synthesized by HopWeaver: Bridge (top) and Comparison (bottom) question.These involve cross-document reasoning via a bridge entity or a shared attribute.</p>
<p>Figure 2 :
2
Figure 2: HopWeaver: Question Synthesis Framework</p>
<p>Figure 3: Fine-Tuning Reranker</p>
<p>Figure 4 :
4
Figure 4: Multi-dimensional quality evaluation to compare HopWeaver-synthesized questions (by different LLMs) against baseline datasets across key criteria, with scale truncated to [3.0, 4.5] for visualization clarity (full scale: [1, 5]).</p>
<p>.0 40.5 51.0 65.0 69.0 70.5 94.0 94.2 Claude-3.7-Sonnet23.0 33.0 50.0 62.0 47.0 50.2</p>
<p>.1 Core Notation Let E = {e 1 , e 2 , ..., e n } be the entity set, R = {r 1 , r 2 , ..., r m } be the relation set, and D = {d 1 , d 2 , ..., d l } be the document set.For each document d j ∈ D, we define the function:T rips(d j ) = {t = (e a , r h , e c )|t in d j } (3)where each triplet t = (e a , r h , e c ) represents a relational fact extracted from document d j .A.2 Bridge QuestionBridge questions link facts across documents through intermediate entities, necessitating crossdocument reasoning.It aims to find a single sequential path P connecting a start entity e s to a target entity e t .P is a sequence of k triplets (e i , r i , e i+1 ) that form a path from e 1 to e k : P = ⟨(e 1 , r 1 , e 2 ), ..., (e k−1 , r k−1 , e k )⟩ (4) This framework imposes two constraints: Fact Distribution Constraint: Each triplet in the path must come from exactly one document.∀(ei , r i , e i+1 ) ∈ P, ∃!d j ∈ D : (e i , r i , e i+1 ) ∈ T rips(d j )(5)No-Shortcut Constraint: No single document can bridge non-adjacent entities in the path.∀i, j : |i − j| &gt; 1, ∀d ∈ D, ∀r ∈ R : (e i , r, e j ) / ∈ T rips(d)</p>
<p>Figure 5: An example of a bridge question synthesized by HopWeaver.The figure illustrates the source documents, the identified bridge entity, the reasoning steps, and the final generated question-answer pair.</p>
<p>Figure 6 :
6
Figure 6: An example of a comparison question synthesized by HopWeaver.This showcases the two entities being compared, the specific attribute, the source evidence snippets, and the resulting question.</p>
<p>-l la m a /l la m a -3 .3-7 0 b -i n s tr u c t m e ta -l la m a /l la m a -4 -m a v e ri c k m is tr a la i/ m is tr a l-s m a ll-3 .1 -2 4 b -i n s tr u c t n v id ia /l la m a -3 .3-n e m o tr o n -s u p e r-4</p>
<p>Figure 7 :
7
Figure 7: Heatmap visualizing AvgSD scores across different LLMs and evaluation dimensions.</p>
<p>Figure 8 :
8
Figure 8: Heatmap visualizing Krippendorff's Alpha scores across different LLMs and evaluation dimensions.</p>
<p>p s e e k /d e e p s e e k -c h a t-v 3 -0 3 2 4 g e m in i-2 .0-f la s h g o o g le /g e m m a -3 -2 7 b -i t g p t-4 o -2 0 2 4 -1 1 -2 0 m e ta -l la m a /l la m a -3 .3-7 0 b -i n s tr u c t m e ta -l la m a /l la m a -4 -m a v e ri c k m is tr a la i/ m is tr a l-s m a ll-3 .1 -2 4 b -i n s tr u c t n v id ia /l la m a -3 .3-n e m o tr o n -s u p e r-4</p>
<p>Figure 9 :
9
Figure 9: Heatmap visualizing Fleiss' Kappa scores across different LLMs and evaluation dimensions.</p>
<ol>
<li>When Finished -Output &lt;|COMPLETE|&gt; Sub-Question Generation Prompt (SUB_QUESTION_GENERATION_PROMPT) Goal Analyze two documents connected by a bridge entity and generate two sequential sub-questions that form a multi-hop reasoning chain.Instructions Analyze how the bridge entity connects both documents by: -Identifying key information about the bridge entity in Document A that is unique to Document A. -Finding related information in Document B that connects via this bridge entity.-Determining a clear reasoning path from Document A to Document B. -If no valid bridge connection exists, return INVALID_BRIDGE_CONNECTION with explanation.Generate two sequential sub-questions: -Sub-question 1: A question about Document A where the answer is the bridge entity.-Sub-question 2: A question that explicitly uses the bridge entity to find related information in Document B. Each sub-question must: -Be answerable from only one document.-Have a definitive answer contained in its document.-Be phrased as a standalone question without document references.-Be specific with clear references to information in its document.-Provide a clear, concise answer.-Together form a logical reasoning chain.Output Format If no valid bridge connection exists: INVALID_BRIDGE_CONNECTION Reason: [Brief explanation] If valid bridge connection exists: ANALYSIS: Bridge connection: [How the bridge entity connects the documents] Document A segments: [Copy of the original Document A segments] Document B segments: [Relevant excerpts from Document B] Reasoning path: [Logical path from Document A to Document B] SUB-QUESTIONS: Sub-question 1: [Question about Document A] Answer 1: [Answer from Document A -about the bridge entity] Sub-question 2: [Question using bridge entity to find answer in Document B] Answer 2: [Answer from Document B] Bridge MHQA Quality Assessment Prompt (MHQA_QUALITY_ASSESSMENT_PROMPT)</li>
</ol>
<p>Table 1 :
1
Quality evaluation of multi-hop questions (on 100 samples, five LLM judges).Multi-Hop (%) shows the proportion of questions authentically involving information from multiple documents.Avg.Score represents quality on a 1-5 scale (1=Very Poor, 5=Very Good) across multiple evaluation criteria (in Appendix F).
Generation SourceBridge QuestionsComparison QuestionsMulti-Hop (%) Avg. Score Multi-Hop (%) Avg. ScoreHopWeaver (Ours)w/ Gemini-2.5-flash96.44.2798.64.45w/ QwQ-32B98.94.2397.44.40w/ Qwen-14B96.94.0995.94.36w/ GLM-4-9B-041489.83.8793.94.26Human Datasets (Baselines)HotpotQA92.84.2395.64.202WikiMultiHopQA92.84.0497.64.42MuSiQue91.23.78N/AN/A</p>
<p>Table 2 :
2
QA</p>
<p>results of comparing model performance with Question Only (Q-Only) and Question + Golden Docs (Q+Docs) using 100 samples.</p>
<p>Table 3 :
3
Evidence</p>
<p>-Accessibility Evaluation Results.Evaluating different retrievers on the HopWeaver synthesized dataset.</p>
<p>Table 4 :
4
Performance of RAG Systems on HopWeaver-
MethodHopWeaver Bri. HopWeaver Comp.HotpotQA2Wiki.Standard RAG0.3180.4170.3530.210REPLUG0.226 (↓29%)0.483 (↑16%)0.312 (↓12%)0.211 (↑1%)SuRe0.328 (↑3%)0.265 (↓36%)0.334 (↓5%)0.206 (↓2%)FLARE0.136 (↓57%)0.303 (↓27%)0.280 (↓21%) 0.339 (↑61%)IRCoT0.158 (↓50%)0.279 (↓33%)0.415 (↑18%) 0.324 (↑54%)Average0.2330.3490.3390.258Synthesized and Human-Annotated Datasets (F1-Score).</p>
<p>As a synthesis framework, HopWeaver can generate questions at any scale; for evaluation purposes, our analysis was conducted on a randomly sampled subset of the generated questions.
DatasetTotal SizeTrain/Dev/Test SplitQuestion TypesAvg. Question Length Avg. Answer LengthHopWeaverScalable Synthesis 1N/ABridge, Comparison20.16 words2.80 wordsHotpotQA112,77990,447 / 7,405 / 7,405Bridge (42%), Comparison (27%) Intersection (15%), Other (16%)14.68 words2.27 words2WikiMultiHopQA 192,606167,454 / 12,576 / 12,576Comparison (30.1%), Comp. (45.2%) Bridge-comp. (20.8%), Inf. (3.9%)11.59 words2.29 wordsMuSiQue24,81419,938 / 2,417 / 2,4592-hop (68%), 3-hop (24%), 4-hop (8%)16.28 words2.60 words1</p>
<p>Table 5 :
5
A statistical comparison of the HopWeaver-synthesized dataset against major human-annotated MHQA benchmarks.
Retrieval StrategySuccess Rate (%) ↑ Avg. Attempts ↓Standard Dense Retrieval70.11.59Diverse (MMR)70.91.62Diverse + Rerank (ZS)74.01.32Diverse + Rerank (FT -Ours)75.31.17</p>
<p>Table 6 :
6
Ablation study on the effectiveness of the finetuned reranker for Bridge question synthesis.</p>
<p>Table 7 :
7
Ablation study on the effectiveness of the Polisher module across different generator LLMs.
Generator VersionBridge QuestionsComparison QuestionsMulti-Hop (%) Avg. Score Multi-Hop (%) Avg. ScoreGeminiOriginal Polished95.2 96.44.26 4.2798.0 98.64.42 4.45QwQ-32BOriginal Polished97.6 98.94.20 4.2397.2 97.44.36 4.40Qwen3-14BOriginal Polished96.8 96.94.03 4.0994.0 95.94.34 4.36GLM-4-9BOriginal Polished84.5 89.83.71 3.8792.8 93.94.13 4.25</p>
<p>Table 10 :
10
Manual evaluation results of reasoning path correctness on 100 samples.</p>
<p>Table 11 :
11
Token consumption analysis.</p>
<p>Table 12 :
12
LLM reliability evaluation results.Performance of candidate LLMs on reliability metrics (AvgSD ↓, Krippendorff's Alpha ↑, Fleiss' Kappa ↑) used to inform judge selection.</p>
<p>Multi-Hop Question Synthesis Prompt (MULTI_HOP_QUESTION_SYNTHESIS_PROMPT)Goal Synthesize a concise, natural multi-hop question that requires reasoning across two documents, connecting two sub-questions into a single logical inquiry.Instructions -FIRST, check if the bridge entity (Answer 1 from the first sub-question) is included in the text of the second sub-question.If not, return NONE.-Review the analysis and sub-questions to trace the full reasoning chain.-Create a single multi-hop question that: -Is ONE cohesive question, not multiple questions combined -Requires distinct information from both Document A and B -Reads naturally as a coherent, conversational question -Cannot be fully answered using only one document -Follows the reasoning path of the sub-questions, using the bridge entity (Answer 1) to link to information in Document B -Is clear, concise, and free of ambiguity -Doesn't explicitly mention the bridge entity or reasoning steps -If the sub-questions cannot be combined into a valid multi-hop question, return NONE with explanation.-Ensure the final answer matches Answer 2 (the answer from Document B) from the sub-questions.Output FormatIf sub-questions cannot be combined: Bridge Polisher Prompt (POLISHER_PROMPT)GoalValidate and refine multi-hop questions to ensure they genuinely require cross-document reasoning and follow a proper reasoning chain where information from one document is essential to answer a question about content in another document.InstructionsYou are a Polisher module responsible for validating and refining multi-hop questions.Given a multi-hop question, its suggested answer, reasoning path, and source document segments, you will evaluate the question's quality and make one of four decisions: 1. PASS: The question is valid, well-formed, and genuinely requires both documents.4. If the question is fundamentally flawed:[REJECTED]Compare Entity Filter Prompt (COMPARE_ENTITY_FILTER_PROMPT)GoalAssess the concreteness of a pre-identified subject entity and the comparability of its extracted attribute values.Assign numerical scores reflecting these assessments on a 1-5 scale to facilitate downstream filtering.Instructions 1. Assess Entity Concreteness: -Evaluate the provided subject_entity_name and subject_entity_type.-Assign a concreteness_score on a scale of 1 to 5:-5 (Highly Concrete): Specific person, place, organization, tangible object, work, or defined event.(e.g., "Mihály Mosonyi", "Paris", "IBM") -4 (Concrete): Specific but less common entity types.(e.g., "Nobel Prize in Physics", "Treaty of Versailles") -3 (Borderline/Slightly Abstract): Broader well-defined categories.(e.g., "Mammal", "Impressionism") -2 (Abstract): General relationships, abstract concepts, fields of study.(e.g., "US-China relations", "Democracy") -1 (Highly Abstract): Very vague concepts, feelings, ambiguous terms.(e.g., "Happiness", "The problem with X") 2. Assess Attribute Comparability: -Evaluate each provided attribute_value.-Assign a comparability_score (scale 1-5):-5 (Excellent): Precise Dates, Years, Numbers, exact Locations, specific Names, well-defined Categories.-4 (Good): Specific but slightly less precise numbers, specific titles.-3 (Fair): Broader categories, year ranges if precise, specific event names.-2 (Poor): Imprecise time, descriptive reasons, lists.-
Alessandro Suglia, Aditya K. Surikuchi, Ece Takmaz, and Alberto Testoni. 2024. Llms instead of human judges? A large scale empirical study across 20 NLP evaluation tasks. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, F T André, Philipp Martins, Vera Mondorf, Sandro Neplenbroek, Pezzelle, 10.48550/ARXIV.2406.18403CoRR, abs/2406.18403Barbara Plank, David Schlangen</p>
<p>The use of mmr, diversity-based reranking for reordering documents and producing summaries. Jaime G Carbonell, Jade Goldstein, 10.1145/290941.291025SIGIR '98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Melbourne, AustraliaACM1998. August 24-28 1998</p>
<p>LLM-based multi-hop question answering with knowledge graph integration in evolving environments. Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai, 10.18653/v1/2024.findings-emnlp.844Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024a</p>
<p>What are the essential factors in crafting effective long context multi-hop instruction datasets? insights and best practices. Zhi Chen, Qiguang Chen, Libo Qin, Qipeng Guo, Haijun Lv, Yicheng Zou, Wanxiang Che, Hang Yan, Kai Chen, Dahua Lin, 10.48550/ARXIV.2409.01893CoRR, abs/2409.018932024b</p>
<p>Can large language models be an alternative to human evaluations?. David Cheng, -Han Chiang, Hung-Yi Lee, 10.18653/V1/2023.ACL-LONG.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Learning to ask: Neural question generation for reading comprehension. Xinya Du, Junru Shao, Claire Cardie, 10.18653/v1/P17-1123Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017Vancouver, Canada2017. July 30 -August 41</p>
<p>CQG: A simple and effective controlled generation framework for multi-hop question generation. Zichu Fei, Qi Zhang, Tao Gui, Di Liang, Sirui Wang, Wei Wu, Xuanjing Huang, 10.18653/V1/2022.ACL-LONG.475Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland2022. May 22-27, 20221ACL 2022. Association for Computational Linguistics</p>
<p>Should chatgpt be biased? challenges and risks of bias in large language models. Emilio Ferrara, 10.5210/FM.V28I11.13346First Monday. 11282023</p>
<p>Measuring nominal scale agreement among many raters. Joseph L Fleiss, Psychological bulletin. 7653781971</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 10.18653/V1/2024.NAACL-LONG.365Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024a. June 16-21, 20241NAACL 2024</p>
<p>QGEval: Benchmarking multidimensional evaluation for question generation. Weiping Fu, Bifan Wei, Jianxiang Hu, Zhongmin Cai, 10.18653/v1/2024.emnlp-main.658Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsJun Liu. 2024b</p>
<p>A survey on neural question generation: Methods, applications, and prospects. Shasha Guo, Lizi Liao, Cuiping Li, Tat-Seng Chua, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024. the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024Jeju, South Korea2024. August 3-9, 2024</p>
<p>Generative AI for synthetic data generation: Methods, challenges and the future. Xu Guo, Yiqiang Chen, 10.48550/ARXIV.2403.04190CoRR, abs/2403.041902024</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/V1/2023.EMNLP-MAIN.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. Adian Liusie, Potsawee Manakul, Mark J F Gales, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics2024. March 17-22, 20241</p>
<p>Machine learning for synthetic data generation: a review. Yingzhou Lu, Huazheng Wang, Wenqi Wei, 10.48550/ARXIV.2302.04062CoRR, abs/2302.040622023</p>
<p>Chain-of-exemplar: Enhancing distractor generation for multimodal educational question generation. Haohao Luo, Yang Deng, Ying Shen, See-Kiong Ng, Tat-Seng Chua, 10.18653/V1/2024.ACL-LONG.432Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand2024. August 11-16, 20241ACL 2024. Association for Computational Linguistics</p>
<p>Source2synth: Synthetic data generation and curation grounded in real data sources. Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli, 10.48550/ARXIV.2409.08239CoRR, abs/2409.082392024</p>
<p>A survey on multi-hop question answering and generation. Vaibhav Mavi, Anubhav Jangra, Adam Jatowt, 10.48550/ARXIV.2204.09140CoRR, abs/2204.091402022</p>
<p>Multi-hop question answering. Vaibhav Mavi, Anubhav Jangra, Adam Jatowt, 10.1561/1500000102Found. Trends Inf. Retr. 1752024</p>
<p>Compositional questions do not necessitate multi-hop reasoning. Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/P19-1416Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
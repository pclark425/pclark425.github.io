<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7840 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7840</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7840</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-271956727</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.13704v2.pdf" target="_blank">DHP Benchmark: Are LLMs Good NLG Evaluators?</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly serving as evaluators in Natural Language Generation (NLG) tasks; this is often referred to as ``LLM-as-a-judge'' paradigm. However, the capabilities of LLMs in evaluating NLG quality remain underexplored. Current studies depend on human assessments and simple metrics that fail to capture the discernment of LLMs across diverse NLG tasks. To address this gap, we propose the Discernment of Hierarchical Perturbation (DHP) benchmarking framework, which provides quantitative discernment scores for LLMs. This framework leverages hierarchically perturbed text data and statistical tests to systematically measure the NLG evaluation capabilities of LLMs. We re-established six evaluation datasets for this benchmark, covering four NLG tasks: Summarization, Story Completion, Question Answering, and Translation. Our comprehensive benchmarking of five major LLM families provides critical insight into their strengths and limitations as NLG evaluators. Our dataset is available at https://huggingface.co/datasets/YCWANGVINCE/DHP_Benchmark.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7840.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7840.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-LLM alignment (correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alignment between LLM evaluators and human scores using correlation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper describes prior work that evaluates LLM-as-a-judge by measuring alignment with human annotation using correlation metrics (e.g., Pearson rho), and it critiques this approach because response-style biases in both humans and LLMs can distort alignment scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>DHP Benchmark: Are LLMs Good NLG Evaluators?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General NLG evaluation (summarization, QA, translation, story completion)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (as in prior studies / dataset annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation coefficient (ρ) and related alignment/correlation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>alignment affected by biased response styles; alignment may favor models sharing style with annotators; alignment does not guarantee fairness or accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper argues that using correlation/alignment to humans is problematic because both human annotators and LLMs exhibit stable response styles that affect absolute scores, causing alignment metrics to reflect stylistic similarity rather than true evaluative competence.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not applicable here (this entry reports on a critique of correlation-based alignment rather than advantages).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Discussion/related-work level: prior studies compared LLM scores to human scores using correlation metrics; no new numeric values reported in this paper for such alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DHP Benchmark: Are LLMs Good NLG Evaluators?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7840.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7840.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Figure-2 response-style comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical comparison of five LLMs' scoring distributions on SummEval demonstrating response-style variance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper presents an in-paper empirical example (Figure 2) using SummEval reference summaries showing that different LLMs (Llama3, Mistral, Qwen, GPT4-Turbo, Vicuna) adjust scores after perturbation but have markedly different score distributions, illustrating response-style bias that undermines raw alignment metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>DHP Benchmark: Are LLMs Good NLG Evaluators?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization evaluation (quality scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval (reference summaries used)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama3, Mistral, Qwen, GPT4-Turbo, Vicuna (five LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Various sizes and families (examples: Llama3-series, Mistral, Qwen-series, GPT4-Turbo, Vicuna1.5); exact configs as used in paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>SummEval human reference annotations (orig. dataset annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation coefficient (ρ) reported in discussion/visualization (no numeric values provided in text)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>different absolute scoring scales across LLMs; biased response-style leads to misleading alignment measures; some models give systematically higher or lower absolute scores</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>All models detect perturbations and change scores appropriately, but absolute score distributions vary substantially: Llama3, Mistral, Qwen assign higher scores to originals and moderate to perturbed; GPT4-Turbo and Vicuna assign moderate to original and much lower to perturbed — demonstrating style differences that affect alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Shows LLMs can detect perturbations (sensitivity), but the example emphasizes distributional/style differences rather than advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>SummEval reference summaries evaluated by five LLMs before and after a fictional named-entity perturbation; averaged scores across four metrics; visualization of Pearson correlation and score distributions (Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DHP Benchmark: Are LLMs Good NLG Evaluators?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7840.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7840.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DHP statistical comparison (Wilcoxon & Discernment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discernment of Hierarchical Perturbation (DHP) framework using Wilcoxon Signed-Rank Tests, harmonic mean p-values, and converted discernment scores</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper proposes and uses the DHP framework to compare LLM judging ability to a content-grounded standard by testing whether an LLM assigns significantly lower scores to systematically perturbed (lower-quality) text versus original text, using Wilcoxon Signed-Rank Tests combined via harmonic mean p-values and converted to 'discernment scores'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>DHP Benchmark: Are LLMs Good NLG Evaluators?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization, Story Completion, Question Answering, Translation (LLM evaluator benchmarking)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval, SumPubMed, Story Cloze Test, Answer Equivalence subset, WMT-22 (DE→EN and ZH→EN) — each with N=100 samples used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Benchmark set: GPT3.5-Turbo, GPT4-Turbo, Llama3-series, Vicuna1.5-series, Mistral-7B, Qwen-series</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Various families and sizes (examples in paper: Llama3-8B/70B, Vicuna1.5-7B/13B, Mistral-7B, Qwen up to 72B, GPT4-Turbo via API); temperature=0, top_p default, 5 scoring runs averaged</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Not used as primary comparator in DHP; original datasets' human annotations referenced but DHP intentionally evaluates LLMs without relying on human scores</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Wilcoxon Signed-Rank Test p-values (one-sided), Harmonic Mean p-value (HMP), HMP with Expert Weights, and derived Discernment Score D = log_{0.05}(p)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>does not measure alignment to human absolute scores; cannot capture broad real-world NLG complexities beyond the designed perturbations; may miss issues not represented by perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>DHP focuses on relative sensitivity (detecting induced quality degradations) and avoids being confounded by absolute response styles; most LLMs achieve average discernment >1 (p<0.05) but fail on some datasets/tasks (e.g., SumPubMed, multilingual translation) and in worst-case perturbations (D_min).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Provides a human-independent, content-oriented quantitative measure of whether an LLM detects targeted quality issues; mitigates confounding from absolute response-style biases; scalable and systematic across perturbation types.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>For each dataset N=100, models evaluate original and P perturbed variants independently (no reference shown), using metric-specific prompts via Auto-CoT; each model scores 5 times per metric (average taken); Wilcoxon one-sided tests compare paired scores; p-values combined by HMP (optionally weighted by expert survey), converted to discernment scores; threshold D>=1 corresponds to p<0.05.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DHP Benchmark: Are LLMs Good NLG Evaluators?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7840.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7840.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluator limitations (bias)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations of human evaluations and impact on LLM-as-judge comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper emphasizes that human annotators have stable response styles influenced by demographics and other factors, so alignment of LLM judges to human scores can conflate style-match with true evaluation quality; human judgments remain important but are biased and cannot fully validate LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>DHP Benchmark: Are LLMs Good NLG Evaluators?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General NLG evaluation (contextual commentary on evaluation methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (dataset annotators / experts referenced in background and limitations discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>human annotator bias; inconsistent absolute scales across annotators; demographic and cultural influences on scoring; hence alignment to humans doesn't guarantee fairness/accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Human evaluations provide depth and context that automated tests may miss, but are subject to response-style biases and variability; therefore, human alignment is an imperfect benchmark and can reward models that mimic annotator style rather than true discernment.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>LLM evaluators can be scaled and their response-style effects can be isolated by methods like DHP; however, they cannot fully replace human insight.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>This is a conceptual/limitations discussion anchored in literature (e.g., Van Vaerenbergh & Thomas 2013) and the authors' arguments; DHP is presented as a complementary tool rather than a replacement for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DHP Benchmark: Are LLMs Good NLG Evaluators?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-Eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>JudgeBench: A benchmark for evaluating LLM-based judges <em>(Rating: 2)</em></li>
                <li>LLM-judge-eval <em>(Rating: 1)</em></li>
                <li>LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models <em>(Rating: 2)</em></li>
                <li>Are LLM-based evaluators confusing NLG quality criteria? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7840",
    "paper_id": "paper-271956727",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Human-LLM alignment (correlation)",
            "name_full": "Alignment between LLM evaluators and human scores using correlation metrics",
            "brief_description": "The paper describes prior work that evaluates LLM-as-a-judge by measuring alignment with human annotation using correlation metrics (e.g., Pearson rho), and it critiques this approach because response-style biases in both humans and LLMs can distort alignment scores.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
            "evaluation_task": "General NLG evaluation (summarization, QA, translation, story completion)",
            "dataset_name": "",
            "judge_model_name": "",
            "judge_model_details": "",
            "human_evaluator_type": "Human annotators (as in prior studies / dataset annotations)",
            "agreement_metric": "Pearson correlation coefficient (ρ) and related alignment/correlation metrics",
            "agreement_score": null,
            "reported_loss_aspects": "alignment affected by biased response styles; alignment may favor models sharing style with annotators; alignment does not guarantee fairness or accuracy",
            "qualitative_findings": "Paper argues that using correlation/alignment to humans is problematic because both human annotators and LLMs exhibit stable response styles that affect absolute scores, causing alignment metrics to reflect stylistic similarity rather than true evaluative competence.",
            "advantages_of_llm_judge": "Not applicable here (this entry reports on a critique of correlation-based alignment rather than advantages).",
            "experimental_setting": "Discussion/related-work level: prior studies compared LLM scores to human scores using correlation metrics; no new numeric values reported in this paper for such alignments.",
            "uuid": "e7840.0",
            "source_info": {
                "paper_title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Figure-2 response-style comparison",
            "name_full": "Empirical comparison of five LLMs' scoring distributions on SummEval demonstrating response-style variance",
            "brief_description": "The paper presents an in-paper empirical example (Figure 2) using SummEval reference summaries showing that different LLMs (Llama3, Mistral, Qwen, GPT4-Turbo, Vicuna) adjust scores after perturbation but have markedly different score distributions, illustrating response-style bias that undermines raw alignment metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
            "evaluation_task": "Summarization evaluation (quality scoring)",
            "dataset_name": "SummEval (reference summaries used)",
            "judge_model_name": "Llama3, Mistral, Qwen, GPT4-Turbo, Vicuna (five LLMs)",
            "judge_model_details": "Various sizes and families (examples: Llama3-series, Mistral, Qwen-series, GPT4-Turbo, Vicuna1.5); exact configs as used in paper's experiments",
            "human_evaluator_type": "SummEval human reference annotations (orig. dataset annotators)",
            "agreement_metric": "Pearson correlation coefficient (ρ) reported in discussion/visualization (no numeric values provided in text)",
            "agreement_score": null,
            "reported_loss_aspects": "different absolute scoring scales across LLMs; biased response-style leads to misleading alignment measures; some models give systematically higher or lower absolute scores",
            "qualitative_findings": "All models detect perturbations and change scores appropriately, but absolute score distributions vary substantially: Llama3, Mistral, Qwen assign higher scores to originals and moderate to perturbed; GPT4-Turbo and Vicuna assign moderate to original and much lower to perturbed — demonstrating style differences that affect alignment.",
            "advantages_of_llm_judge": "Shows LLMs can detect perturbations (sensitivity), but the example emphasizes distributional/style differences rather than advantages.",
            "experimental_setting": "SummEval reference summaries evaluated by five LLMs before and after a fictional named-entity perturbation; averaged scores across four metrics; visualization of Pearson correlation and score distributions (Figure 2).",
            "uuid": "e7840.1",
            "source_info": {
                "paper_title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DHP statistical comparison (Wilcoxon & Discernment)",
            "name_full": "Discernment of Hierarchical Perturbation (DHP) framework using Wilcoxon Signed-Rank Tests, harmonic mean p-values, and converted discernment scores",
            "brief_description": "The paper proposes and uses the DHP framework to compare LLM judging ability to a content-grounded standard by testing whether an LLM assigns significantly lower scores to systematically perturbed (lower-quality) text versus original text, using Wilcoxon Signed-Rank Tests combined via harmonic mean p-values and converted to 'discernment scores'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
            "evaluation_task": "Summarization, Story Completion, Question Answering, Translation (LLM evaluator benchmarking)",
            "dataset_name": "SummEval, SumPubMed, Story Cloze Test, Answer Equivalence subset, WMT-22 (DE→EN and ZH→EN) — each with N=100 samples used in experiments",
            "judge_model_name": "Benchmark set: GPT3.5-Turbo, GPT4-Turbo, Llama3-series, Vicuna1.5-series, Mistral-7B, Qwen-series",
            "judge_model_details": "Various families and sizes (examples in paper: Llama3-8B/70B, Vicuna1.5-7B/13B, Mistral-7B, Qwen up to 72B, GPT4-Turbo via API); temperature=0, top_p default, 5 scoring runs averaged",
            "human_evaluator_type": "Not used as primary comparator in DHP; original datasets' human annotations referenced but DHP intentionally evaluates LLMs without relying on human scores",
            "agreement_metric": "Wilcoxon Signed-Rank Test p-values (one-sided), Harmonic Mean p-value (HMP), HMP with Expert Weights, and derived Discernment Score D = log_{0.05}(p)",
            "agreement_score": null,
            "reported_loss_aspects": "does not measure alignment to human absolute scores; cannot capture broad real-world NLG complexities beyond the designed perturbations; may miss issues not represented by perturbations",
            "qualitative_findings": "DHP focuses on relative sensitivity (detecting induced quality degradations) and avoids being confounded by absolute response styles; most LLMs achieve average discernment &gt;1 (p&lt;0.05) but fail on some datasets/tasks (e.g., SumPubMed, multilingual translation) and in worst-case perturbations (D_min).",
            "advantages_of_llm_judge": "Provides a human-independent, content-oriented quantitative measure of whether an LLM detects targeted quality issues; mitigates confounding from absolute response-style biases; scalable and systematic across perturbation types.",
            "experimental_setting": "For each dataset N=100, models evaluate original and P perturbed variants independently (no reference shown), using metric-specific prompts via Auto-CoT; each model scores 5 times per metric (average taken); Wilcoxon one-sided tests compare paired scores; p-values combined by HMP (optionally weighted by expert survey), converted to discernment scores; threshold D&gt;=1 corresponds to p&lt;0.05.",
            "uuid": "e7840.2",
            "source_info": {
                "paper_title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Human evaluator limitations (bias)",
            "name_full": "Limitations of human evaluations and impact on LLM-as-judge comparisons",
            "brief_description": "The paper emphasizes that human annotators have stable response styles influenced by demographics and other factors, so alignment of LLM judges to human scores can conflate style-match with true evaluation quality; human judgments remain important but are biased and cannot fully validate LLM evaluators.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "paper_title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
            "evaluation_task": "General NLG evaluation (contextual commentary on evaluation methodology)",
            "dataset_name": "",
            "judge_model_name": "",
            "judge_model_details": "",
            "human_evaluator_type": "Human annotators (dataset annotators / experts referenced in background and limitations discussion)",
            "agreement_metric": "",
            "agreement_score": null,
            "reported_loss_aspects": "human annotator bias; inconsistent absolute scales across annotators; demographic and cultural influences on scoring; hence alignment to humans doesn't guarantee fairness/accuracy",
            "qualitative_findings": "Human evaluations provide depth and context that automated tests may miss, but are subject to response-style biases and variability; therefore, human alignment is an imperfect benchmark and can reward models that mimic annotator style rather than true discernment.",
            "advantages_of_llm_judge": "LLM evaluators can be scaled and their response-style effects can be isolated by methods like DHP; however, they cannot fully replace human insight.",
            "experimental_setting": "This is a conceptual/limitations discussion anchored in literature (e.g., Van Vaerenbergh & Thomas 2013) and the authors' arguments; DHP is presented as a complementary tool rather than a replacement for human evaluation.",
            "uuid": "e7840.3",
            "source_info": {
                "paper_title": "DHP Benchmark: Are LLMs Good NLG Evaluators?",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-Eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "JudgeBench: A benchmark for evaluating LLM-based judges",
            "rating": 2,
            "sanitized_title": "judgebench_a_benchmark_for_evaluating_llmbased_judges"
        },
        {
            "paper_title": "LLM-judge-eval",
            "rating": 1,
            "sanitized_title": "llmjudgeeval"
        },
        {
            "paper_title": "LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models",
            "rating": 2,
            "sanitized_title": "llm_comparative_assessment_zeroshot_nlg_evaluation_through_pairwise_comparisons_using_large_language_models"
        },
        {
            "paper_title": "Are LLM-based evaluators confusing NLG quality criteria?",
            "rating": 2,
            "sanitized_title": "are_llmbased_evaluators_confusing_nlg_quality_criteria"
        }
    ],
    "cost": 0.01140775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DHP Benchmark: Are LLMs Good NLG Evaluators?
25 Feb 2025</p>
<p>Yicheng Wang 
Texas A&amp;M University</p>
<p>Jiayi Yuan 
Rice University
3 Axon EnterpriseInc</p>
<p>Yu-Neng Chuang 
Rice University
3 Axon EnterpriseInc</p>
<p>Zhuoer Wang 
Texas A&amp;M University</p>
<p>Yingchi Liu 
Mark Cusick 
Param Kulkarni 
Zhengping Ji 
Yasser Ibrahim 
Xia Hu 
Rice University
3 Axon EnterpriseInc</p>
<p>DHP Benchmark: Are LLMs Good NLG Evaluators?
25 Feb 202501A55FC7A7A31605D4740BDA752AF33BarXiv:2408.13704v2[cs.CL]
Large Language Models (LLMs) are increasingly serving as evaluators in Natural Language Generation (NLG) tasks; this is often referred to as "LLM-as-a-judge" paradigm.However, the capabilities of LLMs in evaluating NLG quality remain underexplored.Current studies depend on human assessments and simple metrics that fail to capture the discernment of LLMs across diverse NLG tasks.To address this gap, we propose the Discernment of Hierarchical Perturbation (DHP) benchmarking framework, which provides quantitative discernment scores for LLMs.This framework leverages hierarchically perturbed text data and statistical tests to systematically measure the NLG evaluation capabilities of LLMs.We re-established six evaluation datasets for this benchmark, covering four NLG tasks: Summarization, Story Completion, Question Answering, and Translation.Our comprehensive benchmarking of five major LLM families provides critical insight into their strengths and limitations as NLG evaluators.Our dataset is available at https://huggingface.co/ datasets/YCWANGVINCE/DHP_Benchmark.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) play a crucial role in the field of Natural Language Generation (NLG), advanced wide real-world applications including education (Latif et al., 2023), healthcare (Yuan et al., 2023), business (Teubner et al., 2023), etc.The strong capabilities of LLMs allow them not only to serve as text generators but also increasingly as powerful evaluators of text quality (Chiang and Lee, 2023;Liu et al., 2023a;Li et al., 2024).Their role as evaluators is crucial for advancements in various applications, such as summarization, story completion, question answering, and translation (Li et al., 2024;Wang et al., 2023;Chuang et al., 2024).† Equal Contribution.</p>
<p>LLMs are expected to serve as NLG evaluators, providing reasonable quality scores based on different quality metrics with specially designed evaluation prompts.</p>
<p>Despite the growing performance of LLMs in evaluation tasks, a significant gap remains in fully comprehending their capabilities in evaluating NLG quality.The question, Are LLMs good NLG evaluators?remains challenging for two main reasons illustrated in Figure 1: (1) Lack of Clear and Unbiased Measurement: There is no clear measurement for the capability of LLM evaluators.Existing methods rely on aligning with human scores (Chiang and Lee, 2023;Liu et al., 2023a), but these scores themselves are subject to biased response styles (Schoch et al., 2020).</p>
<p>(2) Multiple Evaluation Metrics: Evaluating NLG quality requires considering multiple metrics.For example, in summarization tasks, metrics such as coherence, consistency, and fluency are essential considerations (Liu et al., 2023a;Fabbri et al., 2021;Gabriel et al., 2021).However, LLMs might struggle with correlations between these metrics (Hu et al., 2024), potentially leading to misinterpretation and incorrect scoring, which makes it difficult to assess their effectiveness as evaluators.</p>
<p>To address these challenges, we introduce a novel DHP benchmarking framework -Discernment of Hierarchical Perturbation -for quantitatively measuring the evaluation capabilities of LLMs.We propose the concept of discernment scores, systematically derived from hierarchically perturbed text data and statistical tests.Reference data is perturbed using multiple hierarchical methods, and differences in LLM evaluation scores are analyzed using the Wilcoxon Signed-Rank Test (Wilcoxon, 1945).To obtain more reliable overall evaluation results, harmonic mean pvalues and expert-assigned weights are applied to integrate multiple metrics.The final p-value is then converted into a Discernment Score, providing a quantitative measure of the NLG evaluation capabilities of LLMs.This approach enables a more rigorous and comprehensive assessment of LLM performance, independent of the specific response styles exhibited by the models.This study re-establishes six evaluation datasets across four key NLG tasks: Summarization, Story Completion, Question Answering, and Translation.Each dataset undergoes hierarchical perturbation and is utilized to challenge the evaluative capabilities of LLMs in distinct ways, providing a robust foundation for benchmarking.The datasets include a range of text perturbation methods, from minor character-level problems to significant sentencelevel alterations, enabling a thorough examination of the potential discernment limits of LLMs.</p>
<p>Our comprehensive benchmarking, based on newly defined quantitative discernment scores, is conducted across five major LLM series.This methodology uncovers critical insights into their effectiveness as NLG evaluators and provides a detailed understanding of their performance.This benchmark reveals important trends and patterns in the LLM evaluator capacities, highlighting areas of strength as well as potential shortcomings.</p>
<p>The DHP benchmark aims to fill existing gaps by offering a quantitative framework for assessing LLMs' evaluation capabilities and emphasizing the necessity of considering multiple metrics for accurate and reliable evaluations.We summarize our contributions as follows.</p>
<p>• Develop the DHP benchmarking framework, introducing quantitative discernment scores for LLMs as NLG evaluators based on hierarchical perturbation.• Re-establish six evaluation datasets across four NLG tasks to evaluate the discernment of LLM evaluators.• Benchmark five LLM series to analyze their performance and effectiveness in NLG evaluation.</p>
<p>Challenge: Biased Response Styles</p>
<p>Previous studies focus on the alignment between human and LLM evaluators, using correlation metrics to gauge the LLMs' performance in NLG evaluation tasks (Liu et al., 2023a;Chiang and Lee, 2023).However, these studies often overlook an important variable of evaluators: Response Styles which refer to a respondent's consistent manner of answering survey questions, regardless of the content (Van Vaerenbergh and Thomas, 2013).Despite similar levels of professionalism, annotators may assign different scores to the same questionnaire due to differences in age, gender, personality, cultural background, and ethnic group (Van Vaerenbergh and Thomas, 2013;Hui and Triandis, 1989;Kieruj and Moors, 2013).Similarly, LLMs, trained on diverse datasets, may also exhibit biases in their responses (Salecha et al., 2024).This discrepancy casts doubt on the previous methods used to compare human and LLM scores.Since quality-based scoring often relies heavily on a few experts' annotations, the final alignment scores tend to favor models that share similar response styles with these specific experts.We illustrate this with an example of the re- sponse styles of five LLMs tasked with annotating quality scores for human reference data from the SummEval dataset (Fabbri et al., 2021).We averaged the scores across four metrics for each data point and plotted both the Pearson correlation coefficient (ρ) and the average score distributions of the five models.After perturbing the original data by replacing some named entities with fictional ones in the summaries (Fictional Named Entities in Table 1), we repeated the quality evaluation.As shown in Figure 2, all models detected the changes and adjusted their scores accordingly, though their scoring distributions varied significantly.For instance, Llama3 (Dubey et al., 2024), Mistral (Jiang et al., 2023), and Qwen (Bai et al., 2023) models assign higher scores to the original data and moderate scores to the perturbed data.In contrast, GPT4-Turbo (OpenAI, 2023) and Vicuna (Chiang et al., 2023) models tend to give moderate scores to the original data and much lower scores to the perturbed data.The variance in the response distributions indicates the presence of bias that can significantly affect alignment (ρ), illustrating that alignment is not a direct or credible metric for assessing the ability of LLMs as NLG evaluators.It is crucial to develop a new metric and measurement for evaluation that is not influenced by the evaluators' biased response styles, ensuring a more accurate and fair assessment of LLM capabilities.</p>
<p>DHP Benchmarking Framework</p>
<p>We propose our DHP framework: Discernment of Hierarchical Perturbation.Previous studies overlook the essence of NLG evaluation, i.e., the content-oriented scoring (Novikova et al., 2018).</p>
<p>In other words, content that is accurate, fluent, and consistent should receive higher scores than con-tent that is inaccurate, disfluent, and inconsistent.Qualified annotators should be able to recognize inappropriate content without additional references and then assign scores, even though the absolute scores may still reflect their biased response styles.</p>
<p>The fundamental principle of our assessment is that a qualified LLM evaluator should be able to independently identify issues in perturbed data (which contains some quality issues) and assign relatively lower scores compared to the original reference data during two separate evaluations.This approach does not rely on human scores, thus eliminating the influence of human response styles.</p>
<p>The overall framework is shown in Figure 3. First, for a specific NLG task, we employ a hierarchical perturbation pipeline to transform highquality reference data into various forms of lowerquality data.Subsequently, an LLM evaluates both the original and perturbed texts, respectively, using predefined metrics, generating several sets of rating scores.We then conduct a statistical analysis of these scores.For each pair of scores, original and perturbed, we apply the Wilcoxon Signed-Rank Test to determine the differences in their distributions, achieving this with a confidence level expressed as a p-value.This test specifically assesses differences in pairwise scores without focusing on absolute values, thereby minimizing the impact of models' response styles.Following this, we combine the p-values from different metrics, incorporating Expert Weights (EW ) to tailor the aggregated p-values to the specific metrics of the corresponding perturbation methods.These combined p-values are then transformed into discernment scores, which serve as a direct measure for assessing and comparing the NLG evaluation capabilities of LLMs for this particular task.</p>
<p>Step 1: Hierarchical Perturbation</p>
<p>To generate data that have quality issues across various levels, formats, and evaluation difficulties, we propose a hierarchical perturbation approach.</p>
<p>In contrast to the plain perturbations (Sai et al., 2021), our approach encompasses three levels of perturbation content: character, word, and sentence levels; two methods of perturbation: rule-based and LLM-based; and two degrees of perturbation: minor and major as illustrated in Figure 3.</p>
<p>First, at the character level, we alter some characters or letters in the given N original texts independently.At the word and sentence levels, we degrade the text by processing entire words or sentences, respectively.For NLG tasks involving very short texts, sentence-level perturbation is considered optional.For each level of perturbation, we choose either a rule-based or an LLM-based method, enhancing the diversity of the perturbation's content and format.Additionally, if the text data is sufficiently long for more perturbation, we implement two degrees of perturbation-minor and major-for each method.These different degrees of perturbation are the difficulty that LLMs face in detecting issues within the text.The detailed perturbation methods for each task are shown in Table 1.</p>
<p>With this approach, we generate multiple sets of perturbed data, with each set designed to highlight a specific quality issue tied to a distinct type of perturbation method.Competent LLM evaluators should accurately detect these issues and assign correspondingly lower scores to the perturbed data.</p>
<p>Step 2: LLM evaluation</p>
<p>Following the evaluation method outlined in G-Eval (Liu et al., 2023a), we also utilize the automatic chain-of-thought approach (Auto-CoT) (Zhang et al., 2022) to design evaluation prompts for different datasets and evaluation metrics.These prompts are sent to LLMs to assess both the original data and the perturbed, low-quality data.It's important to note that all perturbed data are evaluated independently, without their original  references, to accurately test the models' capabilities in identifying specific quality issues.</p>
<p>After conducting the LLM evaluation on N datapoints, we obtain several sets of absolute evaluation scores shown in Figure 3:
[{S 0 m 1 }, {S 0 m 2 }, . . . , {S 0 m M }], [{S 1 m 1 }, {S 1 m 2 }, . . . , {S 1 m M }], • • • , [{S P m 1 }, {S P m 2 }, . . . , {S P m M }],
where each {S} is a set of N evaluation scores.The superscripts 0, 1, . . ., P on S represent the original data (0) and the P types of perturbed data (1, . . ., P ), respectively.The subscripts m 1 , . . ., m M represent the M different metrics used in the dataset.For instance, in the SummEval dataset (Fabbri et al., 2021), there are four evaluation metrics, namely: coherence, consistency, fluency, and relevance.</p>
<p>Step 3: Statistical Analysis</p>
<p>As illustrated in Figure 3, we conduct a chain of statistical analyses to derive the final discernment scores for LLM evaluators.This process includes the Wilcoxon Signed-Rank Test, Harmonic Mean pvalue and Expert Weights, and the final calculation of discernment scores.</p>
<p>Wilcoxon Signed-Rank Test</p>
<p>The Wilcoxon Signed-Rank Test (W-Test) (Wilcoxon, 1945) is a non-parametric hypothesis test that compares two dependent samples to assess whether their population mean ranks differ significantly.We apply the W-Test to evaluate whether there is a significant difference in the score distributions between the original data and a given type of perturbed data:
p i m j ∼ z i m j = W-Test({S 0 m j }, {S i m j }).
In our analysis, we adopt a one-sided alternative hypothesis.The resulting p-value indicates the confidence level at which we can reject the null hypothesis -that {S 0 m j } and {S i m j } have the same distribution -and accept the alternative hypothesis -that {S 0 m j } has a greater distribution than {S i m j }.We consider a difference to be statistically significant if p i m j &lt; 0.05.A lower p-value represents a more significant score difference between the original data and perturbed data.In total, we can get P sets of p-values for the M metrics, as shown in Figure 3:
[p 1 m 1 , p 1 m 2 , . . . , p 1 m M ], • • • , [p P m 1 , p P m 2 , . . . , p P m M ].
Because the W-Test does not assume any specific distribution for the scores and does not focus on their absolute values, the resulting p-values solely reflect whether the LLMs are able to detect the quality issues and assign lower scores to the perturbed data compared to the original data.Consequently, this testing approach inherently avoids the influence of response styles, instead focusing on the relative quality assessment.Meanwhile, the p-values provide a quantitative evaluation measure to the score difference, i.e., the capability of evaluators to discern low-quality data.</p>
<p>Harmonic Mean p-value and Expert Weights</p>
<p>Given that an evaluation task may involve multiple M evaluation metrics, resulting in multiple pvalues [p i m 1 , p i m 2 , . . ., p i m M ] for a single perturbed set, it is crucial to derive a combined p-value to measure the overall confidence level.We employ the Harmonic Mean p-value (HMP) method (Wilson, 2019) without or with the Expert Weights (EW ) presented in Figure 3:
p i = 1 M j=1 1 p i m j , p i,EW = 1 M j=1 EW i m j p i m j
.</p>
<p>There are two main reasons for using the HMP method: (1) The p-values are dependent as they are derived from the same dataset but differ based on potentially correlated metrics.The HMP method accommodates this dependency (Wilson, 2019;Vovk and Wang, 2020).( 2) The harmonic mean emphasizes the effect of smaller numbers, meaning that even if the LLMs identify and appropriately score a problem in just one metric, the combined pvalue is still apparently small enough.However, a limitation of the simple HMP is that it does not indicate whether the LLM evaluators correctly identify the specific problems related to the corresponding metrics.For example, in the SummEval (Fabbri et al., 2021) dataset, if a perturbation targets the "fluency" metric but the LLM evaluator incorrectly assigns lower scores to "relevance", the Harmonic Mean p-value method might still produce a low combined p-value.This outcome may not accurately reflect the evaluator's ability to identify the specific issue.</p>
<p>To address this, we introduce HMP with Expert Weights (EW ).We conduct a survey involving 10 NLP experts who are presented with the specific NLG evaluation tasks and metric definitions.They are asked to identify which metric should be most impacted by different quality problems corresponding to the perturbation methods.These preferences are then aggregated to construct EW .For instance, a particular quality issue get votes for "coherence", "consistency", and "fluency" are 4, 1, and 5, respectively, the EW for the corresponding perturbation would be [0.4,0.1, 0.5].The EW makes the combination more targeting those p-values that are highly influenced by the perturbation.This weighting makes the p-value combination more targeted, focusing on those metrics most influenced by the perturbation.Consequently, the weighted combined p-values offer a more precise measure of the LLM evaluators' ability to not only detect issues but also correctly assign lower scores to the impacted metrics.</p>
<p>Discernment Scores of LLM Evaluators</p>
<p>To facilitate comparisons, we transform these combined p-values into positive scores, which we define as discernment scores for a specific perturbation i in Figure 3:
D i = log 0.05 (p i ), D i,EW = log 0.05 (p i,EW ).
Here, D i and D i,EW are positive values and the higher the better.A value of 1 for D i and D i,EW is a threshold corresponding to a p-value of 0.05, indicating statistical significance.If D i or D i,EW is less than 1, it means that the LLM evaluators do not assign significantly lower scores to the perturbed data compared to the original data, suggesting a lack of discernment for specific quality issues during the NLG evaluation.</p>
<p>To observe the comprehensive capability and worst-case performance of the LLMs, we calculate both the average and minimum of D i and D i,EW across all perturbation methods i = 1, . . ., P .This results in overall LLM discernment scores D avg , D min , D EW avg , and D EW min .Note that the average discernment scores are calculated using a weighted average across the perturbation levels (character, word, and sentence levels) mentioned previously.We assign equal weights to perturbations within the same level and make sure that the sum of the weights is the same for each level.This weighting approach ensures that each level of perturbation contributes equally to the final scores.</p>
<p>These discernment scores allow us to explicitly evaluate and compare the capabilities of LLMs as evaluators on specific NLG tasks, thereby establishing comprehensive benchmarks for LLMs.Higher average discernment scores (D avg and D EW avg ) indicate that the LLM can generally identify and assign appropriate scores for quality issues in the NLG task, regardless of the specific type of perturbation.The average discernment scores are useful for getting a broad understanding of an LLM's overall performance as an NLG evaluator.On the other hand, the minimum discernment scores D min and D EW min assess the LLM's performance in the most challenging scenarios, where it may struggle to identify certain types of quality issues.These scores represent the lowest discernment score achieved by the LLM across all perturbation methods, indicating its weakest performance.The minimum discernment scores are crucial for understanding the limitations and potential failure modes of an LLM as an NLG evaluator, even if its overall average performance is acceptable.</p>
<p>Benchmarking LLM Discernment</p>
<p>We evaluate five series of LLMs with varying parameter sizes: the GPT-series (Wang et al., 2023;OpenAI, 2023), which includes GPT3.5-Turbo and GPT4-Turbo; the Llama3-series (Dubey et al., 2024); the Vicuna1.5 series (Chiang et al., 2023); Mistral-7B (Jiang et al., 2023); and the Qwenseries (Bai et al., 2023).</p>
<p>The LLMs are evaluated across four NLG tasks using six re-established public datasets: for Summarization, we use SummEval (Fabbri et al., 2021) (news articles) and SumPubMed (Gupta et al., 2020) (scientific articles); for Story Completion, we select data from Story Cloze Test dataset (Mostafazadeh et al., 2017); for Question Answering, we utilize the data and modify the quality metric based on the Answer Equivalence dataset (Bulian et al., 2022); and for Translation, we leverage WMT-22 German-to-English and Chinese-to-English general (news) translation subsets (Kocmi et al., 2022).To ensure comparability, we select N = 100 datapoints from each dataset.The quality metrics and perturbation methods are detailed in Table 1.</p>
<p>We present our DHP benchmarking results in Figure 4.By examining the discernment scores achieved by these models, we can gain insights into their competence as NLG evaluators.</p>
<p>Overall Assessment</p>
<p>Most LLMs that we have evaluated demonstrate the ability to discern quality issues, as indicated by most D avg and D EW avg scores exceeding 1.This suggests they can comprehend most evaluation metrics and detect varying quality in NLG tasks.However, an exception is noted in the WMT22 Chinese-to-English Translation dataset in Figure 4 (f), where Vicuna1.5-7B and Qwen1.5-7Bfail to achieve favorable average discernment scores, possibly due to their weaker multi-lingual capabilities.</p>
<p>Overall, for NLG evaluation, we recommend the GPT series, especially GPT4-Turbo, which demon-strates superior stability and the highest discernment across nearly all tasks.Among open-source models, Vicuna1.5-13B and Llama3-70B are commendable, achieving good average discernment scores and with most D min and D EW min above 1.</p>
<p>Other Observations</p>
<p>Trends regarding the size of LLMs: In general, larger models within one series generally show better discernment.However, there are notable inconsistencies.For example, Qwen1.5-4B unexpectedly outperforms Qwen-7B in translation tasks in Figure 4 (e, f), and Qwen-72B displays variable performance in the Question Answering task in Figure 4 (d), suggesting that not all larger models uniformly perform better across all types of tasks.</p>
<p>Limitations of Smaller LLMs: In more challenging scenarios, represented by D min and D EW min , smaller-sized LLMs underperform.Models with fewer than 8B parameters show significantly lower D min and D EW min , particularly in summarization and translation tasks in Figure 4 (a, b, e, f).Among these smaller models, Llama3-8B and Mistral-7B are relatively competitive with higher average scores but still register very low scores in the summarization tasks.This suggests that smaller models may become unstable and unreliable evaluators in some complex NLG evaluation scenarios.Metric Misunderstanding Phenomenon: Differences between discernment scores with and without expert weights (D and D EW ) are also notable.While most LLMs display consistent D and D EW scores, Llama3-8B's performance in translation tasks in Figure 4 (e, f) shows a significant discrepancy, with D EW min values being substantially lower than D min and even dropping below 1.This indicates the model's misunderstanding in metrics while identifying quality issues.Variations in Task Performance: Among the six datasets, LLMs perform best in the Story Cloze Test in Figure 4 (c), achieving higher and more stable scores.However, the SumPubMed dataset presented in Figure 4 (b) proves the most challenging; all models except GPT4-Turbo score below 1 in D min and D EW min because of the dataset's complex scientific terminology and content.Models lacking sufficient prior knowledge struggle to identify subtle quality issues in such specialized content.Therefore, we encourage the community to test LLM discernment scores for their specific NLG tasks prior to conducting evaluations, ensuring the selected models are competent evaluators.</p>
<p>Recent advancements highlight the significant potential of utilizing LLMs as evaluators for a variety of NLP tasks.Extensive empirical evidence supports this viewpoint, as demonstrated by studies (Liu et al., 2023a;Chiang and Lee, 2023;Hu et al., 2024;Desmond et al., 2024;Wang et al., 2023), which assert that the evaluation behaviors of pretrained LLM-based evaluators are well-aligned with those of human preference (Liu et al., 2023b).Liusie et al. (Liusie et al., 2024) further show that comparative assessments using LLM evaluators outperform prompt-based techniques, though they identify potential positional biases and propose corresponding solutions.Despite the great assessment performance of a single LLM, advanced studies involve multi-LLM agents (Chan et al., 2023;Zhang et al., 2023;Li et al., 2023) or human experts (Gao et al., 2024;Li et al., 2024) to further increase the judging capability.</p>
<p>While the application of LLMs as judges is a burgeoning area of research, it is imperative to assess their reliability and effectiveness in evaluative roles.To this end, several benchmarks have been recently proposed to evaluate LLMs as judges.For example, JudgeBench (Tan et al., 2024) is designed to assess LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding.Additionally, LLM-judge-eval (Wei et al., 2024) evaluates tasks such as summarization and alignment, incorporating metrics like flipping noise and length bias.</p>
<p>However, despite the progress in LLMs as judges, several challenges persist.First, human involvement remains a crucial factor in both evaluation and alignment, which raises concerns about the extent to which human biases influence LLMbased evaluations.Second, human evaluators themselves are inherently biased, meaning that even if an LLM aligns well with human preferences, it does not necessarily guarantee fairness or accuracy.Additionally, LLMs may misinterpret NLG evaluation metrics (Hu et al., 2024), making simple alignment scores unreliable.To overcome these challenges, our work focuses on developing automated and comprehensive methodologies to test the reliability of LLM-based evaluations.</p>
<p>Conclusion</p>
<p>We introduce the DHP benchmark to assess the discernment capabilities of LLMs as evaluators across various NLG tasks.Our approach not only provides benchmarking results for LLMs but also establishes a robust framework to evaluate how effectively LLMs can identify quality issues, thus serving as competent NLG evaluators.While most models generally perform well, their performance is significantly influenced by factors such as model size, task type, and dataset complexity.By identifying specific weaknesses of LLMs in evaluating NLG tasks, this benchmark aids researchers in enhancing "LLM-as-a-judge" methodologies and improving overall LLM performance.</p>
<p>Limitations</p>
<p>While our DHP benchmark provides a systematic and scalable way to assess LLMs' ability to detect targeted quality issues, it does not offer a complete picture of how these models perform on every aspect of NLG evaluation.First, the discernment scores are generated on a dataset-by-dataset basis, so a truly comprehensive assessment of LLMs across different NLG tasks remains an open challenge.Next, although our framework is designed to reduce reliance on human annotations, it does not fully replace the depth and contextual insight that human evaluations provide.Our perturbationdriven approach highlights particular types of errors rather than capturing the broad spectrum of real-world NLG complexities.Consequently, DHP is best viewed as a complementary tool, and further work is needed to expand its scope to more diverse tasks, languages, and cultural settings, as well as to integrate human judgment for a more holistic evaluation of LLMs.</p>
<p>Figure 2 :
2
Figure 2: Response styles of five LLMs evaluated using the SummEval dataset (Fabbri et al., 2021).</p>
<p>Figure 3 :
3
Figure 3: The DHP framework for each NLG task.It includes three steps: (1) Hierarchical Perturbation, (2) LLM Evaluation, and (3) Statistical Analysis.This figure demonstrates the framework with four perturbation types (P = 4) and three evaluation metrics (M = 3).</p>
<p>Random Deletions (R), Random Typos (R) W (M): Fictional Named Entities (L), Grammatical Errors (L) S (M): Reordering (R), Rewriting and Insertion (L) Random Deletions (R), Random Typos (R) W: Fictional Named Entities (L), Grammatical Errors (L) S: Random Ending Sentence (R), Wrong Ending Sentence (R) Random Deletions (R), Random Typos (R) W (M): Fictional Named Entities (L), Grammatical Errors (L) S: Random Answer (R) Translation Accuracy Fluency C (M): Random Deletions (R), Random Typos (R) W (M): Random Deletions (R), Fictional Named Entities (L), Grammatical Errors (L)</p>
<p>Figure 4 :
4
Figure 4: The DHP benchmarking results across four NLG tasks.Notably, in (d) for the Question Answering task, D and D EW are identical because this task utilizes only one evaluation metric.The red lines on the charts represent D or D EW = 1, which indicates the threshold for statistical significance in discernment scores.</p>
<p>ρ</p>
<p>Coherence &gt;ρ Coherence ρ Fluency &gt;ρ Fluency ρ Consistency &lt;ρ Consistency
NLG TasksHuman EvaluationHuman AlignmentSummarizationStory CompetionQuestion AnseringTranslationCoherenceFluencyConsistencyLLM EvaluationAccuracyRelevance...Biased Response StylesHierarchical PerturbationLLM EvaluationDiscernment Score
D &gt;D Figure 1: Challenges in Assessing LLMs as NLG Evaluators: Biased Response Styles and Multiple Evaluation Metrics.Our DHP Framework employs hierarchical perturbation and statistical tests to address these challenges, offering quantitative discernment scores for effective comparison.</p>
<p>Table 1 :
1
The quality metrics and perturbation methods for the four NLG tasks.C: Character Level.W: Word Level.S: Sentence Level.(R): Rule-based Perturbation.(L): LLM-based Perturbation.(M): Major and Minor Perturbations for each method.</p>
<p>A NLG Tasks and MetricsA.1 SummarizationWe utilize the SummEval(Fabbri et al., 2021)(MIT license) and SumPubMed(Gupta et al., 2020)datasets (MIT license) for our summarization tasks.The SummEval dataset comprises 100 news articles, each accompanied by multiple reference and generated summaries.For our analysis, we exclusively use the reference summaries, selecting the one with the highest number of sentences from each article to facilitate perturbation.The SumPubMed dataset contains 32,000 long scientific articles along with their abstracts serving as reference summaries.We only use the "BACKGROUND" sections of these articles and summaries.We randomly select 100 pairs of articles and their corresponding summaries.For the evaluation of summarization performance, we adhere to the metrics defined by SummEval(Fabbri et al., 2021), specifically focusing on Coherence, Consistency, Fluency, and Relevance.A.2 Story CompletionIn this story completion task, we utilize the public Story Cloze Test dataset(Mostafazadeh et al., 2017), which comprises four-sentence stories each paired with a reference and wrong ending.We select 100 datapoints at random from the validation set for our analysis.Given the absence of explicitly defined quality metrics for the dataset, we adapt metrics from summarization tasks-Coherence, Consistency, and Fluency.Coherence evaluates the story's overall structure and narrative flow.Consistency measures how well the ending maintains the established tone, setting, character development, and narrative style of the story.Fluency focuses on the linguistic and stylistic quality of the story's conclusion.A.3 Question AnsweringFor the question answering task, we employ the Answer Equivalence dataset(Bulian et al., 2022)(Apache-2.0license), which is a modified version of the SQuAD dataset(Rajpurkar et al., 2016).We specifically select reference answers that exceed 150 characters to facilitate perturbation.From this filtered set, we randomly choose 100 question-answer pairs.We adapt the original rating tasks of the dataset into a single metric: Answer Quality.This metric assesses whether the answer provides a comprehensive and accurate response to the question, effectively capturing the essence of the content discussed in the paragraph.A.4 TranslationWe utilize two subsets from the WMT-22 general (news) translation dataset: German-to-English and Chinese-to-English sets which are freely available for research purposes.For our analysis, we select the test sets with reference translations, ensuring each translation exceeds 300 characters in length.We randomly choose 100 datapoints from each subset for evaluation.In assessing translation tasks, we adopt two principal metrics from the Multidimensional Quality Metrics (MQM) framework(Burchardt, 2013): Accuracy and Fluency.Accuracy measures how closely the translation mirrors the source text, focusing on the absence of additions, omissions, or mistranslations.Fluency evaluates the translation's compliance with the linguistic norms of the target language, specifically examining spelling, grammar, and consistency.Meta-Llama-3-70B-Instruct huggingface.co/meta-llama/Meta-Llama-3-70B-InstructVicuna1.5-7B vicuna-7b-v1.5-16khuggingface.co/lmsys/vicuna-7b-v1.5-16k Vicuna1.5-13Bvicuna-13b-v1.5-16khuggingface.co/lmsys/vicuna-13b-v1.5-16kMistral-7B Mistral-7B-Instruct-v0.2 huggingface.co/mistralai/Mistral-7B-Instruct-v0.2Qwen1.5-4B Qwen1.5-4B-Chathuggingface.co/Qwen/Qwen1.5-4B-ChatQwen1.5-7BQwen1.5-7B-Chat huggingface.co/Qwen/Qwen1.5-7B-ChatQwen1.5-14BQwen1.5-14B-Chat huggingface.co/Qwen/Qwen1.5-14B-ChatQwen1.5-32BQwen1.5-32B-Chat huggingface.co/Qwen/Qwen1.5-32B-ChatQwen1.5-72BQwen1.5-72B-Chat huggingface.co/Qwen/Qwen1.5-72B-ChatB Hierarchical PerturbationThe specifics of the hierarchical perturbations are detailed in Table2.We perform these perturbations based on character, word, and sentence-level statistical data of the texts, which are presented in Table2.Our rule-based perturbations include simple text deletions, typographical errors using existing software tools, reordering of sentences, and the incorporation of random or incorrect sentences from other data.For LLM-based perturbations, we employ GPT4-Turbo, modifying the reference text via Auto-CoT(Zhang et al., 2022)prompts to generate the detailed procedural perturbation steps.Below, we provide an example of how the "Minor Fictional Named Entities" perturbation is applied to the summarization tasks:Minor Fictional Named Entities Perturbation Prompt:You will be given one summary written for an article.Your task is to adjust the summary by implementing a specific change.Please make sure you read and understand these instructions carefully.Adjustment: Please substitute only one critical named entity within the summary (e.g., a name, a location, a specific number, a technical term, etc.) with a fictional counterpart.Adjustment Steps: 1. Identify the critical named entity within the summary.This could be a person's name, a location, a specific number, or any other specific detail that is crucial to the summary.2. Create a fictional counterpart for the identified entity.This could be a fictional name, a fictional location, a fictional number, a fictional technical term etc. Make sure that the fictional counterpart is appropriate and fits within the context of the summary.3. Replace the identified entity with its fictional counterpart in the summary.Ensure that the replacement is grammatically correct and maintains the overall meaning and flow of the summary.4. Review the adjusted summary to ensure that it still makes sense and conveys the main points of the article, despite the change in one critical named entity.Summary: SUMMARY_HERE Revised Summary:C Expert WeightsWe invite 10 volunteer experts with extensive backgrounds in NLP/NLG research to complete an expert weight survey.The interface of this survey is displayed in Figure5, which includes the survey instructions, definitions of the tasks and metrics, data types, and descriptions of quality issues associated with the perturbation methods.The experts are asked to select the metric they believe is most impacted by each quality issue presented.We then utilize their responses as weights for combining the p-values.The results of these expert evaluations are detailed in Figure6.D LLM EvaluationWe evaluate five series of large language models (LLMs), details of which are provided in Table3. Due to the extensive length of text data from the SumPubMed dataset(Gupta et al., 2020), which can exceed the 4K context window, we evaluate the models capable of processing long texts (≥ 8K tokens).The GPT series is operated using the OpenAI API, and the open-source LLMs are executed on a server with 8 Nvidia A100 GPUs.We set the temperature parameters to 0 and maintain the default values for the top_p parameters.Throughout the evaluation process, each model score 5 times on each metric to calculate a final average score.We use the scipy.stats.wilcoxon to conduct the Wilcoxon Signed-Rank Test.E Evaluation PromptsWe follow the guidelines of G-Eval(Liu et al., 2023a)and utilize the Auto-CoT method(Zhang et al., 2022)to construct our evaluation prompts.Below is an example of the prompt used for assessing the Coherence metric in summarization tasks:You will be given a summary written for an article.Your task is to rate the summary on one metric.Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.Evaluation Criterion: Coherence (1-5) -the collective quality of all sentences.We align this dimension with the DUC quality question of structure and coherence whereby the summary should be well-structured and well-organized.The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.Evaluation Steps: 1. Read the Summary Thoroughly: Before diving into the evaluation, ensure that you have a clear understanding of the entire summary.Reading it more than once might be necessary.2. Identify the Central Topic: A coherent summary will have a clear central topic or theme.Identify this topic and see if the subsequent information revolves around it.3. Check for Logical Flow: Review the summary for logical sequencing.Sentences should follow one another in a way that makes sense and allows the reader to easily follow the progression of information.4. Look for Transitional Elements: Coherent summaries often have clear transitions between sentences or ideas.This could be in the form of transitional words, phrases, or connecting ideas that tie one sentence to the next.5. Identify Redundancies: Check if the same information is repeated in different sentences.Redundancies can disrupt the flow and coherence of a summary.6.Note Any Gaps or Jumps: If there are sudden jumps in topics or if crucial information seems to be missing, this can harm the coherence of the summary.A well-organized summary should present a holistic view of the topic without leaving the reader with questions.7. Assess Clarity: Even if the content is technically accurate, if it's written in a convoluted or unclear manner, it can disrupt coherence.The sentences should be clear and easily understandable.8. Consider the Conclusion: A coherent summary often wraps up or comes to a conclusion that ties the presented information together.It doesn't necessarily need a formal conclusion, but the end should feel natural and not abrupt.9. Rate the Summary: Based on the above steps, assign a score between 1-5 for coherence.-1: Very incoherent.The summary lacks structure, has sudden jumps, and is difficult to follow.-2: Somewhat incoherent.The summary has some semblance of structure, but has significant flaws in flow and organization.-3: Neutral.The summary is decently organized, with minor issues in flow and structure.-4: Mostly coherent.The summary is well-structured with very few minor coherence issues.-5: Highly coherent.The summary is excellently organized, flows seamlessly, and builds information logically from start to end.Source Article: ARTICLE_HERE Summary: SUMMARY_HERE Evaluation Score (please don't give any feedback, just give a score ONLY) -Coherence:
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation. Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Börschinger, Tal Schuster, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Multidimensional quality metrics: a flexible system for assessing translation quality. Aljoscha Burchardt, Proceedings of Translating and the Computer 35. Translating and the Computer 352013</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.019372023arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Spec: a soft prompt-based calibration on performance variability of large language model in clinical notes summarization. Yu-Neng Chuang, Ruixiang Tang, Xiaoqian Jiang, Xia Hu, Journal of Biomedical Informatics. 1511046062024</p>
<p>Evalullm: Llm assisted evaluation of generative outputs. Michael Desmond, Zahra Ashktorab, Qian Pan, Casey Dugan, James M Johnson, Companion Proceedings of the 29th International Conference on Intelligent User Interfaces. 2024</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Summeval: Re-evaluating summarization evaluation. Wojciech Alexander R Fabbri, Bryan Kryściński, Caiming Mc-Cann, Richard Xiong, Dragomir Socher, Radev, Transactions of the Association for Computational Linguistics. 92021</p>
<p>GO FIGURE: A meta evaluation of factuality in summarization. Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao, 10.18653/v1/2021.findings-acl.42Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan, arXiv:2402.01383Llm-based nlg evaluation: Current status and challenges. 2024arXiv preprint</p>
<p>Sumpubmed: Summarization dataset of pubmed scientific article. Vivek Gupta, Prerna Bharti, Pegah Nokhiz, Harish Karnick, Proceedings of the 2021 Conference of the Association for Computational Linguistics: Student Research Workshop. the 2021 Conference of the Association for Computational Linguistics: Student Research WorkshopAssociation for Computational Linguistics2020</p>
<p>Are LLMbased evaluators confusing NLG quality criteria?. Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, Xiaojun Wan, 10.18653/v1/2024.acl-long.516Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Effects of culture and response format on extreme response style. Harry Hui, Harry C Triandis, Journal of cross-cultural psychology. 2031989</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Response style behavior: question format dependent or personal style?. D Natalia, Guy Kieruj, Moors, Quality &amp; Quantity. 472013</p>
<p>Findings of the 2022 conference on machine translation (wmt22). Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)2022</p>
<p>Knowledge distillation of llm for education. Ehsan Latif, Luyang Fang, Ping Ma, Xiaoming Zhai, arXiv:2312.158422023arXiv preprint</p>
<p>Ruosen Li, arXiv:2307.02762Teerth Patel, and Xinya Du. 2023. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint</p>
<p>Leveraging large language models for nlg evaluation: A survey. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao, arXiv:2401.071032024arXiv preprint</p>
<p>G-eval: Nlg evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023a</p>
<p>Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, arXiv:2309.13308Calibrating llmbased evaluator. 2023barXiv preprint</p>
<p>Llm comparative assessment: Zero-shot nlg evaluation through pairwise comparisons using large language models. Adian Liusie, Potsawee Manakul, Mark Gales, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European Chapterthe Association for Computational Linguistics20241Long Papers)</p>
<p>Lsdsem 2017 shared task: The story cloze test. Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, James Allen, Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics. the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics2017</p>
<p>Rankme: Reliable human ratings for natural language generation. Jekaterina Novikova, Ondřej Dušek, Verena Rieser, Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterHuman Language Technologies20182</p>
<p>Squad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>Perturbation checklists for evaluating nlg evaluation metrics. Tanay Ananya B Sai, Dixit, Yashpal Dev, Sreyas Sheth, Mitesh M Mohan, Khapra, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Large language models show human-like social desirability biases in survey responses. Aadesh Salecha, Molly E Ireland, Shashanka Subrahmanya, João Sedoc, Lyle H Ungar, Johannes C Eichstaedt, arXiv:2405.060582024arXiv preprint</p>
<p>this is a problem, don't you agree?" framing and bias in human evaluation for natural language generation. Stephanie Schoch, Diyi Yang, Yangfeng Ji, Proceedings of the 1st Workshop on Evaluating NLG Evaluation. the 1st Workshop on Evaluating NLG Evaluation2020</p>
<p>Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y Tang, Alejandro Cuadron, Chenguang Wang, arXiv:2410.12784Raluca Ada Popa, and Ion Stoica. 2024. Judgebench: A benchmark for evaluating llm-based judges. arXiv preprint</p>
<p>Welcome to the era of chatgpt et al. the prospects of large language models. Timm Teubner, Christoph M Flath, Christof Weinhardt, Oliver Wil Van Der Aalst, Hinz, Business &amp; Information Systems Engineering. 6522023</p>
<p>Response styles in survey research: A literature review of antecedents, consequences, and remedies. Yves Van Vaerenbergh, Troy D Thomas, International journal of public opinion research. 2522013</p>
<p>Combining p-values via averaging. Vladimir Vovk, Ruodu Wang, Biometrika. 10742020</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, arXiv:2303.04048Is chatgpt a good nlg evaluator? a preliminary study. 2023arXiv preprint</p>
<p>Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, Mei Han, arXiv:2408.13006Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates. 2024arXiv preprint</p>
<p>Individual comparisons by ranking methods. Wilcoxon, Biometrics Bulletin. 161945</p>
<p>The harmonic mean p-value for combining dependent tests. Wilson Daniel, Proceedings of the National Academy of Sciences. the National Academy of Sciences2019116</p>
<p>Large language models for healthcare data augmentation: An example on patient-trial matching. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, Xia Hu, AMIA Annual Symposium Proceedings. 202320231324</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li, arXiv:2308.01862Wider and deeper llm networks are fairer llm evaluators. 2023arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
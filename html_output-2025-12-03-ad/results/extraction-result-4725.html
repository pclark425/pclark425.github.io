<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4725 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4725</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4725</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-526c02aa9f649da627f1631687c8b37a2a05868d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/526c02aa9f649da627f1631687c8b37a2a05868d" target="_blank">Feature emergence via margin maximization: case studies in algebraic tasks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper proves that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible group-theoretic representations to perform compositions in general groups, aligning closely with the empirical observations of Nanda et al. and Chughtai etAl.</p>
                <p><strong>Paper Abstract:</strong> Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding how neural networks implement specific target functions, this paper explores a complementary question -- why do networks arrive at particular computational strategies? Our inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. Specifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible group-theoretic representations to perform compositions in general groups, aligning closely with the empirical observations of Nanda et al. and Chughtai et al. More generally, we hope our techniques can help to foster a deeper understanding of why neural networks adopt specific computational strategies.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4725.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4725.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nanda Fourier rotation algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier / rotation algorithm for modular addition (Nanda et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanistic circuit observed in one-layer transformers that represents inputs as complex rotations e^{2π i k a}, composes them multiplicatively to form e^{2π i k(a+b)}, and scores candidates c by multiplying by e^{-2π i k c} and taking the real part (cosine) to recover a+b mod p.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progress measures for grokking via mechanistic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>1-layer transformer (as studied by Nanda et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A single-layer transformer studied in mechanistic interpretability work that learns embeddings for tokens a,b in Z_p and uses attention/MLP circuitry to implement modular addition via sinusoidal/rotational embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Modular addition (a + b mod p)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Embeddings implement discrete-frequency complex rotations e^{2π i k · a/p}; composition is multiplication of rotations; logits for candidate c computed by real part (cosine) of e^{2π i k(a+b-c)/p}; averaging across frequencies yields a sharp signal at correct c.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Mechanistic interpretability analysis in Nanda et al. showed explicit neuron/embedding geometry matching rotations; this paper cites that observation and the identity relating argmax of cosine to modular addition and connects it to Fourier-feature solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Alternative correct classifiers exist (memorizing constructions) that do not use Fourier rotations; ReLU or other activations may produce approximate rather than exact single-frequency neurons; the rotation algorithm is one-of-many possible implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical: Transformers studied in Nanda et al. solved modular addition reliably; this paper does not report numeric accuracy for those transformers but cites their consistent learning of the rotation algorithm. (Implicitly near 100% on the training dataset.)</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Mechanistic probes in Nanda et al. identified embedding geometry and circuits mapping to rotations; this paper uses that as motivation but performs its own max-margin analyses on MLPs rather than re-running transformer probes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>The rotation solution requires presence of sinusoidal/fourier-like features and sufficient capacity; alternative constructions can realize the function without such features (dense/memorization), and various architectures/regularizers may yield only approximate frequency-sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared qualitatively to quadratic one-hidden-layer MLPs in this paper which provably attain Fourier-sparse neurons under margin maximization; ReLU MLPs empirically get near-sparse but not exact.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Feature emergence via margin maximization: case studies in algebraic tasks', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4725.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4725.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quadratic MLP Fourier features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-hidden-layer quadratic MLP solutions for modular addition (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For modular addition tasks, max-margin solutions of sufficiently wide one-hidden-layer quadratic networks concentrate each neuron on a single Fourier frequency (sinusoidal embedding) with phase relations u,v,w satisfying θ_u+θ_v=θ_w, and the network contains at least one neuron per nonzero frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>1-hidden-layer MLP with quadratic activations (phi((u,v,w),a,b)=(u_a+v_b)^2 w)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A homogeneous two-input quadratic neuron architecture with no biases; inputs are one-hot encodings of group elements (dimension d=p), neurons parameterized by u,v,w in R^d; networks trained with L_{2,3}-style regularization and gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Modular addition (a + b mod p) — multiclass classification over p outputs</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Margin-maximizing solutions are Fourier-sparse: each neuron has input and output weight vectors whose discrete Fourier transforms are supported on a single nonzero frequency ζ, and phase offsets satisfy θ_u+θ_v=θ_w. The network aggregates neurons of different frequencies to produce sharp logits for the correct sum.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Theoretical proof (Theorem 7) that any L_{2,3} max-margin solution with sufficient width (m ≥ 4(p-1)) has single-frequency neurons and includes all frequencies; derivation reduces expected class-weighted margin to a cubic form in Fourier coefficients and maximization under norm constraints yields sparsity. Empirical training (p=71, m=500, L_{2,3} reg=1e-4) shows trained weights match predicted single-frequency Fourier structure and normalized margin approaches theoretical value (Figures 1,4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Appendix D constructs alternative networks that perfectly classify but are dense in Fourier space, showing Fourier sparsity is not necessary for correctness—only selected by margin maximization. ReLU networks trained with L2 regularization show near-but-not-exact frequency sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical maximum normalized L_{2,3}-margin: γ* = sqrt(2/27) * [1/(p^{1/2}(p-1))] (as stated in Theorem 7). Empirically, quadratic MLPs trained (p=71) approach this γ*; exact numeric training margin trajectories are shown in figures (training hyperparams: m=500, 40000 steps, learning rate schedule, reg=1e-4). Classification accuracy on the task reaches near 100% (trained to fit full dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Fourier power spectra of learned embedding vectors were plotted (Figure 1,4); quadratic networks under L_{2,3} reg show per-neuron Fourier-power concentrated at a single frequency (power ≈1), supporting the theoretical prediction. ReLU networks under L2 show approximately 1-sparse but less exact.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>The proofs and observed phenomenon require: quadratic (x^2) homogeneous activations, L_{2,ν} regularization with ν equal to homogeneity, sufficient width (m ≥ 4(p-1)), and training towards max-margin regime (vanishing regularization or appropriate dynamics). Without these conditions, networks may learn different (memorizing or dense) solutions. Also the particular γ* formula scales inversely with p^{1/2}(p-1) so margin shrinks with larger p.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Quadratic activation + L_{2,3} yields exact theoretical single-frequency neurons; ReLU + L2 empirically gives approximate sparsity. The 1-layer transformer learned by Nanda et al. implements a related Fourier/rotation algorithm but the MLP analysis here gives a provable max-margin rationale for sinusoidal features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Feature emergence via margin maximization: case studies in algebraic tasks', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4725.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4725.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse-parity x^k features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-hidden-layer x^k MLP solutions for k-sparse parity (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On (n,k)-sparse parity tasks, max-margin solutions for x^k-activated one-hidden-layer networks allocate equal-magnitude weights to all relevant bits and constrain sign patterns so neuron contributions align with the parity monomial; networks require sufficient width m ≥ 2^{k-1}.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>1-hidden-layer MLP with x^k activation (phi(u,w,x)=(u^T x)^k w)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Homogeneous polynomial neurons of degree k, with output weights in R^2 for binary classification; inputs in {±1}^n; trained with L_{2,k+1} regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Sparse parity: predict product of k selected input bits (binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Max-margin neurons place support exactly on the k relevant coordinates S, with equal absolute weight magnitudes across those coordinates, and output weight aligned so the product sign times output projection is nonnegative — effectively implementing the k-th order monomial.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Analytic result (Theorem 8) deriving γ* = k! * sqrt{2 (k+1)^{-(k+1)}} and showing structure constraints on u_i and w_i for any maximizer. Empirical experiment on (n,k)=(10,4) with quadratic/quartic networks (m=40, reg L_{2,5}=1e-3) shows neurons and margins matching theoretical predictions (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The result depends on homogeneous x^k activations and L_{2,k+1} norm; other activations or regularizers may yield different learned features. Computational limits: need width m ≥ 2^{k-1} to realize the full max-margin construction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical maximum margin γ* = k! * sqrt{2 (k+1)^{-(k+1)}} (Theorem 8). Empirically the network's normalized L_{2,k+1} margin approaches this predicted value in the (10,4) experiment; classification accuracy on the full training set is achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Visualization of neurons with highest norm and margin evolution (Figure 5) confirm that learned neuron supports align with the true parity-support coordinates and that margins track theory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Requires exact polynomial activations and matching norm for the proof; for large k the required width grows exponentially (2^{k-1}); other architectures or noisy data may prevent emergence of the idealized monomial features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>This construction and analysis is specialized to x^k homogeneous neurons and is not directly compared to Transformers here; cited related work discusses parity learning limits for various architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Feature emergence via margin maximization: case studies in algebraic tasks', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4725.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4725.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Group irreducible-rep features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Irreducible representation-based features for finite group composition (this paper / Chughtai et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Max-margin solutions for quadratic one-hidden-layer networks learning group composition have neurons whose input/output weight vectors lie in the subspace spanned by a single nontrivial real irreducible representation of the group, and the network contains neurons for (a sufficient subset of) representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A toy model of universality: Reverse engineering how networks learn group operations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>1-hidden-layer quadratic MLPs (and related transformer/MLP observations in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Quadratic homogeneous networks mapping one-hot encoded group elements to logits; analysis uses group representation theory (characters, matrix elements) and L_{2,3} regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Group composition (predict c = a·b for finite group G), multiclass over |G| outputs</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Margin maximization selects neurons whose weight vectors are supported within basis vectors associated to a single irreducible real representation R of G (i.e., neurons implement features proportional to matrix elements R(g)_{i,j}); logits reduce to characters/traces tr(R(a)R(b)R(c^{-1})) which are maximized at c=a b.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Theorem 9 proves that under conditions (sufficient width m ≥ 2 Σ_{n>1} d_R^3 and sign condition Σ d_R^{1.5} χ_R(C) < 0 for nontrivial conjugacy classes) the L_{2,3} max-margin network has neurons spanned by single representations and includes at least one neuron per nontrivial representation; empirical experiments on symmetric groups S3-S5 (trained networks with L_{2,3}) show neuron concentration per representation and margins approaching theoretical predictions (Figure 6 and Appendix figures). This aligns with empirical observations in Chughtai et al.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The full theorem requires a technical conjugacy-class sign condition which fails for some groups (so not universally applicable). Where the condition fails, only a subset of representations may appear on the margin and some conjugacy classes may not be on-margin. Alternative dense constructions exist in general (Appendix D).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theorem 9 gives γ* = (2 / (3 sqrt{3 |G|})) * 1 / (Σ_{n=2}^K d_{R_n}^{2.5}) under stated conditions. Empirically, networks on S3,S4,S5 trained with L_{2,3} approach the predicted γ*; experiments used m up to 2000 for S5 with reg 1e-5 and SGD (see Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Projection of learned u vectors onto representation-basis shows neurons concentrating power in a single representation (Figures 6,8,9); distribution of neurons across representations matches constructive counts in proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Requires real irreducible representations and the technical sign condition for full coverage; sufficient width requirement can be large (m scales with sum of cubes of representation dimensions); not all finite groups satisfy the theorem's assumptions; learning dynamics and regularization regime must approach max-margin behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>This formalizes and explains empirical findings of Chughtai et al. who observed representation-based features in MLPs/transformers; quadratic activation + L_{2,3} yields clean theoretical alignment, whereas other activations/regularizers may give approximate behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Feature emergence via margin maximization: case studies in algebraic tasks', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4725.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4725.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorization dense construction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alternative dense/memorization network construction (Appendix D, this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit construction showing that for modular addition (and group tasks) there exist networks that perfectly memorize the input-output mapping using pairs of localized one-hot-like neurons, producing dense (non-sparse) Fourier spectra; such solutions fit data but have lower margin.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Constructed two-neuron-per-example quadratic MLP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>For each training pair (a,b) the construction uses two neurons with u and v set to one-hot indicators and w chosen to place the correct logit; summing across examples yields a network that outputs exact one-hot logits for training pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Modular addition or arbitrary mapping r(a,b) → c</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Memorization by localized neurons: neurons are one-hot in input space and produce outputs only for a single (a,b) pair; no global Fourier or representation structure is required.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Appendix D gives explicit algebraic construction of neurons (u,v,w and u',v',w') per (a,b) such that summed outputs equal indicator of chosen c, and those weight vectors are one-hot so their Fourier transforms are uniform (dense) rather than sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Although this construction classifies perfectly, it has lower margin than the Fourier-sparse max-margin solutions and thus is not generally found by margin-seeking training dynamics (empirically not observed under the regularized training regimes used).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Construction achieves perfect training classification (1 on correct logit, 0 elsewhere) by design; however no margin-optimality — hence will not be the solution in the vanishing-regularization regime targeted by the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Analytic construction rather than an empirical probe; used to demonstrate non-uniqueness of classifiers and to argue that Fourier sparsity is an inductive-bias-selected property, not an intrinsic necessity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Highly overparameterized in terms of neurons (2 p^2 neurons for cyclic case), not parsimonious, fails to exhibit the structured features seen in trained networks under margin-maximizing dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasts with margin-derived Fourier/representation solutions; serves as a baseline showing many classifiers exist but margin bias selects structured ones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Feature emergence via margin maximization: case studies in algebraic tasks', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability. <em>(Rating: 2)</em></li>
                <li>A toy model of universality: Reverse engineering how networks learn group operations. <em>(Rating: 2)</em></li>
                <li>Grokking modular arithmetic. <em>(Rating: 2)</em></li>
                <li>The clock and the pizza: Two stories in mechanistic explanation of neural networks. <em>(Rating: 1)</em></li>
                <li>Groking: Generalization beyond overfitting on small algorithmic datasets. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4725",
    "paper_id": "paper-526c02aa9f649da627f1631687c8b37a2a05868d",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "Nanda Fourier rotation algorithm",
            "name_full": "Fourier / rotation algorithm for modular addition (Nanda et al.)",
            "brief_description": "A mechanistic circuit observed in one-layer transformers that represents inputs as complex rotations e^{2π i k a}, composes them multiplicatively to form e^{2π i k(a+b)}, and scores candidates c by multiplying by e^{-2π i k c} and taking the real part (cosine) to recover a+b mod p.",
            "citation_title": "Progress measures for grokking via mechanistic interpretability.",
            "mention_or_use": "mention",
            "model_name": "1-layer transformer (as studied by Nanda et al.)",
            "model_description": "A single-layer transformer studied in mechanistic interpretability work that learns embeddings for tokens a,b in Z_p and uses attention/MLP circuitry to implement modular addition via sinusoidal/rotational embeddings.",
            "arithmetic_task_type": "Modular addition (a + b mod p)",
            "mechanism_hypothesis": "Embeddings implement discrete-frequency complex rotations e^{2π i k · a/p}; composition is multiplication of rotations; logits for candidate c computed by real part (cosine) of e^{2π i k(a+b-c)/p}; averaging across frequencies yields a sharp signal at correct c.",
            "evidence_for_mechanism": "Mechanistic interpretability analysis in Nanda et al. showed explicit neuron/embedding geometry matching rotations; this paper cites that observation and the identity relating argmax of cosine to modular addition and connects it to Fourier-feature solutions.",
            "evidence_against_mechanism": "Alternative correct classifiers exist (memorizing constructions) that do not use Fourier rotations; ReLU or other activations may produce approximate rather than exact single-frequency neurons; the rotation algorithm is one-of-many possible implementations.",
            "performance_metrics": "Empirical: Transformers studied in Nanda et al. solved modular addition reliably; this paper does not report numeric accuracy for those transformers but cites their consistent learning of the rotation algorithm. (Implicitly near 100% on the training dataset.)",
            "probing_or_intervention_results": "Mechanistic probes in Nanda et al. identified embedding geometry and circuits mapping to rotations; this paper uses that as motivation but performs its own max-margin analyses on MLPs rather than re-running transformer probes.",
            "limitations_and_failure_modes": "The rotation solution requires presence of sinusoidal/fourier-like features and sufficient capacity; alternative constructions can realize the function without such features (dense/memorization), and various architectures/regularizers may yield only approximate frequency-sparsity.",
            "comparison_to_other_models": "Compared qualitatively to quadratic one-hidden-layer MLPs in this paper which provably attain Fourier-sparse neurons under margin maximization; ReLU MLPs empirically get near-sparse but not exact.",
            "uuid": "e4725.0",
            "source_info": {
                "paper_title": "Feature emergence via margin maximization: case studies in algebraic tasks",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Quadratic MLP Fourier features",
            "name_full": "One-hidden-layer quadratic MLP solutions for modular addition (this paper)",
            "brief_description": "For modular addition tasks, max-margin solutions of sufficiently wide one-hidden-layer quadratic networks concentrate each neuron on a single Fourier frequency (sinusoidal embedding) with phase relations u,v,w satisfying θ_u+θ_v=θ_w, and the network contains at least one neuron per nonzero frequency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "1-hidden-layer MLP with quadratic activations (phi((u,v,w),a,b)=(u_a+v_b)^2 w)",
            "model_description": "A homogeneous two-input quadratic neuron architecture with no biases; inputs are one-hot encodings of group elements (dimension d=p), neurons parameterized by u,v,w in R^d; networks trained with L_{2,3}-style regularization and gradient descent.",
            "arithmetic_task_type": "Modular addition (a + b mod p) — multiclass classification over p outputs",
            "mechanism_hypothesis": "Margin-maximizing solutions are Fourier-sparse: each neuron has input and output weight vectors whose discrete Fourier transforms are supported on a single nonzero frequency ζ, and phase offsets satisfy θ_u+θ_v=θ_w. The network aggregates neurons of different frequencies to produce sharp logits for the correct sum.",
            "evidence_for_mechanism": "Theoretical proof (Theorem 7) that any L_{2,3} max-margin solution with sufficient width (m ≥ 4(p-1)) has single-frequency neurons and includes all frequencies; derivation reduces expected class-weighted margin to a cubic form in Fourier coefficients and maximization under norm constraints yields sparsity. Empirical training (p=71, m=500, L_{2,3} reg=1e-4) shows trained weights match predicted single-frequency Fourier structure and normalized margin approaches theoretical value (Figures 1,4).",
            "evidence_against_mechanism": "Appendix D constructs alternative networks that perfectly classify but are dense in Fourier space, showing Fourier sparsity is not necessary for correctness—only selected by margin maximization. ReLU networks trained with L2 regularization show near-but-not-exact frequency sparsity.",
            "performance_metrics": "Theoretical maximum normalized L_{2,3}-margin: γ* = sqrt(2/27) * [1/(p^{1/2}(p-1))] (as stated in Theorem 7). Empirically, quadratic MLPs trained (p=71) approach this γ*; exact numeric training margin trajectories are shown in figures (training hyperparams: m=500, 40000 steps, learning rate schedule, reg=1e-4). Classification accuracy on the task reaches near 100% (trained to fit full dataset).",
            "probing_or_intervention_results": "Fourier power spectra of learned embedding vectors were plotted (Figure 1,4); quadratic networks under L_{2,3} reg show per-neuron Fourier-power concentrated at a single frequency (power ≈1), supporting the theoretical prediction. ReLU networks under L2 show approximately 1-sparse but less exact.",
            "limitations_and_failure_modes": "The proofs and observed phenomenon require: quadratic (x^2) homogeneous activations, L_{2,ν} regularization with ν equal to homogeneity, sufficient width (m ≥ 4(p-1)), and training towards max-margin regime (vanishing regularization or appropriate dynamics). Without these conditions, networks may learn different (memorizing or dense) solutions. Also the particular γ* formula scales inversely with p^{1/2}(p-1) so margin shrinks with larger p.",
            "comparison_to_other_models": "Quadratic activation + L_{2,3} yields exact theoretical single-frequency neurons; ReLU + L2 empirically gives approximate sparsity. The 1-layer transformer learned by Nanda et al. implements a related Fourier/rotation algorithm but the MLP analysis here gives a provable max-margin rationale for sinusoidal features.",
            "uuid": "e4725.1",
            "source_info": {
                "paper_title": "Feature emergence via margin maximization: case studies in algebraic tasks",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Sparse-parity x^k features",
            "name_full": "One-hidden-layer x^k MLP solutions for k-sparse parity (this paper)",
            "brief_description": "On (n,k)-sparse parity tasks, max-margin solutions for x^k-activated one-hidden-layer networks allocate equal-magnitude weights to all relevant bits and constrain sign patterns so neuron contributions align with the parity monomial; networks require sufficient width m ≥ 2^{k-1}.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "1-hidden-layer MLP with x^k activation (phi(u,w,x)=(u^T x)^k w)",
            "model_description": "Homogeneous polynomial neurons of degree k, with output weights in R^2 for binary classification; inputs in {±1}^n; trained with L_{2,k+1} regularization.",
            "arithmetic_task_type": "Sparse parity: predict product of k selected input bits (binary classification)",
            "mechanism_hypothesis": "Max-margin neurons place support exactly on the k relevant coordinates S, with equal absolute weight magnitudes across those coordinates, and output weight aligned so the product sign times output projection is nonnegative — effectively implementing the k-th order monomial.",
            "evidence_for_mechanism": "Analytic result (Theorem 8) deriving γ* = k! * sqrt{2 (k+1)^{-(k+1)}} and showing structure constraints on u_i and w_i for any maximizer. Empirical experiment on (n,k)=(10,4) with quadratic/quartic networks (m=40, reg L_{2,5}=1e-3) shows neurons and margins matching theoretical predictions (Figure 5).",
            "evidence_against_mechanism": "The result depends on homogeneous x^k activations and L_{2,k+1} norm; other activations or regularizers may yield different learned features. Computational limits: need width m ≥ 2^{k-1} to realize the full max-margin construction.",
            "performance_metrics": "Theoretical maximum margin γ* = k! * sqrt{2 (k+1)^{-(k+1)}} (Theorem 8). Empirically the network's normalized L_{2,k+1} margin approaches this predicted value in the (10,4) experiment; classification accuracy on the full training set is achieved.",
            "probing_or_intervention_results": "Visualization of neurons with highest norm and margin evolution (Figure 5) confirm that learned neuron supports align with the true parity-support coordinates and that margins track theory.",
            "limitations_and_failure_modes": "Requires exact polynomial activations and matching norm for the proof; for large k the required width grows exponentially (2^{k-1}); other architectures or noisy data may prevent emergence of the idealized monomial features.",
            "comparison_to_other_models": "This construction and analysis is specialized to x^k homogeneous neurons and is not directly compared to Transformers here; cited related work discusses parity learning limits for various architectures.",
            "uuid": "e4725.2",
            "source_info": {
                "paper_title": "Feature emergence via margin maximization: case studies in algebraic tasks",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Group irreducible-rep features",
            "name_full": "Irreducible representation-based features for finite group composition (this paper / Chughtai et al.)",
            "brief_description": "Max-margin solutions for quadratic one-hidden-layer networks learning group composition have neurons whose input/output weight vectors lie in the subspace spanned by a single nontrivial real irreducible representation of the group, and the network contains neurons for (a sufficient subset of) representations.",
            "citation_title": "A toy model of universality: Reverse engineering how networks learn group operations.",
            "mention_or_use": "mention",
            "model_name": "1-hidden-layer quadratic MLPs (and related transformer/MLP observations in prior work)",
            "model_description": "Quadratic homogeneous networks mapping one-hot encoded group elements to logits; analysis uses group representation theory (characters, matrix elements) and L_{2,3} regularization.",
            "arithmetic_task_type": "Group composition (predict c = a·b for finite group G), multiclass over |G| outputs",
            "mechanism_hypothesis": "Margin maximization selects neurons whose weight vectors are supported within basis vectors associated to a single irreducible real representation R of G (i.e., neurons implement features proportional to matrix elements R(g)_{i,j}); logits reduce to characters/traces tr(R(a)R(b)R(c^{-1})) which are maximized at c=a b.",
            "evidence_for_mechanism": "Theorem 9 proves that under conditions (sufficient width m ≥ 2 Σ_{n&gt;1} d_R^3 and sign condition Σ d_R^{1.5} χ_R(C) &lt; 0 for nontrivial conjugacy classes) the L_{2,3} max-margin network has neurons spanned by single representations and includes at least one neuron per nontrivial representation; empirical experiments on symmetric groups S3-S5 (trained networks with L_{2,3}) show neuron concentration per representation and margins approaching theoretical predictions (Figure 6 and Appendix figures). This aligns with empirical observations in Chughtai et al.",
            "evidence_against_mechanism": "The full theorem requires a technical conjugacy-class sign condition which fails for some groups (so not universally applicable). Where the condition fails, only a subset of representations may appear on the margin and some conjugacy classes may not be on-margin. Alternative dense constructions exist in general (Appendix D).",
            "performance_metrics": "Theorem 9 gives γ* = (2 / (3 sqrt{3 |G|})) * 1 / (Σ_{n=2}^K d_{R_n}^{2.5}) under stated conditions. Empirically, networks on S3,S4,S5 trained with L_{2,3} approach the predicted γ*; experiments used m up to 2000 for S5 with reg 1e-5 and SGD (see Appendix B).",
            "probing_or_intervention_results": "Projection of learned u vectors onto representation-basis shows neurons concentrating power in a single representation (Figures 6,8,9); distribution of neurons across representations matches constructive counts in proofs.",
            "limitations_and_failure_modes": "Requires real irreducible representations and the technical sign condition for full coverage; sufficient width requirement can be large (m scales with sum of cubes of representation dimensions); not all finite groups satisfy the theorem's assumptions; learning dynamics and regularization regime must approach max-margin behavior.",
            "comparison_to_other_models": "This formalizes and explains empirical findings of Chughtai et al. who observed representation-based features in MLPs/transformers; quadratic activation + L_{2,3} yields clean theoretical alignment, whereas other activations/regularizers may give approximate behavior.",
            "uuid": "e4725.3",
            "source_info": {
                "paper_title": "Feature emergence via margin maximization: case studies in algebraic tasks",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Memorization dense construction",
            "name_full": "Alternative dense/memorization network construction (Appendix D, this paper)",
            "brief_description": "An explicit construction showing that for modular addition (and group tasks) there exist networks that perfectly memorize the input-output mapping using pairs of localized one-hot-like neurons, producing dense (non-sparse) Fourier spectra; such solutions fit data but have lower margin.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Constructed two-neuron-per-example quadratic MLP",
            "model_description": "For each training pair (a,b) the construction uses two neurons with u and v set to one-hot indicators and w chosen to place the correct logit; summing across examples yields a network that outputs exact one-hot logits for training pairs.",
            "arithmetic_task_type": "Modular addition or arbitrary mapping r(a,b) → c",
            "mechanism_hypothesis": "Memorization by localized neurons: neurons are one-hot in input space and produce outputs only for a single (a,b) pair; no global Fourier or representation structure is required.",
            "evidence_for_mechanism": "Appendix D gives explicit algebraic construction of neurons (u,v,w and u',v',w') per (a,b) such that summed outputs equal indicator of chosen c, and those weight vectors are one-hot so their Fourier transforms are uniform (dense) rather than sparse.",
            "evidence_against_mechanism": "Although this construction classifies perfectly, it has lower margin than the Fourier-sparse max-margin solutions and thus is not generally found by margin-seeking training dynamics (empirically not observed under the regularized training regimes used).",
            "performance_metrics": "Construction achieves perfect training classification (1 on correct logit, 0 elsewhere) by design; however no margin-optimality — hence will not be the solution in the vanishing-regularization regime targeted by the paper.",
            "probing_or_intervention_results": "Analytic construction rather than an empirical probe; used to demonstrate non-uniqueness of classifiers and to argue that Fourier sparsity is an inductive-bias-selected property, not an intrinsic necessity.",
            "limitations_and_failure_modes": "Highly overparameterized in terms of neurons (2 p^2 neurons for cyclic case), not parsimonious, fails to exhibit the structured features seen in trained networks under margin-maximizing dynamics.",
            "comparison_to_other_models": "Contrasts with margin-derived Fourier/representation solutions; serves as a baseline showing many classifiers exist but margin bias selects structured ones.",
            "uuid": "e4725.4",
            "source_info": {
                "paper_title": "Feature emergence via margin maximization: case studies in algebraic tasks",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability.",
            "rating": 2
        },
        {
            "paper_title": "A toy model of universality: Reverse engineering how networks learn group operations.",
            "rating": 2
        },
        {
            "paper_title": "Grokking modular arithmetic.",
            "rating": 2
        },
        {
            "paper_title": "The clock and the pizza: Two stories in mechanistic explanation of neural networks.",
            "rating": 1
        },
        {
            "paper_title": "Groking: Generalization beyond overfitting on small algorithmic datasets.",
            "rating": 1
        }
    ],
    "cost": 0.015612499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Feature emergence via margin maximization: case studies in algebraic tasks</h1>
<p>Depen Morwani, Benjamin L. Edelman, Costin-Andrei Oncescu, Rosie Zhao, Sham Kakade<br>Harvard University<br>{dmorwani, bedelman, concescu, rosiezhao}@g.harvard.edu, sham@seas.harvard.edu</p>
<h4>Abstract</h4>
<p>Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding how neural networks implement specific target functions, this paper explores a complementary question - why do networks arrive at particular computational strategies? Our inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. Specifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible group-theoretic representations to perform compositions in general groups, aligning closely with the empirical observations of Nanda et al. (2023) and Chughtai et al. (2023). More generally, we hope our techniques can help to foster a deeper understanding of why neural networks adopt specific computational strategies.</p>
<h2>1 Introduction</h2>
<p>Opening the black box of neural networks has the potential to enable safer and more reliable deployments, justifications for model outputs, and clarity on how model behavior will be affected by changes in the input distribution. The research area of mechanistic interpretability (Olah et al., 2020; Elhage et al., 2021; Olsson et al., 2022; Elhage et al., 2022) aims to dissect individual trained neural networks in order to shed light on internal representations, identifying and interpreting sub-circuits that contribute to the networks' functional behavior. Mechanistic interpretability analyses typically leave open the question of why the observed representations arise as a result of training.</p>
<p>Meanwhile, the theoretical literature on inductive biases in neural networks (Soudry et al., 2018; Shalev-Shwartz \&amp; Ben-David, 2014; Vardi, 2023) aims to derive general principles governing which solutions will be preferred by trained neural networks - in particular, in the presence of underspecification, where there are many distinct ways a network with a given architecture could perform well on the training data. Most work on inductive bias in deep learning is motivated by the question of understanding why networks generalize from their training data to unobserved test data. It can be non-obvious how to apply the results from this literature to understand what solution will be found when a particular architecture is trained on a particular type of dataset.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Final trained embeddings and their Fourier power spectrum for a 1-hidden layer ReLU network trained on a mod-71 addition dataset with $L_{2}$ regularization. Each row corresponds to an arbitrary neuron from the trained network. The red dots represent the actual value of the weights, while the light blue interpolation is obtained by finding the function over the reals with the same Fourier spectrum as the weight vector. (b) Similar plot for 1-hidden layer quadratic activation, trained with $L_{2,3}$ regularization (Section 2) (c) For the quadratic activation, the network asymptotically reaches the maximum $L_{2,3}$ margin predicted by our analysis.</p>
<p>In this work, we show that the empirical findings of Nanda et al. (2023) and Chughtai et al. (2023), about the representations found by networks trained to perform finite group operations, can be analytically explained by the inductive bias of the regularized optimization trajectory towards margin maximization. Informally, the network maximizes the margin if it attains a given confidence level on all the points in the dataset, with the smallest total parameter norm possible. Perhaps surprisingly, the margin maximization property alone typically used for the study of generalization - is sufficient to comprehensively and precisely characterize the richly structured features that are actually learned by neural networks in these settings. Let's begin by reviewing the case of learning modular addition with neural networks, first studied in Power et al. (2022) in their study of "grokking".</p>
<p>Nanda et al.'s striking observations. Nanda et al. (2023) investigated the problem of how neural networks learn modular addition (using a 1-layer transformer); they consider the</p>
<p>problem of computing $a+b \bmod p$, where $p$ is a prime number. The findings were unexpected and intriguing: SGD not only reliably solves this problem (as originally seen in Power et al. (2022)) but also consistently learns to execute a particular algorithm, as illustrated by the learned embedding weights in Figure 1. This geometric algorithm simplifies the task to composing integer rotations around a circle ${ }^{1}$.</p>
<p>The algorithm above fundamentally relies on the following identity: for any $a, b \in \mathbb{Z}<em p="p">{p}$ and $k \in \mathbb{Z}</em> \backslash{0}$,</p>
<p>$$
(a+b) \bmod p=\underset{c \in \mathbb{Z}_{p}}{\arg \max }\left{\cos \left(\frac{2 \pi k(a+b-c)}{p}\right)\right}
$$</p>
<p>This identity also leads to other natural algorithms (still relying on sinusoidal features) that are generally implemented by neural networks, as shown in Zhong et al. (2023).</p>
<p>These findings prompt the question: why does the network consistently prefer such Fourier-based circuits, amidst other potential circuits capable of performing the same modular addition function?</p>
<h1>Our Contributions.</h1>
<ul>
<li>We formulate general techniques for analytically characterizing the maximum margin solutions for tasks exhibiting symmetry.</li>
<li>For sufficiently wide one-hidden layer MLPs with quadratic activations, we use these techniques to characterize the structure of the weights of max-margin solutions for certain algebraic tasks including modular addition, sparse parities and general group operations.</li>
<li>We empirically validate that neural networks trained using gradient descent with small regularization approach the maximum margin solution (Theorem 1), and the weights of trained networks match those predicted by our theory (Figure 1).</li>
</ul>
<p>Our theorem for modular addition shows that Fourier features are indeed the global maximum margin solution:</p>
<p>Informal Theorem (Modular addition). Consider a single hidden layer neural network of width $m$ with $x^{2}$ activations trained on the modular addition task (modulo $p$ ). For $m \geq$ $4(p-1)$, any maximum margin solution for the full population dataset satisfies the following:</p>
<ul>
<li>For every neuron, there exists a frequency such that the Fourier spectra of the input and output weight vectors are supported only on that frequency.</li>
<li>There exists at least one neuron of each frequency in the network.</li>
</ul>
<p>Note that even with this activation function, there are solutions that fit all the data points, but where the weights do not exhibit any sparsity in Fourier space-see Appendix D for an example construction. Such solutions, however, have lower margin and thus are not reached by training.</p>
<p>In the case of $k$-sparse parity learning with an $x^{k}$-activation network, we show margin maximization implies that the weights assigned to all relevant bits are of the same magnitude, and the sign pattern of the weights satisfies a certain condition.</p>
<p>For learning on the symmetric group (or other groups with real representations), we use the machinery of representation theory (Kosmann-Schwarzbach et al., 2010) to show that</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of an individual neuron $\phi({u, v, w}, a, b)$ (left) and the resulting one hidden layer neural network $f(\theta, a, b)$ (right) with quadratic activations.
learned features correspond to the irreducible representations of the group, as observed by Chughtai et al. (2023).</p>
<p>Two closely related works to ours are Gromov (2023) and Bronstein et al. (2022). Gromov (2023) provides an analytic construction of various two-layer quadratic networks that can solve the modular addition task. The construction used in the proof of Theorem 7 is a special case of the given scheme. Bronstein et al. (2022) shows that all max margin solutions of a one-hidden-layer ReLU network (with fixed top weights) trained on read-once DNFs have neurons which align with clauses. However, their proof technique for characterizing max margin solutions is very different. For more details, refer to Appendix A.</p>
<p>Paper organization: In section 1, we delineate our contributions and discuss a few related works. In section 2, we state preliminary definitions. In section 3, we sketch our theoretical methodology, and state general lemmas which will be applied in all three case studies. In sections 4,5 , and 6 , we use the above lemmas to characterize the max margin features for the modular addition, sparse parity and group operation tasks respectively. We discuss and conclude the paper in section 7. Further related work, full proofs, hyperparameter choices, and additional experimental results can be found in the Appendix.</p>
<h1>2 Preliminaries</h1>
<p>In this work, we will consider one-hidden layer neural networks with homogeneous polynomial activations, such as $x^{2}$, and no biases. The network output for a given input $x$ will be represented as $f(\theta, x)$, where $\theta \in \Theta$ represents the parameters of the neural network. The homogeneity constant of the network is defined as a constant $\nu$ such that for any scaling factor $\lambda&gt;0, f(\lambda \theta, x)=\lambda^{\nu} f(\theta, x)$ for all inputs $x$.</p>
<p>In the case of 1 -hidden layer networks, $f$ can be further decomposed as:
$f(\theta, x)=\sum_{i=1}^{m} \phi\left(\omega_{i}, x\right)$, where $\theta=\left{\omega_{1}, \ldots, \omega_{m}\right}, \phi$ represents an individual neuron within the network, and $\omega_{i} \in \Omega$ denotes the weights from the input to the $i^{\text {th }}$ neuron and from the neuron to the output. $\theta=\left{\omega_{1}, \ldots, \omega_{m}\right}$ is said to have directional support on $\Omega^{\prime} \subseteq \Omega$ if for all $i \in{1, \ldots, m}$, either $\omega_{i}=0$ or $\lambda_{i} \omega_{i} \in \Omega^{\prime}$ for some $\lambda_{i}&gt;0$. In this work, we will be primarily concerned with networks that have homogeneous neurons, i.e, $\phi\left(\lambda \omega_{i}, x\right)=\lambda^{\nu} \phi\left(\omega_{i}, x\right)$ for any scaling constant $\lambda&gt;0$.</p>
<p>For Sections 4 and 6 corresponding to cyclic and general finite groups respectively, we will consider neural networks with quadratic activations (Figure 2). A single neuron will be represented as $\phi\left({u, v, w}, x^{(1)}, x^{(2)}\right)=\left(u^{\top} x^{(1)}+v^{\top} x^{(2)}\right)^{2} w$, where $u, v, w \in \mathbb{R}^{d}$ are the weights associated with a neuron and $x^{(1)}, x^{(2)} \in \mathbb{R}^{d}$ are the inputs provided to the network (note that $\phi\left({u, v, w}, x^{(1)}, x^{(2)}\right) \in \mathbb{R}^{d}$ ). For these tasks, we set $d=|G|$, where $G$ refers to either the cyclic group or a general group. We will also consider the inputs $x^{(1)}$ and $x^{(2)}$ to be</p>
<p>one-hot vectors, representing the group elements being provided as inputs. Thus, for given input elements $(a, b)$, a single neuron can be simplified as $\phi({u, v, w}, a, b)=\left(u_{a}+v_{b}\right)^{2} w$, where $u_{a}$ and $v_{b}$ represent the $a^{t h}$ and $b^{t h}$ component of $u$ and $v$ respectively. Overall, the network will be given by</p>
<p>$$
f(\theta, a, b)=\sum_{i=1}^{m} \phi\left(\left{u_{i}, v_{i}, w_{i}\right}, a, b\right)
$$</p>
<p>with $\theta=\left{u_{i}, v_{i}, w_{i}\right}_{i=1}^{m}$ (note that $f(\theta, a, b) \in \mathbb{R}^{d}$ ).
For Section 5, we will consider the $(n, k)$-sparse parity problem, where the parity is computed on $k$ bits out of $n$. For this task, we will consider a neural network with the activation function $x^{k}$. A single neuron within the neural network will be represented as $\phi({u, w}, x)=\left(u^{\top} x\right)^{k} w$, where $u \in \mathbb{R}^{n}, w \in \mathbb{R}^{2}$ are the weights associated with a neuron and $x \in \mathbb{R}^{n}$ is the input provided to the network. The overall network will represented as</p>
<p>$$
f(\theta, x)=\sum_{i=1}^{m} \phi\left(\left{u_{i}, w_{i}\right}, x\right)
$$</p>
<p>where $\theta=\left{u_{i}, w_{i}\right}<em k="k">{i=1}^{m}$.
For any vector $v$ and $k \geq 1,|v|</em>\right}}$ represents $\left(\sum\left|v_{i}\right|^{k}\right)^{1 / k}$. For a given neural network with parameters $\theta=\left{\omega_{i<em a_="a," b="b">{i=1}^{m}$, the $L</em>$ norm of $\theta$ is given by $|\theta|<em i="1">{a, b}=\left(\sum</em>\right|}^{m}\left|\omega_{i<em i="i">{a}^{b}\right)^{1 / b}$. Here $\left{\omega</em>\right}$ represents the concatenated vector of parameters corresponding to a single neuron.</p>
<h1>3 Theoretical Approach</h1>
<p>Suppose we have a dataset $D \subseteq \mathcal{X} \times \mathcal{Y}$, a norm $|\cdot|$ and a class of parameterized functions $\left{f(\theta, \cdot) \mid \theta \in \mathbb{R}^{U}\right}$, where $f: \mathbb{R}^{U} \times \mathcal{X} \rightarrow \mathbb{R}^{\mathcal{Y}}$ and $\Theta={|\theta| \leq 1}$. We define the margin function $g: \mathbb{R}^{U} \times \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}$ as being, for a given datapoint $(x, y) \in D$,</p>
<p>$$
g(\theta, x, y)=f(\theta, x)[y]-\max _{y^{\prime} \in \mathcal{Y} \backslash{y}} f(\theta, x)\left[y^{\prime}\right]
$$</p>
<p>Then, the margin of the dataset $D$ is given by $h: \mathbb{R}^{U} \rightarrow \mathbb{R}$ defined as</p>
<p>$$
h(\theta)=\min _{(x, y) \in D} g(\theta, x, y)
$$</p>
<p>Similarly, we define the normalized margin for a given $\theta$ as $h(\theta /|\theta|)$.
We train using the regularized objective</p>
<p>$$
\mathcal{L}<em D="D" _in="\in" _x_="(x," y_="y)">{\lambda}(\theta)=\frac{1}{|D|} \sum</em>
$$} l(f(\theta, x), y)+\lambda|\theta|^{r</p>
<p>where $l$ is the cross-entropy loss. Let $\theta_{\lambda} \in \arg \min <em _lambda="\lambda">{\theta \in \mathbb{R}^{U}} \mathcal{L}</em>$. Let $\gamma^{}(\theta)$ be a minimum of this objective, and let $\gamma_{\lambda}=h\left(\theta_{\lambda} /\left|\theta_{\lambda}\right|\right)$ be the normalized margin of $\theta_{\lambda<em>}=\max <em _lambda="\lambda">{\theta \in \Theta} h(\theta)$ be the maximum normalized margin. The following theorem of Wei et al. (2019a) states that, when using vanishingly small regularization $\lambda$, the normalized margin of global optimizers of $\mathcal{L}</em>$ converges to $\gamma^{</em>}$.</p>
<p>Theorem 1 (Wei et al. (2019a), Theorem 4.1). For any norm $|\cdot|$, a fixed $r&gt;0$ and any homogeneous function $f$ with homogeneity constant $\nu&gt;0$, if $\gamma^{<em>}&gt;0$, then $\lim <em _lambda="\lambda">{\lambda \rightarrow 0} \gamma</em>=\gamma^{</em>}$.</p>
<p>This provides the motivation behind studying maximum margin classifiers as a proxy for understanding the global minimizers of $\mathcal{L}<em _Theta="\Theta" _in="\in" _theta="\theta">{\lambda}$ as $\lambda \rightarrow 0$. Henceforth, we will focus on characterizing the maximum margin solution: $\Theta^{*}:=\arg \max </em> h(\theta)$.</p>
<p>Note that the maximum margin $\gamma^{*}$ is given by</p>
<p>$$
\begin{aligned}
\gamma^{*} &amp; =\max <em D="D" _in="\in" _x_="(x," y_="y)">{\theta \in \Theta} \min </em> g(\theta, x, y) \
&amp; =\max <em _in="\in" _mathcal_P="\mathcal{P" q="q">{\theta \in \Theta} \min </em>[g(\theta, x, y)]
\end{aligned}
$$}(D)} \underset{(x, y) \sim q}{\mathbb{E}</p>
<p>where $q$ represents a distribution over data points in $D$. The primary approach in this work for characterizing the maximum margin solution is to exhibit a pair $\left(\theta^{<em>}, q^{</em>}\right)$ such that</p>
<p>$$
\begin{aligned}
&amp; q^{<em>} \in \underset{q \in \mathcal{P}(D)}{\arg \min } \underset{(x, y) \sim q}{\mathbb{E}}\left[g\left(\theta^{</em>}, x, y\right)\right] \
&amp; \theta^{<em>} \in \underset{\theta \in \Theta}{\arg \max } \underset{(x, y) \sim q^{</em>}}{\mathbb{E}}[g(\theta, x, y)]
\end{aligned}
$$</p>
<p>That is, $q^{<em>}$ is one of the minimizers of the expected margin with respect to $\theta^{</em>}$ and $\theta^{<em>}$ is one of the maximizers of the expected margin with respect to $q^{</em>}$. The lemma below uses the max-min inequality (Boyd \&amp; Vandenberghe, 2004) to show that exhibiting such a pair is sufficient for establishing that $\theta^{*}$ is indeed a maximum margin solution. The proof for the lemma can be found in Appendix E.</p>
<p>Lemma 2. If a pair $\left(\theta^{<em>}, q^{</em>}\right)$ satisfies Equations 1 and 2, then</p>
<p>$$
\theta^{*} \in \underset{\theta \in \Theta}{\arg \max } \min _{(x, y) \in D} g(\theta, x, y)
$$</p>
<p>In the following subsections, we will describe our approach for finding such a pair for 1-hidden layer homogeneous neural networks. Furthermore, we will show how exhibiting just a single pair of the above form can enable us to characterize the set of all maximum margin solutions. We start off with the case of binary classification, and then extend the techniques to multi-class classification.</p>
<h1>3.1 Binary Classification</h1>
<p>In the context of binary classification where $|\mathcal{Y}|=2$, the margin function $g$ for a given datapoint $(x, y) \in D$ is given by</p>
<p>$$
g(\theta, x, y)=f(\theta, x)[y]-f(\theta, x)\left[y^{\prime}\right]
$$</p>
<p>where $y^{\prime} \neq y$. For 1-hidden layer neural networks, by linearity of expectation, the expected margin is given by</p>
<p>$$
\underset{(x, y) \sim q}{\mathbb{E}}[g(\theta, x, y)]=\sum_{i=1}^{m} \underset{(x, y) \sim q}{\mathbb{E}}\left[\phi\left(\omega_{i}, x\right)[y]-\phi\left(\omega_{i}, x\right)\left[y^{\prime}\right]\right]
$$</p>
<p>where $y^{\prime} \neq y$ and $\theta=\left{\omega_{i}\right}_{i=1}^{m}$. Since the expected margin of the network decomposes into the sum of expected margin of individual neurons, finding a maximum expected margin network simplifies to finding maximum expected margin neurons. Denoting $\psi(\omega, x, y)=$ $\phi(\omega, x)[y]-\phi(\omega, x)\left[y^{\prime}\right]$, the following lemma holds:</p>
<p>Lemma 3. Let $\Theta=\left{\theta:|\theta|<em q="q">{a, b} \leq 1\right}$ and $\Theta</em>^{<em>}=\arg \max <em _sim="\sim" _x_="(x," q="q" y_="y)">{\theta \in \Theta} \mathbb{E}</em>[g(\theta, x, y)]$. Similarly, let $\Omega=\left{\omega:|\omega|<em q="q">{a} \leq 1\right}$ and $\Omega</em>^{</em>}=\arg \max <em _sim="\sim" _x_="(x," q="q" y_="y)">{\omega \in \Omega} \mathbb{E}</em>[\psi(\omega, x, y)]$. For binary classification:</p>
<ul>
<li>Single neuron optimization: Any $\theta \in \Theta_{q}^{<em>}$ has directional support only on $\Omega_{q}^{</em>}$.</li>
<li>Combining neurons: If $b=\nu$ (the homogeneity constant of the network) and $\omega_{1}^{<em>}, \ldots, \omega_{m}^{</em>} \in \Omega_{q}^{<em>}$, then for any neuron scaling factors $\sum \lambda_{i}^{\nu}=1, \lambda_{i} \geq 0$, we have that $\theta=\left{\lambda_{i} \omega_{i}^{</em>}\right}<em q="q">{i=1}^{m}$ belongs to $\Theta</em>$.}^{*</li>
</ul>
<p>The proof for the above lemma can be found in Appendix E.1.
To find a $\left(\theta^{<em>}, q^{</em>}\right)$ pair, we will start with a guess for $q^{<em>}$ (which will be the uniform distribution in our case as the datasets are symmetric). Then, using the first part of Lemma 3, we will find all neurons which can be in the support of $\theta^{</em>}$ satisfying Equation 2 for the given $q^{<em>}$. Finally, for specific norms of the form $|\cdot|_{a, \nu}$, we will combine the obtained neurons using the second part of Lemma 3 to obtain a $\theta^{</em>}$ such that $q^{*}$ satisfies Equation 1.</p>
<p>We think of $\left(\theta^{<em>}, q^{</em>}\right)$ as a "certificate pair". By just identifying this single solution, we can characterize the set of all maximum margin solutions. Denoting $\operatorname{spt}(q)={(x, y) \in D \mid$ $q(x, y)&gt;0}$, the following lemma holds:</p>
<p>Lemma 4. Let $\Theta=\left{\theta:|\theta|<em q="q">{a, b} \leq 1\right}$ and $\Theta</em>^{<em>}=\arg \max <em _sim="\sim" _x_="(x," q="q" y_="y)">{\theta \in \Theta} \mathbb{E}</em>[g(\theta, x, y)]$. Similarly, let $\Omega=\left{\omega:|\omega|<em q="q">{a} \leq 1\right}$ and $\Omega</em>^{</em>}=\arg \max <em _sim="\sim" _x_="(x," q="q" y_="y)">{\omega \in \Omega} \mathbb{E}</em>[\psi(\omega, x, y)]$. For the task of binary classification, if there exists $\left{\theta^{<em>}, q^{</em>}\right}$ satisfying Equation 1 and 2, then any $\hat{\theta} \in$ $\arg \max <em D="D" _in="\in" _x_="(x," y_="y)">{\theta \in \Theta} \min </em> g(\theta, x, y)$ satisfies the following:</p>
<ul>
<li>$\hat{\theta}$ has directional support only on $\Omega_{q^{<em>}}^{</em>}$.</li>
<li>For any $(x, y) \in \operatorname{spt}\left(q^{<em>}\right), f(\hat{\theta}, x, y)-f\left(\hat{\theta}, x, y^{\prime}\right)=\gamma^{</em>}$, where $y^{\prime} \neq y$; i.e., all points in the support of $q^{*}$ are "on the margin" for any maximum margin solution.</li>
</ul>
<p>The proof for the above lemma can be found in Appendix E.1.
Thus, we can say that the neurons found by Lemma 3 are indeed the exhaustive set of neurons for any maximum margin network. Moreover, any maximum margin solution will have the support of $q^{*}$ on the margin.</p>
<h1>3.2 Multi-Class Classification</h1>
<p>The modular addition and general finite group tasks are multi-class classification problems. For multi-class classification, the margin function $g$ for a given datapoint $(x, y) \in D$ is given by</p>
<p>$$
g(\theta, x, y)=f(\theta, x)[y]-\max _{y^{\prime} \in \mathcal{Y} \backslash{y}} f(\theta, x)\left[y^{\prime}\right]
$$</p>
<p>For 1-hidden layer networks, the expected margin is given by</p>
<p>$$
\underset{(x, y) \sim q}{\mathbb{E}}[g(\theta, x, y)]=\underset{(x, y) \sim q}{\mathbb{E}}\left[\sum_{i=1}^{m} \phi\left(\omega_{i}, x\right)[y]-\max <em i="1">{y^{\prime} \in \mathcal{Y} \backslash{y}} \sum</em>\right]\right]
$$}^{m} \phi\left(\omega_{i}, x\right)\left[y^{\prime</p>
<p>Here, due to the max operation, we cannot swap the summation and expectation, and thus the expected margin of the network does not decompose into the expected margins of the neurons as it did in the binary classification case.</p>
<p>To circumvent this issue, we will introduce the notion of class-weighted margin. Consider some $\tau: D \rightarrow \Delta(\mathcal{Y})$ that assigns a weighting of incorrect labels to every datapoint. For any $(x, y) \in D$, let $\tau$ satisfy the properties that $\sum_{y^{\prime} \in \mathcal{Y} \backslash{y}} \tau(x, y)\left[y^{\prime}\right]=1$ and $\tau(x, y)\left[y^{\prime}\right] \geq 0$ for all $y^{\prime} \in \mathcal{Y}$. Using this, we define the class-weighted margin $g^{\prime}$ for a given datapoint $(x, y) \in D$ as</p>
<p>$$
g^{\prime}(\theta, x, y)=f(\theta, x)[y]-\sum_{y^{\prime} \in \mathcal{Y} \backslash{y}} \tau(x, y)\left[y^{\prime}\right] f(\theta, x)\left[y^{\prime}\right]
$$</p>
<p>Note that $g^{\prime}(\theta, x, y) \geq g(\theta, x, y)$ as $g^{\prime}$ replaces the max by a weighted sum. Moreover, by linearity of expectation we can say that</p>
<p>$$
\underset{(x, y) \sim q}{\mathbb{E}}\left[g^{\prime}(\theta, x, y)\right]=\sum_{i=1}^{m} \underset{(x, y) \sim q}{\mathbb{E}}\left[\phi\left(\omega_{i}, x\right)[y]-\sum_{y^{\prime} \in \mathcal{Y} \backslash{y}} \tau(x, y)\left[y^{\prime}\right] \phi\left(\omega_{i}, x\right)\left[y^{\prime}\right]\right]
$$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A schematic illustration of the relation between class-weighted margin $g^{\prime}$ and maximum margin $g$.</p>
<p>Denoting $\psi^{\prime}(\omega, x, y)=\phi(\omega, x)[y]-\sum_{y^{\prime} \in \mathcal{Y} \backslash{y}} \tau(x, y)\left[y^{\prime}\right] \phi(\omega, x)\left[y^{\prime}\right]$, a result analogous to Lemma 3 holds for the class-weighted margin (proof can be found in Appendix E.2):
Lemma 5. Let $\Theta=\left{\theta:|\theta|<em q="q">{a, b} \leq 1\right}$ and $\Theta</em>^{\prime <em>}=\arg \max <em _sim="\sim" _x_="(x," q="q" y_="y)">{\theta \in \Theta} \mathbb{E}</em>(\theta, x, y)\right]$. Similarly, let $\Omega=\left{\omega:|\omega|}\left[g^{\prime<em q="q">{a} \leq 1\right}$ and $\Omega</em>^{\prime </em>}=\arg \max <em _sim="\sim" _x_="(x," q="q" y_="y)">{\omega \in \Omega} \mathbb{E}</em>(\omega, x, y)\right]$. Then:}\left[\psi^{\prime</p>
<ul>
<li>Single neuron optimization: Any $\theta \in \Theta_{q}^{\prime <em>}$ has directional support only on $\Omega_{q}^{\prime </em>}$.</li>
<li>Combining neurons: If $b=\nu$ and $\omega_{1}^{<em>}, \ldots, \omega_{m}^{</em>} \in \Omega_{q}^{\prime <em>}$, then for any neuron scaling factors $\sum \lambda_{i}^{\nu}=1, \lambda_{i} \geq 0$, we have that $\theta=\left{\lambda_{i} \omega_{i}^{</em>}\right}<em q="q">{i=1}^{m}$ belongs to $\Theta</em>$.}^{\prime *</li>
</ul>
<p>The above lemma helps us characterize $\Theta_{q}^{\prime <em>}$ for a given distribution $q$. Thus, applying it to a given $q^{</em>}$, we can find</p>
<p>$$
\theta^{<em>} \in \underset{\theta \in \Theta}{\arg \max } \underset{(x, y) \sim q^{</em>}}{\mathbb{E}}\left[g^{\prime}(\theta, x, y)\right]
$$</p>
<p>To further ensure that $\theta^{<em>}$ also satisfies the corresponding equation for $g$ (i.e., Equation 2) we will consider the following condition:
C. 1 For any $(x, y) \in \operatorname{spt}\left(q^{</em>}\right)$, it holds that $g^{\prime}\left(\theta^{<em>}, x, y\right)=g\left(\theta^{</em>}, x, y\right)$. This translates to any label with non-zero weight being one of the incorrect labels where $f$ is maximized: ${\ell \in \mathcal{Y} \backslash{y}: \tau(x, y)[\ell]&gt;0} \subseteq \underset{\ell \in \mathcal{Y} \backslash{y}}{\arg \max } f\left(\theta^{*}, x\right)[\ell]$.</p>
<p>The main lemma used for finding the maximum margin solutions for multi-class classification is stated below:</p>
<p>Lemma 6. Let $\Theta=\left{\theta:|\theta|<em q="q">{a, b} \leq 1\right}$ and $\Theta</em>^{\prime <em>}=\arg \max <em _sim="\sim" _x_="(x," q="q" y_="y)">{\theta \in \Theta} \mathbb{E}</em>(\theta, x, y)\right]$. Similarly, let $\Omega=\left{\omega:|\omega|}\left[g^{\prime<em q="q">{a} \leq 1\right}$ and $\Omega</em>^{\prime </em>}=\arg \max <em _sim="\sim" _x_="(x," q="q" y_="y)">{\omega \in \Omega} \mathbb{E}</em>(\omega, x, y)\right]$. If $\exists\left{\theta^{}\left[\psi^{\prime<em>}, q^{</em>}\right}$ satisfying Equations 1 and 3, and C. 1 holds, then:</p>
<ul>
<li>$\theta^{*} \in \arg \max _{\theta \in \Theta} g(\theta, x, y)$</li>
<li>Any $\hat{\theta} \in \arg \max <em D="D" _in="\in" _x_="(x," y_="y)">{\theta \in \Theta} \min </em> g(\theta, x, y)$ satisfies the following:</li>
<li>$\hat{\theta}$ has directional support only on $\Omega_{q^{<em>}}^{\prime </em>}$.</li>
<li>For any $(x, y) \in \operatorname{spt}\left(q^{<em>}\right), f(\hat{\theta}, x, y)-\max _{y^{\prime} \in \mathcal{Y} \backslash{y}} f\left(\hat{\theta}, x, y^{\prime}\right)=\gamma^{</em>}$, i.e, all points in the support of $q^{*}$ are on the margin for any maximum margin solution.</li>
</ul>
<p>The first part of the above lemma follows from the fact that $g^{\prime}(\theta, x, y) \geq g(\theta, x, y)$. Thus, any maximizer of $g^{\prime}$ satisfying $g^{\prime}=g$ is also a maximizer of $g$ (See Figure 3). The second part states that the neurons found using Lemma 5 are indeed the exhaustive set of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The maximum normalized power of the embedding vector of a neuron is given by $\max <em 2="2">{i}|\hat{u}[i]|^{2} /\left(\sum|\hat{u}[j]|^{2}\right)$, where $\hat{u}[i]$ represents the $i^{t h}$ component of the Fourier transform of $u$. (a) Initially, the maximum power is randomly distributed. (b) For 1-hidden layer ReLU network trained with $L</em>$ regularization, the final maximum power is almost exactly 1 for all the neurons, so the embeddings are 1-sparse in frequency space, as predicted by the maximum margin analysis.
neurons for any maximum margin network. Moreover, any maximum margin solution has the support of $q^{*}$ on margin. The proof for the lemma can be found in Appendix E.2.}$ regularization, the final distribution of maximum power seems to be concentrated around 0.9 , meaning neurons are nearly 1 -sparse in frequency space but not quite. (c) For 1-hidden layer quadratic network trained with $L_{2,3</p>
<p>Overall, to find a $\left(\theta^{<em>}, q^{</em>}\right)$ pair, we will start with a guess of $q^{<em>}$ (which will be uniform in our case as the datasets are symmetric) and a guess of the weighing $\tau$ (which will be uniform for the modular addition case). Then, using the first part of Lemma 5, we will find all neurons which can be in the support of $\theta^{</em>}$ satisfying Equation 3 for the given $q^{<em>}$. Finally, for specific norms of the form $|\cdot|_{a, \nu}$, we will combine the obtained neurons using the second part of Lemma 5 to obtain a $\theta^{</em>}$ such that it satisfies C. 1 and $q^{*}$ satisfies Equation 1. Thus, we will primarily focus on maximum margin with respect to $L_{2, \nu}$ norm in this work.</p>
<h1>3.3 Blueprint for the case studies</h1>
<p>In each case study, we want to find a certificate pair: a network $\theta^{<em>}$ and a distribution on the input data points $q^{</em>}$, such that Equation 1 and 2 are satisfied. Informally, these are the main steps involved in the proof approach:</p>
<ol>
<li>As the datasets we considered are symmetric, we consider $q^{*}$ to be uniformly distributed on the input data points.</li>
<li>Using the Single neuron optimization part of Lemma 5, we find all neurons that maximize the expected class-weighted margin. Only these neurons can be part of a network $\theta^{*}$ satisfying Equation 3.</li>
<li>Using the Combining neurons part of Lemma 5, we combine the above neurons into a network $\theta^{<em>}$ such that
(a) All input points are on the margin, i.e, $q^{</em>}$ satisfies Equation 1.
(b) The class-weighted margin is equal to the maximum margin, i.e, $\theta^{*}$ satisfies C.1.</li>
</ol>
<p>Then, using Lemma 6, we can say that the network $\theta^{*}$ maximizes the margin.</p>
<h1>4 Cyclic groups (modular addition)</h1>
<p>For a prime $p&gt;2$, let $\mathbb{Z}<em p="p">{p}$ denote the cyclic group on $p$ elements. For a function $f: \mathbb{Z}</em>$ is defined as} \rightarrow \mathbb{C}$, the discrete Fourier transform of $f$ at a frequency $j \in \mathbb{Z}_{p</p>
<p>$$
\hat{f}(j):=\sum_{k \in \mathbb{Z}_{p}} f(k) \exp (-2 \pi i \cdot j k / p)
$$</p>
<p>Note that we can treat a vector $v \in \mathbb{C}^{p}$ as a function $v: \mathbb{Z}<em p="p">{p} \rightarrow \mathbb{C}$, thereby endowing it with a Fourier transform. Consider the input space $\mathcal{X}:=\mathbb{Z}</em>} \times \mathbb{Z<em p="p">{p}$ and output space $\mathcal{Y}:=\mathbb{Z}</em>\right}$.}$. Let the dataset $D_{p}:=\left{((a, b), a+b): a, b \in \mathbb{Z}_{p</p>
<p>Theorem 7. Consider one-hidden layer networks $f(\theta, a, b)$ of the form given in section 2 with $m \geq 4(p-1)$ neurons. The maximum $L_{2,3}$-margin of such a network on the dataset $D_{p}$ is:</p>
<p>$$
\gamma^{*}=\sqrt{\frac{2}{27}}: \frac{1}{p^{1 / 2}(p-1)}
$$</p>
<p>Any network achieving this margin satisfies the following conditions:</p>
<ol>
<li>for each neuron $\phi({u, v, w} ; a, b)$ in the network, there exists a scaling constant $\lambda \in \mathbb{R}$ and a frequency $\zeta \in\left{1, \ldots, \frac{p-1}{2}\right}$ such that</li>
</ol>
<p>$$
\begin{aligned}
u(a) &amp; =\lambda \cos \left(\theta_{u}^{<em>}+2 \pi \zeta a / p\right) \
v(b) &amp; =\lambda \cos \left(\theta_{v}^{</em>}+2 \pi \zeta b / p\right) \
w(c) &amp; =\lambda \cos \left(\theta_{w}^{*}+2 \pi \zeta c / p\right)
\end{aligned}
$$</p>
<p>for some phase offsets $\theta_{u}^{<em>}, \theta_{v}^{</em>}, \theta_{w}^{<em>} \in \mathbb{R}$ satisfying $\theta_{u}^{</em>}+\theta_{v}^{<em>}=\theta_{w}^{</em>}$.
2. For every frequency $\zeta \in\left{1, \ldots, \frac{p-1}{2}\right}$, at least one neuron in the network uses this frequency.</p>
<p>Proof outline. Following the blueprint described in the previous section, we first prove that neurons of the form above (and only these neurons) maximize the expected class-weighted margin $\mathbb{E}_{a, b}\left[\psi^{\prime}(u, v, w)\right]$ with respect to the uniform distribution $q^{*}=\operatorname{unif}(\mathcal{X})$. We will use the uniform class weighting: $\tau(a, b)\left[c^{\prime}\right]:=1 /(p-1)$ for all $c^{\prime} \neq a+b$. As a crucial intermediate step, we prove that</p>
<p>$$
\mathbb{E}<em 0="0" _neq="\neq" j="j">{a, b}\left[\psi^{\prime}({u, v, w}, a, b)\right]=\frac{2}{(p-1) p^{2}} \sum</em>(-j)
$$} \hat{u}(j) \hat{v}(j) \hat{w</p>
<p>Maximizing the above expression subject to the max-margin norm constraint $\sum_{j \neq 0}\left(|\hat{u}(j)|^{2}+|\hat{v}(j)|^{2}+|\hat{w}(j)|^{2}\right) \leq 1$ leads to sparsity in Fourier space.</p>
<p>Then, we describe a network $\theta^{<em>}$ (of width $4(p-1)$ ) composed of such neurons, and that satisfies Equation 1 and condition C.1. By Lemma 6, part (1) of Theorem 7 will follow, and $\theta^{</em>}$ will be an example of a max-margin network. Finally, in order to show that all frequencies are used, we introduce the multidimensional discrete Fourier transform. We prove that each neuron only contributes a single frequency to the multi-dimensional DFT of the network; but that second part of Lemma 6 implies that all frequencies are present in the full network's multidimensional DFT. The full proof can be found in Appendix F.</p>
<p>As demonstrated in Figure 1 and 4, empirical networks trained with gradient descent with $L_{2,3}$ regularization approach the theoretical maximum margin, and have single frequency neurons. Figure 7 in the Appendix verifies that all frequencies are present in the network.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Final neurons with highest norm and the evolution of normalized $L_{2,5}$ margin over training of a 1 -hidden layer quartic network (activation $x^{4}$ ) on $(10,4)$ sparse parity dataset with $L_{2,5}$ regularization. The network approaches the theoretical maximum margin that we predict.</p>
<h1>5 Sparse parity</h1>
<p>In this section, we will establish the max margin features that emerge when training a neural network on the sparse parity task. Consider the $(n, k)$-sparse parity problem, where the parity is computed over $k$ bits out of $n$. To be precise, consider inputs $x_{1}, \ldots, x_{n} \in{ \pm 1}$. For a given subset $S \subseteq[n]$ such that $|S|=k$, the parity function is given by $\Pi_{j \in S} x_{j}$.</p>
<p>Theorem 8. Consider a single hidden layer neural network of width $m$ with the activation function given by $x^{k}$, i.e, $f(x)=\sum_{i=1}^{m}\left(u_{i}^{\top} x\right)^{k} w_{i}$, where $u_{i} \in \mathbb{R}^{n}$ and $w_{i} \in \mathbb{R}^{2}$, trained on the $(n, k)$-sparse parity task. Without loss of generality, assume that the first coordinate of $w_{i}$ corresponds to the output for class $y=+1$. Denote the vector $[1,-1]$ by $\boldsymbol{b}$. Provided $m \geq 2^{k-1}$, the $L_{2, k+1}$ maximum margin is:</p>
<p>$$
\gamma^{*}=k!\sqrt{2(k+1)^{-(k+1)}}
$$</p>
<p>Any network achieving this margin satisfies the following conditions:</p>
<ol>
<li>For every $i$ having $\left|u_{i}\right|&gt;0, \operatorname{spt}\left(u_{i}\right)=S$, $w_{i}$ lies in the span of $\boldsymbol{b}$ and $\forall j \in S$, $\left|u_{i}[j]\right|=\left|w_{i}\right|$</li>
<li>For every $i,\left(\Pi_{j \in S} u_{i}[j]\right)\left(w_{i}^{\top} \boldsymbol{b}\right) \geq 0$.</li>
</ol>
<p>As shown in Figure 5, a network trained with gradient descent and $L_{2, k+1}$ regularization exhibits these properties, and approaches the theoretically-predicted maximum margin. The proof for Theorem 8 can be found in Appendix G.</p>
<h2>6 Finite Groups with Real Representations</h2>
<p>We conclude our case study on algebraic tasks by studying group composition on finite groups $G$. Namely, here we set $\mathcal{X}:=G \times G$ and output space $\mathcal{Y}:=G$. Given inputs $a, b \in G$ we train the network to predict $c=a b$. We wish to characterize the maximum margin features similarly to the case of modular addition; here, our analysis relies on principles from group representation theory.</p>
<h3>6.1 Brief Background and Notation</h3>
<p>The following definitions and notation are essential for stating our main result, and further results are presented with more rigor in Appendix H.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(c) Initial Maximum Power Distribution (d) Final Maximum Power Distribution</p>
<p>Figure 6: This figure demonstrates the training of a 1-hidden layer quadratic network on the symmetric group $S 5$ with $L_{2,3}$ regularization. (a) Evolution of the normalized $L_{2,3}$ margin of the network with training. It approaches the theoretical maximum margin that we predict. (b) Distribution of neurons spanned by a given representation. Higher dimensional representations have more neurons as given by our construction. (c) and (d) Maximum normalized power is given by $\max <em j="j">{i} \hat{u}[i]^{2} /\left(\sum</em>$ representation. This is random at initialization, but towards the end of training, all neurons are concentrated in a single representation, as predicted by maximum margin.} \hat{u}[j]^{2}\right)$ where $\hat{u}[i]$ refers to the component of weight vector $u$ spanned by the basis vectors corresponding to $i^{t h</p>
<p>A real representation of a group $G$ is a finite dimensional real vector space $V=\mathbb{R}^{d}$ and a group homomorphism (i.e. a map preserving the group structure) $R: G \rightarrow G L(V)$. We denote such a representation by $(R, V)$ or just by $R$. The dimension of a representation $R$, denoted $d_{R}$, is the dimension of $V$. Our analysis focuses on unitary, irreducible, real representations of $G$. The number of such representations is precisely equal to the number of conjugacy classes of $G$ where the conjugacy class of $a \in G$ is defined as $C(a)=\left{g a g^{-1}\right.$ : $g \in G}$.</p>
<p>A quantity important to our analysis is the character of a representation $R$, denoted $\chi_{R}: G \rightarrow \mathbb{R}$ given by $\chi_{R}(g)=\operatorname{tr}(R(g))$. It was previously observed by Chughtai et al. (2023) that one-layer ReLU MLPs and transformers learn the task by mapping inputs $a, b$ to their respective matrices $R(a), R(b)$ for some irreducible representation $R$ and performing matrix multiplication with $R\left(c^{-1}\right)$ to output logits proportional to the character $\chi_{R}\left(a b c^{-1}\right)=$ $\operatorname{tr}\left(R(a) R(b) R\left(c^{-1}\right)\right)$, which is in particular maximized when $c=a b$. They also find evidence of network weights being spanned by representations, which we establish rigorously here.</p>
<p>For each representation $R$ we will consider the $|G|$-dimensional vectors by fixing one index in the matrices outputted by $R$, i.e. vectors $\left(R(g)<em G="G" _in="\in" g="g">{(i, j)}\right)</em>\right.$}$ for some $i, j \in\left[d_{R}\right]$. For each $R$, this gives $d_{R}{ }^{2}$ vectors. Letting $K$ be the number of conjugacy classes and $R_{1}, \ldots, R_{K}$ be the corresponding irreducible representations, since $|G|=\sum_{n=1}^{K} d_{R_{n}}^{2}$, taking all such vectors for each representation will form a set of $|G|$ vectors which we will denote $\rho_{1}, \ldots, \rho_{|G|}\left(\rho_{1</p>
<p>is always the vector corresponding to the trivial representation). These vectors are in fact orthogonal, which follows from orthogonality relations of the representation matrix elements $R(g)_{(i, j)}$ (see Appendix H for details). Thus, we refer to this set of vectors as basis vectors for $\mathbb{R}^{|G|}$. One can ask whether the maximum margin solution in this case has neurons which are spanned only by basis vectors corresponding to a single representation $R$, and if all representations are present in the network - the analogous result we obtained for modular addition in Theorem 7. We show that this is indeed the case.</p>
<h1>6.2 The Main Result</h1>
<p>Our main result characterizing the max margin features for group composition is as follows.
Theorem 9. Consider a single hidden layer neural network of width $m$ with quadratic activation trained on learning group composition for $G$ with real irreducible representations. Provided $m \geq 2 \sum_{n=2}^{K} d_{R_{n}}{ }^{3}$ and $\sum_{n=2}^{K} d_{R_{n}}{ }^{1.5} \chi_{R_{n}}(C)&lt;0$ for every non-trivial conjugacy class $C$, the $L_{2,3}$ maximum margin is:</p>
<p>$$
\gamma^{*}=\frac{2}{3 \sqrt{3|G|}} \frac{1}{\left(\sum_{n=2}^{K} d_{R_{n}}^{2.5}\right)}
$$</p>
<p>Any network achieving this margin satisfies the following conditions:</p>
<ol>
<li>For every neuron, there exists a non-trivial representation such that the input and output weight vectors are spanned only by that representation.</li>
<li>There exists at least one neuron spanned by each representation (except for the trivial representation) in the network.</li>
</ol>
<p>The complete proof for Theorem 9 can be found in Appendix I.
The condition that $\sum_{n=2}^{K} d_{R_{n}}{ }^{1.5} \chi_{R_{n}}(C)&lt;0$ for every non-trivial conjugacy class $C$ holds for the symmetric group $S_{k}$ up until $k=5$. In this case, as shown in Figure 6, network weights trained with gradient descent and $L_{2,3}$ regularization exhibit similar properties. The maximum margin of the network approaches what we have predicted in theory. Analogous results for training on $S_{3}$ and $S_{4}$ in Figures 8 and 9 are in the Appendix.</p>
<p>Although Theorem 9 does not apply to all finite groups with real representations, it can be extended to apply more generally. The theorem posits that every representation is present in the network, and every conjugacy class is present on the margin. Instead, for general finite groups, each neuron still satisfies the characteristics of max margin solutions in that it is only spanned by one non-trivial representation, but only a subset of representations are present in the network; moreover, only a subset of conjugacy classes are present on the margin. More details are given in Appendix I.2.</p>
<h2>7 Discussion</h2>
<p>We have shown that the simple condition of margin maximization can, in certain algebraic learning settings, imply very strong conditions on the representations learned by neural networks. The mathematical techniques we introduce are general, and may be able to be adapted to other settings than the ones we consider. Our proof holds for the case of $x^{2}$ activations ( $x^{k}$ activations, in the $k$-sparse parity case) and $L_{2, \nu}$ norm, where $\nu$ is the homogeneity constant of the network. Empirical findings suggest that the results may be transferable to other architectures and norms. In general, we think explaining how neural networks adapt their representations to symmetries and other structure in data is an important subject for future theoretical and experimental inquiry.</p>
<h1>8 Acknowledgments</h1>
<p>We thank Boaz Barak for helpful discussions. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. Sham Kakade acknowledges funding from the Office of Naval Research under award N00014-22-1-2377. Ben Edelman acknowledges funding from the National Science Foundation Graduate Research Fellowship Program under award DGE-214074. Depen Morwani, Costin-Andrei Oncescu and Rosie Zhao acknowledge support from Simons Investigator Fellowship, NSF grant DMS-2134157, DARPA grant W911NF2010021, and DOE grant DE-SC0022199.</p>
<h2>References</h2>
<p>Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances in Neural Information Processing Systems, 35:21750-21764, 2022.</p>
<p>Peter Bartlett. For valid generalization the size of the weights is more important than the size of the network. Advances in neural information processing systems, 9, 1996.</p>
<p>Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004 .</p>
<p>Ido Bronstein, Alon Brutzkus, and Amir Globerson. On the inductive bias of neural networks for learning read-once dnfs. In Uncertainty in Artificial Intelligence, pp. 255-265. PMLR, 2022 .</p>
<p>Nick Cammarata, Gabriel Goh, Shan Carter, Ludwig Schubert, Michael Petrov, and Chris Olah. Curve detectors. Distill, 5(6):e00024-003, 2020.</p>
<p>Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss, 2020.</p>
<p>Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. arXiv preprint arXiv:2302.03025, 2023.</p>
<p>Amit Daniely and Eran Malach. Learning parities with neural networks. Advances in Neural Information Processing Systems, 33:20356-20365, 2020.</p>
<p>Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Pareto frontiers in neural feature learning: Data, compute, width, and luck. arXiv preprint arXiv:2309.03800, 2023.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021.</p>
<p>Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022.</p>
<p>Spencer Frei, Niladri S Chatterji, and Peter L Bartlett. Random feature amplification: Feature learning and generalization in neural networks. arXiv preprint arXiv:2202.07626, 2022a.</p>
<p>Spencer Frei, Gal Vardi, Peter Bartlett, Nathan Srebro, and Wei Hu. Implicit bias in leaky relu networks trained on high-dimensional data. In The Eleventh International Conference on Learning Representations, 2022b.</p>
<p>Spencer Frei, Gal Vardi, Peter Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers and leaky relu networks from kkt conditions for margin maximization. In The Thirty Sixth Annual Conference on Learning Theory, pp. 3173-3228. PMLR, 2023.</p>
<p>Andrey Gromov. Grokking modular arithmetic. arXiv preprint arXiv:2301.02679, 2023.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. Advances in neural information processing systems, 31, 2018.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.</p>
<p>Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. Advances in Neural Information Processing Systems, 33:17176-17186, 2020.</p>
<p>Yvette Kosmann-Schwarzbach et al. Groups and symmetries. Springer, 2010.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and Mike Williams. Towards understanding grokking: An effective theory of representation learning. Advances in Neural Information Processing Systems, 35:34651-34663, 2022.</p>
<p>Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=zDiHoIWa0q1.</p>
<p>Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2019.</p>
<p>Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on twolayer nets: Margin maximization and simplicity bias. Advances in Neural Information Processing Systems, 34:12978-12991, 2021.</p>
<p>Depen Morwani, Jatin Batra, Prateek Jain, and Praneeth Netrapalli. Simplicity bias in 1-hidden layer neural networks, 2023.</p>
<p>Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.</p>
<p>Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024. 001. https://distill.pub/2020/circuits/zoom-in.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2018.</p>
<p>Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.</p>
<p>Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822-2878, 2018.</p>
<p>Gal Vardi. On the implicit bias in deep-learning algorithms. Communications of the ACM, 66(6):86-93, 2023.</p>
<p>Gal Vardi, Ohad Shamir, and Nati Srebro. On margin maximization in linear and relu networks. Advances in Neural Information Processing Systems, 35:37024-37036, 2022.</p>
<p>Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural nets v.s. their induced kernel. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019a. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ 8744cf92c88433f8cb04a02e6db69a0d-Paper.pdf.</p>
<p>Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural nets vs their induced kernel. Advances in Neural Information Processing Systems, 32, 2019b.</p>
<p>Shi Zhenmei, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. In International Conference on Learning Representations, 2022.</p>
<p>Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza: Two stories in mechanistic explanation of neural networks. arXiv preprint arXiv:2306.17844, 2023 .</p>
<h1>Part I</h1>
<h2>Appendix</h2>
<p>Appendix Organization: In Appendix A, we discuss further related work. Appendix B provides the hyperparameter details for various experiments. Appendix C provides additional experimental results. In Appendix D, we describe an alternative network construction for the modular addition task, which does not exhibit Fourier sparsity. The proofs for Section 3 are provided in Appendix E. Proofs for the three case studies have been provided in Appendix F, G and I. Additional group representation theory preliminaries can be found in Appendix H.</p>
<h1>A Further Related Work</h1>
<p>Two closely related works to ours are Gromov (2023) and Bronstein et al. (2022). Gromov (2023) provides an analytic construction of various two-layer quadratic networks that can solve the modular addition task. The construction used in the proof of Theorem 7 is a special case of the given scheme. Bronstein et al. (2022)shows that all max margin solutions of a one-hidden-layer ReLU network (with fixed top weights) trained on read-once DNFs have neurons which align with clauses. However, the proof techniques are significantly different. For any given neural network not satisfying the desired conditions ((neurons aligning with the clauses), Bronstein et al. (2022) construct a perturbed network satisfying the conditions which exhibits a better margin. We rely on the max-min duality for certifying a maximum margin solution, as shown in Section 3.3.</p>
<p>Margin maximization. One branch of results on margin maximization in neural networks involve proving that the optimization of neural networks leads to an implicit bias towards margin maximization. Soudry et al. (2018) show that logistic regression converges in direction to the max margin classifier. Wei et al. (2019b) prove that the global optimum of weakly-regularized cross-entropy loss on homogeneous networks reaches the max margin. Similarly, Lyu \&amp; Li (2019) and Ji \&amp; Telgarsky (2020) show that in homogeneous networks, even in the absence of explicit regularization, if loss becomes low enough then the weights will tend in direction to a KKT point of the max margin optimization objective. This implies margin maximization in deep linear networks, although it is not necessarily the global max margin (Vardi et al., 2022). Chizat \&amp; Bach (2020) prove that infinite-width 2homogeneous networks with mean field initialization will converge to the global max margin solution. In a different setting, Lyu et al. (2021) and Frei et al. (2022b) show that the margin is maximized when training leaky-ReLU one hidden layer networks with gradient flow on linearly separable data, given certain assumptions on the input (eg. presence of symmetries, near-orthogonality). For more on studying inductive biases in neural networks, refer to Vardi (2023).</p>
<p>Numerous other works do not focus on neural network dynamics and instead analyze properties of solutions with good margins (Bartlett, 1996). For instance, Frei et al. (2023) show that the maximum margin KKT points have "benign overfitting" properties. The works by Lyu et al. (2021), Morwani et al. (2023) and Frei et al. (2023) show that max margin implies linear decision boundary for solutions. Gunasekar et al. (2018) show that under certain assumptions, gradient descent on depth-two linear convolutional networks (with weight-sharing in first layer) converges not to the standard $L_{2}$ max margin, but to the global max margin with respect to the $L_{1}$ norm of the Fourier transform of the predictor. Our work follows a similar vein, in which we characterize max margin features in our setting and relate this to trained networks via results from Wei et al. (2019b).</p>
<p>Training on algebraic tasks and mechanistic interpretability. Studying neural networks trained on algebraic tasks has offered insights into their training dynamics and inductive biases, with the simpler setting lending a greater ease of understanding. One such example is the task of modular addition, which was studied in Power et al. (2022) in their study of grokking, leading to multiple follow-up works (Liu et al., 2022, 2023). Another example is the problem of learning parities for neural networks, which has been investigated in numerous works (Daniely \&amp; Malach, 2020; Zhenmei et al., 2022; Frei et al., 2022a; Barak et al., 2022; Edelman et al., 2023). Other mathematical tasks like learning addition have been used to investigate whether models possess algorithmic reasoning capabilities (Saxton et al., 2018; Hendrycks et al., 2021; Lewkowycz et al., 2022).</p>
<p>The area of mechanistic interpretability aims to understand the internal representations of individual neural networks by analyzing its weights. This form of analysis has been applied to understand the motifs and features of neurons in circuits - particular subsets of a neural network - in computer vision models (Olah et al., 2020; Cammarata et al., 2020) and more recently in language models (Elhage et al., 2021; Olsson et al., 2022). However, the ability to fully reverse engineer a neural network is extremely difficult for most tasks and architectures.</p>
<p>Some work in this area has shifted towards finding small, toy models that are easier to interpret, and employing labor intensive approaches to reverse-engineering specific features and circuits in detail(Elhage et al., 2022). In Nanda et al. (2023), the authors manage to fully interpret how one-layer transformers implement modular addition and use this knowledge to define progress measures that precede the grokking phase transition which was previously observed to occur for this task (Power et al., 2022). Chughtai et al. (2023) extends this analysis to learning composition for various finite groups, and identifies analogous results and progress measures. In this work, we show that these empirical findings can be analytically explained via max margin analysis, due to the implicit bias of gradient descent towards margin maximization.</p>
<h1>B Experimental details</h1>
<p>In this section, we will provide the hyperparameter settings for various experiments in the paper.</p>
<h2>B. 1 Cyclic Group</h2>
<p>We train a 1-hidden layer network with $m=500$, using gradient descent on the task of learning modular addition for $p=71$ for 40000 steps. The initial learning rate of the network is 0.05 , which is doubled on the steps - $[1 e 3,2 e 3,3 e 3,4 e 3,5 e 3,6 e 3,7 e 3,8 e 3,9 e 3,10 e 3]$. Thus, the final learning rate of the network is 51.2 . This is done to speed up the training of the network towards the end, as the gradient of the loss goes down exponentially. For quadratic network, we use a $L_{2,3}$ regularization of $1 e-4$. For ReLU network, we use a $L_{2}$ regularization of $1 e-4$.</p>
<h2>B. 2 Sparse parity</h2>
<p>We train a 1-hidden layer quadratic network with $m=40$ on $(10,4)$-sparse parity task. It is trained by gradient descent for 30000 steps with a learning rate of 0.1 and $L_{2,5}$ regularization of $1 e-3$.</p>
<h2>B. 3 General Groups</h2>
<p>The hyperparameters for various groups $S_{3}, S_{4}$, and $S_{5}$ are provided in subsections below.</p>
<h2>B.3.1 S3</h2>
<p>We train a 1-hidden layer quadratic network with $m=30$, using gradient descent for 50000 steps, with a $L_{2,3}$ regularization of $1 e-7$. The initial learning rate is 0.05 , which is doubled on the steps - $[200,400,600,800,1000,1200,1400,1600,1800,2000,2200,2400,2600,5000,10000]$. Thus, the final learning rate is 1638.4 . This is done to speed up the training of the network towards the end, as the gradient of the loss goes down exponentially.</p>
<h2>B.3.2 S4</h2>
<p>We train a 1-hidden layer quadratic network with $m=200$, using gradient descent for 50000 steps, with a $L_{2,3}$ regularization of $1 e-7$. The initial learning rate is 0.05 , which is doubled on the steps - $[200,400,600,800,1000,1200,1400,1600,1800,2000,2200,2400,2600,5000,10000]$. Thus, the final learning rate is 1638.4 . This is done to speed up the training of the network towards the end, as the gradient of the loss goes down exponentially.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) ReLU activation
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) Quadratic activation</p>
<p>Figure 7: Final distribution of the neurons corresponding to a particular frequency in (a) ReLU network trained with $L_{2}$ regularization and (b) Quadratic network trained with $L_{2,3}$ regularization. Similar to our construction, the final distribution across frequencies is close to uniform.</p>
<h1>B.3.3 S5</h1>
<p>We train a 1-hidden layer quadratic network with $m=2000$, using stochastic gradient descent for 75000 steps, with a batch size of 1000 and $L_{2,3}$ regularization of $1 e-5$. The initial learning rate is 0.05 , which is doubled on the steps - $[3000,6000,9000,12000,15000,18000,21000,24000]$. Thus, the final learning rate is 12.8 . This is done to speed up the training of the network towards the end, as the gradient of the loss goes down exponentially.</p>
<h2>C Additional Experiments</h2>
<p>The distribution of neurons of a particular frequency for the modular addition case is shown in Figure 7. As can be seen, for both ReLU and quadratic activation, the distribution is close to uniform.</p>
<p>Experimental results for other symmetric groups $S_{3}$ and $S_{4}$ in Figures 8 and 9 respectively. We observe the same max margin features as stated in Theorem 9 and the $L_{2,3}$ margin approaches the theoretical max margin that we have predicted.</p>
<h2>D Alternative construction</h2>
<p>To argue why the problem of finding correctly classifying networks is overdetermined, we present an alternative construction (which applies to general groups) that does not have an "interesting" Fourier spectrum or any behavioral similarity to the solutions reached by standard training.</p>
<p>For any function $r:[n]^{2} \rightarrow[n]$, there exists a neural network parameterized by $\theta$ of the form considered in Sections 4 and 6 with $2 p^{2}$ neurons such that $f(\theta,(a, b))[c]=\mathbf{1}<em i="i">{c=r(a, b)}$ and that is "dense" in the Fourier spectrum. For each pair $(a, b)$ we use two neurons given by ${u, v, w}$ and $\left{u^{\prime}, v^{\prime}, w^{\prime}\right}$, where $u</em>}=u_{i}^{\prime}=\mathbf{1<em i="i">{i=a}, v</em>}=\mathbf{1<em i="i">{i=b}, v</em>}^{\prime}=-1_{i=b}, w_{i}=\mathbf{1<em i="i">{i=r(a, b)} / 4$ and $w</em>$ logit equal to:}^{\prime}=-\mathbf{1}_{i=r(a, b)} / 4$. When adding together the outputs for these two neurons, for an input of $(i, j)$ we get $k^{\text {th }</p>
<p>$$
\frac{1}{4}\left(\left(\mathbf{1}<em j="b">{i=a}+\mathbf{1}</em>}\right)^{2} \mathbf{1<em i="a">{k=r(i, j)}-\left(\mathbf{1}</em>}-\mathbf{1<em j_="j)" k="r(i,">{j=b}\right)^{2} \mathbf{1}</em>}\right)=\mathbf{1<em j="b">{i=a} \mathbf{1}</em>
$$} \mathbf{1}_{k=r(a, b)</p>
<p>Hence, these two norms help "memorize" the output for $(a, b)$ while not influencing the output for any other input, so when summing together all these neurons we get an $f$ with the aforementioned property. Note that all the vectors used are (up to sign) one-hot encodings and thus have an uniform norm in the Fourier spectrum. This is to show that Fourier sparsity is not present in any correct classifier.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" />
(c) Initial Maximum Power Distribution (d) Final Maximum Power Distribution</p>
<p>Figure 8: This figure demonstrates the training of a 1-hidden layer quadratic network on the symmetric group $S 3$ with $L_{2,3}$ regularization. (a) Evolution of the normalized $L_{2,3}$ margin of the network with training. It approaches the theoretical maximum margin that we predict. (b) Distribution of neurons spanned by a given representation. Higher dimensional representations have more neurons as given by our construction. (c) and (d) Maximum normalized power is given by $\frac{\max \hat{u}[i]^{2}}{L_{i} \cdot \hat{u}[i]^{2}}$ where $\hat{u}[i]$ refers to the component of weight $u$ along $i^{t h}$ representation. Initially, it's random, but towards the end of training, all neurons are concentrated in a single representation, as predicted by maximum margin.</p>
<h1>E Proofs for the Theoretical Approach</h1>
<p>For ease of the reader, we will first restate Equations 1 and 2.</p>
<p>$$
\begin{aligned}
&amp; q^{<em>} \in \underset{q \in \mathcal{P}(D)}{\arg \min } \underset{(x, y) \sim q}{\mathbb{E}}\left[g\left(\theta^{</em>}, x, y\right)\right] \
&amp; \theta^{<em>} \in \underset{\theta \in \Theta}{\arg \max } \underset{(x, y) \sim q^{</em>}}{\mathbb{E}}[g(\theta, x, y)]
\end{aligned}
$$</p>
<p>We will first provide the proof of Lemma 2.
Lemma. If a pair $\left(\theta^{<em>}, q^{</em>}\right)$ satisfies Equations 1 and 2, then</p>
<p>$$
\theta^{*} \in \underset{\theta \in \Theta}{\arg \max } \min _{(x, y) \in D} g(\theta, x, y)
$$</p>
<p>Proof. First, using max-min inequality, we have:</p>
<p>$$
\begin{aligned}
&amp; \max <em D="D" _in="\in" _x_="(x," y_="y)">{\theta \in \Theta} \min </em> g(\theta, x, y)=\max <em _in="\in" _mathcal_P="\mathcal{P" q="q">{\theta \in \Theta} \min </em>[g(\theta, x, y)] \leq \
&amp; \min }(D)} \underset{(x, y) \sim q}{\mathbb{E}<em _Theta="\Theta" _in="\in" _theta="\theta">{q \in \mathcal{P}(D)} \max </em>[g(\theta, x, y)]
\end{aligned}
$$} \underset{(x, y) \sim q}{\mathbb{E}</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The algorithm identified by Nanda et al. (2023) can be seen as a real-valued implementation of the following procedure: Choose a fixed $k$. Embed $a \mapsto e^{2 \pi i k a}, b \mapsto e^{2 \pi i k b}$, representing rotations by $k a$ and $k b$. Multiply these (i.e. compose the rotations) to obtain $e^{2 \pi i k(a+b)}$. Then, for each $c \in \mathbb{Z}_{p}$, multiply by $e^{-2 \pi i k c}$ and take the real part to obtain the $\operatorname{logit}$ for $c$. Moreover, averaging the result over neurons with different frequencies $k$ results in destructive interference when $c \neq a+b$, accentuating the correct answer.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
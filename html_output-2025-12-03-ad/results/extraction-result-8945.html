<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8945 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8945</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8945</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-ebf35cef5c249d90b40043fffa41f8802c27f132</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ebf35cef5c249d90b40043fffa41f8802c27f132" target="_blank">RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size, is introduced.</p>
                <p><strong>Paper Abstract:</strong> Despite their unprecedented success, even the largest language models make mistakes.Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show relative improvements up to 10% in multiple text similarity metrics over other learned, retrieval-augmented or prompting-based critique generators.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8945.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8945.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL4F</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning for Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework that trains an external critique generator (LM_critique) with policy gradient (PPO) to produce natural-language critiques that steer a fixed, black-box task model (GPT-3) to revise and improve its outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (code-davinci-002) as LM_task; T5-large (finetuned + PPO) as LM_critique</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM_task: OpenAI GPT-3 (code-davinci-002, 175B) used frozen via few-shot prompting for PREDICT and REFINE. LM_critique: encoder-decoder T5-large (≈770M parameters for main experiments; also ablations with smaller sizes) initialized from pretrained checkpoints then fine-tuned supervised and with PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RL4F (trained external critique generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train an external critique generator p_theta(c | x, ŷ) (initialized by supervised finetuning on human/synthetic critiques) using Proximal Policy Optimization to maximize an end-task reward (ROUGE or inverse-Levenstein) measured on LM_task's revised outputs; sampled critiques are fed to the frozen LM_task (GPT-3) which produces a refined ŷ_new conditioned on (x, ŷ, ĉ).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Topic-based summarization; Action planning (Interscript); Synthetic Alphabetization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Three tasks where initial model outputs ŷ are potentially incorrect and a critique c plus refinement should produce a corrected y: (1) topic-based summarization (passage + question → summary), (2) action planning (goal → sequence of steps), (3) alphabetization (unsorted list → alphabetically sorted list).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Interscript (using RL4F critiques): BLEURT = -0.92, BERTScore = 87.23, ROUGE-1/2/L = 22.1 / 0.9 / 21.3. Topic-based summarization (RL4F): BLEURT = 0.10, BERTScore = 93.6, ROUGE-1/2/L = 55.1 / 48.2 / 52.6. Alphabetization (RL4F): Exact Match = 66.1%, Inverse-Levenshtein = 0.92.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines: Direct-Refinement (GPT-3 asked to 'Improve the answer'): Interscript BLEURT = -1.07, BERTScore = 86.97, ROUGE-1/2/L = 15.8 / 0.9 / 15.5; Topic summarization BLEURT = 0.09, BERTScore = 93.1, ROUGE-1/2/L = 54.3 / 46.0 / 50.9. Alphabetization initial GPT-3 outputs (no refinement): Exact Match = 63.7%, Inverse-Levenshtein = 0.91.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External critique generator trained with supervised warm-start then PPO (KL-regularized PPO) using end-task lexical/learned rewards; generated natural-language critiques are appended in prompts to the frozen GPT-3 REFINE prompt (few-shot exemplars).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: RL4F improves final task metrics over SUPERVISED and MemPrompt baselines and is often competitive with or closer to gold human critiques: e.g., Interscript ROUGE-1 increases from 19.4 (SUPERVISED) to 22.1 (RL4F); alphabetization exact match improves from 38.9 (SUPERVISED) to 66.1 (RL4F). Authors report relative improvements up to ~10% on text-similarity metrics and show RL4F remains useful when applied iteratively (Fig.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit semantic-drift prevention; semantic drift could occur though authors observed minimal fluency issues in practice. RL4F can still declare already-solved examples as correct (leading iterative application to sometimes not gain or even lose solved examples). Warm-start supervised critiques that do not match LM_task error distribution can misguide (observed in synthetic alphabetization SUPERVISED baseline). RL4F experiments limited to GPT-3 as LM_task and T5-large variants for LM_critique; generality beyond this setup not proven.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against SUPERVISED (supervised critique generation), MemPrompt (retrieval of past critiques), Direct-Refinement (self-repair prompt) and gold human feedback. RL4F outperforms SUPERVISED and MemPrompt across reported metrics and is closer to gold feedback. Direct-Refinement is competitive (particularly on alphabetization) but RL4F often yields better or comparable results overall.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Scaling ablation: RL4F benefits strongly from larger LM_critique sizes (experiments across ~60M to 770M parameters show RL4F performance increases with model size; SUPERVISED shows no similar trend). Other ablations (e.g., PPO hyperparameters) not isolated beyond reporting training settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8945.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8945.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Refinement (self-repair prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple self-refinement baseline where the task model is prompted (few-shot) to revise its own initial answer with an instruction like 'Improve the answer.' without a separately generated natural-language critique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (code-davinci-002, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large frozen GPT-3 model used via few-shot prompting to perform both initial prediction and conditional refinement when asked to 'Improve the answer.'</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Direct-Refinement (self-repair prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the same LM_task with the initial answer and a generic instruction (e.g., 'Improve the answer.') to produce a revised output; can be applied repeatedly (iteratively) but in experiments typically a single refinement is used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Topic-based summarization; Action planning (Interscript); Synthetic Alphabetization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same downstream tasks as RL4F; refinement is performed by the same model without external critique text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Interscript: BLEURT = -1.07, BERTScore = 86.97, ROUGE-1/2/L = 15.8 / 0.9 / 15.5. Topic summarization: BLEURT = 0.09, BERTScore = 93.1, ROUGE-1/2/L = 54.3 / 46.0 / 50.9. Alphabetization: Exact Match = 65.9%, Inverse-Levenshtein = 0.92.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial GPT-3 outputs (no refinement) for alphabetization: Exact Match = 63.7%, Inverse-Levenshtein = 0.91; for other tasks initial performance not precisely enumerated in same table but included in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: few-shot prompts showing examples of initial answer and improved answer; generic improvement instruction substituted for explicit critique text.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Direct-Refinement often matches or exceeds other critique-based methods on some tasks for GPT-3 (e.g., alphabetization direct-refinement achieves 65.9% vs. initial 63.7%); authors cite prior work (Saunders et al., 2022; Bai et al., 2022) arguing direct refinement is a strong baseline especially for very large models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Iterative direct-refinement can degrade already-correct outputs (authors observe the model occasionally scrambles correct orderings when asked to iteratively improve). Prior work suggests that direct self-critique effectiveness depends on model capacity (>50B tends to do better).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct-Refinement is compared as a baseline to RL4F, SUPERVISED critique generation, and MemPrompt; it is competitive on alphabetization but RL4F often outperforms it on planning and summarization metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8945.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8945.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (self-generated critiques)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine / self-generated critiques (Madaan et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where a language model generates its own critique via few-shot prompting and then uses that self-produced critique to condition a refinement of its original answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated with GPT-3 variants (code-davinci-002, text-davinci-002) in this paper's experiments of the approach</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-critique is sampled by prompting the same LM (few-shot); in the paper's diagnostic runs they used code-davinci-002 and text-davinci-002 for sampling self-critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine / self-generated critiques</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the model in few-shot fashion to produce a critique for its own output; then feed that critique back into the model as conditioning for a refinement step (single or iterative cycles in broader literature).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic Alphabetization (diagnostic evaluation in Appendix B.5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Alphabetization of short lists; the authors used a few-shot prompt to ask GPT-3 to describe what's wrong in a provided ordering and then used that feedback for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>When self-generated critiques sampled from code-davinci-002 were used: Exact Match = 21.6% (a large drop); when sampled from text-davinci-002: Exact Match = 58.6% (still below initial 63.6% in reported setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial GPT-3 (code-davinci-002) performance on the same alphabetization samples: Exact Match ≈ 63.6% (reported as initial accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Few-shot prompt to the same model to produce a natural-language critique, then include that critique as input to the model's REFINE prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical negative or mixed evidence in this paper: self-generated critiques sampled from the same-capability model (code-davinci-002) substantially harmed performance (21.6% exact match). Higher-capability variants improved results but still did not surpass initial model outputs in the reported runs (text-davinci-002 gave 58.6%). Authors note other literature where self-refinement can help when sampled from more capable models (e.g., text-davinci-003 in Madaan et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Highly sensitive to the capability of the model used to generate the critique — lower-capacity samplers can produce misleading or erroneous critiques that harm downstream refinement. In the paper's experiments, self-generated critiques from code-davinci-002 drastically reduced alphabetization accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to RL4F (learned external critique generator trained with RL), self-refine (self-generated critiques) performed poorly when the samplers were less capable; RL4F produced useful critiques despite being a much smaller model (T5-large) because it was trained with task reward, whereas naive self-generated critiques from an insufficiently capable model were detrimental.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8945.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8945.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative refinement (apply critiques repeatedly)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative critique-then-refine application</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying sampled critiques and subsequent refinements repeatedly (multiple generate→refine cycles) to attempt incremental improvement of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (code-davinci-002) as LM_task; RL4F's LM_critique (T5-large variants) providing critiques</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same frozen GPT-3 for REFINE; RL4F-trained critique generator samples multiple critiques across rounds and GPT-3 is prompted repeatedly to edit the current answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative critique application (multi-round REFINE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Repeatedly sample a critique from LM_critique conditioned on the current answer and re-run LM_task REFINE using that critique, producing iterative edits to approach correct output.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic Alphabetization (iterative diagnostic reported) and general discussion for other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Apply multiple rounds of critique+refine to see whether the number of correctly solved cases increases over rounds (e.g., alphabetization where discrete corrections are possible).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Iterative application of RL4F critiques produced up to 7 additional corrected alphabetization examples relative to a single pass (Figure 5); exact per-round counts not tabulated as numeric table entries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Iterative Direct-Refinement (self-repair applied repeatedly) sometimes scrambles already-correct orderings and did not yield comparable iterative gains; single-pass initial exact match was 63.7% for alphabetization.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Looped prompting: after each REFINE, the new answer becomes the input ŷ for the next round and LM_critique is sampled again; editing operations are applied via GPT-3 prompts (sometimes constrained to explicit edits for planning tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical: authors report iterative application of RL4F critiques increased the number of corrected alphabetization examples by up to 7 (Figure 5) and that RL4F produced more net corrections than iterative Direct-Refinement (which sometimes degraded correct examples).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Iterative refinement can also deteriorate performance because solved examples are not removed from the pool and LM_critique may incorrectly declare a solution correct or make incorrect edits. The exact per-iteration behavior is task- and model-dependent and not exhaustively quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared qualitatively to iterative Direct-Refinement and to single-pass refinement; RL4F's iterative application showed modest additional gains while iterative Direct-Refinement sometimes harmed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Memory-assisted prompt editing to improve gpt-3 after deployment <em>(Rating: 1)</em></li>
                <li>Reinforcement Learning from AI Feedback (RLAIF) / Constitutional AI (related mentions) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8945",
    "paper_id": "paper-ebf35cef5c249d90b40043fffa41f8802c27f132",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "RL4F",
            "name_full": "Reinforcement Learning for Feedback",
            "brief_description": "A multi-agent framework that trains an external critique generator (LM_critique) with policy gradient (PPO) to produce natural-language critiques that steer a fixed, black-box task model (GPT-3) to revise and improve its outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (code-davinci-002) as LM_task; T5-large (finetuned + PPO) as LM_critique",
            "model_description": "LM_task: OpenAI GPT-3 (code-davinci-002, 175B) used frozen via few-shot prompting for PREDICT and REFINE. LM_critique: encoder-decoder T5-large (≈770M parameters for main experiments; also ablations with smaller sizes) initialized from pretrained checkpoints then fine-tuned supervised and with PPO.",
            "reflection_method_name": "RL4F (trained external critique generation)",
            "reflection_method_description": "Train an external critique generator p_theta(c | x, ŷ) (initialized by supervised finetuning on human/synthetic critiques) using Proximal Policy Optimization to maximize an end-task reward (ROUGE or inverse-Levenstein) measured on LM_task's revised outputs; sampled critiques are fed to the frozen LM_task (GPT-3) which produces a refined ŷ_new conditioned on (x, ŷ, ĉ).",
            "task_name": "Topic-based summarization; Action planning (Interscript); Synthetic Alphabetization",
            "task_description": "Three tasks where initial model outputs ŷ are potentially incorrect and a critique c plus refinement should produce a corrected y: (1) topic-based summarization (passage + question → summary), (2) action planning (goal → sequence of steps), (3) alphabetization (unsorted list → alphabetically sorted list).",
            "performance_with_reflection": "Interscript (using RL4F critiques): BLEURT = -0.92, BERTScore = 87.23, ROUGE-1/2/L = 22.1 / 0.9 / 21.3. Topic-based summarization (RL4F): BLEURT = 0.10, BERTScore = 93.6, ROUGE-1/2/L = 55.1 / 48.2 / 52.6. Alphabetization (RL4F): Exact Match = 66.1%, Inverse-Levenshtein = 0.92.",
            "performance_without_reflection": "Baselines: Direct-Refinement (GPT-3 asked to 'Improve the answer'): Interscript BLEURT = -1.07, BERTScore = 86.97, ROUGE-1/2/L = 15.8 / 0.9 / 15.5; Topic summarization BLEURT = 0.09, BERTScore = 93.1, ROUGE-1/2/L = 54.3 / 46.0 / 50.9. Alphabetization initial GPT-3 outputs (no refinement): Exact Match = 63.7%, Inverse-Levenshtein = 0.91.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External critique generator trained with supervised warm-start then PPO (KL-regularized PPO) using end-task lexical/learned rewards; generated natural-language critiques are appended in prompts to the frozen GPT-3 REFINE prompt (few-shot exemplars).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: RL4F improves final task metrics over SUPERVISED and MemPrompt baselines and is often competitive with or closer to gold human critiques: e.g., Interscript ROUGE-1 increases from 19.4 (SUPERVISED) to 22.1 (RL4F); alphabetization exact match improves from 38.9 (SUPERVISED) to 66.1 (RL4F). Authors report relative improvements up to ~10% on text-similarity metrics and show RL4F remains useful when applied iteratively (Fig.5).",
            "limitations_or_failure_cases": "No explicit semantic-drift prevention; semantic drift could occur though authors observed minimal fluency issues in practice. RL4F can still declare already-solved examples as correct (leading iterative application to sometimes not gain or even lose solved examples). Warm-start supervised critiques that do not match LM_task error distribution can misguide (observed in synthetic alphabetization SUPERVISED baseline). RL4F experiments limited to GPT-3 as LM_task and T5-large variants for LM_critique; generality beyond this setup not proven.",
            "comparison_to_other_methods": "Compared against SUPERVISED (supervised critique generation), MemPrompt (retrieval of past critiques), Direct-Refinement (self-repair prompt) and gold human feedback. RL4F outperforms SUPERVISED and MemPrompt across reported metrics and is closer to gold feedback. Direct-Refinement is competitive (particularly on alphabetization) but RL4F often yields better or comparable results overall.",
            "ablation_study_results": "Scaling ablation: RL4F benefits strongly from larger LM_critique sizes (experiments across ~60M to 770M parameters show RL4F performance increases with model size; SUPERVISED shows no similar trend). Other ablations (e.g., PPO hyperparameters) not isolated beyond reporting training settings.",
            "uuid": "e8945.0",
            "source_info": {
                "paper_title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Direct-Refinement",
            "name_full": "Direct Refinement (self-repair prompt)",
            "brief_description": "A simple self-refinement baseline where the task model is prompted (few-shot) to revise its own initial answer with an instruction like 'Improve the answer.' without a separately generated natural-language critique.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (code-davinci-002, 175B)",
            "model_description": "Large frozen GPT-3 model used via few-shot prompting to perform both initial prediction and conditional refinement when asked to 'Improve the answer.'",
            "reflection_method_name": "Direct-Refinement (self-repair prompt)",
            "reflection_method_description": "Prompt the same LM_task with the initial answer and a generic instruction (e.g., 'Improve the answer.') to produce a revised output; can be applied repeatedly (iteratively) but in experiments typically a single refinement is used as a baseline.",
            "task_name": "Topic-based summarization; Action planning (Interscript); Synthetic Alphabetization",
            "task_description": "Same downstream tasks as RL4F; refinement is performed by the same model without external critique text.",
            "performance_with_reflection": "Interscript: BLEURT = -1.07, BERTScore = 86.97, ROUGE-1/2/L = 15.8 / 0.9 / 15.5. Topic summarization: BLEURT = 0.09, BERTScore = 93.1, ROUGE-1/2/L = 54.3 / 46.0 / 50.9. Alphabetization: Exact Match = 65.9%, Inverse-Levenshtein = 0.92.",
            "performance_without_reflection": "Initial GPT-3 outputs (no refinement) for alphabetization: Exact Match = 63.7%, Inverse-Levenshtein = 0.91; for other tasks initial performance not precisely enumerated in same table but included in comparisons.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: few-shot prompts showing examples of initial answer and improved answer; generic improvement instruction substituted for explicit critique text.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: Direct-Refinement often matches or exceeds other critique-based methods on some tasks for GPT-3 (e.g., alphabetization direct-refinement achieves 65.9% vs. initial 63.7%); authors cite prior work (Saunders et al., 2022; Bai et al., 2022) arguing direct refinement is a strong baseline especially for very large models.",
            "limitations_or_failure_cases": "Iterative direct-refinement can degrade already-correct outputs (authors observe the model occasionally scrambles correct orderings when asked to iteratively improve). Prior work suggests that direct self-critique effectiveness depends on model capacity (&gt;50B tends to do better).",
            "comparison_to_other_methods": "Direct-Refinement is compared as a baseline to RL4F, SUPERVISED critique generation, and MemPrompt; it is competitive on alphabetization but RL4F often outperforms it on planning and summarization metrics.",
            "ablation_study_results": null,
            "uuid": "e8945.1",
            "source_info": {
                "paper_title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-Refine (self-generated critiques)",
            "name_full": "Self-Refine / self-generated critiques (Madaan et al., 2023)",
            "brief_description": "A method where a language model generates its own critique via few-shot prompting and then uses that self-produced critique to condition a refinement of its original answer.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "evaluated with GPT-3 variants (code-davinci-002, text-davinci-002) in this paper's experiments of the approach",
            "model_description": "Self-critique is sampled by prompting the same LM (few-shot); in the paper's diagnostic runs they used code-davinci-002 and text-davinci-002 for sampling self-critiques.",
            "reflection_method_name": "Self-Refine / self-generated critiques",
            "reflection_method_description": "Prompt the model in few-shot fashion to produce a critique for its own output; then feed that critique back into the model as conditioning for a refinement step (single or iterative cycles in broader literature).",
            "task_name": "Synthetic Alphabetization (diagnostic evaluation in Appendix B.5)",
            "task_description": "Alphabetization of short lists; the authors used a few-shot prompt to ask GPT-3 to describe what's wrong in a provided ordering and then used that feedback for refinement.",
            "performance_with_reflection": "When self-generated critiques sampled from code-davinci-002 were used: Exact Match = 21.6% (a large drop); when sampled from text-davinci-002: Exact Match = 58.6% (still below initial 63.6% in reported setting).",
            "performance_without_reflection": "Initial GPT-3 (code-davinci-002) performance on the same alphabetization samples: Exact Match ≈ 63.6% (reported as initial accuracy).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Few-shot prompt to the same model to produce a natural-language critique, then include that critique as input to the model's REFINE prompt.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical negative or mixed evidence in this paper: self-generated critiques sampled from the same-capability model (code-davinci-002) substantially harmed performance (21.6% exact match). Higher-capability variants improved results but still did not surpass initial model outputs in the reported runs (text-davinci-002 gave 58.6%). Authors note other literature where self-refinement can help when sampled from more capable models (e.g., text-davinci-003 in Madaan et al., 2023).",
            "limitations_or_failure_cases": "Highly sensitive to the capability of the model used to generate the critique — lower-capacity samplers can produce misleading or erroneous critiques that harm downstream refinement. In the paper's experiments, self-generated critiques from code-davinci-002 drastically reduced alphabetization accuracy.",
            "comparison_to_other_methods": "Compared to RL4F (learned external critique generator trained with RL), self-refine (self-generated critiques) performed poorly when the samplers were less capable; RL4F produced useful critiques despite being a much smaller model (T5-large) because it was trained with task reward, whereas naive self-generated critiques from an insufficiently capable model were detrimental.",
            "ablation_study_results": null,
            "uuid": "e8945.2",
            "source_info": {
                "paper_title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Iterative refinement (apply critiques repeatedly)",
            "name_full": "Iterative critique-then-refine application",
            "brief_description": "Applying sampled critiques and subsequent refinements repeatedly (multiple generate→refine cycles) to attempt incremental improvement of model outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (code-davinci-002) as LM_task; RL4F's LM_critique (T5-large variants) providing critiques",
            "model_description": "Same frozen GPT-3 for REFINE; RL4F-trained critique generator samples multiple critiques across rounds and GPT-3 is prompted repeatedly to edit the current answer.",
            "reflection_method_name": "Iterative critique application (multi-round REFINE)",
            "reflection_method_description": "Repeatedly sample a critique from LM_critique conditioned on the current answer and re-run LM_task REFINE using that critique, producing iterative edits to approach correct output.",
            "task_name": "Synthetic Alphabetization (iterative diagnostic reported) and general discussion for other tasks",
            "task_description": "Apply multiple rounds of critique+refine to see whether the number of correctly solved cases increases over rounds (e.g., alphabetization where discrete corrections are possible).",
            "performance_with_reflection": "Iterative application of RL4F critiques produced up to 7 additional corrected alphabetization examples relative to a single pass (Figure 5); exact per-round counts not tabulated as numeric table entries.",
            "performance_without_reflection": "Iterative Direct-Refinement (self-repair applied repeatedly) sometimes scrambles already-correct orderings and did not yield comparable iterative gains; single-pass initial exact match was 63.7% for alphabetization.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Looped prompting: after each REFINE, the new answer becomes the input ŷ for the next round and LM_critique is sampled again; editing operations are applied via GPT-3 prompts (sometimes constrained to explicit edits for planning tasks).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical: authors report iterative application of RL4F critiques increased the number of corrected alphabetization examples by up to 7 (Figure 5) and that RL4F produced more net corrections than iterative Direct-Refinement (which sometimes degraded correct examples).",
            "limitations_or_failure_cases": "Iterative refinement can also deteriorate performance because solved examples are not removed from the pool and LM_critique may incorrectly declare a solution correct or make incorrect edits. The exact per-iteration behavior is task- and model-dependent and not exhaustively quantified in the paper.",
            "comparison_to_other_methods": "Compared qualitatively to iterative Direct-Refinement and to single-pass refinement; RL4F's iterative application showed modest additional gains while iterative Direct-Refinement sometimes harmed performance.",
            "ablation_study_results": null,
            "uuid": "e8945.3",
            "source_info": {
                "paper_title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2
        },
        {
            "paper_title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
            "rating": 1
        },
        {
            "paper_title": "Reinforcement Learning from AI Feedback (RLAIF) / Constitutional AI (related mentions)",
            "rating": 1
        }
    ],
    "cost": 0.016704999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs</h1>
<p>Afra Feyza Akyürek ${ }^{1}$ Ekin Akyürek ${ }^{2}$ Ashwin Kalyan ${ }^{4}$ Peter Clark ${ }^{4}$<br>Derry Wijaya ${ }^{1,3}$ Niket Tandon ${ }^{4}$<br>${ }^{1}$ Boston University ${ }^{2}$ MIT CSAIL ${ }^{3}$ Monash University Indonesia<br>${ }^{4}$ Allen Institute for Artificial Intelligence</p>
<h4>Abstract</h4>
<p>Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show relative improvements up to $10 \%$ in multiple text similarity metrics over other learned, retrievalaugmented or prompting-based critique generators. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Correcting model outputs is a pressing challenge in natural language generation (Ribeiro et al., 2018; Reid and Neubig, 2022), emerging across many use-cases such as style transfer (Mallinson et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Two examples for action planning (Tandon et al., 2021) and summarization (Saunders et al., 2022) tasks showcase a scenario where initial predictions by a learned model $(\hat{y})$ are incorrect. Human-written critiques $(c)$ indicate errors in model outputs. While humans can reliably critique each other, machines lack such ability. This paper studies a multiagent collaborative framework where one language model can generate critiques to improve its peer's performance.</p>
<p>2020; Malmi et al., 2022), grammatical (Lichtarge et al., 2019) or factual error correction (Mitchell et al., 2022b), debiasing and detoxification (Schick et al., 2021). Unlike humans who can understand natural language feedback and improve using the information, most of the previous work relied on sequence tagging (Reid and Neubig, 2022), retraining from scratch (Sun et al., 2019) or parameter editing (Mitchell et al., 2022a) to repair model predictions.</p>
<p>Recently, researchers show that large language models can correct their answer given more sophisticated feedback formulated in natural language (Schick et al., 2022; Saunders et al., 2022). For example, in Fig. 1, we present sample feedback for two tasks. Both of these examples exemplify the case where the initial model outputs $\hat{y}$ have flaws. In topic-based summarization, an automatically generated summary of a story involves factually incorrect statements such as "... he was betrayed by his father Bill ..." where an appropriate critique is "Bill is not Michael's father". In action planning, given a goal $x$, the objective is to generate a set of steps $y$ to achieve the goal. The initial sequence of actions in Fig. 1, denoted by $\hat{y}$, has a missing a step. The human-written natural language critiques $c$ describe the ways in which $\hat{y}$ 's are incorrect and $\hat{y}_{\text {new }}$ denotes the corrected prediction conditioned on the critique. Note that in many situations helpful critiques do not necessarily reproduce an entire answer-they may simply point out one way in which the answer could be improved.</p>
<p>Researchers use crowd-sourcing to collect critiques for model outputs (Saunders et al., 2022). However, collecting feedback from humans is infeasible in an online setting where a model is required to produce a rapid stream of outputs. The goal of this paper is to shed light on whether the task of critiquing language model predictions can be effectively passed on to an external agent while keeping the language model itself intact.</p>
<p>Our multi-agent collaborative framework involves two language models where one model's job is to criticize the other as the latter performs a task of interest, such as summarization. This setting comprises a task model, denoted by $\mathrm{LM}<em _critique="{critique" _text="\text">{\text {task }}$, which learns the mapping from an input $x$ (e.g. passage) to a ground truth output $y$ (e.g. summary); and a critiquing model $\mathrm{LM}</em>}}$ which provides natural language critiques for $\mathrm{LM<em _task="{task" _text="\text">{\text {task }}$ 's outputs $\hat{y} \sim \mathrm{LM}</em>}}(x)$. The framework can additionally involve a separate model (say $\mathrm{LM<em _task="{task" _text="\text">{\text {refine }}$ ) for repairing model outputs conditioned on critiques. We follow past work (Schick et al., 2022), and merge $\mathrm{LM}</em>}}$ and $\mathrm{LM<em _task="{task" _text="\text">{\text {refine }}$ into a single model. Hence, in addition to predicting $y$ given $x, \mathrm{LM}</em>)$.}}$ is also tasked to improve its initial output conditioned on a critique $\hat{c}$ sampled from $\mathrm{LM}_{\text {critique }}(x, \hat{y</p>
<p>We introduce RL4F (Reinforcement Learning for Feedback Generation), a cascade (Dohan et al., 2022) of two language models for automatic cri-
tique generation and refinement. RL4F is trained to maximize target task performance of $\mathrm{LM}<em _critique.="{critique." _text="\text">{\text {task }}$ by learning to provide critiques for its outputs via $\mathrm{LM}</em>}}$ RL4F advances retrieval-based methods with learned critique generation (Madaan et al., 2022). Unlike previous work which teaches $\mathrm{LM<em _task="{task" _text="\text">{\text {task }}$ to read a crowd-sourced set of critiques (Schick et al., 2022; Saunders et al., 2022), RL4F learns the particular set of critiques that will steer $\mathrm{LM}</em>}}$ into improving its predictions without requiring any updates to $\mathrm{LM<em _task="{task" _text="\text">{\text {task }}$ parameters. Treating $\mathrm{LM}</em>$ as fixed is especially important in era of limitedaccess large language models which are costly, if not impossible, to fine-tune.}</p>
<p>RL4F is illustrated in Fig. 2(a,c). Previous work demonstrate that language models smaller than roughly 50 billion parameters lack the ability to understand and act upon a natural language critique (Saunders et al., 2022; Bai et al., 2022). Therefore, we chose GPT-3 as the $\mathrm{LM}<em _critique="{critique" _text="\text">{\text {task }}$ model which is a clear example of an inaccessible LM that shows this ability. While RL4F is general enough to accommodate an ensemble of feedback generators, in this work we focus one single model as $\mathrm{LM}</em>$ for simplicity.}</p>
<p>In summary, this work ${ }^{2}$ :</p>
<ul>
<li>Presents a reinforced critique generator which advances simple supervision in improving the end-task performance without retraining the downstream model.</li>
<li>Demonstrates effectiveness of RL4F on three tasks: topic-based summarization, action planning and alphabetization (e.g. sorting a list of words alphabetically) with relative improvements up to $10 \%$.</li>
<li>Showcases that RL4F exhibits promising scaling properties and remains to be useful when applied iteratively.</li>
</ul>
<h2>2 Related Works</h2>
<p>Past works differ to a large extent with respect to what they call human feedback and how they make use of it. In this section, after elucidating the use of human feedback in previous works, we briefly describe connections of RL4F to the parameterefficient fine-tuning and discrete prompt learning literature.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.1 What kind of feedback is used and where does it originate?</h3>
<p>Human feedback on model predictions come in different flavors. The most notable ones include (1) binary feedback, e.g. thumbs up/down and pairwise comparisons (Ouyang et al., 2022; Bai et al., 2022; Gao et al., 2022), (2) natural language critiques (Tandon et al., 2022; Schick et al., 2022; Madaan et al., 2022; Saunders et al., 2022; Murty et al., 2022; Chen et al., 2023; Madaan et al., 2023) and (3) direct textual refinements to outcomes (Scheurer et al., 2022; Shi et al., 2022).</p>
<p>Bai et al. (2022) introduce what they call Reinforcement Learning from AI Feedback (RLAIF) in which they replace human preference labels with those of the model's itself; the model is prompted to evaluate its own predictions in consideration of human values and preferences. In a similar vein, Gao et al. (2022) use accuracy for extractive question answering as a reward signal when fine-tuning their policy model.</p>
<p>In another thread,Schick et al. (2022) use comments from forums and Wikipedia edit histories as natural language feedback. Scheurer et al. (2022) and Shi et al. (2022) collect human natural language critiques and associated refinements. They then fine-tune the task model on the refinements. Our work is similar to these works in that we also use human-generated critiques in the first stage of our algorithm. Aside from human-written critiques, we additionally use synthetically generated critiques in the absence of the former.</p>
<h3>2.2 How is feedback used?</h3>
<p>An overwhelming majority of past work simply fine-tunes their task model using human feedback; whether it is a general purpose language model (Ouyang et al., 2022; Bai et al., 2022) or a taskspecific model (Shi et al., 2022; Gao et al., 2022; Saunders et al., 2022; Scheurer et al., 2022; Schick et al., 2022). Tandon et al. (2022) differently finetunes a separate corrector model which takes in a retrieved critique utterance to correct initial outputs. Similarly, Madaan et al. (2022) retrieves from a memory of previous critiques to improve GPT-3 predictions via few-shot prompting.</p>
<p>Our work separates from existing work by focusing on critique generation and harnessing critiques that yield better final outcomes by $\mathrm{LM}<em _critique="{critique" _text="\text">{\text {task }}$. Similar to Schick et al. (2022), we effectively propose a multi-agent setup by disentangling critique gener-
ation and conditional refinement. Differently, we keep the latter model intact and only train the critique generator $\mathrm{LM}</em>}}$ via reinforcement learning. Moreover, we take a step forward by leveraging end task data for the first time and directly optimize the critique generation process to improve final task performance. In contrast to RLHF whose policy network $\left(\mathrm{LM<em _critique="{critique" _text="\text">{\text {task }}\right)$ is trained to maximize human alignment (Wiener, 1960), our policy network $\left(\mathrm{LM}</em>$. Our proposal RL4F is orthogonal to RLHF; in fact we use an RLHF fine-tuned checkpoint in our experiments. For further discussion, please refer to Fernandes et al. (2023) who catalogue different approaches on integrating natural language feedback to textual generations.}}\right)$ is trained to bootstrap end-task success of $\mathrm{LM}_{\text {task }</p>
<h3>2.3 Adapters \&amp; Discrete Prompt Learning</h3>
<p>A large body of existing work finds that parameterefficient fine-tuning, often referred to as adapters (Pfeiffer et al., 2020) is as effective as full finetuning while being computationally cheap. RL4F can also be interpreted as an alternative "adapter" under the strict setting where only textual access to task model is available. Furthermore, our work can also be viewed from the perspective of learning discrete prompts for language models. Past work propose to generate knowledge pieces (Liu et al., 2022) or arbitrary textual snippets (Deng et al., 2022) which they append to the input via reinforcement learning. These works are different than ours in that their policy is conditioned solely on the input $x$ whereas in our case we sample critiques of machine-generated predictions based on $x$ and $\hat{y}$.</p>
<h2>3 Background</h2>
<p>The problem of learning from critiques entails two major challenges: (1) generating critiques and (2) the task of refining initial answers based on a critique. In our experiments (Section 6), GPT-3 responds very well to ground-truth critiques. This observation suggests that given quality critique, GPT-3 is indeed able to improve a potentially erroneous prediction for the better. Hence, in this study we focus our efforts on (1). Our ultimate goal is to reach, and eventually exceed, human-level critique performance using machines.</p>
<p>Following Saunders et al. (2022), we identify four primary functions towards studying the problem of learning with natural language critiques.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: a) A downstream task model takes in an input (e.g. a passage and a question) and predicts the output (e.g. summary). b) Past work proposed using a supervised learning scheme (Saunders et al., 2022; Schick et al., 2022) or retrieval (Madaan et al., 2022) for critique generation (CRITIQUE) and refinement tasks (REFINE). In our setting, we only train $\mathrm{LM}<em _critique="{critique" _text="\text">{\text {critique }}$ and parameters of the task model are left unchanged. c) RL4F uses $\mathrm{LM}</em>}}$ that was produced as a result of the training in part b. Using task data pairs (e.g. passages and summaries) we continue fine-tuning $\mathrm{LM<em _task="{task" _text="\text">{\text {critique }}$ with policy gradient such that critiques steer $\mathrm{LM}</em>$ to produce better outputs.}</p>
<p>First is PREDICT: the base task of predicting without using critiques to model $x \rightarrow y$. As an example, if $x$ is a passage, $y$ is the summary (see Fig. 1). Moreover, we refer the task of learning to generate critiques $x, \hat{y} \rightarrow c$ where $\hat{y} \sim \mathrm{LM}_{\text {task }}(x)$ as CRITIQUE. Lastly, we call the conditional refinement objective $x, \hat{y}, c \rightarrow y$ as REFINE and repairing an answer without a critique $x, \hat{y} \rightarrow y$ as DIRECTREFINE ${ }^{3}$. We use $\hat{y}$ and $\hat{c}$ notation to indicate an estimate of ground truth $y$, and similarly for $c$, from a respective model.</p>
<h3>3.1 SUPERVISED: Supervised Learning for Critique Generation</h3>
<p>We initialize $\mathrm{LM}<em _critique="{critique" _text="\text">{\text {critique }}$ to be a pretrained encoder-decoder-type model and fine-tune it to generate critiques satisfying the CRITIQUE objective $x, \hat{y} \rightarrow$ $c$ using natural language critiques. Namely, if $\mathrm{LM}</em>)\right]$. We delegate PREDICT and REFINE tasks to GPT-3 via in-context learning. The procedure is depicted in Fig. 2a-b.}}$ is parameterized by $\theta$ we maximize $\mathbb{E}\left[\log p_{\theta}(c \mid x, \hat{y</p>
<p>The main difference of our implementation of SUPERVISED to that of Saunders et al. (2022)'s is that we rely on separate models for CRITIQUE and the rest of the tasks while they train a single GPT-3style model to collectively achieve PREDICT, CRITIQUE, REFINE and DIRECTREFINE; effectively merging $\mathrm{LM}<em _task="{task" _text="\text">{\text {critique }}$ and $\mathrm{LM}</em>$ into a single model. While this may seem parameter-efficient, our version has a few key advantages. First, leaving any}</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$\mathrm{LM}<em _task="{task" _text="\text">{\text {task }}$ model intact (parameters frozen) enables us to work with models that are already-deployed as $\mathrm{LM}</em>$, which is multiple orders of magnitude smaller than GPT-3, is much more computationally efficient and therefore accessible to a broader range of users.}}$ and those with expensive training and inference processes. Moreover, our approach refrains from disturbing overall integrity of a generalpurpose language model by conditioning it to a specific task. Lastly, training $\mathrm{LM}_{\text {critique }</p>
<h3>3.2 Direct-Refinement</h3>
<p>Madaan et al. (2023); Chen et al. (2023) propose that using the critiques from the model itself via few-shot prompting results in improved performance. On the contrary, Saunders et al. (2022) and Bai et al. (2022) argue that direct refinement (as denoted with DIRECTREFINE in this work) i.e. the practice of prompting a language model without self-generated critiques to directly repair its own answers proves a stronger baseline, especially when the model size is large $&gt;50 \mathrm{~B}$. They hypothesize that this is primarily due to model's initial answers getting increasingly more difficult to selfcritique as the model size grows. In fact, both Saunders et al. (2022) and Bai et al. (2022) showed that their largest model achieves superior end-task performance when performing DIRECTREFINE than refining using self-generated critiques. Hence, we use Direct-Refinement as a baseline and describe how we implement it via in-context learning in Section 6 while providing further discussions in Appendix B.5.</p>
<h2>4 RL4F: Reinforcement Learning for Feedback Generation</h2>
<p>SUPERVISED is straightforward to implement but it does not make use of any final task data $(x \rightarrow y)$ that is usually more abundant than natural language critiques. Moreover, it fails to provide ground for adaptation when the critiques in the train set are generally applicable but not entirely tailored to improving a target model. We describe RL4F where we follow supervised training with policy gradient learning using end-task data in order to generate critiques. We assume that the task model $\mathrm{LM}<em _critique="{critique" _text="\text">{\text {task }}$ is already deployed and treat it as a fixed module. In all of our implementations we train the natural language critique generator $\mathrm{LM}</em>$ :}}$ alone. In both SUPERVISED and RL4F, $\mathrm{LM}_{\text {critique }}$ takes in the input $x$ and an initial prediction $\hat{y}$ and produces a (natural language) critique $\hat{c</p>
<p>$$
\mathrm{LM}_{\text {critique }}(x, \hat{y})=\hat{c}
$$</p>
<p>Fig. 2c provides an illustration of RL4F. We implement $\mathrm{LM}_{\text {task }}$ as GPT-3 given its adaptability into new tasks using few-shot prompting. Our implementation which is primarily based on the RL4LMs library ${ }^{4}$ (Ramamurthy et al., 2022) will be publicly available.</p>
<p>Learning via Policy Gradient We warm-start RL4F by first fine-tuning $\mathrm{LM}<em _critique="{critique" _text="\text">{\text {critique }}$ for CRITIQUE which we defined as is the supervised objective of learning to generate natural language critiques $c$ conditioned on $x, \hat{y}$. We continue fine-tuning the policy network $\left(\mathrm{LM}</em>\right)$ to maximize the reward using Proximal Policy Optimization (Schulman et al., 2017). We utilize the implementation of PPO provided by Ramamurthy et al. (2022) and refer the readers to the original work about the details for KL-regularized PPO objective. While any policy gradient approach could be used e.g. REINFORCE (Williams, 1992), our initial experiments showed that PPO works best in this setting.}</p>
<p>Pseudocode for RL4F is provided in Algorithm 1 where we use two sets of in-context examples for prompting GPT-3. We define $E$ to be a set of in-context-learning examples in the form of $(x, y)$ to get GPT-3 solve PREDICT. Similarly, $E^{c}$ contains in-context examples to prompt GPT-3 to fix an initial attempt $\hat{y}$ into $y$ conditioned on the natural language critique $c$ which we termed as REFINE; $E^{c}={(x, \hat{y}, c, y)}$. As per our reward function,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Algorithm 1 RL4F
Pseudocode of the algorithm used to train feedback model.
Input: Dataset $\mathcal{D}=\left{\left(x^{i}, \hat{y}^{i}, y^{i}\right)\right}<em _critique="{critique" _text="\text">{i=1}^{N}$ of size $N$
Input: Initial $\mathrm{LM}</em>}}, \mathrm{LM<em m="m">{\text {task }}$
Input: In-context examples for refinement $E^{c}$
repeat
Sample mini-batch $\mathcal{D}</em>\right)\right}}=\left{\left(x^{m}, y^{m}, \hat{y}^{m<em _critique="{critique" _text="\text">{m=1}^{M} \sim D$
Sample $\hat{c} \sim \mathrm{LM}</em>$ in parallel
Sample $\hat{y}}}(x, \hat{y})$ for $D^{m<em _task="{task" _text="\text">{n e w} \sim \mathrm{LM}</em>$
Compute KL-regularized rewards $R_{t} \quad \triangleright$ Eq. (2)
Compute the advantage estimate $\hat{A}}}\left(E^{c}, x, \hat{y}, \hat{c}\right)$ for $D^{m<em _critique="{critique" _text="\text">{t}$
Update the $\mathrm{LM}</em>$ by maximizing the PPO objective
until convergence and return $\mathrm{LM}_{\text {critique }}$
we opt to use a lexical similarity metric ROUGE (1/2/L) (Lin, 2004) in Eq. (2) for planning and summarization datasets. Measuring ROUGE is computationally fast, making it easy to use in an online learning setting. Reward is only collected at a terminal stage i.e. either when the end of sentence token is produced or maximum number of tokens is reached.}</p>
<p>$$
\mathrm{R}(\hat{y}, y)=\operatorname{mean}(\mathrm{R} 1(\hat{y}, y), \mathrm{R} 2(\hat{y}, y), \mathrm{RL}(\hat{y}, y))
$$</p>
<h2>5 Datasets</h2>
<h3>5.1 Topic-Based Summarization</h3>
<p>Saunders et al. (2022) crowd-sourced natural language critiques for topic-based summarization. The train, validation and test sets contain 14230, 1150 and 2658 tuples of $(x, \hat{y}, \hat{c})$. The dataset provides multiple questions for a given passage each inquiring about a different aspect. Given a passage and question $(x)$ multiple summaries are sampled from the model. Human annotators provide natural language critiques for the answers along with improved summaries. One example is provided in Fig. 1 and more are available in the Appendix (Table 8).</p>
<h3>5.2 Interscript</h3>
<p>Interscript (Tandon et al., 2021) is an action planning dataset for everyday tasks such as "put on a costume" or "play musical chairs". Each goal $x$ is associated with a sequence of ground truth actions $y$. Along with $x, y$ pairs, it contains erroneous action plans $\hat{y}$ and natural language critiques $\hat{c}$ suggesting a fix. An example is provided in Fig. 1 for "put soap in dishwasher". Other examples of critiques are "You need to have music to play musi-</p>
<p>cal chairs." and "You need to pay for the costume before leaving the store". More examples are available in the Appendix (see Table 6). Interscript represents a low-resource scenario: it contains 253, 45 and 169 examples for train, validation and test sets where each example contains 1-4 reference texts.</p>
<h3>5.3 Synthetic Task: Alphabetization</h3>
<p>We synthetically generate a task for alphabetically sorting a list of words with lengths ranging between 3-12. We use the lexicon #11 by Keith Vertanen (2018) which contains 43 K unique English words. Given an unsorted list and a ground truth sorting of the list we identify 5 operations to sample a incorrect sorting of $y$ denoted by $\hat{y}$ and associated critique $c$ articulating what is wrong about $\hat{y}$ in natural language. One example is shown below:</p>
<div class="codehilite"><pre><span></span><code><span class="n">x</span><span class="o">:</span><span class="w"> </span><span class="n">mug</span><span class="o">,</span><span class="w"> </span><span class="n">greek</span><span class="o">,</span><span class="w"> </span><span class="n">book</span><span class="o">,</span><span class="w"> </span><span class="n">house</span>
<span class="n">y</span><span class="o">:</span><span class="w"> </span><span class="n">book</span><span class="o">,</span><span class="w"> </span><span class="n">greek</span><span class="o">,</span><span class="w"> </span><span class="n">house</span><span class="o">,</span><span class="w"> </span><span class="n">mug</span>
</code></pre></div>

<p>$\hat{y}$ : book, greek, house
c: The word mug is missing.
The operations we use for distortion are REORDER, REPLACE, ADD, REPEAT and REMOVE (shown above). We also leave majority of sorted lists intact for which the ground truth critique is "The list is correctly sorted". We use a total of 40 K examples for warm-starting $\mathrm{LM}_{\text {critique }}$ for the CRI-
TIQUE objective and another $10 \mathrm{~K}, 1 \mathrm{~K}$ and 1 K ex-
amples for PPO stage, for train, dev and test splits, respectively. Examples delineating other operations in action and corresponding natural language critiques are provided in Appendix A.</p>
<p>In alphabetization, we use Inverse-Levenstein distance for the reward function R as defined in Eq. (3) where $|\cdot|$ measures length of the list. Levenstein distance is a form of edit distance for single character edit operations such as removal, insertion and substitution in a string. We count word-level operations rather than character-level. Note that the higher inverse-Levenstein score of a predicted ordering, the closer it is to the alphabetically sorted version. The sorted list gets the maximum reward of 1 .</p>
<p>$$
R(\hat{y}, y)=1-\frac{\operatorname{Levenstein}(\hat{y}, y)}{\max (|\hat{y}|,|y|)}
$$</p>
<h2>6 Experiments and Results</h2>
<p>Our experiments are designed to test effectiveness of RL4F, along with other sources of critiques, in
both natural and controlled settings. In our evaluations, we test the usefulness of critiques by looking at the final task performance rather than evaluating generated critiques themselves; as multiple critiques may lead to the same correct answer.</p>
<p>Sampling Critiques We sample critiques from $\mathrm{LM}<em _critique="{critique" _text="\text">{\text {critique }}$ as in Eq. (1) by first concatenating the input and initial prediction. The specific input format for $\mathrm{LM}</em>$ with pretrained T5-large which is a 0.77 M parameter encoder-decoder type language model trained on large web text (Raffel et al., 2020).}}$ we use for Interscript is given below and the other two can be found in Appendix B.1. We initialize $\mathrm{LM}_{\text {critique }</p>
<div class="codehilite"><pre><span></span><code>Goal: {goal}
Steps: {initial_answer}
</code></pre></div>

<p>Downstream Predictions In our experiments, we consider GPT-3 as the $\mathrm{LM}<em _task="{task" _text="\text">{\text {task }}$ model. GPT-3 can handle a wide range of tasks with promptingusing a handful of task examples in the input and without requiring task-specific fine-tuning (Brown et al., 2020). GPT-3 is not only able to tackle numerous tasks conveniently but also can refine initial predictions when given a natural language critique (Madaan et al., 2022). Since our setting requires $\mathrm{LM}</em>$ and 3, 1 and 6 hand-written incontext examples for planning, summarization and alphabetizations tasks, respectively as we exhaust the 4096 token input limit.}}$ model to be able to model both the main task objective $x \rightarrow y$ and the refinement objective $x, \hat{y}, c \rightarrow y$, GPT-3 is a suitable candidate that can adapt to both, using few-shot exemplars. The prompt template we use for the latter is shown in Fig. 3 where we provide the model with an initial attempt to the question initial_answer and re-sample a revised prediction conditioned on the question and critique for the summarization task. We use code-davinci-002 checkpoint via OpenAI API ${ }^{5</p>
<p>In action planning, instead of resampling entire plans, we prompt GPT-3 to produce an edit operation on the initial plan $\hat{y}$. The set of edit operations identified in the original dataset are Insert, Remove and Reorder where each critique comes with a corresponding edit operation. Note that, these operations can algorithmically be applied to $\hat{y}$. While Reorder and Remove are expected to refer to existing steps in $\hat{y}$, we expect Insert to introduce a</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Interscript</th>
<th></th>
<th></th>
<th>Topic-Based Summarization</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Source of Critiques</td>
<td>BLEURT $\uparrow$</td>
<td>BERTScore $\uparrow$</td>
<td>R1/R2/RL $\uparrow$</td>
<td>BLEURT $\uparrow$</td>
<td>BERTScore $\uparrow$</td>
<td>R1/R2/RL $\uparrow$</td>
</tr>
<tr>
<td>Direct-Refinement</td>
<td>-1.07</td>
<td>86.97</td>
<td>$15.8 / 0.9 / 15.5$</td>
<td>0.09</td>
<td>93.1</td>
<td>$54.3 / 46.0 / 50.9$</td>
</tr>
<tr>
<td>SUPERVISED</td>
<td>-1.02</td>
<td>86.99</td>
<td>$19.4 / 0.5 / 18.5$</td>
<td>0.06</td>
<td>92.9</td>
<td>$53.2 / 46.4 / 50.7$</td>
</tr>
<tr>
<td>MemPrompt</td>
<td>-1.18</td>
<td>$\mathbf{8 7 . 4 5}$</td>
<td>$16.9 / \mathbf{1 . 9} / 16.7$</td>
<td>0.09</td>
<td>91.9</td>
<td>$48.8 / 40.4 / 45.6$</td>
</tr>
<tr>
<td>RL4F (Ours)</td>
<td>$\mathbf{- 0 . 9 2}$</td>
<td>87.23</td>
<td>$\mathbf{2 2 . 1 / 0 . 9 / 2 1 . 3}$</td>
<td>$\mathbf{0 . 1 0}$</td>
<td>$\mathbf{9 3 . 6}$</td>
<td>$\mathbf{5 5 . 1 / 4 8 . 2 / 5 2 . 6}$</td>
</tr>
<tr>
<td>With gold feedback</td>
<td>-0.69</td>
<td>89.56</td>
<td>$40.7 / 6.8 / 39.1$</td>
<td>0.22</td>
<td>94.2</td>
<td>$58.3 / 50.3 / 55.8$</td>
</tr>
</tbody>
</table>
<p>Table 1: Results for action sequence generation with Interscript (Tandon et al., 2021) and topic-based summarization by Saunders et al. (2022). We evaluate the performance of different sources for natural language critiques in steering $\mathrm{LM}_{\text {task }}$ to improve its predictions. Best scores in each column are made bold. We compare our method, RL4F, to three strong baselines and human-generated critiques. Self-Refinement prompts GPT-3 to self-repair its answer. MemPrompt uses memory to store human-generated critiques to previous outputs (Madaan et al., 2022). ROUGE and BERTScore are out of 100 while BLEURT can be negative or positive and should be used in comparing different methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source of Critiques</th>
<th style="text-align: center;">Exact Match</th>
<th style="text-align: center;">Inverse Levenstein</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Initial Outputs $(\hat{y})$</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuning davinci</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: left;">MemPrompt</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: left;">Direct-Refinement</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">$\mathbf{0 . 9 2}$</td>
</tr>
<tr>
<td style="text-align: left;">SUPERVISED</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: left;">RL4F</td>
<td style="text-align: center;">$\mathbf{6 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2}$</td>
</tr>
<tr>
<td style="text-align: left;">With gold feedback</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">0.94</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for alphabetization. Best scores are highlighted. Initial Outputs are obtained from GPT-3 (code-davinci-002) via in-context learning. SUPERVISED critiques misguides GPT-3, hurting its initial performance, as with MemPrompt. RL4F improves over the performance of SUPERVISED model by 27 absolute points. Self-Refinement around the same as RL4F. In Fig. 5, we further discuss advantages of RL4F over Self-Refinement when we sample and refine iteratively.
novel action. Hence, we stick with a generic lexical similarity metric in calculating reward (Eq. (2)) for this task. In summarization, we compare humanwritten summaries with the repaired model summaries.</p>
<p>Baselines We compare effectiveness of RL4F to SUPERVISED which is described in Section 3.1. This is the closest baseline to the approach by Saunders et al. (2022) and Schick et al. (2022) that abides by our condition that $\mathrm{LM}_{\text {task }}$ should remain unchanged. We use the same set of initial predictions $\hat{y}$ when comparing different critique generators.</p>
<p>In addition to SUPERVISED, we use a simple Direct-Refinement baseline where we ask $\mathrm{LM}<em _task="{task" _text="\text">{\text {task }}$ to revise the initial prediction given a fixed critique "Improve the answer." (DIRECTREFINE). The prompt template is otherwise the same as in other methods. We configure our in-context examples to show that not all $\hat{y}$ need to be repaired. Hence, $\mathrm{LM}</em>$ is free to update the prediction or leave}</p>
<div class="codehilite"><pre><span></span><code>Edit the below summary of the passage taking into
account the remarks in the feedback.
---
(passage)
Question: (question)
Answer: (initial_answer)
Question: (question) (critique)
Answer: (new_answer)
---
(passage)
Question: _
</code></pre></div>

<p>Figure 3: Prompt template for topic-based summarization. We ask GPT-3 to refine the initial prediction by using critique.
it as is when it is correct. Despite its simplicity, Direct-Refinement has been established as a strong baseline (Saunders et al., 2022).</p>
<p>Moreover, we compare to MemPrompt (Madaan et al., 2022). In their work, authors study a setup where $\mathrm{LM}_{\text {task }}$ generates an understanding along with the target output. For example, given a question "What sounds like good?", the model generates an understanding of the question "The question is asking for homonym." before saying "wood". In their critique retriever, they train a mapping to model $x$ into an understanding $u$. However understanding is redundant in particular tasks e.g. summarization where the question is no different than $u$, thus throughout our experiments, we replace the learned retriever in MemPrompt with BM25 (Harter, 1975).</p>
<p>Lastly, we use human-written critiques (gold feedback) for REFINE in getting $\mathrm{LM}_{\text {task }}$ to repair outputs and report this as an upperbound.</p>
<h3>6.1 Planning and Summarization</h3>
<p>Our main results for Interscript and topic-based summarization are provided in Table 1. Given the</p>
<p>free-from nature of the outputs, we evaluate planning and summarization tasks using text similarity metrics to capture semantic and lexical similarities. We utilize learned metrics such as BLEURT (Sellam et al., 2020) and BERTScore (Zhang* et al., 2020) along with ROUGE (Lin, 2004). We compare the performance achieved by using different sources of critiques to that of human-written critiques. Across all metrics, RL4F yields one of the closest outcomes to human-written critiques.</p>
<h3>6.2 Alphabetization</h3>
<p>We initialize our $\mathrm{LM}_{\text {critique }}$ using the synthetic critiques as described Section 5.3. Our results are provided in Table 2. For alphabetization we compute exact match and inverse-Levenstein scores as defined in Eq. (3). As an additional baseline, we fine-tune davinci (Brown et al., 2020) on the same train set as our RL4F.</p>
<p>Because of the synthetic procedure to create $x, \hat{y}, c$ triplets, the generated $\hat{y}$ as well as $c$ do not necessarily reflect the kinds of errors that $\mathrm{LM}_{\text {task }}$ would do. We observe this in the scores of SUPERVISED which fails to improve upon initial outputs. Nevertheless, RL4F procedure helps the policy network to capture a useful distribution of critiques, improving over SUPERVISED by more than 27 absolute points. In this simple task, DirectRefinement prompt also yields a competitive performance. Compared to full fine-tuning, we observe that despite training substantially fewer parameters RL4F achieves a significantly better accuracy. For a comparison to concurrent work Madaan et al. (2023), please refer to the appendix.</p>
<h2>7 Analysis</h2>
<p>Scaling Properties While we use T5-large as our main model for all of our experiments to initialize $\mathrm{LM}_{\text {critique }}$, we inquire about different model sizes. In Fig. 4, we consider three different model sizes to tackle Interscript ranging from 60 M to 770 M parameters. On the y-axis we provide average of three ROUGE scores for the generated plans. RL4F greatly benefits from an increase in the model size where a similar trend in SUPERVISED is non-existent.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Scaling properties of SUPERVISED and RL4F on Interscript. We observe that RL4F greatly benefits from an increase in the number of parameters.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: We apply REFINE multiple times given critiques from $\mathrm{LM}_{\text {critique }}$ (RL4F) on alphabetization task. RL4F leads to a handful of more corrections when used iteratively.</p>
<p>Semantic Drift In goal-oriented training, semantic drift occurs when the strings produced by the policy begin diverging from initial language (Lee et al., 2019; Blank, 1999). Although, RL4F does not guarantee that $\hat{c} \sim \mathrm{LM}_{\text {critique }}$ will be natural language, we find minimal sign of semantic drift in the sampled critiques with respect to fluency and naturalness. In most cases, generated critiques are intelligible. We speculate that may be due to GPT-3 responding to natural language best than gibberish, though future work should look closely into this to make a more conclusive argument. We provide sample predictions from both models in Appendix C for all three tasks.</p>
<p>Iterative Improvement In Section 6, we provide results with applying only one round of critiques in alphabetization. Past work advocated for iterative editing (Reid and Neubig, 2022; Faltings et al., 2021) as opposed to one-shot editing. In Fig. 5, we sample and apply critiques from $\mathrm{LM}_{\text {critique }}$ to $\hat{y}$ 's iteratively and see if the number of correctly</p>
<p>sorted lists increase or decrease. Note that critiques may also lead to deteriorating performance as we are not eliminating the solved examples and it is at $\mathrm{LM}_{\text {critique }}$ 's discretion to declare a solution as correct e.g. by saying "This list is correctly sorted.". In fact, when we ask GPT-3 to simply improve its predictions iteratively (via Direct-Refinement as described in Section 3.2), it occasionally scrambles an already correct ordering while not scoring any new points. In contrast, RL4F leads to up to 7 more corrections (see Fig. 5).</p>
<h2>8 Conclusion</h2>
<p>We have described a collaborative framework involving two language models where one model, the critique generator, is trained to improve the performance of the task model. We train the former via policy gradient learning while treating the task model as a black-box. We show that RL4F leads to superior final performance across three domains compared to other strong baselines without resulting as the critiques remain fluent and natural. Future work might focus on generalizing the critique generator into a mixture of experts allowing humans and other models to contribute to critiqueing procedure.</p>
<h2>9 Limitations</h2>
<p>RL4F is primarily targeted at improving final performance. While we have found that the critiques learned by RL4F remain natural, we do not introduce any explicit restraints preventing semantic drift. As though it may raise end-task performance, semantic drift would also hinder interpretability. Future work might study datasets that are not covered by this dataset and quantify semantic drift along with proposing measures to prevent it, as necessary. Moreover, this work does not provide an explicit mechanism to incorporate new critique labels that might become available in the future nor it identifies a framework that could combine critiques from multiple experts such humans and other machines. Lastly, we limit our analysis to GPT-3 and focus on a scenario where it is inefficient or impossible to train the task model while this may be a conservative assumption for other settings.</p>
<h2>Acknowledgement</h2>
<p>We thank Aman Madaan for sharing his insights throughout this project. We also thank Anna</p>
<p>Ivanova, Jacob Andreas, Zilu Tang, Shashank Gupta and Ashish Sabharwal for their valuable feedback on earlier drafts of this paper. Finally, we thank Rajkumar Ramamurthy and Prithviraj Ammanabrolu for helpful discussions on using their RL4LMs repository which facilitated the experiments of this work.</p>
<p>Afra Feyza Akyürek is supported in part by the U.S. NSF grant 1838193 and DARPA HR001118S0044 (the LwLL program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government. At MIT, Ekin Akyürek is supported by an MITAmazon ScienceHub fellowship and by the MITIBM Watson AI Lab.</p>
<h2>References</h2>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Andreas Blank. 1999. Why do new meanings occur? a cognitive typology of the motivations for lexical semantic change andreas blank. Cognitive Linguistics Research, page 61.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. 2022. Language model cascades. arXiv preprint arXiv:2207.10342.</p>
<p>Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan. 2021. Text editing by command. In Proceedings of</p>
<p>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5259-5274, Online. Association for Computational Linguistics.</p>
<p>Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José GC de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, et al. 2023. Bridging the gap: A survey on integrating (human) feedback for natural language generation. arXiv preprint arXiv:2305.00955.</p>
<p>Ge Gao, Eunsol Choi, and Yoav Artzi. 2022. Simulating bandit learning from user feedback for extractive question answering. arXiv preprint arXiv:2203.10079.</p>
<p>Stephen P Harter. 1975. A probabilistic approach to automatic keyword indexing. part i. on the distribution of specialty words in a technical literature. Journal of the american society for information science, 26(4):197-206.</p>
<p>Keith Vertanen. 2018. Big english word lists.
Jason Lee, Kyunghyun Cho, and Douwe Kiela. 2019. Countering language drift via visual grounding. arXiv preprint arXiv:1909.04499.</p>
<p>Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam Shazeer, Niki Parmar, and Simon Tong. 2019. Corpora generation for grammatical error correction. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3291-3301, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering. arXiv preprint arXiv:2210.03078.</p>
<p>Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve gpt-3 after deployment. arXiv preprint arXiv:2201.06009.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.</p>
<p>Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, and Guillermo Garrido. 2020. FELIX: Flexible text editing through tagging and insertion. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1244-1255, Online. Association for Computational Linguistics.</p>
<p>Eric Malmi, Yue Dong, Jonathan Mallinson, Aleksandr Chuklin, Jakub Adamek, Daniil Mirylenka, Felix Stahlberg, Sebastian Krause, Shankar Kumar, and Aliaksei Severyn. 2022. Text generation with textediting models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts, pages 1-7, Seattle, United States. Association for Computational Linguistics.</p>
<p>Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 2022a. Fast model editing at scale. In International Conference on Learning Representations.</p>
<p>Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. 2022b. Memorybased model editing at scale. In International Conference on Machine Learning, pages 15817-15831. PMLR.</p>
<p>Shikhar Murty, Christopher D Manning, Scott Lundberg, and Marco Tulio Ribeiro. 2022. Fixing model bugs with natural language patches. arXiv preprint arXiv:2211.03318.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020. Adapterhub: A framework for adapting transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020): Systems Demonstrations, pages 46-54, Online. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67.</p>
<p>Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. 2022. Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241.</p>
<p>Machel Reid and Graham Neubig. 2022. Learning to model editing processes. arXiv preprint arXiv:2205.12374.</p>
<p>Joana Ribeiro, Shashi Narayan, Shay B. Cohen, and Xavier Carreras. 2018. Local string transduction as sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics,</p>
<p>pages 1360-1371, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802.</p>
<p>Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022. Training language models with natural language feedback. arXiv preprint arXiv:2204.14146.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. Peer: A collaborative language model. arXiv preprint arXiv:2208.11663.</p>
<p>Timo Schick, Sahana Udupa, and Hinrich Schütze. 2021. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP. Transactions of the Association for Computational Linguistics, 9:14081424 .</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, and Jing Xu. 2022. When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels. arXiv preprint arXiv:2210.15893.</p>
<p>Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang. 2019. Mitigating gender bias in natural language processing: Literature review. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1630-1640, Florence, Italy. Association for Computational Linguistics.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. 2021. Interscript: A dataset for interactive learning of scripts through error feedback. arXiv preprint arXiv:2112.07867.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 339-352, Seattle, United States. Association for Computational Linguistics.</p>
<p>Norbert Wiener. 1960. Some moral and technical consequences of automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers. Science, 131(3410):1355-1358.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3):229-256.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<h2>A Dataset Processing</h2>
<p>Action Planning Interscript is larger but we are only using a subset, removing distractors. The scripts used for data cleaning will be released along with the codebase.</p>
<p>Alphabetization We sample initial predictions from GPT-3 for alphabetization some of which comprise multiple distortions simultaneously, yet we use one-step distortions to warm-start $\mathrm{LM}_{\text {critique }}$.</p>
<p>Given the following a pair of unsorted and sorted word lists e.g.
x: mug, greek, book, house
y: book, greek, house, mug
below are the operations we used to create our data:</p>
<div class="codehilite"><pre><span></span><code>REORDER
<span class="err">\</span>ddot<span class="p">{</span>y<span class="p">}:</span> book<span class="p">,</span> house<span class="p">,</span> greek<span class="p">,</span> mug
c<span class="p">:</span> The word greek is placed <span class="k">in</span> an
incorrect position<span class="o">.</span>
REPLACE
<span class="err">\</span>ddot<span class="p">{</span>y<span class="p">}:</span> book<span class="p">,</span> greek<span class="p">,</span> house<span class="p">,</span> mud
c<span class="p">:</span> The word mug is replaced <span class="k">with</span> mud
REMOVE
<span class="err">\</span>ddot<span class="p">{</span>y<span class="p">}:</span> book<span class="p">,</span> greek<span class="p">,</span> mug
c<span class="p">:</span> The word house is missing
REPEAT
<span class="err">\</span>ddot<span class="p">{</span>y<span class="p">}:</span> book<span class="p">,</span> house<span class="p">,</span> greek<span class="p">,</span> house<span class="p">,</span> mug
c<span class="p">:</span> The word house is repeated
ADD
<span class="err">\</span>ddot<span class="p">{</span>y<span class="p">}:</span> book<span class="p">,</span> hair<span class="p">,</span> greek<span class="p">,</span> house<span class="p">,</span> mug
c<span class="p">:</span> The word hair is not <span class="k">in</span> the
original list
</code></pre></div>

<p>NOTHING</p>
<p>$\hat{y}$ : book, greek, house, mug
c: The list is correctly sorted.</p>
<h2>B Experiment Details</h2>
<p>We use code-davinci-002 as GPT-3 unless otherwise specified. We compute ROUGE implementation in the datasets library and set use_stemmer=True for summarization and Interscript.</p>
<h2>B. 1 Data Formats</h2>
<p>We use the following input formats for $\mathrm{LM}_{\text {critique }}$ :</p>
<h2>- Summarization:</h2>
<p>{passage}
Question: {question}
Answer: {initial_answer}</p>
<h2>- Planning:</h2>
<p>Goal: {goal} Steps: {steps}</p>
<h2>- Alphabetization:</h2>
<p>{unsorted_list} ||| {initial_answer}
We train separate models for each of the datasets and evaluate individually. We use T5-large provided by transformers library.</p>
<h2>B. 2 Prompts for GPT-3</h2>
<p>We provide prompt templates used for Alphabetization and Interscript when prompting GPT-3 for REFINE. Template for summarization is provided in the main text.</p>
<h2>- Interscript:</h2>
<p>Goal: ${$ goal $}$
Steps: {steps}
Feedback: {critique}
Edit:</p>
<h2>- Alphabetization:</h2>
<p>{unsorted_list} ||| {initial_answer}
Feedback: {critique}
Edit:
In Direct-Refinement, templates remain the same and critique's are replaced with "Improve the answer.". Exact prompt exemplars will be made available in the released code repository.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Params</th>
<th style="text-align: left;">value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">supervised</td>
<td style="text-align: left;">batch size: 8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">epochs: 10</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">learning rate: 0.00001</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">learning rate scheduler: cosine</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">weight decay: 0.01</td>
</tr>
<tr>
<td style="text-align: left;">supervised+ppo</td>
<td style="text-align: left;">steps per update: 240</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">total number of steps: 96,000</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">batch size: 24</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">epochs per update: 5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">learning rate: 0.000001</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">entropy coefficient: 0.001</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">initial kl coeff: 0.00001</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">target kl: 3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">discount factor: 0.99</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">gae lambda: 0.95</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">clip ratio: 0.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">value function coeff: 0.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">rollouts top k: 100</td>
</tr>
<tr>
<td style="text-align: left;">decoding</td>
<td style="text-align: left;">sampling: True</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">temperature: 0.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">min length: 5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">max new tokens: 20</td>
</tr>
<tr>
<td style="text-align: left;">tokenizer</td>
<td style="text-align: left;">padding side: left</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">truncation side: right</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">max length: 512</td>
</tr>
</tbody>
</table>
<p>Table 3: List of hyperparameters for Alphabetization.</p>
<h2>B. 3 Standard Deviations</h2>
<p>Standard deviations of R1/R2/RL scores across 5 runs in Interscript are 1.4/0.1/1.1 for SUPERVISED and $0.5 / 0.4 / 0.5$ for RL4F.</p>
<h2>B. 4 Hyperparameters</h2>
<p>In all of our experiments we use temperature 0 for prompting GPT-3 except when sampling initial predictions for alphabetization we set it to 0.5 . We provide hyperparameters for RL4LMs (Ramamurthy et al., 2022) in Table 3, Table 4 and Table 5.</p>
<h2>B. 5 Results for Self-Refine (Madaan et al., 2023) on Alphabetization</h2>
<p>Madaan et al. (2023) and Chen et al. (2023) propose that self-generated critiques (sampling critiques simply via few-shot prompting) is useful for a range of tasks. We examine if self-generated critiques are more useful for Alphabetization task than other techniques proposed in Table 2. Doing so, we curate a few-shot prompt:</p>
<p>Below is a given list of words which are supposed to be sorted in alphabetical order.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Params</th>
<th style="text-align: center;">value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">supervised</td>
<td style="text-align: center;">batch size: 5 <br> epochs: 5 <br> learning rate: 0.00001 <br> learning rate scheduler: cosine <br> weight decay: 0.01</td>
</tr>
<tr>
<td style="text-align: center;">supervised+ppo</td>
<td style="text-align: center;">steps per update: 256 <br> total number of epochs: 256,000 <br> batch size: 8 <br> epochs per update: 5 <br> learning rate: 0.0000005 <br> entropy coefficient: 0.001 <br> initial kl coeff: 0.01 <br> target kl: 2 <br> discount factor: 0.99 <br> gae lambda: 0.95 <br> clip ratio: 0.2 <br> value function coeff: 0.5 <br> rollouts temperature: 0.7</td>
</tr>
<tr>
<td style="text-align: center;">decoding</td>
<td style="text-align: center;">sampling: True <br> temperature: 0.3 <br> min length: 15 <br> max new tokens: 50 <br> repetition penalty: 0.2</td>
</tr>
<tr>
<td style="text-align: center;">tokenizer</td>
<td style="text-align: center;">padding side: left <br> truncation side: left <br> max length: 512</td>
</tr>
</tbody>
</table>
<p>Table 4: List of hyperparameters for Interscript.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Params</th>
<th style="text-align: left;">value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">supervised</td>
<td style="text-align: left;">batch size: 4 <br> epochs: 7 <br> learning rate: 0.00001 <br> learning rate scheduler: cosine <br> weight decay: 0.01</td>
</tr>
<tr>
<td style="text-align: left;">supervised+ppo</td>
<td style="text-align: left;">steps per update: 1024 <br> total number of epochs: 143,360 <br> batch size: 4 <br> epochs per update: 3 <br> learning rate: 0.0000001 <br> entropy coefficient: 0.001 <br> initial kl coeff: 0.01 <br> target kl: 2 <br> rollouts temperature: 0.7</td>
</tr>
<tr>
<td style="text-align: left;">decoding</td>
<td style="text-align: left;">sampling: True <br> temperature: 0.7 <br> min length: 20 <br> max new tokens: 150 <br> repetition penalty: 0.2</td>
</tr>
<tr>
<td style="text-align: left;">tokenizer</td>
<td style="text-align: left;">padding side: right <br> truncation side: right <br> max length: 1024</td>
</tr>
</tbody>
</table>
<p>Table 5: List of hyperparameters for Topic-Based Summarization dataset by Saunders et al. (2022).</p>
<p>Describe what is wrong in the provided ordering.</p>
<div class="codehilite"><pre><span></span><code><span class="o">---</span>
<span class="nv">Ordering</span>:<span class="w"> </span><span class="nv">quirky</span><span class="w"> </span><span class="nv">whimsical</span><span class="w"> </span><span class="nv">bubbly</span><span class="w"> </span><span class="nv">joyous</span>
<span class="nv">delightful</span><span class="w"> </span><span class="nv">melodic</span><span class="w"> </span><span class="nv">glimmering</span><span class="w"> </span><span class="nv">vivacious</span>
<span class="nv">radiant</span><span class="w"> </span><span class="nv">lively</span><span class="w"> </span><span class="nv">zestful</span><span class="w"> </span><span class="nv">spontaneous</span>
<span class="nv">Feedback</span>:<span class="w"> </span><span class="nv">Whimsical</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">come</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="k">end</span>.
<span class="nv">Delightful</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">come</span><span class="w"> </span><span class="nv">before</span><span class="w"> </span><span class="nv">joyous</span>.
</code></pre></div>

<div class="codehilite"><pre><span></span><code>---
Ordering: airy amiable animated ardent
astute beaming blithe brilliant
Feedback: This listed is correctly sorted.
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gd">--- </span>
Ordering: curious sprightly vivacious
tenacious passionate vivacious
Feedback: The list contains duplicates
and passionate should come before
sprightly.
</code></pre></div>

<div class="codehilite"><pre><span></span><code>---
Ordering: \{ordering\}
Feedback:
</code></pre></div>

<p>After sampling critiques using the above prompt, we follow the same steps described in Section 6 for refinement. We obtain $21.6 \%$ exact match accuracy using these critiques which is a significant drop from code-davinci-002's initial performance. When we sample critiques and refinements from text-davinci-002, the exact match score increases notably (to $58.6 \%$ ) while still hurting the initial accuracy of $63.6 \%$. SelfRefine may improve over initial performance when critiques are sampled from more capable models such as text-davinci-003 (Madaan et al., 2023). Nonetheless, having initialized as T5-large, RL4F's critique model already produces useful feedback despite being significantly smaller than text-davinci-003.</p>
<h2>C Sample Predictions</h2>
<p>Sample predictions for all three tasks are provided in Table 6, Table 7 and Table 8. A manual examination of the generated critiques revealed that close to $100 \%$ of the critiques generated for alphabetization and action planning are grammatical. However, critiques for the topic-based-summarization task often involve repetitions or generic calls to fix or improve the answer e.g. "The answer is not adequate. The answer is wrong, it is wrong and should be fixed."</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">SUPERVISED Critique</th>
<th style="text-align: center;">Edit w/ SUPERVISED</th>
<th style="text-align: center;">RL4F Critique</th>
<th style="text-align: center;">Edit w/ RL4F</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Goal: go to the hardware store <br> Steps: <br> 1. pay for the items <br> 2. decided to go to the hardware store <br> 3. pack items in a bag <br> 4. create a shopping list <br> 5. go to the grocery store <br> 6. drive to the store <br> 7. walk around the store <br> 8. put items in cart <br> 9. pay for the groceries</td>
<td style="text-align: center;">You can't pay for groceries if you don't pay for the groceries. [Repetition]</td>
<td style="text-align: center;">Insert 'pay for the groceries' before 9 [Repeated nodes due to critique.]</td>
<td style="text-align: center;">The plan tells you to pay for the items before you go to the hardware store. [Correctly points out an error.]</td>
<td style="text-align: center;">Reorder the edge between 1 and 2 [GPT-3 correctly identifies the node to be replaced, yet the new location is not correct.]</td>
</tr>
<tr>
<td style="text-align: center;">Goal: ride the bus <br> Steps: <br> 1. decided to ride the bus <br> 2. put on shoes and clothing <br> 3. walk to the front door <br> 4. grab the handle <br> 5. open the door <br> 6. walk out of the house <br> 7. lock the door <br> 8. ride the bus</td>
<td style="text-align: center;">You have to be in the house before you can get out. You can't get out of the house until you're in the house. [Logically sound, but unhelpful critique.]</td>
<td style="text-align: center;">Reorder the edge between 5 and 6 [GPT-3 fixes what it thinks the critique is saying.]</td>
<td style="text-align: center;">You have to be at the bus stop before you can ride it. [Points out an helpful correction.]</td>
<td style="text-align: center;">Insert 'walk to the bus stop' after 8 [A missing node is added to an incorrect place.]</td>
</tr>
<tr>
<td style="text-align: center;">Goal: turn on computer <br> Steps: <br> 1. sit down at computer <br> 2. pull the chair closer to the computer <br> 3. look for the power switch on the computer <br> 4. check the power connection <br> 5. move a finger to the power switch <br> 6. press the power switch on the computer <br> 7. relax until the computer boots up <br> 8. turn on computer</td>
<td style="text-align: center;">You can't press the power switch on the computer if you don't have the power switch. [Unhelpful critique.]</td>
<td style="text-align: center;">Insert node 'find the power switch' before 6 [Critique results in incorrect edit.]</td>
<td style="text-align: center;">You have to press the power switch to turn on the computer. You have to do this before you can relax. [Points out what's already correct in the plan.]</td>
<td style="text-align: center;">Reorder the edge between 6 and 7 [Critique results in incorrect edit.]</td>
</tr>
<tr>
<td style="text-align: center;">Goal: pick up large kettle from counter Steps: <br> 1. make sure ingredients are all ready <br> 2. look for the pot <br> 3. extend hand out <br> 4. reach for the pot <br> 5. grip the pot <br> 6. lift the pot <br> 7. pick up pot from counter</td>
<td style="text-align: center;">You need to reach for the pot before you can reach for it. [Incorrect critique involving repetition.]</td>
<td style="text-align: center;">Reorder the edge between 3 and 4 [Critique results in an unnecessary edit.]</td>
<td style="text-align: center;">Plan tells you to reach for the kettle before you reach for it. [Points out a redundancy in the plan.]</td>
<td style="text-align: center;">Remove node 3 [Removes unnecessary node.]</td>
</tr>
</tbody>
</table>
<p>Table 6: Samples for Interscript (Tandon et al., 2021). The authors' comments evaluating the generations are given in brackets. Red comments indicate what went wrong and blue means correct. Refinements use the corresponding critiques.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">SUPERVISED Critique</th>
<th style="text-align: center;">SUPERVISED Refinement</th>
<th style="text-align: center;">RL4F Critique</th>
<th style="text-align: center;">RL4F Refinement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">accomplishments catwalk accessory incidental sixty gad remission melted juices fellowship unto dredge III accessory accomplishments catwalk dredge fellowship gad incidental juices melted remission sixty unto</td>
<td style="text-align: center;">The word melted is placed in an incorrect position. [The word melted is in the correct position.]</td>
<td style="text-align: center;">accessory accomplishments catwalk dredge fellowship gad incidental melted juices remission sixty unto</td>
<td style="text-align: center;">The list is correctly sorted. [Correct.]</td>
<td style="text-align: center;">accessory accomplishments catwalk dredge fellowship gad incidental juices melted remission sixty unto [Correct.]</td>
</tr>
<tr>
<td style="text-align: center;">inseparable snapping astrology advertising unjust indies dismantle bangkok boosters brasil businessmen III advertising astrology bangkok brasil businessmen boosters dismantle indies inseparable injustice snapping</td>
<td style="text-align: center;">The word unjust is missing. [Correctly points out that unjust is missing.]</td>
<td style="text-align: center;">advertising - astrology - bangkok brasil businessmen boosters dismantle indies injustice inseparable snapping [GPT-3 fails to add unjust; keeps the word injustice which was not in the original list.]</td>
<td style="text-align: center;">The word unjust is replaced with injustice. [Correctly indicates unjust is replaced with injustice]</td>
<td style="text-align: center;">advertising - astrology - bangkok brasil businessmen boosters dismantle indies inseparable injustice snapping [GPT-3 fails to replace injustice with unjust.]</td>
</tr>
</tbody>
</table>
<p>Table 7: Samples for Alphabetization. Unsorted list and GPT-3's attempt in sorting them are separated with III in the input.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">SUPERVISED Critique</th>
<th style="text-align: center;">SUPERVISED Refinement</th>
<th style="text-align: center;">RL4F Critique</th>
<th style="text-align: center;">RL4F Refinement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Jeffery was sitting in his statistics class nodding away as the information was simply passing by him. He could hardly wait to get out of class. Mondays, and Fridays were fine because there was a small chance of him seeing her. However, on Wednesdays she was always on time. He could see it now walking out from his this room strolling down Taylor Mall.[...]</td>
<td style="text-align: center;">The response does not mention that Jeffery's friend is killed. The response only mentions that he is saved. [No mention of murder in the story.]</td>
<td style="text-align: center;">Summer is a student in Jeffery's statistics class. She is a beautiful girl with long dark hair and a sunflower dress. Every Wednesday at 12:30, she waits for Jeffery to come out of class so she can see his smiling face. However, she has never had the courage to ask him to a date. [GPT-3 ignores the critique and leaves the answer as is. It's Jeffery who is interested in asking Summer out not the other way around.]</td>
<td style="text-align: center;">The answer doesn't mention Jeffery asking her to a date. It should mention that he is not sure how to ask her to a date. [This is an important detail that's is confused by the model's initial prediction.]</td>
<td style="text-align: center;">Summer is a student in Jeffery's statistics class. She is a beautiful girl with long dark hair and a sunflower dress. Every Wednesday at 12:30, she waits for Jeffery to come out of class so she can see his smiling face. However, she has never had the courage to ask him to a date. Jeffery is not sure how to ask her to a date. [GPT-3 successfully adds that it's Jeffery who is into Summer, yet it fails to remove the previous incorrect sentence suggesting otherwise.]</td>
</tr>
<tr>
<td style="text-align: center;">Gary left the house at six am on Wednesday; headed out to endure another long workday. After he walked through the yard to the driveway, he made sure to latch the gate securely, because that's what Sarah liked. He smiled as he did so, and thought of how earlier, he had closed the shower curtain all the way (another Sarah preference), and gently lowered the toilet seat. [...]Summarize how Gary feels about his life.</td>
<td style="text-align: center;">The answer should mention that Gary gets a friend for dinner. There are other examples of a friendship that are mentioned in the text. [The feedback is not helpful with respect to the question which asks how Gary is feeling about life.]</td>
<td style="text-align: center;">Gary feels that his life is boring and that he is stuck in a rut. He feels that he is a slave to his wife and his job. He wants to be free of his responsibilities and wants to be a bartender. However, he gets a friend for dinner and there are other examples of a friendship that are mentioned in the text. [GPT-3 appends the irrelevant details suggested by the critique.]</td>
<td style="text-align: center;">It's omitted that he is the one who is angry. He has a son, Steven. [Critique suggests a detail about his anger.]</td>
<td style="text-align: center;">Gary feels that his life is boring and that he is stuck in a rut. He feels that he is a slave to his wife and his job. He wants to be free of his responsibilities and wants to be a bartender. He is angry at his wife and has a son, Steven. [Having seen the critique finds out that Gary's anger is actually towards his wife and improves its answer accordingly.]</td>
</tr>
<tr>
<td style="text-align: center;">Answer: Gary feels that his life is boring and that he is stuck in a rut. He feels that he is a slave to his wife and his job. He wants to be free of his responsibilities and wants to be a bartender.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SR-25 pattern magazine <br> An SR-25, AR-10 or LR308 PATTERN MAGAZINE is a type of detachable firearm magazine based on the original Armalite AR-10 "waffle" magazine design introduced in the late 1950s. [...]What does the text say about the SR-25, AR10, or LR-308 Pattern Magazine?</td>
<td style="text-align: center;">The response includes inaccurate information. Firstly, a reference to the SR-25 pattern is omitted. [The response does not contain inaccurate information and AR10A indeed contains SR25 pattern.]</td>
<td style="text-align: center;">A detachable firearm magazine based on the original Armalite AR-10 'waffle' magazine design introduced in the late 1950s, the design has been used on many AR-10 derivatives. It was reintroduced in 2012 with their new AR10A model. [GPT-3 repeats the initial answer.]</td>
<td style="text-align: center;">The response is missing several important information about the magazine. The first sentence should be more specific. [Critique calls for a more detailed summary.]</td>
<td style="text-align: center;">A detachable firearm magazine based on the original Armalite AR-10 'waffle' magazine design introduced in the late 1950s, the design has been used on many AR-10 derivatives. It was reintroduced in 2012 with their new AR-10A model. The magazine is used in many AR-10 derivatives such as the Knight's Armament SR-25 and DPMS Panther LR-308, as well as bolt action rifles such as the Mossberg MVP.308, Ruger Precision Rifle and Q FIX. Aftermarket magazines are produced by Magpul, Lancer Systems, and others. [More comprehensive summary of magazine designs.]</td>
</tr>
</tbody>
</table>
<p>Table 8: Samples for Topic-Based Summarization by Saunders et al. (2022). Passages are truncated.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: As RL4F is trained, we track how evaluation metrics evolve for dev and test sets. Here, we display the results assessing the revised outputs conditioned on the critiques for test samples.</p>
<h1>D Learning Curves for Reinforcement Learning</h1>
<p>In Fig. 6, we provide how evaluation metrics progress as $\mathrm{LM}_{\text {critique }}$ in RL4F is trained.</p>
<h1>ACL 2023 Responsible NLP Checklist</h1>
<h2>A For every submission:</h2>
<p>A1. Did you describe the limitations of your work? 9</p>
<p>A2. Did you discuss any potential risks of your work?
Left blank.
A3. Do the abstract and introduction summarize the paper's main claims?
Left blank.
\A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<p>6
B1. Did you cite the creators of artifacts you used?
Left blank.
\A B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Datasets we used are made publicly available by the authors for scientific use. Interscript is available via Apache 2.0.</p>
<p>A B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Left blank.
\A B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Left blank.
\A B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Left blank.
\A B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Left blank.</p>
<h2>C Did you run computational experiments?</h2>
<p>6
C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>6, Appendix B
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
6, Appendix B
C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
6
C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
6, Appendix
D Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.</p>
<p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We have identified a handful of examples where a pair of train and test examples differs by only a single concept e.g. all occurrences of "noodle" in the train sample was replaced with "food" to produce the test sample. The goal and steps are the same otherwise. MemPrompt does exceedingly well on these 7 cases, hence performing occasionally higher, yet fails in the rest of the test/val examples.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
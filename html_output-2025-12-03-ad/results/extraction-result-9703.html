<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9703 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9703</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9703</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-273501717</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.15393v1.pdf" target="_blank">CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges</a></p>
                <p><strong>Paper Abstract:</strong> The use of large language models (LLMs) as automated evaluation tools to assess the quality of generated natural language, known as LLMs-as-Judges, has demonstrated promising capabilities and is rapidly gaining widespread attention. However, when applied to pairwise comparisons of candidate responses, LLM-based evaluators often exhibit selection bias. Specifically, their judgments may become inconsistent when the option positions or ID tokens are swapped, compromising the effectiveness and fairness of the evaluation result. To address this challenge, we introduce CalibraEval, a novel label-free method for mitigating selection bias during inference. Specifically, CalibraEval reformulates debiasing as an optimization task aimed at adjusting observed prediction distributions to align with unbiased prediction distributions. To solve this optimization problem, we propose a non-parametric order-preserving algorithm (NOA). This algorithm leverages the partial order relationships between model prediction distributions, thereby eliminating the need for explicit labels and precise mathematical function modeling.Empirical evaluations of LLMs in multiple representative benchmarks demonstrate that CalibraEval effectively mitigates selection bias and improves performance compared to existing debiasing methods. This work marks a step toward building more robust and unbiased automated evaluation frameworks, paving the way for improved reliability in AI-driven assessments</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9703.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9703.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CalibraEval-study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study of LLMs used as automated pairwise evaluators, documenting selection biases (position/token) relative to human-labelled references and quantifying what is degraded when LLMs act as judges; introduces CalibraEval to reduce those degradations at inference time without labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reference-free and reference-based evaluation of generated natural language (reward-model evaluation, multi-turn responses, preference benchmarks â€” tasks include chat, safety, reasoning and multi-turn dialogue).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama-3-8B, Llama-3.1-8B, Qwen-14B, Qwen-72B, ChatGPT (gpt-3.5-turbo-1106), GPT-4o (evaluated as judges).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise comparison prompts (e.g., 'Which response is Better? A: ... B: ...') using default and alternative prompt templates; evaluations included swapping option positions and swapping option ID tokens (A/B etc.) to probe selection bias; judged responses on three public benchmarks (RewardBench, MTBench, PreferenceBench).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Ground-truth human preferences from benchmark datasets: RewardBench (2,985 prompt-choice-rejection trios across Chat/Chat_Hard/Safety/Reasoning), MTBench (3,355 expert-level pairwise human preferences for 80 questions), PreferenceBench (2,000 human-labeled response pairs and 200 evaluation criteria). (Human labels from those datasets used as reference for accuracy and recall-based metrics.)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Consistency/agreement measured with Fleiss' Kappa and Intraclass Correlation Coefficients ICC(2,k) and ICC(3,k); reference-based agreement to human labels measured by standard deviation of recalls (RStd) and Accuracy. Example reported values: ChatGPT baseline ICC(2,k)=62.67, ICC(3,k)=70.75, RStd=16.79, Accuracy=65.27; after CalibraEval ICC(2,k)=77.25, ICC(3,k)=77.60, RStd=5.50, Accuracy=67.13 (Table 3 and Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When LLMs act as judges instead of humans the paper documents: (1) selection bias â€” inconsistent judgments when option positions or ID tokens are swapped (position/token bias), which degrades fairness and reliability; (2) loss of evaluative information if one simply discards inconsistent judgments (â€˜tiesâ€™) â€” the paper explicitly notes discarding inconsistencies 'may lead to a loss of evaluative information'; (3) degraded agreement with human labels (increased RStd and lower accuracy) caused by selection bias; (4) higher variance and instability across prompt templates and ID-token choices, and instruction-only debiasing can even degrade consistency; overall, a reduction in validity and robustness of evaluation compared to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete examples given in the paper: (a) Illustrative failure mode where simply swapping option positions or ID tokens leads to different LLM judgments (Figure 1 / Figure 2); (b) Quantitative example: ChatGPT's pre-debiasing RStd on RewardBench = 16.79 with Accuracy = 65.27, indicating substantial imbalance vs human references; (c) The paper states that excluding inconsistent judgments as 'ties' improves consistency but 'may lead to a loss of evaluative information' (Introduction / Related Work). (d) Debiasing-by-instruction sometimes reduces consistency (Main Results).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Caveats and counterexamples noted: (1) Pairwise comparisons by LLMs often better reflect human judgment than pointwise scoring (cited prior work and discussed in Related Work); (2) On stronger LLMs (e.g., Qwen-72B) baseline consistency can already be high and the absolute gains from some baselines (e.g., Pride) are small; (3) Effective mitigation (CalibraEval) not only reduces selection bias but can improve agreement with human references (e.g., ChatGPT accuracy rose from 65.27 to 67.13 and RStd fell from 16.79 to 5.50), implying some failures are specifically due to removable selection bias rather than an inherent inability to match humans; (4) some mitigation methods (split-and-merge, multi-agent discussion) can help but are more costly and their effectiveness is uncertain (Related Work / Main Results).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introductions and motivation (Introduction, Related Work); empirical comparisons and metrics (Sections 4 and 5); Tables 1, 2, 3; Figures 1â€“3; statements about loss of information and instruction-degradation in Introduction and Main Results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Humans or llms as the judge? a study on judgement biases <em>(Rating: 2)</em></li>
                <li>Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs <em>(Rating: 2)</em></li>
                <li>Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge <em>(Rating: 2)</em></li>
                <li>Large Language Models are not Fair Evaluators <em>(Rating: 2)</em></li>
                <li>Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models <em>(Rating: 2)</em></li>
                <li>Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment <em>(Rating: 1)</em></li>
                <li>Mitigating the Bias of Large Language Model Evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9703",
    "paper_id": "paper-273501717",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "CalibraEval-study",
            "name_full": "CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges (this paper)",
            "brief_description": "Empirical study of LLMs used as automated pairwise evaluators, documenting selection biases (position/token) relative to human-labelled references and quantifying what is degraded when LLMs act as judges; introduces CalibraEval to reduce those degradations at inference time without labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Reference-free and reference-based evaluation of generated natural language (reward-model evaluation, multi-turn responses, preference benchmarks â€” tasks include chat, safety, reasoning and multi-turn dialogue).",
            "llm_judge_model": "Llama-3-8B, Llama-3.1-8B, Qwen-14B, Qwen-72B, ChatGPT (gpt-3.5-turbo-1106), GPT-4o (evaluated as judges).",
            "llm_judge_setup": "Pairwise comparison prompts (e.g., 'Which response is Better? A: ... B: ...') using default and alternative prompt templates; evaluations included swapping option positions and swapping option ID tokens (A/B etc.) to probe selection bias; judged responses on three public benchmarks (RewardBench, MTBench, PreferenceBench).",
            "human_evaluation_setup": "Ground-truth human preferences from benchmark datasets: RewardBench (2,985 prompt-choice-rejection trios across Chat/Chat_Hard/Safety/Reasoning), MTBench (3,355 expert-level pairwise human preferences for 80 questions), PreferenceBench (2,000 human-labeled response pairs and 200 evaluation criteria). (Human labels from those datasets used as reference for accuracy and recall-based metrics.)",
            "agreement_metric": "Consistency/agreement measured with Fleiss' Kappa and Intraclass Correlation Coefficients ICC(2,k) and ICC(3,k); reference-based agreement to human labels measured by standard deviation of recalls (RStd) and Accuracy. Example reported values: ChatGPT baseline ICC(2,k)=62.67, ICC(3,k)=70.75, RStd=16.79, Accuracy=65.27; after CalibraEval ICC(2,k)=77.25, ICC(3,k)=77.60, RStd=5.50, Accuracy=67.13 (Table 3 and Table 2).",
            "losses_identified": "When LLMs act as judges instead of humans the paper documents: (1) selection bias â€” inconsistent judgments when option positions or ID tokens are swapped (position/token bias), which degrades fairness and reliability; (2) loss of evaluative information if one simply discards inconsistent judgments (â€˜tiesâ€™) â€” the paper explicitly notes discarding inconsistencies 'may lead to a loss of evaluative information'; (3) degraded agreement with human labels (increased RStd and lower accuracy) caused by selection bias; (4) higher variance and instability across prompt templates and ID-token choices, and instruction-only debiasing can even degrade consistency; overall, a reduction in validity and robustness of evaluation compared to human judgments.",
            "examples_of_loss": "Concrete examples given in the paper: (a) Illustrative failure mode where simply swapping option positions or ID tokens leads to different LLM judgments (Figure 1 / Figure 2); (b) Quantitative example: ChatGPT's pre-debiasing RStd on RewardBench = 16.79 with Accuracy = 65.27, indicating substantial imbalance vs human references; (c) The paper states that excluding inconsistent judgments as 'ties' improves consistency but 'may lead to a loss of evaluative information' (Introduction / Related Work). (d) Debiasing-by-instruction sometimes reduces consistency (Main Results).",
            "counterexamples_or_caveats": "Caveats and counterexamples noted: (1) Pairwise comparisons by LLMs often better reflect human judgment than pointwise scoring (cited prior work and discussed in Related Work); (2) On stronger LLMs (e.g., Qwen-72B) baseline consistency can already be high and the absolute gains from some baselines (e.g., Pride) are small; (3) Effective mitigation (CalibraEval) not only reduces selection bias but can improve agreement with human references (e.g., ChatGPT accuracy rose from 65.27 to 67.13 and RStd fell from 16.79 to 5.50), implying some failures are specifically due to removable selection bias rather than an inherent inability to match humans; (4) some mitigation methods (split-and-merge, multi-agent discussion) can help but are more costly and their effectiveness is uncertain (Related Work / Main Results).",
            "paper_reference": "Introductions and motivation (Introduction, Related Work); empirical comparisons and metrics (Sections 4 and 5); Tables 1, 2, 3; Figures 1â€“3; statements about loss of information and instruction-degradation in Introduction and Main Results.",
            "uuid": "e9703.0",
            "source_info": {
                "paper_title": "CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Humans or llms as the judge? a study on judgement biases",
            "rating": 2,
            "sanitized_title": "humans_or_llms_as_the_judge_a_study_on_judgement_biases"
        },
        {
            "paper_title": "Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs",
            "rating": 2,
            "sanitized_title": "judging_the_judges_a_systematic_investigation_of_position_bias_in_pairwise_comparative_assessments_by_llms"
        },
        {
            "paper_title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge",
            "rating": 2,
            "sanitized_title": "justice_or_prejudice_quantifying_biases_in_llmasajudge"
        },
        {
            "paper_title": "Large Language Models are not Fair Evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models",
            "rating": 2,
            "sanitized_title": "unveiling_selection_biases_exploring_order_and_token_sensitivity_in_large_language_models"
        },
        {
            "paper_title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
            "rating": 1,
            "sanitized_title": "is_llmasajudge_robust_investigating_universal_adversarial_attacks_on_zeroshot_llm_assessment"
        },
        {
            "paper_title": "Mitigating the Bias of Large Language Model Evaluation",
            "rating": 1,
            "sanitized_title": "mitigating_the_bias_of_large_language_model_evaluation"
        }
    ],
    "cost": 0.0101325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges
20 Oct 2024</p>
<p>Haitao Li 
Junjie Chen chenjj24@mails.tsinghua.edu.cn 
Qingyao Ai 
Zhumin Chu 
Yujia Zhou zhouyujia@tsinghua.edu.cn 
Qian Dong 
Yiqun Liu yiqunliu@tsinghua.edu.cn 
Calibraeval </p>
<p>DCST
Tsinghua University Quan Cheng Laboratory</p>
<p>DCST
Tsinghua University Quan Cheng Laboratory</p>
<p>DCST
Tsinghua University Quan Cheng Laboratory</p>
<p>DCST
Tsinghua University Quan Cheng Laboratory</p>
<p>DCST
Tsinghua University Quan Cheng Laboratory</p>
<p>DCST
Tsinghua University Quan Cheng Laboratory</p>
<p>DCST
Tsinghua University Zhongguancun Laboratory</p>
<p>WashingtonDCUSA</p>
<p>CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges
20 Oct 2024BC39EDF79256D85DF0399DE1302DDF8B10.1145/nnnnnnn.nnnnnnnarXiv:2410.15393v1[cs.CL]LLM-as-JudgesSeclection BiasCalibrate Prediction Distribution
The use of large language models (LLMs) as automated evaluation tools to assess the quality of generated natural language, known as "LLMs-as-Judges", has demonstrated promising capabilities and is rapidly gaining widespread attention.However, when applied to pairwise comparisons of candidate responses, LLM-based evaluators often exhibit selection bias.Specifically, their judgments may become inconsistent when the option positions or ID tokens are swapped, compromising the effectiveness and fairness of the evaluation result.To address this challenge, we introduce CalibraEval, a novel label-free method for mitigating selection bias during inference.Specifically, CalibraEval reformulates debiasing as an optimization task aimed at adjusting observed prediction distributions to align with unbiased prediction distributions.To solve this optimization problem, we propose a non-parametric orderpreserving algorithm (NOA).This algorithm leverages the partial order relationships between model prediction distributions, thereby eliminating the need for explicit labels and precise mathematical function modeling.Empirical evaluations of LLMs in multiple representative benchmarks demonstrate that CalibraEval effectively mitigates selection bias and improves performance compared to existing debiasing methods.This work marks a step toward building more robust and unbiased automated evaluation frameworks, paving the way for improved reliability in AI-driven assessments 1 .</p>
<p>INTRODUCTION</p>
<p>In recent years, large language models (LLMs) have attracted widespread attention in both academia and industry [16,23,35].These models achieve significant performance in a wide range of tasks, sometimes even exceeding human capabilities [8].However, evaluating the quality of the texts generated by LLMs is difficult, particularly in subjective tasks such as open-ended story creation and summarization.Traditional n-gram metrics (like BLEU [24] and ROUGE [21]) and semantic-based metrics (such as BERTScore [37] and BARTScore [33]) are insufficient to comprehensively reflect the capabilities of LLMs.Human evaluation, often regarded as the "gold standard", can measure model performance most accurately and provide valuable feedback, but it is costly and time-consuming.Therefore, the demand for effective automated evaluation methods is growing increasingly [27].</p>
<p>Some powerful commercial LLMs, such as GPT-4, have been widely applied to evaluate the quality of texts generated in response to open-ended questions.This paradigm, known as "LLMsas-Judges", provides a scalable and transparent alternative to human evaluation of text quality.Within this paradigm, two common methods are pointwise and pairwise evaluations.In pointwise evaluation, LLMs assign scores to individual responses based on specific criteria, while in pairwise comparison, LLMs select the better response between two options.Pointwise evaluation tends to be unstable and susceptible to noise, as subtle differences in wording or interpretation may lead to inconsistent results.In contrast, pairwise Which response is Better?A: In response toâ€¦</p>
<p>[1] B: A relevant point... [2] Which response is Better?B: In response toâ€¦</p>
<p>[1] A: A relevant point... [2] A: In response toâ€¦ [1] A: A relevant pointâ€¦ [2] Token A &gt; Token B comparison can better reflect human judgment [22,40], resulting in its widespread application and considerable attention.</p>
<p>Despite the success, LLMs are not perfect evaluators and are believed to exhibit certain biases [39,40].As shown in Figure 1, when applied to pairwise comparisons of candidate responses, simply changing the positions or the ID tokens may lead to inconsistent evaluation results.Previous studies have classified these biases as position bias [27,40] and token bias [25,26].Positional bias refers to the tendency of LLMs to favor answers based on their specific positions (e.g., first or last), and token bias indicates that LLMs may assign more probability to certain option ID tokens (e.g., A or B).Given the inherent link between option tokens and their positions, we collectively refer to them as selection bias in this paper.</p>
<p>Addressing selection bias in "LLMs-as-Judges" is crucial for ensuring valid and fair evaluations.However, this task is not trivial, as selection bias is influenced by task-specific characteristics, such as domain and difficulty, as well as the inherent properties of LLMs, such as context window, family characteristics, and model capabilities [27,32].A straightforward method is to exclude inconsistent judgments or consider them "ties" [4,40].While this approach enhances consistency and reliability, it may lead to a loss of evaluative information.Furthermore, more advanced methods, such as split and merge [20] or discussions [3,19] among multiple agents, have been proposed to improve evaluation effectiveness.However, these approaches typically require multiple rounds of interaction, making them costly and time-consuming, and their effectiveness in mitigating selection bias remains uncertain.</p>
<p>To address these limitations, we propose CalibraEval, a label-free, inference-time method for mitigating selection bias.CalibraEval reformulates the debiasing problem as an optimization task to build a projection function that maps the original prediction distribution to an unbiased distribution.Our optimization objective is based on consistency judgments obtained after swapping option positions and ID tokens.Moreover, we propose a non-parametric order-preserving algorithm (NOA).The NOA narrows the solution space by preserving the partial order relationship between predicted distributions of observed samples.It derives the optimal calibration function by exploiting the relationship between the prediction distributions from different combinations of options.This approach effectively minimizes the reliance on explicit labels and precise mathematical function modeling, enhancing scalability and transferability.</p>
<p>We conduct extensive experiments on representative evaluation benchmarks with various LLMs.The experimental results indicate that CalibraEval outperforms strong baselines in debiasing performance and achieves state-of-the-art results.Furthermore, we validate CalibraEval's robustness across diverse prompt templates, varied option tokens, and in-context learning scenarios, demonstrating its potential for application in a variety of contexts.To summarize, we make the following contributions: Experimental results demonstrate the effectiveness and robustness of CalibraEval.</p>
<p>RELATED WORK 2.1 LLMs as Judges</p>
<p>The rapid development of large language models (LLMs) in recent years has highlighted the urgent need for effective evaluation methods [5,15,27,30].Traditional evaluation metrics, such as BLEU [24] and ROUGE [21], fall short in comprehensively capturing model performance.These metrics typically overlook nuanced aspects of generated texts, such as coherence, relevance, and contextual appropriateness.Moreover, manual evaluation can provide more accurate assessments and nuanced insights, but it is both costly and time-consuming, making it impractical for large-scale assessments.This situation highlights the urgent need for more advanced and efficient automated evaluation techniques that can keep pace with the evolving capabilities of LLMs [6,17,18,31].</p>
<p>To tackle these challenges, the "LLMs-as-Judges" approach has emerged as a promising alternative [32].This method utilizes powerful, widely recognized LLMs, such as , to facilitate automated evaluation, thereby reducing the dependence on manual assessment.Generally speaking, the "LLMs-as-Judges" evaluation approach can be classified into two categories: pointwise [12] and pairwise [14,43].Pointwise evaluation involves LLM judges scoring individual responses based on specific criteria.Pairwise comparison requires choosing the better answer from two responses.Pairwise comparison evaluation has gained widespread adoption and particular attention due to its outstanding performance.Wang et al. [29] discovered that pairwise comparison methods outperform traditional score-based evaluation approaches in terms of consistency with human assessments.Liu et al. [22] observed that pairwise comparisons better reflect human evaluation standards compared to other methods.This advantage may be attributed to the fact that LLMs often utilize pairwise preference or ranking data during the Reinforcement Learning from Human Feedback (RLHF) training phase [9].</p>
<p>Furthermore, some researchers have explored integrating multiple LLMs into evaluation systems, aiming to produce effective results through collaboration [3], discussion [19], and debate [6] among the models.However, these approaches typically require multiple rounds of interaction, leading to increased resource consumption.In summary, pairwise comparison evaluation is relatively more straightforward and less resource-intensive, which is regarded as a more economical and effective solution.</p>
<p>Bias in LLM Judges</p>
<p>While "LLMs-as-Judges" has emerged as a promising alternative to human evaluation in many tasks, concerns have been raised about the reliability of these judges due to potential biases inherent in LLMs.These biases pose significant challenges to the effectiveness and fairness of such evaluation systems [27,41].</p>
<p>Recent research has identified various biases affecting LLM evaluations, including selection bias [40], position bias [20], contextual bias [42], and self-reinforcing bias [19,36].Among these, selection bias has emerged as a particularly critical issue, as it is prevalent across various tasks and affects both open-source and commercial models, significantly impacting their performance.This bias is typically evident in pairwise comparison evaluations: if an LLM evaluation model consistently favors a specific option even after swapping positions or IDs, this indicates the presence of selection bias.Two types of bias may contribute to selection bias: position bias and token bias.However, there is still no consensus on which of these two is the dominant factor [25,26].</p>
<p>Effectively mitigating bias remains an unresolved issue.Scholars are exploring various approaches to identify and reduce biases in LLMs [27,36,41].Shi et al. [27] conducted a systematic study on positional bias through pairwise comparison evaluations, providing detailed recommendations for selecting judgment LLMs that balance consistency, fairness, and cost-effectiveness.Chua et al. [7] proposed Bias-Consistent Training (BCT) to fine-tune models, aiming to enhance consistent reasoning between prompts with or without biased features.Li et al. [20] introduced the "split and merge" method, which divides answers into multiple parts and aligns similar content in candidate answers to calibrate position bias.Zheng et al. [39] use prior estimates from partial samples to address selection bias.Furthermore, Liu et al. [22] argued that existing calibration techniques aimed at reducing bias are insufficient for calibrating LLM evaluators, even with supervised data.Therefore, mitigating biases in "LLM-as-Judges" is a widespread, significantly impactful, and challenging issue.</p>
<p>CALIBRAEVAL</p>
<p>In this section, we first present the problem statement of the debiasing process and the optimization objective.Then, we provide a detailed introduction to the non-parametric order-preserving algorithm (NOA).</p>
<p>Problem Statement</p>
<p>In this paper, we focus on addressing the selection bias present in "LLMs-as-Judges".Selection bias refers to the phenomenon where LLMs consistently prefer a specific option during pairwise comparisons, regardless of the content.</p>
<p>To standardize the terminology, we define the following terms:</p>
<p>represent the option ID tokens (e.g., A, B), and   denotes the specific option contents (e.g., Response_x, Response_y).Additionally, let  represent the input instruction, and  0 represent the default connection of option ID tokens and contents, that is,
ğ‘‹ 0 = [(ğ‘¡ 1 , ğ‘œ 1 ); (ğ‘¡ 2 , ğ‘œ 2 )].
Following previous studies [39], we assume that when the LLM serves as an evaluator, the observed probability distribution    on   can be decomposed into a combination of the prior distribution   and the debiased distribution   , i.e.,
ğ‘ƒ ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘‘ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 ) = ğ‘“ (ğ‘ƒ ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 ), ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 )) (1)
where  (â€¢) is a function that represents the relationship between    ,   and   .Accurately estimating the form of  (â€¢) is challenging.Firstly, the interaction between   and   is complex and may not be simply multiplicative or additive.Secondly, the observed probability distributions    may be affected by noise, complicating the identification of the precise form of  (â€¢).In previous work, Zheng et al. [39] proposed Pride, which simplify the problem by assuming that  (â€¢) is a linear multiplication, i.e.,
ğ‘ƒ ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘‘ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 ) âˆ ğ‘ âˆ’1 ğ¼,ğ‘‹ 0 ğ‘ƒ ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 )Ã—ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 ) (2) where ğ‘ âˆ’1 ğ¼,ğ‘‹ 0
is the normalization item.Zheng et al. [39] select a subset of test samples and then use the average observed probability distributions from different arrangements as the prior estimates P (  ).The debiasing is then performed using the following equation:
ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 ) âˆ ğ‘ƒ ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘‘ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 )/ Pğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ (ğ‘¡ ğ‘– )(3)
Although Pride is effective, its simplified assumption overlooks the complex relationships between probability distributions, leading to suboptimal performance.</p>
<p>In this paper, considering the complexity of  (â€¢), we do not attempt to directly create a precise mathematical function of  (â€¢).Instead, we focus on determining a calibration function (â€¢), which can map the observed probabilities to an unbiased probability distribution, i.e.,
ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 ) = ğ‘”(ğ‘ƒ ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘‘ (ğ‘¡ ğ‘– |ğ¼, ğ‘‹ 0 ))(4)</p>
<p>Optimization Objective</p>
<p>In this section, we reformulate the debiasing problem as an optimization task, with the unbiased probability distribution serving as the optimization objective.Intuitively, an unbiased evaluator should provide consistent judgments even when the option position or ID tokens are swapped.Specifically, in pairwise comparisons, there are four possible combinations of positions and ID tokens:
ğ‘‹ 0 = [(ğ‘¡ 1 , ğ‘œ 1 ); (ğ‘¡ 2 , ğ‘œ 2 )], ğ‘‹ 1 = <a href="5">(ğ‘¡ 2 , ğ‘œ 2 ); (ğ‘¡ 1 , ğ‘œ 1 )</a>ğ‘‹ 2 = [(ğ‘¡ 1 , ğ‘œ 2 ); (ğ‘¡ 2 , ğ‘œ 1 )], ğ‘‹ 3 = <a href="6">(ğ‘¡ 2 , ğ‘œ 1 ); (ğ‘¡ 1 , ğ‘œ 2 )</a>
In Figure 2, we present the relationship among these four combinations.An unbiased evaluator can accurately determine the correct option context, regardless of changes in option orders (Swap Positions) or option ID tokens (Swap Tokens).Suppose that the ground truth is  1 , the evaluator should satisfy the following conditions:  Since Equations (7) and Equations ( 8) are duals, we only need to select one as the optimization objective.Also, we can simply normalize the original token prediction probabilities, ensuring that the sum of the probabilities for outputs  1 and  2 equals 100%, i.e.,
ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ 1 |ğ¼, ğ‘‹ 0 ) = ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ 1 |ğ¼, ğ‘‹ 1 ) = ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ 2 |ğ¼, ğ‘‹ 2 ) (7) ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ 2 |ğ¼, ğ‘‹ 2 ) = ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ 2 |ğ¼, ğ‘‹ 3 ) = ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ 1 |ğ¼, ğ‘‹ 0 )(8)ğ‘‹ # = [ ğ‘¡ " , ğ‘œ # ; ğ‘¡ # , ğ‘œ " ] Which response is Better? B: Response 1 A: Response 2 ğ‘‹ $ = [ ğ‘¡ # , ğ‘œ " ; ğ‘¡ " , ğ‘œ # ]ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ 1 |ğ¼, ğ‘‹ 0 ) = 1 âˆ’ ğ‘ƒ ğ‘‘ğ‘’ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ (ğ‘¡ 2 |ğ¼, ğ‘‹ 0 )(9)
With the above reasoning, we formulate the debiasing problem on  samples as follows:
min ğ‘”âˆˆ G ğ¾ âˆ‘ï¸ ğ‘–=1 [ğ‘”(ğ‘  ğ‘– 0 ) + ğ‘”(ğ‘  ğ‘– 2 ) âˆ’ 1] 2 + [ğ‘”(ğ‘  ğ‘– 0 ) âˆ’ ğ‘”(ğ‘  ğ‘– 1 )] 2 âˆ’ ğœ†[ğ‘”(ğ‘  ğ‘– 0 ) âˆ’ ğ‘”(ğ‘  ğ‘– 2 )] 2(10)ğ‘ .ğ‘¡ .ğ‘  ğ‘– ğ‘— = ğ‘ƒ ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿ ğ‘£ğ‘’ğ‘‘ (ğ‘¡ 1 |ğ¼, ğ‘‹ ğ‘— ), ğ‘— = 0, 1, 2, ğ‘– = 1, ..., ğ¾ .(11)
where (â€¢) is the mapping function for the probability of token  1 .G denotes the solution space of (â€¢). is a hyper-parameter.For each option ID token, a corresponding mapping function (â€¢) is defined.</p>
<p>In the following process, we use (â€¢) as an example.In Equations (10), the first term ensures consistent judgments when option ID tokens are swapped.The second term aims to maintain consistent judgment when option positions are exchanged.The third term serves as a regularization term, which prevents convergence to the trivial solution (â€¢) = 0.5.</p>
<p>Non-parametric Order-Preserving Algorithm (NOA)</p>
<p>The optimization problem presented in Equation ( 10) is an NP problem, featuring an extensive solution space G. Furthermore, the absence of explicit labels prevents us from employing supervised methods to determine (â€¢).</p>
<p>To address these limitations, we propose a non-parametric orderpreserving algorithm called NOA.Non-parametric methods do not rely on specific model assumptions, making them well-suited for handling high-dimensional data or complex functions.NOA searches for the optimal solution by directly evaluating the output of the calibration function, eliminating the need for explicit labels or precise mathematical modeling.</p>
<p>To narrow the solution space G, we assume that the mapping function (â€¢) is order-preserving for the same ID token.This assumption, widely and implicitly applied in previous work [38,39], rests on the premise that the prior distribution   reflects the LLM's inherent bias toward certain option ID tokens, which remains conditionally independent of the unbiased probability distribution   .Intuitively, for a given LLM, the partial order relationship under the same prior bias should remain consistent, meaning higher observed probabilities generally correspond to higher unbiased probabilities for the same ID token.</p>
<p>Specifically, we first collect an estimation set with  samples.Each sample is processed by swapping ID tokens and swapping positions, resulting in three probabilities  0 (default output),  1 (swap positions), and  2 (swap ID tokens).The probabilities from all samples are combined into a set  = {  0 ,   1 ,   2 | âˆˆ 1, ...,  }.Then, we sort  in ascending order to form a sequence  1 â‰¤  2 â‰¤ ... â‰¤   âˆ’1 , where  = 3 + 1.We then append boundary conditions to the sorted sequence by defining  0 = 0 and   = 1, producing the complete sequence  = { 0 ,  1 , ...,   âˆ’1 ,   }.</p>
<p>To optimize the model, we introduce a set of parameters   ( = 0, 1, 2, ..., ) initialized to the values of   .These parameters will be optimized during the process.Then, we define the mapping function (â€¢) using the softmax-like expression:
ğ‘”(ğ‘§ ğ‘˜ ) = ğ‘˜ ğ‘–=0 ğ‘’ğ‘¥ğ‘ (ğ‘‘ ğ‘– ) ğ‘€ ğ‘–=0 ğ‘’ğ‘¥ğ‘ (ğ‘‘ ğ‘– )(12)
(â€¢) is a discrete mapping function with parameters   , which satisfies the constraint of order preservation.We employ gradient descent methods to iteratively update the parameters   .The update rule is given by:
ğ‘‘ (ğ‘›ğ‘’ğ‘¤ ) ğ‘˜ = ğ‘‘ (ğ‘œğ‘™ğ‘‘ ) ğ‘˜ âˆ’ ğ›¾ ğœ•ğ¿ ğœ•ğ‘‘ ğ‘˜ (13)
where  is the learning rate,
ğ¿ = [ğ‘”(ğ‘  ğ‘– 0 ) + ğ‘”(ğ‘  ğ‘– 2 ) âˆ’ 1] 2 + [ğ‘”(ğ‘  ğ‘– 0 ) âˆ’ ğ‘”(ğ‘  ğ‘– 1 )] 2 âˆ’ ğœ†[ğ‘”(ğ‘  ğ‘– 0 ) âˆ’ ğ‘”(ğ‘  ğ‘– 2 )] 2
. This iterative process allows the parameters to converge toward the optimal values that minimize the loss, thereby reducing the bias in the probability distribution.</p>
<p>For    , we derive the following equation.The detailed derivation process can be found in Appendix A.
ğœ•ğ¿ ğœ•ğ‘‘ ğ‘˜ = 2 ğ‘” ğ‘  ğ‘– 0 + ğ‘” ğ‘  ğ‘– 2 âˆ’ 1 + 2 ğ‘” ğ‘  ğ‘– 0 âˆ’ ğ‘” ğ‘  ğ‘– 1 ğœ•ğ‘” ğ‘  ğ‘– 0 ğœ•ğ‘‘ ğ‘˜ + âˆ’2 ğ‘” ğ‘  ğ‘– 0 âˆ’ ğ‘” ğ‘  ğ‘– 1 ğœ•ğ‘” ğ‘  ğ‘– 1 ğœ•ğ‘‘ ğ‘˜ + 2 ğ‘” ğ‘  ğ‘– 0 + ğ‘” ğ‘  ğ‘– 2 âˆ’ 1 âˆ’ 2ğœ† ğ‘” ğ‘  ğ‘– 0 âˆ’ ğ‘” ğ‘  ğ‘– 2 ğœ•ğ‘” ğ‘  ğ‘– 2 ğœ•ğ‘‘ ğ‘˜ (14) ğœ•ğ‘” ğ‘§ ğ‘— ğœ•ğ‘‘ ğ‘˜ = ï£± ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£³ âˆ’ ğ‘— ğ‘–=0 exp(ğ‘‘ ğ‘– ) exp(ğ‘‘ ğ‘˜ ) ğ‘€ ğ‘–=0 exp(ğ‘‘ ğ‘– ) 2 ( ğ‘— &lt; ğ‘˜) exp(ğ‘‘ ğ‘˜ ) ğ‘€ ğ‘–=0 exp(ğ‘‘ ğ‘– ) âˆ’ ğ‘— ğ‘–=0 exp(ğ‘‘ ğ‘– ) ğ‘€ ğ‘–=0 exp(ğ‘‘ ğ‘– ) 2 ( ğ‘— â‰¥ ğ‘˜)(15)
We note that there are infinite solutions that satisfy the optimization problem.This is because any constant change in the value of   does not affect the relative values in the exponential terms of Equation (12).To obtain a unique solution, we apply the normalization constraint  =0   = 0 after each iteration.The optimization proceeds until a convergence criterion is met, such as the loss function  reaching a minimum threshold or the parameter updates becoming sufficiently small.</p>
<p>After the solution process converges, we obtain the sample points  = { 1 , ...,   âˆ’1 } and their corresponding calibrated values  = {( 1 ), ..., (  âˆ’1 }.For sample points not included in  , we use existing sample points to learn the continuous calibration function  * (â€¢).The goal is to identify a set of non-decreasing piecewise linear functions that minimize the sum of squared deviations between the estimated values and the calibrated values of the samples.Specifically, we fit the calibration values by minimizing the following objective function:
ğ‘šğ‘–ğ‘› ğ‘€ âˆ’1 âˆ‘ï¸ ğ‘–=1 ğ‘¤ ğ‘– (ğ‘”(ğ‘§ ğ‘– ) âˆ’ ğ‘” * (ğ‘§ ğ‘– )) 2(16)ğ‘ .ğ‘¡ .ğ‘§ 1 â‰¤ ğ‘§ 2 ... â‰¤ ğ‘§ ğ‘€ âˆ’1(17)
 âˆ’1
âˆ‘ï¸ ğ‘–=1 ğ‘¤ ğ‘– = 1, ğ‘¤ ğ‘– â‰¥ 0(18)
The above problem is a weighted least squares quadratic programming problem.We apply the Pool Adjacent Violators Algorithm (PAVA) [34] to derive the continuous calibration function  * (â€¢).</p>
<p>It is worth noting that CalibraEval does not require explicit labels and can be executed during inference with minimal computational cost.The calibration function can be calculated after observing all test samples or by utilizing a subset of samples.The entire process of CalibraEval is summarized in Algorithm 1 in the Appendix.</p>
<p>EXPERIMENT SETUP 4.1 Datasets and Metrics</p>
<p>We conduct experiments on three representative benchmarks.The statistics are shown in Appendix B.1.</p>
<p>â€¢ RewardBench [14] is a benchmark dataset designed for evaluating reward models.It contains 2,985 prompt-choice-rejection trios across four task categories: Chat, Chat Hard, Safety, and Reasoning.</p>
<p>â€¢ MTBench [40] is a multi-turn response dataset.It contains 3,355 expert-level pairwise human preferences for responses, generated by 6 models for 80 MTBench questions.â€¢ PreferenceBench [13] is a test set designed to assess the evaluation capabilities of LLMs, comprising 2,000 response pairs (classified as "win" or "lose") and 200 evaluation criteria.</p>
<p>In the evaluation, we primarily utilize reference-free metrics to measure the consistency of model evaluations.We compute Fleiss's Kappa coefficient [10] and intraclass correlation coefficient (ICC) [2] between the evaluation results obtained after swapping option ID tokens and option positions.We report two specific ICC metrics: ICC(2,k) and ICC (3,k) in this paper.</p>
<p>For the reference-based evaluation, we report the standard deviation of recalls (RStd) and accuracy.Following Zheng et al. [39], the balance of recalls serves as an effective measure of the extent of selection bias.A greater imbalance in recalls signifies a more pronounced selection bias.In addition, MTBench includes "tie" options assessed by human evaluators.We exclude all "tie" options when calculating the reference-based metrics.In Appendix B.2, we provide the details of evaluation metrics.</p>
<p>Baselines</p>
<p>We employ the following methods as our baselines.Since CalibraEval is a label-free method, we do not compare it with supervised methods.</p>
<p>â€¢ Debiasing Instruct (DI) is implemented by including the instruction: "Avoid any position bias and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain tokens of the option.Be as objective as possible".â€¢ Contextual Calibration (CC) [38] involves applying an affine transformation to model outputs in order to calibrate LLM predictions.It estimates the bias for each option tokens by requesting its prediction with a prompt alongside a content-free input, such as "N/A".â€¢ Domain-context Calibration (DC) [11] is designed to minimize label bias in in-context learning.It estimates a contextual prior by using a random in-domain sequence, achieving state-ofthe-art results.â€¢ Pride [39] estimates the model's prior bias toward option ID token by reorganizing the test samples and then removes this bias using a division operation.</p>
<p>Implementation Details</p>
<p>We evaluate six models from three LLM-families including: Llama-3-8B [28], Llama-3.1-8B[28], Qwen-14B [1], Qwen-72B [1], Chat-GPT [23], and GPT-4o [23].The version of ChatGPT used is gpt-3.5-turbo-1106.The estimation set used to derive the calibration function can be constructed either by sampling from the test data or by using the entire test set without the gold labels.For a fair comparison, we opted for the latter approach.In the main experiment, all baselines used the full test data as the prior estimation set.For CC, we use the predefined token "N/A" to replace the option contents, generating content-free input.For DC, we randomly extract words from the task corpus to construct the content-free input.Moreover, we set  = 0.5 and  = 10.We employ the batch gradient descent method with a batch size of 32.The optimization process stops when the parameters change range is less than the threshold  i.e.,  =1 â–³  &lt; .The  is set to 0.001.All experiments presented in this paper are conducted on 8 NVIDIA Tesla A100 GPUs.All the prompts used in this paper can be found in Appendix D.</p>
<p>EXPERIMENT RESULT 5.1 Main Results</p>
<p>To validate the effectiveness of CalibraEval in mitigating selection bias, we test the consistency of evaluation results among different models on benchmarks.The performance comparison of CalibraEval with baselines is presented in Table 1.Based on the experimental results, we can draw the following conclusions.</p>
<p>â€¢ Debiasing Instruct does not consistently lead to improved or more robust performance, as its effectiveness is limited by the instruction-following capabilities of LLMs and the nature of tasks.</p>
<p>In some cases, adding debiasing instructions may even result in consistency degradation.Consequently, relying solely on instructions is not a reliable approach for effective debiasing.â€¢ CC and DC are originally designed to mitigate label bias in incontext learning.Therefore, their estimated priors may not accurately reflect the inherent selection bias in LLMs-as-Judges, leading to suboptimal debiasing performance and difficulties in interpretation.</p>
<p>â€¢ When applied to lower-capability LLMs, such as Llama-3-8B and Qwen-14B, Pride effectively estimates bias and improves consistency.However, its effectiveness diminishes with more advanced models (e.g., GPT4o).This limitation may arise from the simplified probabilistic relationships employed in Pride.â€¢ CalibraEval consistently improves performance across various LLMs and tasks.On average, CalibraEval shows enhancements over all the baselines.Overall, CalibraEval is a versatile technique applicable to multiple evaluation tasks, delivering stable performance improvement.This also indicates that CalibraEval can effectively reduce selection bias in LLMs-as-judges, leading to more consistent and fair evaluation results.</p>
<p>Table 2 presents the performance of the reference-based metrics.Due to space constraints, we only report the experimental results for Llama-3-8B, Qwen-14B, and ChatGPT, while the complete results are available in Appendix C. For a fair comparison, we report the average values of Rstd and Accuracy under the conditions of swapping option positions and option IDs.Across the average performance of the three datasets, CalibraEval consistently achieves lower Rstd and higher accuracy, outperforming other baselines.Surprisingly, although this is not the original intent, CalibraEval frequently improves accuracy.We believe this may indicate that selection bias influences the model's judgments, leading to reduced accuracy.Therefore, effective bias mitigation methods can enhance the model's performance in its evaluative role.Additionally, we found that lower Rstd is often associated with higher accuracy.The more pronounced the debiasing effect, the more significant the performance improvement.For example, on RewardBench, ChatGPT's Rstd decreased from 16.79 to 5.51, while its accuracy increased from 65.27 to 67.13.Overall, CalibraEval not only enhances the reliability of model evaluations but also unlocks the potential for these LLMs to perform optimally in various tasks.</p>
<p>Robustness Analysis</p>
<p>In this section, we conduct additional experiments to further validate the effectiveness of CalibraEval across diverse scenarios.Due to the high cost of GPT-4o, we opt for Qwen-72B and ChatGPT on the RewardBench for the following experiments.Unless otherwise stated, the ICC for subsequent experiments is ICC(2,k).</p>
<p>Different prompt templates.</p>
<p>We conduct experiments on four distinct prompt templates (see Appendix D for details).Figure D  shows performance comparisons on RewardBench.We observed that model outputs without bias correction exhibit low consistency and high variance.While Pride improves consistency, it still exhibited considerable variance.In contrast, CalibreEval demonstrates substantial performance enhancement while maintaining low variance, indicating its consistent effectiveness across different prompt templates.</p>
<p>5.2.2 Different ID tokens.We also conduct experiments using four distinct sets of ID tokens: A/B, a/b, Alice/Bob, and X/Y. Figure 4 illustrates the performance comparison.CalibraEval consistently achieves significant performance improvements with low variance across all tested ID tokens.This highlights its robustness and effectiveness regardless of the specific tokens used.Furthermore, when applied to the highly consistent model Qwen-72B, the improvement of Pride is negligible, while CalibreEval continued to enhance consistency even further.</p>
<p>Ablation Studies</p>
<p>To better illustrate the rationality and effectiveness of model design, we conduct two ablation experiments.We first analyze the effectiveness of well-defined optimization objectives.Specifically, we consider two variants.The first variant focuses solely on ensuring that the model maintains consistent judgments after swap ID tokens, i.e.,
ğ¿ 1 = ğ‘ğ‘Ÿğ‘” min ğ‘”âˆˆğº ğ¾ âˆ‘ï¸ ğ‘–=1 [ğ‘”(ğ‘  ğ‘– 0 ) + ğ‘”(ğ‘  ğ‘– 2 ) âˆ’ 1] 2 âˆ’ ğœ†[ğ‘”(ğ‘  ğ‘– 0 ) âˆ’ ğ‘”(ğ‘  ğ‘– 2 )] 2(19)
The other variant focuses on ensuring that the model maintains consistent judgments after position exchanges, represented as: Since this variant does not involve   2 , the regularization term is modified to [(  0 ) âˆ’ 0.5] 2 to prevent the model from converging to a trivial solution.
ğ¿ 2 = ğ‘ğ‘Ÿğ‘” min ğ‘”âˆˆğº ğ¾ âˆ‘ï¸ ğ‘–=1 [ğ‘”(ğ‘  ğ‘– 0 ) âˆ’ ğ‘”(ğ‘  ğ‘– 1 )] 2 âˆ’ ğœ†[ğ‘”(ğ‘  ğ‘– 0 ) âˆ’ 0.5] 2(20)
Table 3 illustrates the impact of different optimization objectives.Both objectives contribute to the calibration benefits observed.When the model is significantly influenced by position bias, the improvements from  2 are more substantial.Conversely, when token bias is more prevalent,  1 leads to better improvements.The combination of both objectives, which defines our CalibraEval optimization goal, achieves optimal performance.These experiments validate the effectiveness of our chosen optimization settings.</p>
<p>In Figure 6, we further test the impact of the estimation set size on the performance.We randomly sampled a certain proportion of test data to estimate the calibration function, which is then applied to debias the entire test set.We found that increasing the size of the estimation set can better enhance consistency.Additionally, a smaller estimation set can also effectively support CalibraEval in reducing bias.For ChatGPT, using only 10% of the data resulted in improvements of over 85% compared to the full dataset.Overall, even with limited data, CalibraEval can still produce reliable calibration functions.</p>
<p>CONCLUSION</p>
<p>In this paper, we propose CalibraEval to mitigate the selection bias present in LLM-as-judges.We reformulate the debiasing problem as an optimization problem and utilize the characteristics of unbiased evaluators as our optimization objectives.Moreover, we propose the  (3,k).ICC(2,k) measures the consistency of ratings from multiple raters for the same set of subjects under a random effects model, while ICC(3,k) assesses the consistency of ratings from specific and fixed raters for the same subjects under a fixed effects model.Both are useful for measuring the reliability and consistency of ratings.</p>
<p>The ICC(2,k) is calculated using the formula:
ICC(2, ğ‘˜) = ğœ 2 ğµ âˆ’ ğœ 2 ğ‘Š ğœ 2 ğµ + (ğ‘˜ âˆ’ 1)ğœ 2 ğ‘Š(32)
The ICC(3,k) is calculated using the formula:
ICC(3, ğ‘˜) = ğœ 2 ğµ âˆ’ ğœ 2 ğ‘Š ğœ 2 ğµ + ğ‘˜ â€¢ ğœ 2 ğ‘Š (33)
where  2  is the variance between the subjects. 2  is the variance within the subjects. is the number of raters.</p>
<p>B.2.2</p>
<p>Reference-based Metrics.The standard deviation of recalls (RStd) quantifies the variability in recall scores across different evaluations.It is calculated using the formula:
ğ‘…ğ‘†ğ‘¡ğ‘‘ = 1 ğ‘ âˆ’ 1 ğ‘ âˆ‘ï¸ ğ‘–=1 ğ‘… ğ‘– âˆ’ R 2(34)
where   is the recall for the -th evaluation.R is the mean recall across all evaluations. is the total number of evaluations.</p>
<p>Accuracy is a widely used evaluation metric that measures the overall correctness of a model's predictions: Accuracy =   +     +   +   +   (35) where   represents the number of instances that are correctly predicted as positive, while   denotes the number of instances that are correctly predicted as negative.Conversely,   indicates the number of instances that are incorrectly predicted as positive, and   refers to the number of instances that are incorrectly predicted as negative.</p>
<p>C MORE EVALUATION RESULT</p>
<p>In Table 5, we present the complete results of the reference-based experimental metrics.On average, CalibraEval achieved better improvements in Rstd and accuracy.This indicates that CalibraEval effectively reduces selection bias and enables the model to realize its potential.By mitigating selection bias in the evaluation process, CalibraEval contributes to achieving more accurate and reliable results, paving the way for further advancements in model calibration and evaluation methodologies.</p>
<p>D THE DESIGN OF PROMPT</p>
<p>Figure 1 :
1
Figure 1: Illustration of selection bias in LLMs-as-Judges.Selection bias manifests in two aspects: prefers a specific position or prefers a specific token.</p>
<p>( 1 )
1
We propose a label-free, inference-time calibrated method CalibraEval.By learning a lightweight calibration function, CalibraEval effectively mitigates selection bias, demonstrating both significant effectiveness and efficiency.(2)We reformulate the debiasing problem as an optimization task and propose a non-parametric order-preserving algorithm (NOA) to solve it efficiently.(3) We conduct extensive experiments on public benchmarks.</p>
<p>Figure 2 :
2
Figure 2: Four different types of combinations. 1 / 2 represents the option IDs (A/B), while  1 / 2 denotes the corresponding option contents.An unbiased evaluator consistently ranks the responses regardless of changes in option order (Swap Positions) or option ID tokens (Swap Tokens), ensuring fairness and consistency in the results.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Performance comparison across different prompt templates.</p>
<p>Figure 5 :
5
Figure 5: Performance comparison under in-context learning.</p>
<p>Figure 6 :
6
Figure 6: Performance of CalibraEval across different estimate set sizes."Percentage" refers to the proportion of the test set selected for use as the estimation set.</p>
<p>!= [  " ,  " ;  # ,  # ]  " = [  # ,  # ;  " ,  " ]
Which response isWhich response isBetter?Better?A: Response 1B: Response 2B: Response 2SwapA: Response 1PositionsSwapSwapTokensTokensWhich response isSwapBetter?PositionsA: Response 2B: Response 1</p>
<p>Table 1 :
1
Performance comparison between CalibraEval and baselines.We report the Fleiss' Kappa (%) and Intraclass Correlation Coefficient (%) for each dataset and the averages.The row corresponding to the model name represents the default results without applying any debiasing methods.Best performances are marked bold.
92.7793.1371.3590.3391.1682.7794.7894.9077.4792.6393.06DI77.3392.2792.6368.1789.1389.9983.3195.0995.1176.2792.1692.58CC69.2388.7189.9270.6490.2990.7278.0892.6393.2472.6590.5491.29DC66.6187.0387.9064.5987.4688.4574.0391.0192.0168.4188.5089.45Pride78.6492.9993.3071.5090.5491.2783.4494.8995.0077.8692.8193.19CalibraEval82.8095.4795.7571.8895.7096.7185.2597.5697.5779.9896.2496.68ChatGPT20.0862.6770.7537.2573.9076.9264.6287.6387.8140.6574.7378.49DI24.5270.2371.5824.3366.8067.6956.0082.9084.8934.9573.3174.72CC24.2857.2558.2323.7164.0671.9161.6386.1886.3836.5469.1672.17DC27.9466.0970.5416.6858.3770.2655.3381.9282.5833.3268.7974.46Pride28.2570.3873.1639.0276.6177.6164.6487.5687.8243.9778.1879.53CalibraEval32.0277.2577.6039.7179.0779.8565.5287.7787.9245.7581.3681.79GPT4o82.5794.8394.8972.4292.9993.2079.4293.5094.1178.1493.7794.07DI78.5793.7294.0174.2193.3593.5079.9794.4894.9377.5893.8594.15CC81.3894.4794.4867.1090.3090.7877.5692.7693.4375.3592.5192.90DC76.8992.2892.3568.9490.9491.6470.4889.3090.0272.1090.8491.34Pride82.5394.9494.9870.3492.3592.7479.7493.6294.2077.5493.6493.97CalibraEval83.2596.2596.2772.6095.0495.2079.7397.2997.6078.5396.1996.36</p>
<p>Table 2 :
2
Results of reference-based metrics.We report the Standard Deviation of Recalls (RStd) and Accuracy (Acc.), with the best results highlighted in bold.â†“ indicates that lower values correspond to better performance.
ModelRewardBench Rstd Acc.(%) Rstd Acc.(%) Rstd MTbench Preference Bench Acc.(%)Llama-3-8B 15.0165.7916.4267.083.3683.43Pride7.5166.5411.6470.634.3583.24CalibraEval 6.4868.125.2270.633.4283.98Qwen-14B 11.6363.1417.2465.6111.9980.68Pride4.1864.0916.3165.297.3683.55CalibraEval 2.7264.256.2668.645.1283.88ChatGPT16.7965.277.6672.673.0485.61Pride8.5466.366.0172.863.5185.68CalibraEval 5.5167.135.2072.982.8285.98</p>
<p>Table 3 :
3
Ablation study on RewardBench.Best results are marked bold.Experiments involving six LLMs across three representative datasets demonstrate that CalibraEval effectively reduces selection bias while enhancing accuracy.We argue that mitigating selection bias is essential for developing more reliable LLM evaluators.In the future, we plan to investigate additional biases in LLM-as-judges applications to create even more robust and trustworthy automated evaluations of large models.
ModelKappa ICC(2,k) ICC(3,k) Rstdâ†“ Acc.ChatGPT20.0862.6770.7516.79 65.27w. ğ¿ 126.7872.0473.6610.32 65.33w. ğ¿ 227.3872.7375.998.6466.04w. both32.0277.2577.605.50 67.13Qwen-72B 78.2892.7793.134.0187.20w. ğ¿ 181.9594.9495.042.4287.78w. ğ¿ 281.3293.7795.522.7887.74w. both82.8095.4795.750.94 88.06non-parametric order-preserving algorithm (NOA) to determine thecalibration function.</p>
<p>Table 4 :
4
Statistics of benchmark datasets.Intraclass Correlation Coefficient (ICC) is a measure of reliability that assesses the consistency or agreement of measurements made by different raters or instruments.In this paper, we report two specific Intraclass Correlation Coefficient (ICC) metrics: ICC(2,k) and ICC
DatasetsTotal_numAvergae_Length prompt answer_a answer_b first second tie Label_numTypeType_numChat358RewardBench29851771667658149014950Chat_Hard Safety456 739Reasoning1432MTBench335540391524151212931282780Turn1 Turn21689 1666PreferenceBench1998248588689398010180--</p>
<p>Table 6
6
presents all the prompts used in this paper.The default prompt, employed in the main experiments, serves as the foundational basis for assessing model performance.In the robustness experiments, four distinct prompts are utilized to evaluate variations in model responses.</p>
<p>Conference'17, July 2017, Washington, DC, USA Haitao Li, Junjie Chen, Qingyao Ai, Zhumin Chu, Yujia Zhou, Qian Dong, and Yiqun Liu
A SUPPLEMENTARY PROOFFor (  0 ):For (  1 ):For (  2 ):Then, We substitute the derivatives back into the equation 22:Next, using the quotient rule, the derivative of (  ) with respect to   is:For  &lt; ,   affects the denominator:For  &gt;= ,   affects both the numerator and the denominator:The final formula is as follows.ğœ•ğ‘” ğ‘§
Model RewardBench MTBench PreferenceBench Average Kappa ICC(2,k) ICC(3,k) Kappa ICC(2,k) ICC(3,k) Kappa ICC(2,k) ICC(3,k) Kappa ICC(2,k) ICC(3,k) Llama-3-8B. </p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023. 2023arXiv preprint</p>
<p>The intraclass correlation coefficient as a measure of reliability. J John, Bartko, Psychological reports. 191966. 1966</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023. 2023arXiv preprint</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang, arXiv:2402.10669Humans or llms as the judge? a study on judgement biases. 2024. 2024arXiv preprint</p>
<p>An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation. Junjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, Shaoping Ma, arXiv:2410.12265[cs.CL2024</p>
<p>PRE: A Peer Review Based Large Language Model Evaluator. Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, Yiqun Liu, arXiv:2401.15641[cs.IR2024</p>
<p>James Chua, Edward Rees, Hunar Batra, Julian Samuel R Bowman, Ethan Michael, Miles Perez, Turpin, arXiv:2403.05518Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought. 2024. 2024arXiv preprint</p>
<p>Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback. Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma, 10.1145/3626772.3657689Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information RetrievalWashington DC, USA; New York, NY, USAAssociation for Computing Machinery2024SIGIR '24)</p>
<p>Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback. Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma, arXiv:2309.170782024cs.IR</p>
<p>Fleiss' kappa statistic without paradoxes. Rosa Falotico, Piero Quatto, Quality &amp; Quantity. 492015. 2015</p>
<p>Yu Fei, Yifan Hou, Zeming Chen, Antoine Bosselut, arXiv:2305.19148Mitigating label biases for in-context learning. 2023. 2023arXiv preprint</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024. 2024arXiv preprint</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Choi, arXiv:2403.13787Rewardbench: Evaluating reward models for language modeling. 2024. 2024arXiv preprint</p>
<p>SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval. Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Yueyue Wu, Yiqun Liu, Chong Chen, Qi Tian, 10.1145/3539618.3591761Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information RetrievalTaipei, Taiwan; New York, NY, USAAssociation for Computing Machinery2023SIGIR '23)</p>
<p>BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models. Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, Qi Tian, arXiv:2403.18365[cs.CL2024</p>
<p>Haitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, Yiqun Liu, arXiv:2409.20288[cs.CLLexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models. 2024</p>
<p>LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset. Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, Yiqun Liu, 10.1145/3626772.3657887Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information RetrievalWashington DC, USA; New York, NY, USAAssociation for Computing Machinery2024SIGIR '24)</p>
<p>Ruosen Li, arXiv:2307.02762Teerth Patel, and Xinya Du. 2023. Prd: Peer rank and discussion improve large language model based evaluations. 2023arXiv preprint</p>
<p>Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, arXiv:2310.01432Split and merge: Aligning position biases in large language model based evaluators. 2023. 2023arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, Nigel Collier, arXiv:2403.169502024. 2024arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Large language models sensitivity to the order of options in multiple-choice questions. Pouya Pezeshkpour, Estevam Hruschka, arXiv:2308.114832023. 2023arXiv preprint</p>
<p>Adian Vyas Raina, Mark Liusie, Gales, arXiv:2402.14016Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment. 2024. 2024arXiv preprint</p>
<p>Lin Shi, Weicheng Ma, Soroush Vosoughi, arXiv:2406.07791Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs. 2024. 2024arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.17926[cs.CLLarge Language Models are not Fair Evaluators. 2023</p>
<p>Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models. Sheng-Lun Wei, Cheng-Kuang Wu, Hen-Hsen Huang, Hsin-Hsi Chen, arXiv:2406.030092024. 2024arXiv preprint</p>
<p>T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking. Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, Jin Ma, 10.1145/3539618.3591874Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information RetrievalTaipei, Taiwan; New York, NY, USAAssociation for Computing Machinery2023SIGIR '23)</p>
<p>Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, arXiv:2410.02736Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge. 2024. 2024arXiv preprint</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 342021. 2021</p>
<p>Pool adjacent violators algorithm. Grzegorz Zadora, Agnieszka Martyna, Daniel Ramos, Colin Aitken, 2014. 2014</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022. 2022arXiv preprint</p>
<p>Ruizhe Zhang, Haitao Li, Yueyue Wu, Qingyao Ai, Yiqun Liu, Min Zhang, Shaoping Ma, arXiv:2403.11152[cs.CLEvaluation Ethics of LLMs in Legal Domain. 2024</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019. 2019arXiv preprint</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International conference on machine learning. PMLR2021</p>
<p>Large language models are not robust multiple choice selectors. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Hongli Zhou, Hui Huang, Yunfei Long, Bing Xu, Conghui Zhu, Hailong Cao, Muyun Yang, Tiejun Zhao, arXiv:2409.16788Mitigating the Bias of Large Language Model Evaluation. 2024. 2024arXiv preprint</p>
<p>Batch calibration: Rethinking calibration for in-context learning and prompt engineering. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy, arXiv:2309.172492023. 2023arXiv preprint</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, arXiv:2310.176312023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
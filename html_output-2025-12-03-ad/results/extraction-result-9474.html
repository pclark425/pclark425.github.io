<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9474 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9474</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9474</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-269005868</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2024.nlp4science-1.10.pdf" target="_blank">Hypothesis Generation with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle Long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform supervised learning by 12.1% and 11.6% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9474",
    "paper_id": "paper-269005868",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005068,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Hypothesis Generation with Large Language Models</p>
<p>Yangqiaoyu Zhou 
Department of Computer Science
University of Chicago ♣
Toyota Technological Institute at Chicago † Chicago
60637ILUSA</p>
<p>Haokun Liu 
Department of Computer Science
University of Chicago ♣
Toyota Technological Institute at Chicago † Chicago
60637ILUSA</p>
<p>Tejes Srivastava 
Department of Computer Science
University of Chicago ♣
Toyota Technological Institute at Chicago † Chicago
60637ILUSA</p>
<p>Hongyuan Mei hongyuan@ttic.edu 
Department of Computer Science
University of Chicago ♣
Toyota Technological Institute at Chicago † Chicago
60637ILUSA</p>
<p>Chenhao Tan chenhao@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Toyota Technological Institute at Chicago † Chicago
60637ILUSA</p>
<p>Hypothesis Generation with Large Language Models
D58319A870808DF5B8A15A8F37BC5DF6
Effective generation of novel hypotheses is instrumental to scientific progress.So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment).In this paper, we examine the potential of large language models (LLMs) to generate hypotheses.We focus on hypothesis generation based on data (i.e., labeled examples).To enable LLMs to handle long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses.Inspired by multi-armed bandits, we design a reward function to inform the exploitationexploration tradeoff in the update process.Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets.We also outperform supervised learning by 12.1% and 11.6% on two challenging real-world datasets.Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.</p>
<p>Introduction</p>
<p>Hypothesis generation drives scientific progress.Mendel's hypothesis on allele pairs lays the foundation for modern genetics; Einstein's hypothesis in general theory of relativity led to the prediction and subsequent confirmation of gravitational waves.In the context of language modeling, the hypothesis on scaling law inspires recent progress in large language models (LLMs) (Kaplan et al., 2020).Despite the importance of hypothesis generation, as Ludwig and Mullainathan (2024) point out, science has been curiously asymmetric.While many scientific publications present extensive formal and empirical evaluation of hypotheses, the generation of hypotheses happens off-stage by researchers.In order to generate novel hypotheses, researchers may read literature, analyze data, pick the brain of each other, and even "hallucinate" (see Kekulé's discovery of the structure of the benzene molecule (Rothenberg, 1995)).</p>
<p>Given the rise of large language models (Brown et al., 2020;Anthropic, 2023;OpenAI, 2023b), we examine their potential of providing much needed assistance in hypothesis generation in this work.</p>
<p>In particular, we focus on hypothesis generation based on data, a common approach in empirical sciences.Our main question is how we can enable LLMs to generate hypotheses of high-quality.While one can easily prompt LLMs to generate hypotheses, LLMs may not be able to effectively leverage the input examples in a single long prompt.Moreover, it is important to have measures of quality in the generation process so that we can filter bad hypotheses and come up with better ones.These two observations motivate us to start with a setup analogous to supervised learning.We can iteratively prompt an LLM to generate hypotheses based on the training examples and use training accuracy as a measure of quality to guide the generation process.Conveniently, we can also evaluate the quality of the final generated hypotheses with their performance on held-out examples, similar to supervised learning.</p>
<p>To generate high-quality hypotheses with LLMs, we propose an algorithm inspired by the upper confidence bound algorithm in multi-armed bandits (Auer, 2002) (HypoGeniC1 , Hypothesis Generation in Context; see Figure 1).Given initial hypotheses generated from a small number of examples, we need to assess their quality and propose new hypotheses to address their deficiencies.To navigate this exploration-exploitation tradeoff, we introduce a reward function and evaluate the top k hypotheses for each training example.We maintain a wrong example bank to capture the gap in knowledge of the hypotheses pool, and generate new hypotheses based on the wrong example bank to close the gap.</p>
<p>The generated hypotheses naturally enable an interpretable hypothesis-based classifier.We propose a suite of inference strategies given a set of hypotheses.We apply our method to one synthetic task where there is a single known valid hypothesis and three real-world tasks (DECEPTIVE REVIEWS, HEADLINE POPULAR-ITY, and TWEET POPULARITY).The real-world tasks focus on deception detection and message popularity prediction, which are known to be challenging even for humans (Ott et al., 2011;Salganik et al., 2006).Our al-New hypothesis: Tweets with named entities like people, places, or organizations tend to get more retweets by being more specific.</p>
<p>Reward=0.83</p>
<p>Tweet 1: How do preseason stars fare during the regular season?[link] Some scoring stats there.</p>
<p>Tweet 2: In the last decade, Melo and Kobe are the only two to lead the NBA in scoring in both preseason and regular season.</p>
<p>[link] Top k</p>
<p>A training example</p>
<p>Hypothesis bank</p>
<p>Reward Update</p>
<p>Hypothesis Generation</p>
<p>Wrong example bank</p>
<p>Hypothesis-based Inference</p>
<p>if prediction is wrong</p>
<p>Hypothesis 1: Tweets with imperative commands like \"RT this\" are more likely to be retweeted as they directly ask people to retweet.Reward=0.71</p>
<p>Hypothesis 2: Tweets with more information, context, or detailed explanations tend to get more retweets than very short or cryptic tweets.Reward=0.66</p>
<p>Hypothesis 3: Tweets that create a sense of urgency or excitement by using words like \"now\", \"today\", \"tonight\", \"fastest\", \"seriously\", \"excited\" are more likely to be retweeted.Reward=0.64</p>
<p>Hypothesis 4: Tweets that include a link are less likely to be retweeted.Reward=0.32</p>
<p>. . .gorithm can recover the hypothesis in the synthetic task and also provide useful hypotheses for the real-world tasks.In fact, our generated hypotheses consistently outperform few-shot in-context learning baselines across all four tasks (31.7% in SHOE SALES, 13.9% in DECEP-TIVE REVIEWS, 3.3% in HEADLINE POPULARITY, and 24.9% in TWEET POPULARITY).The predictive performance matches and even outperforms oracle supervised learning with RoBERTa and Llama-2-7B except in DE-CEPTIVE REVIEWS.</p>
<p>It is important to emphasize that although the utility of hypotheses in assisting downstream classification serves as an indicator for LLMs' ability to generate hypotheses, our goal is not to maximize the classification performance.Rather, our primary interest lies in the quality of the hypotheses.Thus, it is critical for the hypotheses to be interpretable beyond the LLM used to produce the hypotheses.We show that hypotheses generated by one LLM (e.g.,  can be used to make accurate inference by another LLM (e.g., Mixtral).On an out-of-distribution dataset for DECEPTIVE RE-VIEWS, we can even outperform the oracle fine-tuned RoBERTa.Such cross generalization provides strong evidence that we are able to generate hypotheses of high quality.Furthermore, through a qualitative analysis, our generated hypotheses not only confirm theories from existing literature but also provide new insights about the task.For instance, one novel hypothesis is that "reviews that mention personal experiences or special occasions, such as birthdays, anniversaries, or weddings, are more likely to be truthful".We encourage future research on deception detection to explore these novel hypotheses.</p>
<p>To summarize, we make the following contributions:</p>
<p>• We propose a novel computational framework for generating and evaluating hypotheses with LLMs.</p>
<p>• Our generated hypotheses enable interpretable hypothesis-based classifiers that outperform incontext learning and even supervised learning for one synthetic and three real-world datasets.These hypotheses are also robust across different LLMs and out-of-distribution datasets.</p>
<p>• Our generated hypotheses corroborate existing findings while also providing new insights for the tasks.</p>
<p>Method</p>
<p>We begin with a description of the problem formulation.Given a set S = {(x 1 , y 1 ), . . ., (x n , y n )} where x i is an example and y i is the corresponding label, the goal is to learn a set of hypotheses H = {h 1 , ..., h m } that describe theories of relationships between x and y.To this end, we prompt an LLM to summarize demonstration examples into high-level hypotheses ( § 2.1).Then, during inference, the LLM makes inference based on the generated hypothesis ( § 2.2).</p>
<p>Hypothesis Generation</p>
<p>Our hypothesis generation algorithm (Algorithm 1) is inspired by the upper confidence bound (UCB) algorithm (Auer, 2002).Given a set of initial examples S init ⊂ S, we first prompt an LLM to generate hypotheses for S init , which serve as our initial hypothesis bank H.While initialized hypotheses may explain some portions of data, they often fall short of encompassing the full scope of the examples.We thus introduce an update stage which serves a dual purpose: 1) it increases the percentage of data explainable by the hypotheses and 2) it replaces any hypotheses that are found to be inaccurate.</p>
<p>In the update stage, for a training example s, we select the top k high-reward hypotheses from the hypothesis bank H.The LLM is prompted to make a prediction with each of the top k high-reward hypotheses on s.Then we compute the accuracy of the inference and accordingly update the reward for each of the hypotheses.If w hyp hypotheses predict incorrectly for the example s, then s is added to a wrong example pool W. Once the wrong example pool reaches a max size of w max , the wrong examples in W are used to generate new hypotheses.The wrong example pool represents the gap in knowledge that the current pool of hypotheses has for the dataset.Thus, by generating new hypotheses, the algorithm fills in these gaps.We update H with the newly generated hypotheses as per the rewards.Reward.As mentioned above, each hypothesis has an associated reward.In our algorithm, we use the reward function in the UCB algorithm due to similarities between the multi-arm bandit problem and our problem formulation.In particular, we consider each hypothesis to be an arm and each training example to be a "pull".We note, however, that unlike the multi-arm bandit problem, multiple hypotheses are tested for a singular train example.Moreover, there can be new arms after hypotheses are updated, altering the setting from the standard static arms scenario to a dynamic arms scenario.Formally, the reward is defined as
r i = (xj ,yj )∈Si I(y j = ŷj ) |S i | + α log t |S i | ,(1)
where S i is the set of examples that have been used to evaluate the hypothesis h i , t is train time step, and α is a hyperparameter that controls the exploration term.The first term in the reward function denotes the accuracy of the hypothesis for all S i .The second term is the exploration term, which is computed based on the number of times the hypothesis has been selected and the number of training examples visited so far.The accuracy term urges the algorithm to use well-performing hypotheses, whereas the exploration term encourages the algorithm to explore hypotheses that have not been selected many times.Thus, the reward function strikes a balance between exploration and exploitation.</p>
<p>For more details on implementation of HypoGeniC, refer to Appendix B.1.</p>
<p>Hypothesis-based Inference</p>
<p>For efficiency purposes, we use each hypothesis on its own without accounting for their combinatorial effect during training; however, we should leverage the set of hypotheses as a whole during inference for at least two reasons.Firstly, some hypotheses may only apply to a subset of examples.Second, competing theories may require head-to-head comparisons.Hence, we develop multiple inference strategies to account for these different styles of reasoning (see Appendix A for prompts and Appendix B.2 for implementation details).</p>
<p>• Best-accuracy hypothesis.The hypothesis h with the highest accuracy from the hypothesis bank is included in the prompt to guide the model to perform inference.</p>
<p>• Filter and weighted vote.One hypothesis may not be enough to explain the data.Thus, this approach uses a combination of relevant hypotheses to make predictions for a single example.We first filter hypotheses by prompting an LLM to judge which hypotheses are relevant to the example.</p>
<p>Next, an LLM is prompted to generate predictions for each of the relevant hypotheses, and these predictions are aggregated with weighted vote, where the weight is the training accuracy of the corresponding hypothesis.</p>
<p>• Single-step adaptive inference.Similar to filter and weighted vote, this approach leverages contextual information to choose hypotheses.The difference, however, is that it selects the most applicable hypothesis for each test example.Specifically, for a given test example, the LLM is tasked with identifying the most applicable hypothesis from a set of options.For each hypothesis, we provide instances from the training set where the hypothesis was accurate.Then, the LLM selects the most relevant hypothesis by comparing the test example to these training examples and evaluating their similarity.Thereafter, we apply the hypothesis to the test ex-ample to perform inference.Please note that this is all done in one step with a long prompt.</p>
<p>• Two-step adaptive inference.We divide the previous inference strategy into two steps: 1.The LLM determines the most relevant set of examples by comparing the test example with the corresponding examples of the hypotheses.2.Then, the corresponding hypothesis is provided to the LLM, which it uses to perform inference on the test example in a second prompt.</p>
<p>Experiment Setup</p>
<p>We introduce the experiment setup to evaluate Hy-poGeniC.</p>
<p>Tasks and Datasets</p>
<p>The choice of appropriate tasks is critical for evaluating the ability of LLMs to generate hypothesis.The focus of our work is on generating hypotheses based on observed data.A prerequisite is that potential hypotheses do exist.</p>
<p>In the context of classification, it implies that the classification performance is non-trivial.In addition, we need to ensure that the hypotheses describing the data are likely not a priori known by LLMs, which rules out standard tasks such as sentiment analysis.Therefore, we use four datasets that satisfy these requirements: a synthetic task with a known true hypothesis and three real-world datasets that exhibit complex underlying patterns and constitute widely studied social science problems.SHOE SALES is a synthetic task we created to investigate the scenario where there is only one single valid hypothesis.The task is to predict the color of the shoe that the customer will buy based on their appearance.The input provides appearance features, namely, age, height, gender, color of the hat, color of the shirt, color of the bag, and size of the bag.We construct this dataset such that the color of the shoe must match the color of the shirt.Since there are six colors in total, this becomes a 6-class classification problem.</p>
<p>Deceptive review detection is an instance of deception detection, a widely studied phenomenon in psychology and other social sciences (Granhag and Vrij, 2005).This particular task (DECEPTIVE REVIEWS) requires distinguishing genuine reviews from fictitious ones (Ott et al., 2011), where human performance is about chance (Lai and Tan, 2019).The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.</p>
<p>Predicting popularity is a notoriously challenging task in social sciences because it is known to be affected by seemingly random factors (Salganik et al., 2006).We use two datasets in this work: HEADLINE POPULARITY and TWEET POPULARITY.HEADLINE POPULARITY is derived from a dataset in the Upworthy Research Archive (Matias et al., 2021).The original dataset was collected through A/B testing, where each user was shown pairs of a headline and image for multiple packages (articles).Each user was exposed to only one of these pairs per package, and the clicks were recorded for each pair per package.2This process resulted in a total of 150,816 headlines across 22,666 packages.We construct a binary classification dataset by choosing the headlines that received the most clicks and least clicks for each package.We remove all sets of duplicate headlines, which results in our version of the HEADLINE POPULARITY dataset.The task for this dataset is to deduce which headline had more clicks in a pair.TWEET POPULARITY uses a dataset of 13,174 tweet pairs (Tan et al., 2014), which are matched by the topic and the author.Similar to HEADLINE POPULARITY, the task is to predict which one received more retweets.</p>
<p>Baselines, Oracles, and Evaluation Metrics</p>
<p>We use three different LLMs in our experiments (Mixtral (Mistral, 2023), GPT-3.5-turbo (OpenAI, 2023a), and Claude-2.1 (Anthropic, 2023)).We compare our approach with the following methods.</p>
<ol>
<li>
<p>Zero-shot and few-shot prompting.We provide LLMs with task-specific instructions (zero-shot), optionally accompanied by three demonstration examples (few-shot).</p>
</li>
<li>
<p>No updates.To assess the value of the update stage in our algorithm, we evaluate the performance of the initialized hypotheses.In particular, we pick the best-performing hypothesis on the training set and use it for inference on the test set.</p>
</li>
<li>
<p>Supervised Learning.We fine-tune RoBERTa (Liu et al., 2019) and Llama-2-7B (Touvron et al., 2023)</p>
</li>
</ol>
<p>Results</p>
<p>To demonstrate the effectiveness of our hypothesis generation approach, we present results via three evaluation methods.First, we show that in the standard supervised learning setup, our generated hypotheses enable more accurate predictions than baselines and even oracles when using a small set of examples.Second, we evaluate the generated hypotheses by checking whether they can generalize across different inference LLMs and to out-of-distribution datasets.We find surprisingly consistent performance even when using a different LLM to make inference from the generated hypotheses.So, we conduct a qualitative analysis to show that the generated hypotheses not only corroborate existing theories but also provide novel insights about the tasks at hand.</p>
<p>Performance on Heldout Test Sets</p>
<p>As discussed in the introduction, a side product of our approach is an interpretable hypothesis-based classifier.We compare its performance with standard supervised learning with the fine-tuned models and few-shot incontext learning (Table 1).</p>
<p>Our generated hypotheses improve inference over standard zero-shot and few-shot inference.Across all LLMs, HypoGeniC outperforms the zero-shot learning by an average of 60% on SHOE SALES, 22.7% on DECEPTIVE REVIEWS, 5.1% on HEADLINE POPULAR-ITY, and 30.6% on TWEET POPULARITY.Similarly, we find that HypoGeniC shows an increase from few-shot learning by 31.7% on SHOE SALES, 13.9% on DECEP-TIVE REVIEWS, 3.3% on HEADLINE POPULARITY, and 24.9% on TWEET POPULARITY.Note that these results are inflated on TWEET POPULARITY as safety mode is triggered for Mixtral and Claude-2.1 for zero-shot and few-shot learning respectively.After computing the 95% confidence intervals (with a binomial distribution assumption) for our results, the following results are significant for the real life datasets: HypoGeniC for DECEPTIVE REVIEWS and TWEET POPULARITY with Claude-2.1 and Mixtral, when comparing to their respective few shot baselines.If we relax the confidence interval to 90%, the result for HEADLINE POPULARITY with Mixtral is also statistically significant.These results demonstrate that hypothesis-based inference can increase the performance of LLMs significantly.Further results can be found in Table 5.One exception is that our method performs slightly worse (by 1%) than the few-shot baseline in the TWEET POPULARITY with GPT-3.5-turbo.One possible reason is that the few-shot demonstrations are effective at eliciting the pretraining knowledge in GPT-3.5-turbo,possibly due to a large amount of tweets in pretraining data.More detailed results are in Appendix C.</p>
<p>We also evaluate generated hypotheses with oracle inference, where the model retrospectively picks the best hypothesis for each prediction from the bank.With oracle inference, HypoGeniC achieves on average 88.6% on DECEPTIVE REVIEWS, 84.1% on HEADLINE POPU-LARITY, and 88% on TWEET POPULARITY across all LLMs, which are superior to results in Table 1.This result further suggests that hypotheses generated by Hy-poGeniC are of high quality and can lead to accurate predictions when the correct hypothesis is selected.</p>
<p>HypoGeniC matches or even exceeds the fine-tuned models with the same number of training examples on most datasets.Both HypoGeniC and the fine-tuned models yield 100% on the syntheic dataset.Moreover, HypoGeniC is 12.8% and 11.2% better than RoBERTa, and 12.1% and 11.6% better than Llama-2-7B, on HEADLINE POPULARITY and TWEET POPULAR-ITY respectively with 200 training examples.Since the fine-tuned models learns by updating model weights to minimize the cross-entropy loss, it tends to benefit from more training examples, so we increase training examples to 1000 for the fine-tuned models.Despite the accuracy boost from more training examples, we find that HypoGeniC's best result still outperforms RoBERTa by 3.7% and 0.7%, and Llama-2-7B by 3.7% and 11.4%, on HEADLINE POPULARITY and TWEET POPULARITY, respectively.One exception, however, is the DECEP-TIVE REVIEWS dataset.We suspect that as word-level features are very useful in this dataset (Ott et al., 2011), they could be tougher for LLMs to extract but easier for fine-tuned models to grasp.</p>
<p>Updating the hypothesis bank leads to hypotheses of higher quality.Comparing HypoGeniC with the "no updates" results, we find that updating hypotheses generally leads to better hypotheses, suggesting that our algorithm is effective at improving hypothesis quality.The improvement is on average 0.7% on SHOE SALES, 5.8% on DECEPTIVE REVIEWS, 8.1% on HEADLINE POPULARITY, and 7% on TWEET POPULARITY.Another advantage of HypoGeniC over "no updates" is that sometimes the training examples exceed the context window size of LLMs, which can lead to degraded performance (Figures 4 and 5).</p>
<p>Effect of inference strategy.Figure 2 shows Hy-poGeniC results with different inference strategies on DECEPTIVE REVIEWS.Single-step adaptive inference is the most effective.Generally, we find hypotheses to be one-sided, focusing on either characteristics of truthful or deceptive reviews.We thus need to consider more than one hypothesis to make a correct prediction, so best-accuracy hypothesis or two-step adaptive inference are not ideal.On the other datasets, we find that the effect of inference strategy is much smaller (Figure 3).Best-accuracy hypothesis is sufficient for SHOE SALES and HEADLINE POPULARITY, and filter and weighted vote works best for TWEET POPULARITY.Whichever inference strategy we use, the trend of HypoGeniC against few-shot learning and the fine-tuned models remains largely the same.For the real-world datasets, however, the performance sometimes peaks at training size at 25 or 100 before reaching to 200.We suspect that the evaluation of the hypothesis bank would be less stable for the realworld datasets, since more than one correct hypotheses are needed for the task.We also find that using a hypothesis pool of size 20 leads to better performance than using a pool of size 3.</p>
<p>Although this classification experiment is convenient to run and demonstrates that our generated hypotheses are reasonable, our main goal is to generate high-quality hypotheses rather than maximizing the performance of this particular way of using the hypotheses.The next two experiments are essential in understanding the quality of hypotheses through generalization and manual analysis.</p>
<p>Generalization of the Generated Hypotheses</p>
<p>Our primary interest lies in the quality of the hypotheses.A good hypothesis should enable accurate inference by any AI model or even human and also generalize to unseen out-of-distribution dataset.In this subsection, we mix and match different LLMs for generation and inference.We also evaluate the hypotheses in deceptive review prediction on a new out-of-distribution (OOD) dataset (Li et al., 2013).</p>
<p>We find that the hypotheses generated by Hy-poGeniC generalize across models (Table 2).Generally, we find Claude-2.1 and Mixtral to be better at inference.Thus, substituting the inference model with them lead to better performance for hypothesis generated with GPT-3.5-turbo.Substituting Claude-2.1 and Mixtral as each other's inference model lead to small changes in performance.On SHOE SALES, the performance remains high for any inference model used.</p>
<p>Performance even increases for DECEPTIVE RE-VIEWS and HEADLINE POPULARITY when using Claude-2.1 as the inference model.For the cases where performance drops from Claude-2.1 to Mixtral, the decrease is marginal: 2.3% on DECEPTIVE REVIEWS and 2.7% on TWEET POPULARITY.</p>
<p>These results suggest that the hypotheses generated by HypoGeniC are generalizable across different LLMs, which somewhat contradicts the claim in Qiu et al. (2024) that LLMs cannot reliably interpret the hypotheses.We suspect that the reason is that our tasks only rely on natural language, while their tasks rely on notions of worlds and are fed into symbolic interpreters.GPT-3.5-turboFew shot 52.0 (↓3.0)GPT-3.5-turboHypoGeniC 60.7 (↑3.4)Table 3: Performance on OOD deceptive reviews.</p>
<p>Our generated hypotheses generalize to an out-ofdistribution dataset.Table 3 presents an overview for the OOD deceptive review dataset.This dataset differs from DECEPTIVE REVIEWS by including reviews from four cities sourced from different websites (Li et al., 2013).We find that HypoGeniC outperforms few-shot learning by an average of 19.1%.Despite the distribution shift, HypoGeniC surprisingly increases accuracy from DECEPTIVE REVIEWS by an average of 3.3%, suggesting our hypotheses generalize well to this OOD dataset.Claude-2.1 remains the best performing model.In comparison, the performance of RoBERTa drops by 11%, and Llama-2-7B drops by 10%.As a result, Hy-poGeniC with Claude-2.1 outperforms RoBERTa by 1.7%, demonstrating the robustness of hypothesis-based inference.Refer to Appendix C.3 for more details.</p>
<p>Qualitative Analysis</p>
<p>For the synthetic dataset, all models are able to find the true underlying hypothesis for SHOE SALES: "customers tend to buy shoes that match the color of their shirt."For the real-world datasets, we search for studies on these datasets on Google Scholar and compare our hypotheses with findings from the literature.We confirm the validity of some of our hypotheses and discover new insights about the tasks that previous studies did not touch upon.We show a few examples in Table 4, and the full list of hypotheses can be found in Appendix D.</p>
<p>Our generated hypotheses align with useful features in existing literature.For DECEPTIVE REVIEWS, we find that deceptive reviews are more likely to be emotional, use superlatives, or contain information that could not have been directly experienced.Similar findings are also found by previous studies on DECEPTIVE REVIEWS (Lai et al., 2020;Anderson and Simester, 2014;Ott et al., 2011;Li et al., 2014).For TWEET POP-ULARITY, we discover that tweets that are concise, with specific or relevant hashtags, or with emotional tones are more likely to be retweeted more, aligning with prior studies (Tan et al., 2014;Gligorić et al., 2019).For HEADLINE POPULARITY, we find that revealing something new or using vivid language and imagery can drive engagement from readers to click on headlines.Previous studies also find these rules apply to online news headlines (Banerjee and Urminsky, 2021;Sadoski et al., 2000).</p>
<p>We also discover new insights with our generated hypotheses.For the DECEPTIVE REVIEWS dataset, truthful reviews could mention the reviewer's purpose for staying at the hotel (e.g., business trip, vacation), but deceptive ones tend not to have this information.For HEADLINE POPULARITY, we find that headlines that frame the content in a personal or relatable way are clicked more.For TWEET POPULARITY, tweets that mention influential individuals or organizations are more likely to be retweeted.</p>
<p>Intriguingly, one of our hypotheses contradicts a feature engineering result.Ott et al. (2011) find that the token "future" is associated with deceptive reviews, while one of our hypotheses says that mentions of "past experiences or future travel plans" are indicative of truthfulness.This discrepancy is interesting, because the context for the token "future" is unclear.It could be in the context of future plans but could also be as a complaint about "never going to stay at the hotel in the future."Feature engineering is limited by contextual ambiguity, whereas our generated hypotheses and their interpretation by LLMs overcome such limitations.Table 4: Selected examples of generated hypotheses (on the real-world datasets) and whether they support existing findings or are novel.</p>
<p>Our automatic evaluation of hypothesis quality also reflects negative findings.Given mixed evidence from previous literature on the effect of "reading ease" on headline clicks, Banerjee and Urminsky (2021) finds that reading ease negatively impacts click-through rates in HEADLINE POPULARITY through careful feature engineering.Consistent with this result, we found that the hypotheses that claim "straightforward" and "clear" writing to be indicative of higher click-through rates have relatively lower accuracies during training.Reasoning with LLMs.Although it is not our primary goal, our results show that hypothesis-based classifiers can outperform few-shot prompting.As hypotheses may be viewed as a form of reasoning, it is related to reasoning with LLMs (Wei et al., 2022;Wang et al., 2023, i.a.).In particular, our work differs from chainof-thought reasoning because no predefined reasoning structure is available.Moreover, an important distinction between reasoning and hypothesis generation is that the former leverages established reasoning, while the latter requires both proposition and verification of the hypotheses, to discover unknown knowledge.</p>
<p>LLMs for (social) sciences.Increasing attention has been brought to the use of LLMs in social science research (Ziems et al., 2024;Kim and Lee, 2023, i.a.).</p>
<p>Our experiments demonstrate the potential of LLMs in generating hypotheses for social science research to discover unknown knowledge in the data.Furthermore, our approach can be extended to natural sciences for general scientific discovery.</p>
<p>Conclusion &amp; Further Discussion</p>
<p>In this work, we propose HypoGeniC, a novel datadriven and automated method that leverages LLMs to generate hypotheses with the goal of discovering unknown knowledge.With HypoGeniC, we are not only are able to generate human-interpretable hypotheses but also achieve better predictive performance against competitive baselines and even oracles.Furthermore, our method can generalize well with different models and datasets, including open models.Notably, with our generated hypotheses, we uncover new insights in realworld tasks that are widely studied in social sciences.</p>
<p>The key to success in HypoGeniC is not that LLMs remembers the correct hypotheses, but lies in their ability to "hallucinate" and combine potentially relevant concepts.The exploration-exploitation process then identifies the valuable hypotheses.HypoGeniC can be directly applied to complex social science tasks.We encourage future work to explore hypothesis generation that requires additional modalities and/or leverages existing literature along with past observations.</p>
<p>Limitations</p>
<p>We address common concerns using a Q&amp;A format.</p>
<p>Q: Why only experiment with social science tasks?A: Math and physics problems and hypotheses are hard to represent in natural language and usually require symbolic parsers (Trinh et al., 2024).We leverage LLMs to perform tasks that it is naturally adept at, which lead us to social science tasks.We find that HypoGeniC demonstrates strong results for the selected tasks, indicating new possibilities in using LLMs for scientific discovery.We leave extending our framework to natural science tasks as future work.</p>
<p>Q: Why is HypoGeniC effective, given that the accuracy improvement is not significant in some settings?</p>
<p>A: Even if there is no significant improvement in accuracy, the benefits of HypoGeniC are found in the quality of hypotheses.We find that the generated hypotheses discover new patterns that were previously unseen, as discussed in § 4.3.Additionally, it is worth noting that LLMs are imperfect at reasoning.Thus, hypothesis-based inference with LLMs may not accurately reflect the quality of the hypotheses.</p>
<p>Q: Since you worked on some old datasets, what if the LLMs have pre-trained knowledge about these tasks?</p>
<p>A: In Table 1, the zero/few-shot learning results suggest that the models cannot solve the tasks by memorizing the data.Additionally in § 4.3, we show that HypoGeniC reveal new hypotheses, based on the literature space that we can manually search.Even if the models have been pre-trained on the datasets, these hypotheses were not reported in previous literature.This suggests that even experienced researchers still struggle in finding the hypotheses that HypoGeniC generate.</p>
<p>Q: What hyperparameters have you tried?A: We aim to provide a robust framework for hypothesis generation, as opposed to focusing on the optimization of results.Thus, we did not perform an extensive hyperparameter search with the generation portion of HypoGeniC.We did not adjust the value of k, which determines H top in Algorithm 1 to maintain efficiency.Additionally, we only considered the effect of using a hypothesis bank size of 3 and 20 to only test using an extremely small hypothesis bank size and a large one.The ideal hypothesis bank size may require further investigation.Finally, we only tested the size of our wrong example bank w max as 10 to strike a balance between context window sizes and generation of good quality hypotheses.We believe that a more thorough hyperparameter search could improve the performance of our methodology.</p>
<p>Q: How costly is your approach?A: HypoGeniC has high latency, specifically when using inference methods that require multiple prompts.For example, the filter and weighted vote inference policy requires iterating through the top hypotheses to determine relevance and then performing inference if it is relevant.For single-step adaptive inference and best accuracy hypothesis, however, HypoGeniC is efficient.Given that we request reasoning for all inference prompts, the procedure can be time-consuming and require financial costs (e.g., GPT-3.5-turbotakes $2.05 on average over 76 experiments with an average of 1.5 hours per experiment).This concern is alleviated when using open models.However, all these processes are still relatively cheap compared to human efforts.</p>
<p>Q: What are some potential risks of hypothesis generation?</p>
<p>A: One potential risk of hypothesis generation is that there is little guard regarding steorotypes and biases being confirmed if given data that may seem to enforce them.As a result, it can be potentially harmful to use HypoGeniC in a real-world setting without proper oversight.Additionally, if the data reveals personal information regarding people, there is no guarantee that the hypotheses generated will not reveal this information.We highly recommend human-AI collaboration in using HypoGeniC to ensure that the generated hypotheses are ethical and unbiased.</p>
<p>A Prompts</p>
<p>We follow the general prompt engineering guide from Claude (Anthropic, 2023) to craft the prompts.Specifically for all the prompts we use for LLMs, we split them into instruction and user prompts.In the instruction prompt, we first set a tone and context, followed by an explicit task description, and then specify the answer format.The user prompt then includes useful information such as past examples and learned hypothesis.By the end of the user prompt, we ask the LLM to make a prediction.At generation time, we input the instruction prompt to LLMs as system prompt, wrapped by the corresponding system prompt tokens for each model.Below are some example templates for the prompts associated with each task.Example 1: Hypothesis Generation.</p>
<p>A.1 Shoe Sales</p>
<p>Instruction Prompt</p>
<p>You are a shoe salesman and want to recommend shoes to customers.There are white, red, orange, green, blue, and black shoes.From past experiences, you learned some patterns.Now, at each time, you should apply the learned pattern, given below, to a new customer and recommend a shoe color.Give an answer for the shoe color recommendation.The answer should be one color word.It has to be one of white, red, orange, green, blue, and black.</p>
<p>User Prompt</p>
<p>Our learned pattern: <hypothesis_high_reward> New customer: <appearance> is buying a pair of shoes, the shoes should be which color? Answer:</p>
<p>Example 2: Hypothesis-based Inference.</p>
<p>A.2 Deceptive Reviews</p>
<p>Instruction Prompt You're a professional hotel review analyst.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.Using the given examples, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided reviews.Each hypothesis should contain a pair of the following: 1.A hypothesis about what makes reviews more likely to be truthful 2. The opposite hypothesis about what makes reviews more likely to be deceptive Generate them in the format of 1. [hypothesis], 2.</p>
<p>[hypothesis], ... <num_hypotheses>.[hypothesis].</p>
<p>The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.</p>
<p>User Prompt</p>
<p>We have seen some hotel reviews: Example 5: Hypothesis Generation.</p>
<p>Instruction Prompt You are a professional deceptive detection agent and your job is to determine whether a hotel review is truthful or deceptive.</p>
<p>In other words, we want to know whether the review is written by someone who had real experiences with the hotel.</p>
<p>From past experiences, you learned a pattern.You need to determine whether each of the patterns holds for the current hotel review, and also predict whether the current hotel review is truthful or deceptive.Give an answer.The answer should be one word ( truthful or deceptive).</p>
<p>Give your final answer in the format of {Final answer: answer} User Prompt Our learned pattern: <hypothesis_high_reward> A hotel review is the following: <review> Given the pattern you learned above, give an answer of whether the hotel review above is deceptive or truthful.Think step by step.First step: Think about which pattern can be applied to the hotel review.</p>
<p>Second step: Based on the pattern, is this hotel review deceptive or truthful?</p>
<p>Example 6: Hypothesis-based Inference.</p>
<p>Instruction Prompt You are a deceptive detection agent and want to determine whether a hotel review is truthful or deceptive.</p>
<p>In other words, we want to know whether the review is written by a someone who actually lived in the hotel.You need to determine whether this pattern holds for the current hotel review, and also predict whether the current hotel review is truthful or deceptive.</p>
<p>Give an answer.The answer should be one word ( truthful or deceptive).</p>
<p>User Prompt</p>
<p>We have seen some hotel reviews: Next, apply the pattern on the new sample to determine whether the new hotel review is deceptive or truthful.</p>
<p>Finally, give an answer.The answer should be one word (deceptive or truthful).Please give your final answer in the following format: Reasoning for choosing pattern: reason, Chosen pattern: pattern, Reasoning for choice of prediction: reason, Final Answer: answer User Prompt Here are some previously generated patterns with some example where it predicted correctly if a hotel review is deceptive or truthful.<adaptive_info_prompt> A hotel review is the following: <review> Is this hotel review truthful or deceptive?Think step-by-step.</p>
<p>Step 1: Look at the new hotel review and compare it with the set of examples associated with each provided pattern.</p>
<p>Step 2: Find the set of examples that is the most similar to the new hotel review, pick and repeat the pattern associated with that set of examples.</p>
<p>Step 3: Apply the pattern you picked to the new hotel review and predict whether the new hotel review is deceptive or truthful.</p>
<p>Step 4: Give your final answer.Answer:</p>
<p>Example 8: Example-based Hypothesis Selection and Inference.<adaptive_info_prompt> consists of several hypotheses and the corresponding examples they got correct during generation time.</p>
<p>A.3 Headlines With More Clicks</p>
<p>Instruction Prompt You are a professional writer for an online newspaper company.Given a pair of headlines created for the same article, you are asked to determine which will get more clicks.It is likely that the pair of headlines shares similarities, so please focus on their differences.What difference in two headlines leads to more clicks on one than the other?You will be given a set of observations of the format: Headline 1: [ [hypothesis], ... <num_hypotheses>.[hypothesis].</p>
<p>Proposed hypotheses:</p>
<p>Example 9: Hypothesis Generation.</p>
<p>Instruction Prompt</p>
<p>You are a professional writer for an online newspaper company.</p>
<p>Given a pair of headlines created for the same article, you are asked to determine which will get more clicks.It is likely that the pair of headlines shares similarities, so please focus on their differences.From past experiences, you learned some patterns.Now, at each time, you should apply the learned pattern to a new pair of headlines that are created for a new article and determine which headline gets clicked more.</p>
<p>The answer for the higher clicks should be in the form "Headline <em>" where _ is either 1 or 2. Please give your final answer in the format of { Final Answer: Headline </em>.} User Prompt Learned pattern: <hypothesis_high_reward> Given the pattern you learned above, predict which of the following headlines will get more clicks: Headline 1: <headline_1> Headline 2: <headline_2> Think step by step.</p>
<p>Step 1: Think about whether the pattern can be applied to the headlines.</p>
<p>Step 2: Analyze the difference between "Headline 1" and "Headline 2".</p>
<p>Step 3: Based on the pattern, which headline is likely to get more clicks?</p>
<p>Example 10: Hypothesis-based Inference.</p>
<p>Instruction Prompt YYou are a writer for an online newspaper company.So you are excellent at determining which headlines are more likely to cause users to click on the article.You will be given two headlines, and determine which headline was clicked more often.You are only to give your answer.</p>
<p>The answer for the higher clicks should be of the form "Headline _" where _ is either 1 or 2.</p>
<p>Give your final answer in the following format: "Answer: Headline _"</p>
<p>User Prompt</p>
<p>Here are some previous examples to help you: Example 13: Hypothesis Generation.</p>
<p>Instruction Prompt</p>
<p>You are a social media expert.Given a pair of tweets, you are asked to predict which tweet will be retweeted more.Please note that the paired tweets are about the same content and are posted by the same user, so you should focus on the wording difference between the two tweets.From past experiences, you learned a pattern.Now, at each time, you should apply a learned pattern to a pair of tweets and determine which one will get more retweets.The answer for the higher retweets should be of the form "the _ tweet" where _ is either first or second.Please give your final answer in the format of { Final answer: the _ tweet} User Prompt Our learned pattern: <hypothesis_high_reward> The first tweet: <first_tweet> The second tweet: <second_tweet> Given the pattern you learned above, predict which one of the two tweets will get more retweets.Think step by step.First step: Think about if the pattern can be applied to the tweets.Second step: Analyze the textual difference between the two tweets.Third step: Based on the pattern, which tweet is more likely to get more retweets?Final step: Give your final answer in the format of {Final answer: the _ tweet} Final answer:</p>
<p>Example 14: Hypothesis-based Inference.</p>
<p>Instruction Prompt You are a social media expert.Given a pair of tweets, you are asked to predict which tweet will be retweeted more.Please note that the paired tweets are about the same content and are posted by the same user, so you should focus on the wording difference between the two tweets.The answer for the higher retweets should be of the form "the _ tweet" where _ is either first or second.Please give your final answer in the format of { Example 15: Zero/Few-shot Inference.</p>
<p>Instruction Prompt You are a social media expert.Given a pair of tweets, you are asked to predict which tweet will be retweeted more.Please note that the paired tweets are about the same content and are posted by the same user, so you should focus on the wording difference between the two tweets.From past experiences, you learned some patterns.You should apply a learned pattern to a pair of tweets and determine which one will get more retweets.For each pattern, you will also see a couple of examples that worked for each pattern.Please choose a pattern.To do this, look at the examples associated with each pattern, and find which set of the examples are closest to the given pair of tweets.Please choose the pattern corresponding to that set of examples.Please give your final answer in the following format: Reasoning for choosing pattern: reason, Chosen pattern: pattern, Reasoning for choice of prediction: reason, Final Answer: answer User Prompt Here are some previously generated patterns with some examples where it predicted which tweet will will be retweeted more.<adaptive_info_prompt> The first tweet: <first_tweet> The second tweet: <second_tweet> Which one of the two tweets will get more retweets?Think step by step.</p>
<p>Step 1: Look at the new pair of tweets and compare them with the examples associated with each pattern.</p>
<p>Step 2: Find the set of examples that is closest to the given pair of tweets, and pick the pattern associated with that set of examples.</p>
<p>Step 3: Analyze the textual difference between the two tweets.</p>
<p>Step 4: Apply the picked pattern to the new pair of tweets.Based on that pattern, think about which one out of the pair of headlines will get more clicks.</p>
<p>Step 5: Give your final answer.</p>
<p>Dynamic hypotheses update</p>
<p>In Algorithm 1, we display how we generate and update the hypotheses pool H.In particular, we add an example s to the wrong example bank W if the number of hypotheses that incorrectly predict s is greater than w hyp .In our implementation, we use a linearly increasing w hyp as training time t increases.This allows our algorithm to update the hypotheses more frequently at early stage of training, and less frequently at the end.</p>
<p>B.2 Inference method implementations</p>
<p>Filter and weighted vote In order to filter the hypotheses, we iterate through the top k hypotheses ranked by reward.For each hypothesis, we ask the Large Language Model (LLM) if it is relevant.Thereafter, for each of the relevant hypotheses, the LLM is prompted to use the hypothesis to make predictions.Then, for each predicted label, we add up the accuracy scores from the hypotheses that outputted that particular label.The final label is the one that has highest total accuracy score.</p>
<p>One-step adaptive and two-step adaptive inference</p>
<p>The detailed framework of our adaptive inference methods is split into two parts -hypotheses pruning and hypotheses selection.In the case where we have a large number of hypotheses, it is likely that some hypotheses in H have overlaps or are paraphrases of each other.</p>
<p>We address this issue with the following procedure:</p>
<p>1.During training, we record the examples that each hypothesis correctly predicts.</p>
<ol>
<li>
<p>Then we create one-hot encodings for each hypothesis, where the i-th element of the one-hot encoding is 1 if the hypothesis correctly predicts the i-th example, and 0 otherwise.We subsequently compute a similarity matrix between each pair of hypotheses by taking the pairwise cosine similarities.</p>
</li>
<li>
<p>Lastly, we create a linear program with the objective of maximizing the sum of accuracies of the selected hypotheses, subject to the constraint that every pair of the selected hypotheses has a similarity score below a predefined threshold γ.</p>
</li>
</ol>
<p>After pruning the set of hypotheses, we prompt the LLM to pick one hypothesis for its final prediction, as described in § 2.2.For the single-step adaptive inference, we ask the LLM to select a hypothesis and make a prediction in one prompt.On the other hand, with the two-step adaptive inference, we first prompt the LLM to select a hypothesis and then prompt the LLM again to make a prediction based on the selected hypothesis.</p>
<p>B.3 Hyperparameters</p>
<p>For the training stage, we set a limit on the hypothesis bank size, experimenting with sizes H = 3 and H = 20 to determine the impact of utilizing a larger number of hypotheses.Throughout all the experiments, we use the reward coefficient α = 0.5, w max = 10, num_init = 10, and we have two different sets of the rest of hyperparameters for hypothesis bank sizes of 3 and 20.</p>
<p>• With H = 3, we use k = 2 and generate 1 hypothesis per update.For inference, we employ all 3 hypotheses for filter and weighted vote.For singlestep and two-step adaptive inference, we use all 3 hypotheses with γ = 0.3 and provide 5 examples to each hypothesis.</p>
<p>• In the case of H = 20, we use k = 10 and generate 5 hypotheses per update.Then we take the top 5 hypotheses, ranked by their training accuracies, for filter and weighted vote.For single-step and two-step adaptive inference, we use the top 5 hypotheses with γ = 0.7 and provide 5 examples each.</p>
<p>B.4 Licensing Details</p>
<p>The DECEPTIVE REVIEWS and TWEET POPULARITY datasets have not been released with any licenses, but are free to use for research purposes based upon the authors.</p>
<p>The HEADLINE POPULARITY dataset is released under the Creative Commons Attribution 4.0 International License.The SHOE SALES dataset will be released under the same licensing as this work, CC BY 4.0 License, should it be accepted.</p>
<p>In regards to models, we find that GPT-3.5-turbo and Claude-2.1 are all proprietary models and are not released under any open-source licenses.On the other hand, Mixtral is released under the Apache License 2.0.RoBERTa is not released under specific licensing but is free to use for research purposes.However, Llama-2-7B is released under their own licensing found at https://ai.meta.com/llama/license/.</p>
<p>Per our extensive search, we find that we are in compliance with the licensing agreements of all the datasets and models used in this work.</p>
<p>C Detailed Results</p>
<p>C.1 HypoGeniC Performance across inference strategies</p>
<p>Figure 3 presents the best results for all of our inference strategies, considering every dataset and all hyperparameter configurations.</p>
<p>For SHOE SALES, we observe that all the models perform effectively by using the best hypothesis inference strategy.Surprisingly, Mixtral is unable to perform perfectly.This is because despite generating the hypothesis that fully describes the data, Mixtral opts not to apply the hypotheses, favoring to choose a random label for the sake of "variety".Both GPT-3.5-turbo and Mixtral display similar patterns across the inference strategies, with best-accuracy hypothesis, filter and weighted vote, and two-step adaptive inference all having comparable performance.However, for all models we find single-step adaptive inference drops in accuracy.Given that two-step adaptive inference performs well, it is likely that the long prompt causes the model difficulty in choosing the correct hypotheses.For Claude-2.1, we see that filter and weighted vote drops in performance.As this method searches for relevant hypotheses, the model is likely finding that inaccurate patterns relevant, which end up outweighing the inference of the best hypothesis.</p>
<p>For DECEPTIVE REVIEWS, Claude-2.1 is the best performing model across all inference policies.Across the models, we highlight that single-step adaptive inference method works best for this dataset.In this inference method, the prompt specifically includes the aims of determining if a review is deceptive.This likely helps the model use the context provided to better decide which set of example resembles the test example most.Hence, splitting up the prompt may have caused performance to suffer.</p>
<p>We find that HEADLINE POPULARITY is the most challenging dataset.As mentioned in § 3.1, the original dataset was created with both images and headlines paired together.In our version of the dataset, we only use the headlines, so we are missing a crucial variable that contributes to understanding click behavior.Therefore, based off only headlines, it is difficult to generate hypotheses that truly capture the data.Despite this challenge, we note that our hypotheses can still adeptly capture a large portion of data with 63.7% being our highest accuracy.Specifically, we find that the best-accuracy hypothesis strategy performs best.We also note that filter and weighted vote can provide strong performance as in the case of Claude-2.1 and GPT-3.5-turbo,suggesting that hypotheses corroborating with each other can lead to better performance.We observe that GPT-3.5-turbo is the best performing model here, with all inference policies (aside from single-step adaptive) having high</p>
<p>Tweet 1 :Figure 1 :
11
Figure 1: Illustration of HypoGeniC.During update stage, we evaluate the top k hypotheses on each new training example and update the reward based on the prediction correctness.If the number of hypotheses that got the example wrong exceeds a certain threshold, we add the example to a wrong example bank.The wrong example bank is then used to generate new hypotheses.</p>
<p>more emotional terms.Li et al. (2014) Truthful reviews would mention weddings or special occasions.HEADLINE POPULARITY Using vivid language and imagery helps.Banerjee and Urminsky (2021)Headlines that frame the content in a personal or relatable way are clicked more.TWEET POPULARITY Tweets with emotional tones are retweeted more.Tan et al. (2014) Mentioning influential individuals or organizations leads to more retweets.</p>
<p>Final answer: the _ tweet} User Prompt Here are some examples: ••• more examples here ••• The first tweet: <first_tweet> The second tweet: <second_tweet> Which one of the two tweets will get more retweets?</p>
<p>Example 16 :
16
Example-based Hypothesis Selection and Inference.<adaptive_info_prompt> consists of several hypotheses and the corresponding examples they got correct during generation time.B Implementation and Setup Details B.1 HypoGeniC implementation Sampling When initializing the rewards of newly generated hypotheses, we use the examples in the wrong example bank to do so.Given that we work in a low data regime, for hypotheses generated near the end of the training loop, the accuracies of hypotheses are likely to be biased.To counter this phenomenon, we also allow for the hypotheses to use the initial examples S init for initializing rewards.By allowing the hypotheses to initialize reward with more examples, the accuracy lies closer to its true value, allowing for fair comparison between earlier generated hypotheses and newer ones.</p>
<p>Figure 4 :
4
Figure 4: Claude-2.1 results for baselines, HypoGeniC (no update), and HypoGeniC (best) with hypothesis bank size 3 and 20 across multiple training samples</p>
<p>Figure 5 :
5
Figure 5: GPT-3.5-turboresults for baselines, HypoGeniC (no update), and HypoGeniC (best) with hypothesis bank size 3 and 20 across multiple training samples</p>
<p>To understand the effect of the number of training examples, we evaluate the performance of all methods at 10, 25, 50, 100, and 200 training examples.We also experiment with two different hypothesis bank sizes: 3 and 20 hypotheses to evaluate the impact of utilizing a larger number of hypotheses.The detailed hyperparameters of our approach can be found in Appendix B.3.
on each of the datasets to serve as a non-interpretable oracle. We include results for training on 200 examples and 1000 examples. Since fine-tuning update model weights, we expect RoBERTa and Llama-2-7B to set the upper bound on in-distribution datasets.We randomly sample 200 training examples and 300 test examples for each dataset. Since all our datasets are classification tasks with ground truth labels, we use accuracy as our evaluation metric.</p>
<p>Table 1 :
1
Prediction accuracies with 200 examples.We report the best numbers across all hyperparameter configurations, number of training examples, and inference strategies for HypoGeniC (we discuss their effect in details in § 4.1).The sensitive nature of the TWEET POPULARITY dataset may cause models to have their safety mode triggered.These results are marked by * in the table.
Generally, having more training examples and a larger hypothesis pool improves performance. We show performance for different methods as number of training examples increase in Figures 4-6. We find Hy-poGeniC accuracy steadily increases as training size increases on SHOE SALES, suggesting that an LLM is
more likely to generate the best hypothesis given more examples.</p>
<p>Table 2 :
2
Performance of cross-model generation and inference with train size = 200 using best-accuracy hypothesis inference and the best hypothesis bank size between 3 and 20.
Claude-2.1Claude-2.1 Mixtral GPT-3.5-turbo100.0 94.0 100.067.3 65.0 60.757.7 57.7 56.362.0 59.3 57.7MixtralClaude-2.1 Mixtral GPT-3.5-turbo99.0 98.0 90.069.7 61.3 56.759.0 57.7 55.358.7 59.3 53.0GPT-3.5-turboClaude-2.1 Mixtral GPT-3.5-turbo100.0 98.0 100.075.3 62.0 57.360.3 60.0 58.759.0 62.3 56.3ModelsOODRoBERTa (Oracle) Llama-2-7B (Oracle)73.0 (↓11.0) 78.7 (↓10.0)Claude-2.1 Few shot Claude-2.1 HypoGeniC41.7 (↓9.3) 74.7 (↑4.7)Mixtral Few shot Mixtral HypoGeniC49.0 (↓7.3) 64.7 (↑1.7)</p>
<p>Instruction PromptYou are a shoe salesman and want to recommend shoes to customers.There are white, red, orange, green, blue, and black shoes.Give your answer for the shoe color recommendation.The answer should be one color word.It has to be one of white, red, orange, green, blue, and black.If you do not have enough information to make a recommendation, you should give the answer "unknown".Instruction PromptYou are a shoe salesman and want to recommend shoes to customers.There are white, red, orange, green, blue, and black shoes.From past experiences, you learned some patterns.For each pattern, you will also see a couple of examples that worked for each pattern.Choose a pattern.To do this, look at the examples of each pattern, and see which of the examples the current customer is closest to.Choose the pattern corresponding to that example.Give an answer for the shoe color recommendation.The answer should be one word.It has to be one of white, red, orange, green, blue, and black.
Give your final answer in the format of "Finalanswer: [answer]."User PromptHere are some examples of customers with certain features buying certain products: ••• more examples here •••New customer: <appearance> is buying a pair ofshoes, the shoes should be which color?Answer:Example 3: Zero/Few-shot Inference.Give your final answer in the following format:Reasoning for choosing pattern: reason,Chosen pattern: pattern,Reasoning for choice of prediction: reason,Final Answer: answerUser PromptHere are some previously generated patterns withsome example where it predicted correcly whatcolor of shoe the customer bought.<adaptive_info_prompt>New customer: <appearance> is buying a pair ofshoes, the shoes should be which color?Answer:Example 4: Example-based Hypothesis Selection and Inference. <adaptive_info_prompt> consists of several hypotheses and the corresponding examples they got correct during generation time.</p>
<p>Figure 6: Mixtral results for baselines, HypoGeniC (no update), and HypoGeniC (best) with hypothesis bank size 3 and 20 across multiple training samples
ModelsMethodsIND DECEPTIVE REVIEWS OOD DECEPTIVE REVIEWSRoBERTa (Oracle)Train 200 Train 100084.0 91.073.0 (↓11.0) 79.7 (↓11.3)Llama-2-7B (Oracle) Train 200 Train 100088.7 92.378.7 (↓10.0) 88.7 (↓3.6)Claude-2.1Zero shot Few shot HypoGeniC (Best-accuracy hypothesis) HypoGeniC (Filter and weighted vote) HypoGeniC (One-step adaptive) HypoGeniC (Two-step adaptive)31.0 51.0 67.3 68.0 70.0 67.727.7 (↓3.3) 41.7 (↓9.3) 71.7 (↑4.4) 74.7 (↑6.7) 68.3 (↓1.7) 70.7 (↑3.0)MixtralZero shot Few shot HypoGeniC (Best-accuracy hypothesis) HypoGeniC (Filter and weighted vote) HypoGeniC (One-step adaptive) HypoGeniC (Two-step adaptive)55.0 56.3 61.3 62.0 63.0 61.349.7 (↓5.3) 49.0 (↓7.3) 64.7 (↑3.4) 61.0 (↓1.0) 54.7 (↓8.3) 64.7 (↑3.4)GPT-3.5-turboZero shot Few shot HypoGeniC (Best-accuracy hypothesis) HypoGeniC (Filter and weighted vote) HypoGeniC (One-step adaptive) HypoGeniC (Two-step adaptive)50.0 55.0 57.3 55.3 55.7 54.749.0 (↓1.0) 52.0 (↓3.0) 60.7 (↑3.4) 55.7 (↑0.4) 51.7 (↓4.0) 59.0 (↑4.3)RoBERTa Llama-2-7BZero shotFew shotMethods No updateshyp_3 (best)hyp_20 (best)137</p>
<p>Table 6 :
6
Performance of baselines and compared to our methods on the out-of-distribution deceptive reviews and DECEPTIVE REVIEWS.</p>
<p>We have publicly released the code and data for HypoGeniC at https://github.com/ChicagoHAI/ hypothesis-generation.
The Upworthy Research Archive only provides the image IDs instead of the graphics. We thus only use the headlines for our dataset.
AcknowledgmentsWe thank the anonymous reviewers for their suggestions.We also thank members of the Chicago Human+AI Lab for their helpful comments.This work is supported in part by NSF grants IIS-2126602.accuracy.Finally, over the TWEET POPULARITY dataset, we find that the filter and weighted vote is the best choice for inference policy, with it being the best inference method for GPT-3.5-turbo and Mixtral.This indicates that using hypotheses in conjunction is useful as multiple variables together adeptly characterize the dataset.The performance of the rest of the inference policies has no clear pattern over this dataset.We also present our results with confidence intervals.We specifically see that compared to the Oracle Methods, HypoGeniC shows performance statistically significant benefits when comparing to the 200 training examples for HEADLINE POPULARITY and TWEET POPULARITY.However, this is not the case for DE-CEPTIVE REVIEWS, because there are word level features that make the task easier for unsupervised methods.We note that HypoGeniC has statistically significant performance increases for DECEPTIVE REVIEWS with Claude-2.1 and Mixtral and for TWEET POPULARITY with Claude-2.1 and Mixtral.C.2 HypoGeniC Performance across training examplesFigure4presents the results for the performance of Figure5displays the accuracy for HypoGeniC with GPT-3.5-turbo for the different training examples.We observe that unlike HypoGeniC performance with Claude-2.1,our results are mixed for when our method outperforms the few shot inference.Specifically, in TWEET POPULARITY, the few shot inference surpasses our results, indicating that in this set hypotheses provide less benefits than using examples.As HypoGeniC exceeds the accuracy of zero shot's, the proposed method still provides benefits to the base model.C.3 Full OOD resultsTable6shows results for the OOD deceptive reviews dataset for all inference strategies for each model.We find that HypoGeniC outperforms both zero shot and few shot learning across all models and inference policies.The best-accuracy hypothesis and two-step adaptive inference methods are the most robust, showing an average increase of 3.7% and 3.6% respectively.We claim that although the filter and weighted vote strategy at first glance may seem to have mixed performance, the method is still robust.The drop in accuracy for Mixtral with filter and weighted is minimal (1%), and both GPT-3.5-turbo and Claude-2.1 exhibit increases in accuracy.Hence, the inference policy is consistent across DECEPTIVE REVIEWS and the OOD deceptive review datset.Interestingly, the single-step adaptive inference method exhibits drops in performance despite being the best performing inference model in DECEPTIVE RE-VIEWS.In single-step adaptive inference, the LLM sees both the hypotheses with the sets of examples along with the final question of determining whether the review is deceptive.Even though the LLM is prompted to only use one chosen hypotheses, these training examples from DECEPTIVE REVIEWS negatively impact the model because they are part of the context and are thus inherently used by LLMs.On the other hand, for two-step adaptive inference, since there is a dedicated prompt for hypothesis selection, the application of the hypothesis is unaffected from the DECEPTIVE REVIEWS training samples.D Qualitative Analysis on Generated HypothesesWe include findings from the generated hypotheses on DECEPTIVE REVIEWS, HEADLINE POPULARITY, and TWEET POPULARITY datasets in
Reviews without a purchase: Low ratings, loyal customers, and deception. Eric T , Anderson , Duncan I Simester, Journal of Marketing Research. 5132014</p>
<p>. Anthropic, 2023. Claude 2</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. Peter Auer, Journal of Machine Learning Research. 32002. Nov</p>
<p>ResearchAgent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024Preprint</p>
<p>The language that drives engagement: A systematic largescale analysis of headline experiments. Akshina Banerjee, Oleg Urminsky, Social Science Research Network. 2021</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford202033Proceedings of NeurIPS</p>
<p>DreamCoder: growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning. Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, Joshua B Tenenbaum, 2020Philosophical Transactions of the Royal Society A, 381</p>
<p>Causal effects of brevity on style and success in social media. Proceedings of ACM HCI. Kristina Gligorić, Ashton Anderson, Robert West, ACM HCI2019</p>
<p>Deception detection. Psychology and law: An empirical perspective. 2005Pär Anders Granhag and Aldert Vrij</p>
<p>Instruction induction: From few examples to natural language task descriptions. Or Honovich, Uri Shaham, R Samuel, Omer Bowman, Levy, Proceedings of ACL. ACL2022</p>
<p>Scaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, CoRR, abs/2001.083612020</p>
<p>AIaugmented surveys: Leveraging large language models and surveys for opinion prediction. Junsol Kim, Byungkyu Lee, arXiv:2305.096202023Preprint</p>
<p>Towards building modeldriven tutorials for humans. Vivian Lai, Han Liu, Chenhao Tan, 10.1145/3313831.3376873Proceedings of CHI. CHI2020Why is 'Chicago' deceptive?</p>
<p>On human predictions with explanations and predictions of machine learning models: A case study on deception detection. Vivian Lai, Chenhao Tan, Proceedings of FAccT. FAccT2019</p>
<p>Identifying manipulated offerings on review portals. Jiwei Li, Myle Ott, Claire Cardie, Proceedings of EMNLP. EMNLP2013</p>
<p>Towards a general rule for identifying deceptive opinion spam. Jiwei Li, Myle Ott, Claire Cardie, Eduard Hovy, 10.3115/v1/P14-1147Proceedings of ACL. ACLBaltimore, MarylandAssociation for Computational Linguistics2014</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, RoBERTa: A robustly optimized bert pretraining approach. ArXiv2019</p>
<p>Machine learning as a tool for hypothesis generation*. Jens Ludwig, Sendhil Mullainathan, 10.1093/qje/qjad055The Quarterly Journal of Economics. 552024</p>
<p>Finding deceptive opinion spam by any stretch of the imagination. Jorge Nathan, Matias , Kevin Munger, Marianne Aubin Le Quere, Charles R Ebersole, Mixtral of experts. OpenAI. 2023a. Chatgpt. OpenAI. 2023b. Gpt-4 technical report. 487 experiments in U.S. media. Scientific Data, 8. Mistral. Myle Ott, Yejin Choi, Claire Cardie, Jeffrey T Hancock, 2021. 2023. 201132Proceedings of ACL</p>
<p>Topicgpt: A prompt-based topic modeling framework. Minh Chau, Alexander Pham, Simeng Hoyle, Mohit Sun, Iyyer, Proceedings of NAACL. NAACL2024</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren ; Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, 10.1038/s41586-023-06924-6Proceedings of ICLR. Bernardino Romera-Paredes, Mohammadamin Barekatain. ICLR. Bernardino Romera-Paredes, Mohammadamin Barekatain2024. 2024625Mathematical discoveries from program search with large language models</p>
<p>Creative cognitive processes in kekule's discovery of the structure of the benzene molecule. Albert Rothenberg, The American journal of psychology. 1995</p>
<p>Engaging texts: Effects of concreteness on comprehensibility, interest, and recall in four text types. Mark Sadoski, Ernest T Goetz, Maximo Rodriguez, Journal of Educational Psychology. 921852000</p>
<p>Experimental study of inequality and unpredictability in an artificial cultural market. J Matthew, Peter Sheridan Salganik, Duncan J Dodds, Watts, Science. 31157622006</p>
<p>The effect of wording on message propagation: Topicand author-controlled natural experiments on twitter. Chenhao Tan, Lillian Lee, Bo Pang, Proceedings of ACL. ACL2014</p>
<p>Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, Noah D Goodman, How to grow a mind: Statistics, structure, and abstraction. 2011331</p>
<p>Angela Fan. Hugo Touvron, Louis Martin, Kevin R Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M Bikel, Lukas Blecher, Cantón Cristian, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony S Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel M Khabsa, A V Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Pushkar Mihaylov, Igor Mishra, Yixin Molybog, Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, R Smith, Xia Subramanian, Binh Tan, Ross Tang, Adina Taylor, Jian Williams, Puxin Xiang Kuan, Zhengxu Xu, Iliyan Yan, Yuchen Zarov, Zhang, ArXivMelanie Kambadur, Sharan Narang; Robert Stojnic, Sergey Edunovand Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Solving olympiad geometry without human demonstrations. Trieu Trinh, Tony Yuhuai, Quoc Wu, He Le, Thang He, Luong, Nature. 6252024</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proceedings of ACL. ACL2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, Proceedings of ICLR. ICLR2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, Proceedings of NeurIPS. NeurIPS2022</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of EACL. EACL2024a</p>
<p>Soujanya Poria, and Erik Cambria. 2024b. Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Proceedings of ACL. ACL</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Proceedings of NeurIPS. NeurIPS2023</p>
<p>Can large language models transform computational social science? Computational Linguistics. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
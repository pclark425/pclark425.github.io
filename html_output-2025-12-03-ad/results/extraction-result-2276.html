<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2276 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2276</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2276</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-239616232</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2110.11466v2.pdf" target="_blank">MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems</a></p>
                <p><strong>Paper Abstract:</strong> Scientific communities are increasingly adopting machine learning and deep learning models in their applications to accelerate scientific insights. High performance computing systems are pushing the frontiers of performance with a rich diversity of hardware resources and massive scale-out capabilities. There is a critical need to understand fair and effective benchmarking of machine learning applications that are representative of real-world scientific use cases. MLPerf is a community-driven standard to benchmark machine learning workloads, focusing on end-to-end performance metrics. In this paper, we introduce MLPerf HPC, a benchmark suite of large-scale scientific machine learning training applications driven by the MLCommons Association. We present the results from the first submission round, including a diverse set of some of the world's largest HPC systems. We develop a systematic framework for their joint analysis and compare them in terms of data staging, algorithmic convergence, and compute performance. As a result, we gain a quantitative understanding of optimizations on different subsystems such as staging and on-node loading of data, compute-unit utilization, and communication scheduling, enabling overall $>10 \times$ (end-to-end) performance improvements through system scaling. Notably, our analysis shows a scale-dependent interplay between the dataset size, a system's memory hierarchy, and training convergence that underlines the importance of near-compute storage. To overcome the data-parallel scalability challenge at large batch sizes, we discuss specific learning techniques and hybrid data-and-model parallelism that are effective on large systems. We conclude by characterizing each benchmark with respect to low-level memory, I/O, and network behavior to parameterize extended roofline performance models in future rounds.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2276.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2276.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CosmoFlow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CosmoFlow (3D convolutional neural network for cosmological parameter inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning regression benchmark that predicts four cosmological parameters from simulated 3D dark-matter volumetric histograms using a 3D CNN; used in MLPerf HPC to evaluate large-scale training performance and scaling behavior on HPC systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CosmoFlow: using deep learning to learn the universe at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Cosmology (inference of cosmological parameters from N-body simulation outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Estimate four cosmological parameters (Ω_M, σ_8, n_s, H_0) from 3D binned dark-matter density fields produced by N-body simulations; inputs are large volumetric histograms, outputs are continuous parameter estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant, simulation-derived and labeled: full benchmark dataset is large (5.1 TB), consisting of 262,144 training samples and 65,536 testing samples stored as TFRecord files; data is available from the ExaLearn / NERSC portal used by MLPerf HPC submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional volumetric data: 3D tensors (512^3 originally, split into 128^3 sub-cubes) with 4 channels (different redshift bins); structured dense grid (voxel) data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: very high-dimensional inputs (128^3 volumes), nonlinear mapping from mass distribution to cosmological parameters, moderate-to-large model size (5 conv layers + 3 FC layers), and strong I/O and memory footprint constraints due to TB-scale datasets and large per-sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established scientific domain (cosmology) with well-developed simulation tools; application of deep learning is an emerging but rapidly adopted approach for parameter inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-high — scientific relevance requires convergence to physically motivated error thresholds (target MAE) and validation; predictions are used for scientific inference so reproducibility and validation are important though the model is a predictive black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>3D convolutional neural network (deep supervised learning) with SGD; hybrid data-and-model parallelism at scale</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A 3D CNN adapted from prior work: five 3D convolutional layers (kernel size 2, filters 32*i in ith layer) each followed by 3D max-pool, then three fully-connected layers (128, 64, and output 4), leaky ReLU activations, dropout after first two FC layers (p=0.5), tanh-scaled output. Trained with mean-squared-error loss using standard SGD and a scheduled learning-rate decay; baseline global batch size 64. Large-scale submissions used data-parallelism, and for extreme scale (Fugaku) spatial model-parallel partitioning of Conv3D layers (Mesh TensorFlow extension) to form hybrid data-and-model parallelism. Additional optimizations applied in open-division: linear learning-rate decay, data augmentation, disabling dropout; system-level optimizations included mixed-precision, NVIDIA DALI for data loading, on-node caching (RAM disks / SSD), and compression/archiving of files for faster staging.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (regression) with distributed training (data-parallel and hybrid model+data-parallel)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate: deep supervised learning maps well to the problem of parameter regression from high-dimensional simulation outputs; required system-aware modifications (batch scaling techniques, parallelism strategies, I/O staging) to be effective at HPC scale.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Scientific target: MAE < 0.124 (quality target). Dataset: 5.1 TB, 262,144 train samples, 65,536 test samples. MLPerf HPC submissions achieved large speedups across system scales: overall >10× end-to-end time-to-train improvement across systems (closed-division range up to 8–16× scale); example times to solution from v0.7: Piz-Daint-128 closed 461.01 min, Piz-Daint-256 closed 327.01 min, Fugaku-512 closed 268.77 min, Fugaku-8192 closed 101.49 min, Fugaku-16384 open 30.07 min, ABCI-2048 open 13.21 min. Compute budget examples: Cori-GPU-64 compute budget 389.04 h·acc, ABCI-2048 450.96 h·acc. Open-division optimizations enabled up to ~3.38× speedup on Fugaku relative to closed.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Worked well for producing scientifically relevant parameter estimates when trained to the specified target; however, training stability and convergence degrade with very large batch sizes unless model-specific learning techniques are applied. Key limitations: I/O and data-staging overheads (staging can contribute <1% to ~20% of total time depending on system and dataset compressibility), memory capacity constraints for caching, and network/communication bottlenecks at large scale; hybrid model-parallelism allowed scaling beyond pure data-parallel batch-size limits but introduced increased communication (halo exchanges) that reduced scaling efficiency unless overlapped with computation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables rapid training of scientific inference models at scale, reducing time-to-solution by an order of magnitude on large HPC systems, which can accelerate scientific workflows and enable faster parameter studies; highlights system design needs (near-compute storage, memory capacity, network scheduling) for AI-driven science.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly against smaller-scale/pure data-parallel implementations: pure data-parallel scaling hit limits at large global batch sizes (≥512) due to increased epochs-to-converge; hybrid model+data parallelism enabled further scaling (to 8,192–16,384 CPUs) at the cost of communication overhead. Open-division hyperparameter tweaks (LR schedules, disable dropout, augmentation) improved large-batch convergence compared to closed reference schedule. Mixed-precision and data-loading (DALI) optimizations increased throughput versus FP32 and naive data loaders.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Success depended on (1) system-level resources (RAM to cache dataset, on-node SSDs), (2) software/data pipeline optimizations (DALI, TF data caching, file compression), (3) appropriate training hyperparameters and large-batch learning techniques (LR scaling, decay schedules, disabling dropout, augmentation), and (4) parallelization strategy (hybrid model+data parallelism when batch-scaling alone fails).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep supervised models can provide accurate cosmological parameter inference at scale, but effectiveness at extreme HPC scale is governed as much by data movement, memory hierarchy, and convergence behavior with batch size as by raw compute; near-compute storage and hybrid parallelism are often necessary to achieve scalable time-to-solution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2276.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2276.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepCAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepCAM (deep segmentation network for extreme weather detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolutional encoder–decoder segmentation model trained on high-resolution climate simulation data to identify extreme weather phenomena (atmospheric rivers, tropical cyclones) used in MLPerf HPC as a representative large-scale scientific ML workload.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exascale Deep Learning for Climate Analytics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Climate / Earth system science (extreme weather detection and segmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Pixel-level semantic segmentation of high-resolution climate simulation outputs to identify extreme weather features (three classes: background, tropical cyclone/hurricane, atmospheric river) using simulated CAM5 data and heuristic segmentation masks generated by TECA.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant, simulation-derived labeled dataset: 121,266 training and 15,158 testing samples; total dataset size ~8.8 TB; samples stored as HDF5 and available via NERSC/Globus endpoint as used by MLPerf HPC.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-resolution multi-channel image data: 16-channel images of shape 16 × 1152 × 768 (16 physical variables/channels), i.e., large dense image tensors (very high spatial resolution compared to ImageNet), with per-pixel categorical labels (segmentation masks).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: very large per-sample spatial resolution and many channels (1152×768×16), strong class imbalance (≈95% background), large model (Xception-based encoder-decoder with depthwise-separable convolutions, atrous spatial pyramid pooling), and substantial I/O demands; mapping from fields to event masks is complex and multi-scale.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established domain with mature simulation tools (CAM5) and heuristic labeling toolkit (TECA); application of deep learning for climate analytics is a developed but evolving research area.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium: outputs (segmentation masks) are directly interpretable and scientifically meaningful; the model must meet scientifically motivated quality thresholds (IOU target) but full mechanistic explainability beyond accurate detection is not strictly required for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Encoder-decoder convolutional segmentation network (Xception-based backbone) trained with weighted cross-entropy and LAMB optimizer; mixed-precision and data-loading optimizations</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>DeepCAM uses an Xception-based encoder replacing ResNet-50, with 20 residual blocks of depthwise-separable convolutions (grouped convolutions), atrous spatial pyramid pooling at the bottleneck, and a deconvolutional decoder with a single skip connection to propagate low-level features. Inputs are 16×1152×768 tensors; outputs are 1152×768 segmentation masks for three classes. Training uses weighted cross-entropy to address severe class imbalance (~95% background) and the LAMB optimizer (later replaced from Adam/LARS). System-level optimizations include mixed-precision training via NVIDIA Apex, pinned (page-locked) memory and increased data-loading worker processes, caching strategies, and GradSkip (open-division) which selectively skips gradient updates for layers identified as low-impact to speed training.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (semantic segmentation) with distributed training</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate: segmentation architecture maps directly to the scientific problem of detecting and localizing extreme events; weighting loss for class imbalance and using large, multi-scale convolutional modules is suitable. However, meeting IOU targets at scale required careful hyperparameter tuning and system optimizations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Scientific target: IOU > 0.82 (82% similarity). Dataset: ~8.8 TB, 121,266 train samples, 15,158 test samples. MLPerf HPC times: ABCI-1024 closed submission DeepCAM time-to-solution 11.71 minutes (closed), ABCI-1024 open 10.49 minutes (open). DeepCAM requires fewer epochs than CosmoFlow (20–25% the number of epochs) and shows lower run-to-run variability (1.7% std). GradSkip achieved reported speedups: first stage skipping yielded 12.4% speedup and further stage 22.1% speedup compared to no GradSkip.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>The architecture is effective at producing scientifically relevant segmentation masks and is more stable in convergence than CosmoFlow. Key bottlenecks were I/O and evaluation frequency (evaluation triggered by steps can lead to much more frequent IOU computation), class imbalance required weighted loss, and at scale network congestion and data staging constrained batch-size scaling. System-level optimizations (pinned memory, data loaders, mixed precision, GradSkip) meaningfully improved throughput and reduced time-to-solution.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables scalable detection of extreme weather events from large climate simulation datasets, improving analysis throughput for climate science and enabling near-real-time or large ensemble analyses previously infeasible due to compute/I/O limits. Demonstrated scalability to large HPC systems with modest compute budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to earlier ResNet-based encoders and different optimizers — Xception backbone + LAMB offered improved large-batch and segmentation performance. GradSkip (open-division) provided a trade-off of small accuracy impact for reduced training cost; weighted cross-entropy addressed imbalance versus unweighted losses. Mixed-precision provided throughput improvements versus FP32.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Success depended on (1) architecture choice (Xception + ASPP) for multi-scale feature extraction, (2) optimizer selection (LAMB) and hyperparameter tuning (warmup steps), (3) data pipeline optimizations (pinned memory, worker processes), and (4) techniques like GradSkip to reduce backward-pass cost while retaining accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>High-resolution, multi-channel segmentation models are effective for extreme-event detection, but their practical scalability is constrained by I/O, class imbalance, and communication; careful model, optimizer, and pipeline choices (LAMB, mixed-precision, GradSkip, pinned memory) substantially improve time-to-solution while meeting scientifically motivated IOU targets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2276.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2276.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling & optimization techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-batch training, distributed parallelism, and system-level optimizations (mixed-precision, DALI, TF caching, GradSkip, hybrid model+data parallelism, learning-rate schedules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of ML training methodologies and system optimizations applied to enable scalable scientific ML on HPC: includes large-batch training techniques (LR scaling/decay, warmup), hybrid data-and-model parallelism, mixed-precision training, data-loading acceleration (NVIDIA DALI), TF data caching, file compression/archiving for staging, and GradSkip to skip gradients for low-impact layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General (applied across the CosmoFlow cosmology and DeepCAM climate segmentation benchmarks and intended for scientific ML workflows on HPC)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Enable efficient, stable, and accurate training of large deep models on TB-scale scientific datasets across thousands to tens of thousands of HPC processors/accelerators while minimizing time-to-solution and respecting scientific quality targets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies by application; techniques were designed for abundant, labeled, simulation-derived datasets (TB-scale) which cannot all be held in on-device memory; staging to node-local storage and caching strategies are necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Techniques are applicable to dense high-dimensional data (3D volumes and large multi-channel images) and to workloads where per-sample size is large and I/O-bound.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Complex: large per-sample memory footprint, heavy communication demands for distributed synchronous gradient aggregation, sensitivity of convergence to batch-size and optimizer hyperparameters, and I/O bottlenecks from parallel file systems.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging engineering best-practices for scientific ML at HPC scale; borrows from industrial large-batch training literature but adapted to scientific dataset characteristics and HPC system constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium intrinsic — these are engineering and algorithmic techniques aimed at preserving predictive performance while improving throughput; interpretability requirements are task-dependent but techniques themselves are not focused on explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Distributed training strategies and training-time optimizations (large-batch methods, LR scaling & decay, warmup, mixed-precision, DALI data loader, TF data cache, GradSkip, hybrid model+data parallelism)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Applied methods include: (1) Large-batch scaling with learning-rate scaling (multiply LR by batch-size factor or its sqrt) and bespoke decay/warmup schedules to maintain convergence; (2) Mixed-precision training (FP16/FP32) to reduce memory traffic and increase throughput; (3) NVIDIA DALI and multiple data-loader workers, TF data.Dataset.cache and staging to node-local SSD/RAM disk to mitigate I/O; (4) Hybrid data-and-model parallelism implemented by spatially partitioning Conv3D inputs (Mesh TensorFlow extensions) to enable scaling beyond batch-size limits; (5) GradSkip to skip gradients for early layers during later training steps identified as low-impact for a net speedup; (6) Increasing validation batch size to speed up evaluation; (7) Fine-grained communication scheduling and use of NCCL/Horovod with tuned tensor fusion thresholds and hierarchical all-reduce to maximize training throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Distributed supervised training engineering; large-batch optimization; hybrid parallelism</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>These techniques were essential and applicable for training large scientific ML models on leadership HPC systems; applicability depends on system memory (for caching), interconnect bandwidth (for communication-bound gradients), and dataset compressibility (for staging speedups).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported improvements include: overall >10× end-to-end performance improvements across system scaling; mixed-precision yielded 1.77× training speedup (on ABCI single GPU for CosmoFlow) and reduced memory traffic to ~50% in mixed precision vs FP32 in some measurements; DALI improved training by 1.26× and evaluation by 1.69× on a single GPU for CosmoFlow; increasing validation batch size improved evaluation by 3.03×; GradSkip provided cumulative speedups up to 22.1% in DeepCAM open submission; open-division hyperparameter optimizations enabled scaling of CosmoFlow local/global batch sizes from 512 to 2,048+ with reduced epoch overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Highly effective when appropriately combined and tuned; allowed both benchmarks to run at scales (many hundreds to tens of thousands of compute units) while meeting scientific targets. Limitations: large-batch training can increase epochs-to-converge unless mitigated by LR schedules, data augmentation or architecture-specific adjustments; hybrid model-parallelism reduces steps but increases communication (halo exchanges) which can become dominant unless overlapped with computation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — these techniques make it feasible to leverage leadership-class HPC for scientific ML, reducing wall-clock training time and enabling experiments and model development that were previously too expensive; they also inform system design (importance of on-node storage, memory capacity, network scheduling) for future HPC procurement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to naïve scaling: pure data-parallel scaling without large-batch adaptation hits convergence limits at moderate global batch sizes (≥512 for CosmoFlow); hybrid model+data parallelism surpasses that limit but trades increased communication; mixed-precision outperforms pure FP32 in throughput and memory bandwidth usage but requires numerical care; GradSkip trades some layer update fidelity for reduced backward-pass cost and net speed gains.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Success required matching algorithmic techniques (LR schedules, optimizer choice) to parallelism strategy, and ensuring system-level support (enough RAM to cache datasets, fast local SSDs, high-bandwidth interconnect, tuned collective implementations). Effective instrumentation and logging (mlperf-logging) and reproducible submission workflows helped quantify and validate gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Achieving scalable scientific ML on HPC requires co-design: algorithmic adjustments (large-batch LR scheduling, GradSkip) and software/data-pipeline optimizations (mixed-precision, DALI, on-node caching) must be combined with appropriate parallelism (data, model, or hybrid) to overcome I/O, memory, and communication bottlenecks while preserving scientific convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CosmoFlow: using deep learning to learn the universe at scale <em>(Rating: 2)</em></li>
                <li>Exascale Deep Learning for Climate Analytics <em>(Rating: 2)</em></li>
                <li>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes <em>(Rating: 2)</em></li>
                <li>TECA: A Parallel Toolkit for Extreme Climate Analysis <em>(Rating: 1)</em></li>
                <li>MLPerf TM HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2276",
    "paper_id": "paper-239616232",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "CosmoFlow",
            "name_full": "CosmoFlow (3D convolutional neural network for cosmological parameter inference)",
            "brief_description": "A deep learning regression benchmark that predicts four cosmological parameters from simulated 3D dark-matter volumetric histograms using a 3D CNN; used in MLPerf HPC to evaluate large-scale training performance and scaling behavior on HPC systems.",
            "citation_title": "CosmoFlow: using deep learning to learn the universe at scale",
            "mention_or_use": "use",
            "scientific_problem_domain": "Cosmology (inference of cosmological parameters from N-body simulation outputs)",
            "problem_description": "Estimate four cosmological parameters (Ω_M, σ_8, n_s, H_0) from 3D binned dark-matter density fields produced by N-body simulations; inputs are large volumetric histograms, outputs are continuous parameter estimates.",
            "data_availability": "Abundant, simulation-derived and labeled: full benchmark dataset is large (5.1 TB), consisting of 262,144 training samples and 65,536 testing samples stored as TFRecord files; data is available from the ExaLearn / NERSC portal used by MLPerf HPC submissions.",
            "data_structure": "High-dimensional volumetric data: 3D tensors (512^3 originally, split into 128^3 sub-cubes) with 4 channels (different redshift bins); structured dense grid (voxel) data.",
            "problem_complexity": "High: very high-dimensional inputs (128^3 volumes), nonlinear mapping from mass distribution to cosmological parameters, moderate-to-large model size (5 conv layers + 3 FC layers), and strong I/O and memory footprint constraints due to TB-scale datasets and large per-sample size.",
            "domain_maturity": "Established scientific domain (cosmology) with well-developed simulation tools; application of deep learning is an emerging but rapidly adopted approach for parameter inference.",
            "mechanistic_understanding_requirements": "Medium-high — scientific relevance requires convergence to physically motivated error thresholds (target MAE) and validation; predictions are used for scientific inference so reproducibility and validation are important though the model is a predictive black-box.",
            "ai_methodology_name": "3D convolutional neural network (deep supervised learning) with SGD; hybrid data-and-model parallelism at scale",
            "ai_methodology_description": "A 3D CNN adapted from prior work: five 3D convolutional layers (kernel size 2, filters 32*i in ith layer) each followed by 3D max-pool, then three fully-connected layers (128, 64, and output 4), leaky ReLU activations, dropout after first two FC layers (p=0.5), tanh-scaled output. Trained with mean-squared-error loss using standard SGD and a scheduled learning-rate decay; baseline global batch size 64. Large-scale submissions used data-parallelism, and for extreme scale (Fugaku) spatial model-parallel partitioning of Conv3D layers (Mesh TensorFlow extension) to form hybrid data-and-model parallelism. Additional optimizations applied in open-division: linear learning-rate decay, data augmentation, disabling dropout; system-level optimizations included mixed-precision, NVIDIA DALI for data loading, on-node caching (RAM disks / SSD), and compression/archiving of files for faster staging.",
            "ai_methodology_category": "Supervised deep learning (regression) with distributed training (data-parallel and hybrid model+data-parallel)",
            "applicability": "Applicable and appropriate: deep supervised learning maps well to the problem of parameter regression from high-dimensional simulation outputs; required system-aware modifications (batch scaling techniques, parallelism strategies, I/O staging) to be effective at HPC scale.",
            "effectiveness_quantitative": "Scientific target: MAE &lt; 0.124 (quality target). Dataset: 5.1 TB, 262,144 train samples, 65,536 test samples. MLPerf HPC submissions achieved large speedups across system scales: overall &gt;10× end-to-end time-to-train improvement across systems (closed-division range up to 8–16× scale); example times to solution from v0.7: Piz-Daint-128 closed 461.01 min, Piz-Daint-256 closed 327.01 min, Fugaku-512 closed 268.77 min, Fugaku-8192 closed 101.49 min, Fugaku-16384 open 30.07 min, ABCI-2048 open 13.21 min. Compute budget examples: Cori-GPU-64 compute budget 389.04 h·acc, ABCI-2048 450.96 h·acc. Open-division optimizations enabled up to ~3.38× speedup on Fugaku relative to closed.",
            "effectiveness_qualitative": "Worked well for producing scientifically relevant parameter estimates when trained to the specified target; however, training stability and convergence degrade with very large batch sizes unless model-specific learning techniques are applied. Key limitations: I/O and data-staging overheads (staging can contribute &lt;1% to ~20% of total time depending on system and dataset compressibility), memory capacity constraints for caching, and network/communication bottlenecks at large scale; hybrid model-parallelism allowed scaling beyond pure data-parallel batch-size limits but introduced increased communication (halo exchanges) that reduced scaling efficiency unless overlapped with computation.",
            "impact_potential": "High — enables rapid training of scientific inference models at scale, reducing time-to-solution by an order of magnitude on large HPC systems, which can accelerate scientific workflows and enable faster parameter studies; highlights system design needs (near-compute storage, memory capacity, network scheduling) for AI-driven science.",
            "comparison_to_alternatives": "Compared implicitly against smaller-scale/pure data-parallel implementations: pure data-parallel scaling hit limits at large global batch sizes (≥512) due to increased epochs-to-converge; hybrid model+data parallelism enabled further scaling (to 8,192–16,384 CPUs) at the cost of communication overhead. Open-division hyperparameter tweaks (LR schedules, disable dropout, augmentation) improved large-batch convergence compared to closed reference schedule. Mixed-precision and data-loading (DALI) optimizations increased throughput versus FP32 and naive data loaders.",
            "success_factors": "Success depended on (1) system-level resources (RAM to cache dataset, on-node SSDs), (2) software/data pipeline optimizations (DALI, TF data caching, file compression), (3) appropriate training hyperparameters and large-batch learning techniques (LR scaling, decay schedules, disabling dropout, augmentation), and (4) parallelization strategy (hybrid model+data parallelism when batch-scaling alone fails).",
            "key_insight": "Deep supervised models can provide accurate cosmological parameter inference at scale, but effectiveness at extreme HPC scale is governed as much by data movement, memory hierarchy, and convergence behavior with batch size as by raw compute; near-compute storage and hybrid parallelism are often necessary to achieve scalable time-to-solution.",
            "uuid": "e2276.0",
            "source_info": {
                "paper_title": "MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "DeepCAM",
            "name_full": "DeepCAM (deep segmentation network for extreme weather detection)",
            "brief_description": "A convolutional encoder–decoder segmentation model trained on high-resolution climate simulation data to identify extreme weather phenomena (atmospheric rivers, tropical cyclones) used in MLPerf HPC as a representative large-scale scientific ML workload.",
            "citation_title": "Exascale Deep Learning for Climate Analytics",
            "mention_or_use": "use",
            "scientific_problem_domain": "Climate / Earth system science (extreme weather detection and segmentation)",
            "problem_description": "Pixel-level semantic segmentation of high-resolution climate simulation outputs to identify extreme weather features (three classes: background, tropical cyclone/hurricane, atmospheric river) using simulated CAM5 data and heuristic segmentation masks generated by TECA.",
            "data_availability": "Abundant, simulation-derived labeled dataset: 121,266 training and 15,158 testing samples; total dataset size ~8.8 TB; samples stored as HDF5 and available via NERSC/Globus endpoint as used by MLPerf HPC.",
            "data_structure": "High-resolution multi-channel image data: 16-channel images of shape 16 × 1152 × 768 (16 physical variables/channels), i.e., large dense image tensors (very high spatial resolution compared to ImageNet), with per-pixel categorical labels (segmentation masks).",
            "problem_complexity": "High: very large per-sample spatial resolution and many channels (1152×768×16), strong class imbalance (≈95% background), large model (Xception-based encoder-decoder with depthwise-separable convolutions, atrous spatial pyramid pooling), and substantial I/O demands; mapping from fields to event masks is complex and multi-scale.",
            "domain_maturity": "Established domain with mature simulation tools (CAM5) and heuristic labeling toolkit (TECA); application of deep learning for climate analytics is a developed but evolving research area.",
            "mechanistic_understanding_requirements": "Medium: outputs (segmentation masks) are directly interpretable and scientifically meaningful; the model must meet scientifically motivated quality thresholds (IOU target) but full mechanistic explainability beyond accurate detection is not strictly required for this task.",
            "ai_methodology_name": "Encoder-decoder convolutional segmentation network (Xception-based backbone) trained with weighted cross-entropy and LAMB optimizer; mixed-precision and data-loading optimizations",
            "ai_methodology_description": "DeepCAM uses an Xception-based encoder replacing ResNet-50, with 20 residual blocks of depthwise-separable convolutions (grouped convolutions), atrous spatial pyramid pooling at the bottleneck, and a deconvolutional decoder with a single skip connection to propagate low-level features. Inputs are 16×1152×768 tensors; outputs are 1152×768 segmentation masks for three classes. Training uses weighted cross-entropy to address severe class imbalance (~95% background) and the LAMB optimizer (later replaced from Adam/LARS). System-level optimizations include mixed-precision training via NVIDIA Apex, pinned (page-locked) memory and increased data-loading worker processes, caching strategies, and GradSkip (open-division) which selectively skips gradient updates for layers identified as low-impact to speed training.",
            "ai_methodology_category": "Supervised deep learning (semantic segmentation) with distributed training",
            "applicability": "Appropriate: segmentation architecture maps directly to the scientific problem of detecting and localizing extreme events; weighting loss for class imbalance and using large, multi-scale convolutional modules is suitable. However, meeting IOU targets at scale required careful hyperparameter tuning and system optimizations.",
            "effectiveness_quantitative": "Scientific target: IOU &gt; 0.82 (82% similarity). Dataset: ~8.8 TB, 121,266 train samples, 15,158 test samples. MLPerf HPC times: ABCI-1024 closed submission DeepCAM time-to-solution 11.71 minutes (closed), ABCI-1024 open 10.49 minutes (open). DeepCAM requires fewer epochs than CosmoFlow (20–25% the number of epochs) and shows lower run-to-run variability (1.7% std). GradSkip achieved reported speedups: first stage skipping yielded 12.4% speedup and further stage 22.1% speedup compared to no GradSkip.",
            "effectiveness_qualitative": "The architecture is effective at producing scientifically relevant segmentation masks and is more stable in convergence than CosmoFlow. Key bottlenecks were I/O and evaluation frequency (evaluation triggered by steps can lead to much more frequent IOU computation), class imbalance required weighted loss, and at scale network congestion and data staging constrained batch-size scaling. System-level optimizations (pinned memory, data loaders, mixed precision, GradSkip) meaningfully improved throughput and reduced time-to-solution.",
            "impact_potential": "High — enables scalable detection of extreme weather events from large climate simulation datasets, improving analysis throughput for climate science and enabling near-real-time or large ensemble analyses previously infeasible due to compute/I/O limits. Demonstrated scalability to large HPC systems with modest compute budgets.",
            "comparison_to_alternatives": "Compared implicitly to earlier ResNet-based encoders and different optimizers — Xception backbone + LAMB offered improved large-batch and segmentation performance. GradSkip (open-division) provided a trade-off of small accuracy impact for reduced training cost; weighted cross-entropy addressed imbalance versus unweighted losses. Mixed-precision provided throughput improvements versus FP32.",
            "success_factors": "Success depended on (1) architecture choice (Xception + ASPP) for multi-scale feature extraction, (2) optimizer selection (LAMB) and hyperparameter tuning (warmup steps), (3) data pipeline optimizations (pinned memory, worker processes), and (4) techniques like GradSkip to reduce backward-pass cost while retaining accuracy.",
            "key_insight": "High-resolution, multi-channel segmentation models are effective for extreme-event detection, but their practical scalability is constrained by I/O, class imbalance, and communication; careful model, optimizer, and pipeline choices (LAMB, mixed-precision, GradSkip, pinned memory) substantially improve time-to-solution while meeting scientifically motivated IOU targets.",
            "uuid": "e2276.1",
            "source_info": {
                "paper_title": "MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Scaling & optimization techniques",
            "name_full": "Large-batch training, distributed parallelism, and system-level optimizations (mixed-precision, DALI, TF caching, GradSkip, hybrid model+data parallelism, learning-rate schedules)",
            "brief_description": "A collection of ML training methodologies and system optimizations applied to enable scalable scientific ML on HPC: includes large-batch training techniques (LR scaling/decay, warmup), hybrid data-and-model parallelism, mixed-precision training, data-loading acceleration (NVIDIA DALI), TF data caching, file compression/archiving for staging, and GradSkip to skip gradients for low-impact layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "General (applied across the CosmoFlow cosmology and DeepCAM climate segmentation benchmarks and intended for scientific ML workflows on HPC)",
            "problem_description": "Enable efficient, stable, and accurate training of large deep models on TB-scale scientific datasets across thousands to tens of thousands of HPC processors/accelerators while minimizing time-to-solution and respecting scientific quality targets.",
            "data_availability": "Varies by application; techniques were designed for abundant, labeled, simulation-derived datasets (TB-scale) which cannot all be held in on-device memory; staging to node-local storage and caching strategies are necessary.",
            "data_structure": "Techniques are applicable to dense high-dimensional data (3D volumes and large multi-channel images) and to workloads where per-sample size is large and I/O-bound.",
            "problem_complexity": "Complex: large per-sample memory footprint, heavy communication demands for distributed synchronous gradient aggregation, sensitivity of convergence to batch-size and optimizer hyperparameters, and I/O bottlenecks from parallel file systems.",
            "domain_maturity": "Emerging engineering best-practices for scientific ML at HPC scale; borrows from industrial large-batch training literature but adapted to scientific dataset characteristics and HPC system constraints.",
            "mechanistic_understanding_requirements": "Low-to-medium intrinsic — these are engineering and algorithmic techniques aimed at preserving predictive performance while improving throughput; interpretability requirements are task-dependent but techniques themselves are not focused on explainability.",
            "ai_methodology_name": "Distributed training strategies and training-time optimizations (large-batch methods, LR scaling & decay, warmup, mixed-precision, DALI data loader, TF data cache, GradSkip, hybrid model+data parallelism)",
            "ai_methodology_description": "Applied methods include: (1) Large-batch scaling with learning-rate scaling (multiply LR by batch-size factor or its sqrt) and bespoke decay/warmup schedules to maintain convergence; (2) Mixed-precision training (FP16/FP32) to reduce memory traffic and increase throughput; (3) NVIDIA DALI and multiple data-loader workers, TF data.Dataset.cache and staging to node-local SSD/RAM disk to mitigate I/O; (4) Hybrid data-and-model parallelism implemented by spatially partitioning Conv3D inputs (Mesh TensorFlow extensions) to enable scaling beyond batch-size limits; (5) GradSkip to skip gradients for early layers during later training steps identified as low-impact for a net speedup; (6) Increasing validation batch size to speed up evaluation; (7) Fine-grained communication scheduling and use of NCCL/Horovod with tuned tensor fusion thresholds and hierarchical all-reduce to maximize training throughput.",
            "ai_methodology_category": "Distributed supervised training engineering; large-batch optimization; hybrid parallelism",
            "applicability": "These techniques were essential and applicable for training large scientific ML models on leadership HPC systems; applicability depends on system memory (for caching), interconnect bandwidth (for communication-bound gradients), and dataset compressibility (for staging speedups).",
            "effectiveness_quantitative": "Reported improvements include: overall &gt;10× end-to-end performance improvements across system scaling; mixed-precision yielded 1.77× training speedup (on ABCI single GPU for CosmoFlow) and reduced memory traffic to ~50% in mixed precision vs FP32 in some measurements; DALI improved training by 1.26× and evaluation by 1.69× on a single GPU for CosmoFlow; increasing validation batch size improved evaluation by 3.03×; GradSkip provided cumulative speedups up to 22.1% in DeepCAM open submission; open-division hyperparameter optimizations enabled scaling of CosmoFlow local/global batch sizes from 512 to 2,048+ with reduced epoch overhead.",
            "effectiveness_qualitative": "Highly effective when appropriately combined and tuned; allowed both benchmarks to run at scales (many hundreds to tens of thousands of compute units) while meeting scientific targets. Limitations: large-batch training can increase epochs-to-converge unless mitigated by LR schedules, data augmentation or architecture-specific adjustments; hybrid model-parallelism reduces steps but increases communication (halo exchanges) which can become dominant unless overlapped with computation.",
            "impact_potential": "High — these techniques make it feasible to leverage leadership-class HPC for scientific ML, reducing wall-clock training time and enabling experiments and model development that were previously too expensive; they also inform system design (importance of on-node storage, memory capacity, network scheduling) for future HPC procurement.",
            "comparison_to_alternatives": "Compared to naïve scaling: pure data-parallel scaling without large-batch adaptation hits convergence limits at moderate global batch sizes (≥512 for CosmoFlow); hybrid model+data parallelism surpasses that limit but trades increased communication; mixed-precision outperforms pure FP32 in throughput and memory bandwidth usage but requires numerical care; GradSkip trades some layer update fidelity for reduced backward-pass cost and net speed gains.",
            "success_factors": "Success required matching algorithmic techniques (LR schedules, optimizer choice) to parallelism strategy, and ensuring system-level support (enough RAM to cache datasets, fast local SSDs, high-bandwidth interconnect, tuned collective implementations). Effective instrumentation and logging (mlperf-logging) and reproducible submission workflows helped quantify and validate gains.",
            "key_insight": "Achieving scalable scientific ML on HPC requires co-design: algorithmic adjustments (large-batch LR scheduling, GradSkip) and software/data-pipeline optimizations (mixed-precision, DALI, on-node caching) must be combined with appropriate parallelism (data, model, or hybrid) to overcome I/O, memory, and communication bottlenecks while preserving scientific convergence.",
            "uuid": "e2276.2",
            "source_info": {
                "paper_title": "MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CosmoFlow: using deep learning to learn the universe at scale",
            "rating": 2,
            "sanitized_title": "cosmoflow_using_deep_learning_to_learn_the_universe_at_scale"
        },
        {
            "paper_title": "Exascale Deep Learning for Climate Analytics",
            "rating": 2,
            "sanitized_title": "exascale_deep_learning_for_climate_analytics"
        },
        {
            "paper_title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
            "rating": 2,
            "sanitized_title": "large_batch_optimization_for_deep_learning_training_bert_in_76_minutes"
        },
        {
            "paper_title": "TECA: A Parallel Toolkit for Extreme Climate Analysis",
            "rating": 1,
            "sanitized_title": "teca_a_parallel_toolkit_for_extreme_climate_analysis"
        },
        {
            "paper_title": "MLPerf TM HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems",
            "rating": 2,
            "sanitized_title": "mlperf_tm_hpc_a_holistic_benchmark_suite_for_scientific_machine_learning_on_hpc_systems"
        }
    ],
    "cost": 0.018255249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MLPerf TM HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems</p>
<p>Steven Farrell 
Murali Emani 
Argonne National Laboratory
USA</p>
<p>Jacob Balma 
Maxwell Labs
USA</p>
<p>Lukas Drescher 
Swiss National Supercomputing Centre
Switzerland</p>
<p>Aleksandr Drozd 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Andreas Fink 
Swiss National Supercomputing Centre
Switzerland</p>
<p>Geoffrey Fox 
David Kanter 
Thorsten Kurth 
Argonne National Laboratory
USA</p>
<p>Peter Mattson 
Argonne National Laboratory
USA</p>
<p>Maxwell Labs
USA</p>
<p>Dawei Mu 
Maxwell Labs
USA</p>
<p>Amit Ruhela 
Kento Sato 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>¶ 
Koichi Shirahata 
Tsuguchika Tabaru 
Aristeidis Tsaris 
Jan Balewski 
Ben Cumming 
Swiss National Supercomputing Centre
Switzerland</p>
<p>Takumi Danjo 
Jens Domke 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Takaaki Fukai 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Naoto Fukumoto 
Tatsuya Fukushi 
Balazs Gerofi 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Takumi Honda 
Toshiyuki Imamura 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Akihiko Kasagi 
Kentaro Kawakami 
Shuhei Kudo 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Akiyoshi Kuroda 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Maxime Martinasso 
Swiss National Supercomputing Centre
Switzerland</p>
<p>Satoshi Matsuoka 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Henrique Mendonça 
Swiss National Supercomputing Centre
Switzerland</p>
<p>Kazuki Minami 
RIKEN Center for Computational Science
xvi National Institute of Advanced Industrial Science and Technology
Japan University of Virginia
USA, Japan</p>
<p>Prabhat Ram xivTakashi Sawada 
Mallikarjun Shankar 
Tom St 
John 
Akihiro Tabuchi 
Venkatram Vishwanath 
Argonne National Laboratory
USA</p>
<p>Mohamed Wahib 
Masafumi Yamazaki 
Junqi Yin </p>
<p>Lawrence Berkeley National Laboratory
USA</p>
<p>MLPerf TM HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems</p>
<p>Abstract-Scientific communities are increasingly adopting machine learning and deep learning models in their applications to accelerate scientific insights. High performance computing systems are pushing the frontiers of performance with a rich diversity of hardware resources and massive scale-out capabilities. There is a critical need to understand fair and effective benchmarking of machine learning applications that are representative of real-world scientific use cases. MLPerf TM is a community-driven standard to benchmark machine learning workloads, focusing on end-to-end performance metrics. In this paper, we introduce MLPerf HPC, a benchmark suite of largescale scientific machine learning training applications, driven by the MLCommons TM Association. We present the results from the first submission round including a diverse set of some of the world's largest HPC systems. We develop a systematic framework for their joint analysis and compare them in terms of data staging, algorithmic convergence and compute performance. As a result, we gain a quantitative understanding of optimizations on different subsystems such as staging and on-node loading of data, compute-unit utilization and communication scheduling enabling overall &gt; 10× (end-to-end) performance improvements through system scaling. Notably, our analysis shows a scale-dependent interplay between the dataset size, a system's memory hierarchy and training convergence that underlines the importance of nearcompute storage. To overcome the data-parallel scalability challenge at large batch-sizes, we discuss specific learning techniques and hybrid data-and-model parallelism that are effective on large systems. We conclude by characterizing each benchmark with respect to low-level memory, I/O and network behaviour to parameterize extended roofline performance models in future rounds.</p>
<p>Index Terms-Deep Learning, Benchmarks, Supercomputers, Scientific Applications</p>
<p>I. INTRODUCTION</p>
<p>Scientific applications are leveraging the potential of machine learning (ML) and, especially, deep learning to accel-erate scientific discovery. This trend is prevalent in multiple domains, such as cosmology, particle physics, biology and clean energy. These applications are innately distinct from traditional industrial applications with respect to the type and volume of data and the resulting model complexity. Recently, ML-driven applications have led to major insights and scientific discoveries that would have taken years using traditional methods. A recent breakthrough in addressing one such grand challenge in biology was the development of an ML model that solved the protein folding problem with the AlphaFold tool by Alphabet's Deepmind [1]. Large-scale experiments will yield unprecedented volumes of scientific data. The authors of [2] highlight the challenges faced with understanding the exponential growth of experimental data and present opportunities of using machine learning techniques to advance science. The AI for Science report [3] put forth by stakeholders from leadership computing facilities, DOE labs, and academia details a vision for leveraging AI in science applications critical to US DOE missions. The vision outlined in this report emphasizes the need for a systematic understanding of how these applications perform on diverse supercomputers.</p>
<p>An ideal benchmark suite will help assess HPC system performance while driving innovation in systems and methods. Developing one is difficult because of the inherent trade-off between building generalizable and representative proxies of real ML workloads, and building high-fidelity proxies that probe specific features of an HPC system. Benchmarks of the former type are general enough to capture what the average user on a large multi-user system is doing with AI while benchmarks of the latter type focus on how a specific kernel performs on the system. Implementation of ML models on supercomputers at full scale poses challenges that are not typically exposed at small-scale, such as the I/O impact of large-scale datasets. Hence, adopting existing benchmarking approaches for scientific machine learning problems would not be able to capture realistic behaviour of the scientific applications.</p>
<p>There have been significant efforts in benchmarking supercomputers with traditional HPC workloads with major ones listed in Table I. TOP500 [4] ranks supercomputers across the world and publishes their performance numbers (in Flops) with High Performance Linpack (HPL). It captures many of the general features that large-scale scientific applications share, such as domain decomposition and heavy use of linear algebra solvers. A single benchmark can be run across the entire system in a weak-scaling fashion. Green500 [5] ranks supercomputers based on their energy efficiency. The list reports the performance per rated watt using the LINPACK benchmark at double precision format. Several benchmarking efforts have previously aimed to characterize performance of machine learning workloads, including Deep500 [6], HPCAI500 [7], and HPL-AI [8]. The largest dataset used across these attempts is obtained from the Extreme Weather Dataset [9] of about 1.65 TB. Other benchmarks aimed at analyzing model performance include DAWNBench [10], DeepBench [11], Fathom [12], ParaDNN [13], HPE DLBS [14], and XSP [15]. The challenges and limitations of existing benchmarking efforts drive the need to develop a benchmark suite with science applications that run at scale.</p>
<p>In this paper, we present MLPerf HPC, a benchmark suite aimed to address the limitations of prior efforts. This benchmark suite is a part of MLPerf, driven by MLCommons [16], an open engineering consortium that aims to accelerate machine learning innovation through MLPerf benchmarks, public datasets, and best practices. MLPerf Training benchmarks [17] aim to measure the performance of training models while MLPerf Inference benchmarks [18] aim to measure how fast systems can produce results using a trained model. MLCommons takes a neutral stand about any form of comparison of results across submissions. The primary contributions of this work are: 1) Introduce the MLPerf HPC benchmark suite, the newest ML training benchmark suite from the MLCommons consortium, and the first to specifically focus on scientific ML applications relevant for HPC systems (section II). We will describe the first two benchmark applications, CosmoFlow and DeepCAM, as well as the benchmarking methodology and process (section III). 2) Present results from the inaugural MLPerf HPC submission round in 2020 (section IV). These results feature measurements from leading supercomputing platforms around the world, innovations in scalable model-and-data-parallel training and learning algorithms, and the largest scale MLPerf submission to date. 3) Develop a systematic framework for the joint analysis of the publicly available MLPerf HPC submission results to clearly understand and compare submissions in terms of data staging, algorithmic convergence and system compute performance in a condensed set of plots that enable new insights (subsection IV-A). 4) Gain a quantitative understanding of how system-level optimizations lead to &gt; 10× improvements in end-toend performance for both benchmarks (closed division), what constrains these submissions from scaling further and discuss alternative learning techniques that enable further improvements by a factor of 1.1-3.4× (open division, subsection IV-B). 5) Introduce techniques to characterize memory, network, and I/O behaviours of the benchmark applications across all involved systems, relate them to MLPerf HPC submission results and, hereby, lay the groundwork for a future characterization of the submissions in extended roofline performance models [19] (section V).</p>
<p>II. MLPERF HPC BENCHMARK SUITE</p>
<p>The MLPerf HPC benchmarks are holistic in the sense that they capture critical features of emerging scientific machine learning workloads: massive data volume, large complex models, and training schemes that incorporate data-parallelism, model-parallelism, and hyper-parameter optimization. The goal is to drive innovation in HPC system and software design for machine learning workloads, especially those applications that depend heavily on accelerator devices for fast compute, interconnects for high-bandwidth, low-latency communication, or I/O subsystems that govern the rate at which data can be accessed. Additional requirements of the benchmark suite, such as training to convergence, profile generation, and coarsegrained time reporting enable each individual benchmark's performance to be characterized relative to its utilization of a system's I/O, communication, memory, and compute capabilities. This makes the MLPerf HPC benchmark suite uniquely capable of characterizing the ability of existing HPC systems to run an exciting class of new workloads, while simultaneously providing engineers a standard set of benchmarks for informing the design of tomorrow's large-scale high performance computers. The MLPerf HPC benchmarks are also unique in that they offer performance characterization capability for state-of-the-art practices in scientific discovery. The emerging domain of coupling simulations with AI motivates the first two benchmarks included in the suite, CosmoFlow and DeepCAM. The reference implementations of these applications are available at [20]. Both benchmarks incorporate massive datasets based on simulations and predict important parameters with unprecedented accuracy in their respective domains -cosmology and extreme weather. Details for each benchmark is provided in the following subsections.</p>
<p>Future benchmarks for the suite will be community driven, aiming to share similar characteristics with CosmoFlow and DeepCAM while also expanding the relevance of the benchmark suite to other scientific regimes where AI is quickly becoming an important tool for discovery, such as computational biology, materials science and personalized medicine. </p>
<p>A. CosmoFlow</p>
<p>The CosmoFlow benchmark is based on the work by Mathuriya et. al. [21], continued by the ECP ExaLearn project [22]. The task is to predict cosmological parameters from the distribution and structure of dark matter in the universe. The dataset comes from N-body cosmology simulations produced by the ExaLearn team [23] binned into 3D volumetric histograms of size 512 3 with four channels representing different red-shift values. These massive volumes present considerable challenges for training models due to large memory footprint, and so, similar to what is done in [21], the samples are split into smaller cubes of size 128 3 with four red-shift channels. The target quantities are four cosmology parameters, Ω M , σ 8 , n s , and H 0 , which are important to describe the evolution of the universe. The final dataset used for this benchmark has 262,144 samples for training and 65,536 samples for testing and is stored in TFRecord [24] files. The CosmoFlow reference model was adapted from [25] which introduced some modifications with respect to the original published work. The model is a 3D convolutional neural network with five convolutional layers and three fullyconnected layers. Each convolutional layer has kernel size 2 with 32 × i filters in the ith layer. The first two fully connected layers have sizes 128 and 64, respectively. The final layer has output size 4, corresponding to the predicted target quantities. All hidden layers have leaky ReLU activations, with the exception of the output layer which has a tanh activation scaled by a factor 1.2. After each convolutional layer, there is a 3D Max-Pool operation reducing the sample size by half along each dimension. Finally, the model has dropout layers after the first two fully connected layers with dropout probability 0.5. The model is trained with a mean-squared-error (MSE) loss function and the standard SGD optimizer with a baseline learning rate schedule consisting of an initial learning rate of 0.001 which is dropped to 2.5 × 10 −4 at 32 epochs and 1.25 × 10 −4 at 64 epochs. The baseline global batch size is set to 64. To scale above this configuration, the reference implementation multiplies the baseline learning rate by the factor of batch size increase, respectively its square-root.</p>
<p>The target quality is chosen to be mean-absolute-error (MAE) &lt; 0.124 when scaling the batch size and learning rate above the reference configuration. CosmoFlow training exhibited high variability in the number of epochs to converge, which motivated a requirement of 10 training runs to get a reliable measurement of the time to train.</p>
<p>B. DeepCAM</p>
<p>DeepCAM [26] implements a convolutional encoderdecoder segmentation architecture trained on CAM5 climate simulation data [27] with TECA generated heuristic segmentation masks [28] to identify extreme weather phenomena such as atmospheric rivers and tropical cyclones. DeepCAM was the first deep learning application which scaled to the full OLCF Summit system [29] and was awarded the ACM Gordon Bell Prize in 2018 [30]. Since then, the model was developed into its current form: the ResNet-50 [31] encoder backend was replaced with an Xception [32] network, batch normalization was re-introduced and LAMB [33] replaced the original ADAM/LARS optimizer [34], [35]. The most notable features of this network are 20 residual blocks comprised of depthwise-separable convolutions, which are themselves comprised of grouped convolutions with maximal group count, followed by a point-wise convolution. The bottleneck layer employs atrous spatial pyramid pooling [36] with various filter sizes, and global average pooling to extract features at different scales. The results of those operations are concatenated and fed to the deconvolutional decoder. Outside the residual blocks, the network has a single skip connection which propagates low level features directly to the decoder without routing them through the bottleneck layer.</p>
<p>The network takes 16 × 1152 × 768 sized input tensors and predicts 1152×768 sized segmentation masks for three classes (background, tropical cyclone/hurricane, atmospheric river).</p>
<p>There are 121,266 training and 15,158 testing samples and no data augmentation is used. DeepCAM is trained with weighted cross-entropy loss, to account for the high class imbalance (about 95% of the pixels are background). The target score is the intersection-over-union (IOU) between the predictions and the targets. The scientifically motivated target score is 0.82, which corresponds to a similarity of 82%.</p>
<p>It is critical to understand what differentiates these benchmarks from typical commercial applications. CosmoFlow is trained on volumetric 3D data, rather than the 2D data commonly employed in training image classifiers. DeepCAM is trained on images with 768 × 1152 pixels and 16 channels, which is substantially larger than standard vision datasets like ImageNet, where the average image is 469 × 387 pixels with at most 3 or 4 channels. Moreover, the massive dataset sizes, 5.1 TB for CosmoFlow and 8.8 TB for DeepCAM, are over an order of magnitude larger than ImageNet (150GB) and introduce significant I/O challenges that expose storage and interconnect performance limitations. These characteristics offer unique performance differences from industrial machine learning workloads which must be addressed to optimize future systems where ML-augmented science applications are expected to play a critical role.</p>
<p>III. BENCHMARKING PROCESS</p>
<p>The MLPerf HPC benchmarking methodology is closely modeled after the MLPerf Training benchmark, including the general design, metrics, division rules, and submission and review procedures. For instance, the MLPerf HPC benchmarks use the same holistic and user-centric view of performance as MLPerf Training benchmarks and report time to train as the primary metric. This choice creates a metric that is directly relevant to users and customers that captures end-toend performance including both system speed and accuracy. A few changes were made in the rules to improve the relevance of the benchmarks for scientific workloads in the HPC setting. For example, we introduced a rule to include data staging in the measured benchmark time to capture the impact of datamovement for massive scientific datasets on large HPC parallel file systems and node-local accelerated storage.</p>
<p>A. Measurement</p>
<p>Here we describe the finer details and rules relating to measuring time to train performance in the benchmarks.</p>
<p>1) Divisions: MLPerf HPC has two types of submissions, closed division and open division. In the closed division, the submissions need to be equivalent to the reference implementation. This means that they must have mathematically equivalent model definitions and training algorithms. Such a process enables a direct comparison of the systems. In the open division, submitters are allowed to change the model architectures and training procedure freely but are restricted to evaluate the quality metric in the same way as the reference. This division aims to encourage innovations to further optimize the benchmarks.</p>
<p>2) Timing rules: At the start of a run, the benchmark dataset must reside on the parallel file system of the HPC center and on-node caches must be reset. We do not require resetting system-level caches because it is difficult to perform consistently across different systems and is often extremely disruptive. The benchmark timer begins as soon as the dataset is touched, which includes staging into node-local storage such as an on-node SSD or RAM, if capacity is sufficient. The timer stops when the convergence criteria, as described in the rules, is met (Table II).</p>
<p>3) Run results: ML model training is inherently stochastic due to random initialization, dataset shuffling, etc. Therefore, to get an accurate measurement of the expected time to train, submitters must run the benchmarks a specified number of times to convergence. In the final scoring, we drop the fastest and slowest results and report the arithmetic mean of the remaining measured times.</p>
<p>4) Logging:</p>
<p>The benchmarks use the mlperf-logging library [37], which provides logging utilities and helper functions for all submissions. These help in collecting metadata and evaluating if the submissions meet compliance checks with the set run rules.</p>
<p>B. Submission</p>
<p>The submission process is designed to be fair, robust, and reproducible. This is achieved through the enforcement of a required structure for submissions and a peer review process. A submission schedule specifies when benchmarks and rules are finalized, when the submission window opens, the deadline for all submissions, as well as the schedule for the reviews and final deadline for results publication. A submission can be made to either open or closed divisions. A submitter can make as many submissions in either division and each submission entry can vary in the amount of system resources used.</p>
<p>1) Structure: Reproducibility is a key goal for MLPerf HPC. Accordingly, submitters must upload their full code used to produce results, as well as system descriptions and the result log files containing the timing information. The submissions must conform to a specified file and directory structure and naming scheme for parsing, summarizing, and peer-review. The required submission structure is described in [38].</p>
<p>2) Review: After the submission deadline, the peer-review process begins. A set of scripts from the mlperf-logging library are first used to check submissions and log files for compliance with the rules. Then, submitters review each other's implementations and results to further verify that they are compliant, sensible, and comprehensible. During the review stage, submitters are also allowed to perform "hyperparameter borrowing", in which they may perform additional sets of training runs using the hyperparameter settings of other submissions (but still using their original implementations). This discourages excessive hyperapameter tuning by submitters and avoids giving unfair advantages to teams with greater computational resources. </p>
<p>IV. RESULTS</p>
<p>The inaugural MLPerf HPC submission round (v0.7) † took place during the summer of 2020. The results from submissions on 7 supercomputers, released in November, showcased the capabilities of HPC systems listed in Table III, for training large-scale scientific problems. The results are summarized in Table IV for both closed and open divisions. At first glance, we observe a system scale range of up to a factor of 8-16× in closed division for both benchmarks and results showing &gt; 10× improvements in time to train across this range. Furthermore, we can see the effect of innovations in open division by comparing each of these submissions to the fastest submission on the same system in closed division. For instance, on the CPU-based system Fugaku, there is an 3.38× improvement in open division for CosmoFlow while for the GPU-based system ABCI, there are speedups of 2.61× and 1.12× in open division for CosmoFlow and DeepCAM respectively. * In this section, we present a detailed analysis of the submissions in a framework that allows to clearly understand and separate the effects of data-staging, algorithmic convergence and system compute throughput (training and evaluation) in time to solution from the submitted logs (subsection IV-A) and complement this with a set of highlights on implementations from few systems (subsection IV-B). † naming chosen to be consistent with submission round in MLPerf Training * These numbers are comparing time to solution in open and closed division submissions from the same system, which does not necessitate equal number of processors/accelerators in both submissions (cf . Table IV).</p>
<p>A. Analysis</p>
<p>The time to solution broken into data staging, training and evaluation components is shown in Fig. 1 for each submission.</p>
<p>1) Discussion of data staging time: The data staging time, T staging , is shown in Table V for the systems where it was measured. We observe that it is very different for the two benchmarks on ABCI -by interpolation to 1,024 GPUs, staging is handled more than 5× faster (in absolute time) for CosmoFlow than for DeepCAM. This difference comes not only from the fact that DeepCAM's data set is 73% larger than CosmoFlow's, but also from the data compression ratio, which is 88% for CosmoFlow compared to only 23 % for DeepCAM † .</p>
<p>To understand the relative importance of staging (T staging ) in time to solution, T solution , we assume that T solution = T staging + T compute + T extra (1) † As a consequence, compression was not applied in data staging for DeepCAM.   With T compute = T epoch · #epochs and T extra ≈ 0 (Fig. 1
), where T epoch is the average epoch time, we get T staging T solution ≈ T staging /T epoch T staging /T epoch + #epochs(2)
The ratio T staging /T epoch , thus, quantifies the relative overhead in units of compute epochs that staging adds to time to solution irrespective of convergence ‡ or the exact amount of data used § . Since it relates the compute to staging throughput, it is, however, dependent on the model's computational complexity. As DeepCAM is more compute-intensive than CosmoFlow, the relative overhead in "extra compute epochs" shown in the last column of Table V is reduced to a factor of 2.5 − 3.5× that of CosmoFlow from what is expected purely from dataset compressibility. The reason for DeepCAM on ABCI still having 10× the share of staging compared to CosmoFlow, is that CosmoFlow requires 4× more epochs to converge (Table VI), which causes all its submissions to have marginal share of staging (&lt; 5% in Figure 1). Finally, Fugaku's staging times are the highest at 8,192 ‡ This number depends on the batch size through T epoch , though. § As long as it does not traverse a capacity limit in the memory hierarchy closer to compute units than the staging target (e.g. RAM capacity if staging to on-node SSD). processors due to 16-way replication of the data set (4-way for 16,384). This is an overhead of model-parallelism and could be avoided by partitioned reading and broadcasting of data across MPI ranks within each model-parallel group. These findings show that the overhead of staging is highly application-specific, depending on data compressibility, but also a model's computational complexity and convergence and generally affects smaller systems with fewer epochs to converge more than larger ones.</p>
<p>2) Analysis of compute time: This subsection presents an analysis of the time spent in training and evaluation, T compute in eq. (1), after data staging is completed. It represents ≥ 90% of time to solution in CosmoFlow and ≥ 80% in DeepCAM (Figure 1).</p>
<p>(a) Compute analysis for CosmoFlow: We observe that the number of epochs to convergence in T compute = T epoch · #epochs is primarily an algorithmic property of SGD (holding the data and model fixed), and as such, only dependent on the optimizer and choice of hyperparameters, but not the particular system or implementation (up to floating point precision). On the other hand, T epoch is a system-specific property that depends on both the hardware and the specific parallel implementation of the model as well as the choice of optimizer, but not necessarily on all hyperparameters. In fact, from the rule set in Table II, T epoch only depends on the value of the batch size hyperparameter. Therefore, we provide an analysis of the epoch throughput T −1 epoch and number of epochs to converge separately for each of the submissions by means of Figure 2.</p>
<p>Submission parameters: We focus on the number and type of compute unit (accelerators for GPU-based systems, processors for CPU-based systems) to characterize the system and the batch size as most important parameters. The choice of these parameters for each submission is shown in Figure  Fig Table IV that characterizes the data-parallel unit per train step, we can observe that all CosmoFlow submissions with a ratio ≥ 1 chose a purely data-parallel implementation -Piz Daint being the only with a local batch size of 2, all others 1 -whereas those with a ratio &lt; 1, i.e. Fugaku-8192 and Fugaku-16384, used model-parallelism in addition within each data-parallel unit of 16 and 4 CPUs respectively, resulting in a hybrid form of model-and data-parallelism that will be discussed in subsection IV-B1. We note that submissions with the same batch size to number of compute units ratio (constant along diagonal lines) that use the same data-parallel unit (Table IV)  Epoch scaling: In Figure 2 (b), we show the scaling of epochs required to converge as a function of the batch size (dependent variable on the x-axis). As discussed above, this is a system-independent property up to floating point precision. The level lines along the diagonal identify points of an identical number of train steps to solution, which puts a limit on data-parallel scalability. That is, once an increase in the batch size leads to a larger number of train steps to solution, a system can no longer train a model faster by growing the compute resources proportionally (ignoring caching effects in the memory hierarchy). The submissions ABCI-2048 and Fugaku-16384 are close to this limit (cf. Figure 4(a)) and the specialized techniques to converge at these very large batch sizes will be discussed in greater detail in subsection IV-B1. The remaining submissions all closely follow the reference implementation described in subsection II-A. This turns out to scale efficiently to a batch size of 256 with only 1.3× more epochs to converge for 4× batch size increase from 64, but past this point becomes significantly harder to train and less stable (1.6× epochs for doubling the batch size). Closed division submissions were limited to a maximum batch size of 512 due to convergence issues at batch size 1,024 that were only overcome by Cori-KNL-1024 in open division with a slight modification of the learning rate schedule (section IV-B3) .</p>
<p>Throughput scaling: In Figure 2 (c), compute throughput is shown as a function of the number of compute units and, implicitly, the batch size through the shared x-axis with Figure  2 (a). For each submission, we plot sample throughput (left axis) for training ( ) and evaluation (×) as well as the combined sample throughput (# samples/T epoch , distribution with mean at • || ) according to the split of the data set (80% training and 20% evaluation) at the abscissa corresponding to the number of compute units (illustrated on the ABCI-512). On the diagonal we find lines of constant per-computeunit throughput, commonly used to analyze scaling efficiency. Dividing the combined throughput by the overall number of samples in the data set, we obtain the epoch throughput, T −1 epoch , which can be read off for the distribution from the right scale. Note that throughput is only algorithmically relevant together with a specified batch size, so that Figure 2 (c) and (a) are meant to be read together. The resulting pair of plots allows us to understand the scaling efficiency of training, evaluation, and combined throughput for submissions that relate through weak (same data-parallel unit in Table IV and same diagonal in Figure 2 (a)) or strong (same batch size in Figure 2 (a)) scaling, or extrapolation thereof, and compare different systems with each other.</p>
<p>Interestingly, we observe that for GPU-based systems, there is a transition occurring from smaller systems to those with 256 GPUs and more, where the gap between training and evaluation throughput becomes very high. Evaluation is inherently bound by data access speed in CosmoFlow and it turns out that the memory configurations of these systems is exactly such that the larger ones, i.e. Piz-Daint-256, ABCI-512 and ABCI-2048, benefit from caching the dataset in RAM. As a result, these systems spend very little (&lt; 5%) of their time in evaluation ( Figure 1). This is different for the CPUbased systems that are used at larger processor count and, thus, higher RAM capacity. To reduce the I/O-related performance penalty, smaller systems could selectively cache the evaluation dataset in RAM.</p>
<p>At the other end of the scale, we can observe the network effects on training throughput. Specifically, the difference in scaling efficiency between training and evaluation for submissions related by weak-scaling shows that training at scale increasingly becomes network-bound whereas evaluation with its low communication overhead almost scales ideally. This is the case for e.g. ABCI-512 and ABCI-2048 -the system with the highest measured throughput in this round -with 63.2 % efficiency for training vs. 98.2 % for evaluation and similarly CoriKNL-512 and CoriKNL-1024 (92.6 % for training vs.</p>
<p>% for evaluation).</p>
<p>A further interesting point is the relation of HAL-64, || The curved lines from training ( ) and evaluation (×) rooted at the combined mean (•) illustrate the averaging by holding the point cloud together.</p>
<p>CoriGPU-64 and ABCI-512 that all have the same accelerator type but a different CPU-to-GPU ratio. Without the optimizations on ABCI described in IV-B2, its per-GPU training performance is similar to HAL-64. Both of these systems have one CPU per 2 GPUs and the same number of GPUs per node. In contrast, Cori-GPU represents a more consolidated system with 8 GPUs per node, which reduces the load on the network, but also with smaller CPU resources per GPU at one CPU per four GPUs. This difference can play a role in implementations with significant CPU-overhead to keep the GPUs busy and here in parts explains the uniform difference seen in training and evaluation throughput between HAL and Cori-GPU. Compute time: Figure 2(d) shows T compute -the outcome of the competition between epoch and throughput scaling when growing the system. To obtain it, we join the epoch scaling ( Figure 2b) and throughput scaling (Figure 2c/a) of each submission on the batch size along the shared axes. The compute time T compute = T epoch · #epochs is constant along diagonal lines indicated. We observe that while Fugaku-512 has a higher sample throughput than HAL-64, the smaller batch size of HAL-64 causes it to still converge slightly faster in time overall (similarly for Cori-KNL-1024 and Piz-Daint-256). As a further insight, we are able to trace back the run-to-run variation in time to solution, which is 2.8% for ABCI-2048, Fugaku-16384 and 11.1% for all other CosmoFlow submissions, to the number of epochs rather than system throughput. This is confirmed by a logarithmic principle component analysis (log-PCA, Figure 3), which shows a first component with strong horizontal alignment in all submissions. Furthermore, we see that Frontera-RTX-64 exhibits especially high relative throughput variability, which is explained by the parallel Lustre filesystem from where data is loaded continuously during training and that is shared by thousands of users at any time. Finally, the technique of grouping throughput-and epoch-scaling plots together in this setting allows the inexpensive prediction of compute time to convergence at system configurations other than the ones used in the submissions. This can be done either by additional throughput measurements in Figure 2(c) at batch size with known epochs to convergence or approximately ** by dataparallel extrapolation of throughput along diagonal lines of ** Ignoring network effects. constant throughput per compute unit † † and joining that value with the known epoch scaling from Figure 2(b) on the batch size. Further details on CosmoFlow's results are presented in section IV-B.</p>
<p>(b) Compute analysis for DeepCAM: Figure 1 shows that more than a third (36.3%) of time to solution is spent in the evaluation phase on Cori-GPU compared to only 4% (closed) and 1 % (open) on ABCI. The reason for this is not primarily the different evaluation throughput (Table  VI), but that evaluation is triggered after a fixed number of training steps instead of once per epoch and delayed in open division. As a consequence, Cori-GPU calculates the IOU score 8× and 35× more often than ABCI's closed and open submissions respectively. We omit a graphical analysis of DeepCAM similar to CosmoFlow here due to the lack of space (it is available from the linked artifact in the appendix).</p>
<p>(c) Compute analysis comparison: Table VI summarises the compute characteristics of both applications in the benchmark suite on Cori-GPU and ABCI. DeepCAM requires only 20-25% the number of epochs to converge of CosmoFlow, but the epochs growth rate as a function of the batch size is similar for both applications. DeepCAM, however, exhibits much more stable convergence (1.7% per-run std deviation of time to solution vs. 11.1% for CosmoFlow submissions following the reference implementation). For sample throughput, we find that (1) CosmoFlow has higher throughput in both training (3 − 5×) and evaluation (7 − 20×), which can be attributed to the lower number of layers, whereas (2) DeepCAM has a much smaller gap between training and evaluation throughput (1.4× vs. 5.7×). Comparing the resource footprint, we find that to reach convergence, the compute budget (time to solution in hours × number of accelerators/processors) is 1.5 − 2.6× larger for CosmoFlow than for DeepCAM, with correspondingly higher time to solution that is 2.6 − 2.9× that of DeepCAM for closed division. Notably, though, we see for both benchmarks that despite a relatively large increase in number of accelerators by 8 − 16×, the compute budget with the right optimizations can be controlled and a decrease in ∼ 10× time to solution is reliably possible.</p>
<p>B. Highlights</p>
<p>In this section, we present details of the implementations and present highlights from v0.7 submissions.</p>
<p>1) Fugaku: Submissions on the Fugaku supercomputer to CosmoFlow utilized hybrid data-and model-parallel execution. To reduce staging time, data reformatting by compressing and archiving multiple files was effective. Training data is staged in RAM disks in advance. Only in the case of the 512node submission, RAM disks do not have enough capacity, so data staging is performed to local SSDs on I/O nodes that are assigned to each unit of 16 nodes. Also, the data cache function of TensorFlow (tf.data.Dataset.cache) is used to improve the bandwidth of data loading during training. † † Weak scaling: batch size/# compute units = const., ignoring effects of dataset size For the open division, the following accuracy improvement techniques were applied: (1) use linear learning rate decay schedule, (2) apply data augmentation, (3) disable dropout layers. When applying the three techniques in isolation at batch size 1,024, the number of epochs until reaching a relaxed target of MAE &lt; 0.15 (cf . Table II) is (1) 64, (2) 68, (3) 19, compared to 67 without any of the three. Although disabling dropout layers is the most effective among the three, none of them converges to the target MAE &lt; 0.124 alone. Applying all the three techniques together is necessary to increase the batch size above the closed division bottleneck of 512. The resulting epoch scaling for the open division is shown in Figure 4(a) together with extrapolated time to solution for data-parallelism on Fugaku (based on the submission at 512 CPUs and ideal throughput scaling ‡ ‡ ). We see that the time to solution ideally scales up to batch size 4,096 with dataparallelism at local batch size one, which was confirmed experimentally on Fugaku. To summarize, the result of open division optimizations on the CosmoFlow submissions can be seen in Figure 2b -the batch size can be increased from the closed division bottleneck at 512 to 2,048 without adding additional epochs overhead (&gt; 4X reduction in train steps in ABCI-2048, where the same techniques are applied) and, alternatively, -slightly less efficiently -to 4,096 with 1.4X extra epochs (&gt; 5X reduction in train steps in Fugaku-16384).</p>
<p>Since the accuracy could not reach the target using a batch size larger than 4,096 even after extensive hyperparameter ‡ ‡ Using T solution ∝ T staging /T epoch + #epochs from equation 1. tuning, model parallelism is necessary to scale beyond 4,096 processors. Therefore, a hybrid approach utilizing data-and model-parallelism is implemented for CosmoFlow on Fugaku. Model parallelism is implemented by extending Mesh Ten-sorFlow so that multi processing can be applied for both data and model parallelism, and the hybrid parallelism is applied to Conv3d layers, that have a high spatial locality, by partitioning input tensors spatially in two dimensions (Figure 4(b)).</p>
<p>To compare the hybrid data-and-model parallelism to pure data parallelism, Figure 5 shows a breakdown of the elapsed time for the first two epochs in CosmoFlow. The communication share increases as the number of CPUs increases in both cases. But whereas the scalability of data parallelism is limited mainly by the increase of the number of epochs ( Figure  4(a)/2b), that of the hybrid parallelism is constrained by the increase of communication time (Figure 5(b)). As a result, the hybrid parallelism enabled scaling the number of CPUs in the submissions up to 8,192 with 4x4 spatial partitioning for closed division and 16,384 with 4x1 spatial partitioning for open division (2.62× and 1.98× speedup of training throughput by model-parallelism compared to Fugaku-512, resp. an extrapolation thereof to batch size 4,096, Figure 2(c)). There is still room for improving the scaling efficiency of the hybrid parallelism further by reducing the communication overhead. In particular, we expect that parallelizing the halo-exchange of the Conv3D-layers within the model-parallel group with the evaluation of the layer in the non-halo region will give a significant speedup.</p>
<p>2) ABCI: For both of the benchmarks, data reformatting by compressing and archiving multiple files was effective to reduce data staging time. Data shuffling was applied only among intra-node GPUs after each epoch, since the dataset is too large to fit on local storage and a partial dataset is shared only intra-node after data staging.</p>
<p>For CosmoFlow, the following performance optimizations were applied to improve training and evaluation throughput: (1) improve data loader bandwidth using NVIDIA Data Loading Library (DALI), (2) apply mixed-precision training, (3) increase validation batch size. When applying the three optimizations sequentially on a single GPU, DALI is effective for both training (1.26X speedup) and evaluation (1.69X), adding mixed precision is effective only for training (1.77X), and increasing validation batch size further improves evaluation (3.03X). With data-parallelism, training time scales up to batch size 512 at a local batch size one for closed division. For open division, after the hyperparameter tuning techniques mentioned in section IV-B1 were applied, batch size 2,048 with local batch size one was optimal. Using a larger batch size than 2,048 did not achieve additional speedup because network bandwidth degraded due to congestion and the number of epochs increases significantly (Figure 4(a)). We have already seen a manifestation of this in the weak-scaling analysis of ABCI-512 and ABCI-2048 in subsection IV-A2 when comparing training and evaluation scaling efficiency (Figure 2c).</p>
<p>For DeepCAM, page-locked memory (a.k.a pinned memory) is used to improve memory bandwidth, and four additional worker processes were forked for data loading to improve I/O bandwidth. Hyperparameters were tuned to reduce the number of epochs to convergence. Training time scales up to batch size 2,048 with local batch size two for closed and open divisions after hyperparameter tuning. Especially tuning the warmup steps was effective to reduce the number of epochs to convergence. For the open division submission, the Gradient Skipping (GradSkip) technique [47] was applied that avoids updating weights in some layers in the training process, by finding layers which have little effect on accuracy, based on automatic analysis of the content of data during training. Effectively, GradSkip skips the gradient calculation of these layers in the backward pass. For our submissions, we apply it in two steps. After an average of 53.8% train steps, we skip the first 62 out of 301 layers (12.4% speedup) and after another 20.1% train steps, we skip the first 260 layers (22.1% speedup over no GradSkip).</p>
<p>3) Cori/Cori-GPU: Submissions on the Cori supercomputer at NERSC utilized both the primary KNL partition as well as the Cori-GPU testbed. System details are available at [41], [42].</p>
<p>CosmoFlow was trained on Cori KNL on 512 nodes in the closed division and 1024 nodes in the open division. For the open division submission, an additional learning rate decay by a factor of 0.5 at 96 epochs was added to the schedule in section II-A in order to enable convergence at global batch size 1024. The implementation used Intel-optimized TensorFlow with MKL-DNN for optimized performance on the Intel processors. Runtime settings for inter-and intraparallelism threads, OpenMP threads, and affinity were tuned for maximal throughput. Shifter containers were used to launch training, which prevented scalability issues in shared library loading from the parallel file system at scale. These results show that large CPU systems like Cori can still be useful for training computationally-expensive deep learning models.</p>
<p>On an 8-node Cori-GPU system (64 V100 GPUs), Horovod with NCCL-based allreduce was used to achieve efficient dataparallel training. Additionally, node-local SSDs were used to store local partitions of the full dataset. The staging time from the Cori scratch filesystem to the node-local SSDs was considerably longer than other submissions with data-staging, indicating there is further room for optimization. DeepCAM was similarly trained on the Cori-GPU system utilizing 64 V100 GPUs. The implementation in PyTorch utilized the NVIDIA Apex library for automatic mixed precision and used NCCL for optimized distributed data-parallel training.</p>
<p>4) Piz Daint: Submissions on Piz Daint [39] at CSCS focused on two data-parallel configurations in the closed division of CosmoFlow with 128 and 256 GPUs, one GPU per node. Sarus [48], a container engine with near-native performance for Docker-compatible containers, was used to rapidly test and tune distributed training with Horovod and NCCL for finegrained communication to obtain near optimal weak scaling in the range of 100-1000 nodes ( Figure 6). A low cycle time, tensor fusion threshold and the usage of the hierarchical, treebased all-reduce implementation proved to be key to achieve this performance. Single node optimizations within Tensorflow had a comparatively smaller effect on training throughput, except for tuning intra-/inter-op parallelism threads [49]. To find the optimal batch size at a fixed node count, throughput scaling due to increased local batch size and, thus, GPU data-parallelism has to be traded off against epoch scaling (Fig.  2 b). Specifically, Figure 6 shows that the throughput ratio for a local batch size 4, 2, and 1 relative to a maximum local batch size of 8 which is 91%, 64%, and 35% and roughly coincides with the strong scaling efficiency at a batch size of 1,024. Due to the particular closed division epoch scaling in Figure 2b, especially the strong slope starting from batch size 256 to 512 and a measured value of 59 epochs for batch size 128, a local batch size of 2 turns out to give the fastest time to solution for both configurations. To further reduce it, we expect an alternative approach to data-parallelism, which only gives another 11% improvement by doubling the resources, to be more efficient.</p>
<p>Curiously, Figure 2 (c) shows faster than ideal throughput scaling from 128 to 256 GPUs, +8% compared to what is expected for training and +167% for evaluation. So the time to solution is 12% faster on 256 GPUs than expected based on the epoch scaling. This is a result of caching the data set in RAM with 256 nodes, whereas at 128 nodes, parallel file system I/O (measured in section V) is a bottleneck which could be alleviated using near-compute storage.</p>
<p>In summary, fine-grained communication together with the addition of near-compute storage are identified as key optimizations for CosmoFlow on Piz Daint.</p>
<p>V. WORKLOAD CHARACTERIZATION</p>
<p>In this section, we present techniques to characterize the benchmarks in terms of memory, network and I/O performance metrics. This is motivated by the fact that these metrics parameterize extended roofline models [19] that allow to characterize future MLPerf HPC submissions with respect to system capabilities and identify remaining optimization potential in submissions both at the hardware-and softwarelevel. This will complement the information available from the high-level logs (section IV). It is to be noted that these metrics were captured separately from v0.7 submissions in additional runs, with 2 epochs per run in a purely data-parallel setup unless noted otherwise. For CosmoFlow, we used a quarter of the full dataset (for each training and evaluation) and a local batch size of 1, while for DeepCAM, we used all data samples and a local batch size of 2.</p>
<p>A. Memory Bandwidth</p>
<p>We measure memory traffic of these benchmark implementations to estimate how much bandwidth is used for memory reads and writes to the off-chip DRAM on respective systems. More concisely, we measure the accelerator memory bandwidth aggregated across the system. Global memory bandwidth is usually influenced by the underlying cache implementations and may not reflect the memory traffic in its entirety. Hence, we measure DRAM read and write throughput. Table VII lists the average bi-directional bandwidth (read and write) across different systems.</p>
<p>On ABCI, we used Nvidia Nvprof to calculate the average memory bandwidth of all kernels based on the elapsed time for each CUDA kernel and the memory bandwidth between L2 cache and HBM memory. Since Fugaku does not have GPUs, we used Perf [50] to extract read and write memory bandwidths measured at 1ms intervals and the average bandwidths are calculated for each. While Nvprof measures the bandwidth of CUDA kernel time only, Perf measures the bandwidth of the training interval at regular intervals. On ThetaGPU and Summit we used Nvidia Nsight compute [51] to extract the memory bandwidth of all kernels using the metric dram__bytes.sum.per_second.</p>
<p>Observations: Using an additional FP-32 memory bandwidth measurement on ABCI (Table VII † ), we observe that the memory bandwidth of CosmoFlow on Fugaku is 3.85X smaller than on ABCI without mixed precision, even though the maximum effective memory bandwidth in a stream benchmark is similar for the two. Taking into account the throughput relations at 512 compute units in the submission results ( Figure  2c) and the 1.4X speedup of mixed-precision training over  </p>
<p>B. Network Bandwidth</p>
<p>To understand the networking behavior, we profiled the heavy collective communication calls, incl. All Reduce operations, across all implementations. Since CosmoFlow's reference implementation uses Horovod, we used Horovod Timeline [52] to obtain the average network bandwidth for collective communications as mpitrace [53] was unable to correctly capture overlapping communication and computation. The average network bandwidth is calculated based on the NCCL time obtained from the Horovod timeline, excluding the waiting time for data fusions. DeepCAM uses NCCL communication through NVIDIA Apex [54] in the reference implementation. Since Horovod timeline and mpitrace cannot be used here, synchronization operations and timers are inserted before and after the collective communications of NVIDIA Apex to measure the communication time. Then we calculate the average communication bandwidth from the amount of actual data transferred. On Fugaku, as Mesh-TensorFlow was used for CosmoFlow, Horovod timeline cannot be used here and we use mpitrace and mpiP [55] together to calculate the average communication bandwidth (results in Table VII).</p>
<p>Observations: Between 10-40% of total application time is spent in collective communications as we scale across GPUs. On Fugaku, we observe that for model-parallelism the scalability of the computation time is lower than that for the data-parallel execution. This is because model parallelism is applied only to the Conv3d layer (Figure 4(b)). Also, the communication time in absolute terms grows faster as the degree of model parallelism is increased than with data-parallelism, where it is roughly constant. This is due to the communication overhead caused by the data transfer in the halo region when performing spatial partitioning in the Conv3d layer. Lastly, we observe that the small message size for Piz Daint is a direct consequence of the fine-grained communication optimization described in section IV-B4.</p>
<p>C. I/O Bandwidth</p>
<p>As the MLPerf HPC benchmarks have massive input data sets, it is important to understand the I/O performance. Table  VII shows the average I/O bandwidth per worker on different systems. We used Darshan [56] to get the average I/O bandwidth that captures all I/O-related activity, such as types and number of files and aggregate performance combined with shared and unique files worked by all ranks on certain systems. On ABCI, Darshan cannot measure the I/O volume accurately since the implementation of CosmoFlow used NVIDIA DALI which partly performs mmap-based I/O. Hence, we used Nvprof to measure the time of the kernel (TFRecordReader) that is performing I/O to calculate the average I/O bandwidth. On Fugaku, we insert timers before and after the data loads, and calculated from the elapsed time and the amount of data.</p>
<p>Observations: From Table VII, it can be observed that the measured I/O bandwidth is similar for systems with on-node storage § § and we can expect that it is high enough to hide I/O behind computation. For example, for DeepCAM on ABCI, I/O bandwidth is 2.36 GB/s per process, using 256 processes with a full training dataset consisting of 7.7 TB. In this case, estimated I/O time per epoch is 7,700 GB / 256 processes / 2.36 GB/s = 12.8 seconds, while measured average run time per epoch that includes the I/O time is 99.6 seconds.</p>
<p>VI. KEY INSIGHTS AND CONCLUSION</p>
<p>To summarize, we presented MLPerf HPC, a benchmark suite aimed at representative scientific machine learning applications with two applications, CosmoFlow and DeepCAM.</p>
<p>We presented the results of initial submissions from leadership supercomputers and developed a framework for the § § This excludes Piz Daint (cf. section IV-B4) as well as Frontera-RTX with 64 GPUs in the submission due to insufficient on-node SSD capacity. systematic analysis of time to solution in terms of different components from data staging, algorithmic convergence and system compute throughput. This serves the critical need in the HPC community to understand large-scale scientific ML workloads from a holistic perspective. Furthermore, we introduced a set of techniques for workload characterization in terms of I/O, memory and network performance metrics to enable the parameterization of extended roofline models and, thereby, relate MLPerf HPC application performance to hardware capabilities in future rounds.</p>
<p>The key insights from this round are: • Data staging adds a highly variable overhead across applications (&lt; 1% to 20% of time-to-train) that depends on both model and dataset characteristics. Where effective, compression and archiving of multiple files together should be done, especially on smaller systems, where convergence in general is achieved in fewer epochs. • Accelerated storage solutions like on-node SSDs are critical for I/O-performance with large datasets. The results for CosmoFlow showed a narrow range of scale where GPUbased systems operated most efficiently by being able to fit the dataset in RAM and not yet experiencing prohibitive overhead from epoch scaling. If RAM capacity is insufficient, selectively caching the evaluation dataset in RAM should be attempted and performance can be further enhanced by improving data loader bandwidth and restricting data shuffling to intra-node (DeepCAM). • Mixed-precision training and increasing the validation batch size significantly increase compute performance • Efficient scheduling of communication is crucial for both data-and model-parallelism -while fine-grained communication is required in some data-parallel frameworks, modelparallel scalability heavily depends on overlapping halo exchanges with local layer-wise computations. • Scaling to large batch sizes is challenging, requiring modelspecific techniques such as special learning rate schedules, data augmentation and disabling dropout-layers that can exhibit a complicated interplay with convergence. This reinforces the need for efficient strong scaling methods, such as the hybrid model-and-data parallelism on Fugaku. Future Work: In future releases of the benchmark suite, we aim to add new benchmarks for greater diversity and coverage of scientific ML workloads, including state-of-art models such as transformers and graph neural networks, as well as consider applications from emerging focus areas such as AI-driven simulations. We also plan to expand the set of collected metrics to enhance performance models and, thus, increase utility and relevance to HPC users.</p>
<p>Fig. 1 .
1Relative breakdown of time to train normalized to range [0-1], into staging (green), evaluation (orange) and training (blue). Lower three entries on y-axis are for DeepCAM, rest are for CosmoFlow.</p>
<p>(s) processed by number of compute units forming a data-parallel unit in each train step. E.g. Piz-Daint-128 processes 2 samples ("local batch size") on each GPU (pure data-parallelism, batch size 128 × 2 = 256), whereas Fugaku-8192 processes 1 sample in each group of 16 CPUs (through model-parallelism within this group, data-parallelism across these groups of which there are 8192/16 = 512 = batch size).</p>
<p>. 2 .
2Compute analysis of CosmoFlow with figures on a) parameter choice, b) epoch scaling (dependent variable on x-axis), c) throughput scaling (arrows in ABCI-512 explain how to read y-axes, distribution is given for the combined/epoch throughput, curved lines illustrate averaging of training and evaluation to combined/epoch throughput), and d) compute time. Note that axes are shared along rows (y-axis in a &amp; b: batch size, in c &amp; d: epoch throughput) and columns (x-axis in a &amp; c: # compute units, in b &amp; d: # epochs to solution).</p>
<p>2
(a). The ratio of batch size to number of compute units gives the amount of samples processed by a single compute unit on average per train step. Together with the parallelism column in</p>
<p>are related by (data-parallel) weak scaling. From another point of view, this ratio roughly behaves inversely proportional to the communication intensity, i.e. the amount of communication per computation, per train step for a given type of parallelism ¶ , so that regions of different communication ¶ assuming communication to be roughly constant per train step with dataparallel scale-out and the amount of computation scaling with the batch size intensity can be identified inFigure 2(a).</p>
<p>Fig. 3 .
3Log-PCA on epochs &amp; throughput (relative scales) in CosmoFlow. Note that log(Tcompute) = log(# epochs) − log(epoch throughput). Half-axes of ellipses correspond to standard deviation of principal components.</p>
<p>Fig. 4 .Fig. 5 .
45(a) The number of epochs to solution (left y-axis) in open division for CosmoFlow (see ABCI-2048, Fugaku-16384 in Figure 2b) and extrapolated time to solution (right y-axis, relative to 512 CPUs) as a function of global batch size on Fugaku with data parallelism (local batch size is one). (b) Spatial partitioning (2x1) for model-parallelism in CosmoFlow. Elapsed time for the first two epochs of CosmoFlow on Fugaku using data-parallel scaling (local batch size one, no spatial partitioning) and (hybrid) model-parallel scaling (global batch size 512, with spatial partitioning) starting from a data-parallel baseline with 512 CPUs. Time spent in communication (MPI) is measured as in section V.</p>
<p>Fig. 6 .
6Weak-scaling of training throughput for CosmoFlow on Piz Daint before (hollow symbols, dashed lines) and after (filled symbols, solid lines) applying fine-grained communication in the batch size region 128-1024.</p>
<p>Communication patterns in CosmoFlow and DeepCAM applications.</p>
<p>Figure 5 and 7(a) presents the share of time spent in communication for CosmoFlow and Figure 7(b) shows similar measurements for DeepCAM.</p>
<p>TABLE I HPC
IMACHINE LEARNING BENCHMARKS Targets representative scientific machine learning applications with massive datasets. Provision of two types submissions, closed and open enable novel optimizations. Time to solution metric and focused timing captures holistic model performanceBenchmark 
Performance 
metrics </p>
<p>Application 
domain </p>
<p>Data vol-
ume </p>
<p>Comments </p>
<p>HPL, HPL-AI Flops, 
Flops/Watt </p>
<p>Random dense 
system of lin-
ear equations </p>
<p>Variable 
Used in Top500 and Green500 to rank supercomputers. Problem size scaled to optimize 
the performance for machine size. HPL measures performance at double precision, HPL-
AI measures performance in mixed precision 
HPCAI500 
Valid 
Flops, 
Valid 
Flops/Watt </p>
<p>Image 
classification, 
Weather 
analytics </p>
<p>150 GB &amp; 
1.65 TB </p>
<p>Convolution and GEMM layers measure the performance in valid Flops which impose 
penalty based on failure to meet target accuracy. Limited to Microbenchmarks, Object 
Detection and Image Classification tasks with microscopic view of common deep 
learning models (Faster-RCNN, ResNet) 
Deep500 
Throughput, 
Time 
to 
solution </p>
<p>any machine 
learning 
application </p>
<p>150 GB 
Provides infrastructure to help evaluate different framework implementations and mul-
tiple levels of operators. Challenging to integrate into scientific applications. Evaluated 
with ImageNet dataset. 
MLPerf HPC 
Time 
to 
train </p>
<p>Cosmology 
and 
weather 
analytics </p>
<p>5.1 TB &amp; 
8.8 TB </p>
<p>TABLE II 
MLPERF HPC BENCHMARKS OVERVIEW </p>
<p>Benchmark Quality target #Runs 
Tunable 
hyperparameters </p>
<p>CosmoFlow 
MAE &lt; 0.124 
10 
batch size, learning rates 
DeepCAM 
IOU &gt; 0.82 
5 
optimizer (LAMB or 
AdamW), 
batch size, learning rates </p>
<p>TABLE III HPC
IIISYSTEM DETAILS dual-rail EDR InfiniBand network (Fat Tree), 23GB/s node injection bandwidth * Measured performance metrics but did not submit for v0.7 submissionsSystem </p>
<h1>Nodes</h1>
<p>Processors (per node) 
Accelerators 
(per node) </p>
<p>Memory (per 
node) </p>
<p>Node-local storage 
(per node) </p>
<p>Interconnect topology and bandwidth </p>
<p>Piz Daint [39] 
5,704 
1x Intel Xeon E5-2690 v3 
1x 
NVIDIA 
P100 (16 GB) </p>
<p>64 GB 
N/A 
Cray Aries (Dragonfly), 9.7 GB/s intern-
ode bi-directional 
ABCI [40] 
1,088 
2x Intel Xeon Gold 6148 
4x 
NVIDIA 
V100 (16 GB) </p>
<p>384 GB 
1600 GB (SSD + 
NVMe) </p>
<p>InfiniBand EDR (100Gbps) ×2, full-
bisection bandwidth in the same rack 
(34 compute nodes) 
Cori-KNL [41] 
9,688 
1x Intel Xeon Phi 7250 
N/A 
96 GB DDR4 
+ 16 GB MC-
DRAM </p>
<p>N/A 
Cray Aries (Dragonfly), ¿45 TB/s global 
peak bisection bandwidth </p>
<p>Cori-GPU [42] 
18 
2x Intel Xeon Gold 6148 
8x 
NVIDIA 
V100 (16 GB) </p>
<p>384 GB DDR4 
930 GB (NVMe) 
4 dual-port Mellanox MT27800 
ConnectX-5 EDR InfiniBand network 
(Fat Tree) 
HAL [43] 
16 
2x IBM POWER 9 model 2.2 
4x 
NVIDIA 
V100 (16 GB) </p>
<p>256 GB DDR4 
N/A 
2-Port EDR (Single Level) IB 
ConnectX-5 Adapter, 100 Gb/s 
Frontera-RTX 
[44] </p>
<p>90 
2x Intel Xeon E5-2620 v4 
4x 
NVIDIA 
Quadro RTX 
5000 (16 GB) </p>
<p>128 GB DDR4 
240 GB (SSD) 
FDR InfiniBand MT27500 ConnectX-3 
Adapter (Fat Tree), 56 Gb/s </p>
<p>Fugaku [45] 
158,976 
1x Fujitsu A64FX 
N/A 
32 GB HBM2 
1.6 TB (NVMe 
SSD, shared among 
16 compute nodes) </p>
<p>TofuD, (6D Mesh/Torus Network), 
68GB/s x2 (in/out) </p>
<p>ThetaGPU [46] * 
24 
2x AMD EPYC 7742 
8x 
NVIDIA 
A100 (40 GB) </p>
<p>1 TB DDR4 
15TB SSD, 3.84TB 
NVMe </p>
<p>20 Mellanox QM9700 HDR200 40-port 
switches (Fat Tree), 25 GB/s node in-
jection bandwidth 
Summit [29] * 
4,600 
2x IBM 3.07 GHz POWER9 
6x 
NVIDIA 
V100 (16 GB) </p>
<p>512 GB DDR4 
1.6TB 
(NVMe 
SSD) </p>
<p>TABLE IV PERFORMANCE
IVMETRICS (TIME TO SOLUTION IN MINUTES) FROM SUBMISSIONS IN CLOSED AND OPEN DIVISIONSDivision System 
Submission 
Software </p>
<h1>Processors #Accelerators</h1>
<p>Parallelism  † CosmoFlow DeepCAM </p>
<p>Closed 
Piz Daint 
Piz-Daint-128 
TensorFlow 2.2.0 
128 
128 
2 s/1 GPU 
461.01 
-
Piz Daint 
Piz-Daint-256 
TensorFlow 2.2.0 
256 
256 
2 s/1 GPU 
327.01 
-
ABCI 
ABCI-1024 
PyTorch 1.6.0 
512 
1,024 
2 s/1 GPU 
-
11.71 
ABCI 
ABCI-512 
TensorFlow 2.2.0 
256 
512 
1 s/1 GPU 
34.42 
-
Fugaku 
Fugaku-512 
TensorFlow 2.2.0 + 
Mesh TensorFlow </p>
<h2>512</h2>
<p>1 s/1 CPU 
268.77 
-</p>
<p>Fugaku 
Fugaku-8192 
TensorFlow 2.2.0 + 
Mesh TensorFlow </p>
<p>8,192 
-1 s/16 CPUs 
101.49 
-</p>
<p>Cori-GPU 
Cori-GPU-64 
PyTorch 1.6.0 
16 
64 
2 s/1 GPU 
-
139.29 
Cori-GPU 
Cori-GPU-64 
TensorFlow 1.15.0 
16 
64 
1 s/1 GPU 
364.73 
-
Cori-KNL 
Cori-KNL-512 
TensorFlow 1.15.2 
512 
-
1 s/1 CPU 
536.06 
-
HAL 
HAL-64 
TensorFlow 1.15.0 
32 
64 
1 s/1 GPU 
265.59 
-
Frontera-RTX 
Frontera-RTX-64 TensorFlow 1.15.2 
32 
64 
1 s/1 GPU 
602.23 
-
Open 
ABCI 
ABCI-1024 
PyTorch 1.6.0 
512 
1,024 
2 s/1 GPU 
-
10.49 
ABCI 
ABCI-2048 
TensorFlow 2.2.0 
1,024 
2,048 
1 s/1 GPU 
13.21 
-
Fugaku 
Fugaku-16384 
TensorFlow 2.2.0 + 
Mesh TensorFlow </p>
<h2>16,384</h2>
<p>1 s/4 CPUs 
30.07 
-</p>
<p>Cori-KNL 
Cori-KNL-1024 
TensorFlow 1.15.2 
1,024 
-
1 s/1 CPU 
419.69 
-</p>
<p>† Data-parallel granularity of train step: # samples </p>
<p>TABLE V DATA
VSTAGING TIMEBenchmark 
Submission 
Staging time 
(minutes) </p>
<p>T staging 
T epoch </p>
<p>CosmoFlow 
Cori-GPU-64 
16.49 ± 0.61 
2.55 
ABCI-512 
0.76 ± 0.004 
2.27 
ABCI-2048 
0.20 ± 0.004 
1.56 
Fugaku-512 
1.55 ± 0.11 
0.64 
Fugaku-8192 
3.77 ± 0.51 
3.59 
Fugaku-16384 
0.88 ± 0.08 
4.93 </p>
<p>DeepCAM 
ABCI-1024 
2.20 ± 0.01 
5.55 
ABCI-1024 
1.96 ± 0.08 
5.45 </p>
<p>TABLE VI SCALING
VIOF REQUIRED EPOCHS, THROUGHPUT, TIME TO SOLUTION AND COMPUTE BUDGET IN COSMOFLOW VS. DEEPCAM.Benchmark 
Submission 
Batch 
size </p>
<h1>Epochs</h1>
<h1>GPUs</h1>
<p>Training 
throughput/# acc. 
(samples/second) </p>
<p>Evaluation 
throughput/# acc. 
(samples/second) </p>
<p>Time to solution 
(minutes) </p>
<p>Compute 
budget 
(h · acc) </p>
<p>CosmoFlow Cori-GPU-64 
64 
53.88 ± 4.85 
64 
12.07 ± 0.09 
21.17 ± 0.56 
364.73 ± 32.77 
389.04 
ABCI-512 
512 
100.00 ± 13.50 
512 
26.59 ± 0.90 
151.88 ± 0.68 
34.42 ± 4.03 
293.03 
ABCI-2048 
2,048 
98.50 ± 2.56 
2,048 
16.79 ± 0.23 
149.21 ± 0.28 
13.21 ± 0.35 
450.96 </p>
<p>DeepCAM 
Cori-GPU-64 
128 
10.00 ± 0.00 
64 
3.56 ± 0.13 
3.55 ± 0.18 
139.29 ± 3.63 
148.58 
ABCI-1024 
2,048 
24.00 ± 0.00 
1,024 
5.24 ± 0.02 
7.37 ± 0.01 
11.71 ± 0.02 
199.78 
ABCI-1024 
2,048 
23.67 ± 1.16 
1,024 
5.57 ± 0.13 
4.67 ± 0.82 
10.49 ± 0.23 
178.95 </p>
<p>TABLE VII WORKLOAD
VIICHARACTERIZATION: MEMORY BANDWIDTH (SINGLE CPU/GPU), NETWORK AND PER-WORKER I/O BANDWIDTH MEASUREMENTS † mixed-precision used on ABCI (memory bandwidth with FP32-training: 427.1 GB/sec), no model-parallelism used on Fugaku measurementsFP-32 (section IV-B2), we can compare the systems in their caching efficiency. It turns out that FP32 on ABCI has a similar (90 %) memory traffic per train step as Fugaku, whereas mixed-precision training transfers only 50 % as much memory, which underlines its cache-friendly implementation.Benchmark System 
Memory Tool Memory BW 
(GB/sec) </p>
<p>Network Tool # units 
Network BW 
(GB/sec) </p>
<p>Size 
(MB) </p>
<p>I/O Tool 
I/O BW 
(GB/sec) </p>
<p>CosmoFlow ABCI  † 
Nvprof 
335.4 
Horovod TL 
512 GPUs 3.41 
19.97 
Nvprof 
1.65 
Fugaku  † 
Perf 
110.8 
Mpitrace 
512 CPUs 
0.75 
21.71 
Timer-based 2.57 
Piz Daint 
Nvprof 
-
Horovod TL 
256 GPUs 1.86 
2.21 
Darshan 
0.51 
Summit 
Nsight 
233.1 
Horovod TL 
510 GPUs 2.24 
22.0 
Darshan 
1.46 
ThetaGPU Nsight 
194.5 
Horovod TL 
128 GPUs 1.95 
15.20 
Darshan 
1.98 
DeepCAM ABCI 
Nvprof 
153.1 
Timer-based 
512 GPUs 3.73 
37.77 
Darshan 
2.36 
Summit 
Nsight 
254.7 
Timer-based 
510 GPUs 4.50 
225.0 </p>
<p>ACKNOWLEDGMENT This research was funded in part by the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.APPENDIX ARTIFACT DESCRIPTION/ARTIFACT EVALUATION SUMMARY OF THE EXPERIMENTS REPORTEDWe ran MLPerf TM HPC benchmarks, CosmoFlow and Deep-CAM on several supercomputers such as Cori, Fugaku and Piz Daint with Tensorflow, Horovod and PyTorch. The details are listed in sections III and IV in the paper.The baseline implementations are available at https:// github. com/ mlcommons/ hpc. The logging package can be installed as the instructions listed in this URL. # Install the package into your python environment. git clone -b hpc-0.5.0 https://github.com/mlperf-hpc /logging.git mlperf-logging pip install [--user] -e mlperf-logging # Test compliance of a specific mlperf hpc log file python -m mlperf_logging.compliance_checker -ruleset hpc_0.5.0 $logFile a) CosmoFlow: The dataset we use for this benchmark comes from simulations run by the ExaLearn group and hosted at NERSC at https:// portal.nersc.gov/ project/ m3363/ . The latest pre-processed dataset in TFRecord format is in the cosmoUniverse 2019 05 4parE tf folder, which contains training and validation subfolders. There are currently 262144 samples for training and 65536 samples for validation/testing. The combined size of the dataset is 5.1 TB. Example submission scripts are in scripts and YAML configuration files are in configs directories. To run a code at NERSC, use sbatch -N 64 scripts/train cori.sh and modify the scripts accordingly to run on other systems. b) DeepCAM: The dataset for this benchmark comes from CAM5 simulations and is hosted at NERSC. The samples are stored in HDF5 files with input images of shape (768, 1152, 16) and pixel-level labels of shape (768, 1152). The globus endpoint for the dataset is at https:// tinyurl.com/ 3hnd2z9c. The splitting scripts to split the dataset to train, test, and validate are under src/utils. Example submission scripts are at src/deepCam/run scripts. These can be modified to run on other systems.The v0.7 submissions are hosted at https:// github.com/ mlcommons/ hpc results v0.7. There is a separate directory for each submitting organization with the details of its submissions. It contains the following three sub-directories:• benchmarks: This directory contains the benchmark implementations and manual for system setup used in that organization's submissions. In particular, the detailed steps including the scripts to configure a system and run the benchmark are mentioned in a README file in 〈submitter〉/〈benchmark-name〉/implementations/〈submission-entry〉/ for each submission. The code used by a submission is referenced from there and available from 〈submitter〉/〈benchmark-name〉/implementations/〈implementation-entry〉/ (multiple submissions can use the same code) and the system used is detailed at
Improved protein structure prediction using potentials from deep learning. A Senior, R Evans, J Jumper, J Kirkpatrick, L Sifre, T Green, C Qin, A Žídek, A Nelson, A Bridgland, H Penedones, S Petersen, K Simonyan, S Crossan, P Kohli, D Jones, D Silver, K Kavukcuoglu, D Hassabis, Nature. 577A. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin, A.Žídek, A. Nelson, A. Bridgland, H. Penedones, S. Petersen, K. Simonyan, S. Crossan, P. Kohli, D. Jones, D. Silver, K. Kavukcuoglu, and D. Hassabis, "Improved protein structure prediction using potentials from deep learning," Nature, vol. 577, pp. 1-5, 01 2020.</p>
<p>machine learning and big scientific data. T Hey, K Butler, S Jackson, J Thiyagalingam, 0320190054T. Hey, K. Butler, S. Jackson, and J. Thiyagalingam, "machine learning and big scientific data," p. 20190054, 03 2020.</p>
<p>AI for Science. R Stevens, V Taylor, J Nichols, A B Maccabe, K Yelick, D Brown, 2R. Stevens, V. Taylor, J. Nichols, A. B. Maccabe, K. Yelick, and D. Brown, "AI for Science," 2 2020. [Online]. Available: https://www.osti.gov/biblio/1604756</p>
<p>Top500 list. "Top500 list: November 2020," https://www.top500.org/lists/2020/11/, 2021.</p>
<p>Making a case for a Green500 list. S Sharma, Chung-Hsing Hsu, Wu-Chun Feng, Proceedings 20th IEEE International Parallel Distributed Processing Symposium. 20th IEEE International Parallel Distributed Processing SymposiumS. Sharma, Chung-Hsing Hsu, and Wu-chun Feng, "Making a case for a Green500 list," in Proceedings 20th IEEE International Parallel Distributed Processing Symposium, 2006.</p>
<p>A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning. T Ben-Nun, M Besta, S Huber, A N Ziogas, D Peter, T Hoefler, 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS). T. Ben-Nun, M. Besta, S. Huber, A. N. Ziogas, D. Peter, and T. Hoefler, "A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning," in 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 2019, pp. 66-77.</p>
<p>Z Jiang, L Wang, X Xiong, W Gao, C Luo, F Tang, C Lan, H Li, J Zhan, HPC AI500: The Methodology, Tools, Roofline Performance Models, and Metrics for Benchmarking HPC AI Systems. Z. Jiang, L. Wang, X. Xiong, W. Gao, C. Luo, F. Tang, C. Lan, H. Li, and J. Zhan, "HPC AI500: The Methodology, Tools, Roofline Performance Models, and Metrics for Benchmarking HPC AI Systems," 2020.</p>
<p>HPL-AI Mixed-Precision Benchmark. "HPL-AI Mixed-Precision Benchmark," https://icl.bitbucket.io/hpl-ai/, 2021.</p>
<p>ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events. E Racah, C Beckham, T Maharaj, S Kahou, M Prabhat, C Pal, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30E. Racah, C. Beckham, T. Maharaj, S. Kahou, M. Prabhat, and C. Pal, "ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events," in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 3405-3416.</p>
<p>Dawnbench: An endto-end deep learning benchmark and competition. C Coleman, D Narayanan, D Kang, T Zhao, J Zhang, L Nardi, P Bailis, K Olukotun, C Ré, M Zaharia, C. Coleman, D. Narayanan, D. Kang, T. Zhao, J. Zhang, L. Nardi, P. Bailis, K. Olukotun, C. Ré, and M. Zaharia, "Dawnbench: An end- to-end deep learning benchmark and competition."</p>
<p>Deepbench. 2021"Deepbench," https://github.com/baidu-research/DeepBench, 2021.</p>
<p>Fathom: reference workloads for modern deep learning methods. R Adolf, S Rama, B Reagen, G Wei, D Brooks, 2016 IEEE International Symposium on Workload Characterization (IISWC). R. Adolf, S. Rama, B. Reagen, G. Wei, and D. Brooks, "Fathom: reference workloads for modern deep learning methods," in 2016 IEEE International Symposium on Workload Characterization (IISWC), 2016, pp. 1-10.</p>
<p>Benchmarking TPU, GPU, and CPU Platforms for Deep Learning. Y Wang, G Wei, D Brooks, CoRR. Y. Wang, G. Wei, and D. Brooks, "Benchmarking TPU, GPU, and CPU Platforms for Deep Learning," CoRR, vol. abs/1907.10701, 2019. [Online]. Available: http://arxiv.org/abs/1907.10701</p>
<p>Hpe deep learning benchmarking suite. "Hpe deep learning benchmarking suite," https://github.com/ HewlettPackard/dlcookbook-dlbs/, 2021.</p>
<p>XSP: Across-Stack Profiling and Analysis of Machine Learning Models on GPUs. C Li, A Dakkak, J Xiong, W Wei, L Xu, W Hwu, 2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS). C. Li, A. Dakkak, J. Xiong, W. Wei, L. Xu, and W. Hwu, "XSP: Across- Stack Profiling and Analysis of Machine Learning Models on GPUs," in 2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 2020, pp. 326-327.</p>
<p>Mlcommons. "Mlcommons," https://mlcommons.org/en/, 2021.</p>
<p>MLPerf Training Benchmark. P Mattson, C Cheng, G Diamos, C Coleman, P Micikevicius, D Patterson, H Tang, G.-Y Wei, P Bailis, V Bittorf, D Brooks, D Chen, D Dutta, U Gupta, K Hazelwood, A Hock, X Huang, D Kang, D Kanter, N Kumar, J Liao, D Narayanan, T Oguntebi, G Pekhimenko, L Pentecost, V Reddi, T Robie, T St John, C.-J Wu, L Xu, C Young, M Zaharia, Proceedings of Machine Learning and Systems. I. Dhillon, D. Papailiopoulos, and V. SzeMachine Learning and Systems2P. Mattson, C. Cheng, G. Diamos, C. Coleman, P. Micikevicius, D. Patterson, H. Tang, G.-Y. Wei, P. Bailis, V. Bittorf, D. Brooks, D. Chen, D. Dutta, U. Gupta, K. Hazelwood, A. Hock, X. Huang, D. Kang, D. Kanter, N. Kumar, J. Liao, D. Narayanan, T. Oguntebi, G. Pekhimenko, L. Pentecost, V. Janapa Reddi, T. Robie, T. St John, C.-J. Wu, L. Xu, C. Young, and M. Zaharia, "MLPerf Train- ing Benchmark," in Proceedings of Machine Learning and Systems, I. Dhillon, D. Papailiopoulos, and V. Sze, Eds., vol. 2, 2020, pp. 336-349. [Online]. Available: https://proceedings.mlsys.org/paper/2020/ file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf</p>
<p>MLPerf Inference Benchmark. V J Reddi, C Cheng, D Kanter, P Mattson, G Schmuelling, C Wu, B Anderson, M Breughe, M Charlebois, W Chou, R Chukka, C Coleman, S Davis, P Deng, G Diamos, J Duke, D Fick, J S Gardner, I Hubara, S Idgunji, T B Jablin, J Jiao, T S John, P Kanwar, D Lee, J Liao, A Lokhmotov, F Massa, P Meng, P Micikevicius, C Osborne, G Pekhimenko, A T R Rajan, D Sequeira, A Sirasao, F Sun, H Tang, M Thomson, F Wei, E Wu, L Xu, K Yamada, B Yu, G Yuan, A Zhong, P Zhang, Y Zhou, 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). V. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C. Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Cole- man, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar, D. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius, C. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, and Y. Zhou, "MLPerf Inference Benchmark," in 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), 2020, pp. 446-459.</p>
<p>An extended roofline model with communication-awareness for distributed-memory HPC systems. D Cardwell, F Song, Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region. the International Conference on High Performance Computing in Asia-Pacific RegionD. Cardwell and F. Song, "An extended roofline model with communication-awareness for distributed-memory HPC systems," in Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region, 2019, pp. 26-35.</p>
<p>MLPerf HPC Benchmark Suite. 2021"MLPerf HPC Benchmark Suite," https://github.com/mlcommons/hpc, 2021.</p>
<p>CosmoFlow: using deep learning to learn the universe at scale. A Mathuriya, D Bard, P Mendygral, L Meadows, J Arnemann, L Shao, S He, T Kärnä, D Moise, S J Pennycook, K J Maschhoff, J Sewall, N Kumar, S Ho, M F Ringenburg, V W Prabhat, Lee, Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis. the International Conference for High Performance Computing, Networking, Storage, and AnalysisSC; Dallas, TX, USA6511A. Mathuriya, D. Bard, P. Mendygral, L. Meadows, J. Arnemann, L. Shao, S. He, T. Kärnä, D. Moise, S. J. Pennycook, K. J. Maschhoff, J. Sewall, N. Kumar, S. Ho, M. F. Ringenburg, Prabhat, and V. W. Lee, "CosmoFlow: using deep learning to learn the universe at scale," in Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis, SC 2018, Dallas, TX, USA, November 11-16, 2018. IEEE / ACM, 2018, pp. 65:1-65:11.</p>
<p>Exalearn project. "Exalearn project," https://petreldata.net/exalearn/, 2021.</p>
<p>Cosmoflow datasets. TFRecord. 2021"Cosmoflow datasets," https://portal.nersc.gov/project/m3363/, 2021. [24] "TFRecord," https://www.tensorflow.org/tutorials/load data/tfrecord, 2021.</p>
<p>Cosmoflow. J Balewski, 2021J. Balewski, "Cosmoflow," https://bitbucket.org/balewski/cosmoflow, 2021.</p>
<p>Exascale Deep Learning for Climate Analytics. T Kurth, S Treichler, J Romero, M Mudigonda, N Luehr, E Phillips, A Mahesh, M Matheson, J Deslippe, M Fatica, P Prabhat, M Houston, SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. T. Kurth, S. Treichler, J. Romero, M. Mudigonda, N. Luehr, E. Phillips, A. Mahesh, M. Matheson, J. Deslippe, M. Fatica, P. Prabhat, and M. Houston, "Exascale Deep Learning for Climate Analytics," in SC18: International Conference for High Performance Computing, Networking, Storage and Analysis, 2018, pp. 649-660.</p>
<p>NCAR Community Atmosphere Model (CAM 5.0). "NCAR Community Atmosphere Model (CAM 5.0)," https://www.cesm. ucar.edu/models/cesm1.0/cam/docs/description/cam5 desc.pdf, 2021.</p>
<p>TECA: A Parallel Toolkit for Extreme Climate Analysis. O Prabhat, S Rübel, K Byna, F Wu, M Li, W Wehner, Bethel, proceedings of the International Conference on Computational Science. the International Conference on Computational Science9ICCS 2012Prabhat, O. Rübel, S. Byna, K. Wu, F. Li, M. Wehner, and W. Bethel, "TECA: A Parallel Toolkit for Extreme Climate Analysis," Procedia Computer Science, vol. 9, pp. 866-876, 2012, proceedings of the International Conference on Computational Science, ICCS 2012.</p>
<p>The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems. S S Vazhkudai, B R De Supinski, A S Bland, A Geist, J Sexton, J Kahle, C J Zimmer, S Atchley, S H Oral, D E Maxwell, V G Vergara Larrea, A Bertsch, R Goldstone, W Joubert, C Chambreau, D Appelhans, R Blackmore, B Casses, G Chochia, G Davison, M A Ezell, E Gonsiorowski, L Grinberg, B Hanson, B Hartner, I Karlin, M L Leininger, D Leverman, C Marroquin, A Moody, M Ohmacht, R Pankajakshan, F Pizzano, J H Rogers, B Rosenburg, D Schmidt, M Shankar, F Wang, P Watson, B Walkup, L D Weems, J Yin, S. S. Vazhkudai, B. R. de Supinski, A. S. Bland, A. Geist, J. Sexton, J. Kahle, C. J. Zimmer, S. Atchley, S. H. Oral, D. E. Maxwell, V. G. Vergara Larrea, A. Bertsch, R. Goldstone, W. Joubert, C. Chambreau, D. Appelhans, R. Blackmore, B. Casses, G. Chochia, G. Davison, M. A. Ezell, E. Gonsiorowski, L. Grinberg, B. Hanson, B. Hartner, I. Karlin, M. L. Leininger, D. Leverman, C. Marroquin, A. Moody, M. Ohmacht, R. Pankajakshan, F. Pizzano, J. H. Rogers, B. Rosenburg, D. Schmidt, M. Shankar, F. Wang, P. Watson, B. Walkup, L. D. Weems, and J. Yin, "The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems," 7 2018. [Online]. Available: https://www.osti.gov/biblio/1489443</p>
<p>T Kurth, S Treichler, J Romero, M Mudigonda, N Luehr, E Phillips, A Mahesh, M Matheson, J Deslippe, M Fatica, M Prabhat, Houston, Exascale Deep Learning for Climate Analytics. T. Kurth, S. Treichler, J. Romero, M. Mudigonda, N. Luehr, E. Phillips, A. Mahesh, M. Matheson, J. Deslippe, M. Fatica, Prabhat, and M. Hous- ton, "Exascale Deep Learning for Climate Analytics," 2018.</p>
<p>Deep Residual Learning for Image Recognition. K He, X Zhang, S Ren, J Sun, K. He, X. Zhang, S. Ren, and J. Sun, "Deep Residual Learning for Image Recognition," 2015.</p>
<p>Xception: Deep Learning with Depthwise Separable Convolutions. F Chollet, F. Chollet, "Xception: Deep Learning with Depthwise Separable Con- volutions," 2017.</p>
<p>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes. Y You, J Li, S Reddi, J Hseu, S Kumar, S Bhojanapalli, X Song, J Demmel, K Keutzer, C.-J Hsieh, Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh, "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes," 2020.</p>
<p>Adam: A Method for Stochastic Optimization. D P Kingma, J Ba, D. P. Kingma and J. Ba, "Adam: A Method for Stochastic Optimization," 2017.</p>
<p>Large Batch Training of Convolutional Networks. Y You, I Gitman, B Ginsburg, Y. You, I. Gitman, and B. Ginsburg, "Large Batch Training of Convo- lutional Networks," 2017.</p>
<p>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs," 2017.</p>
<p>MLPerf Logging Library. 2021"MLPerf Logging Library," https://github.com/mlcommons/logging, 2021.</p>
<p>General mlperf submission rules. "General mlperf submission rules," https://github.com/mlcommons/ policies/blob/master/submission rules.adoc, 2021.</p>
<p>The Supercomputer Piz Daint. C Swiss, National Supercomputing Center. C. Swiss National Supercomputing Center. (2018) The Supercomputer Piz Daint. [Online]. Available: https://www.cscs.ch/computers/piz-daint/</p>
<p>AI Bridging Cloud Infrastructure (ABCI). "AI Bridging Cloud Infrastructure (ABCI)," https://abci.ai/en/about abci/, 2021.</p>
<p>. Cori System, "Cori System," https://docs.nersc.gov/systems/cori/, 2021.</p>
<p>. Gpu Cori, Nodes, "Cori GPU Nodes," https://docs-dev.nersc.gov/cgpu/, 2021.</p>
<p>HAL: Computer System for Scalable Deep Learning. V Kindratenko, D Mu, Y Zhan, J Maloney, S H Hashemi, B Rabe, K Xu, R Campbell, J Peng, W Gropp, Practice and Experience in Advanced Research Computing, ser. PEARC '20. New York, NY, USAAssociation for Computing MachineryV. Kindratenko, D. Mu, Y. Zhan, J. Maloney, S. H. Hashemi, B. Rabe, K. Xu, R. Campbell, J. Peng, and W. Gropp, "HAL: Computer System for Scalable Deep Learning," in Practice and Experience in Advanced Research Computing, ser. PEARC '20. New York, NY, USA: Association for Computing Machinery, 2020, p. 41-48. [Online].</p>
<p>. 10.1145/3311790.3396649Available: https://doi.org/10.1145/3311790.3396649</p>
<p>Frontera: The Evolution of Leadership Computing at the National Science Foundation. D Stanzione, J West, R T Evans, T Minyard, O Ghattas, D K Panda, 10.1145/3311790.3396656Practice and Experience in Advanced Research Computing, ser. PEARC '20. New York, NY, USAAssociation for Computing MachineryD. Stanzione, J. West, R. T. Evans, T. Minyard, O. Ghattas, and D. K. Panda, "Frontera: The Evolution of Leadership Computing at the National Science Foundation," in Practice and Experience in Advanced Research Computing, ser. PEARC '20. New York, NY, USA: Association for Computing Machinery, 2020, p. 106-111. [Online]. Available: https://doi.org/10.1145/3311790.3396656</p>
<p>The Supercomputer Fugaku. 2021"The Supercomputer Fugaku," https://www.r-ccs.riken.jp/en/fugaku/ project/outline, 2021.</p>
<p>Thetagpu. 2021"Thetagpu," https://www.alcf.anl.gov/alcf-resources/theta, 2021.</p>
<p>K Shirahata, Y Hara, Y Sakai, M Miwa, A Tabuchi, H Gao, Content-Aware Computing Technology for Accelerating Increasingly Complex and Massive AI Processing. Tech. Rep.K. Shirahata, Y. Hara, Y. Sakai, M. Miwa, A. Tabuchi, and H. Gao, "Content-Aware Computing Technology for Accelerating Increasingly Complex and Massive AI Processing," https://www.fujitsu.com/global/ about/resources/publications/technicalreview/topics/article009.html, Fu- jitsu, Tech. Rep., 2021.</p>
<p>Sarus: Highly Scalable Docker Containers for HPC Systems. L Benedicic, F A Cruz, A Madonna, K Mariotti, International Conference on High Performance Computing. SpringerL. Benedicic, F. A. Cruz, A. Madonna, and K. Mariotti, "Sarus: Highly Scalable Docker Containers for HPC Systems," in International Conference on High Performance Computing. Springer, 2019, pp. 46- 60.</p>
<p>Exploiting Parallelism Opportunities with Deep Learning Frameworks. Y E Wang, C.-J Wu, X Wang, K Hazelwood, D Brooks, ACM Transactions on Architecture and Code Optimization (TACO). 181Y. E. Wang, C.-J. Wu, X. Wang, K. Hazelwood, and D. Brooks, "Exploiting Parallelism Opportunities with Deep Learning Frameworks," ACM Transactions on Architecture and Code Optimization (TACO), vol. 18, no. 1, pp. 1-23, 2020.</p>
<p>Perf profiling tool. "Perf profiling tool," https://perf.wiki.kernel.org/index.php/Main Page, 2021.</p>
<p>Nvidia Nsight Compute Profiler. "Nvidia Nsight Compute Profiler," https://developer.nvidia.com/ nsight-compute, 2021.</p>
<p>. &quot;horovod Timeline, "Horovod Timeline," https://horovod.readthedocs.io/en/stable/timeline include.html, 2021.</p>
<p>Mpitrace tool. 2021"Mpitrace tool," https://github.com/IBM/mpitrace, 2021.</p>
<p>Nvidia Apex Extension. 2021"Nvidia Apex Extension," https://github.com/NVIDIA/apex, 2021.</p>
<p>mpip profiling tool. 2021"mpip profiling tool," https://github.com/LLNL/mpiP, 2021.</p>
<p>24/7 Characterization of petascale I/O workloads. P H Carns, R Latham, R B Ross, K Iskra, S Lang, K Riley, CLUSTER. IEEE Computer SocietyP. H. Carns, R. Latham, R. B. Ross, K. Iskra, S. Lang, and K. Riley, "24/7 Characterization of petascale I/O workloads." in CLUSTER. IEEE Computer Society, 2009, pp. 1-10. [Online]. Available: http://dblp.uni-trier.de/db/conf/cluster/cluster2009.html#CarnsLRILR09</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3298 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3298</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3298</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-8868a6d452b06bf4ad33237d0f3952d895ca20e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8868a6d452b06bf4ad33237d0f3952d895ca20e7" target="_blank">Improving Large Language Model Fine-tuning for Solving Math Problems</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A fine-tuning recipe is designed that yields an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting and a multi-task sequential fine- Tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance.</p>
                <p><strong>Paper Abstract:</strong> Despite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM 2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3298.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3298.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 - Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large PaLM 2 variant evaluated in this paper; used as the primary experimental model for fine-tuning, re-ranking/evaluation, and multi-task sequential training on the MATH benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large variant of Google's PaLM 2 family (transformer-based autoregressive LLM). Used as both generator (solution producer) and evaluator (solution verifier) in experiments; pretrained checkpoint provided by PaLM 2 paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['step-by-step solution generation (chain-of-thought style)', 'sampling + majority voting (self-consistency)', 'solution-cluster re-ranking (evaluator-based re-ranking)', 'multi-task sequential fine-tuning (generator↔evaluator↔generator)', 'contrastive / margin loss (sequence-level ranking)', 'classification-based verification (correct vs incorrect token probability)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Step-by-step solutions: models are fine-tuned to generate detailed solution traces (MLE) similar to chain-of-thought. Sampling + majority voting: nucleus sampling (T=0.6, top-p=0.95) is used to produce multiple candidate solutions and majority-voting (Maj1@N) selects the most frequent mathematical-equivalence cluster. Solution-cluster re-ranking (SCR): an evaluator LLM is fine-tuned (classification or margin loss) to score candidate solutions using the model probability of the token 'correct' vs 'incorrect'; re-ranking can be applied to all candidates (RR.All) or only top-K majority clusters (RR.Top-K). Multi-task sequential fine-tuning: sequentially fine-tune as generator (MLE), then evaluator (classification/margin), then generator again (MLE) to transfer evaluation signal into generation. Contrastive/margin loss: used to push higher sequence likelihood for correct solutions vs incorrect ones. Classification-based verifier: format prompt T(P, X) and use next-token probability of 'correct'/'incorrect' as score, trained with cross-entropy or margin losses.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>The paper uses a set of similar reasoning styles (step-by-step chain-of-thought style solutions) as the core reasoning representation but applies a diverse set of training and inference strategies (sampling + self-consistency, evaluator re-ranking, margin/contrastive objectives, and multi-task sequential fine-tuning). Determination is via controlled fine-tuning and ablation experiments: same core solution style but different evaluator and re-ranking strategies are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MATH is a benchmark of high-school math competition problems with human-written step-by-step solutions and ground-truth answers; grading checks mathematical equivalence rather than textual match.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Few-shot pre-trained PaLM 2-L (4-shot prompt): Pass@1 = 33.4%, Maj1@64 = 47.6%, Pass@64 = 79.4%. Supervised solution fine-tuning (SSFT) on combined MATH + PRM800K: Pass@1 = 35.6%, Maj1@64 = 55.2%, Pass@64 = 82.8% (Table 2). Solution-cluster re-ranking (PaLM 2-L evaluator): RR.All -> 57.0% (margin loss) or 56.8% (xent) re-ranked accuracy (Table 3); RR.Top-8 -> 58.8% (both loss choices). Multi-task sequential fine-tuning (generator→evaluator→generator): with evaluator margin loss: Pass@1 = 37.6%, Maj1@64 = 57.2%, Pass@64 = 82.6%; with evaluator xent: Pass@1 = 36.2%, Maj1@64 = 56.6%, Pass@64 = 82.2% (Table 4). Re-ranking strategy comparisons (optimistic best hyperparameters, PaLM 2-L): RR.MajK = 59.4%, W.RR = 60.8%, W.RR.MajK = 60.8-61.2% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>SSFT (fine-tuning generator only) gives modest gains vs few-shot; applying an evaluator for re-ranking substantially improves over Pass@1 and majority-voting baselines (e.g., Maj1@64=55.2% vs RR.Top-8=58.8%). Re-ranking only top-K clusters (MajK re-ranking) outperforms re-ranking all candidates and is more compute-efficient. Weighted re-ranking variants (W.RR and W.RR.MajK) achieve the best optimistic accuracy (~60.8–61.2%). Multi-task sequential fine-tuning (generator→evaluator→generator) improves generator Pass@1 and Maj1@64 relative to SSFT baseline (e.g., Pass@1 up to 37.6% vs 35.6%). The paper ablates evaluator loss functions (margin vs cross-entropy) and re-ranking scopes (All vs Top-8) and reports consistent trends.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Fine-grained, well-formatted step-by-step solutions (style/quality) strongly affect fine-tuning outcomes. 2) Evaluator-based re-ranking improves accuracy beyond majority voting; re-ranking only top-K clusters is both more accurate and more computationally efficient than re-ranking all candidates. 3) Multi-task sequential fine-tuning that trains the model on both evaluation and generation tasks sequentially yields better generation performance than generator-only SSFT. 4) Weighted re-ranking (and its MajK variant) yields the best optimistic re-ranking accuracy (~61%).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Contrastive/sequence-level margin objective used jointly with MLE was found difficult to balance and led to overfitting; thus the authors optimized multi-task objectives sequentially rather than jointly. Evaluator re-ranking is hard for smaller models: PaLM 2-S* evaluators did not reliably outperform majority voting. Some evaluator configurations (e.g., naive re-ranking of all solutions) perform worse than targeted Top-K re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Large Language Model Fine-tuning for Solving Math Problems', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3298.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3298.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-S*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 - Small*</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller PaLM 2 variant tested in the paper; used to study scale effects on re-ranking and fine-tuning benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-S*</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller PaLM 2 variant used as generator and evaluator in experiments to test scaling effects. Fine-tuned on MATH and PRM800K datasets similar to PaLM 2-L.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['step-by-step solution generation (SSFT)', 'sampling + majority voting (self-consistency)', 'solution-cluster re-ranking (SCR) as evaluator']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same reasoning representation (step-by-step solutions) and inference strategies (sampling + majority voting). Evaluator re-ranking trained similarly (margin or xent) but applied to smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses similar style (step-by-step) and the same set of inference strategies, but the smaller capacity limits successful use of evaluator-based re-ranking according to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>High-school math competition problems with human solutions; automatic grader checks mathematical equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Few-shot pre-trained PaLM 2-S*: Pass@1 = 17.4%, Maj1@64 = 27.2%, Pass@64 = 67.8% (Table 2). Supervised solution fine-tuning (MATH + PRM800K): Pass@1 = 22.6%, Maj1@64 = 38.8%, Pass@64 = 72.6% (Table 2). Re-ranking with PaLM 2-S* evaluator: RR.All -> 32.4% (margin) or 33.6% (xent) re-ranked accuracy; RR.Top-8 -> 36.6% (margin) or 35.4% (xent) (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>For PaLM 2-S*, re-ranking improves over its Pass@1 baseline, but the evaluator does not reliably surpass majority-voting baseline; PaLM 2-S* is less effective at evaluator-based re-ranking than PaLM 2-L, highlighting scale-dependence for the evaluation task.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller models benefit from fine-tuning but are less capable at the evaluator/re-ranker role; re-ranking top clusters helps but may still not outperform strong majority-voting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PaLM 2-S* evaluators were ineffective at re-ranking both PaLM 2-S* and PaLM 2-L generated solutions in cross-model tests (Table 6). This indicates that evaluation/re-ranking requires sufficient model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Large Language Model Fine-tuning for Solving Math Problems', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3298.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3298.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCR (Solution-Cluster Re-Ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solution-Cluster Re-Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A re-ranking technique introduced in this paper that combines majority-voting clustering with a learned solution evaluator to re-rank only top-K clusters, improving accuracy and compute efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method (not a single model): group sampled candidate answers by mathematical equivalence into clusters, then apply a fine-tuned solution evaluator (LLM) to score and re-rank solutions inside the most frequent clusters (Top-K) instead of all candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['clustered majority voting followed by evaluator re-ranking', "scoring via classification next-token probability ('correct' vs 'incorrect')", 'margin loss and cross-entropy training objectives for evaluator']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>First cluster N sampled solutions by mathematical equivalence (auto-grader). Then select top-K most frequent clusters. For each candidate in those clusters, prompt evaluator T(P, X) and use p_M('correct'|prompt) normalized against p('incorrect') as a score. Evaluator trained either with pairwise margin loss or cross-entropy classification loss. Final selection is max-scored solution within top-K clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Combines the similar reasoning style (step-by-step solution traces) with a diverse inference pipeline (clustering + focused re-ranking); diversity comes from combining majority voting (self-consistency) and learned evaluation rather than using different internal reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>High-school math competition problems requiring multi-step reasoning and numerical manipulation; solutions judged by mathematical equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Using PaLM 2-L evaluator and 64 sampled candidates: RR.All yields ~56.8–57.0% (loss-dependent), RR.Top-8 yields 58.8% (Table 3). Optimistic best configs: RR.MajK = 59.4%, W.RR = 60.8%, W.RR.MajK = 60.8–61.2% (Table 7). Re-ranking top-K clusters often outperforms re-ranking all candidates and majority voting (Maj1@64 = 55.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>SCR (Top-K) outperforms naive RR.All and plain majority-voting, while weighted cluster-based re-ranking variants yield the highest optimistic accuracy. The paper reports SR advantages in both accuracy and evaluator compute cost (Figure 1), and shows oracle upper bounds where re-ranking top-2 clusters could reach 64.0% with a perfect evaluator (Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Re-ranking restricted to top-K majority clusters is more accurate and efficient than re-ranking all candidates. Combining majority-voting clustering with learned evaluators leverages strengths of both methods and reduces evaluator computation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Fine-tuned evaluators trained on smaller models (PaLM 2-S*) do not generalize well and fail to surpass majority-voting; naive RR.All can be suboptimal in both accuracy and compute compared to focused Top-K re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Large Language Model Fine-tuning for Solving Math Problems', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3298.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3298.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Task Sequential Fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-task Sequential Fine-tuning (generator→evaluator→generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential fine-tuning recipe that alternates training the LLM as generator and as evaluator to transfer evaluation knowledge into improved generation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training protocol: (1) fine-tune model as a generator on step-by-step solutions (MLE), (2) fine-tune same model as solution evaluator using classification/margin objectives formatted as generation, (3) fine-tune again briefly as generator (MLE) to regain generative fluency while keeping evaluation-induced improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['generation (MLE) of step-by-step solutions', "evaluation framed as generation (classification tokens 'correct'/'incorrect')", 'sequential application of these objectives to bootstrap generation quality']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Sequence of training tasks uses the same next-token prediction paradigm by reformatting evaluation as a natural language generation problem (prompt T(P,X) with 'correct'/'incorrect' tokens). This allows evaluator supervision to be represented in a way congenial to the pretraining objective and transferred back to generation via a final brief MLE fine-tune.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>This method intentionally mixes roles (generation and evaluation) to diversify supervision signals received by a single model, while retaining a consistent step-by-step solution representation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>High-school math competition problems requiring chain-of-thought-style solutions; evaluation by mathematical equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Baseline generator (SSFT on MATH+PRM800K): Pass@1 = 35.6%, Maj1@64 = 55.2%, Pass@64 = 82.8%. Multi-task sequential (with evaluator margin loss): Pass@1 = 37.6%, Maj1@64 = 57.2%, Pass@64 = 82.6%. With evaluator xent: Pass@1 = 36.2%, Maj1@64 = 56.6%, Pass@64 = 82.2% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Multi-task sequential fine-tuning improves generation metrics (Pass@1 and Maj1@64) compared to generator-only SSFT. Margin-trained evaluator in the middle of the sequence produced the largest gains. Jointly optimizing MLE and contrastive objectives was attempted but led to overfitting; sequential scheduling was more effective.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Converting the evaluation task into a next-token prediction and training sequentially (generator→evaluator→generator) transfers useful evaluation signal to generation and yields measurable improvements in generator accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Direct joint optimization of MLE and contrastive/evaluation losses led to overfitting and was difficult to balance; the paper therefore recommends sequential optimization. Gains are modest and sensitive to evaluator training choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Large Language Model Fine-tuning for Solving Math Problems', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3298.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3298.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (PRM800K solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (used to generate PRM800K solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 generated step-by-step solutions (PRM800K dataset) were used as higher-quality fine-tuning targets; fine-grained GPT-4 solutions improve downstream fine-tuned model performance when used as training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4, referenced in Lightman et al. (2023) PRM800K dataset; GPT-4 chain-of-thought style outputs provided higher-granularity step-by-step solutions used as fine-tuning targets in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought / step-by-step solution generation (used to create PRM800K)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-4 outputs were produced using chain-of-thought prompting to elicit fine-grained, detailed step-by-step solutions which were then used as supervised targets (PRM800K) for fine-tuning PaLM 2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>GPT-4 here is a source of a particular solution style (fine-grained chain-of-thought); the paper uses these solutions to introduce higher-quality, more detailed step-by-step training examples rather than mixing multiple reasoning styles from GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH (as training data source PRM800K)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>PRM800K is a subset augmentation of MATH containing GPT-4-generated correct solutions used to increase fine-tuning data quality and quantity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Models fine-tuned on PRM800K (GPT-4 solutions) achieved higher performance than those fine-tuned only on original MATH solutions: PaLM 2-L fine-tuned on PRM800K: Pass@1 = 34.8%, Maj1@64 = 54.2%, Pass@64 = 83.4% (Table 2); combining MATH+PRM800K gave Pass@1 = 35.6%, Maj1@64 = 55.2%, Pass@64 = 82.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper compares fine-tuning on original human MATH solutions vs PRM800K GPT-4 solutions and finds PRM800K (more fine-grained) leads to better fine-tuned model performance, indicating style/quality of solution traces matters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-quality, fine-grained step-by-step solutions (like GPT-4 PRM800K) are more effective fine-tuning targets than the more abstract original MATH solutions; however, few-shot inference is relatively insensitive to the reference solution style, so fine-tuning is needed to realize the benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Replacing few-shot prompt exemplars with PRM800K-style solutions did not materially change few-shot performance of the pretrained models — improvements showed up after fine-tuning only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Large Language Model Fine-tuning for Solving Math Problems', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3298",
    "paper_id": "paper-8868a6d452b06bf4ad33237d0f3952d895ca20e7",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "PaLM 2-L",
            "name_full": "PaLM 2 - Large",
            "brief_description": "A large PaLM 2 variant evaluated in this paper; used as the primary experimental model for fine-tuning, re-ranking/evaluation, and multi-task sequential training on the MATH benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L",
            "model_description": "Large variant of Google's PaLM 2 family (transformer-based autoregressive LLM). Used as both generator (solution producer) and evaluator (solution verifier) in experiments; pretrained checkpoint provided by PaLM 2 paper.",
            "model_size": null,
            "reasoning_methods": [
                "step-by-step solution generation (chain-of-thought style)",
                "sampling + majority voting (self-consistency)",
                "solution-cluster re-ranking (evaluator-based re-ranking)",
                "multi-task sequential fine-tuning (generator↔evaluator↔generator)",
                "contrastive / margin loss (sequence-level ranking)",
                "classification-based verification (correct vs incorrect token probability)"
            ],
            "reasoning_methods_description": "Step-by-step solutions: models are fine-tuned to generate detailed solution traces (MLE) similar to chain-of-thought. Sampling + majority voting: nucleus sampling (T=0.6, top-p=0.95) is used to produce multiple candidate solutions and majority-voting (Maj1@N) selects the most frequent mathematical-equivalence cluster. Solution-cluster re-ranking (SCR): an evaluator LLM is fine-tuned (classification or margin loss) to score candidate solutions using the model probability of the token 'correct' vs 'incorrect'; re-ranking can be applied to all candidates (RR.All) or only top-K majority clusters (RR.Top-K). Multi-task sequential fine-tuning: sequentially fine-tune as generator (MLE), then evaluator (classification/margin), then generator again (MLE) to transfer evaluation signal into generation. Contrastive/margin loss: used to push higher sequence likelihood for correct solutions vs incorrect ones. Classification-based verifier: format prompt T(P, X) and use next-token probability of 'correct'/'incorrect' as score, trained with cross-entropy or margin losses.",
            "diversity_of_methods": "The paper uses a set of similar reasoning styles (step-by-step chain-of-thought style solutions) as the core reasoning representation but applies a diverse set of training and inference strategies (sampling + self-consistency, evaluator re-ranking, margin/contrastive objectives, and multi-task sequential fine-tuning). Determination is via controlled fine-tuning and ablation experiments: same core solution style but different evaluator and re-ranking strategies are compared.",
            "reasoning_task_name": "MATH",
            "reasoning_task_description": "MATH is a benchmark of high-school math competition problems with human-written step-by-step solutions and ground-truth answers; grading checks mathematical equivalence rather than textual match.",
            "performance_by_method": "Few-shot pre-trained PaLM 2-L (4-shot prompt): Pass@1 = 33.4%, Maj1@64 = 47.6%, Pass@64 = 79.4%. Supervised solution fine-tuning (SSFT) on combined MATH + PRM800K: Pass@1 = 35.6%, Maj1@64 = 55.2%, Pass@64 = 82.8% (Table 2). Solution-cluster re-ranking (PaLM 2-L evaluator): RR.All -&gt; 57.0% (margin loss) or 56.8% (xent) re-ranked accuracy (Table 3); RR.Top-8 -&gt; 58.8% (both loss choices). Multi-task sequential fine-tuning (generator→evaluator→generator): with evaluator margin loss: Pass@1 = 37.6%, Maj1@64 = 57.2%, Pass@64 = 82.6%; with evaluator xent: Pass@1 = 36.2%, Maj1@64 = 56.6%, Pass@64 = 82.2% (Table 4). Re-ranking strategy comparisons (optimistic best hyperparameters, PaLM 2-L): RR.MajK = 59.4%, W.RR = 60.8%, W.RR.MajK = 60.8-61.2% (Table 7).",
            "comparison_of_methods": "SSFT (fine-tuning generator only) gives modest gains vs few-shot; applying an evaluator for re-ranking substantially improves over Pass@1 and majority-voting baselines (e.g., Maj1@64=55.2% vs RR.Top-8=58.8%). Re-ranking only top-K clusters (MajK re-ranking) outperforms re-ranking all candidates and is more compute-efficient. Weighted re-ranking variants (W.RR and W.RR.MajK) achieve the best optimistic accuracy (~60.8–61.2%). Multi-task sequential fine-tuning (generator→evaluator→generator) improves generator Pass@1 and Maj1@64 relative to SSFT baseline (e.g., Pass@1 up to 37.6% vs 35.6%). The paper ablates evaluator loss functions (margin vs cross-entropy) and re-ranking scopes (All vs Top-8) and reports consistent trends.",
            "key_findings": "1) Fine-grained, well-formatted step-by-step solutions (style/quality) strongly affect fine-tuning outcomes. 2) Evaluator-based re-ranking improves accuracy beyond majority voting; re-ranking only top-K clusters is both more accurate and more computationally efficient than re-ranking all candidates. 3) Multi-task sequential fine-tuning that trains the model on both evaluation and generation tasks sequentially yields better generation performance than generator-only SSFT. 4) Weighted re-ranking (and its MajK variant) yields the best optimistic re-ranking accuracy (~61%).",
            "counter_examples_or_negative_results": "Contrastive/sequence-level margin objective used jointly with MLE was found difficult to balance and led to overfitting; thus the authors optimized multi-task objectives sequentially rather than jointly. Evaluator re-ranking is hard for smaller models: PaLM 2-S* evaluators did not reliably outperform majority voting. Some evaluator configurations (e.g., naive re-ranking of all solutions) perform worse than targeted Top-K re-ranking.",
            "uuid": "e3298.0",
            "source_info": {
                "paper_title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM 2-S*",
            "name_full": "PaLM 2 - Small*",
            "brief_description": "A smaller PaLM 2 variant tested in the paper; used to study scale effects on re-ranking and fine-tuning benefits.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-S*",
            "model_description": "Smaller PaLM 2 variant used as generator and evaluator in experiments to test scaling effects. Fine-tuned on MATH and PRM800K datasets similar to PaLM 2-L.",
            "model_size": null,
            "reasoning_methods": [
                "step-by-step solution generation (SSFT)",
                "sampling + majority voting (self-consistency)",
                "solution-cluster re-ranking (SCR) as evaluator"
            ],
            "reasoning_methods_description": "Same reasoning representation (step-by-step solutions) and inference strategies (sampling + majority voting). Evaluator re-ranking trained similarly (margin or xent) but applied to smaller model.",
            "diversity_of_methods": "Uses similar style (step-by-step) and the same set of inference strategies, but the smaller capacity limits successful use of evaluator-based re-ranking according to experiments.",
            "reasoning_task_name": "MATH",
            "reasoning_task_description": "High-school math competition problems with human solutions; automatic grader checks mathematical equivalence.",
            "performance_by_method": "Few-shot pre-trained PaLM 2-S*: Pass@1 = 17.4%, Maj1@64 = 27.2%, Pass@64 = 67.8% (Table 2). Supervised solution fine-tuning (MATH + PRM800K): Pass@1 = 22.6%, Maj1@64 = 38.8%, Pass@64 = 72.6% (Table 2). Re-ranking with PaLM 2-S* evaluator: RR.All -&gt; 32.4% (margin) or 33.6% (xent) re-ranked accuracy; RR.Top-8 -&gt; 36.6% (margin) or 35.4% (xent) (Table 3).",
            "comparison_of_methods": "For PaLM 2-S*, re-ranking improves over its Pass@1 baseline, but the evaluator does not reliably surpass majority-voting baseline; PaLM 2-S* is less effective at evaluator-based re-ranking than PaLM 2-L, highlighting scale-dependence for the evaluation task.",
            "key_findings": "Smaller models benefit from fine-tuning but are less capable at the evaluator/re-ranker role; re-ranking top clusters helps but may still not outperform strong majority-voting baselines.",
            "counter_examples_or_negative_results": "PaLM 2-S* evaluators were ineffective at re-ranking both PaLM 2-S* and PaLM 2-L generated solutions in cross-model tests (Table 6). This indicates that evaluation/re-ranking requires sufficient model capacity.",
            "uuid": "e3298.1",
            "source_info": {
                "paper_title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SCR (Solution-Cluster Re-Ranking)",
            "name_full": "Solution-Cluster Re-Ranking",
            "brief_description": "A re-ranking technique introduced in this paper that combines majority-voting clustering with a learned solution evaluator to re-rank only top-K clusters, improving accuracy and compute efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "method",
            "model_description": "Method (not a single model): group sampled candidate answers by mathematical equivalence into clusters, then apply a fine-tuned solution evaluator (LLM) to score and re-rank solutions inside the most frequent clusters (Top-K) instead of all candidates.",
            "model_size": null,
            "reasoning_methods": [
                "clustered majority voting followed by evaluator re-ranking",
                "scoring via classification next-token probability ('correct' vs 'incorrect')",
                "margin loss and cross-entropy training objectives for evaluator"
            ],
            "reasoning_methods_description": "First cluster N sampled solutions by mathematical equivalence (auto-grader). Then select top-K most frequent clusters. For each candidate in those clusters, prompt evaluator T(P, X) and use p_M('correct'|prompt) normalized against p('incorrect') as a score. Evaluator trained either with pairwise margin loss or cross-entropy classification loss. Final selection is max-scored solution within top-K clusters.",
            "diversity_of_methods": "Combines the similar reasoning style (step-by-step solution traces) with a diverse inference pipeline (clustering + focused re-ranking); diversity comes from combining majority voting (self-consistency) and learned evaluation rather than using different internal reasoning styles.",
            "reasoning_task_name": "MATH",
            "reasoning_task_description": "High-school math competition problems requiring multi-step reasoning and numerical manipulation; solutions judged by mathematical equivalence.",
            "performance_by_method": "Using PaLM 2-L evaluator and 64 sampled candidates: RR.All yields ~56.8–57.0% (loss-dependent), RR.Top-8 yields 58.8% (Table 3). Optimistic best configs: RR.MajK = 59.4%, W.RR = 60.8%, W.RR.MajK = 60.8–61.2% (Table 7). Re-ranking top-K clusters often outperforms re-ranking all candidates and majority voting (Maj1@64 = 55.2%).",
            "comparison_of_methods": "SCR (Top-K) outperforms naive RR.All and plain majority-voting, while weighted cluster-based re-ranking variants yield the highest optimistic accuracy. The paper reports SR advantages in both accuracy and evaluator compute cost (Figure 1), and shows oracle upper bounds where re-ranking top-2 clusters could reach 64.0% with a perfect evaluator (Appendix B).",
            "key_findings": "Re-ranking restricted to top-K majority clusters is more accurate and efficient than re-ranking all candidates. Combining majority-voting clustering with learned evaluators leverages strengths of both methods and reduces evaluator computation.",
            "counter_examples_or_negative_results": "Fine-tuned evaluators trained on smaller models (PaLM 2-S*) do not generalize well and fail to surpass majority-voting; naive RR.All can be suboptimal in both accuracy and compute compared to focused Top-K re-ranking.",
            "uuid": "e3298.2",
            "source_info": {
                "paper_title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Multi-Task Sequential Fine-tuning",
            "name_full": "Multi-task Sequential Fine-tuning (generator→evaluator→generator)",
            "brief_description": "A sequential fine-tuning recipe that alternates training the LLM as generator and as evaluator to transfer evaluation knowledge into improved generation performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "method",
            "model_description": "Training protocol: (1) fine-tune model as a generator on step-by-step solutions (MLE), (2) fine-tune same model as solution evaluator using classification/margin objectives formatted as generation, (3) fine-tune again briefly as generator (MLE) to regain generative fluency while keeping evaluation-induced improvements.",
            "model_size": null,
            "reasoning_methods": [
                "generation (MLE) of step-by-step solutions",
                "evaluation framed as generation (classification tokens 'correct'/'incorrect')",
                "sequential application of these objectives to bootstrap generation quality"
            ],
            "reasoning_methods_description": "Sequence of training tasks uses the same next-token prediction paradigm by reformatting evaluation as a natural language generation problem (prompt T(P,X) with 'correct'/'incorrect' tokens). This allows evaluator supervision to be represented in a way congenial to the pretraining objective and transferred back to generation via a final brief MLE fine-tune.",
            "diversity_of_methods": "This method intentionally mixes roles (generation and evaluation) to diversify supervision signals received by a single model, while retaining a consistent step-by-step solution representation.",
            "reasoning_task_name": "MATH",
            "reasoning_task_description": "High-school math competition problems requiring chain-of-thought-style solutions; evaluation by mathematical equivalence.",
            "performance_by_method": "Baseline generator (SSFT on MATH+PRM800K): Pass@1 = 35.6%, Maj1@64 = 55.2%, Pass@64 = 82.8%. Multi-task sequential (with evaluator margin loss): Pass@1 = 37.6%, Maj1@64 = 57.2%, Pass@64 = 82.6%. With evaluator xent: Pass@1 = 36.2%, Maj1@64 = 56.6%, Pass@64 = 82.2% (Table 4).",
            "comparison_of_methods": "Multi-task sequential fine-tuning improves generation metrics (Pass@1 and Maj1@64) compared to generator-only SSFT. Margin-trained evaluator in the middle of the sequence produced the largest gains. Jointly optimizing MLE and contrastive objectives was attempted but led to overfitting; sequential scheduling was more effective.",
            "key_findings": "Converting the evaluation task into a next-token prediction and training sequentially (generator→evaluator→generator) transfers useful evaluation signal to generation and yields measurable improvements in generator accuracy.",
            "counter_examples_or_negative_results": "Direct joint optimization of MLE and contrastive/evaluation losses led to overfitting and was difficult to balance; the paper therefore recommends sequential optimization. Gains are modest and sensitive to evaluator training choices.",
            "uuid": "e3298.3",
            "source_info": {
                "paper_title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (PRM800K solutions)",
            "name_full": "GPT-4 (used to generate PRM800K solutions)",
            "brief_description": "GPT-4 generated step-by-step solutions (PRM800K dataset) were used as higher-quality fine-tuning targets; fine-grained GPT-4 solutions improve downstream fine-tuned model performance when used as training data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4, referenced in Lightman et al. (2023) PRM800K dataset; GPT-4 chain-of-thought style outputs provided higher-granularity step-by-step solutions used as fine-tuning targets in this paper.",
            "model_size": null,
            "reasoning_methods": [
                "chain-of-thought / step-by-step solution generation (used to create PRM800K)"
            ],
            "reasoning_methods_description": "GPT-4 outputs were produced using chain-of-thought prompting to elicit fine-grained, detailed step-by-step solutions which were then used as supervised targets (PRM800K) for fine-tuning PaLM 2 variants.",
            "diversity_of_methods": "GPT-4 here is a source of a particular solution style (fine-grained chain-of-thought); the paper uses these solutions to introduce higher-quality, more detailed step-by-step training examples rather than mixing multiple reasoning styles from GPT-4.",
            "reasoning_task_name": "MATH (as training data source PRM800K)",
            "reasoning_task_description": "PRM800K is a subset augmentation of MATH containing GPT-4-generated correct solutions used to increase fine-tuning data quality and quantity.",
            "performance_by_method": "Models fine-tuned on PRM800K (GPT-4 solutions) achieved higher performance than those fine-tuned only on original MATH solutions: PaLM 2-L fine-tuned on PRM800K: Pass@1 = 34.8%, Maj1@64 = 54.2%, Pass@64 = 83.4% (Table 2); combining MATH+PRM800K gave Pass@1 = 35.6%, Maj1@64 = 55.2%, Pass@64 = 82.8%.",
            "comparison_of_methods": "The paper compares fine-tuning on original human MATH solutions vs PRM800K GPT-4 solutions and finds PRM800K (more fine-grained) leads to better fine-tuned model performance, indicating style/quality of solution traces matters.",
            "key_findings": "High-quality, fine-grained step-by-step solutions (like GPT-4 PRM800K) are more effective fine-tuning targets than the more abstract original MATH solutions; however, few-shot inference is relatively insensitive to the reference solution style, so fine-tuning is needed to realize the benefit.",
            "counter_examples_or_negative_results": "Replacing few-shot prompt exemplars with PRM800K-style solutions did not materially change few-shot performance of the pretrained models — improvements showed up after fine-tuning only.",
            "uuid": "e3298.4",
            "source_info": {
                "paper_title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 1
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        }
    ],
    "cost": 0.015143249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Improving Large Language Model Fine-tuning FOR SOLVING MATH PROBLEMS</h1>
<p>Yixin Liu ${ }^{1}$, Avi Singh ${ }^{2}$, C. Daniel Freeman ${ }^{2}$, John D. Co-Reyes ${ }^{2}$, Peter J. Liu ${ }^{2}$<br>${ }^{1}$ Yale University, ${ }^{2}$ Google DeepMind<br>yixin.liu@yale.edu, peterjliu@google.com</p>
<h4>Abstract</h4>
<p>Despite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM 2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately $58.8 \%$ accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an $11.2 \%$ accuracy improvement over the fewshot performance of pre-trained PaLM 2-L model with majority voting.</p>
<h2>1 INTRODUCTION</h2>
<p>Solving mathematical problems is a challenging task for even the state-of-the-art large language models (LLMs), e.g., GPT-4 (OpenAI, 2023) and PaLM 2 (Anil et al., 2023), since it requires the ability of creative thinking, mathematical reasoning and numerical calculation. However, LLMs are already showing potential of achieving better performance on this math problem solving task, as the likelihood of LLM's being able to find a correct answer is significantly higher when they are allowed to attempt the problem several times. For example, with greedy decoding, the pre-trained PaLM 2L can achieve around $33.4 \%$ accuracy. However, when sampling 64 solutions using temperature sampling, there is at least one correct solution (pass@64) $79.4 \%$ of the time (Table 2). This large performance gap suggests that LLMs can be capable of generating correct solutions while struggling to discriminate correct from incorrect solutions.</p>
<p>Therefore, we study task-specific fine-tuning methods that can improve the LLM's solution generation and evaluation ability such that the aforementioned performance gap can be reduced. Specifically, we explore three fine-tuning methods:
(1) Supervised step-by-step solution fine-tuning (SSFT). As a baseline method, we investigate whether the pre-trained LLMs can benefit from a supervised fine-tuning stage. To this end, we finetune the LLMs to generate the step-by-step solution and final answer as in Lightman et al. (2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(2) Solution-cluster Re-ranking (SCR). To enhance the LLM’s solution evaluation ability, we continue fine-tuning the generator as a solution evaluator for candidate solution re-ranking. While such a solution sample-rank, or re-ranking, has been investigated in previous work (Cobbe et al., 2021), we propose a new technique that can bring the benefits of both majority voting (Wang et al., 2023) and re-ranking together, while reducing ranking costs. Specifically, we first group the candidate answers into different clusters according to their mathematical equivalence, which is an intermediate step in majority voting. Then, we apply the solution evaluator to the solutions in the most-frequent clusters to gain further improvement over the majority voting results.
(3) Multi-task Sequential Fine-tuning. Apart from the solution evaluation task, we are also interested in improving the LLM’s performance on the solution generation task and exploring whether the training objective of solution evaluation can be beneficial to the solution generation model. To this end, we propose a sequential multi-task learning setting for the generation model where the solution evaluation task is formatted in the form of a natural language generation task, such that its training objective can provide meaningful supervision signal to the solution generation model. Concretely, we fine-tune the model in a sequential manner: fine-tuning (1) as a solution generator (SSFT), (2) as a solution evaluator (SCR), (3) as a generator (SSFT) again.
We conduct comprehensive experiments on the challenging MATH (Hendrycks et al., 2021b) dataset with PaLM 2-S* and PaLM 2-L- the small and large variants of PaLM 2 respectively (Anil et al., 2023) - which leads to these findings:</p>
<ul>
<li>For SSFT, the quality and the style of the step-by-step solutions can make a large impact on fine-tuned model, as they benefit more from fine-grained, well-formatted solutions.</li>
<li>Re-ranking solutions in the most-frequent solution clusters can yield better performance than re-ranking all the solutions while simultaneously achieving better computational efficiency, which we believe can be a better standard practice for future work.</li>
<li>Our proposed multi-task sequential fine-tuning can more effectively improve the solution generation model performance compared with supervised solution fine-tuning only, showing the benefit of training the model for both solution generation and evaluation tasks, presenting a successful attempt of leveraging learning signal of a binary evaluation task for a generation model.</li>
</ul>
<h1>2 BACKGROUND</h1>
<p>Mathematical problem solving is an important task (Hendrycks et al., 2021a;b; Cobbe et al., 2021) for measuring the LLMs’ reasoning and numerical computation abilities. In this work, we focus on the MATH dataset (Hendrycks et al., 2021b), which consists of problems collected from high school math competitions, along with the human-written solutions containing both natural language explanations and the final ground-truth solutions. MATH dataset is challenging to even the recent start-of-the-art large language models (LLMs), such as GPT-4 (OpenAI, 2023) and PaLM 2 (Anil et al., 2023), since they can only achieve 42.5\% and 33.2\% pass-at-1 accuracy (Anil et al., 2023). The accuracy is usually calculated through an automatic grading function $g$ checking the mathematical equivalence between the ground truth solution $A$ and the model solution $\tilde{A}$ :</p>
<p>$$
g(A, \tilde{A})= \begin{cases}1 &amp; \text { if } \tilde{A} \text { is equivalent to } A, \ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>Recent work has proposed various methods to improve LLM performance on the math problem solving task. In particular, majority voting, or self-consistency, can yield a significant improvement compared with the baseline performance of LLMs (Lewkowycz et al., 2022; Wang et al., 2023). Throughout this work, we will use the model’s pass-at-1, pass-at-N, and majority voting performance for model evaluation and comparison. The specific definitions are:
(1) Pass@1 (pass-at-1): the accuracy of the model’s greedy-decoded solution $A_{G}$, i.e., $g\left(A, A_{G}\right)$.
(2) Pass@N (pass-at-N): the oracle performance that always selects the correct solution when it is presented in $N$ temperature sampled solutions, $\left{\tilde{A}<em 2="2">{1}, \tilde{A}</em>}, \ldots, \tilde{A<em N_125_="N}" _in_123_1_2_="\in{1,2," _ldots_="\ldots," i="i">{N}\right}$, i.e, $\max </em>\right)$.} g\left(A, \tilde{A}_{i</p>
<p>(3) Maj1@N (majority-voting-at-N): $N$ sampled solutions are first clustered by their math equivalence, i.e., $g\left(\tilde{A}<em j="j">{i}, \tilde{A}</em>^{}\right)$. Then, one solution $\tilde{A<em>}$ from a most frequent cluster is selected for calculating the accuracy, $g\left(A, \tilde{A}^{</em>}\right)$.
(4) MajK@N: Similar to Pass@N, we define an oracle that always selects the correct solution when it is presented in the top-K majority-voting clusters, $\left{\tilde{A}<em K="K">{1}^{<em>}, \tilde{A}_{2}^{</em>}, \ldots, \tilde{A}</em>^{<em>}\right}$, so its accuracy is $\max <em i="i">{i \in{1,2, \ldots, K}} g\left(A, \tilde{A}</em>^{</em>}\right)$.
Another line of work leverages external tools such as Python programs to enhance the LLMs' ability (Chen et al., 2022; Wu et al., 2023; Yue et al., 2023; Zhou et al., 2023). In this work, we focus on improving the LLMs' inherent ability to solve math problems without help from external tools.</p>
<h1>3 MEthods</h1>
<h3>3.1 Supervised Solution Fine-tuning</h3>
<p>In Hendrycks et al. (2021b); Cobbe et al. (2021) models are fine-tuned to generate not only the final answer but also the step-by-step process for solving the math problem.</p>
<p>$$
S, A \leftarrow M(P)
$$</p>
<p>where $P$ is the math problem, $S, A$ are the ground-truth step-by-step solution and the final answer respectively, and $M$ is an LLM. In training, the solution $S$ and the final answer $A$ are concatenated into a single text sequence $X$, and the model is fine-tuned with the cross-entropy loss following the maximum likelihood estimation (MLE) paradigm:</p>
<p>$$
L_{\text {mle }}=-\log p_{M}(X \mid P)
$$</p>
<p>where $p_{M}$ is the probably distribution given by the auto-regressive language model $M$ :</p>
<p>$$
p_{M}(X \mid P)=\prod_{i} p_{M}\left(x_{i} \mid X_{0, \ldots, i-1}, P\right)
$$</p>
<p>Here, $x_{i}$ is the $i$-th token in $X, X_{0, \ldots, i-1}$ is the prefix before $x_{i}$.
To collect the ground-truth step-by-step solutions, we use two sources: (1) the original humanwritten solutions in the MATH dataset, (2) GPT-4 generated solutions provided in Lightman et al. (2023) with the chain-of-thought prompting eliciting step-by-step solutions. Our preliminary analysis found that the original solutions in the MATH dataset are more abstract while the solutions generated by GPT-4 are more fine-grained and detailed.</p>
<h3>3.2 Solution-Cluster Re-Ranking</h3>
<p>We note that there are two significant gaps for LLMs' math problem solving performance in Table 2: (1) the gap between the model's greedy-decoding (Pass@1) and majority-voting (Maj1@N) results; (2) the gap between the model's majority-voting best-at-1 (Maj1@N) and best-at-K performance (MajK@N). To narrow these gaps, we fine-tune the pre-trained LLM as a solution verifier/evaluator, following Cobbe et al. (2021). However, unlike in the previous work where a large number (e.g., 1000) of candidate solutions are all reranked by the evaluator, we combine the strength of majority voting and re-ranking together by only re-ranking the top-K solution clusters. We believe this reranking strategy is both more robust and cost-efficient, as will be elaborated in the following section.
To use the evaluator to score each candidate solution, we formulate the scoring task as a classification problem in a text completion format, inspired by related work on using LLMs for text evaluation (Liu et al., 2023; Fu et al., 2023). Concretely, we define a mapping function $T$ converting the math problem $P$ and a candidate solution $\tilde{X}$ into a prompt $T(P, \tilde{X})$ : "Here is a math problem: $P$, Here is a candidate solution: $\tilde{X}$, The above candidate solution is ". We then interpret the model-predicted probability of the word "correct" (or "incorrect") being the next token ${ }^{1}$ as the probability of the solution being correct (or incorrect):</p>
<p>$$
p_{\mathrm{cls}}\left(" \text { correct" } \mid \tilde{X}, P\right)=p_{M}\left(" \text { correct" } \mid T(P, \tilde{X})\right)
$$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$$
p_{\mathrm{cls}}\left(\text { "incorrect" } \mid \tilde{X}, P\right)=p_{M}\left(\text { "incorrect" } \mid T(P, \tilde{X})\right)
$$</p>
<p>We can then define the following normalized probability as the candidate solution score:</p>
<p>$$
S_{\mathrm{cls}}(\tilde{X} \mid P)=\frac{p_{\mathrm{cls}}\left(\text { "correct" } \mid \tilde{X}, P\right)}{p_{\mathrm{cls}}\left(\text { "correct" } \mid \tilde{X}, P\right)+p_{\mathrm{cls}}\left(\text { "incorrect" } \mid \tilde{X}, P\right)}
$$</p>
<p>With this scoring format, we investigate two training objectives:</p>
<h1>(1) Margin loss for pairwise comparison:</h1>
<p>$$
L_{\text {cls }- \text { margin }}=\max \left(0, \log S_{\mathrm{cls}}\left(\tilde{X}<em _mathrm_cls="\mathrm{cls">{\text {incorrect }} \mid P\right)-\log S</em> \mid P\right)+\lambda\right)
$$}}\left(\tilde{X}_{\text {correct }</p>
<p>where $\tilde{X}<em _incorrect="{incorrect" _text="\text">{\text {correct }}$ and $\tilde{X}</em>$ stand for a correct and an incorrect solution respectively, and $\lambda$ is a hyper-parameter for the margin.
(2) Cross-entropy loss for classification. The scoring format we designed is equivalent to a multiclass classification problem where "correct" and "incorrect" are the only valid options. Therefore, we can fine-tune the model using the cross-entropy loss for this classification task:}</p>
<p>$$
\begin{aligned}
L_{\text {cls-xent }} &amp; =-\mathbb{1}<em c="c" l="l" s="s">{\left{\tilde{X} \text { is correct }\right}}(\tilde{X}) \log p</em>, P\right) \
&amp; +\mathbb{1}}\left(\text { "correct" } \mid \tilde{X<em c="c" l="l" s="s">{\left{\tilde{X} \text { is incorrect }\right}}(\tilde{X}) \log p</em>, P\right)
\end{aligned}
$$}\left(\text { "incorrect" } \mid \tilde{X</p>
<h3>3.3 Multi-task SEQUENTIAL FINE-TUNING</h3>
<p>The MLE-based training objective defined in Eq. 3 is somewhat at odds with the ultimate binary evaluation target - whether the final answer is correct or not. Related work has explored better aligning training with the task evaluation using a contrastive learning objective (Edunov et al., 2018; Liu et al., 2022; Zhao et al., 2023), which interprets the model-predicted probability of a candidate solution $\tilde{X}$ as its quality score and uses the margin loss to encourage the model to assign higher probabilities to better candidates:</p>
<p>$$
L_{\text {seq }}=\max \left(0, \log p_{M}\left(\tilde{X}<em M="M">{\text {incorrect }} \mid P\right)-\log p</em> \mid P\right)+\lambda\right)
$$}\left(\tilde{X}_{\text {correct }</p>
<p>Then, the model is fine-tuned with the MLE and contrastive training objectives jointly:</p>
<p>$$
L_{\mathrm{ctr}}=L_{\mathrm{seq}}+\alpha_{1} L_{\mathrm{mle}}
$$</p>
<p>where $\alpha_{1}$ is a hyper-parameter.
However, the contrastive learning objective may not be as suitable for the math problem solving task because of the binary nature of the task - the objective requires the model to use the token likelihoods for two purposes: (1) predicting the next token, and (2) evaluating the quality of the entire text sequence. It can be a reasonable objective for natural language generation tasks such as text summarization where the next token prediction correctness is closely related to overall text quality. However, for math problem solving, the correctness of a solution might be decided by just a few tokens, making the next-token prediction task more distant and incompatible with the solution evaluation task. Consequently, we combine the training objectives in $\S 3.1$ (Eq. 3) and $\S 3.2$ (Eq. 8 and Eq. 9), introducing a new learning setting that formulates both math problem solution generation and evaluation tasks as natural language generation tasks:</p>
<p>$$
\begin{aligned}
L_{\text {mul-margin }} &amp; =L_{\text {cls-margin }}+\alpha_{2} L_{\mathrm{mle}} \
L_{\text {mul-xent }} &amp; =L_{\text {cls-xent }}+\alpha_{3} L_{\mathrm{mle}}
\end{aligned}
$$</p>
<p>where $\alpha_{2}$ and $\alpha_{3}$ are hyper-parameters. We believe we can better leverage the capacity of LLMs with this training setting since it is closer to the pre-training task (i.e., next-token prediction). In our preliminary experiments, we found that it is difficult to balance the two loss terms in Eq. 12 and Eq. 13 and the models start to overfit the MLE training objective very soon, possibly because of the limited size of the dataset. As the result, we optimize the multi-task objective in a sequential manner - instead of fine-tuning the model on both training objectives, we first fine-tuned the model as a generator (Eq. 8 or Eq. 9), then as an evaluator (Eq. 3), then finally as a generator again.</p>
<p>Table 1: Number of examples in dataset splits and the average length in tokens of the math problems and solutions.</p>
<table>
<thead>
<tr>
<th>Data Source</th>
<th># Training</th>
<th># Validation</th>
<th># Test</th>
<th>Problem Length</th>
<th>Solution Length</th>
</tr>
</thead>
<tbody>
<tr>
<td>MATH</td>
<td>11000</td>
<td>1000</td>
<td>500</td>
<td>90.2</td>
<td>249.6</td>
</tr>
<tr>
<td>PRM800K</td>
<td>6473</td>
<td>564</td>
<td>512</td>
<td>57.3</td>
<td>305.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of supervised solution fine-tuning. Different training data sources are compared, which are the MATH dataset and the PRM800K dataset.</p>
<p>|  | PaLM 2-S* | | | PaLM 2-L | | | | Pass@1 | Maj1@64 | Pass@64 |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Few-shot | Pass@1 | Maj1@64 | Pass@64 | Pass@1 | Maj1@64 | Pass@64 |  |  |  |
|  | $17.4 \%$ | $27.2 \%$ | $67.8 \%$ | $33.4 \%$ | $47.6 \%$ | $79.4 \%$ |  |  |  |
| MATH | $19.8 \%$ | $32.4 \%$ | $74.8 \%$ | $32.8 \%$ | $49.2 \%$ | $82.2 \%$ |  |  |  |
| PRM800K | $20.8 \%$ | $41.2 \%$ | $73.8 \%$ | $34.8 \%$ | $54.2 \%$ | $83.4 \%$ |  |  |  |
| MATH + PRM800K | $22.6 \%$ | $38.8 \%$ | $72.6 \%$ | $35.6 \%$ | $55.2 \%$ | $82.8 \%$ |  |  |  |</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 EXPERIMENTAL SETTING</h3>
<p>Datasets Our experiments are conducted on the MATH dataset. To prevent overfitting, we follow Lightman et al. (2023) by using the data splits they provided, ${ }^{2}$ where 4.5 K original test examples are used for training and validation, and the remaining 500 test examples are used for model evaluation. We leverage two sources of correct step-by-step solutions for the model training: (1) the original human-written explanations provided in the MATH dataset; (2) the model-generated correct solutions provided in PRM800K (Lightman et al., 2023), which only covers a subset problems in the original MATH dataset. The dataset statistics are provided in Table 1.</p>
<p>Evaluation We report the average solution accuracy (or correctness) for all the experiments. The correctness of the generated solution is compared against the ground-truth solution using the automatic grading script provided by Lightman et al. (2023). This script checks the mathematical equivalence instead of the simple textual equivalence. Two solution generation methods are mainly used to evaluate the model performance: (1) greedy decoding for the Pass@1 performance, (2) nucleus sampling (Holtzman et al., 2020) for the majority voting performance (Maj1@N), where we used the same sampling hyper-parameters as in Lewkowycz et al. (2022). Specifically, the sampling temperature is set to 0.6 , and the top- $p$ value is set to 0.95 .</p>
<h3>4.2 EXPERIMENT I: SUPERVISED SOLUTION FINE-TUNING</h3>
<p>We fine-tune PaLM 2-S* and PaLM 2-L on the step-by-step solutions with the MLE training objective (Eq. 3). Three specific fine-tuning strategies are explored: (1) fine-tuning using the original MATH solutions only; (2) fine-tuning using the PRM800K GPT-4 solutions only; (3) fine-tuning on both MATH and PRM800K solutions. We used the model performance on the validation set for checkpoint selection, and all the fine-tuned models achieved the best performance within two epochs. The results are shown in Table 2, where the few-shot performance with the pre-trained PaLM 2 models are provided for comparison. The few-shot results are obtained using a customized 4 -shot prompt designed in Lewkowycz et al. (2022).</p>
<p>We observe that the fine-tuning is generally helpful for the model to achieve better performance compared with the few-shot performance of the pre-trained checkpoints. Moreover, the quality and the style of the solutions can have a large impact on the model performance, since the models finetuned on PRM800K solutions achieve significantly better performance than the ones fine-tuned on the original MATH solutions.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Results of solution-cluster re-ranking. Two loss functions are compared, i.e., $L_{\text {cls-margin }}$ (Eq. 8) and $L_{\text {cls-xent }}$ (Eq. 9). Two re-ranking strategies are used: (1) re-ranking all the candidate solutions (RR.All), (2) re-ranking all solutions in the top-8 solution clusters (RR.Top-8). The baseline performance (Pass@1 and Maj1@64) are reported for comparison.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Loss Function</th>
<th style="text-align: center;">Pass@1</th>
<th style="text-align: center;">Maj1@64</th>
<th style="text-align: center;">RR.All</th>
<th style="text-align: center;">RR.Top-8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PaLM 2-S*</td>
<td style="text-align: center;">$L_{\text {cls-margin }}$</td>
<td style="text-align: center;">22.6\%</td>
<td style="text-align: center;">38.8\%</td>
<td style="text-align: center;">32.4\%</td>
<td style="text-align: center;">36.6\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$L_{\text {cls-xent }}$</td>
<td style="text-align: center;">22.6\%</td>
<td style="text-align: center;">38.8\%</td>
<td style="text-align: center;">33.6\%</td>
<td style="text-align: center;">35.4\%</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">$L_{\text {cls-margin }}$</td>
<td style="text-align: center;">35.6\%</td>
<td style="text-align: center;">55.2\%</td>
<td style="text-align: center;">57.0\%</td>
<td style="text-align: center;">58.8\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$L_{\text {cls-xent }}$</td>
<td style="text-align: center;">35.6\%</td>
<td style="text-align: center;">55.2\%</td>
<td style="text-align: center;">56.8\%</td>
<td style="text-align: center;">58.8\%</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of multi-task sequential fine-tuning. The model performance of the generator finetuned with the multi-task sequential setting is compared with the baseline generator trained with the MLE training objective only. Two model variants with different solution evaluation training objectives ( $L_{\text {cls-margin }}$ and $L_{\text {cls-xent }}$ ) are compared. All the model checkpoints are PaLM 2-L based.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;">Evaluator Loss Function</th>
<th style="text-align: center;">Pass@1</th>
<th style="text-align: center;">Maj1@64</th>
<th style="text-align: center;">Pass@64</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.6\%</td>
<td style="text-align: center;">55.2\%</td>
<td style="text-align: center;">82.8\%</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">$L_{\text {cls-margin }}$</td>
<td style="text-align: center;">37.6\%</td>
<td style="text-align: center;">57.2\%</td>
<td style="text-align: center;">82.6\%</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">$L_{\text {cls-xent }}$</td>
<td style="text-align: center;">36.2\%</td>
<td style="text-align: center;">56.6\%</td>
<td style="text-align: center;">82.2\%</td>
</tr>
</tbody>
</table>
<h1>4.3 EXPERIMENT II: Solution-Cluster Re-Ranking</h1>
<p>In Table 2, we observe that there is a large performance gap between the model's Pass@1 and Pass@64 performance, indicating that the model already has the ability to search for correct solutions, but fails to differentiate its different search results. Therefore, we continue fine-tuning the models as solution evaluators for the solution re-ranking task.</p>
<p>We investigate two loss functions from $\S 3.2$, the margin loss (Eq. 8) and the cross-entropy loss (Eq. 9) for the model training. For Eq. 8 we found that the model performance is not sensitive to the margin hyper-parameter $\lambda$, so we keep it fixed as $\log 2$ which intuitively requires the model to assign at least twice the probability to the correct solution. The checkpoints from $\S 3.2$ that are trained on both MATH and PRM800K solutions are used for this experiment, and the 64 candidate solutions are sampled from these checkpoints themselves for each problem. They are used to construct 10 training examples for the margin loss, and all of them are used for the cross-entropy loss. We observe all of the experiments converged within one epoch, possibly because of the redundancy in the training data. We make the following observations based on the results in Table 3:
(1) For both PaLM 2-S* and PaLM 2-L, the re-ranking result can outperform the Pass@1 performance of the baseline model. However, only the PaLM 2-L evaluator can outperform the robust majority-voting baseline, showing the re-ranking is a difficult task for relatively smaller models.
(2) Re-ranking only the solutions in the top solution clusters are consistently better than re-ranking all the solutions for both PaLM 2-S and PaLM 2-L. This re-ranking strategy is also more computationally efficient since there are fewer solutions to rank.</p>
<p>We provide further analysis in $\S 5$ and Appendix B.</p>
<h3>4.4 EXPERIMENT III: Multi-TASK SEQUENTIAL FINE-TUNING</h3>
<p>Having shown that better performance can be gained by fine-tuning the LLMs as a solution evaluator, we now investigate whether the training objective for solution evaluation is also helpful for the models to become better solution generators. To this end, we aim to use the multi-task sequential training objectives (Eq. 12 and Eq. 13) we proposed in $\S 3.3$ for the model fine-tuning: (1) the first step is exactly the experiments in $\S 4.2$, where the model is fine-tuned as a solution generator; (2) the model is then fine-tuned as a solution evaluator as in $\S 4.3$; (3) the evaluator is fine-tuned again with the MLE training objective to regain its ability as a solution generator. We note that in the last step the models are only trained for around 200 steps before they achieve the best performance.</p>
<p>Table 5: Analysis of the effect of the reference solution style and quality on the few-shot performance of pre-trained models and zero-shot performance of fine-tuned models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PaLM 2-S*</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaLM 2-L</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Maj1@64</td>
<td style="text-align: center;">Pass@64</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Maj1@64</td>
<td style="text-align: center;">Pass@64</td>
</tr>
<tr>
<td style="text-align: left;">MATH Few-shot</td>
<td style="text-align: center;">$17.4 \%$</td>
<td style="text-align: center;">$27.2 \%$</td>
<td style="text-align: center;">$67.8 \%$</td>
<td style="text-align: center;">$33.4 \%$</td>
<td style="text-align: center;">$47.6 \%$</td>
<td style="text-align: center;">$79.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PRM800K Few-shot</td>
<td style="text-align: center;">$19.2 \%$</td>
<td style="text-align: center;">$27.8 \%$</td>
<td style="text-align: center;">$70.0 \%$</td>
<td style="text-align: center;">$31.4 \%$</td>
<td style="text-align: center;">$47.2 \%$</td>
<td style="text-align: center;">$82.4 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Generalization ability of the solution evaluator. The evaluators are tested on solutions generated from different models. The evaluator checkpoints are trained with $L_{\text {cls-sent }}$ (Eq. 9).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluator</th>
<th style="text-align: center;">Solution Generator</th>
<th style="text-align: center;">Pass@1</th>
<th style="text-align: center;">Maj1@64</th>
<th style="text-align: center;">RR.All</th>
<th style="text-align: center;">RR.Top-8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PaLM 2-S*</td>
<td style="text-align: center;">PaLM 2-S*</td>
<td style="text-align: center;">$22.6 \%$</td>
<td style="text-align: center;">$38.8 \%$</td>
<td style="text-align: center;">$32.4 \%$</td>
<td style="text-align: center;">$36.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">$35.6 \%$</td>
<td style="text-align: center;">$55.2 \%$</td>
<td style="text-align: center;">$48.4 \%$</td>
<td style="text-align: center;">$50.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2-L</td>
<td style="text-align: center;">PaLM 2-S*</td>
<td style="text-align: center;">$22.6 \%$</td>
<td style="text-align: center;">$38.8 \%$</td>
<td style="text-align: center;">$46.0 \%$</td>
<td style="text-align: center;">$46.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">$35.6 \%$</td>
<td style="text-align: center;">$55.2 \%$</td>
<td style="text-align: center;">$56.8 \%$</td>
<td style="text-align: center;">$58.8 \%$</td>
</tr>
</tbody>
</table>
<p>The results in Table 4 show that the models fine-tuned with the multi-task learning objective can achieve better performance than models fine-tuned on the MLE training objective only (§4.2). It indicates that the training objective of the solution evaluation task can provide useful supervision signals to the solution generation model. We believe this is because formulating the solution evaluation task as next-word prediction can better leverage the LLM's ability gained during pre-training.</p>
<h1>5 ANALYSIS</h1>
<h3>5.1 Understanding the Effect of Step-by-step Solution Style</h3>
<p>In $\S 4.2$, we found that the models fine-tuned on the PRM800K solutions significantly outperform the ones fine-tuned on the original MATH solutions. We hypothesize this is because the PRM800K solutions are finer-grained and follows more closely to the step-by-step solution format (an example is shown in Figure 2). To investigate whether this difference can also affect the pre-trained models' few-shot performance, we rewrite the custom 4 -shot prompt used in Lewkowycz et al. (2022) by replacing the original MATH solution in the prompt with the PRM800K solution. The PRM800K solution is missing for one problem, which we replace with a similar problem with a valid solution.
Table 5 shows that the few-shot performance is relatively invariant to the difference of the reference solutions, indicating that fine-tuning is necessary for the model to benefit from the potentially higherquality solutions. We leave it as future work to investigate whether fine-tuning on the model's own generated solutions with the same style can achieve a similar effect (Zelikman et al., 2022).</p>
<h3>5.2 Evaluating the Generalization ability of the Solution Evaluator</h3>
<p>In $\S 4.3$, the PaLM 2-L solution evaluator shows a strong performance at re-ranking the candidate solutions generated by the related PaLM 2-L fine-tuned solution generator. We now investigate whether this evaluator can also be used to re-rank solutions generated by other models. The results in Table 6 prove the generalization ability of the fine-tuned PaLM 2-L evaluator. On the other hand, PaLM 2-S<em> is ineffective at re-ranking both the PaLM 2-L and PaLM 2-S</em> solutions, which suggests that solution evaluation is a non-trivial task that requires sufficiently large models.</p>
<h3>5.3 Comparing Different Solution Re-Ranking Strategies</h3>
<p>Previous work has proposed various re-ranking strategies for math problem candidate solutions. Therefore, we compare them using the fine-tuned PaLM 2-L evaluators with respect to their performance in re-ranked solution accuracy and efficiency. The re-ranking strategies compared are:</p>
<p>Table 7: Comparison of different re-ranking strategies with the PaLM 2-L evaluator fine-tuned with $L_{\text {cls-margin }}$ (Eq. 8) and $L_{\text {cls-xent }}$ (Eq. 9) respectively. The optimistic performance with the optimal hyper-parameter configuration is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Loss Function</th>
<th style="text-align: center;">RR.All</th>
<th style="text-align: center;">RR.MajK</th>
<th style="text-align: center;">W.RR</th>
<th style="text-align: center;">W.RR.MajK</th>
<th style="text-align: center;">Maj1</th>
<th style="text-align: center;">Maj1.TopN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$L_{\text {cls-margin }}$</td>
<td style="text-align: center;">$57.0 \%$</td>
<td style="text-align: center;">$59.4 \%$</td>
<td style="text-align: center;">$60.8 \%$</td>
<td style="text-align: center;">$60.8 \%$</td>
<td style="text-align: center;">$55.2 \%$</td>
<td style="text-align: center;">$59.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$L_{\text {cls-xent }}$</td>
<td style="text-align: center;">$56.8 \%$</td>
<td style="text-align: center;">$59.4 \%$</td>
<td style="text-align: center;">$60.8 \%$</td>
<td style="text-align: center;">$61.2 \%$</td>
<td style="text-align: center;">$55.2 \%$</td>
<td style="text-align: center;">$60.0 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Analysis of different re-ranking strategies. In the left figure, the performance of different re-ranking methods are plotted against the hyper-parameters. In the right figure, the evaluator computation cost is computed. The PaLM 2-L evaluator trained with the $L_{\text {cls-xent }}$ (Eq. 9) is used.</p>
<ol>
<li>Vanilla re-ranking (RR.All). The final solution $\tilde{X}^{*}$ is selected via</li>
</ol>
<p>$$
\tilde{X}^{*}=\arg \max <em c="c" l="l" s="s">{\tilde{X} \in \tilde{\mathcal{X}}} S</em> \mid P)
$$}(\tilde{X</p>
<p>where $S_{c l s}$ is the scoring function parameterized by the solution evaluator (Eq. 7), $P$ is a math problem, $\tilde{\mathcal{X}}$ is a set of candidate solutions.
2. MajK re-ranking (RR.MajK). This is the strategy we used in $\S 4.3$, which only re-ranks the set of solutions $\tilde{\mathcal{X}}_{K}$ in the top-K clusters from majority voting:</p>
<p>$$
\tilde{X}^{*}=\arg \max_{\tilde{X} \in \tilde{\mathcal{X}}<em c="c" l="l" s="s">{K}} S</em> \mid P)
$$}(\tilde{X</p>
<ol>
<li>Weighted re-ranking (W.RR). This strategy is proposed in Li et al. (2023) and adopted by Uesato et al. (2022), re-ranking answer clusters according to the sum of the answer scores in each clusters:</li>
</ol>
<p>$$
\tilde{X}^{*}=\arg \max_{\tilde{X} \in \tilde{\mathcal{X}}} \sum_{\tilde{X} \in \mathcal{X}} g(\tilde{X}, \hat{X}) S_{c l s}(\tilde{X} \mid P)
$$</p>
<p>where $g$ is the auto-grader that assigns 1 when two answers are equivalent and 0 otherwise (Eq. 1).
4. Weighted MajK re-ranking (W.RR.MajK). It combines re-ranking strategy 2 and 3 together:</p>
<p>$$
\tilde{X}^{*}=\arg \max_{\tilde{X} \in \tilde{\mathcal{X}}<em _tilde_X="\tilde{X">{K}} \sum</em>} \in \mathcal{X<em c="c" l="l" s="s">{K}} g(\tilde{X}, \hat{X}) S</em> \mid P)
$$}(\tilde{X</p>
<ol>
<li>(Self-consistency) majority voting (Maj1) (Wang et al., 2023):</li>
</ol>
<p>$$
\tilde{X}^{*}=\arg \max_{\tilde{X} \in \tilde{\mathcal{X}}} \sum_{\tilde{X} \in \tilde{\mathcal{X}}} g(\tilde{X}, \hat{X})
$$</p>
<ol>
<li>Majority voting of top-N solutions (Maj1.TopN). Cobbe et al. (2021) proposes a method that applies majority voting only on the top-N solutions $\hat{X}_{N}$ selected by the solution evaluator:</li>
</ol>
<p>$$
\hat{X}^{*}=\arg \max <em N="N">{\hat{X} \in \hat{X}</em>)
$$}} \sum_{\hat{X} \in \hat{X}_{N}} g(\hat{X}, \hat{X</p>
<p>In Table 7, we show the optimistic performance of each re-ranking strategy with the best hyperparameter configuration using the PaLM 2-L re-rankers and 64 generated candidate solutions. In Figure 1 we provide a detailed analysis with respect to both the re-ranking performance and the computation efficiency. We found that the weighted re-ranking strategy (W.RR) has strong performance, and the modified version we proposed (W.RR.MajK) can achieve comparable performance while reducing the computational cost of the solution evaluator.</p>
<h1>6 Related Work</h1>
<p>Hendrycks et al. (2021b) introduced the MATH dataset and fine-tuned GPT-2 (Radford et al., 2019) and smaller GPT-3 language models using (1) solution-and-answer and (2) answer-only as targets, but achieving less than $7 \%$ test accuracy, demonstrating the difficulty of the task at the time. Subsequently Cobbe et al. (2021) introduced GSM8K, a similar but easier, elementary-school math problem-solving dataset and showed that sampling followed by re-ranking using a trained verifier (or reward model) could improve performance significantly over a single sample.</p>
<p>Recent work has explored different methods for improving the LLMs' math solving ability. Specifically, Lewkowycz et al. (2022) proposes to continue pre-training the LLMs on math-specific corpora, gaining a significant improvement from the original pre-trained models. Uesato et al. (2022) extends outcome verifiers/reward-models (ORMs) to process reward models (PRMs) to judge solutions at the step-level by collecting human-annotated labels. They report a negative result in reranking using PRMs compared to ORMs on GSM8K but use them successfully to tune models using reinforcement learning. Lightman et al. (2023) in contrast scaled human annotation of process-level labels dramatically and achieved improved performance from reranking using PRMs on MATH with GPT4 (OpenAI, 2023) as the base model achieving state-of-the-art performance.</p>
<p>Apart from training methods that directly improve the base model performance, inference-time, prompt-engineering techniques, such as chain-of-thought (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022) and self-consistency (majority voting) (Wang et al., 2023), have also demonstrated their effectiveness with large language models such as PaLM/PaLM2 (Chowdhery et al., 2022; Anil et al., 2023) and GPT-3/4 (Brown et al., 2020; OpenAI, 2023), to the point that state-of-the-art performance on GSM8K is nearing 100\%, and hence renewed focus on the harder MATH task. Some recent work (Chen et al., 2022; Gao et al., 2023; Wu et al., 2023; Yue et al., 2023) focuses on leveraging external tools, such as Python programs, to complement the LLM's ability, which shows further improvement over pure LLM-based methods. However we focus on the setting with no tools.</p>
<p>Zelikman et al. (2022) shows that iteratively sampling and fine-tuning to the correct model solutions can improve mathematical performance (GSM8K). Although we do not employ this technique, it is orthogonal and conceivably may further improve performance.</p>
<h2>7 CONCLUSION</h2>
<p>In this work we investigated different fine-tuning methods to improve the LLMs' performance on math problem solving. Starting with supervised step-by-step fine-tuning, we first demonstrated the importance of step-by-step solutions for improving fine-tuned LLM performance. We then studied re-ranking methods for fine-tuning the LLMs as solution evaluators, and proposed a new re-ranking method which combines the benefit of majority voting and re-ranking together, simultaneously achieving better solution accuracy and computational efficiency. Lastly, we introduced a multi-task sequential fine-tuning method, aiming at improving the model's solution generation ability with the training objective of the solution evaluation. Our method outperforms the baseline fine-tuning method based on the solution generation training objective only, demonstrating its ability of improving a generation task using the supervision signal of the corresponding evaluation task.</p>
<h1>REFERENCES</h1>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El Shafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, C Crépy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, M. C. D’iaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag, Xavier García, Sebastian Gehrmann, Lucas González, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. ArXiv, abs/2305.10403, 2023. URL https://api.semanticscholar.org/ CorpusID:258740735.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL https://api.semanticscholar.org/CorpusID:239998651.</p>
<p>Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc'Aurelio Ranzato. Classical structured prediction losses for sequence to sequence learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 355-364, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1033. URL https://aclanthology.org/N18-1033.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In Andreas Krause, Emma</p>
<p>Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 10764-10799. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/gao23f.html.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021a.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021b.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rygGQyrFvH.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.</p>
<p>Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.acl-long.291. URL https://aclanthology.org/2023.acl-long.291.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. ArXiv, abs/2305.20050, 2023. URL https://api.semanticscholar.org/ CorpusID:258987659.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.</p>
<p>Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. BRIO: Bringing order to abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2890-2903, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.207. URL https://aclanthology.org/2022.acl-long. 207.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021.</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api. semanticscholar.org/CorpusID:257532815.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQiMeSB_J.</p>
<p>Yiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with gpt-4. arXiv preprint arXiv:2306.01337, 2023.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 15476-15488. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/639a9a172c044fbb64175b5fad42e9a5-Paper-Conference.pdf.</p>
<p>Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=0qSOodKmJaN.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. ArXiv, abs/2308.07921, 2023. URL https://api.semanticscholar.org/CorpusID:260900008.</p>
<h1>A EXAMPLES OF STEP-BY-STEP SOLUTIONS</h1>
<h2>Problem</h2>
<p>What is the greatest common factor of 20 ! and 200,000 ? (Reminder: If $n$ is a positive integer, then $n$ ! stands for the product $1 \cdot 2 \cdot 3 \cdots \cdot(n-1) \cdot n$.)</p>
<h2>MATH Solution</h2>
<p>The prime factorization of 200,000 is $2^{6} \cdot 5^{5}$. Then count the number of factors of 2 and 5 in 20 ! Since there are 10 even numbers, there are more than 6 factors of 2 . There are 4 factors of 5 . So the greatest common factor is $2^{6} \cdot 5^{4}=40,000$.</p>
<h2>PRM800k solution</h2>
<p>I want to find the largest positive integer that divides both 20 ! and 200,000 evenly. One way to do this is to factor both numbers into prime factors and look for the common ones.
I know that $200,000=2^{5} \cdot 10^{4}=2^{9} \cdot 5^{4}$.
To find the prime factorization of 20 !, I can use the fact that it is the product of all the positive integers from 1 to 20 .
For each prime number $p$ between 1 and 20 , I can count how many multiples of $p$ are in that range.
For example, there are 10 multiples of 2 between 1 and 20 , namely $2,4,6, \ldots, 20$.
But there are also 5 multiples of 4 , which is $2^{2}$, and 2 multiples of 8 , which is $2^{3}$, and 1 multiple of 16 , which is $2^{4}$.
So, the total power of 2 in 20 ! is $10+5+2+1=18$.
Similarly, there are 4 multiples of 5 , namely $5,10,15,20$, so the power of 5 in 20 ! is 4 .
There are 6 multiples of 3 , namely $3,6,9, \ldots, 18$, but there are also 2 multiples of 9 , which is $3^{2}$, so the power of 3 in 20 ! is $6+2=8$.
There are 2 multiples of 7 , namely 7 and 14 , so the power of 7 in 20 ! is 2 .
There are 1 multiple of each of the other prime numbers $11,13,17$, and 19 , so the powers of those primes in 20 ! are 1 each.
Therefore, the prime factorization of 20 ! is $2^{18} \cdot 3^{8} \cdot 5^{4} \cdot 7^{2} \cdot 11 \cdot 13 \cdot 17 \cdot 19$.
To find the greatest common factor of 20 ! and 200,000 , I need to take the lowest power of each common prime factor.
The only common prime factors are 2 and 5 , and the lowest powers are 9 and 4 , respectively.
So, the greatest common factor is $2^{9} \cdot 5^{4}=512 \cdot 625=320,000$.
# Answer
320,000
Figure 2: Example comparing MATH and GPT-4 generated step-by-step solutions.</p>
<h2>B ANALYSIS OF SOLUTION CLUSTERS</h2>
<p>We conduct an in-depth analysis to better understand the property of the solution clusters. We first analyze the performance upper-bound of the solution-cluster re-ranking approach in Figure 3, which indicates that the evaluators we trained in $\S 4.3$ can still be further improved. In fact, with a perfect solution evaluator, an accuracy of $64.0 \%$ can be achieved by re-ranking just the top-2 clusters, while the majority-voting performance is only $55.2 \%$.</p>
<p>In Figure 4, we further investigate the difficulty of the re-ranking task by analyzing the characteristics of the PaLM 2-L solution clusters. Specifically, we compare the size of the correct solution cluster against (1) the number of all sampled solutions, (2) the size of the top-1 solution cluster when the correct solution is not selected by the majority voting. The results show two trends: (1) in general the re-ranking task is difficult, since the correct solutions only consist of less than $5 \%$ of all solutions in $50 \%$ of the time; (2) re-ranking top-clusters is relatively easier, since the ratio of the size of correct solution cluster and the top-1 solution cluster is much more balanced. We believe these observations can partially explain the benefit of the solution-cluster re-ranking method we proposed.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Performance comparison of the majority voting (Maj1@64) and the oracle that always selects the correct solution in the top-K clusters (MajK@64).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Distributions of (1) the ratio of the number of correct solutions v.s. all solutions, (2) the ratio of number of correct solutions v.s. Maj1@64 solutions when the correct solution is not the Maj1@64 solution.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/openai/prm800k&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
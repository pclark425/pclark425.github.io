<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9874 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9874</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9874</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-d2f00070f820375f7013b9466e464c239c238070</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d2f00070f820375f7013b9466e464c239c238070" target="_blank">What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a CoE discrimination approach and conducts a comparative analysis between CoE and Non-CoE samples across significance, deceptiveness, and robustness, revealing the LLM's preference for external knowledge that aligns with CoE features.</p>
                <p><strong>Paper Abstract:</strong> Incorporating external knowledge has emerged as a promising way to mitigate outdated knowledge and hallucinations in LLM. However, external knowledge is often imperfect, encompassing substantial extraneous or even inaccurate content, which interferes with the LLM's utilization of useful knowledge in the context. This paper seeks to characterize the features of preferred external knowledge and perform empirical studies in imperfect contexts. Inspired by the chain of evidence (CoE), we characterize that the knowledge preferred by LLMs should maintain both relevance to the question and mutual support among the textual pieces. Accordingly, we propose a CoE discrimination approach and conduct a comparative analysis between CoE and Non-CoE samples across significance, deceptiveness, and robustness, revealing the LLM's preference for external knowledge that aligns with CoE features. Furthermore, we selected three representative tasks (RAG-based multi-hop QA, external knowledge poisoning and poisoning defense), along with corresponding SOTA or prevalent baselines. By integrating CoE features, the variants achieved significant improvements over the original baselines.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9874.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9874.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Evidence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A characterization for preferred external knowledge for LLMs requiring two properties: (1) relevance to the question (intent), and (2) interconnectivity (mutual support among textual pieces via evidence nodes and evidence relations). Designed to improve LLM robustness in multi-hop QA by forming structured reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Five representative LLMs spanning closed-source (GPT-3.5, GPT-4) and open-source (Llama2-13B, Llama3-70B, Qwen2.5-32B) models used for evaluation across scales and architectures in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing — Multi-hop Question Answering / Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated empirical evaluation using controlled contexts (CoE vs Non-CoE) with injected noise/poisoning, judged by GPT-4 (automated judge) to compute accuracy and attack success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Significance (improvement in answer accuracy), Deceptiveness (attack success rate when CoE is poisoned), Robustness (accuracy under conflicting misinformation), Usability (improvements when integrated into RAG pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>HotpotQA and 2WikiMultihopQA as multi-hop benchmarks; additionally a single-hop RGB dataset used for single-hop experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CoE-aligned external knowledge yields large empirical gains: average ACC ~92.0% across five LLMs and two datasets, outperforming Non-CoE_SenP by 22.5% and Non-CoE_WordP by 16.3%. CoE's ACC degrades only modestly with up to 75% extraneous information (drop ~1.8%), whereas Non-CoE variants drop much more. CoE also yields higher attack success rates when poisoned (average ASR 85.4%) and greater robustness against injected conflicting knowledge (average ACC 84.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CoE can contain incorrect facts and then mislead LLMs; extraction of CoE features relies on prompt-based GPT-4o extraction (possible extraction errors); method operates at textual level and is less suitable for purely vector-based retrieval pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Evaluation relied on automated LLM judging and empirical metrics rather than human expert review; no direct human-expert comparison is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prioritize retrieval/reranking strategies that favor CoE features (intent first, then relations, then nodes) to improve multi-hop QA reliability, but apply elevated safeguards because CoE-structured falsehoods are highly deceptive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9874.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9874.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoE-Disc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoE Discrimination (feature-extraction + alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-driven pipeline that extracts intent, evidence nodes, and evidence relations from a question (via GPT-4o) and then classifies whether a candidate external knowledge snippet contains each feature (textual entailment for intent, containment checks for nodes/relations). A snippet is CoE-aligned only when all features are present.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o (used for extraction and discrimination); evaluated LLMs: GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4o used as an assistant for structured prompting/extraction and binary classification of feature coverage; the evaluated LLMs are as above.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP — information extraction and retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated extraction and binary discrimination followed by using resulting labels to assemble CoE vs Non-CoE contexts; empirical evaluation on benchmark QA pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness of extraction (impact on downstream ACC measured), binary yes/no judgments for intent/node/relation coverage, downstream QA performance (ACC/ASR).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Constructed CoE/Non-CoE corpora from 1,336 instances sampled from HotpotQA and 2WikiMultihopQA; additional single-hop RGB dataset for ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Extraction and discrimination pipeline produced 676 CoE items from HotpotQA candidates and 660 from 2Wiki; downstream ACC remains high even under imperfect extraction (accuracy drop from ~90.2% to ~89.3%-89.4% under simulated extraction errors).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Extraction depends on GPT-4o prompts and can produce missing or spurious evidence nodes; such extraction errors change CoE identification counts and slightly reduce downstream accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No human annotation/evaluation of extracted features reported; discrimination is automated and integrated into experimental pipeline rather than cross-checked by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use few-shot and carefully designed prompts for extraction; tolerate some extraction noise as downstream performance is robust, but validate when possible to avoid systematic mis-identification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9874.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9874.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eval-Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Evaluation Metrics (ACC and ASR with GPT-4 judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses automated metrics: Accuracy (ACC) for correctness of answers and Attack Success Rate (ASR) for deception effectiveness; GPT-4 (or GPT-4o) is used as an automated judge to compute these metrics by comparing model outputs to ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (used as automated judge); evaluated LLMs: GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 (and GPT-4o for some pipeline steps) served as the evaluator to compute ACC/ASR automatically instead of manual human scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated judging by GPT-4 comparing generated answers to ground-truth; scoped to question-answer matching and attack-success definitions (percentage of outputs conforming to poisoned target).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>ACC: proportion of correct answers compared to ground truth; ASR: proportion of model outputs that follow injected (poisoned) incorrect answers; statistical significance evaluated (p < 0.05) for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>HotpotQA and 2WikiMultihopQA (multi-hop); RGB (single-hop) used for select experiments; evaluations reported across varying noise/poison ratios (0 to 0.75).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ACC and ASR trends illustrate CoE superiority: average ACC of CoE ~92.0% vs large drops for Non-CoE; average ASR for poisoned CoE ~85.4% vs ~64-69% for Non-CoE variants; metric sensitivity reported across extraneous and misinformation proportions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Automated judging by LLMs may inherit biases and is not equivalent to human expert evaluation; reliance on automated judge may obscure nuanced correctness/partial-credit judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No parallel human-judged evaluations reported; method uses LLM-based automatic evaluation following prior QA evaluation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When using automated LLM judges, report significance and examine robustness across noise/proportion sweeps; consider human checks for critical deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9874.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9874.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perturbation Strategies (SenP, WordP, extraneous & misinformation injection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled perturbations applied to CoE to create Non-CoE variants: (SenP) sentence-level removal to simulate missing evidence; (WordP) word-level substitution replacing evidence nodes with higher-level expressions; plus injection of extraneous (lexically similar but irrelevant) text and contradictory misinformation at controlled proportions (0, 0.25, 0.5, 0.75).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Evaluated LLMs: GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>These perturbations were applied when querying the evaluated LLMs to measure sensitivity to missing details, higher-level node substitutions, irrelevant noise, and direct contradictions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP robustness testing / adversarial evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Systematic ablation and perturbation experiments where CoE contexts are modified and model ACC/ASR measured across proportion sweeps of extraneous and misinformation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Impact on ACC (significance and robustness), impact on ASR (deceptiveness), stability across noise levels; ablations removing intent, nodes, relations measure feature importance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Constructed CoE/Non-CoE instances from HotpotQA and 2WikiMultihopQA (1,336 total QA pairs) and single-hop RGB tests for cross-checks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SenP produces larger performance degradation than WordP; removing intent causes largest accuracy drop (33.9% in one ablation), evidence nodes removal reduces accuracy by ~13.6%, relations removal by ~10.7%. CoE remains stable with increasing extraneous proportion (ACC drop ~1.8% up to 75% extraneous), whereas Non-CoE variants drop far more.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Perturbations are synthetic and controlled—real-world corruptions may differ; WordP keeps more original semantics and thus can be less damaging than sentence removal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>This is an automated adversarial evaluation approach as opposed to human adversarial or domain-expert perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multi-granularity perturbations (sentence- and word-level) and proportion sweeps to assess robustness; prioritize intent coverage and evidence relations in retrieval/reranking to mitigate perturbation impact.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9874.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9874.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmark Datasets (HotpotQA, 2WikiMultihopQA, RGB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Datasets used as benchmarks for multi-hop and single-hop evaluation: HotpotQA and 2WikiMultihopQA provide multi-hop QA instances with supporting facts for chain construction; RGB used as a single-hop dataset for ablations and generalizability checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Evaluated LLMs: GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Benchmarks supply test QA pairs and supporting evidence used to build CoE and Non-CoE contexts that are fed to the LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP — QA benchmarks for reasoning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Sampled 1,000 instances from each dataset, applied CoE discrimination to supporting evidence and constructed CoE/Non-CoE contexts; evaluated LLM outputs on ACC/ASR under noise/poisoning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-question ACC and ASR, breakdowns by hop count (one-hop, two-hop, three-hop) and robustness to varying noise/poisoning proportions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>HotpotQA: multi-hop QA with supporting facts; 2WikiMultihopQA: constructed multi-hop dataset for reasoning steps; RGB: single-hop dataset used for ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>From sampled candidates, 676 CoE items were identified from HotpotQA (avg 4.0 sentences) and 660 from 2WikiMultihopQA (avg 3.4 sentences). CoE improvements hold across datasets and across hop counts, with single-hop showing highest stability (>92% ACC).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Datasets have curated supporting facts which may bias towards CoE-like structures; real-world retrieval outputs may be noisier or differently distributed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Benchmarks provide ground-truth answers for automated comparison; no human scoring reported.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multi-hop benchmarks with annotated supporting facts to construct and validate CoE; evaluate across hop counts to ensure generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9874.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9874.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG+CoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coverage-based Reranking (RAG+CoE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RAG variant that augments standard retrieve-rank-generate pipelines by reranking retrieved snippets based on coverage of CoE features (intent, evidence relations, evidence nodes), using a minimal coverage selection algorithm to prioritize intent first, then relations, then nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applied with LLMs used as generators/evaluators including GPT-3.5, GPT-4, Llama2-33B/70B variants, Qwen2.5-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Reranker enhancement is model-agnostic; the improved contexts feed into the same generation LLMs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP — retrieval-augmented generation and reranking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Empirical comparison of vanilla RAG vs RAG+CoE on HotpotQA multi-hop QA, and integration into poisoning and defense pipelines (PoisonedRAG and InstructRAG variants).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Multi-hop QA accuracy (ACC), poisoning attack success rate (ASR) for attack variants, defense ACC/ASR for defense variants.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>HotpotQA used for RAG evaluations; Google Search API top-5 snippets formed baseline contexts for RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RAG+CoE yields average ACC improvement of 10.4% over standard RAG (example: GPT-4 RAG ACC 72.9% → RAG+CoE 82.6%). In poisoning, PR+CoE increases ASR by ~11.0% over PR; in defenses, IR+CoE increases ACC by ~27.2% and reduces ASR by ~35.8% compared to IR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Technique assumes availability of textual snippets and post-hoc textual discrimination—less applicable to purely vector-based retrieval systems; potential to prioritize CoE that is factually incorrect, increasing risk of deception.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Compared empirically to standard RAG pipelines and state-of-the-art poisoning/defense baselines; no human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>In reranking, prioritize intent coverage, then relations, then nodes; combine CoE-based reranking with verification mechanisms to avoid following incorrect CoE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9874.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9874.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Empirical-Results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical Evaluation Results & Ablations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregated quantitative outcomes and ablation findings showing CoE's effects on significance, deceptiveness and robustness: numeric ACC and ASR summaries, feature ablations (intent/nodes/relations), and behavior across hop numbers and noise proportions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Results are aggregated across the five LLMs tested to demonstrate trends that generalize across architectures and scales.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Controlled experiments with CoE vs Non-CoE across extraneous and misinformation proportions, plus ablations removing intent/nodes/relations, and comparisons by number of reasoning hops.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>ACC and ASR values, percentage differences between groups, statistical significance (p < 0.05) for comparisons to CoE, stability metrics across proportion sweeps.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>HotpotQA, 2WikiMultihopQA, and single-hop RGB dataset for cross-checks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key reported numbers: average CoE ACC 92.0% vs Non-CoE_SenP/WordP significantly lower; CoE ASR (when poisoned) average 85.4% vs Non-CoE lower by ~16–20%; ablation: removing intent caused largest ACC drop (~33.9%), evidence nodes (~13.6%), relations (~10.7%); CoE ACC stable across extraneous proportion (0→0.75) with ~1.8% drop.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Findings depend on constructed CoE and synthetic perturbations; CoE can be abused to create potent poisoning attacks; automated judge and dataset choices affect generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>All quantitative comparisons are between automated-system variants and baselines rather than human baseline theory-evaluation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use feature-level ablations to identify critical components (intent most important), incorporate CoE-awareness in retrieval/reranking, and add safeguards against CoE-structured poisoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>HotpotQA: A dataset for diverse, explainable multi-hop question answering <em>(Rating: 2)</em></li>
                <li>Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps <em>(Rating: 2)</em></li>
                <li>Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models <em>(Rating: 2)</em></li>
                <li>InstructRAG: Instructing retrieval-augmented generation via self-synthesized rationales <em>(Rating: 1)</em></li>
                <li>Evaluating correctness and faithfulness of instruction-following models for question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9874",
    "paper_id": "paper-d2f00070f820375f7013b9466e464c239c238070",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "CoE",
            "name_full": "Chain of Evidence",
            "brief_description": "A characterization for preferred external knowledge for LLMs requiring two properties: (1) relevance to the question (intent), and (2) interconnectivity (mutual support among textual pieces via evidence nodes and evidence relations). Designed to improve LLM robustness in multi-hop QA by forming structured reasoning chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B",
            "llm_description": "Five representative LLMs spanning closed-source (GPT-3.5, GPT-4) and open-source (Llama2-13B, Llama3-70B, Qwen2.5-32B) models used for evaluation across scales and architectures in the paper.",
            "scientific_domain": "Natural Language Processing — Multi-hop Question Answering / Retrieval-Augmented Generation",
            "evaluation_method": "Automated empirical evaluation using controlled contexts (CoE vs Non-CoE) with injected noise/poisoning, judged by GPT-4 (automated judge) to compute accuracy and attack success rates.",
            "evaluation_criteria": "Significance (improvement in answer accuracy), Deceptiveness (attack success rate when CoE is poisoned), Robustness (accuracy under conflicting misinformation), Usability (improvements when integrated into RAG pipelines).",
            "benchmark_or_dataset": "HotpotQA and 2WikiMultihopQA as multi-hop benchmarks; additionally a single-hop RGB dataset used for single-hop experiments.",
            "results_summary": "CoE-aligned external knowledge yields large empirical gains: average ACC ~92.0% across five LLMs and two datasets, outperforming Non-CoE_SenP by 22.5% and Non-CoE_WordP by 16.3%. CoE's ACC degrades only modestly with up to 75% extraneous information (drop ~1.8%), whereas Non-CoE variants drop much more. CoE also yields higher attack success rates when poisoned (average ASR 85.4%) and greater robustness against injected conflicting knowledge (average ACC 84.1%).",
            "limitations_or_challenges": "CoE can contain incorrect facts and then mislead LLMs; extraction of CoE features relies on prompt-based GPT-4o extraction (possible extraction errors); method operates at textual level and is less suitable for purely vector-based retrieval pipelines.",
            "comparison_to_human_or_traditional": "Evaluation relied on automated LLM judging and empirical metrics rather than human expert review; no direct human-expert comparison is reported.",
            "recommendations_or_best_practices": "Prioritize retrieval/reranking strategies that favor CoE features (intent first, then relations, then nodes) to improve multi-hop QA reliability, but apply elevated safeguards because CoE-structured falsehoods are highly deceptive.",
            "uuid": "e9874.0",
            "source_info": {
                "paper_title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CoE-Disc",
            "name_full": "CoE Discrimination (feature-extraction + alignment)",
            "brief_description": "A prompt-driven pipeline that extracts intent, evidence nodes, and evidence relations from a question (via GPT-4o) and then classifies whether a candidate external knowledge snippet contains each feature (textual entailment for intent, containment checks for nodes/relations). A snippet is CoE-aligned only when all features are present.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o (used for extraction and discrimination); evaluated LLMs: GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B",
            "llm_description": "GPT-4o used as an assistant for structured prompting/extraction and binary classification of feature coverage; the evaluated LLMs are as above.",
            "scientific_domain": "NLP — information extraction and retrieval-augmented generation",
            "evaluation_method": "Automated extraction and binary discrimination followed by using resulting labels to assemble CoE vs Non-CoE contexts; empirical evaluation on benchmark QA pairs.",
            "evaluation_criteria": "Correctness of extraction (impact on downstream ACC measured), binary yes/no judgments for intent/node/relation coverage, downstream QA performance (ACC/ASR).",
            "benchmark_or_dataset": "Constructed CoE/Non-CoE corpora from 1,336 instances sampled from HotpotQA and 2WikiMultihopQA; additional single-hop RGB dataset for ablations.",
            "results_summary": "Extraction and discrimination pipeline produced 676 CoE items from HotpotQA candidates and 660 from 2Wiki; downstream ACC remains high even under imperfect extraction (accuracy drop from ~90.2% to ~89.3%-89.4% under simulated extraction errors).",
            "limitations_or_challenges": "Extraction depends on GPT-4o prompts and can produce missing or spurious evidence nodes; such extraction errors change CoE identification counts and slightly reduce downstream accuracy.",
            "comparison_to_human_or_traditional": "No human annotation/evaluation of extracted features reported; discrimination is automated and integrated into experimental pipeline rather than cross-checked by experts.",
            "recommendations_or_best_practices": "Use few-shot and carefully designed prompts for extraction; tolerate some extraction noise as downstream performance is robust, but validate when possible to avoid systematic mis-identification.",
            "uuid": "e9874.1",
            "source_info": {
                "paper_title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Eval-Metrics",
            "name_full": "Automated Evaluation Metrics (ACC and ASR with GPT-4 judge)",
            "brief_description": "The paper uses automated metrics: Accuracy (ACC) for correctness of answers and Attack Success Rate (ASR) for deception effectiveness; GPT-4 (or GPT-4o) is used as an automated judge to compute these metrics by comparing model outputs to ground-truth answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (used as automated judge); evaluated LLMs: GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B",
            "llm_description": "GPT-4 (and GPT-4o for some pipeline steps) served as the evaluator to compute ACC/ASR automatically instead of manual human scoring.",
            "scientific_domain": "NLP evaluation methodology",
            "evaluation_method": "Automated judging by GPT-4 comparing generated answers to ground-truth; scoped to question-answer matching and attack-success definitions (percentage of outputs conforming to poisoned target).",
            "evaluation_criteria": "ACC: proportion of correct answers compared to ground truth; ASR: proportion of model outputs that follow injected (poisoned) incorrect answers; statistical significance evaluated (p &lt; 0.05) for comparisons.",
            "benchmark_or_dataset": "HotpotQA and 2WikiMultihopQA (multi-hop); RGB (single-hop) used for select experiments; evaluations reported across varying noise/poison ratios (0 to 0.75).",
            "results_summary": "ACC and ASR trends illustrate CoE superiority: average ACC of CoE ~92.0% vs large drops for Non-CoE; average ASR for poisoned CoE ~85.4% vs ~64-69% for Non-CoE variants; metric sensitivity reported across extraneous and misinformation proportions.",
            "limitations_or_challenges": "Automated judging by LLMs may inherit biases and is not equivalent to human expert evaluation; reliance on automated judge may obscure nuanced correctness/partial-credit judgments.",
            "comparison_to_human_or_traditional": "No parallel human-judged evaluations reported; method uses LLM-based automatic evaluation following prior QA evaluation protocols.",
            "recommendations_or_best_practices": "When using automated LLM judges, report significance and examine robustness across noise/proportion sweeps; consider human checks for critical deployments.",
            "uuid": "e9874.2",
            "source_info": {
                "paper_title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Perturbations",
            "name_full": "Perturbation Strategies (SenP, WordP, extraneous & misinformation injection)",
            "brief_description": "Controlled perturbations applied to CoE to create Non-CoE variants: (SenP) sentence-level removal to simulate missing evidence; (WordP) word-level substitution replacing evidence nodes with higher-level expressions; plus injection of extraneous (lexically similar but irrelevant) text and contradictory misinformation at controlled proportions (0, 0.25, 0.5, 0.75).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Evaluated LLMs: GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B",
            "llm_description": "These perturbations were applied when querying the evaluated LLMs to measure sensitivity to missing details, higher-level node substitutions, irrelevant noise, and direct contradictions.",
            "scientific_domain": "NLP robustness testing / adversarial evaluation",
            "evaluation_method": "Systematic ablation and perturbation experiments where CoE contexts are modified and model ACC/ASR measured across proportion sweeps of extraneous and misinformation.",
            "evaluation_criteria": "Impact on ACC (significance and robustness), impact on ASR (deceptiveness), stability across noise levels; ablations removing intent, nodes, relations measure feature importance.",
            "benchmark_or_dataset": "Constructed CoE/Non-CoE instances from HotpotQA and 2WikiMultihopQA (1,336 total QA pairs) and single-hop RGB tests for cross-checks.",
            "results_summary": "SenP produces larger performance degradation than WordP; removing intent causes largest accuracy drop (33.9% in one ablation), evidence nodes removal reduces accuracy by ~13.6%, relations removal by ~10.7%. CoE remains stable with increasing extraneous proportion (ACC drop ~1.8% up to 75% extraneous), whereas Non-CoE variants drop far more.",
            "limitations_or_challenges": "Perturbations are synthetic and controlled—real-world corruptions may differ; WordP keeps more original semantics and thus can be less damaging than sentence removal.",
            "comparison_to_human_or_traditional": "This is an automated adversarial evaluation approach as opposed to human adversarial or domain-expert perturbations.",
            "recommendations_or_best_practices": "Use multi-granularity perturbations (sentence- and word-level) and proportion sweeps to assess robustness; prioritize intent coverage and evidence relations in retrieval/reranking to mitigate perturbation impact.",
            "uuid": "e9874.3",
            "source_info": {
                "paper_title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Benchmarks",
            "name_full": "Benchmark Datasets (HotpotQA, 2WikiMultihopQA, RGB)",
            "brief_description": "Datasets used as benchmarks for multi-hop and single-hop evaluation: HotpotQA and 2WikiMultihopQA provide multi-hop QA instances with supporting facts for chain construction; RGB used as a single-hop dataset for ablations and generalizability checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Evaluated LLMs: GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B",
            "llm_description": "Benchmarks supply test QA pairs and supporting evidence used to build CoE and Non-CoE contexts that are fed to the LLMs.",
            "scientific_domain": "NLP — QA benchmarks for reasoning evaluation",
            "evaluation_method": "Sampled 1,000 instances from each dataset, applied CoE discrimination to supporting evidence and constructed CoE/Non-CoE contexts; evaluated LLM outputs on ACC/ASR under noise/poisoning experiments.",
            "evaluation_criteria": "Per-question ACC and ASR, breakdowns by hop count (one-hop, two-hop, three-hop) and robustness to varying noise/poisoning proportions.",
            "benchmark_or_dataset": "HotpotQA: multi-hop QA with supporting facts; 2WikiMultihopQA: constructed multi-hop dataset for reasoning steps; RGB: single-hop dataset used for ablation.",
            "results_summary": "From sampled candidates, 676 CoE items were identified from HotpotQA (avg 4.0 sentences) and 660 from 2WikiMultihopQA (avg 3.4 sentences). CoE improvements hold across datasets and across hop counts, with single-hop showing highest stability (&gt;92% ACC).",
            "limitations_or_challenges": "Datasets have curated supporting facts which may bias towards CoE-like structures; real-world retrieval outputs may be noisier or differently distributed.",
            "comparison_to_human_or_traditional": "Benchmarks provide ground-truth answers for automated comparison; no human scoring reported.",
            "recommendations_or_best_practices": "Use multi-hop benchmarks with annotated supporting facts to construct and validate CoE; evaluate across hop counts to ensure generalizability.",
            "uuid": "e9874.4",
            "source_info": {
                "paper_title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "RAG+CoE",
            "name_full": "Coverage-based Reranking (RAG+CoE)",
            "brief_description": "A RAG variant that augments standard retrieve-rank-generate pipelines by reranking retrieved snippets based on coverage of CoE features (intent, evidence relations, evidence nodes), using a minimal coverage selection algorithm to prioritize intent first, then relations, then nodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applied with LLMs used as generators/evaluators including GPT-3.5, GPT-4, Llama2-33B/70B variants, Qwen2.5-32B",
            "llm_description": "Reranker enhancement is model-agnostic; the improved contexts feed into the same generation LLMs in experiments.",
            "scientific_domain": "NLP — retrieval-augmented generation and reranking",
            "evaluation_method": "Empirical comparison of vanilla RAG vs RAG+CoE on HotpotQA multi-hop QA, and integration into poisoning and defense pipelines (PoisonedRAG and InstructRAG variants).",
            "evaluation_criteria": "Multi-hop QA accuracy (ACC), poisoning attack success rate (ASR) for attack variants, defense ACC/ASR for defense variants.",
            "benchmark_or_dataset": "HotpotQA used for RAG evaluations; Google Search API top-5 snippets formed baseline contexts for RAG.",
            "results_summary": "RAG+CoE yields average ACC improvement of 10.4% over standard RAG (example: GPT-4 RAG ACC 72.9% → RAG+CoE 82.6%). In poisoning, PR+CoE increases ASR by ~11.0% over PR; in defenses, IR+CoE increases ACC by ~27.2% and reduces ASR by ~35.8% compared to IR.",
            "limitations_or_challenges": "Technique assumes availability of textual snippets and post-hoc textual discrimination—less applicable to purely vector-based retrieval systems; potential to prioritize CoE that is factually incorrect, increasing risk of deception.",
            "comparison_to_human_or_traditional": "Compared empirically to standard RAG pipelines and state-of-the-art poisoning/defense baselines; no human evaluation.",
            "recommendations_or_best_practices": "In reranking, prioritize intent coverage, then relations, then nodes; combine CoE-based reranking with verification mechanisms to avoid following incorrect CoE.",
            "uuid": "e9874.5",
            "source_info": {
                "paper_title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Empirical-Results",
            "name_full": "Empirical Evaluation Results & Ablations",
            "brief_description": "Aggregated quantitative outcomes and ablation findings showing CoE's effects on significance, deceptiveness and robustness: numeric ACC and ASR summaries, feature ablations (intent/nodes/relations), and behavior across hop numbers and noise proportions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-3.5, GPT-4, Llama2-13B, Llama3-70B, Qwen2.5-32B",
            "llm_description": "Results are aggregated across the five LLMs tested to demonstrate trends that generalize across architectures and scales.",
            "scientific_domain": "NLP empirical evaluation",
            "evaluation_method": "Controlled experiments with CoE vs Non-CoE across extraneous and misinformation proportions, plus ablations removing intent/nodes/relations, and comparisons by number of reasoning hops.",
            "evaluation_criteria": "ACC and ASR values, percentage differences between groups, statistical significance (p &lt; 0.05) for comparisons to CoE, stability metrics across proportion sweeps.",
            "benchmark_or_dataset": "HotpotQA, 2WikiMultihopQA, and single-hop RGB dataset for cross-checks.",
            "results_summary": "Key reported numbers: average CoE ACC 92.0% vs Non-CoE_SenP/WordP significantly lower; CoE ASR (when poisoned) average 85.4% vs Non-CoE lower by ~16–20%; ablation: removing intent caused largest ACC drop (~33.9%), evidence nodes (~13.6%), relations (~10.7%); CoE ACC stable across extraneous proportion (0→0.75) with ~1.8% drop.",
            "limitations_or_challenges": "Findings depend on constructed CoE and synthetic perturbations; CoE can be abused to create potent poisoning attacks; automated judge and dataset choices affect generalizability.",
            "comparison_to_human_or_traditional": "All quantitative comparisons are between automated-system variants and baselines rather than human baseline theory-evaluation methods.",
            "recommendations_or_best_practices": "Use feature-level ablations to identify critical components (intent most important), incorporate CoE-awareness in retrieval/reranking, and add safeguards against CoE-structured poisoning.",
            "uuid": "e9874.6",
            "source_info": {
                "paper_title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "rating": 2
        },
        {
            "paper_title": "Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps",
            "rating": 2
        },
        {
            "paper_title": "Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models",
            "rating": 2
        },
        {
            "paper_title": "InstructRAG: Instructing retrieval-augmented generation via self-synthesized rationales",
            "rating": 1
        },
        {
            "paper_title": "Evaluating correctness and faithfulness of instruction-following models for question answering",
            "rating": 1
        }
    ],
    "cost": 0.01609025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context for Multi-Hop QA</h1>
<p>Zhiyuan Chang ${ }^{1,2,3}$ Mingyang Li ${ }^{1,2,3}$ Xiaojun Jia ${ }^{4}$ Junjie Wang ${ }^{1,2,3}$<br>Yuekai Huang ${ }^{1,2,3}$ Qing Wang ${ }^{1,2,3}$ Yihao Huang ${ }^{4}$ Yang Liu ${ }^{4}$<br>${ }^{1}$ State Key Laboratory of Intelligent Game, Beijing, China<br>${ }^{2}$ Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China<br>${ }^{3}$ University of Chinese Academy of Sciences ${ }^{4}$ Nanyang Technological University</p>
<h4>Abstract</h4>
<p>Incorporating external knowledge has emerged as a promising way to mitigate outdated knowledge and hallucinations in LLM. However, external knowledge is often imperfect, encompassing substantial extraneous or even inaccurate content, which interferes with the LLM's utilization of useful knowledge in the context. This paper seeks to characterize the features of preferred external knowledge and perform empirical studies in imperfect contexts. Inspired by the chain of evidence (CoE), we characterize that the knowledge preferred by LLMs should maintain both relevance to the question and mutual support among the textual pieces. Accordingly, we propose a CoE discrimination approach and conduct a comparative analysis between CoE and Non-CoE samples across significance, deceptiveness, and robustness, revealing the LLM's preference for external knowledge that aligns with CoE features. Furthermore, we selected three representative tasks (RAG-based multihop QA, external knowledge poisoning and poisoning defense), along with corresponding SOTA or prevalent baselines. By integrating CoE features, the variants achieved significant improvements over the original baselines.</p>
<h2>1 Introduction</h2>
<p>The parameterized knowledge acquired by large language models (LLMs) through pre-training at a specific point in time becomes outdated with the knowledge evolution or produces hallucination (Achiam et al., 2023; Touvron et al., 2023a; Anil et al., 2023). Incorporating external knowledge into LLM has emerged as an effective approach to mitigate this problem (Tu et al., 2024; Zhao et al., 2024). In this context, properties such as the accuracy and reliability of external knowledge are critical for LLMs to provide accurate answers.</p>
<p>However, external knowledge is often imperfect. In addition to the useful knowledge that users expect LLMs to follow, the context typically contains two
types of noise (Chen et al., 2024; Zou et al., 2024): 1) extraneous information, despite showing textual similarities with the question, cannot support the correct answer (Chen et al., 2024; Xiang et al., 2024); 2) inaccurate information, which can mislead LLM to produce incorrect answers (Liu et al., 2024). Especially when dealing with complex scenarios such as multi-hop QA, the acquisition of such noise is inevitable due to limitations of retrievers or quality deficiencies in the specialized knowledge bases (Wang et al., 2024; Dai et al., 2024; Tang and Yang, 2024). This hinders LLMs from effectively using useful knowledge within external contexts and leads to incorrect answers.</p>
<p>Consequently, numerous studies aim to characterize the features of external knowledge that LLMs tend to follow in imperfect contexts (such as confirmation bias, completeness bias, coherent bias, etc.) (Xie et al., 2023; Zhang et al., 2024); or on approaches such as reranking or retrieval to prioritize knowledge with high relevance (Asai et al., 2023; Dong et al., 2024). However, previous studies primarily suffer from two main deficiencies. First, while their focus is on qualitative findings, it remains uncertain whether such findings can effectively guide performance improvements in representative tasks (Zhang et al., 2024). Second, their research focuses on single-hop QA, in which a single piece of knowledge suffices to answer the question. However, the generalizability of these findings to more complex scenarios (e.g., multi-hop QA) has yet to be confirmed.</p>
<p>In our study, we focus on characterizing what external knowledge is more capable of resisting the surrounding noise and guiding LLMs for better generation. Inspired by the Chain of Evidence (CoE) theory in criminal procedural law (Murphy, 2013), which requires case-decisive evidence to demonstrate both relevance (pertaining to the case) and interconnectivity (evidence mutually supporting each other) in judicial decisions. In multi-hop</p>
<p>QA, analogously to the scenario where LLMs rely on external knowledge for answering, we consider that the preferred knowledge should show relevance to the question (relevance) and mutual support and complementarity among textual pieces in addressing the question (interconnectivity). Based on the principle, we first characterize what knowledge can be considered CoE and propose a discrimination approach to determine whether the given external knowledge aligns to the CoE features. Subsequently, we conduct a comparative analysis of CoE versus Non-CoE samples, examining LLMs' preference for CoE-aligned content across four dimensions below.</p>
<ul>
<li>Significance, we examine whether LLMs demonstrate superior performance when provided with external knowledge exhibiting CoE characteristics, versus cases where the knowledge is relevant but lacks COE characteristics.</li>
<li>Deceptiveness, we investigate whether samples that conform to COE characteristics but lead to incorrect answers exhibit higher deceptive potential, effectively inducing LLMs to generate incorrect output.</li>
<li>Robustness, we investigate whether the approach effectively mitigates knowledge conflicts and enhances question-answering performance in multi-hop scenarios.</li>
<li>Usability, we select three representative tasks (RAG-based multi-hop QA, external knowledge poisoning and defenses) to explore whether CoE can be effectively integrated and enhance the effectiveness of baselines.</li>
</ul>
<p>Using HotpotQA (Yang et al., 2018) and 2WikiMultihopQA (Ho et al., 2020) as data sources, we constructed 1,336 multi-hop QA pairs and the corresponding CoE based on the automatic discrimination. By applying perturbations to CoE, we also build Non-CoE samples (that is, knowledge lacking the necessary relevance or interconnectivity to establish CoE) for each QA pair. Subsequently, we conducted a comprehensive evaluation in five state-of-the-art LLMs (GPT-3.5 (OpenAI, 2022), GPT-4 (Achiam et al., 2023), LLama2-13B (Touvron et al., 2023b), LLama3-70B (Touvron et al., 2023a), and Qwen2.5-32B (Qwen Team, 2024).</p>
<p>The empirical analysis implies that if external knowledge in the context exhibits CoE characterization, it can better resist interference from extraneous
and even inaccurate information and improve multihop QA performance. Building upon these findings, we can effectively enhance existing multi-hop QA approaches and poisoning defenses through performance improvements. Nevertheless, the observed preference for CoE-compliant external knowledge creates a vulnerability. Adversaries can deliberately construct false information with CoE characteristics to successfully trick LLMs into generating answers containing factual errors. Empirically, our investigation uncovers characteristic preferences of LLMs toward external knowledge from both relevance and interconnectivity perspectives, which informs the optimization of knowledge representation and retrieval mechanisms in RAG systems. Practically, our studies demonstrate significant improvements over the SOTA or prevalent baselines in three representative tasks. The reproduction package is available at: https://anonymous.4open. science/r/ScopeCOE-78D3.</p>
<h2>2 Related Work</h2>
<p>In imperfect knowledge augmentation, there is growing interest in understanding LLMs' knowledge preferences, especially in contexts involving conflicts between external and internal knowledge, as well as contradictions within internal knowledge (Xie et al., 2023; Kasai et al., 2023; Tan et al., 2024; Jin et al., 2024; Xu et al., 2024b,a).</p>
<p>Xie et al. (2023) demonstrated LLMs' bias towards coherent knowledge, revealing that LLMs are highly receptive to external knowledge when presented coherently, even when it conflicts with their parametric knowledge. Jin et al. (2024) found that LLMs demonstrate confirmation bias, manifested as their inclination to choose knowledge consistent with their internal memory, regardless of whether it is correct or incorrect. Chen et al. (2022) demonstrated LLMs' preference for highly relevant knowledge by manipulating retrieved snippets based on attention scores, showing that LLMs prioritize knowledge with greater relevance to questions. Zhang et al. (2024) found LLMs perform better when given complete external knowledge, showing completeness bias.</p>
<p>Although existing studies have documented LLMs' knowledge preferences, there exists a significant gap in understanding and measuring the essential features that govern these preferences, especially in complex scenarios like multi-hop QA.</p>
<p>To this end, we manage to characterize and discriminate external knowledge that can help LLMs generate correct responses.</p>
<h2>3 CoE Characterization and Discrimination</h2>
<h3>3.1 CoE Characterization</h3>
<p>Drawing from the law of criminal procedure, judicial decisions in cases require the formation of a CoE through evidence collection (Edmond and Roach, 2011; Murphy, 2013). Such a CoE must demonstrate two properties: relevance (pertaining to the case) and interconnectivity (evidence mutually supporting each other). In multi-hop QA, the user question is analogous to a legal case, where external contexts constitute the evidentiary collection, and the LLM's answer represents the judicial conclusion drawn through iterative reasoning processes. Based on this analogy, we hypothesize that in the reasoning process from user query to final answer, LLMs tend to prioritize external knowledge that demonstrates both relevance and interconnectivity.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of CoE and the CoE features.
Next, we will characterize relevance and interconnectivity from the following three perspectives of textual features presented by external knowledge.</p>
<ul>
<li>Intent is a noun or noun phrase representing the user's desired answer to their question, and it aims to align the purpose of the user's question with the ultimate facts derived from external knowledge.</li>
<li>Evidence Nodes are the key entities in a user's question, which imply critical knowledge elements for multi-hop reasoning. It ensures logical consistency between the starting and ending points of a single reasoning hop, aligning the user's query with external knowledge.</li>
<li>Evidence Relations are logical predicates within the question, indicating the semantic associations between each pair of evidence
nodes. It is used to verify whether the implicit semantic connections between entities in external knowledge are consistent with the inherent logic in the question.</li>
</ul>
<p>Taking Figure 1 as an example, intent specifies "state location of business" as the goal, indicating that the user wants to find the state where the business operates. The evidence nodes, "drug stores", "CEO", and "Warren Bryant", serve as essential nodes for multi-hop reasoning. Evidence relations show how these entities are linked, with "have" connecting "drug stores" to "CEO", and "is" linking "CEO" to "Warren Bryant". The effectiveness of CoE stems from the synergistic interaction of these three features. The integration of all three features creates a comprehensive logic chain tailored to the specific question.</p>
<h3>3.2 CoE Discrimination</h3>
<p>Based on the characterized features, we design an approach to discriminate whether external knowledge exhibits CoE features. Generally, discrimination extracts three CoE features from the user query, i.e., intent, evidence node, and evidence relation. These features represent the underlying logic embedded within the user question and serve as objectives for external knowledge alignment. Subsequently, for a given piece of external knowledge, we verify whether it simultaneously satisfies all three features. The following introduces the details for the implementation of CoE discrimination.</p>
<p>First, for a user question, we extract its intent, the evidence nodes and the evidence relations using GPT-4o with a hand-crafted prompt. Second, with the extracted CoE features, we discriminate whether external knowledge exhibits them using GPT-4o. As for intent discrimination, we frame it as a textual entailment task, where external knowledge as a premise and intent as a hypothesis. We reason whether the hypothesis holds on the basis of the given premise using GPT-4o with prompting. To discriminate the nodes and relations, we uniformly treat this as a classification task about the "containment" logic. In implementation, we manually construct distinct prompts for each type of feature, instructing GPT-4o to classify whether an external knowledge contains the extracted nodes and relations. Finally, external knowledge is considered aligned to the user question only when all required CoE features are present. The discrimination prompts are detailed in Appendix I.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The overview of CoE discrimination.</p>
<h2>4 Subject Dataset</h2>
<h3>4.1 CoE Knowledge Construction</h3>
<p>Oriented to multi-hop QA, we selected two commonly used datasets, HotpotQA and 2WikiMulti-hopQA as sources. Both datasets contain samples comprising not just QA pairs, but also the supporting knowledge required to derive each answer. During construction, the supporting knowledge was specifically designed to capture the necessary logical chains for multi-hop reasoning. We consider it to be highly compatible with the COE features, thus qualifying it as a candidate COE knowledge.</p>
<p>Referring to the sample size in previous studies (jin2024coefficient, chen2024coefficient), we randomly sampled 1,000 instances from each dataset and applied the CoE discrimination approach to check whether each candidate exhibits CoE features. Finally, we obtained 676 and 660 knowledge pieces that contain CoE from candidates, with an average of 4.0 and 3.4 sentences within the supporting knowledge for two datasets, respectively.</p>
<h3>4.2 Non-CoE Knowledge Construction</h3>
<p>Based on the CoE Knowledge, we construct Non-CoE knowledge using two controlled perturbation strategies that are commonly employed in empirical studies of LLM knowledge construction (xie2023sense): sentence-level perturbation (SenP) and word-level perturbation (WordP). SenP simulates incomplete knowledge by removing key evidence pieces, while WordP replaces specific evidence nodes with their higher-level expressions. These strategies ensure fair comparison by maintaining the same question context while only varying the CoE completeness. Detailed perturbation procedures and examples are provided in Appendix E. Subsequently, we construct complete LLM contexts by augmenting COE and Non-COE knowledge with extraneous and inaccurate information. Through comparative analysis, we systematically examine their performance differences across various scenarios.</p>
<h2>5 Significance Assessment</h2>
<p>In practice, the context of LLMs often contains non-trivial noise due to limitations in knowledge base quality or retriever performance (liu2024sense). This results in situations where even when useful knowledge is retrieved, other noise may interfere with it, ultimately preventing LLMs from generating desired content. In this section, we aim to investigate whether knowledge that aligns with CoE features can more effectively resist noise in the context and help LLMs answer multi-hop questions. To this end, we employ a comparative analysis methodology. We retrieve text fragments that are lexically similar but offer limited relevance to QA, simulating potential extraneous information interference in contexts. These fragments are then combined with either CoE or Non-CoE knowledge to form complete contexts, followed by an analysis of whether significant performance differences exist between the two groups.</p>
<h3>5.1 Experimental Design</h3>
<p>We design a comprehensive experimental framework to evaluate the effectiveness of CoE and Non-CoE (Non-CoE<sub>SenP</sub> and Non-CoE<sub>WordP</sub>) under different noise conditions. Our analysis focuses on four key dimensions: (1) the performance comparison between CoE and Non-CoE on LLMs, (2) the impact of varying extraneous ratios on their effectiveness, and (3) the performance of CoE in single-hop and multi-hop scenarios. (4) the influence of different CoE features on LLM performance. We inject extraneous information at four different ratios (from 0 to 0.75, with 0.25 intervals) to examine how each approach maintains its effectiveness.</p>
<p>For evaluation, we select five representative LLMs that span both closed-source (GPT-3.5, GPT-4) and open-source LLMs (LLama2-13B, LLama3-70B, Qwen2.5-32B) to ensure comprehensive coverage across different model scales and architectures. Following general QA evaluation protocols Adlakha2024, we use GPT-4 as the judge to compute the accuracy (ACC) between model outputs and ground truth answers. To understand CoE's significance more comprehensively, we conduct experiments on a single-hop dataset and perform</p>
<p>Table 1: LLMs’ Accuracy (ACC) on CoE and Non-CoE.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Irrelevant Proportion</th>
<th>HotpotQA</th>
<th></th>
<th></th>
<th>2WikiMultihopQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>CoE</td>
<td>Non-CoE</td>
<td></td>
<td>CoE</td>
<td>Non-CoE</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>SenP</td>
<td>WordP</td>
<td></td>
<td>SenP</td>
<td>WordP</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0</td>
<td>91.9\%</td>
<td>77.9\%*</td>
<td>79.1\%*</td>
<td>97.4\%</td>
<td>74.1\%*</td>
<td>83.5\%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>90.3\%</td>
<td>75.6\%*</td>
<td>77.5\%*</td>
<td>96.9\%</td>
<td>68.2\%*</td>
<td>81.2\%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>89.9\%</td>
<td>73.1\%*</td>
<td>75.4\%*</td>
<td>96.5\%</td>
<td>66.4\%*</td>
<td>82.6\%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>88.9\%</td>
<td>65.7\%*</td>
<td>74.5\%*</td>
<td>95.4\%</td>
<td>58.4\%*</td>
<td>70.8\%*</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0</td>
<td>93.5\%</td>
<td>83.4\%*</td>
<td>86.4\%*</td>
<td>93.7\%</td>
<td>67.7\%*</td>
<td>79.4\%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>93.4\%</td>
<td>82.3\%*</td>
<td>86.4\%*</td>
<td>94.0\%</td>
<td>70.9\%*</td>
<td>80.1\%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>91.8\%</td>
<td>82.0\%*</td>
<td>86.5\%*</td>
<td>95.4\%</td>
<td>71.5\%*</td>
<td>77.3\%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>91.2\%</td>
<td>80.1\%*</td>
<td>83.8\%*</td>
<td>95.9\%</td>
<td>64.9\%*</td>
<td>74.4\%*</td>
</tr>
<tr>
<td>Llama2-13B</td>
<td>0</td>
<td>89.9\%</td>
<td>87.1\%*</td>
<td>88.8\%*</td>
<td>96.5\%</td>
<td>95.3\%*</td>
<td>93.3\%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>87.9\%</td>
<td>84.2\%*</td>
<td>85.2\%*</td>
<td>95.9\%</td>
<td>93.7\%*</td>
<td>91.9\%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>86.4\%</td>
<td>82.8\%*</td>
<td>84.0\%*</td>
<td>93.8\%</td>
<td>91.2\%*</td>
<td>90.0\%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>85.8\%</td>
<td>79.5\%*</td>
<td>82.9\%*</td>
<td>90.9\%</td>
<td>86.8\%*</td>
<td>86.3\%*</td>
</tr>
<tr>
<td>Llama3-70B</td>
<td>0</td>
<td>92.5\%</td>
<td>76.8\%*</td>
<td>74.5\%*</td>
<td>95.7\%</td>
<td>79.5\%*</td>
<td>73.3\%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>92.9\%</td>
<td>74.1\%*</td>
<td>76.1\%*</td>
<td>93.7\%</td>
<td>80.3\%*</td>
<td>71.4\%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>91.1\%</td>
<td>72.6\%*</td>
<td>76.8\%*</td>
<td>95.9\%</td>
<td>76.7\%*</td>
<td>69.6\%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>90.5\%</td>
<td>69.8\%*</td>
<td>68.3\%*</td>
<td>93.1\%</td>
<td>72.3\%*</td>
<td>67.3\%*</td>
</tr>
<tr>
<td>Qwen2.5-32B</td>
<td>0</td>
<td>87.8\%</td>
<td>71.3\%*</td>
<td>75.7\%*</td>
<td>90.7\%</td>
<td>53.1\%*</td>
<td>67.0\%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>87.2\%</td>
<td>38.6\%*</td>
<td>64.9\%*</td>
<td>91.3\%</td>
<td>29.5\%*</td>
<td>49.4\%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>86.1\%</td>
<td>37.7\%*</td>
<td>64.3\%*</td>
<td>92.1\%</td>
<td>27.8\%*</td>
<td>47.5\%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>88.0\%</td>
<td>35.3\%*</td>
<td>57.2\%*</td>
<td>91.9\%</td>
<td>22.2\%*</td>
<td>45.9\%*</td>
</tr>
<tr>
<td>* indicates statistical significance compared to CoE (p &lt; 0.05)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>ablation studies by removing different CoE features. Detailed experimental settings are provided in Appendix B and C.</p>
<h3>5.2 Results and Discussion</h3>
<p>Table 1 shows the ACC in different ratios of extraneous information. Comparing the CoE and NonCoE groups, the results show that CoE achieves an average ACC of $92.0 \%$ across five LLMs and two datasets, outperforming Non-CoE $<em _WordP="{WordP" _text="\text">{\text {SenP }}$ and Non-CoE $</em>$ by $22.5 \%$ and $16.3 \%$, respectively. This substantial improvement suggests that external knowledge that exhibiting CoE features enables LLMs to utilize it and achieve better performance.}</p>
<p>For the proportion of extraneous information in the context, as the ratio increases from $0 \%$ to $75 \%$, CoE's ACC only decreases by $1.8 \%$, while the ACC of Non-CoE variants decreases more significantly: $12.9 \%$ for Non-CoE $<em _WordP="{WordP" _text="\text">{\text {SenP }}$ and $9.0 \%$ for Non-CoE $</em>$. This robustness suggests that external knowledge that exhibits CoE features helps LLMs maintain consistent comprehension and reasoning faced with noise of different magnitudes.}</p>
<p>Analyzing the impact of reasoning complexity, experiments show that single-hop reasoning maintains the most stable performance ( $&gt;92.0 \% \mathrm{ACC}$ ) under increasing extraneous information, followed by three-hop reasoning ( $&gt;90.0 \% \mathrm{ACC}$ ), while twohop reasoning exhibits higher sensitivity, with ACC dropping from $91.0 \%$ to $88.0 \%$. This pattern suggests that CoE is particularly effective in simpler
reasoning scenarios, while maintaining strong performance in more complex cases. Detailed experimental results and analysis are provided in Appendix C.</p>
<p>In addition, ablation studies further demonstrate the varying impacts of CoE features: removing intent causes the largest accuracy drop ( $33.9 \%$ ), followed by evidence nodes ( $13.6 \%$ ), while evidence relation removal has the smallest impact ( $10.7 \%$ ). It indicates that explicit reasoning intent is crucial for guiding LLMs' responses. Detailed experimental results and analysis are provided in Appendix B.</p>
<p>Summary: If external knowledge exhibits CoE characterization, it can better resist interference from extraneous information and improve multihop QA performance. Moreover, LLMs exhibit greater resistance if there exists external knowledge exhibiting CoE features, as the proportion of extraneous information increases. For practical guidance, optimizing the retriever to prioritize knowledge exhibiting CoE features can effectively enhance performance of multi-hop QA.</p>
<h2>6 Deceptiveness Assessment</h2>
<p>Given that CoE represents structured reasoning chains, it is crucial to examine whether such wellformed evidence paths could amplify the deceptive effect of poisoned knowledge. Therefore, we investigate a more challenging scenario, where the CoE contains factual errors, to determine whether LLMs can still be effectively misled and produce answers consistent with the incorrect information embedded in the CoE.</p>
<h3>6.1 Experimental Design</h3>
<p>To investigate the deceptiveness of incorrect external knowledge under imperfect conditions, we design a comprehensive evaluation framework comparing CoE and Non-CoE. Our analysis focuses on three key dimensions: (1) the comparative effectiveness between CoE and Non-CoE (Non-CoE $<em _WordP="{WordP" _text="\text">{\text {SenP }}$ and Non-CoE $</em>$ ) in misleading LLMs with incorrect information, and (2) how their deceptive capabilities change under varying ratios of irrelevant information, and (3) the influence of different CoE features on LLM deception effectiveness.}</p>
<p>In constructing incorrect information for both CoE and Non-CoE, we carefully preserve the semantic type and format of original answers (e.g., replacing "United States" with "Canada" while maintaining consistent structures) to ensure fair</p>
<p>Table 2: Attack Success Rate (ASR) of CoE and Non-CoE against LLMs.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Irrelevant Proportion</th>
<th>HotpotQA</th>
<th></th>
<th></th>
<th>2WikiMultihopQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>CoE</td>
<td>Non-CoE</td>
<td></td>
<td>CoE</td>
<td>Non-CoE</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>SenP</td>
<td>WordP</td>
<td></td>
<td>SenP</td>
<td>WordP</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0</td>
<td>86.1%</td>
<td>75.6%*</td>
<td>83.1%*</td>
<td>85.0%</td>
<td>58.5%*</td>
<td>57.4%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>85.8%</td>
<td>76.0%*</td>
<td>79.1%*</td>
<td>86.5%</td>
<td>53.8%*</td>
<td>52.4%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>84.7%</td>
<td>72.2%*</td>
<td>77.8%*</td>
<td>84.2%</td>
<td>50.0%*</td>
<td>48.8%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>78.4%</td>
<td>72.0%*</td>
<td>73.7%*</td>
<td>83.3%</td>
<td>45.2%*</td>
<td>44.9%*</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0</td>
<td>86.5%</td>
<td>52.2%*</td>
<td>59.0%*</td>
<td>85.4%</td>
<td>68.8%*</td>
<td>76.2%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>85.5%</td>
<td>50.5%*</td>
<td>58.9%*</td>
<td>87.2%</td>
<td>67.0%*</td>
<td>73.2%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>84.0%</td>
<td>46.8%*</td>
<td>52.7%*</td>
<td>90.6%</td>
<td>65.2%*</td>
<td>76.8%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>78.2%</td>
<td>43.2%*</td>
<td>50.5%*</td>
<td>92.7%</td>
<td>62.3%*</td>
<td>75.1%*</td>
</tr>
<tr>
<td>Llama2-13B</td>
<td>0</td>
<td>78.2%</td>
<td>76.9%*</td>
<td>72.9%*</td>
<td>91.5%</td>
<td>89.8%*</td>
<td>88.6%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>77.1%</td>
<td>74.1%*</td>
<td>67.3%*</td>
<td>89.8%</td>
<td>87.5%*</td>
<td>86.3%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>71.6%</td>
<td>70.0%*</td>
<td>67.5%*</td>
<td>89.1%</td>
<td>86.8%*</td>
<td>85.1%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>69.1%</td>
<td>64.5%*</td>
<td>64.8%*</td>
<td>84.1%</td>
<td>81.6%*</td>
<td>82.1%*</td>
</tr>
<tr>
<td>Llama3-70B</td>
<td>0</td>
<td>82.8%</td>
<td>76.9%*</td>
<td>72.8%*</td>
<td>89.7%</td>
<td>77.1%*</td>
<td>72.1%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>81.6%</td>
<td>75.1%*</td>
<td>71.9%*</td>
<td>89.5%</td>
<td>72.1%*</td>
<td>70.4%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>78.0%</td>
<td>71.7%*</td>
<td>68.0%*</td>
<td>88.9%</td>
<td>69.4%*</td>
<td>66.5%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>78.2%</td>
<td>62.9%*</td>
<td>64.1%*</td>
<td>89.8%</td>
<td>51.4%*</td>
<td>53.7%*</td>
</tr>
<tr>
<td>Qwen2.5-32B</td>
<td>0</td>
<td>90.6%</td>
<td>68.9%*</td>
<td>79.1%*</td>
<td>93.7%</td>
<td>43.5%*</td>
<td>65.8%*</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>87.7%</td>
<td>67.3%*</td>
<td>80.0%*</td>
<td>93.6%</td>
<td>47.2%*</td>
<td>67.3%*</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>86.3%</td>
<td>64.1%*</td>
<td>78.5%*</td>
<td>93.1%</td>
<td>47.0%*</td>
<td>68.6%*</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>85.8%</td>
<td>62.9%*</td>
<td>74.2%*</td>
<td>94.0%</td>
<td>46.5%*</td>
<td>65.6%*</td>
</tr>
</tbody>
</table>
<p>comparison. Following Section 5.1, we inject irrelevant information at the same ratios to examine how each approach maintains its deceptive effectiveness.</p>
<p>Using the same subject LLMs as Section 5.1 and following standard evaluation protocols <em>Adlakha et al. (2024)</em>, we use GPT-4o as the judge to compute the Attack Success Rate (ASR), defined as the proportion of successfully misled LLM outputs. We also conduct ablation studies to analyze how different CoE features midleading LLM responses. Detailed experimental settings are provided in B.</p>
<h3>6.2 Results and Discussion</h3>
<p>Table 3 shows the ASR of LLMs with external knowledge under CoE and two types of Non-CoE samples leading to incorrect answers. The results show that the average ASR reaches 85.4% for the COE group, which is 20.6% and 16.2% higher than Non-CoE_{SenP} and Non-CoE_{WordP}, respectively. The results imply that CoE demonstrates significant deception in misleading LLMs when it contains factual errors.</p>
<p>Combining with the ratio of the irrelevant information, as the ratio increases from 0% to 75%, CoE’s ASR only decreases by 3.6%, while the attack effectiveness of Non-CoE variants drops more significantly (9.7% for Non-CoE_{SenP} and 7.9% for Non-CoE_{WordP}). In general, CoE’s attack effectiveness remains more stable against LLMs when faced with irrelevant noise variations, outperforming two types of Non-CoE samples. Another noteworthy finding is that when the CoE leads to an incorrect answer, its performance metrics are on average 6.6% lower than those of a correct CoE (Table 1). This phenomenon probably stems from the parametric knowledge inherent in LLMs, which confers a degree of resistance to poisoned or erroneous knowledge input.</p>
<p>Ablation studies reveal the varying impacts of CoE components on LLM deception: removing evidence nodes leads to the highest ASR (78.4%), followed by removing evidence relations (64.4%) and intent (53.6%). However, the absence of evidence nodes results in reduced stability against irrelevant knowledge, with ASR dropping by 9.4%, indicating their vital role in maintaining deceptive effectiveness under noisy scenarios. Detailed analysis is provided in Appendix B.</p>
<p>Summary: External knowledge exhibiting CoE characteristics demonstrates significant deceptiveness in misleading LLMs when it contains factual errors. This implies that external knowledge matching CoE features requires elevated prioritized safeguards due to their potent deceptiveness.</p>
<h2>7 Robustness Assessment</h2>
<p>In addition to examining CoE’s performance when confronted with irrelevant information, we further investigate its resilience against knowledge conflicts in the context, e.g., cases where adversarial attacks have compromised other retrieved contexts.</p>
<h3>7.1 Experimental Design</h3>
<p>Starting from the CoE and Non-CoE samples, we continuously inject conflicting knowledge into their context at varying ratios to simulate scenarios where incorrect knowledge is retrieved or context is poisoned. We then observe and compare the performance of both sample groups in multi-hop QA to analyze whether CoE samples exhibit stronger robustness to misinformation interference.</p>
<p>First, we construct conflicting knowledge through two strategies: (1) replacing correct statements with contradictory ones in CoE/Non-CoE sentences, and (2) using GPT-4o to generate diverse contradictory expressions following previous work <em>Chen et al. (2023); Jin et al. (2024)</em>. We then inject these contradictory statements and gradually increase their proportion (from 0 to 0.75, with 0.25 intervals) in the contexts. After that, we obtain the answers from LLMs with the question and the constructed CoE and Non-CoE contexts. Following standard evaluation protocols <em>Adlakha et al. (2024)</em></p>
<p>Table 3: LLMs’ Accuracy (ACC) with CoE and Non-CoE surrounded by misinformation.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Misinformation Proportion</th>
<th>HotpotQA</th>
<th></th>
<th></th>
<th>2WikiMultihopQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>CoE</td>
<td>Non-CoE</td>
<td></td>
<td>CoE</td>
<td>Non-CoE</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>SenP</td>
<td>WordP</td>
<td></td>
<td>SenP</td>
<td>WordP</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0</td>
<td>91.9\%</td>
<td>77.9\%</td>
<td>79.1\%</td>
<td>97.4\%</td>
<td>74.1\%</td>
<td>83.5\%</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>81.8\%</td>
<td>62.5\%</td>
<td>64.0\%</td>
<td>85.3\%</td>
<td>40.6\%</td>
<td>63.8\%</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>82.0\%</td>
<td>63.0\%</td>
<td>65.7\%</td>
<td>65.5\%</td>
<td>43.4\%</td>
<td>52.3\%</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>75.7\%</td>
<td>58.9\%</td>
<td>60.8\%</td>
<td>55.5\%</td>
<td>29.8\%</td>
<td>30.4\%</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0</td>
<td>93.5\%</td>
<td>83.4\%</td>
<td>86.4\%</td>
<td>93.7\%</td>
<td>67.7\%</td>
<td>79.4\%</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>95.3\%</td>
<td>89.7\%</td>
<td>89.9\%</td>
<td>96.5\%</td>
<td>86.0\%</td>
<td>91.9\%</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>90.7\%</td>
<td>84.6\%</td>
<td>87.4\%</td>
<td>90.7\%</td>
<td>78.3\%</td>
<td>84.2\%</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>86.6\%</td>
<td>75.2\%</td>
<td>78.1\%</td>
<td>85.0\%</td>
<td>60.7\%</td>
<td>69.4\%</td>
</tr>
<tr>
<td>Llama2-13B</td>
<td>0</td>
<td>89.9\%</td>
<td>87.1\%</td>
<td>88.8\%</td>
<td>96.5\%</td>
<td>95.3\%</td>
<td>93.3\%</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>74.8\%</td>
<td>70.6\%</td>
<td>67.6\%</td>
<td>78.5\%</td>
<td>73.9\%</td>
<td>67.7\%</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>63.5\%</td>
<td>59.2\%</td>
<td>56.5\%</td>
<td>57.9\%</td>
<td>52.0\%</td>
<td>52.7\%</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>57.0\%</td>
<td>42.1\%</td>
<td>44.9\%</td>
<td>49.7\%</td>
<td>34.9\%</td>
<td>41.8\%</td>
</tr>
<tr>
<td>Llama3-70B</td>
<td>0</td>
<td>92.5\%</td>
<td>76.8\%</td>
<td>74.5\%</td>
<td>95.7\%</td>
<td>79.5\%</td>
<td>73.3\%</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>87.4\%</td>
<td>71.3\%</td>
<td>67.3\%</td>
<td>93.1\%</td>
<td>72.6\%</td>
<td>61.2\%</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>82.1\%</td>
<td>64.8\%</td>
<td>62.5\%</td>
<td>88.3\%</td>
<td>64.1\%</td>
<td>55.8\%</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>84.0\%</td>
<td>58.9\%</td>
<td>57.6\%</td>
<td>85.6\%</td>
<td>56.5\%</td>
<td>52.4\%</td>
</tr>
<tr>
<td>Qwen2.5-32B</td>
<td>0</td>
<td>87.8\%</td>
<td>71.3\%</td>
<td>75.7\%</td>
<td>90.7\%</td>
<td>53.1\%</td>
<td>67.0\%</td>
</tr>
<tr>
<td></td>
<td>0.25</td>
<td>95.1\%</td>
<td>79.5\%</td>
<td>83.4\%</td>
<td>97.4\%</td>
<td>63.5\%</td>
<td>75.4\%</td>
</tr>
<tr>
<td></td>
<td>0.5</td>
<td>88.5\%</td>
<td>72.3\%</td>
<td>71.7\%</td>
<td>92.1\%</td>
<td>40.6\%</td>
<td>64.5\%</td>
</tr>
<tr>
<td></td>
<td>0.75</td>
<td>83.0\%</td>
<td>66.0\%</td>
<td>67.3\%</td>
<td>86.9\%</td>
<td>39.6\%</td>
<td>55.0\%</td>
</tr>
</tbody>
</table>
<p>for multi-hop QA, we use GPT-4o as the evaluator and compute ACC. We also conduct ablation studies to analyze how different CoE features affect conflict handling capabilities. Detailed experimental settings are provided in Appendix B.</p>
<h3>7.2 Results and Discussion</h3>
<p>Table 3 shows ACC after adding inaccurate information and produce knowledge conflicts with CoE and two types of Non-CoE. The results show that the average ACC reaches $84.1 \%$ for the CoE group, which is $21.4 \%$ and $15.3 \%$ higher than Non-CoE $<em _WordP="{WordP" _text="\text">{\text {SenP }}$ and Non-CoE $</em>$, respectively. These results demonstrate CoE's superior ability in maintaining correct output when faced with conflicting information. Furthermore, as the ratio increases from $0 \%$ to $75 \%$, CoE's ACC decreases by $18.0 \%$, while Non-CoE variants show greater drops ( $24.2 \%$ for Non-CoE $}<em _WordP="{WordP" _text="\text">{\text {SenP }}$ and $24.3 \%$ for Non-CoE $</em>$ ). This indicates CoE's more stable performance against conflicting knowledge. Considering certain context poisoning methods (such as PoisonRAG (Zou et al., 2024)) which involve injecting multiple pieces of incorrect knowledge into the context, resulting in a high conflict ratio, CoE can also better help LLMs resist such attacks.}</p>
<p>Ablation studies demonstrate the crucial role of evidence relations in handling conflicting knowledge, with their removal leading to a substantial $62.1 \%$ ACC drop when contradictory information is present. By connecting nodes and maintaining logical consistency, evidence relations make LLMs more resilient to contradictions. Detailed analysis is provided in Appendix B.</p>
<p>Summary: When inaccurate information exists in the context, CoE can help LLMs effectively maintain the robustness against such interference. This suggests that existing RAG defense methods could benefit from incorporating such structured evidence chains to enhance their robustness against misleading information.</p>
<h2>8 Usability Assessment</h2>
<p>Based on the above findings, we selected three representative tasks that leverages external knowledge, i.e., RAG-based multi-hop QA, external knowledge poisoning, and poisoning defenses (Zhou et al., 2024). For each task, we chodse the corresponding SOTA or prevalent baseline and modified certain components under the guidance of CoE-oriented findings to explore whether CoE-enhanced variants could achieve performance improvements.</p>
<h3>8.1 RAG-based Multi-Hop QA</h3>
<p>Given the complexity of multi-hop QA, RAG has emerged as a prevalent way for addressing such problems. A prevalent RAG framework follows a retrieve-rank-generate pipeline: first retrieving relevant knowledge snippets using a search engine, then employing a reranker model ${ }^{1}$ to rank snippets based on relevance to the question, and finally using the ranked snippets as context for LLM generation. We select this standard RAG approach as our baseline because it represents the mainstream implementation of current RAG systems (Chen et al., 2024), which retrieve top-5 snippets from the Google Search API as context for the generation of LLM answers.</p>
<p>Based on this prevalent RAG framework, our variant primarily enhances the reranking process to introduce more CoE-compliant external knowledge into the context. Specifically, while the original reranker focuses on pure relevance matching, CoE features from questions can provide additional structural guidance, as shown in Figure 4.</p>
<ul>
<li>CoE Feature Judgment: The CoE features (intent, evidence nodes and relations) extracted from questions can be used to judge their presence in knowledge snippets through feature discrimination, producing judgments on feature coverage.</li>
<li>Coverage-based Selection: The reranking process can prioritize snippets containing</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>| Model | Multi-Hop QA(ACC) | | Attack (ASR) | | Defense (ACC/ASR) | |
| | RAG | RAG+CoE | PR | PR+CoE | IR | IR+CoE |
| --- | --- | --- | --- | --- | --- | --- |
| GPT-3.5 | 68.1% | 76.0% | 69.0% | 79.0% | 49.0%/42.0% | 78.0%/8.0% |
| GPT-4 | 72.9% | 82.6% | 49.0% | 62.0% | 56.0%/38.0% | 80.0%/5.0% |
| Llama2-33B | 64.4% | 74.1% | 62.0% | 71.0% | 45.0%/51.0% | 79.0%/6.0% |
| Llama3-70B | 67.8% | 79.5% | 60.0% | 76.0% | 60.0%/37.0% | 84.0%/6.0% |
| Qwen2.5-32B | 63.8% | 77.0% | 73.0% | 80.0% | 51.0%/42.0% | 76.0%/6.0% |</p>
<p>Table 4: Performance comparison between baselines and after adding CoE in three application scenarios.</p>
<p>more CoE features, particularly focusing on intent coverage first, followed by evidence relations and nodes. The detailed selection process is shown in Appendix F.</p>
<p>This optimized snippet selection serves as enhanced context for LLM generation.</p>
<h3>8.2 External Knowledge Poisoning</h3>
<p>External knowledge poisoning attacks aim to manipulate RAG systems by injecting malicious content into the knowledge base. We select PoisonedRAG (PR) <em>Zou et al. (2024)</em> as our baseline, which uses LLM to generate false supporting documents for incorrect answers and injects them into the RAG knowledge base, causing the retriever to select these poisoned documents as context and mislead LLM to generate target answers.</p>
<p>Building upon PR, the SOTA knowledge poisoning attack, our variant enhances the document generation process by incorporating CoE features from target questions. By extracting CoE features from questions and integrating them into the generation process, the generated false knowledge exhibits stronger logical and semantic alignment with the questions. The detailed generation prompts incorporating these structural features are provided in Appendix J.</p>
<h3>8.3 Poisoning Defense</h3>
<p>Poisoning defenses aim to protect RAG systems against knowledge poisoning attacks. We select InstructRAG (IR) <em>Wei et al. (2024)</em> as our baseline, which asks LLMs to first rationalize evidence relevance and uses these rationales for context selection, enhancing the system’s ability to identify and reject misleading information.</p>
<p>Building upon IR, the SOTA RAG defense framework, our variant strengthens its defensive capability by incorporating CoE-structured knowledge validation. By generating knowledge containing CoE features extracted from questions, these CoE-structured knowledge are injected into the knowledge base. It provides more systematic supporting evidence when the framework requests document rationales. This enables LLMs to effectively select defensive knowledge pieces within the framework. The detailed generation process is provided in Appendix J.</p>
<h3>8.4 Evaluation and Results</h3>
<p>We evaluate the effectiveness of CoE across three RAG scenarios using the HotpotQA dataset. To investigate how CoE enhances RAG performance in multi-hop QA, we measure the effectiveness using accuracy (ACC). Table 4 shows that RAG+CoE achieves an average ACC improvement of 10.4% compared to RAG. Notably, RAG+CoE with GPT-4 achieves the highest ACC of 82.6%, while Qwen2.5-32B shows the most significant improvement when CoE is integrated into RAG, with a 13.2% increase in ACC. The results illustrate that knowledge structured through CoE provides more effective context for LLMs to reason and generate accurate responses.</p>
<p>In the knowledge poisoning attack, we measure the attack effectiveness using attack success rate (ASR). PR+CoE achieves 11.0% higher ASR on average across LLMs compared to PR, revealing that malicious knowledge deliberately structured through CoE becomes more effective at manipulating LLM outputs.</p>
<p>For RAG defense evaluation, we examine both ACC and ASR. IR+CoE demonstrates strong defensive capability with 27.2% higher ACC and 35.8% lower ASR compared to IR, indicating that CoE-structured defensive knowledge enables LLMs to better identify and resist misleading information while maintaining accurate responses.</p>
<h2>9 Conclusion</h2>
<p>In this paper, we introduce CoE and investigate its impact on LLMs in imperfect external knowledge for multi-hop QA. We characterize the CoE features and propose a discrimination approach to judge whether external knowledge exhibits the features within the user question. Generally, our study reveals that external knowledge aligned with CoE features exhibits stronger significance, deceptiveness, and robustness against extraneous and inaccurate information in contexts. We further validate the CoE-oriented findings by applying them to tasks that leverage external knowledge, demonstrating that the CoE-enhanced variants consistently outperform their original baseline counterparts.</p>
<h2>Limitations</h2>
<p>There are three limitations to the current study. Firstly, we apply the $R A G+C O E$ to search for CoE in external knowledge, but there is no step to verify the correctness of answers within the CoE. If the retrieved CoE contains incorrect information, it may mislead the LLM to generate inaccurate responses. In Section 6, we discuss LLMs' Following Rate to CoE containing factual errors, showing that LLMs are highly likely to follow the knowledge provided in CoE.</p>
<p>Secondly, the usability of our proposed CoEbased reranking strategy $(R A G+C O E)$ has inherent constraints across RAG scenarios. For instance, some RAG scenarios convert external knowledge into vectors and store them in vector databases, then search for question-relevant knowledge at the vector level during the retrieval phase.. Our approach, which operates at the textual level, is not suitable for such vector-based RAG scenarios.</p>
<p>Thirdly, our approach relies on prompt-based extraction of evidence nodes using GPT-4o, potential extraction errors (either incorrect identification or missing of evidence nodes) may affect CoE's performance. We systematically analyze these scenarios in Appendix D, where experiments on 1,000 HotpotQA samples demonstrate the robustness of our method: even under imperfect extraction conditions, the accuracy only drops marginally (from $90.2 \%$ to $89.3 \%$ and $89.4 \%$ ). This suggests that while evidence node extraction quality matters, our approach maintains strong performance even with occasional extraction imperfections.</p>
<h2>Ethical Statement</h2>
<p>Our exploration of CoE-enhanced knowledge poisoning attacks is conducted strictly for red-team testing purposes to identify and understand potential vulnerabilities in RAG systems. Following responsible security research practices, we have promptly reported our findings to relevant RAG system providers and knowledge base platforms. We present only high-level methodological insights necessary for academic understanding, without releasing detailed attack implementations. Our goal is to help develop more robust RAG systems by revealing potential weaknesses in their knowledge base integration, thereby contributing to improved security measures rather than facilitating malicious exploits.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2024. Evaluating correctness and faithfulness of instructionfollowing models for question answering. Transactions of the Association for Computational Linguistics, 12:681-699.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, et al. 2023. Palm 2 technical report.</p>
<p>Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511.</p>
<p>Hung-Ting Chen, Michael J. Q. Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2292-2307.</p>
<p>Hung-Ting Chen, Michael JQ Zhang, and Eunsol Choi. 2022. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. arXiv preprint arXiv:2210.13701.</p>
<p>Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17754-17762.</p>
<p>Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, and Jun Xu. 2024. Unifying bias and unfairness in information retrieval: A survey of challenges and opportunities with large language models. arXiv preprint arXiv:2404.11457.</p>
<p>Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, and Anton Tsitsulin. 2024. Don't forget to connect! improving RAG with graph-based reranking. CoRR, abs/2405.18414.</p>
<p>Gary Edmond and Kent Roach. 2011. A contextual approach to the admissibility of the state's forensic science and medical evidence. University of Toronto Law Journal, 61(3):343-409.</p>
<p>Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics.</p>
<p>Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Qiuxia Li, and Jun Zhao. 2024.</p>
<p>Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models. arXiv preprint arXiv:2402.14409.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, yoichi takahashi, Ronan Le Bras, Akari Asai, Xinyan Velocity Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. 2023. Realtime QA: What's the answer right now? In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.</p>
<p>Siyi Liu, Qiang Ning, Kishaloy Halder, Wei Xiao, Zheng Qi, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, et al. 2024. Open domain question answering with conflicting contexts. arXiv preprint arXiv:2410.12311.</p>
<p>Erin Murphy. 2013. The mismatch between twenty-firstcentury forensic evidence and our antiquated criminal justice system. S. Cal. L. Rev., 87:633.</p>
<p>OpenAI. 2022. Chatgpt. https://openai.com/blog/ chatgpt.</p>
<p>Qwen Team. 2024. Qwen2.5: A party of foundation models! Blog post.</p>
<p>Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng. 2024. Blinded by generated contexts: How language models merge generated and retrieved contexts when knowledge conflicts? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6207-6227.</p>
<p>Yixuan Tang and Yi Yang. 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, and Juanzi Li. 2024. R-eval: A unified toolkit for evaluating domain knowledge of retrieval augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5813-5824.</p>
<p>Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan Ö. Arık. 2024. Astute rag: Overcoming imperfect retrieval augmentation and knowledge conflicts for large language models.</p>
<p>Zhepei Wei, Wei-Lin Chen, and Yu Meng. 2024. Instructrag: Instructing retrieval-augmented generation via self-synthesized rationales. arXiv preprint arXiv:2406.13629.</p>
<p>Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. 2024. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556.</p>
<p>Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300.</p>
<p>Rongwu Xu, Brian Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, and Han Qiu. 2024a. The earth is flat because...: Investigating LLMs' belief towards misinformation via persuasive conversation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024b. Knowledge conflicts for llms: A survey. arXiv preprint arXiv:2403.08319.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Hao Zhang, Yuyang Zhang, Xiaoguang Li, Wenxuan Shi, Haonan Xu, Huanshuo Liu, Yusheng Wang, Lifeng Shang, Qun Liu, Yong Liu, et al. 2024. Evaluating the external and parametric knowledge fusion of large language models. arXiv preprint arXiv:2405.19010.</p>
<p>Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-augmented generation for ai-generated content: A survey. arXiv preprint arXiv:2402.19473.</p>
<p>Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models. arXiv preprint arXiv:2303.11315.</p>
<p>Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, TsungYi Ho, and Philip S Yu. 2024. Trustworthiness in retrieval-augmented generation systems: A survey. arXiv preprint arXiv:2409.10102.</p>
<p>Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Type</th>
<th>Sample Num</th>
<th>Knowledge Piece Num</th>
</tr>
</thead>
<tbody>
<tr>
<td>HotpotQA</td>
<td>CoE</td>
<td>676</td>
<td>4.0</td>
</tr>
<tr>
<td></td>
<td>SenP</td>
<td>676</td>
<td>2.1</td>
</tr>
<tr>
<td></td>
<td>WordP</td>
<td>676</td>
<td>4.0</td>
</tr>
<tr>
<td>2WikiMultihopQA</td>
<td>CoE</td>
<td>660</td>
<td>3.4</td>
</tr>
<tr>
<td></td>
<td>SenP</td>
<td>660</td>
<td>1.9</td>
</tr>
<tr>
<td></td>
<td>WordP</td>
<td>660</td>
<td>3.4</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>B</th>
<th>Feature Effectiveness Analysis on LLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Performance</td>
<td></td>
</tr>
<tr>
<td>In this analysis, we examine three types of feature</td>
<td></td>
</tr>
<tr>
<td>perturbations: WordP, Evidence RelationP (ERP),</td>
<td></td>
</tr>
<tr>
<td>and IntentP using GPT-3.5 as our testing model</td>
<td></td>
</tr>
<tr>
<td>on the HotpotQA dataset. WordP involves per-</td>
<td></td>
</tr>
<tr>
<td>turbing evidence node as detailed in Section 4.2.</td>
<td></td>
</tr>
<tr>
<td>ERP removes evidence relations from the external</td>
<td></td>
</tr>
<tr>
<td>knowledge (CoE) by prompting the LLM to modify</td>
<td></td>
</tr>
<tr>
<td>the text while preserving other features. Similarly,</td>
<td></td>
</tr>
<tr>
<td>IntentP removes intent information from CoE while</td>
<td></td>
</tr>
<tr>
<td>maintaining other features. The experimental re-</td>
<td></td>
</tr>
<tr>
<td>sults are presented in Table 6.</td>
<td></td>
</tr>
<tr>
<td>The significance analysis (RQ1) reveals that evi-</td>
<td></td>
</tr>
<tr>
<td>dence relation perturbation has the least impact on</td>
<td></td>
</tr>
<tr>
<td>LLM accuracy, followed by evidence node pertur-</td>
<td></td>
</tr>
<tr>
<td>bation, and then intent perturbation. This suggests</td>
<td></td>
</tr>
<tr>
<td>that intent information plays the most crucial role</td>
<td></td>
</tr>
<tr>
<td>in maintaining LLM accuracy.</td>
<td></td>
</tr>
<tr>
<td>Regarding deceptiveness (RQ2), CoE achieves</td>
<td></td>
</tr>
<tr>
<td>the highest attack success rate when lacking evi-</td>
<td></td>
</tr>
<tr>
<td>dence nodes, followed by missing evidence relation,</td>
<td></td>
</tr>
<tr>
<td>and then intent. This highlights the significance</td>
<td></td>
</tr>
<tr>
<td>of relationships and intent in affecting LLM vul-</td>
<td></td>
</tr>
<tr>
<td>nerability. However, CoE lacking evidence nodes</td>
<td></td>
</tr>
<tr>
<td>demonstrates weaker stability against irrelevant ex-</td>
<td></td>
</tr>
<tr>
<td>ternal knowledge under attack scenarios, indicating</td>
<td></td>
</tr>
<tr>
<td>that evidence nodes play a vital role in maintaining</td>
<td></td>
</tr>
<tr>
<td>attack effectiveness when facing noisy knowledge</td>
<td></td>
</tr>
<tr>
<td>during deception attempts.</td>
<td></td>
</tr>
<tr>
<td>For robustness against misinformation (RQ3),</td>
<td></td>
</tr>
<tr>
<td>the absence of evidence relations leads to the most</td>
<td></td>
</tr>
<tr>
<td>significant decrease in LLM accuracy when mis-</td>
<td></td>
</tr>
<tr>
<td>leading information is introduced. This underscores</td>
<td></td>
</tr>
<tr>
<td>that evidence relations are crucial features for con-</td>
<td></td>
</tr>
<tr>
<td>structing complete evidence chains and maintaining</td>
<td></td>
</tr>
<tr>
<td>model reliability. In conclusion, each feature</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 6: Performance of GPT-3.5 with CoE and Non-CoE on HotpotQA Dataset</p>
<table>
<thead>
<tr>
<th>RQ</th>
<th>Metric</th>
<th>Proportion Type</th>
<th>Proportion</th>
<th>CoE</th>
<th>WordP</th>
<th>ERP</th>
<th>IntentP</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>ACC</td>
<td>Irrelevant</td>
<td>0</td>
<td>91.9%</td>
<td>79.1%</td>
<td>81.1%</td>
<td>59.9%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.25</td>
<td>90.3%</td>
<td>77.5%</td>
<td>81.6%</td>
<td>56.5%</td>
</tr>
<tr>
<td>RQ1</td>
<td>ACC</td>
<td>Irrelevant</td>
<td>0.50</td>
<td>89.9%</td>
<td>75.4%</td>
<td>78.5%</td>
<td>54.5%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.75</td>
<td>88.9%</td>
<td>74.5%</td>
<td>76.9%</td>
<td>54.5%</td>
</tr>
<tr>
<td>RQ2</td>
<td>ASR</td>
<td>Irrelevant</td>
<td>0</td>
<td>86.1%</td>
<td>83.1%</td>
<td>69.2%</td>
<td>57.3%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.25</td>
<td>85.8%</td>
<td>79.1%</td>
<td>64.8%</td>
<td>54.8%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.50</td>
<td>84.7%</td>
<td>77.8%</td>
<td>61.4%</td>
<td>53.2%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.75</td>
<td>78.4%</td>
<td>73.7%</td>
<td>58.1%</td>
<td>49.0%</td>
</tr>
<tr>
<td>RQ3</td>
<td>ACC</td>
<td>Misinformation</td>
<td>0</td>
<td>91.9%</td>
<td>79.1%</td>
<td>81.1%</td>
<td>59.9%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.25</td>
<td>81.8%</td>
<td>64.0%</td>
<td>21.7%</td>
<td>53.1%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.50</td>
<td>82.0%</td>
<td>65.7%</td>
<td>21.2%</td>
<td>52.1%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.75</td>
<td>75.7%</td>
<td>60.8%</td>
<td>19.0%</td>
<td>47.8%</td>
</tr>
</tbody>
</table>
<p>demonstrates distinct strengths in different scenarios: intent information is crucial for maintaining overall accuracy, relationships are vital for constructing evidence chains and misinformation resistance, while evidence nodes play a key role in handling irrelevant knowledge under misinformation scenarios. This diverse functionality suggests that intent, evidence relations and evidence nodes are all indispensable components in constructing effective Chain-of-Evidence (CoE) for robust LLM performance.</p>
<h2>C Effectiveness of CoE in Single-hop QA and Analysis of Hop Numbers</h2>
<p>To provide a comprehensive evaluation of CoE's effectiveness, we conducted additional experiments on single-hop scenarios alongside our main multihop experiments. Multi-hop questions are particularly challenging for LLMs as they require sophisticated knowledge integration and logical reasoning capabilities. However, examining single-hop scenarios helps establish the generalizability of our approach across different reasoning complexity levels.</p>
<p>We evaluated GPT-3.5 on a single-hop dataset (RGB) following the experimental settings from RQ1-RQ3. The results shown in Table 7 reveal several interesting findings:</p>
<ul>
<li>
<p>CoE demonstrates consistent effectiveness in both single-hop and multi-hop scenarios, as shown in RQ1. However, both CoE and Non-CoE exhibit stronger resistance to irrelevant information in single-hop scenarios, which can be attributed to the reduced complexity of single-step reasoning tasks.</p>
</li>
<li>
<p>The core advantages of CoE observed in RQ2 and RQ3 remain consistent across both singlehop and multi-hop contexts, supporting the broader applicability of our approach.</p>
</li>
<li>Our comparative analysis reveals that while the number of reasoning hops does not significantly impact CoE's significance and robustness, it notably affects Non-CoE. As the number of hops increases, SenP and WordP show decreased resistance to imperfect knowledge. This pattern emerges because multihop reasoning requires both individual knowledge comprehension and cross-hop integration, making the LLM more vulnerable to irrelevant or misleading information.</li>
</ul>
<p>These findings further validate CoE's capability to effectively guide LLM reasoning regardless of the reasoning complexity, while highlighting its particular advantages in more challenging multihop scenarios.</p>
<p>Besides, to analyze the robustness of CoE across different reasoning complexity levels, We conduct statistical analysis based on results from Table 1 and Table 7 on GPT-3.5's performance on questions requiring one-hop, two-hop, and three-hop reasoning while gradually introducing irrelevant knowledge. The results reveal interesting patterns across reasoning depths. For one-hop questions, CoE maintains consistently high accuracy (above 92.0\%) even with increasing irrelevant knowledge, demonstrating strong robustness in simple reasoning scenarios where direct evidence-to-answer mapping is sufficient. The performance on two-hop questions shows more sensitivity to irrelevant knowledge, with accuracy declining from $91.0 \%$ to $88.0 \%$. This suggests that intermediate reasoning steps are more vulnerable to distraction from irrelevant information. Interestingly, for three-hop questions, despite the higher reasoning complexity, the model shows better resilience than two-hop cases, maintaining accuracy above $90 \%$ in most scenarios. This counter-intuitive improvement may be attributed to the LLM's enhanced focus when processing more complex reasoning chains.</p>
<p>Table 7: Performance of GPT-3.5 with CoE and NonCoE on Single-hop Dataset</p>
<table>
<thead>
<tr>
<th>RQ</th>
<th>Metric</th>
<th>Proportion Type</th>
<th>Proportion</th>
<th>CoE</th>
<th>SenP</th>
<th>WordP</th>
</tr>
</thead>
<tbody>
<tr>
<td>RQ1</td>
<td>ACC</td>
<td>Irrelevant</td>
<td>0</td>
<td>$93.0 \%$</td>
<td>$74.0 \%$</td>
<td>$84.1 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.25</td>
<td>$93.4 \%$</td>
<td>$77.9 \%$</td>
<td>$84.4 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.50</td>
<td>$93.4 \%$</td>
<td>$80.2 \%$</td>
<td>$84.8 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.75</td>
<td>$92.6 \%$</td>
<td>$79.8 \%$</td>
<td>$85.6 \%$</td>
</tr>
<tr>
<td>RQ2</td>
<td>ASR</td>
<td>Irrelevant</td>
<td>0</td>
<td>$87.9 \%$</td>
<td>$55.0 \%$</td>
<td>$85.1 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.25</td>
<td>$79.4 \%$</td>
<td>$47.4 \%$</td>
<td>$66.3 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.50</td>
<td>$67.4 \%$</td>
<td>$40.4 \%$</td>
<td>$52.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.75</td>
<td>$62.7 \%$</td>
<td>$32.7 \%$</td>
<td>$47.0 \%$</td>
</tr>
<tr>
<td>RQ3</td>
<td>ACC</td>
<td>Misinformation</td>
<td>0</td>
<td>$93.0 \%$</td>
<td>$74.0 \%$</td>
<td>$84.1 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.25</td>
<td>$86.8 \%$</td>
<td>$65.1 \%$</td>
<td>$74.0 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.50</td>
<td>$83.3 \%$</td>
<td>$65.8 \%$</td>
<td>$67.4 \%$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0.75</td>
<td>$77.5 \%$</td>
<td>$60.0 \%$</td>
<td>$60.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: Accuracy of GPT-3.5 under Different Hop Num</p>
<table>
<thead>
<tr>
<th>Irrelevant Proportion</th>
<th>One-hop</th>
<th>Two-hop</th>
<th>Three-hop</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>$93.0 \%$</td>
<td>$91.0 \%$</td>
<td>$94.0 \%$</td>
</tr>
<tr>
<td>0.25</td>
<td>$93.4 \%$</td>
<td>$89.0 \%$</td>
<td>$90.0 \%$</td>
</tr>
<tr>
<td>0.50</td>
<td>$93.4 \%$</td>
<td>$88.0 \%$</td>
<td>$92.0 \%$</td>
</tr>
<tr>
<td>0.75</td>
<td>$92.6 \%$</td>
<td>$88.0 \%$</td>
<td>$92.0 \%$</td>
</tr>
</tbody>
</table>
<h2>D Reliability of automated evidence nodes extraction for CoE and its impact on performance</h2>
<p>In our approach, we define evidence nodes and provide few-shot examples in the prompt for GPT40 to perform evidence node extraction. Given that automated evidence node extraction may contain errors in real-world applications, we conducted a systematic analysis of potential evidence node extraction errors. These errors primarily manifest in two ways: 1) Extraction Errors: incorrectly identifying intent-related content as evidence nodes; 2) Missing Errors: failing to extract essential evidence nodes. For example, as shown in Figure 1, Extraction Errors would occur when "state" from the intent/question is incorrectly included in the evidence nodes, while Missing Errors would happen when essential evidence node like "CEO" are not extracted, both of which could affect the accuracy of CoE identification. To assess the impact of these potential errors, we designed corresponding perturbation operations and simulated both error types on our test dataset. The detailed experimental results and analysis are presented in Table 9.</p>
<p>To examine the impact of imperfect extraction, we conducted experiments on 1,000 HotpotQA samples by either adding a shared entity from intent/question (Extraction Errors) or randomly removing one evidence node (Missing Errors). The result show that Missing Errors led to over-identification</p>
<p>Table 9: Accuracy of GPT-3.5 under Different Evidence Nodes Error Types</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Irrelevant Proportion</th>
<th style="text-align: center;">Our</th>
<th style="text-align: center;">Missing Errors</th>
<th style="text-align: center;">Extraction Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$91.9 \%$</td>
<td style="text-align: center;">$91.2 \%$</td>
<td style="text-align: center;">$91.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">$90.3 \%$</td>
<td style="text-align: center;">$90.1 \%$</td>
<td style="text-align: center;">$90.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$89.9 \%$</td>
<td style="text-align: center;">$89.2 \%$</td>
<td style="text-align: center;">$88.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$88.9 \%$</td>
<td style="text-align: center;">$87.4 \%$</td>
<td style="text-align: center;">$87.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">676</td>
<td style="text-align: center;">803</td>
<td style="text-align: center;">641</td>
</tr>
</tbody>
</table>
<p>of CoE (803 vs. 676 Our), while Extraction Errors resulted in under-identification (641). Both scenarios slightly decreased response accuracy compared to normal conditions ( $90.2 \%$ Our, $89.4 \%$ Missing Errors, 89.3\% Extraction Errors).</p>
<h2>E Perturbation Strategies for Non-CoE Construction</h2>
<p>We employ two controlled perturbation strategies to construct Non-CoE samples while maintaining the same question context:</p>
<p>Sentence-Level Perturbation (SenP). For multihop QA, we simulate incomplete knowledge scenarios by removing knowledge pieces from CoE. We segment CoE into sentences and identify candidates containing question-mentioned evidence nodes (excluding answer nodes). We iteratively remove these candidates until CoE discrimination confirms that the remaining knowledge no longer contains complete CoE. This sentence-level approach helps understand how LLMs behave when key evidence pieces are entirely missing within the same reasoning context. Figure 3 shows this sentence-level perturbation process.</p>
<p>Word-Level Perturbation (WordP). We create Non-CoE by replacing specific evidence nodes with their GPT-4 generated higher-level expressions (e.g., replacing hotel company" with business organization"), maintaining more original information compared to sentence removal. This finer-grained approach examines LLMs' sensitivity to evidence nodes while preserving most of the original semantic information. Figure 3 demonstrates this word-level perturbation approach.</p>
<h2>F The Algorithm for the Coverage-Based Selection</h2>
<p>We show the detailed algorithm 1 for the minimal coverage search in $R A G+C o E$.</p>
<h2>G The Details in RAG+CoE</h2>
<p>We show the overview of $R A G+C o E$ in Figure 4.</p>
<p>CoE: The Oberoi Group's main headquarters is located at 7, Sham Nath Marg Delhi, 110054 IN. This hotel company has employees across 6 continents, including AsiaAfricaNorth
SenP: The Oberoi Group's main headquarters is located at 7, Sham Nath Marg Delhi, 110054 IN.
Word Substitution
WordP: The Oberoi Group's main headquarters is located at 7, Sham Nath Marg Delhi, 110054 IN. This business organization has employees across 6 continents, including AsiaAfricaNorth
Figure 3: Examples of CoE and two types of Non-CoE.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The overview of $R A G+C o E$.</p>
<h2>H Details of Information Extraction Prompts</h2>
<p>The details of the information extraction prompts are illustrated below. In pipeline, we replace the placeholders in the following prompts with the question and evidence nodes.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">Coverage</span><span class="o">-</span><span class="n">Based</span><span class="w"> </span><span class="n">Selection</span>
<span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="k">External</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">Judged</span><span class="w"> </span><span class="k">external</span>
<span class="w">        </span><span class="n">knowledge</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">item</span><span class="w"> </span><span class="k">contains</span>
<span class="w">        </span><span class="n">Intent</span><span class="p">,</span><span class="w"> </span><span class="n">Evidence</span><span class="w"> </span><span class="n">Relations</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Evidence</span>
<span class="w">        </span><span class="n">nodes</span><span class="w"> </span><span class="n">judgments</span>
<span class="k">Output</span><span class="err">:</span><span class="w"> </span><span class="k">Set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">minimal</span><span class="w"> </span><span class="n">coverage</span><span class="w"> </span><span class="k">external</span>
<span class="w">        </span><span class="n">knowledge</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\#</span><span class="w"> </span><span class="n">Phase</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">Intent</span><span class="w"> </span><span class="n">Coverage</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">|</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="o">|-</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">\</span><span class="p">).</span><span class="w"> </span><span class="n">Intent</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">=</span><span class="n">T</span><span class="w"> </span><span class="n">R</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">E</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="err">\{</span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">\}\</span><span class="p">)</span>
<span class="err">\#</span><span class="w"> </span><span class="n">Phase</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="n">Evidence</span><span class="w"> </span><span class="n">Relation</span><span class="w"> </span><span class="n">Coverage</span><span class="p">;</span>
<span class="err">\</span><span class="p">(</span><span class="n">R_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">uncovered</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="n">GetUncoveredEvidencerelation</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="n">R_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">uncovered</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">|</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="o">|-</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">\</span><span class="p">).</span><span class="n">Evidencerelation</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">[</span><span class="n">r</span><span class="o">]=</span><span class="n">T</span><span class="w"> </span><span class="n">R</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">E</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="k">then</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="err">\{</span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">\}</span><span class="w"> </span><span class="p">;</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="k">break</span><span class="p">;</span>
<span class="mi">13</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="n">Phase</span><span class="w"> </span><span class="mi">3</span><span class="err">:</span><span class="w"> </span><span class="n">Evidence</span><span class="w"> </span><span class="n">Node</span><span class="w"> </span><span class="n">Coverage</span><span class="p">;</span>
<span class="mi">14</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">K_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">uncovered</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="n">GetUncoveredEvidencenodes</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="mi">15</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="n">K_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">uncovered</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">|</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="o">|-</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">I</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">\</span><span class="p">).</span><span class="n">Evidencenode</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">[</span><span class="n">k</span><span class="o">]=</span><span class="n">T</span><span class="w"> </span><span class="n">R</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">E</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="err">\{</span><span class="n">E</span><span class="w"> </span><span class="n">K</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">\}</span><span class="w"> </span><span class="p">;</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="k">break</span><span class="p">;</span>
</code></pre></div>

<p>20 return $S$;</p>
<h2>Intent and evidence node Extraction Prompt:</h2>
<p>Please extract both the intent and evidence nodes of the question, using the following criteria:</p>
<p>1) As for intent, please indicate the content intent of the evidence that the question expects, without going into specific details.
2) As for evidence nodes, Please extract the specific details of the question.
The output must be in json format, consistent with the sample. Here are some examples:</p>
<h2>Example1:</h2>
<p>Question:750 7th Avenue and 101 Park Avenue, are located in which city?
Output: { "Intent": "City address Information", "evidence nodes": ["750 7th Avenue", "101 Park Avenue"] }</p>
<h2>Example2:</h2>
<p>Question: The Oberoi family is part of a hotel company that has a head office in what city?
Output: { "Intent": "City address Information", "evidence nodes": ["Oberoi family", "head office"] }</p>
<h2>Example3:</h2>
<p>Question: What nationality was James Henry Miller's wife?
Output: { "Intent": "Nationality of person", "evidence nodes": ["James Henry Miller", "wife"] }</p>
<h2>Example4:</h2>
<p>Question: What is the length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged? Output: { "Intent": "Length of track", "evidence nodes": ["2013 Liqui Moly Bathurst 12 Hour"] }</p>
<h2>Example5:</h2>
<p>Question: In which American football game was Malcolm Smith named Most Valuable player? Output: { "Intent": "Name of American football game", "evidence nodes": ["Malcolm Smith", "Most Valuable player"] }
Question: [Question] Output:</p>
<h2>Evidence Relations Extraction Prompt:</h2>
<p>Please extract evidence relations based on the input questions and evidence nodes, using the following criteria:</p>
<p>1) Each evidence relation has two elements, the implied evidence nodes and the textual description of the evidence relations.
2) The description of the evidence relations is limited to the two evidence nodes and does not involve other evidence nodes.
3) If there is no evidence relation between evidence nodes, no extraction is required.
The output must be in json format, consistent with the examples. Here are some examples:
The output must be in json format, consistent with the sample. Here are some examples:</p>
<h2>Example1:</h2>
<p>Question:750 7th Avenue and 101 Park Avenue, are located in which city?
Evidence nodes:["750 7th Avenue", "101 Park Avenue"]
Output: []
Example2:
Question: Lee Jun-fan played what character in The Green Hornetfelevision series?
Evidence nodes:["Lee Jun-fan", "The Green Hornet"] Output: [["Evidence nodes":["Lee Jun-fan", "The Green Hornet"], "Evidence Relations: "played character in" $}]$</p>
<h2>Example3:</h2>
<p>Question: In which stadium do the teams owned by Myra Kraft's husband play?
Evidence nodes: ["teams", "Myra Kraft's husband"] Output: [["Evidence nodes":["teams", "Myra Kraft's husband"], "Evidence Relations": "is owned by"]]</p>
<h2>Example4:</h2>
<p>Question: The Colts' first ever draft pick was a halfback who won the Heisman Trophy in what year? Evidence nodes:["Colts' first ever draft pick", "halfback", "Heisman Trophy"]
Output:[["Evidence nodes":["Colts' first ever draft pick", "halfback"], "Evidence Relations": "was"]]</p>
<h2>Example5:</h2>
<p>Question: The Golden Globe Award winner for best actor from "Roseanne" starred along what actress in Gigantic?
Evidence nodes:["Golden Globe Award winner", "best actor", "Roseanne", "Gigantic"]
Output: [["Evidence nodes":["Golden Globe Award winner", "best actor"], "Evidence Relations": "for"], ["Evidence nodes":["best actor", "Roseanne"], "Evidence Relations": "starred in " $}]$ ]</p>
<h2>Question: [Question]</h2>
<p>Evidence nodes: [Evidence node]
Output:</p>
<h2>I Details of Feature Discrimination Prompts</h2>
<p>The details of the Feature Discrimination prompts are illustrated below. In pipeline, we replace the placeholders in the following prompts with the external knowledge, intent, evidence node, and evidence relation.</p>
<h2>Intent Discrimination Prompt:</h2>
<p>Please determine whether the input intent is covered in the input external knowledge. Please output only "yes" or "no".
Input intent: [Intent]
Input external knowledge: [External Knowledge]</p>
<h2>Evidence Nodes Discrimination Prompt:</h2>
<p>Please determine if the input evidence node is mentioned in the input external knowledge. It doesn't necessarily need to be an exact character match; partial matches or semantic similarities are also acceptable. Please output only "yes" or "no".
Input evidence node: [Evidence node]
Input external knowledge: [External Knowledge]</p>
<h2>Evidence Relations Discrimination Prompt:</h2>
<p>Please determine if the input external knowledge supports the logical relationship between the two given evidence nodes. If there is explicit evidence in the input knowledge that confirms the evidence node-evidence relation-evidence node triple, output "yes"; otherwise output "no". Please respond only with "yes" or "no".
Input triple: {evidence node1, evidence relation, evidence node2}
Input external knowledge: [External Knowledge]</p>
<h2>J Details of the CoE-Guided Knowledge Generation Prompts</h2>
<p>CoE-Guided Knowledge Generation Prompt:<br>Given:<br>Evidence Nodes: [Nodes]<br>Evidence Relations: [Relations]<br>Intent: [Intent]<br>Question: [Question]<br>Answer: [Answer]<br>Create a corpus that:<br>1. Integrate all given [Nodes] naturally, do not replace [Nodes] with pronouns, each sentence in the corpus include as many [Nodes] as possible.<br>2. If there are [Relations] between [Nodes], when constructing sentences containing these [Nodes], try to express the [Relations] between them.<br>3. The corpus should align with the meaning of the [Intent].<br>4. Flows naturally as coherent text<br>5. Please limited the corpus to 100 words.<br>The corpus should serve as a comprehensive basis for supporting why the [Question] has the answer:[Answer].<br>It's a creative game focusing on generating the support for the specified answer: [Answer], without requiring factual accuracy.</p>
<h2>K Details of the Answer Generation Prompts</h2>
<p>The details of the Answer Generation prompts are illustrated below. In pipeline, we replace the</p>
<h2>Answer Generation Prompt:</h2>
<p>For the input phrase, please generate a phrase of similar type and format, but not the same. Just output the phrase, no explanation is needed, the expression form is consistent with the examples. Here are some examples:</p>
<h2>Example1:</h2>
<p>Input phrase: United States
Output: Canada
Example2:
Input phrase: alcohol
Output: Soda
Example3:
Input phrase: September 29, 1784
Output: April 22, 1964
Example4:
Input phrase: Laura Ellen Kirk
Output: Elon Musk
Example5:
Input phrase: 39,134
Output: 19,203
Input phrase: [Correct Answer]
Output:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/BAAI/bge-reranker-large&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2986 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2986</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2986</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-268358225</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.05845v1.pdf" target="_blank">Reverse That Number! Decoding Order Matters in Arithmetic Learning</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations. However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step. Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity. We have developed and applied this method in a comprehensive set of experiments. Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of in accuracy while requiring only a third of the tokens typically used during training. For the purpose of facilitating replication and further research, we have made our code and dataset publicly available at \url{https://anonymous.4open.science/r/RAIT-9FB7/}.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2986.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2986.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Little-Endian Fine-Tuning (LEFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning method that reverses digit order (Little-Endian) so models emit least-significant digits first, reducing per-digit input context and learning complexity; applied with/without step-by-step for different operations and shown to improve accuracy and token efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama2-13B base checkpoint (13B parameter transformer) extended to 4,096 token context length and fine-tuned on arithmetic datasets using the LEFT formatting and optional step-by-step intermediates.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition, subtraction, and multiplication (input digits balanced in range 5–12; train/test splits: 5K train per op, 1K test per op; experiments include joint and per-operation training).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Reversing output/input digit order (Little-Endian) reduces conditional input complexity per output digit, enabling local carry reconstruction (model recomputes carry from previous least-significant digits) and thereby simplifying arithmetic into primarily local digit computations; multiplication still benefits from stepwise decomposition of digit-wise products but performed in LE order.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical gains: LEFT yields +11.1% overall accuracy vs prior SOTA in joint training and +35.7% on multiplication; large reductions in training tokens required (claims: LEFT uses 5.2% of prior SOTA tokens for add/sub in some settings; joint training token usage reported 3,040,616 vs Scratchpad-Detailed 10,995,191). Interpretability: attention visualizations show specific heads correlate output digits to relevant input digits and a deeper layer (layer 22) showing signals consistent with carry reconstruction. Error analysis: Scratchpad-Detailed had 417 intermediate errors (140 intermediate-product, 236 accumulate-sum) vs LEFT having 99 intermediate errors (77 intermediate-product, 22 accumulate-sum), indicating reduced error propagation when using LE.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>LEFT's advantages are not universal: multiplication cannot be learned without step-by-step decomposition when only 5K training cases are available (i.e., LE alone fails for multiplication in low-data regime). Pretraining bias: models pretrained on standard (Big-Endian) numerals show slower/fragile learning for subtraction in LE without step-by-step, indicating mismatch between pretraining numeral order and LEFT can hurt initial learning. Performance degrades with larger input digit lengths, especially for multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning with Little-Endian input/output formatting; optional addition of step-by-step (scratchpad / chain-of-thought style) intermediate tokens for operations that need decomposition (multiplication).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved accuracy (per-table: addition 98.8%, subtraction 95.9%, multiplication 88.5%, overall 94.4% in joint eval), substantial token-efficiency gains (reported 11.1% overall improvement vs previous SOTA while using fewer tokens; in isolated add/sub runs LEFT used ~160K tokens vs Scratchpad-Detailed 2.9M–3.2M for similar performance), faster convergence in many settings, and fewer intermediate-step errors. For multiplication, LE + step-by-step produced best results; removing LE or step-by-step harms learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (joint training, 5K examples per op): LEFT add ACC=98.8%, sub ACC=95.9%, mul ACC=88.5%, overall ACC=94.4%; token usage for LEFT (joint) = 3,040,616 tokens. Baseline Scratchpad-Detailed: add 99.8%, sub 97.3%, mul 52.8%, overall 83.3%, tokens = 10,995,191. Other reported comparisons: End-To-End and Chain-of-Thought values shown in Table 1; authors state LEFT used only 160K/161K tokens when training addition/subtraction individually in a lower-resource setting versus Scratchpad-Detailed using ~2.9M/3.25M tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Performance drops with increasing input digit length (especially multiplication); multiplication cannot be learned without stepwise decomposition under limited training data; pretraining mismatch (Big-Endian pretraining) can cause early training instability (notably for subtraction) when switching to LE; step-by-step methods introduce opportunities for intermediate-step error propagation (though LE reduces this).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors claim LEFT makes LLM addition behave more like humans by beginning with least-significant digits and recomputing carries similarly to human mental arithmetic; LEFT reduces need for external calculators/tools and outperforms prior step-by-step (symbolic-like) supervised scratchpad approaches in both efficiency and final accuracy on many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reverse That Number! Decoding Order Matters in Arithmetic Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2986.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2986.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 - 13 Billion parameter model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Openly released transformer language model (13B parameters) used as base checkpoint for fine-tuning experiments; context length extended to 4,096 tokens for arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open Foundation and Fine-Tuned Chat Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter transformer (Llama2 family) base checkpoint; in these experiments adapted to longer context (4,096) and fine-tuned on arithmetic datasets with variants (Big-Endian vs Little-Endian, step-by-step vs end-to-end).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition, subtraction, multiplication (5–12 digit inputs in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>As an autoregressive next-token predictor, it tends to predict most-significant digits first under Big-Endian training; this forward-planning limitation leads to carry-related failures when digits are emitted MSB-first. When fine-tuned with LE, the model appears to learn localized per-digit computations and carry reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Performance differences across formatting (Big-Endian vs Little-Endian) and intervention types: Big-Endian end-to-end yields poor accuracy and carry errors; LE fine-tuning yields high accuracy and reduced intermediate errors; attention maps show digit-to-digit correlations and carry signals in particular attention heads/layers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Model pretraining on Big-Endian numerals causes slower initiation of LE training in some tasks (subtraction); some arithmetic (multiplication) still requires explicit stepwise decomposition despite LE formatting, indicating limitations of mere endian change.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning on arithmetic datasets with LE formatting; optional chain-of-thought / scratchpad intermediate supervision for multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large increases in arithmetic accuracy (see LEFT metrics), token-efficient learning; faster convergence in certain LE+step settings (multiplication converged faster when numbers were Big-Endian in stepwise experiments per the paper's ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as the backbone across reported experiments; see LEFT metrics (add 98.8%, sub 95.9%, mul 88.5% in joint LEFT fine-tuning). Baseline big-endian end-to-end results (same backbone) are substantially lower (e.g., End-To-End big-endian add 63.3%, sub 32.3% per Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Pretraining-inherited biases favor Big-Endian representations; limited forward planning causes MSB-first decoding that mismanages carries; scaling to longer digits remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Model behavior under LE more closely parallels human least-significant-first arithmetic; still not a symbolic calculator and benefits from algorithmic decomposition (step-by-step) for multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reverse That Number! Decoding Order Matters in Arithmetic Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2986.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2986.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad-Detailed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpad-Detailed (detailed intermediate supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A detailed scratchpad / step-by-step baseline that provides explicit intermediate computations (intermediate products and cumulative sums) during training to break down arithmetic tasks into substeps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching algorithmic reasoning via in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B (baseline fine-tuned with scratchpad supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same backbone used as comparison, fine-tuned with detailed step-by-step intermediate-token supervision (Big-Endian formatting in experiments reported).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition, subtraction, multiplication (5–12 digit inputs in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Provides explicit intermediate computational states (scratchpad) so model can learn multi-step algorithmic decomposition of arithmetic rather than relying solely on next-token emission of final result.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High accuracy on addition/subtraction (reported add 99.8%, sub 97.3%) when ample token budget is used; demonstrated prior SOTA for some tasks before LEFT.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>High token cost (reported ~10.99M tokens) and poorer transfer to multiplication (reported multiplication ACC 52.8% in Table 1) and high intermediate-step error propagation; empirical error analysis shows many intermediate errors (417 intermediate errors in one analysis) leading to final failures.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised step-by-step training (scratchpad) with auxiliary intermediate tokens representing subcomputations.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Greatly improves correctness for some tasks (addition/subtraction) when trained with large token budgets, but is less token-efficient and more error-prone for accumulation/multiplication compared to LEFT; induces higher opportunity for error propagation across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (Table 1): add ACC=99.8%, sub ACC=97.3%, mul ACC=52.8%, overall ACC=83.3%; token usage reported 10,995,191 tokens in the evaluated run.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Intermediate-step errors (both computing intermediate products and accumulating sums) are common and amplify into final incorrect results; poor token efficiency; struggles in multiplication relative to LEFT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Approach approximates explicit algorithmic steps (closer to symbolic computation) by exposing intermediates, unlike LE which achieves comparable or better final performance with fewer tokens by changing representation rather than providing full explicit computation traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reverse That Number! Decoding Order Matters in Arithmetic Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2986.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2986.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general prompting/training paradigm that elicits multi-step intermediate reasoning tokens to help LLMs solve complex tasks by decomposing them into a chain of sub-reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B (baseline with CoT supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as an intervention to encourage stepwise intermediate explanations; used as a baseline in experiments comparing Big-Endian CoT vs LE settings.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used for multi-digit arithmetic tasks (add, sub, mul) as a stepwise baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Encourages multi-step intermediate generation to reveal latent algorithmic chains, reducing difficulty by decomposing tasks across tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Chain-of-Thought improved performance over naive end-to-end in some cases but underperforms LEFT (CoT big-endian lags behind LE methods), indicating CoT alone does not solve carry-order issues introduced by Big-Endian decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT without LE still shows substantial carry-related errors and is less token-efficient than LEFT; authors report Chain-of-Thought configurations significantly lag LEFT in both accuracy and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting/fine-tuning with intermediate reasoning tokens (chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improves accuracy relative to naive end-to-end training but is outperformed by LE formatting; introduces higher token cost and additional error-propagation opportunities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Per Table 1: Chain-of-Thought big-endian reported add ACC=88.0%, sub ACC=83.5%, mul ACC=0.0? (table shows 0 for multiplication in some baselines), overall and token usage shown in paper (Chain-of-Thought tokens ~4,938,148 in the reported comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Doesn't fix MSB-first decoding carry problems; susceptible to error propagation across steps; less token-efficient than LE-only approach for addition/subtraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>CoT resembles showing work (human-like intermediate reasoning) but in these experiments was less effective than changing numeral representation to LE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reverse That Number! Decoding Order Matters in Arithmetic Learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Teaching algorithmic reasoning via in-context learning <em>(Rating: 2)</em></li>
                <li>GPT can solve mathematical problems without a calculator <em>(Rating: 1)</em></li>
                <li>Length generalization in arithmetic transformers <em>(Rating: 1)</em></li>
                <li>Limitations of language models in arithmetic and symbolic induction <em>(Rating: 1)</em></li>
                <li>Evaluating transformer language models on arithmetic operations using number decomposition <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2986",
    "paper_id": "paper-268358225",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "LEFT",
            "name_full": "Little-Endian Fine-Tuning (LEFT)",
            "brief_description": "A fine-tuning method that reverses digit order (Little-Endian) so models emit least-significant digits first, reducing per-digit input context and learning complexity; applied with/without step-by-step for different operations and shown to improve accuracy and token efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-13B (fine-tuned)",
            "model_description": "Llama2-13B base checkpoint (13B parameter transformer) extended to 4,096 token context length and fine-tuned on arithmetic datasets using the LEFT formatting and optional step-by-step intermediates.",
            "arithmetic_task_type": "Multi-digit addition, subtraction, and multiplication (input digits balanced in range 5–12; train/test splits: 5K train per op, 1K test per op; experiments include joint and per-operation training).",
            "reported_mechanism": "Reversing output/input digit order (Little-Endian) reduces conditional input complexity per output digit, enabling local carry reconstruction (model recomputes carry from previous least-significant digits) and thereby simplifying arithmetic into primarily local digit computations; multiplication still benefits from stepwise decomposition of digit-wise products but performed in LE order.",
            "evidence_for_mechanism": "Empirical gains: LEFT yields +11.1% overall accuracy vs prior SOTA in joint training and +35.7% on multiplication; large reductions in training tokens required (claims: LEFT uses 5.2% of prior SOTA tokens for add/sub in some settings; joint training token usage reported 3,040,616 vs Scratchpad-Detailed 10,995,191). Interpretability: attention visualizations show specific heads correlate output digits to relevant input digits and a deeper layer (layer 22) showing signals consistent with carry reconstruction. Error analysis: Scratchpad-Detailed had 417 intermediate errors (140 intermediate-product, 236 accumulate-sum) vs LEFT having 99 intermediate errors (77 intermediate-product, 22 accumulate-sum), indicating reduced error propagation when using LE.",
            "evidence_against_mechanism": "LEFT's advantages are not universal: multiplication cannot be learned without step-by-step decomposition when only 5K training cases are available (i.e., LE alone fails for multiplication in low-data regime). Pretraining bias: models pretrained on standard (Big-Endian) numerals show slower/fragile learning for subtraction in LE without step-by-step, indicating mismatch between pretraining numeral order and LEFT can hurt initial learning. Performance degrades with larger input digit lengths, especially for multiplication.",
            "intervention_type": "Fine-tuning with Little-Endian input/output formatting; optional addition of step-by-step (scratchpad / chain-of-thought style) intermediate tokens for operations that need decomposition (multiplication).",
            "effect_of_intervention": "Improved accuracy (per-table: addition 98.8%, subtraction 95.9%, multiplication 88.5%, overall 94.4% in joint eval), substantial token-efficiency gains (reported 11.1% overall improvement vs previous SOTA while using fewer tokens; in isolated add/sub runs LEFT used ~160K tokens vs Scratchpad-Detailed 2.9M–3.2M for similar performance), faster convergence in many settings, and fewer intermediate-step errors. For multiplication, LE + step-by-step produced best results; removing LE or step-by-step harms learning.",
            "performance_metrics": "Reported (joint training, 5K examples per op): LEFT add ACC=98.8%, sub ACC=95.9%, mul ACC=88.5%, overall ACC=94.4%; token usage for LEFT (joint) = 3,040,616 tokens. Baseline Scratchpad-Detailed: add 99.8%, sub 97.3%, mul 52.8%, overall 83.3%, tokens = 10,995,191. Other reported comparisons: End-To-End and Chain-of-Thought values shown in Table 1; authors state LEFT used only 160K/161K tokens when training addition/subtraction individually in a lower-resource setting versus Scratchpad-Detailed using ~2.9M/3.25M tokens.",
            "notable_failure_modes": "Performance drops with increasing input digit length (especially multiplication); multiplication cannot be learned without stepwise decomposition under limited training data; pretraining mismatch (Big-Endian pretraining) can cause early training instability (notably for subtraction) when switching to LE; step-by-step methods introduce opportunities for intermediate-step error propagation (though LE reduces this).",
            "comparison_to_humans_or_symbolic": "Authors claim LEFT makes LLM addition behave more like humans by beginning with least-significant digits and recomputing carries similarly to human mental arithmetic; LEFT reduces need for external calculators/tools and outperforms prior step-by-step (symbolic-like) supervised scratchpad approaches in both efficiency and final accuracy on many settings.",
            "uuid": "e2986.0",
            "source_info": {
                "paper_title": "Reverse That Number! Decoding Order Matters in Arithmetic Learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Llama2-13B",
            "name_full": "Llama 2 - 13 Billion parameter model",
            "brief_description": "Openly released transformer language model (13B parameters) used as base checkpoint for fine-tuning experiments; context length extended to 4,096 tokens for arithmetic tasks.",
            "citation_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "mention_or_use": "use",
            "model_name": "Llama2-13B",
            "model_description": "13B-parameter transformer (Llama2 family) base checkpoint; in these experiments adapted to longer context (4,096) and fine-tuned on arithmetic datasets with variants (Big-Endian vs Little-Endian, step-by-step vs end-to-end).",
            "arithmetic_task_type": "Multi-digit addition, subtraction, multiplication (5–12 digit inputs in experiments).",
            "reported_mechanism": "As an autoregressive next-token predictor, it tends to predict most-significant digits first under Big-Endian training; this forward-planning limitation leads to carry-related failures when digits are emitted MSB-first. When fine-tuned with LE, the model appears to learn localized per-digit computations and carry reconstruction.",
            "evidence_for_mechanism": "Performance differences across formatting (Big-Endian vs Little-Endian) and intervention types: Big-Endian end-to-end yields poor accuracy and carry errors; LE fine-tuning yields high accuracy and reduced intermediate errors; attention maps show digit-to-digit correlations and carry signals in particular attention heads/layers.",
            "evidence_against_mechanism": "Model pretraining on Big-Endian numerals causes slower initiation of LE training in some tasks (subtraction); some arithmetic (multiplication) still requires explicit stepwise decomposition despite LE formatting, indicating limitations of mere endian change.",
            "intervention_type": "Fine-tuning on arithmetic datasets with LE formatting; optional chain-of-thought / scratchpad intermediate supervision for multiplication.",
            "effect_of_intervention": "Large increases in arithmetic accuracy (see LEFT metrics), token-efficient learning; faster convergence in certain LE+step settings (multiplication converged faster when numbers were Big-Endian in stepwise experiments per the paper's ablations).",
            "performance_metrics": "Used as the backbone across reported experiments; see LEFT metrics (add 98.8%, sub 95.9%, mul 88.5% in joint LEFT fine-tuning). Baseline big-endian end-to-end results (same backbone) are substantially lower (e.g., End-To-End big-endian add 63.3%, sub 32.3% per Table 1).",
            "notable_failure_modes": "Pretraining-inherited biases favor Big-Endian representations; limited forward planning causes MSB-first decoding that mismanages carries; scaling to longer digits remains challenging.",
            "comparison_to_humans_or_symbolic": "Model behavior under LE more closely parallels human least-significant-first arithmetic; still not a symbolic calculator and benefits from algorithmic decomposition (step-by-step) for multiplication.",
            "uuid": "e2986.1",
            "source_info": {
                "paper_title": "Reverse That Number! Decoding Order Matters in Arithmetic Learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Scratchpad-Detailed",
            "name_full": "Scratchpad-Detailed (detailed intermediate supervision)",
            "brief_description": "A detailed scratchpad / step-by-step baseline that provides explicit intermediate computations (intermediate products and cumulative sums) during training to break down arithmetic tasks into substeps.",
            "citation_title": "Teaching algorithmic reasoning via in-context learning",
            "mention_or_use": "use",
            "model_name": "Llama2-13B (baseline fine-tuned with scratchpad supervision)",
            "model_description": "Same backbone used as comparison, fine-tuned with detailed step-by-step intermediate-token supervision (Big-Endian formatting in experiments reported).",
            "arithmetic_task_type": "Multi-digit addition, subtraction, multiplication (5–12 digit inputs in comparisons).",
            "reported_mechanism": "Provides explicit intermediate computational states (scratchpad) so model can learn multi-step algorithmic decomposition of arithmetic rather than relying solely on next-token emission of final result.",
            "evidence_for_mechanism": "High accuracy on addition/subtraction (reported add 99.8%, sub 97.3%) when ample token budget is used; demonstrated prior SOTA for some tasks before LEFT.",
            "evidence_against_mechanism": "High token cost (reported ~10.99M tokens) and poorer transfer to multiplication (reported multiplication ACC 52.8% in Table 1) and high intermediate-step error propagation; empirical error analysis shows many intermediate errors (417 intermediate errors in one analysis) leading to final failures.",
            "intervention_type": "Supervised step-by-step training (scratchpad) with auxiliary intermediate tokens representing subcomputations.",
            "effect_of_intervention": "Greatly improves correctness for some tasks (addition/subtraction) when trained with large token budgets, but is less token-efficient and more error-prone for accumulation/multiplication compared to LEFT; induces higher opportunity for error propagation across steps.",
            "performance_metrics": "Reported (Table 1): add ACC=99.8%, sub ACC=97.3%, mul ACC=52.8%, overall ACC=83.3%; token usage reported 10,995,191 tokens in the evaluated run.",
            "notable_failure_modes": "Intermediate-step errors (both computing intermediate products and accumulating sums) are common and amplify into final incorrect results; poor token efficiency; struggles in multiplication relative to LEFT.",
            "comparison_to_humans_or_symbolic": "Approach approximates explicit algorithmic steps (closer to symbolic computation) by exposing intermediates, unlike LE which achieves comparable or better final performance with fewer tokens by changing representation rather than providing full explicit computation traces.",
            "uuid": "e2986.2",
            "source_info": {
                "paper_title": "Reverse That Number! Decoding Order Matters in Arithmetic Learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A general prompting/training paradigm that elicits multi-step intermediate reasoning tokens to help LLMs solve complex tasks by decomposing them into a chain of sub-reasoning steps.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Llama2-13B (baseline with CoT supervision)",
            "model_description": "Applied as an intervention to encourage stepwise intermediate explanations; used as a baseline in experiments comparing Big-Endian CoT vs LE settings.",
            "arithmetic_task_type": "Used for multi-digit arithmetic tasks (add, sub, mul) as a stepwise baseline.",
            "reported_mechanism": "Encourages multi-step intermediate generation to reveal latent algorithmic chains, reducing difficulty by decomposing tasks across tokens.",
            "evidence_for_mechanism": "Chain-of-Thought improved performance over naive end-to-end in some cases but underperforms LEFT (CoT big-endian lags behind LE methods), indicating CoT alone does not solve carry-order issues introduced by Big-Endian decoding.",
            "evidence_against_mechanism": "CoT without LE still shows substantial carry-related errors and is less token-efficient than LEFT; authors report Chain-of-Thought configurations significantly lag LEFT in both accuracy and efficiency.",
            "intervention_type": "Prompting/fine-tuning with intermediate reasoning tokens (chain-of-thought).",
            "effect_of_intervention": "Improves accuracy relative to naive end-to-end training but is outperformed by LE formatting; introduces higher token cost and additional error-propagation opportunities.",
            "performance_metrics": "Per Table 1: Chain-of-Thought big-endian reported add ACC=88.0%, sub ACC=83.5%, mul ACC=0.0? (table shows 0 for multiplication in some baselines), overall and token usage shown in paper (Chain-of-Thought tokens ~4,938,148 in the reported comparison).",
            "notable_failure_modes": "Doesn't fix MSB-first decoding carry problems; susceptible to error propagation across steps; less token-efficient than LE-only approach for addition/subtraction.",
            "comparison_to_humans_or_symbolic": "CoT resembles showing work (human-like intermediate reasoning) but in these experiments was less effective than changing numeral representation to LE.",
            "uuid": "e2986.3",
            "source_info": {
                "paper_title": "Reverse That Number! Decoding Order Matters in Arithmetic Learning",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Teaching algorithmic reasoning via in-context learning",
            "rating": 2,
            "sanitized_title": "teaching_algorithmic_reasoning_via_incontext_learning"
        },
        {
            "paper_title": "GPT can solve mathematical problems without a calculator",
            "rating": 1,
            "sanitized_title": "gpt_can_solve_mathematical_problems_without_a_calculator"
        },
        {
            "paper_title": "Length generalization in arithmetic transformers",
            "rating": 1,
            "sanitized_title": "length_generalization_in_arithmetic_transformers"
        },
        {
            "paper_title": "Limitations of language models in arithmetic and symbolic induction",
            "rating": 1,
            "sanitized_title": "limitations_of_language_models_in_arithmetic_and_symbolic_induction"
        },
        {
            "paper_title": "Evaluating transformer language models on arithmetic operations using number decomposition",
            "rating": 1,
            "sanitized_title": "evaluating_transformer_language_models_on_arithmetic_operations_using_number_decomposition"
        }
    ],
    "cost": 0.01196975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reverse That Number! Decoding Order Matters in Arithmetic Learning
9 Mar 2024</p>
<p>Daniel Zhang-Li 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Nianyi Lin 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Jifan Yu 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Zheyuan Zhang 
Zijun Yao 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Xiaokang Zhang 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Renmin University of China</p>
<p>Lei Hou 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Jing Zhang 
Renmin University of China</p>
<p>Juanzi Li lijuanzi@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Reverse That Number! Decoding Order Matters in Arithmetic Learning
9 Mar 2024B72A34C11CD2372462EAD955A05F8600arXiv:2403.05845v1[cs.CL]
Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations.However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, stepby-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step.Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity.We have developed and applied this method in a comprehensive set of experiments.Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of 11.1% in accuracy while requiring only a third of the tokens typically used during training.For the purpose of facilitating replication and further research, we have made our code and dataset publicly available at https:// anonymous.4open.science/r/RAIT-9FB7/.</p>
<p>Introduction</p>
<p>Large language models (LLMs), though proficient in a range of tasks (Ouyang et al., 2022;Achiam et al., 2023;Anil et al., 2023), encounter challenges in arithmetic operations due to their inherent design limitations, such as reliance on next-token prediction methods and limited working memory (Bubeck et al., 2023).Despite their capability to utilize external tools for circumventing direct arithmetic computations during inference (Gao et al., 2023;Imani et al., 2023;Schick et al., 2023), efficiently and effectively incorporating arithmetic proficiency within LLMs is an unresolved issue.However, previous studies have demonstrated that LLMs can learn arithmetic effectively through pretraining (Yang et al., 2023).This suggests that it might be feasible to efficiently teach LLMs arithmetic operations through fine-tuning alone, without the need external tool such as calculators.</p>
<p>The prevailing challenge in employing Large Language Models for arithmetic tasks is intricately linked to their next-token prediction mechanism.This mechanism often leads to a reversed computation order, where more significant digits are calculated before less significant ones, a flaw attributed to LLMs' inherent limitation in forward planning (Bubeck et al., 2023).This characteristic has led to the perception that arithmetic in LLMs is akin to other complex symbolic and logical tasks, necessitating a similar approach (Nye et al., 2021).Consequently, prior research has predominantly focused on the necessity of a step-by-step methodology, breaking down arithmetic into a series of sub-steps, as a critical strategy for addressing these challenges (Wei et al., 2022;Lee et al., 2023).</p>
<p>Such a technique achieves significant gains in performance but introduces a trade-off between efficiency and effectiveness, necessitating a balance between the number of tokens per training case and
* b 0 b 1 b 2 b 3 b 4 = u 0 u 1 u 2 u 3 u 4 u 5 = 4 0 3 6 6 3</p>
<p>Print Order</p>
<p>Decoding Order
8 * 4 5 7 8 8 a 4 * b 4 b 3 b 2 b 1 b 0 = u 5 u 4 u 3 u 2 u 1 u 0 = 3 6 6 3 0 4
Figure 2: Example training data for Multiplication.Where the task is solved using a step-by-step process.During the ith intermediate step, the intermediate product is first computed.Then, inspired by the human process, we set the least significant digits(U high ) unchanged and directly added the product to the remaining digits(U low ) of the cumulative sum.Finally, we pop the least significant digit from the updated U high and append it into U low as it will not be added with non-zero digits in later steps.During decoding, we express all numbers in Little-Endian, where the least significant digit goes first.We convert all the numbers back to Big-Endian before printing.the total number of training cases.To enhance both efficiency and effectiveness without resorting to a brute-force integration of step-by-step processes, we adopt a novel approach termed LEFT (Little-Endian Fine-Tuning).Rather than incrementally integrating step-by-step mechanisms, we employ a strategy that reverses the number representation, prioritizing the computation of less significant digits.This approach utilizes the concept of Little-Endian, where numbers are represented with the least significant digits first, while maintaining the position of any negative signs.In contrast, the standard numeral representation is referred to as Big-Endian.Figure 1 demonstrates that initiating output generation with the most significant digit may result in carry-related errors.In contrast, employing a Little-Endian format, where the model produces the number 100863 as 368001, simplifies carry operations resulting in a correct solution.We present experimental results (Sec.5) showcasing that LEFT not only improves accuracy by 11.1% against the current state-of-the-art (SOTA) for large digit inputs but also demonstrates efficiency by utilizing just 5.2% of the training tokens required by the previous SOTA for addition and subtraction tasks.Specifically, in multiplication, LEFT records a 35.7% performance gain while consuming only 56.6% of the training tokens used by prior SOTA.</p>
<p>The key contributions of this paper include:</p>
<p>• We proposed a novel method, LEFT, leverag-ing Little-Endian to reduce the complexity of learning arithmetic operations.</p>
<p>• We conduct detailed evaluation and demonstrate LEFT achieves better performance with lesser token used during training.</p>
<p>• Observations from our experiments indicate that, by reversing digit order, LLMs are capable of solving addition in human alike manner.</p>
<p>Problem Formulation</p>
<p>Consider the simple case where the input (I) consists of two numbers, A and B, combined with an operator op.We denote the digits of A as A = m−1 i=0 10 i a i , where each a i is a single-digit integer (0 ≤ a i ≤ 9), and a m−1 ̸ = 0 to ensure no leading zeros.Similarly, for B, we express its digits as B = n−1 i=0 10 i b i , where each b i is a single-digit integer (0 ≤ b i ≤ 9), and b n−1 ̸ = 0.</p>
<p>We assume the ground truth output is a k-digit number, C = k−1 i=0 10 i c i (for C &lt; 0, we usec −1 to represent the negative sign).The trained LLM outputs an ordered sequence O = {o 1 , o 2 , . ..}, which includes the output number C ⊆ O.</p>
<p>As step-by-step designs often incorporate intermediate results, we denote the ith intermediate result as U i .Finally, we define the remaining output as auxiliary tokens (X = O \ {U i | ∀i} ∪ {C}).</p>
<p>Little-Endian Fine-Tuning</p>
<p>In order to effectively and efficiently teach LLMs arithmetic, we need to address three crucial questions: 1.What is the complexity in standard Big-Endian training(where no step-by-step is applied)?2. Are there spaces for optimizing the standard method?3. How to optimize cases when stepby-step is required?In the remaining parts of this section, we tackle such questions one by one.</p>
<p>Learning Complexity of Arithmetic</p>
<p>Autoregressive LLMs are interpreted as probabilistic models that predict output sequences by maximizing the likelihood of generating the correct output.In operations such as addition, this process of prediction can be formalized as follows:
arg max c i P (c i |a 0∼n−1 , b 0∼m−1 , c i+1∼k )
Considering the specific nature of addition, where the outcome of each digit is influenced only by digits of equal or lesser significance, the process is refined to concentrate on pertinent inputs:
arg max c i P (c i |a 0∼i , b 0∼i )(1)
Assuming that all numbers involved possess an identical number of digits simplifies the analysis.Under this assumption, during the generation of each digit, there exist 10 potential inputs from each of the two numbers, resulting in 10 2i+2 possible input combinations.Given that the output digit can assume 10 possible values, the complexity of predicting a single digit's value transitions from 10 2i+2 input conditions to 10 output conditions.</p>
<p>The overall learning complexity is quantified by summing the probabilities of accurately predicting each digit, based on the inputs up to that digit:
L Big = − n i=0 log P (c i |a 0∼i , b 0∼i ) (2)
Accordingly, the cumulative learning complexity, denoted as C Big , is conceptualized as the aggregate of complexities across all digits, with the input variations providing a lower bound:
C Big = n i=0 10 2i+2 ≥ 10 2n+2
(3)</p>
<p>This model illustrates the exponential increase in learning complexity with the increment of digit count n, presenting a significant scalability challenge in teaching arithmetic to LLMs.</p>
<p>Optimizing Complexity via Little-Endian</p>
<p>In addressing the complexity of arithmetic operations, it is noted that the output token with the greatest complexity is typically the most significant digit.Interestingly, unlike computational models, humans often do not consider all input digits simultaneously.Instead, they start from the least significant digit, using any carry-over to simplify the computation.Assuming the model can similarly infer the carry from the previous digit (a i−1 , b i−1 , c i−1 ), we can streamline the optimization target by focusing on this simplified context:
arg max c i P (c i |a i , a i−1 , b i , b i−1 , c i−1 )
Such adjustment leads to a significant reduction in input complexity, now quantified as 10 5 .By adopting this revised generating order, the task becomes markedly less challenging:
C Little = n i=0 10 5 ≤ n • 10 5
For cases where n ≥ 2, this model showcases a substantial decrease in learning complexity compared to the conventional approach (C Little ≤ n • 10 5 &lt; 10 2n+2 ≤ C Big ).Such findings illuminate the potential benefits of inverting the decoding order to mitigate complexity.Motivated by this insight, we propose abandoning the classic, step-by-step design prevalent in previous methodologies in favor of revising addition and subtraction training to leverage this more efficient strategy.</p>
<p>Addition.In addressing addition within LEFT, the traditional approach of processing numbers from the most significant digit to the least significant is reimagined.By reversing both the input and output numbers, the calculation aligns with the Little-Endian format, where operations commence from the least significant digit and progress towards the most significant.Such conversion simplifies the decoding order, making it more intuitive and akin to human arithmetic practices.We hypothesized that the model can autonomously recompute the necessary carry for the subsequent significant digit.This method eliminates the need for a step-by-step design or the introduction of auxiliary tokens, streamlining the addition process without necessitating any extra tokens beyond the sum itself.</p>
<p>Subtraction.</p>
<p>For subtraction, the model simplifies the process by first determining if the result will be negative, then applying the operation in Little-Endian order.This approach, which keeps the negative sign's position unchanged (e.g., -256 becomes -652), enhances efficiency by eliminating the need for intermediate results that assume a non-negative outcome.This streamlined method contrasts with traditional digit-wise subtraction, offering a more straightforward computation strategy.</p>
<p>Augmenting Step-by-Step</p>
<p>The application of Little-Endian formatting extends beyond the realms of addition and subtraction, offering substantial benefits in operations that inherently require a step-by-step approach due to their complexity.One prime example of such an operation is multiplication, where the intricacies of the computation process are significantly amplified.</p>
<p>Multiplication.Traditional methods often involve breaking down the solving process into manageable chunks, typically computing the product of a single digit with a multi-digit number, and then summing these intermediate products.This conventional approach, however, often operates under the Big-Endian framework, starting with the most significant digits and potentially complicating the computation of intermediate products.</p>
<p>In contrast, the use of Little-Endian proposes a significant optimization.By reversing the order of digits-starting from the least significant-this method aligns with the natural flow of human computation, simplifying both the computation of intermediate product and subsequent sums.</p>
<p>Implementation</p>
<p>In this section, we delve into the detailed implementation of LEFT and explore the methodologies applied in our experiments, along with the baselines for comparison.Our discussion spans from the step-by-step design utilized in the experiments (Sec.4.1) to dataset generation (Sec.4.2) and other settings for the experiments(Sec.4.3).</p>
<p>Step-By-Step Design</p>
<p>Addition/Subtraction. While our hypothesis posits that the step-by-step process might not be essential for efficiently learning addition and subtraction, we incorporate it as a comparative measure to validate our assumption.We adopt the step-bystep design from the chain-of-thought methodology (Wei et al., 2022), as reproduced in previous studies (Zhou et al., 2022), for LEFT's addition and subtraction tasks when necessary for evaluation.</p>
<p>Addition/Subtraction. Contrary to our initial hypothesis that a step-by-step process may not be crucial for efficiently mastering addition and subtraction, we included it for comparative analysis to test our theory.Thus, we utilized the Chain-Of-Thought approach (Wei et al., 2022), as previously replicated (Zhou et al., 2022), in evaluating LEFT joined with step-by-step on addition/subtraction.</p>
<p>Multiplication.We previously outlined the key features of the step-by-step approach for multiplication within LEFT, yet a direct implementation was not provided.As shown in Figure 2, with the reversal of all numbers, the task is divided into numerous substeps.Each substep iterates over the digits of the first input number, a i ∈ A, starting from the least significant digit.In each iteration, the process begins by multiplying the current digit with the second input number to generate an intermediate product.This intermediate product is then added to the cumulative sum of products from previous iterations.Since the lower i digits of the product are always zero, these are not explicitly represented; instead, the product is directly added to the higher section of the cumulative sum.The higher section is defined as the part of the cumulative sum obtained in the last step of the previous iteration, which considers the lower i-digits as a fixed result and defines the remaining digits as the higher section of the cumulative sum.</p>
<p>This refined step-by-step design for multiplication highlights the efficiency and adaptability of the Little-Endian approach in managing complex arithmetic operations.By streamlining the integration of intermediate products into a simplified cumulative sum, this method not only improves the performance and clarity of the model but also showcases the extensive utility of Little-Endian formatting in enhancing computational processes.</p>
<p>Dataset</p>
<p>The inherent characteristics of arithmetic calculations, which do not necessitate human-generated labels, enable the automated generation of training and testing sets in our study.Our primary objective is to create a dataset that is fair, isolated, and balanced, facilitating a comprehensive evaluation of the LEFT's effectiveness and efficiency.</p>
<p>Fairness.Given that different methods may operate on varied data inputs, we aim to minimize the variance in performance attributable to different inputs as much as possible.To achieve this, we initiate the process by generating a set of meta data during the data generation phase.Each piece of meta data is conceptualized as a triplet in the form (A, op, B).This triplet serves as a unified seed for generating training and testing data for each method, ensuring that the same set of input is utilized across methods.Then, each triplet is expanded and formatted to suit the specific requirements of each method's data format.</p>
<p>Isolation.Recognizing the critical importance of preventing data leakage, we take meticulous steps to ensure the uniqueness of input number sets, denoted by {A, B}.This strategy guarantees that the test set contains no identical input number pairs as found in the training set, thereby also ensuring the uniqueness of each training and testing set.</p>
<p>Digit Distribution Balancing.Echoing previous methods that have highlighted the importance of balanced data distribution (Lee et al., 2023), we ensure that both the training and test sets are balanced such that the maximum quantity of any single number in each data slice falls within the digit range of [5,12].Specifically, we generate in total of 15K training data and 3K test data, with 5K points for each operation, accompanied by 1K test data points for each operation, to maintain this balance.</p>
<p>Experiment Setup</p>
<p>Baseline.We first include End-To-End training used in during pretraining methods (Yang et al., 2023) as a ground to compare performance in previous methods.We then include Scratchpad (Nye et al., 2021), one of the early founders in using step-by-step approaches to break down arithmetic into multiple steps.We also include Chain-Of-Thought (Wei et al., 2022) which provided a general approach of breaking step-by-step to a wide range of complex tasks.In addition, we include the Detailed-Scratchpad method introduced in (Zhou et al., 2022).(Zhou et al., 2022) also introduces Algorithmic-Prompting technique but as it requires too many auxiliary tokens making it hard to fit 12digit training into the context length.As a result, we exclude it during our evaluation.</p>
<p>Metric.As arithmetic reasoning is strongly affected by error propagation, solutions with interme-diate errors are almost impossible to provide the correct solution.As a result, we directly use the accuracy (ACC) of the predicted output to evaluate the effectiveness of the methods.As the discussion for efficiency is aimed at training betterperformed models using fewer resources, we record the amount of tokens used for training and observe the change in accuracy as more tokens are used.</p>
<p>Backbone Model.The base checkpoint for our experimental framework is Llama2-13B (Touvron et al., 2023), chosen for its status as a well-regarded and openly accessible LLM.To address the need for processing longer sequences, the model's context length has been extended to 4, 096 tokens.</p>
<p>Experiments</p>
<p>We now turn to a systematic evaluation of the proposed method.Specifically, we design and conduct a series of comprehensive analysis which seeks to answer the following research questions:</p>
<p>Q1 Is LEFT effective and efficient?(Sec.5.1) Q2 What grants LEFT the ability to effectively tackles the provided task?(Sec.5.2) Q3 What can be further done on LEFT?(Sec.5.3)</p>
<p>Direct Evaluation Over Performance</p>
<p>We began our analysis with the overall performance of LEFT against previous methods for jointly trained and evaluated addition, subtraction, and multiplication performance.We then conduct operation-by-operation analysis to observe the results of training when jointly training is opt-out.</p>
<p>Observation 1: LEFT Learns Faster Than Baselines.Table 1 shows the resulting performance of each method after training.We order the baselines according to token used during training.LEFT used the least amount of training token among all the step-by-step methods, yet achieving 11.1% performance improvement over previous SOTA.</p>
<p>Specifically, LEFT's accuracy on addition and subtraction is slightly below Scratchpad-Detailed.However, LEFT only used 160K and 161K tokens for learning addition and subtraction.But Scratchpad-Detailed used 2, 936K and 3, 254K for training.This means LEFT uses only 1/20 of training data yet still achieves similar performance.LEFT also achieved 35.7% accuracy improvement over previous SOTA on multiplication, further highlighting LEFT's effectiveness and efficiency.As shown in Figure 3, we observe that the use of Little-Endian outperforms other settings in both operations, despite the use of fewer tokens when compared to the step-by-step settings.</p>
<p>Moreover, we observe that the conventional Chain-Of-Thought approach, which does not incorporate Little-Endian formatting, also significantly lags behind the LEFT configuration.This outcome suggests that employing a step-by-step methodology does not invariably enhance performance.Particularly in addition, both the presence and absence of Little-Endian in the settings lead to inferior results compared to employing Little-Endian without a step-by-step approach.This implies that reversing the endian inherently captures critical information, which the step-by-step process aimed to convey in digit generation.Consequently, not only does the step-by-step application decrease efficiency, but it also deteriorates model performance by introducing additional chance of error propagation.</p>
<p>On the other hand, by taking a closer observation of subtraction, we see whether the use of step-bystep is integrated or not, the integration of Little-Endian brings much better performance.However, the learning curve of Little-Endian without step-by- step is smoother than in addition.We believe this could be related to the pretraining setting, where the model is trained with Big-Endian.On addition, when the carry is not occurring, knowing what endian is involved doesn't have a strong effect on the result, the model could falsely interpret the task as aligning the numbers with the leftmost digit and still achieve some level of performance.However, on subtraction, the endian greatly affects the result, as whether the result is negative is affected by the most significant digit, which is strongly related to the endian.Such difference resulted in poor performance in the beginning, as the model will have a great chance of failing unless it actually understands the task.But it also brings faster learning as the chance for the model to falsely understand the task reduces.We believe such case highlights that the arithmetic ability of a fine-tuned model could be further improved with a backbone model that is pretrained with Little-Endian representation.</p>
<p>Observation 3: Little-Endian And Step-by-Step Are Both Crucial For Multiplication.We now conduct a detailed examination for multiplication.We re-evaluate our backbone model to examine our designs on multuplication.For better comparison,  we include two additional settings other than the standard End-To-End.We first include a similar design as we proposed for solving addition and subtraction, where the model directly outputs the result but the input and output are both in Little-Endian.</p>
<p>We then include LEFT's step-by-step design but convert the numbers into Big-Endian.We also measure the different performances after different epochs of training to observe the convergence for the same amount of training cases.</p>
<p>The results are shown in Table 2.We first observe that when the use of step-by-step is removed, it becomes impossible to learn multiplication.This demonstrates the need for step-by-step to break down the complexity in solving multiplication is still needed when only 5K of training data is available.We also observe that when Little-Endian is removed, the performance further improves over the step-by-step setting.The model also converges much faster, as the performance after 2 epochs of training is already close to the performance of the last epoch, an accuracy of 91.6%.We are amazed that LEFT achieves better performance when the model is trained only on multiplication, suggesting the potential for further optimization.</p>
<p>We also observe the number of tokens used during LEFT's training in multiplication is approximately half of the tokens used by Scratchpad-Detailed.In addition and subtraction training, tokens are better off with a factor of 20.This shows that LEFT with better performance achieves even greater improvement in token efficiency.</p>
<p>Case Studies</p>
<p>We now conduct a detailed study of the results obtained in the previous section, seeking to discover findings that can help future studies.</p>
<p>Finding 1: Little-Endian Reduces Step-By-Step Errors.In this section, we conduct an error anal-ysis for the errors in our main experiment in order to find an explanation of the performance gain caused by changing the endian.To do so, we first selected the place where the first error occurred as an indication of the error of each falsely inferred test case.This is because error propagation is critical in arithmetic.We then focused on two crucial parts during each inference step, calculating the intermediate 1-by-n product and the cumulative sum.As a result, we find that among the 417 errors that occurred during intermediate calculations in Scratchpad-Detailed: 1. 140 errors occurred during calculating the intermediate product; 2. 236 errors occurred during accumulating sum.Both operations had much better performance in LEFT, where only 77 errors were observed during computing the intermediate product and only 22 errors were observed when updating the cumulative sum.The error occurrence is decreased by a factor of 10 for summation and by a factor of 2 for the intermediate product.We believe this is because the carry is easier than to compute when the less significant digits are already shown, which possibly could reduce the complexity in computing the result for the current digit.The error for the intermediate sum is reduced by a greater factor as the addition training is transferable when accumulating sum on LEFT, whereas in Scratchpad-Detailed, the addition task stands more on its own.Despite slightly better performing while evaluated on addition, it cannot transfer its ability to other tasks like multiplication.</p>
<p>Finding 2: LEFT Conducts Addition Just Like Humans We now take a closer observation of how LEFT conducts addition.By logging the attention (Vaswani et al., 2017) scores in the model, we observe a correlation between the output digit and related digits from the input numbers, as shown in Figure 4. We observe that the input digits are recognized when computing the corresponding output during generation in some attention heads.We also observed that, in the 22th layer, shown traits suggest the fine-tuned LLM has learned to re-compute the carry from the previous digits.Adressing our hypothesized during the method design, this proofs the assumption that the model can recover the carry when it's used (Sec.3.2).This is a interesting indication because it suggests Little-Endian might be conducting training in a manner similar to how humans conduct addition without a draft paper.Table 3: Accuracy trends with increasing max input digits.We observe a steeper decline in multiplication's performance compared to other operations.</p>
<p>Additional Error Analysis</p>
<p>Finally, we look at the errors occurred in LEFT's joint experiment in the perspective of different maximum amount of input digits.As shown in Table 3, LEFT is able to perform well in lower digits, but when it is challenged towards higher digits of inputs, it loses part of its performance.Such a drop in performance is mostly significant when it comes to higher-digit multiplications, the digits being operated become much more complicated comparing to addition and subtraction.This stated that, despite well in performance, LEFT still faces challenges when inputted with larger digits, highlighting the need for future studies to not only focus on effectiveness and efficiency but also continue to narrow the gap for the LLMs' inability to scale towards larger inputs and the amazing capability in humans.</p>
<p>Related Works</p>
<p>Conclusion</p>
<p>In this study, we introduced a novel approach for teaching arithmetic to LLMs by reversing the number order to emphasize the least significant digit.This strategy, which aligns with human arithmetic practices, significantly reduces computational complexity and training data requirements, demonstrating an 11.1% increase in overall accuracy over previous SOTA and showcasing efficiency in token usage during training.The success of our method suggests the potential for broader applications in mathematical problem-solving and in environments with limited resources.We hope this study of ours paves the way for future investigations into optimizing LLM training techniques for numerical reasoning and arithmetic precision.</p>
<p>Limitations</p>
<p>Our study introduces a novel approach to arithmetic learning in LLMs but is not without limitations.Firstly, our focus on basic arithmetic operations such as addition, subtraction, and multiplication leaves unexplored territories in more complex arithmetic and mathematical problem-solving areas.Secondly, the generalizability of our method to domains beyond arithmetic is yet to be determined.A critical consideration is the reliance on LLMs pretrained with standard numeral expressions; our experiments did not explore the potential benefits of pretraining models directly with reversed numeral expressions.Addressing these limitations could further enhance the applicability and efficiency of LLMs in numerical reasoning and arithmetic precision, suggesting a promising direction for future research to broaden the scope of operations covered and to investigate the impact of pretraining strategies.</p>
<p>Ethics Statement</p>
<p>Our research contributes to the field of artificial intelligence by proposing an innovative approach to improve the efficiency and accuracy of LLMs in performing arithmetic operations.This advancement has the potential to positively impact areas where numerical understanding is crucial, including but not limited to, educational technologies, data analysis, and automated reasoning systems.By improving the capability of LLMs to process and understand arithmetic, our work aims to support further developments in technology that can assist in educational settings, enhance scientific research, and provide more reliable computational tools for industries relying on accurate numerical data processing.</p>
<p>We are mindful of the importance of conducting our research with a commitment to ethical principles, ensuring that our methodologies and results are transparent, reproducible, and contribute constructively to the academic community and society at large.While our work primarily focuses on the technical aspects of improving LLMs' arithmetic abilities, we recognize the broader implications of AI and machine learning advancements.Therefore, we encourage the responsible use and continuous ethical evaluation of AI technologies, emphasizing the importance of using such advancements to foster positive societal outcomes.</p>
<p>Figure 1 :
1
Figure 1: Reversing the numbers in training enables models to better learn to do arithmetic operations.</p>
<p>Figure 3 :
3
Figure 3: Performance when integrating step-by-step.BE stands for Big-Endian and LE stands for Little-Endian.The graph on the left shows the results after training on addition.The the right figure shows results for trained and evaluated on subtraction.</p>
<p>Figure 4 :
4
Figure 4: Visualization of attention weights during inference, with rows representing output tokens and columns indicating input tokens involved in generation.Attention weights are square-root transformed for enhanced visibility of correlations.The attention on the left(layer 14) reveals output digits are correlate with their inputs, while attention(right) from layer 22 suggests carry information reconstruction.</p>
<p>Previous methods that seek to teach LLMs to learn arithmetic mainly focus on the use of step-by-step processes.Scratchpad(Nye et al., 2021) was one of the early founders that recognized the use of stepby-step arithmetic solving.Zhou et al. focused on in-context learning and showed that a detailed version of Scratchpad could significantly improve the accuracy.Qian et al. recognized the challenger where LLM performance drops as repeated symbols increase.Goat (Liu and Low, 2023) classified tasks discussed the learnability of different operations and conducted supervised fine-tuning.Lee and Kim proposed the Recursion of Thought to divide the solving process into short contexts.On the other hand, some works also focus on analyzing arithmetic learning.Yuan et al. proposed MATH 401 to evaluate LLM's arithmetic ability.Jelassi et al. discussed the length generalization ability in arithmetic.Muffo et al. evaluated the ability of Transformer to perform arithmetic operations following a pipeline that decomposes numbers in decimal before performing computations and demonstrated that this method was 60% more accurate than GPT-3 on 5-digit addition and subtraction tasks, but was inferior to GPT-3 on 2digit multiplication tasks.Lee et al. conducted a compressive analysis on training strategies and discussed that reversing the output of addition can speed up the learning process.</p>
<p>Table 1 :
1
Performance comparison between methods, trained with 5K data for each operation with randomly generated data.The maximum digits of input numbers for each data are equally distributed in the range of[5, 12]for each operation.The test set is generated in a similar manner but with only 1K data per operation.LEFT uses Little-Endian to represent all numbers and excludes the step-by-step process for addition and subtraction.
MethodEndian StepByStep+−×Overall Token UsageEnd-To-EndBigNo63.3 32.3 00.031.9494,815Chain-of-ThoughtBigYes88.0 83.5 08.259.94,938,148ScratchpadBigYes94.8 73.1 00.056.05,747,670Scratchpad-DetailedBigYes99.8 97.3 52.883.310,995,191LEFT (Our)LittleMix98.8 95.9 88.594.43,040,616Observation 2: Using Little-Endian Alone Ob-tains Better Efficiency On Addition/Subtraction.During method design(Sec. 3.2), we proposed thatLittle-Endian is a better substitute than existingmethods, which leverage step-by-step to reducethe complexity required for arithmetic. However,we have not yet examined such a statement. Thisraised two major questions: (1) Would it be betterto contain step-by-step? (2) How does step-by-step
itself perform?As a result, we apply step-by-step for closer observation.We scale down the training data to half and a quarter of training cases than the joint evaluation and observe the change in performance.To omit influences caused by joint training, we train addition and subtraction separately.</p>
<p>Table 2 :
2
Multiplication scores by different epochs and token usage.We observe settings without step-by-step solution failed to learn the task.</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, 10.48550/arXiv.2303.08774CoRR, abs/2303.08774Shyamal Anadkat, et al. 2023. GPT-4 technical report. </p>
<p>. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Sygnowski, 10.48550/arXiv.2312.118052023Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USA2023. July 2023</p>
<p>MathPrompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, Harsh Shrivastava, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics20235Industry Track)</p>
<p>Yuanzhi Li, and François Charton. 2023. Length generalization in arithmetic transformers. Samy Jelassi, Carles Stéphane D'ascoli, Yuhuai Domingo-Enrich, Wu, 10.48550/arXiv.2306.15400CoRR, abs/2306.15400</p>
<p>Teaching arithmetic to small transformers. Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, Dimitris Papailiopoulos, 10.48550/arXiv.2307.03381CoRR, abs/2307.033812023</p>
<p>Recursion of thought: A divide-and-conquer approach to multicontext reasoning with language models. Soochan Lee, Gunhee Kim, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. Tiedong Liu, Bryan Kian, Hsiang Low, 2023</p>
<p>Evaluating transformer language models on arithmetic operations using number decomposition. Matteo Muffo, Aldo Cocco, Enrico Bertino, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2022</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, Show your work: Scratchpads for intermediate computation with language models. 2021</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 2022</p>
<p>Limitations of language models in arithmetic and symbolic induction. Jing Qian, Hong Wang, Zekun Li, Shiyang Li, Xifeng Yan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 10.48550/arXiv.2302.04761CoRR, abs/2302.047612023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USA2017. December 4-9, 2017</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>GPT can solve mathematical problems without a calculator. Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, Jie Tang, 10.48550/arXiv.2309.03241CoRR, abs/2309.032412023</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, 10.48550/arXiv.2304.02015CoRR, abs/2304.020152023</p>
<p>Teaching algorithmic reasoning via in-context learning. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron C Courville, Behnam Neyshabur, Hanie Sedghi, 10.48550/arXiv.2211.09066CoRR, abs/2211.090662022</p>            </div>
        </div>

    </div>
</body>
</html>
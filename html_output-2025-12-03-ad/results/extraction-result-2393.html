<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2393 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2393</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2393</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-d671aa30498544df784cf14ca64e31b0e0f97948</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d671aa30498544df784cf14ca64e31b0e0f97948" target="_blank">Acceleron: A Tool to Accelerate Research Ideation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Acceleron, a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process, and addresses challenges inherent in LLMs, such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability.</p>
                <p><strong>Paper Abstract:</strong> Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose `Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the proposed problem. We leverage the reasoning and domain-specific skills of Large Language Models (LLMs) to create an agent-based architecture incorporating colleague and mentor personas for LLMs. The LLM agents emulate the ideation process undertaken by researchers, engaging researchers in an interactive fashion to aid in the development of the research proposal. Notably, our tool addresses challenges inherent in LLMs, such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability. As evaluation, we illustrate the execution of our motivation validation and method synthesis workflows on proposals from the ML and NLP domain, given by 3 distinct researchers. Our observations and evaluations provided by the researchers illustrate the efficacy of the tool in terms of assisting researchers with appropriate inputs at distinct stages and thus leading to improved time efficiency.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2393.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2393.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Acceleron</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Acceleron: A Research Accelerator for Ideation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive web tool that uses LLM-based agents (colleague and mentor personas), retrieval-augmented pipelines, and iterative human-in-the-loop workflows to validate research motivations and synthesize plausible methods for research proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Acceleron</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Acceleron is an LLM-agent based system that automates parts of the research-ideation process via two complementary workflows: Motivation Validation and Method Synthesis. It (1) retrieves top-K candidate papers from a global corpus using Specter embeddings on title+abstract, (2) chunks and stores selected papers in a per-user vectorized user corpus, (3) uses a 'colleague' LLM to extract motivations, generate binary validation questions, and run retrieval-augmented QA on paper chunks (RAG) returning Yes/No/Unanswerable answers with explanations, (4) uses a 'mentor' LLM to extract limitations/gaps, synthesize updated proposals, generate similar/sub-problems and synthesize plausible methods. Components to mitigate hallucination include restricting answers to retrieved context (RAG), introducing explicit 'unanswerable' outputs when retrieved paragraphs lack an answer, requiring chain-of-thought style justifications, and keeping the system semi-automated so researchers can edit/remove outputs. A two-stage aspect-based retrieval is used: first-stage high-recall retrieval of top-K papers; second-stage lower-K, aspect-based retrieval on paper chunks for higher precision.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / natural language processing (demonstrated); intended for general scientific research ideation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted ideation / interdisciplinary synthesis (producing validated, novel research proposals and plausible solution methods)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Implicit novelty validation via literature gap checking: novelty is operationalized as absence of supporting evidence in retrieved literature — specifically, the fraction (or count) of retrieved paper × question pairs that are answered 'Yes' to binary questions that would demonstrate the literature already addresses the proposal's motivation; novelty validated when all answers are 'No' or 'Unanswerable'. (No formal numeric novelty formula is provided beyond these counts/proportions.)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Reported qualitatively via example counts in the paper (e.g., out of top-50 retrieved papers, 5 answered 'Yes' in one case; in another case 0/50 answered 'Yes'); no standardized numeric score or units given.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative human-in-the-loop refinement: the system produces candidate gaps/methods which the researcher edits/selects; two-stage retrieval balances recall (stage 1) and precision (stage 2); explicit 'unanswerable' labeling avoids forced/misleading answers; no automated novelty–feasibility multi-objective optimizer is reported — balance is achieved by researcher selection and iterative cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Qualitative evaluation with 3 researchers in ML/NLP domains. Researchers validated outputs, edited proposals and methods, and reported time-efficiency gains (approx. 5x, 8x, 10x for different tasks; overall ~7.5x). Examples: for one proposal 5/50 retrieved papers answered 'Yes' and researcher agreed with 4 justifications; for another, 0/50 answered 'Yes', supporting novelty. No formal numeric assessments of novelty vs feasibility trade-offs were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Manual researcher-driven literature search and ideation (human-only process)</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared qualitatively to manual process; authors report substantial time-efficiency improvements (examples: ~5×, ~8×, ~10× speedups for specific workflows). No formal statistical or quantitative baseline evaluation on novelty/feasibility metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Demonstrated on ML/NLP proposals; approach leverages domain-specific global corpus (S2ORC/Semantic Scholar) and Specter embeddings; authors note that domain-specific aspects may require different meta-process instances for life-sciences or materials science in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Acceleron: A Tool to Accelerate Research Ideation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2393.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2393.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Colleague agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Colleague LLM Agent (lightweight persona)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight LLM persona (implemented with GPT-turbo-3.5 in the paper) assigned extraction and retrieval-oriented tasks such as extracting motivations/problems, generating binary validation questions, retrieving and answering questions over paper chunks (RAG), and extracting method paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Colleague LLM Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The colleague agent is an LLM persona that performs lower-complexity steps: parses user proposal to extract motivation and problem, generates binary questions for motivation validation, retrieves relevant chunks from the user corpus, and runs retrieval-augmented QA constrained to the provided context to answer Yes/No/Unanswerable with short explanations. It also extracts method paragraphs from retrieved papers for Method Synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / NLP (demonstrated); applicable to other domains with appropriate corpora</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted extraction and verification tasks within ideation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Contributes to novelty assessment by producing binary question answers for each (paper, question) pair; novelty is inferred from absence of 'Yes' answers across retrieved papers (i.e., proportion of 'Yes' responses).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Shown in examples as counts (e.g., 5 'Yes' out of 50 in one case; 0/50 in another), but no formal numeric score scheme published.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Precision-control via two-stage retrieval (colleague uses first-stage retrieval outputs and low-K aspect retrievals) and 'unanswerable' output to avoid false positives; human verification step to remove hallucinated 'Yes' answers.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Researchers reviewed and edited the colleague agent's questions and Yes/No/Unanswerable outputs; examples show researchers agreed with most justifications but also removed hallucinated or disagreeable outputs. No quantitative inter-rater or scoring statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Manual question generation and manual literature checking</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reported large time savings in question-generation and per-paper checking compared to manual work; no numeric comparison on hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Works on ML/NLP corpora; effectiveness depends on quality of retrieved chunks and user edits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Acceleron: A Tool to Accelerate Research Ideation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2393.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2393.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mentor agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mentor LLM Agent (reasoning persona)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heavier LLM persona (implemented with GPT-4 in the paper) assigned higher-complexity reasoning tasks: extracting limitations from papers, reformulating motivation, generating similar/sub-problems, and synthesizing plausible methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mentor LLM Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The mentor agent uses parametric knowledge of a large LLM to perform complex reasoning: given proposal, selected paper chunks, and identified questions/gaps, it extracts limitations, re-writes proposals addressing new gaps, generates similar/generalized problems, decomposes problems into sub-tasks, and synthesizes plausible solution methods by combining literature-derived approaches with its internal knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / NLP (demonstrated); intended generalizable</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>interdisciplinary synthesis / targeted ideation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Indirect: mentor uses extracted gaps (from literature) to reformulate proposal so that novelty is increased; novelty still operationalized via subsequent validation workflow (absence of 'Yes' answers). Mentor does not compute an explicit numeric novelty metric.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Implicitly handled via researcher selection/acceptance of synthesized methods — mentor suggests plausible methods but no automatic feasibility scoring or numeric feasibility metric is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative synthesis and human selection: mentor proposes methods; researcher selects plausible subset and may request rewrites; this human-in-the-loop selection functions as the mechanism to balance novelty vs apparent practicality.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Researchers found mentor-generated methods relevant and usable as first-cut plausible approaches (example: 10 suggested methodologies for reference-free evaluation task, researcher agreed they were well-suited though requiring further work). No formal feasibility evaluation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Human-only ideation and literature synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reported qualitative gains in efficiency (e.g., ~10× faster for method-synthesis in one example); no quantitative measure of method quality vs baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Demonstrated generation of similar problems/subtasks and method lists for ML/NLP tasks (e.g., reference-free evaluation metric proposal).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Acceleron: A Tool to Accelerate Research Ideation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2393.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2393.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-stage aspect-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage Aspect-based Retrieval (high-recall then aspect-focused precision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval design that first performs high-recall retrieval of top-K papers using title+abstract embeddings, then chunks and stores those papers and runs lower-K, aspect-focused retrieval on paragraph-level chunks to extract precise information (methods, limitations) while controlling precision-recall trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Two-stage aspect-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Stage 1: retrieve top-K papers (high K) from the global corpus using Specter embeddings of titles+abstracts to ensure coverage (high recall) of works with similar motivations/problems. Chunk the selected papers into paragraph-level segments and index into a user-specific corpus. Stage 2: for targeted aspects (e.g., 'methods', 'limitations'), perform low-K retrieval on the user corpus to obtain precise paragraph(s) for downstream RAG or extraction. This two-stage design aims to reduce false negatives in initial retrieval while enabling precise aspect extraction in the second stage.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>information retrieval for scientific literature; used in ML/NLP examples</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted extraction within ideation pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Architectural evidence of precision–recall trade-off: first stage favors high recall (reducing false negatives), second stage favors precision (reducing false positives); no quantitative tradeoff curves or numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Design-level optimization: separate retrieval into a recall-oriented first stage and a precision-oriented second stage; allow researcher edits to correct false positives/negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Authors report qualitative improvements in relevance of extracted aspects and successful downstream validation/synthesis tasks; no numeric retrieval metrics (e.g., recall@K, precision@K) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Single-stage retrieval on title/abstract or full-text</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Described qualitatively as improving coverage and precision for ideation tasks; no quantitative baseline numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Effective on S2ORC-derived AI/ML/NLP corpus and for extracting methods and limitations from papers in those domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Acceleron: A Tool to Accelerate Research Ideation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2393.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2393.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG + Unanswerability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation with explicit Unanswerability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of retrieval-augmented QA constrained to retrieved context, returning Yes/No/Unanswerable answers; 'Unanswerable' is explicitly supported to avoid hallucination when retrieved chunks lack an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) with Unanswerability</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system uses RAG to answer binary validation questions by constraining the LLM to retrieved paragraph-contexts and instructing it to respond 'Yes' only when the context clearly supports it, otherwise 'No' or 'Unanswerable'. The 'Unanswerable' class is used when retrieved paragraphs are irrelevant or insufficient, which helps avoid hallucinated affirmative answers and reduces false positive validation of motivations.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>scientific literature QA and ideation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>verification / evidence-based validation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Operational novelty check: counts/proportion of (paper,question) pairs answered 'Yes' versus 'No'/'Unanswerable' (zero 'Yes' implies novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Examples reported in the paper as raw counts (e.g., 0/50 or 5/50), but no normalized score or formula.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Constrain LLM answers to retrieved context and require concise justification when 'Yes' is returned; use 'Unanswerable' to avoid forced answers; human-in-the-loop review to drop hallucinated responses.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Researchers inspected and accepted/rejected RAG outputs; reported that the explicit 'Unanswerable' option and justifications helped identify hallucinations and irrelevant papers. No numeric measures of RAG accuracy or unanswerability rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Unconstrained LLM answers without retrieval or without 'Unanswerable' class</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Qualitative claim that retrieval constraint + unanswerability reduces hallucinations; no quantitative comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Applied on ML/NLP paper chunks; retrieval and unanswerability helpful for motivation validation and method extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Acceleron: A Tool to Accelerate Research Ideation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2393.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2393.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought justification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (COT) style justifications for LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of prompting to force LLM agents to provide justifications (chain-of-thought) for outputs, exposing reasoning to the researcher to enable verification and reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought Justification</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system prompts LLM agents to provide concise justifications or reasoning traces for their outputs (especially when returning 'Yes' answers or synthesized proposals/methods). These justifications are surfaced to the researcher who can judge plausibility and remove hallucinated or incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>LLM-based ideation and verification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>explainable output verification</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Human-in-the-loop verification aided by exposed reasoning; no automated scoring trade-off optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Researchers used the provided justifications to accept or remove outputs and found this helpful to mitigate hallucinations. No numeric evaluation of COT effectiveness is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Opaque LLM outputs without justifications</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Qualitative improvement in researcher ability to detect hallucinations; no quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Acceleron: A Tool to Accelerate Research Ideation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Survey on Large Language Model based Autonomous Agents <em>(Rating: 2)</em></li>
                <li>Can large language models provide useful feedback on research papers? <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>SPECTER: Document-level Representation Learning using Citation-informed Transformers <em>(Rating: 1)</em></li>
                <li>ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2393",
    "paper_id": "paper-d671aa30498544df784cf14ca64e31b0e0f97948",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "Acceleron",
            "name_full": "Acceleron: A Research Accelerator for Ideation",
            "brief_description": "An interactive web tool that uses LLM-based agents (colleague and mentor personas), retrieval-augmented pipelines, and iterative human-in-the-loop workflows to validate research motivations and synthesize plausible methods for research proposals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Acceleron",
            "system_description": "Acceleron is an LLM-agent based system that automates parts of the research-ideation process via two complementary workflows: Motivation Validation and Method Synthesis. It (1) retrieves top-K candidate papers from a global corpus using Specter embeddings on title+abstract, (2) chunks and stores selected papers in a per-user vectorized user corpus, (3) uses a 'colleague' LLM to extract motivations, generate binary validation questions, and run retrieval-augmented QA on paper chunks (RAG) returning Yes/No/Unanswerable answers with explanations, (4) uses a 'mentor' LLM to extract limitations/gaps, synthesize updated proposals, generate similar/sub-problems and synthesize plausible methods. Components to mitigate hallucination include restricting answers to retrieved context (RAG), introducing explicit 'unanswerable' outputs when retrieved paragraphs lack an answer, requiring chain-of-thought style justifications, and keeping the system semi-automated so researchers can edit/remove outputs. A two-stage aspect-based retrieval is used: first-stage high-recall retrieval of top-K papers; second-stage lower-K, aspect-based retrieval on paper chunks for higher precision.",
            "research_domain": "machine learning / natural language processing (demonstrated); intended for general scientific research ideation",
            "problem_type": "targeted ideation / interdisciplinary synthesis (producing validated, novel research proposals and plausible solution methods)",
            "novelty_metric": "Implicit novelty validation via literature gap checking: novelty is operationalized as absence of supporting evidence in retrieved literature — specifically, the fraction (or count) of retrieved paper × question pairs that are answered 'Yes' to binary questions that would demonstrate the literature already addresses the proposal's motivation; novelty validated when all answers are 'No' or 'Unanswerable'. (No formal numeric novelty formula is provided beyond these counts/proportions.)",
            "novelty_score": "Reported qualitatively via example counts in the paper (e.g., out of top-50 retrieved papers, 5 answered 'Yes' in one case; in another case 0/50 answered 'Yes'); no standardized numeric score or units given.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Iterative human-in-the-loop refinement: the system produces candidate gaps/methods which the researcher edits/selects; two-stage retrieval balances recall (stage 1) and precision (stage 2); explicit 'unanswerable' labeling avoids forced/misleading answers; no automated novelty–feasibility multi-objective optimizer is reported — balance is achieved by researcher selection and iterative cycles.",
            "human_evaluation": true,
            "human_evaluation_results": "Qualitative evaluation with 3 researchers in ML/NLP domains. Researchers validated outputs, edited proposals and methods, and reported time-efficiency gains (approx. 5x, 8x, 10x for different tasks; overall ~7.5x). Examples: for one proposal 5/50 retrieved papers answered 'Yes' and researcher agreed with 4 justifications; for another, 0/50 answered 'Yes', supporting novelty. No formal numeric assessments of novelty vs feasibility trade-offs were reported.",
            "comparative_baseline": "Manual researcher-driven literature search and ideation (human-only process)",
            "comparative_results": "Compared qualitatively to manual process; authors report substantial time-efficiency improvements (examples: ~5×, ~8×, ~10× speedups for specific workflows). No formal statistical or quantitative baseline evaluation on novelty/feasibility metrics.",
            "domain_specific_findings": "Demonstrated on ML/NLP proposals; approach leverages domain-specific global corpus (S2ORC/Semantic Scholar) and Specter embeddings; authors note that domain-specific aspects may require different meta-process instances for life-sciences or materials science in future work.",
            "uuid": "e2393.0",
            "source_info": {
                "paper_title": "Acceleron: A Tool to Accelerate Research Ideation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Colleague agent",
            "name_full": "Colleague LLM Agent (lightweight persona)",
            "brief_description": "A lightweight LLM persona (implemented with GPT-turbo-3.5 in the paper) assigned extraction and retrieval-oriented tasks such as extracting motivations/problems, generating binary validation questions, retrieving and answering questions over paper chunks (RAG), and extracting method paragraphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Colleague LLM Agent",
            "system_description": "The colleague agent is an LLM persona that performs lower-complexity steps: parses user proposal to extract motivation and problem, generates binary questions for motivation validation, retrieves relevant chunks from the user corpus, and runs retrieval-augmented QA constrained to the provided context to answer Yes/No/Unanswerable with short explanations. It also extracts method paragraphs from retrieved papers for Method Synthesis.",
            "research_domain": "machine learning / NLP (demonstrated); applicable to other domains with appropriate corpora",
            "problem_type": "targeted extraction and verification tasks within ideation",
            "novelty_metric": "Contributes to novelty assessment by producing binary question answers for each (paper, question) pair; novelty is inferred from absence of 'Yes' answers across retrieved papers (i.e., proportion of 'Yes' responses).",
            "novelty_score": "Shown in examples as counts (e.g., 5 'Yes' out of 50 in one case; 0/50 in another), but no formal numeric score scheme published.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Precision-control via two-stage retrieval (colleague uses first-stage retrieval outputs and low-K aspect retrievals) and 'unanswerable' output to avoid false positives; human verification step to remove hallucinated 'Yes' answers.",
            "human_evaluation": true,
            "human_evaluation_results": "Researchers reviewed and edited the colleague agent's questions and Yes/No/Unanswerable outputs; examples show researchers agreed with most justifications but also removed hallucinated or disagreeable outputs. No quantitative inter-rater or scoring statistics provided.",
            "comparative_baseline": "Manual question generation and manual literature checking",
            "comparative_results": "Reported large time savings in question-generation and per-paper checking compared to manual work; no numeric comparison on hypothesis quality.",
            "domain_specific_findings": "Works on ML/NLP corpora; effectiveness depends on quality of retrieved chunks and user edits.",
            "uuid": "e2393.1",
            "source_info": {
                "paper_title": "Acceleron: A Tool to Accelerate Research Ideation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Mentor agent",
            "name_full": "Mentor LLM Agent (reasoning persona)",
            "brief_description": "A heavier LLM persona (implemented with GPT-4 in the paper) assigned higher-complexity reasoning tasks: extracting limitations from papers, reformulating motivation, generating similar/sub-problems, and synthesizing plausible methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Mentor LLM Agent",
            "system_description": "The mentor agent uses parametric knowledge of a large LLM to perform complex reasoning: given proposal, selected paper chunks, and identified questions/gaps, it extracts limitations, re-writes proposals addressing new gaps, generates similar/generalized problems, decomposes problems into sub-tasks, and synthesizes plausible solution methods by combining literature-derived approaches with its internal knowledge.",
            "research_domain": "machine learning / NLP (demonstrated); intended generalizable",
            "problem_type": "interdisciplinary synthesis / targeted ideation",
            "novelty_metric": "Indirect: mentor uses extracted gaps (from literature) to reformulate proposal so that novelty is increased; novelty still operationalized via subsequent validation workflow (absence of 'Yes' answers). Mentor does not compute an explicit numeric novelty metric.",
            "novelty_score": null,
            "feasibility_metric": "Implicitly handled via researcher selection/acceptance of synthesized methods — mentor suggests plausible methods but no automatic feasibility scoring or numeric feasibility metric is provided.",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Iterative synthesis and human selection: mentor proposes methods; researcher selects plausible subset and may request rewrites; this human-in-the-loop selection functions as the mechanism to balance novelty vs apparent practicality.",
            "human_evaluation": true,
            "human_evaluation_results": "Researchers found mentor-generated methods relevant and usable as first-cut plausible approaches (example: 10 suggested methodologies for reference-free evaluation task, researcher agreed they were well-suited though requiring further work). No formal feasibility evaluation reported.",
            "comparative_baseline": "Human-only ideation and literature synthesis",
            "comparative_results": "Reported qualitative gains in efficiency (e.g., ~10× faster for method-synthesis in one example); no quantitative measure of method quality vs baseline.",
            "domain_specific_findings": "Demonstrated generation of similar problems/subtasks and method lists for ML/NLP tasks (e.g., reference-free evaluation metric proposal).",
            "uuid": "e2393.2",
            "source_info": {
                "paper_title": "Acceleron: A Tool to Accelerate Research Ideation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Two-stage aspect-based retrieval",
            "name_full": "Two-stage Aspect-based Retrieval (high-recall then aspect-focused precision)",
            "brief_description": "A retrieval design that first performs high-recall retrieval of top-K papers using title+abstract embeddings, then chunks and stores those papers and runs lower-K, aspect-focused retrieval on paragraph-level chunks to extract precise information (methods, limitations) while controlling precision-recall trade-offs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Two-stage aspect-based retrieval",
            "system_description": "Stage 1: retrieve top-K papers (high K) from the global corpus using Specter embeddings of titles+abstracts to ensure coverage (high recall) of works with similar motivations/problems. Chunk the selected papers into paragraph-level segments and index into a user-specific corpus. Stage 2: for targeted aspects (e.g., 'methods', 'limitations'), perform low-K retrieval on the user corpus to obtain precise paragraph(s) for downstream RAG or extraction. This two-stage design aims to reduce false negatives in initial retrieval while enabling precise aspect extraction in the second stage.",
            "research_domain": "information retrieval for scientific literature; used in ML/NLP examples",
            "problem_type": "targeted extraction within ideation pipelines",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "Architectural evidence of precision–recall trade-off: first stage favors high recall (reducing false negatives), second stage favors precision (reducing false positives); no quantitative tradeoff curves or numbers reported.",
            "optimization_strategy": "Design-level optimization: separate retrieval into a recall-oriented first stage and a precision-oriented second stage; allow researcher edits to correct false positives/negatives.",
            "human_evaluation": true,
            "human_evaluation_results": "Authors report qualitative improvements in relevance of extracted aspects and successful downstream validation/synthesis tasks; no numeric retrieval metrics (e.g., recall@K, precision@K) reported.",
            "comparative_baseline": "Single-stage retrieval on title/abstract or full-text",
            "comparative_results": "Described qualitatively as improving coverage and precision for ideation tasks; no quantitative baseline numbers.",
            "domain_specific_findings": "Effective on S2ORC-derived AI/ML/NLP corpus and for extracting methods and limitations from papers in those domains.",
            "uuid": "e2393.3",
            "source_info": {
                "paper_title": "Acceleron: A Tool to Accelerate Research Ideation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "RAG + Unanswerability",
            "name_full": "Retrieval-Augmented Generation with explicit Unanswerability",
            "brief_description": "Use of retrieval-augmented QA constrained to retrieved context, returning Yes/No/Unanswerable answers; 'Unanswerable' is explicitly supported to avoid hallucination when retrieved chunks lack an answer.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG) with Unanswerability",
            "system_description": "The system uses RAG to answer binary validation questions by constraining the LLM to retrieved paragraph-contexts and instructing it to respond 'Yes' only when the context clearly supports it, otherwise 'No' or 'Unanswerable'. The 'Unanswerable' class is used when retrieved paragraphs are irrelevant or insufficient, which helps avoid hallucinated affirmative answers and reduces false positive validation of motivations.",
            "research_domain": "scientific literature QA and ideation",
            "problem_type": "verification / evidence-based validation",
            "novelty_metric": "Operational novelty check: counts/proportion of (paper,question) pairs answered 'Yes' versus 'No'/'Unanswerable' (zero 'Yes' implies novelty).",
            "novelty_score": "Examples reported in the paper as raw counts (e.g., 0/50 or 5/50), but no normalized score or formula.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Constrain LLM answers to retrieved context and require concise justification when 'Yes' is returned; use 'Unanswerable' to avoid forced answers; human-in-the-loop review to drop hallucinated responses.",
            "human_evaluation": true,
            "human_evaluation_results": "Researchers inspected and accepted/rejected RAG outputs; reported that the explicit 'Unanswerable' option and justifications helped identify hallucinations and irrelevant papers. No numeric measures of RAG accuracy or unanswerability rates provided.",
            "comparative_baseline": "Unconstrained LLM answers without retrieval or without 'Unanswerable' class",
            "comparative_results": "Qualitative claim that retrieval constraint + unanswerability reduces hallucinations; no quantitative comparison provided.",
            "domain_specific_findings": "Applied on ML/NLP paper chunks; retrieval and unanswerability helpful for motivation validation and method extraction.",
            "uuid": "e2393.4",
            "source_info": {
                "paper_title": "Acceleron: A Tool to Accelerate Research Ideation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chain-of-Thought justification",
            "name_full": "Chain-of-Thought (COT) style justifications for LLM outputs",
            "brief_description": "Use of prompting to force LLM agents to provide justifications (chain-of-thought) for outputs, exposing reasoning to the researcher to enable verification and reduce hallucination.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought Justification",
            "system_description": "The system prompts LLM agents to provide concise justifications or reasoning traces for their outputs (especially when returning 'Yes' answers or synthesized proposals/methods). These justifications are surfaced to the researcher who can judge plausibility and remove hallucinated or incorrect outputs.",
            "research_domain": "LLM-based ideation and verification",
            "problem_type": "explainable output verification",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Human-in-the-loop verification aided by exposed reasoning; no automated scoring trade-off optimizer.",
            "human_evaluation": true,
            "human_evaluation_results": "Researchers used the provided justifications to accept or remove outputs and found this helpful to mitigate hallucinations. No numeric evaluation of COT effectiveness is provided.",
            "comparative_baseline": "Opaque LLM outputs without justifications",
            "comparative_results": "Qualitative improvement in researcher ability to detect hallucinations; no quantitative comparison.",
            "uuid": "e2393.5",
            "source_info": {
                "paper_title": "Acceleron: A Tool to Accelerate Research Ideation",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Survey on Large Language Model based Autonomous Agents",
            "rating": 2
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers?",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers",
            "rating": 1
        },
        {
            "paper_title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
            "rating": 1
        }
    ],
    "cost": 0.013396749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Acceleron: A Tool to Accelerate Research Ideation</h1>
<p>Harshit Nigam ${ }^{1}$, Manasi Patwardhan ${ }^{2}$, Lovekesh Vig ${ }^{3}$, Gautam Shroff ${ }^{4}$<br>TCS Research<br>${ }^{1}$ h.nigam@tcs.com, ${ }^{2}$ manasi.patwardhan@tcs.com, ${ }^{3}$ lovekesh.vig@tcs.com, ${ }^{4}$ gautam.shroff@tcs.com</p>
<h4>Abstract</h4>
<p>Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose 'Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the proposed problem. We leverage the reasoning and domain-specific skills of Large Language Models (LLMs) to create an agent-based architecture incorporating colleague and mentor personas for LLMs. The LLM agents emulate the ideation process undertaken by researchers, engaging researchers in an interactive fashion to aid in the development of the research proposal. Notably, our tool addresses challenges inherent in LLMs, such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability. To showcase the ideation capabilities of 'Acceleron', we illustrate the execution of our motivation validation and method synthesis workflows on proposals from the machine learning and natural language processing domain, given as an input by 3 distinct researchers. Our observations and evaluations provided by the researchers illustrate the efficacy of the tool in terms of assisting researchers with appropriate inputs at distinct stages and thus leading to improved time efficiency.</p>
<h2>Introduction</h2>
<p>With fast-paced research happening in every field, we are witnessing an exponential growth in the number of scientific articles and research papers on the web. It is difficult for an individual researcher or a small research team to keep abreast of the relevant advances amidst this information explosion. This has a downstream impact on the ability to be</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Acceleron Interface
consistently appraised and ensure novelty of a proposed solution at various stages of the research life cycle. Thus there is an urgent need for a tools that can aid researchers to 1) understand, evaluate and incorporate the latest developments in the literature and 2) Formulate/Modify the current proposed solution accordingly to ensure novelty and impact.</p>
<p>Existing tools facilitate searching of research papers relevant to one's topic of interest based on a query (e.g. Elicit ${ }^{1}$, Raxter.io ${ }^{2}$, SciSpace ${ }^{3}$ ), keywords (e.g. Litmaps ${ }^{4}$, Iris.ai ${ }^{5}$ ), paper titles (e.g. inciteful ${ }^{6}$ ), through citation graphs (e.g. Litmaps $^{4}$, inciteful ${ }^{6}$, Research Rabbit ${ }^{7}$, Connected Papers ${ }^{8}$ ) or facts and insights (e.g. Scite ${ }^{9}$ ) as inputs. They further help</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>in (i) extracting relevant information from the papers with predefined templates (e.g. Elicit^{1}, Iris.ai^{5}) or based on user queries (e.g. SciSpace^{3}), (ii) sharing the papers in collaborative fashion (e.g. Litmaps^{4}, Raxter.io^{2}, Research Rabbit^{7}), (iii) notifying researchers about articles relevant to their searches, tracking trends (e.g. Scite^{9}, Research Rabbit^{7}), etc. Some tools act as the facilitator for reading or writing (e.g. Paperpal^{10}) manuscripts by allowing highlighting, extracting and documenting important aspects of the paper (e.g. Raxter.io^{2}), communicating in more interactive fashion to analyze the paper in depth (e.g. SciSpace^{3}, Iris.ai^{5}), recommending relevant quality citations (e.g. Scite^{9}, Raxter.io^{2}) or summarizing papers (e.g. Elicit^{1}, Iris.ai^{5}). Thus, most of the existing tools focus on notifying and recommending researchers with relevant literature, facilitate exploration of existing literature and/or writing research manuscripts. Researchers have also proposed learning representations for retrieval of relevant scientific articles (Singh et al. 2022; Cohan et al. 2020; Ostendorff et al. 2022; Mysore, Cohan, and Hope 2021), literature Review Generation (Hu and Wan 2014; Kasanishi et al. 2023; Chen et al. 2021), Question Answering over scientific articles (Saikh et al. 2022; Dasigi et al. 2021; Lee et al. 2023), Scientific document summarization (Hayashi et al. 2020), citation recommendation (Ali et al. 2021, 2022; Medic and Snajder 2023) citation intent detection (Cohan et al. 2019; Berrebbi, Huynh, and Balalau 2022; Roman et al. 2021; Lauscher et al. 2021), critical review and rebuttal generation (Ruggeri, Mesgar, and Gurevych 2022; D’Arcy et al. 2023; Kennard et al. 2021; Dycke, Kuznetsov, and Gurevych 2022; Wu et al. 2022), etc. However, to the best of our knowledge, no tool or no approach in the literature facilitates a researcher during the most arduous ideation stage of the research life-cycle. Ideation involves: (i) Analyzing the existing literature to critically evaluate the motivation behind the research problem a researcher is trying to address to ensure that the mentioned research gap(s) still exist(s), (ii) Reformulating the proposed research problem and objectives based on the validation stage output and re-identification of research gaps, (iii) Identifying analogous research problems or sub-problems addressed in the literature and utilizing their solutions, available in the literature, to derive a set-of approaches or synthesizing a set-of plausible methods as a solution to the problem, (iv) Designing experimentation strategy for the given problem and selected methodology.</p>
<p>Most of the tasks involved in research require domain expertise and complex reasoning skills. The recent advancement in Large Language Models (LLMs) and Generative Artificial Intelligence (GenAI) has made it possible to partially automate some of these tasks (Liu and Shah 2023; Liang et al. 2023; Zhang et al. 2023b; Lahiri, Sanyal, and Mukherjee 2023; Kunnath, Pride, and Knoth 2023). In this work, we propose 'Acceleron' (Figure 1), a tool to accelerate the research life cycle. By exploiting the reasoning and domain specific skills of LLM based agents, the goal of the tool is to assist with research activities, alleviating the burden of researchers. The aim is not to replace a researcher, but to assist</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: System Architecture</p>
<p>the researcher by providing relevant inputs in an interactive fashion, at various stages of research life cycle, viz. ideate, explore, codify, experiment, communicate, critique, inform; to expedite the process of meeting the research objectives. In this paper we are focusing on the 'ideate' module of the tool, which facilitates the ideation of a research problem, specified by the researcher. We expect the researcher to provide a short paragraph along with the title of the research problem the researcher plan to address, along with the description of what motivates the researcher to solve the problem. With LLM powered mentor and colleague agents, the tool, in an interactive fashion, helps the researcher to develop the research proposal consisting of a validated motivation, a well-defined research problem focusing of research gaps in the literature, a proposed approach selected from a set-of plausible synthesized methods and possible set-of experiments to be conducted to evaluate the approach for the research problem. To the best of our knowledge, we are the first ones to mimic the ideation process, followed by researchers, using the LLM agents. We demonstrate the ideation power of 'Acceleron' by illustrating examples from natural language processing.</p>
<h2>System Architecture</h2>
<p>Acceleron provides a web-based interface for researchers to interact. The system architecture is illustrated in Figure 2. We define an LLM Agent based architecture (Wang et al. 2023b), with agents of two distinct types of profiles or personas^{11}. A Colleague persona^{12} performs less complex tasks including extraction of relevant information from user inputs, generation of relevant questions from extracted information or retrieval of relevant data from scientific documents. Whereas, mentor persona^{13} performs more complex tasks requiring reasoning such as understanding the limitations or gaps of the existing work, identifying problems similar to the problem discussed in the proposal, identifying sub-tasks of the problem being solved in the proposal, solving similar problems and/or sub-tasks to synthesize a solution to the proposed problem and re-write the proposal</p>
<p><sup>11</sup>We use Langchain framework https://www.langchain.com for implementation</p>
<p><sup>12</sup>OpenAPI's GPT-turbo-3.5 model</p>
<p><sup>13</sup>OpenAPI's GPT4 model</p>
<p>given a plausible set-of approaches or possible limitations of related work. The architecture is flexible such that the LLM agents can interact with (i) LLMs using API calls or (ii) open-source LLMs which reside on an internal hosting server.</p>
<p>We expect to have a global repository which is a vector store of domain specific scientific articles which are indexed by the Specter embeddings [cohan2020specter] produced using the paper's title and abstract. We also have a User Specific corpus which has chunks of all the retrieved papers relevant to the current proposal the researcher is working on. The paper chunks are created with our in-house parser treating paragraphs as semantic segments. If a paragraph does not fit into the the maximum token length of LLM agents, while chunking it is further split to fit into the maximum token length. The chunks are further converted to vector embeddings and indexed for efficient retrieval based on semantic similarity with a query. This user corpus acts as a shared memory for the LLM agents.</p>
<h2>Approach</h2>
<p>In this section we describe the ideation process adopted by Acceleron which simulates the ideation process followed by researchers using LLM agents. The process involves interaction between a researcher and the LLM agents, where the LLM agents perform actions based on the feedback received by the researcher or another agent. The process takes a proposal as an input from a researcher with a research problem description specified at a high level along with the motivation behind the problem. The output of the ideation process is the updated proposal with a (i) Validated motivation or updated research problem by identifying gaps mentioned in the literature addressing the motivation (ii) Plausible methods to address the research problem. The overall ideation task is split into two workflows: (i) Motivation Validation and (ii) Method Synthesis. The detailed prompts for the steps in each of the workflow are illustrated in the Appendix Section A.</p>
<h2>Motivation Validation Workflow</h2>
<p>This is the first phase of the ideation process. The workflow is elaborated in Figure 3. The steps of the workflow are explained in detail here: 1. The researcher provides the title and abstract of the proposal elaborating the motivation behind the proposal and a high level description of the problem statement the research wishes to solve. 2. The retriever functionality of Acceleron uses this title and abstract of the proposal as a query and gets a vector representation of the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Motivation Validation Workflow</p>
<p>same 3. This representation is used to retrieve top-K articles similar to the contents of the proposal from the global corpus of scientific articles. 4. The top-K articles along with a description of the relevance of each article to the proposal is shown to the researcher. 5. These articles are editable by the researcher who can delete articles they find irrelevant or by manually adding relevant articles. 6. The parser and indexer functionality chunks the finalized set-of papers in paragraphs (semantic segments) 7. The chunks are stored in the user corpus with appropriate indexing mechanisms 8. The colleague agent fetches the proposal title and abstract provided by the researcher 9. It extracts the motivation out of the proposal (Prompt 1) and generates a list of questions to be posed on the shortlisted scientific articles to validate the motivation of the proposal (Prompt 2). The questions are binary and formulated such that if, for a scientific article, the answer to the question is 'yes', then the article is already addressing the motivation of the proposal mentioned by that question. For example, if the researcher proposes to develop a technique to solve a novel aspect of a problem, a question would be generated of the form 'Does the research paper address that specific aspect of the problem?'. If this question is answered as 'yes' by a scientific article then it implies that the article addresses that aspect of the problem and hence the motivation behind the study is weak or invalidated. 10. The generated set-of questions are shown to the researcher 11. The researcher can edit the questions by updating the format of the questions, deleting questions that the researcher deems irrelevant or adding missing relevant questions 12. For each question and a retrieved paper stored in the user corpus, the colleague agent retrieves the chunks of the paper relevant to that question and tries to answer that question using Retrieval Augmented Generation (RAG) (Prompt 3). The answer can be 'yes', 'no' or unanswerable along with an explanation. 13. If all the papers answer either 'no' or 'unanswerable' for all the questions generated to validate the motivation; it indicates that the existing literature is not addressing the motivation behind the proposal and hence this phase is ended with a comment shown to the researcher that the motivation of the proposal is validated. Otherwise, the question-research paper pairs with only 'yes' as an answer, along with explanation are shown to the researcher. 14. The researcher can edit this output in terms of removing papers which he doesn't agree to address the question based on the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Method Synthesis Workflow</p>
<p>explanation provided. If the answers are hallucinated, this step enables the researcher to remove such papers. 15. The <em>mentor</em> agent uses the chunks of each of the shortlisted paper, the original proposal and the description of the prior question addressing the motivation 16. It extracts the limitations or gaps of each of the papers (Prompt 4), which has been identifying to be addressing the motivation of the proposal, such that the gaps can help redefining the problem in the proposal. 17. The research gaps are shown to the researcher 18. The researcher can ignore the gaps found to be irrelevant and selects some of the gaps that address part of the research problem. If the researcher does not agree with any of the specified research gaps, they can add their own set of research gaps 19. The <em>mentor</em> agent uses these gaps along with the original proposal to re-formulate the motivation (Prompt 5) and the problem statement of the proposal to address the new research gaps. 20. The modified proposal is presented to the researcher (Prompt 6). 21. The modified proposal can be edited by the researcher to finalize the same or he can reject the edits and go back to the prior proposal. This workflow can be applied to the proposal in an iterative manner. Meaning, the motivation behind the resultant updated proposal can be again validated by initiating the same workflow. This can be executed in an iterative fashion, until there are no scientific articles retrieved to be addressing the motivation behind the proposal, validating the novelty of the proposal.</p>
<h3>Method Synthesis Workflow</h3>
<p>The steps of the method synthesis workflow are illustrated in Figure 4 and are elaborated here:</p>
<ol>
<li>The method synthesis phase begins with the proposal whose motivation is validated based on reviewing the literature and accepted by the researcher.</li>
<li>The <em>colleague</em> agent takes the proposal as an input.</li>
<li>It extracts the problem defined in the proposal (Prompt 7).</li>
<li>The <em>mentor</em> agent takes this problem as an input.</li>
<li>Uses its' parametric knowledge to generate a plausible set of similar research problems (Prompt 8).</li>
</ol>
<p>For example, if the problem defined in the proposal is "To design a reference-free evaluation metric for question answering task". A similar problem can be "To come up with an evaluation metric for text summarization which can have multiple possible reference summaries". The <em>mentor</em> agent also uses its parametric knowledge to decompose the problem defined in the proposal into sub-tasks (if any) (Prompt 11). For example, the problem of "question answering over scientific papers" can be decomposed into "extraction of text from PDF document of the scientific paper", "segmenting the paper and storing it for efficient retrieval", "retrieval of paragraphs from the paper related to the question", "answering the question using the retrieved paragraphs as context", "evaluating the retrieved paragraphs" and "evaluating the answers".
6. The generated similar and sub problems are shown to the researcher.
7. The researcher can edit them by removing the ones found to be irrelevant, updating them or adding missing ones. This helps in removing the problems hallucinated by the agent.
8. Each of the edited similar problem or sub-problem is used as a query to retrieve scientific articles from the global corpus which address those respective problems.
9. The retrieved scientific articles are parsed and chunked.
10. Are stored in the user corpus.
11. For each retrieved paper, the <em>colleague</em> agent extracts the paragraphs which talk about the method or approach taken by the paper to solve the respective problem.
12. Generates a consolidated list of similar or sub problem and solution pairs (Prompt 9, 10, 12).
13. This list is showcased to the researcher who makes edits to the same.
14. The extracted problem of the proposal along with the consolidated list of similar or sub problems along with their solutions in the literature is provided to the <em>mentor</em> agent (Prompt 14, 15).
15. The <em>mentor</em> agent uses this information along with its' parametric knowledge to synthesize a list of plausible methods to solve the problem defined in the proposal (Prompt 16).
16. The list of plausible methods is shown to the researcher (Prompt 13).
17. Researcher can choose a subset of these methods which they deem most plausible and further edit these if need be.
18. The updated list is provided to the <em>mentor</em> agent along with the original proposal. The <em>mentor</em> agent re-writes the proposal including these methods (Prompt 17).
19. This updated proposal is shown to the researcher.
20. The researcher can further edit the proposal and finalize the same.</p>
<h3>Novel Components</h3>
<p><strong>LLM agents for research ideation:</strong> To the best of our knowledge, ours is the first LLM agent based tool which assists in the complex task of ideation for research. We have devised with two novel portfolios for LLMS, viz., <em>colleague</em> and <em>mentor</em>, allocating less complex tasks to the <em>colleague</em> agent and more complex reasoning based tasks to the <em>mentor</em> agent. The user corpus acts as the shared memory for the agents, whereas the agents perform fixed set of actions at various stages of the workflow based on the provided inputs as discussed in the prior sections. Rather than using a costly LLM like GPT4 for all the tasks involved in the workflows; dividing the tasks as per the difficulty level and leveraging less costly LLM such as GPT-turbo-3.5 for colleague agent, performing less complex tasks, provides a cost-effective solution for workflows.</p>
<p><strong>Mitigation of hallucination:</strong> Hallucination is one of the major difficulties of using LLMs for knowledge based tasks (Zhang et al. 2023a; Wang et al. 2023a). We mitigate this problem using a two-fold solution: (i) There are retrieval augmented components of the workflows, viz. the motiva</p>
<p>tion validation workflow poses questions generated to validate the motivation of the proposal on the retrieved articles stored in the user corpus or extract limitations of the articles which address the proposal motivation or the method synthesis workflow extracts approaches used to solve similar or sub problems from the retrieved articles. For these retrieval augmented tasks through proper prompt engineering, we ensure that the answers are provided by restricting the knowledge to the retrieved context only. We observe this helps to mitigate hallucinations. (ii) There are components of the workflows which rely on parametric knowledge of LLMs, for example the motivation validation involves re-writing the proposal and the method synthesis involves generating similar sub problems for the problem defined in the proposal and synthesizing methods. For these tasks the output can not be restricted to the provided input. In such cases, there is a higher chance of hallucinated outputs. For such scenarios, we ensure mitigation of hallucinated outputs, by keeping the system semi-automated and allowing user-interactions at every step to edit or delete hallucinated outputs. Moreover at every stage of the workflow, the LLM agents are asked to justify their outputs and the provided justification is exposed to the researcher through the interface. This forces the model to apply Chain-of-Thoughts (COT) (Wei et al. 2022) and allows the researcher to validate the output and check if it is in sync with the justification provided. This assists in alleviating the effect of hallucinations.</p>
<p>Two-stage aspect based retrieval: The global corpus contains a large number of scientific articles stored with the Specter embedding of the title and abstract of the papers. The title and abstract of the papers contains information about motivation and problem statement of the papers and a high level mention of the methodology and the results. For ideation we require more in-depth information from the papers across various aspects such as methodology, limitations, etc. To achieve this we perform retrieval in two stages. In motivation validation workflow, we first retrieve top-K papers from the global corpus with the proposal as the query and high value of K for good recall. This allows us to have a set-of papers with similar motivation and problem statement to that of the proposal. These papers are chunked and stored in the user corpus for further aspect based retrieval, such as papers with similar motivation to that of the proposal and paper paragraphs mentioning the research gaps of these papers. In method synthesis workflow, we first retrieve top-K papers from the global corpus with similar sub problem statements as the query and high value of K for good recall. This allows us to have a set-of papers with problems similar to the problem described in the proposal or similar to any of the sub-tasks of the problem described in the proposal. These papers are chunked and stored in the user corpus for further aspect based retrieval such as extracting the approaches of the papers. Note that keeping high-recall for the first stage of retrieval ensures coverage of papers, whereas for the second stage we favor more precise outcomes for aspect based retrieval.</p>
<p>Introduction of Unanswerability: The output of aspect based retrieval is always top-K paragraphs from the retrieved and chunked papers. We keep the value of K low to get more
precise retrieval for the given aspect based query. However, there is a possibility that the retrieved paragraphs do not have the answer to the query (the query is unanswerable). For example, in the motivation validation workflow the retrieved paragraphs from the papers do not answer the question of whether the paper addresses a specific motivation of the proposal and does not specify the limitations of the paper which would help to refine the problem defined in the proposal. Similarly, for the method synthesis workflow the retrieved paragraphs may not have an approach to solve a similar problem. In such cases, the LLM based agents check the relevancy of retrieved paragraphs for the given query and identifies the query as 'unanswerable' in case if all the retrieved paragraphs are irrelevant, avoiding irrelevant outputs. Allowing unasnwerability also assists in reduction of hallucinations.</p>
<h2>Qualitative Analysis of the Workflows</h2>
<p>In the absence of an appropriate dataset for the tasks relevant to the ideation process, we provide a qualitative analysis of the workflows with 3 proposals from distinct researchers, specifically in the domain of Artificial Intelligence (AI), Machine Learning (ML) and Natural Language Processing (NLP). The topics of these proposals are: (i) Datasets for Computational Study of Peer Reviews (ii) Topic-based citation retrieval for research proposal and (iii) Reference-free evaluation metric for retrieval augmented question answering These researchers are from our lab, working on distinct research problems, for which they intend to write proposals. We use Semantic Scholar data fetched using S2ORC dataset (Lo et al. 2020) as our global repository, which has a variety of papers in the AI, ML and NLP domain. We utilize the logging functionality of 'Acceleron' to keep track of the interactions between the researcher and the LLM Agents and derive the following observations.</p>
<p>The abstract of the proposal with topic 'Dataset for Computational Study of Peer Reviews' is: 'Peer review constitutes a core component of scholarly publishing and demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed datasets and multidomain corpora prevent the systematic study of NLP for peer review. To remedy this, we plan to introduce an ethically sourced multi-domain corpus of papers and review reports from five different venues.'. With the proposal topic and abstract as an input, the motivation validation workflow generates the question: 'Is the research paper addressing the lack of clearly licensed datasets for studying natural language processing for peer review?'. As a part of this workflow, to validate the motivation of the proposal, the Colleague LLM agent poses this question on the chunks of top-50 scientific articles, retrieved to be similar to the proposal topic and abstract and stored in the user corpus. Out of these 50 scientific articles, 5 articles are retrieved with 'yes' as a answer, out of which the researcher agrees to the justification provided for four and disagrees with the justification of one. Following are the 4 research articles with a valid justification: (i) NLPeer: A Unified Resource for the Computational Study</p>
<p>of Peer Review (ii) A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications (iii) Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach (iv) MOPRD: A multidisciplinary open peer review dataset. Whereas, the article with title 'What Can We Do to Improve Peer Review in NLP?' agrees with the motivation of the proposal, however, does not address the same. As the next part of the motivation validation workflow, the colleague LLM agent extracts the following research gaps from the scientific articles: (i) NLPeer: A Unified Resource for the Computational Study of Peer Review: (a) The paper does not contain blind reviewing data which is a standard practice in most research fields, (b) The paper does not perform extensive hyper parameter search and tuning of the models, which could limit the effectiveness of the models, (c) The paper acknowledges the risk of "lazy reading" where reviewers might only read the paragraphs suggested by the model, but does not provide a solution to prevent this. (ii) A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications: (a) The models used in the research are relatively simple, which may limit their effectiveness in complex peer review scenarios, (b) The paper leave room for further research in areas such as demographic biases in accept/reject decisions, (c) The research does not provide a multidomain corpus of papers and review reports from different venues, limiting the scope of its application. (iii) Investigating Fairness Disparities in Peer Review, A Language Model Enhanced Approach: (a) The research paper does not draw any causal conclusions from the fairness analysis, limiting the depth of understanding about the underlying mechanisms of bias in peer review, (b) The research paper does not investigate the effect of rebuttals in the peer review process, which could be a significant factor in the final decision on a paper, (c) The research paper does not provide a thorough analysis of the continual learning problem in Pretrained Language Models (PLMs), which could be crucial for improving automatic review generation. (iv) MOPRD: A multidisciplinary open peer review dataset: (a) The research paper acknowledges that the multidisciplinary bias of their method could be a concern, indicating a need for more balanced datasets across various disciplines, (b) The paper points out that the input limits still exist as the maximum length of the input text only reaches 16,384 tokens, indicating a need for models that can handle larger input sizes, (c) The paper mentions that the performance of attention mechanisms will be compromised as the input length increases, indicating a need for more efficient attention mechanisms or alternative models for handling long input sequences. This showcases the quality of outputs provided by the workflow in terms of identifying the research gaps in the existing articles, partially eliminating the need of detailed literature survey of these articles to find gaps and thus alleviating research efforts.</p>
<p>We further observe that the researcher selects a subset of these research gaps, she finds to be relevant. The mentor LLM agent further uses these selected gaps to re-write the proposal. Following is the updated proposal provided as an outcome of the workflow taking into account the selected research gaps. The current literature presents several
gaps that motivate the need for a more comprehensive approach to the application of Natural Language Processing (NLP) in peer review. The absence of blind reviewing data, a standard practice in most research fields is a significant limitation. The lack of a multidomain corpus of papers and review reports from different venues restricts the scope of its application. The difficulty of studying the review process compared to the decision process, the unexplored effect of rebuttals in the peer review process indicate a need for more sophisticated methods or tools. The multidisciplinary bias of the method, the input limits of Language Models, and the compromised performance of attention mechanisms as the input length increases, all suggest a need for improved NLP models for peer review, more balanced datasets across various disciplines, and models that can handle larger input sizes and long input sequences. To address these gaps, we propose to introduce an ethically sourced multidomain corpus of papers and review reports from five different venues. The mentor LLM agent takes into consideration the following selected research gaps and introduces them in the revised version of the proposal: (i) availability of multi-disciplinary, multi-venue, blind-review corpus, (ii) no studies of effect of rebuttals (iii) context limits of LMs to tackle long reviews of existing work in this domain. We further observe that the researcher makes a very few edits to the the revised proposal abstract and accepts the same. The total time required for validating the motivation and updating the proposal abstract accordingly is substantially ( $\sim 5 \mathrm{x}$ for this proposal as mentioned by the researcher) reduced by this workflow.</p>
<p>We receive an input from another researcher with a proposal titled 'Topic-based citation retrieval for research proposal' and the corresponding abstract 'Retrieval of research articles pertinent to a given query represents a thoroughly investigated research challenge. Typically, queries take the form of a title and abstract of a research article, or a specific sentence or paragraph from an existing research article requiring citation. However, existing approaches presuppose the availability of a well-constructed manuscript, an assumption that is inappropriate during the initial research proposal writing stage. At this initial phase, researchers seek pertinent literature for citing in their proposals, often focusing on specific topics or intents and further build the proposal. In this work, we aim to tackle the issue of topicbased citation retrieval for research proposals. We anticipate researchers providing the title and abstract of their research proposals, encompassing elements such as the research gap, problem statement, and a high-level overview of the proposed methodology and experiments. Additionally, researchers will furnish a list of topics for which relevant scientific articles need to be retrieved. Our proposed algorithm intends not only to fetch research articles pertinent to the given proposal from a corpus, but also to establish a crucial many-to-many mapping between these articles and the specified topics.' The colleague LLM agent generates the following questions for validation of the motivation: 1. Is the research paper specifically addressing the retrieval of research articles relevant to a topic of a research proposal? and 2. Is the research paper developing a technique to map research articles to specified topics in research proposals?. Out</p>
<p>of top-50 research articles used to validate the motivation of the proposal by posing the above mentioned questions, the following four got retrieved to be answering as 'yes' to at the least one of the above questions and thus invalidating the motivation behind the proposal: 1 Citation Recommendation: Approaches and Datasets 2. CitationIE: Leveraging the Citation Graph for Scientific Information Extraction 3. Content-Based Citation Recommendation and 4. unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network. However, the justifications provided for these papers highlight that paper no 1. and 3. introduce an approach for citation recommendations during the writing phase of the target manuscripts and not at the proposal writing stage. Also, scientific article 2. leverages contents of a target paper and citation graph to extract scientific information. The outcome of the scientific article 4.is a dataset which can be useful for the proposal, but does not address the task of 'topic-based citation retrieval for research proposal'. Thus, we observe that after evaluating the retrieved scientific articles claimed to be invalidating the proposal, the researcher disagrees with the justifications provided for each of the retrieved articles for addressing the motivation behind the proposal, hence validating the novelty of the proposal. This exemplifies the need as well as the effectiveness of the user interaction facility provided by the tool for the workflow. This example demonstrates acceleration of motivation validation stage of the research-life cycle ( $\sim 8 \mathrm{x}$ for this proposal as mentioned by the researcher), by eliminating the need for the researcher to manually go through multiple relevant research articles retrieved by generic or academic search engines to ensure that the literature does not have a solution for the specific problem the researcher is trying to address, leading to a time consuming process.</p>
<p>We receive input from another researcher with the proposal titled 'Reference-Free evaluation metric for Retrieval augmented question answering task' and the abstract 'We observe that questions with long answers on long documents do not have unique reference evidences (relevant paragraphs from the document) and answers. Rather, there is a distribution over reference answers, making expert based evaluation expensive and existing unique reference-based evaluation metrics inadequate. We also do not find any reference-free evaluation metric designed for evaluating retrieval augmented question answering task. Hence, this this work we propose to define this metric.'. The colleague LLM agent generates the following question to validate the motivation of the proposal: Is the research paper proposes a reference-free evaluation metric designed for evaluating retrieval augmented question answering tasks?. We observe that out of top-50 retrieved scientific articles relevant to the proposal none of the articles provides answer as 'yes' to the question, leading to retrieval of no paper which invalidates the motivation of the proposal. Manual analysis of the top50 retrieved articles performed by the researcher (as well as other relevant articles manually visited by the researcher) to evaluate this outcome of the workflow, substantiates the results.</p>
<p>For the next workflow of method synthesis for the above proposal, the mentor LLM agent generates following set of
research problems similar to the problem defined in the proposal: 1 Evaluating complex tasks where there is no unique correct answer or reference. 2. Designing evaluation metrics for tasks that involve retrieval and interpretation of large amounts of data. 3. Creating reference-free evaluation metrics for tasks where reference-based metrics are inadequate or impractical. 4. Assessing the quality of answers in tasks where the answers can be long and drawn from extensive documents. The mentor LLM agent also generates the following sub-tasks for the problem defined in the proposal: 1. Defining a new metric that can effectively evaluate retrieval augmented question answering tasks. and 2. Overcoming the inadequacy of existing unique reference-based evaluation metrics for questions with long answers on long documents. With these similar and sub-problems as queries, the colleague LLM agent first retrieves Top-10 similar scientific articles per statement (total 40 articles with some overlap as same article may be retrieved for multiple queries) and then poses the question that 'if the article provides a methodology or an approach to solve the above defined problem or sub-task'. The researcher receives total 17 scientific articles which answers 'yes' to the question along with a description of the methodology implemented for each of the above problems or task. Out of these papers, the researcher accepts the following 11 scientific articles, finding them to be more relevant to the problem the researcher is trying to address: 1. AVA: an Automatic eValuation Approach to Question Answering Systems. 2.Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. 3. Re-visiting Automated Topic Model Evaluation with Large Language Models. 4. SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics. 5. Quantified Reproducibility Assessment of NLP Results. 6. Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation. 7. A Critical Evaluation of Evaluations for Long-form Question Answering. 8. Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the Direct-Answer AI2 Reasoning Challenge. 9. More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering. 10. Hurdles to Progress in Long-form Question Answering. 11. A Critical Evaluation of Evaluations for Long-form Question Answering.</p>
<p>Based on approaches discussed in these scientific articles to solve the above enlisted similar problems and/or sub-tasks, the mentor LLM agent suggests the following 10 plausible methodologies for the proposal. Theses suggestions with the exact verbatim provided by the agent are as follows: 1. AVA (Automatic Evaluation Approach): This approach uses Transformer-based language models to encode the question, answer, and reference text. By measuring the similarity between the reference and an automatic answer, biased towards the question semantics, AVA can estimate the system accuracy. This approach can be used to evaluate retrieval augmented question answering tasks and overcome the inadequacy of existing unique referencebased evaluation metrics. 2. Informedness and Markedness Measures: These new probabilistic and information theoretic measures can be used to evaluate complex tasks where</p>
<p>there is no unique correct answer or reference. They can also be used to design evaluation metrics for tasks that involve retrieval and interpretation of large amounts of data. 3. Large Language Models (LLMs): LLMs can be used as automated metrics for evaluating tasks that involve retrieval and interpretation of large amounts of data. They can also be used to assess the quality of answers in tasks where the answers can be long and drawn from extensive documents. 4. SacreROUGE: This open-source library dedicated to the development of summarization evaluation metrics can be used to develop reference-free evaluation metrics for tasks where reference-based metrics are inadequate or impractical. 5. Quantified Reproducibility Assessment (QRA): This method, based on concepts and definitions from metrology, can be used to develop reference-free evaluation metrics for tasks where reference-based metrics are inadequate or impractical. 6. RoSE Benchmark: This benchmark can be used to provide a more robust evaluation across different tasks. It can also be used to develop reference-free evaluation metrics for tasks where reference-based metrics are inadequate or impractical. 7. GENIE6: This human-in-theloop leaderboard framework can be used to score answers in tasks where the answers can be long and drawn from extensive documents. 8. Sparse Attention and Contrastive Retriever Learning: This system conditions answer generation on identified documents and uses a dense retriever trained by scaling up a distantly supervised algorithm. It can be used to overcome the inadequacy of existing unique referencebased evaluation metrics for questions with long answers on long documents. 9. Unified Evaluation Benchmark for Long-form Answers: This approach involves conducting a thorough study of evaluation, including both human and automatic evaluation protocols. It can be used to overcome the inadequacy of existing unique reference-based evaluation metrics for questions with long answers on long documents. 10. Training an LFQA Evaluation Metric Directly on Human-Annotated Preference Judgments: This approach involves fine-tuning pre-trained Language Models based on human judgement scores for the task. This output showcases the quality of method recommendations provided by the tool for the given proposal. Though mentioned at high-level, the researcher agrees that most of these methods are well-suited as a plausible approach for the given proposal. Though there is a need for further work to finalize the most appropriate plausible method for proposal, the researcher finds this first cut of output provided by the tool to be relevant and the overall process to be $\sim 10$ times more efficient than the regular process followed by the researcher for constructing a plausible set-of approaches for the given problem, by searching through the relevant literature from scratch.</p>
<p>These examples illustrating the outcomes of the motivation validation and method synthesis phases of the ideation workflow of the tool, demonstrates the efficacy of the tool, in terms of providing relevant outputs at each stage of the workflow. The observations made in terms of time saved by the researchers with the tool usage for the respective tasks demonstrates the power of the tool with regards to time efficiency gains.</p>
<h2>Conclusion</h2>
<p>In this work, we have demonstrated a tool called 'Acceleron', developed to accelerate the ideation phase of the research life-cycle. To the best of our knowledge this is the first tool which addresses the tasks involved in the ideation stage. To emulate the ideation process, we use LLM agents with colleague and mentor personas to execute two workflows, viz. motivation validation and method synthesis, which engage researchers in an interactive fashion to develop the research proposal. Our workflow involves novel components to (i) alleviate the hallucinations of LLMs, (ii) ensure relevant outcomes by two-stage aspect based retrieval, where first stage introduces higher recall reducing False Negatives and False Positives are corrected by user interaction, second stage of more precise finegrained aspect-based retrieval and introduction of unanswerability. The qualitative analysis performed with three proposals from distinct researchers, in the domain of Machine Learning and Natural Language Processing, demonstrates precise outcomes for various stages in the workflow with $\sim 7.5 \mathrm{x}$ gains in the time efficiency for various stages of the ideation phase.</p>
<h2>Future Works</h2>
<p>This is an ongoing work. We plan to augment the ideation functionality of the tool for other scientific domains such as life-sciences or material sciences, etc. We plan to emulate the domain specific aspects of the ideation process for these domains, which may differ from the current process specifically designed for Machine Learning and Artificial Intelligence based research projects in Computer Science. This would allow us to define a meta-process for ideation, which is domain independent and domain specific instances of this meta-process. Currently 'Acceleron' supports OpenAI models. We plan to augment the tool with Open-Source LMs such as Llama-2(Touvron et al. 2023), Zypher(Tunstall et al. 2023), etc. The logging functionality of 'Acceleron' keeps track of every input provided to the researcher as well as LLM agents and every output from them along with the corresponding timestamps. We are saving these logs for each user interactions for all the sessions. We plan to use these logs with treating user validated inputs as ground truth annotations, to develop a datasets for the ideation process. The logs would be used for developing datasets for tasks such as: (i) retrieval of research papers with similar motivation (ii) proposal re-writing with addressing researchgaps (iii) retrieval of research papers with similar problems and/or (iv) method-synthesis from a set-of relevant papers. The datasets will be used to instruction-tune the above mentioned Open-Source LMs, which can replace the existing LLMs yielding more cost-effective solution. We plan to extend the implementation of current ideation phase to generate a list of experiments to be performed for the problem defined in the proposal and the methodology selected by the researcher. This would lead to generation of a (set-of) results table(s) in a semi-automated fashion, with baseline approaches, planned experiments (ablations) and appropriate metric(s) used for evaluation.</p>
<h2>References</h2>
<p>Ali, Z.; Qi, G.; Kefalas, P.; Khusro, S.; Khan, I.; and Muhammad, K. 2022. SPR-SMN: scientific paper recommendation employing SPECTER with memory network. Scientometrics, 127: 6763-6785.
Ali, Z.; Qi, G.; Muhammad, K.; Kefalas, P.; and Khusro, S. 2021. Global citation recommendation employing generative adversarial network. Expert Syst. Appl., 180: 114888.
Berrebbi, D.; Huynh, N.; and Balalau, O. 2022. GraphCite: Citation Intent Classification in Scientific Publications via Graph Embeddings. Companion Proceedings of the Web Conference 2022.
Chen, X.; Alamro, H.; Mingzhe, L.; Gao, S.; Zhang, X.; Zhao, D.; and Yan, R. 2021. Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation. In Annual Meeting of the Association for Computational Linguistics.
Cohan, A.; Ammar, W.; van Zuylen, M.; and Cady, F. 2019. Structural Scaffolds for Citation Intent Classification in Scientific Publications. ArXiv, abs/1904.01608.
Cohan, A.; Feldman, S.; Beltagy, I.; Downey, D.; and Weld, D. S. 2020. SPECTER: Document-level Representation Learning using Citation-informed Transformers. ArXiv, abs/2004.07180.
D'Arcy, M.; Ross, A.; Bransom, E.; Kuehl, B.; Bragg, J.; Hope, T.; and Downey, D. 2023. ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews. arXiv preprint arXiv:2306.12587.
Dasigi, P.; Lo, K.; Beltagy, I.; Cohan, A.; Smith, N. A.; and Gardner, M. 2021. A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 4599-4610.
Dycke, N.; Kuznetsov, I.; and Gurevych, I. 2022. NLPeer: A Unified Resource for the Computational Study of Peer Review. ArXiv, abs/2211.06651.
Hayashi, H.; Kryscinski, W.; McCann, B.; Rajani, N.; and Xiong, C. 2020. What's New? Summarizing Contributions in Scientific Literature. ArXiv, abs/2011.03161.
Hu, Y.; and Wan, X. 2014. Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach. In Conference on Empirical Methods in Natural Language Processing.
Kasanishi, T.; Isonuma, M.; Mori, J.; and Sakata, I. 2023. SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation. In Annual Meeting of the Association for Computational Linguistics.
Kennard, N. N.; O’Gorman, T. J.; Sharma, A.; Bagchi, C.; Clinton, M.; Yelugam, P. K.; Das, R.; Zamani, H.; and McCallum, A. 2021. DISAPERE: A Dataset for Discourse Structure in Peer Review Discussions. ArXiv, abs/2110.08520.
Kunnath, S. N.; Pride, D.; and Knoth, P. 2023. Prompting Strategies for Citation Classification. Proceedings of the 32nd ACM International Conference on Information and Knowledge Management.</p>
<p>Lahiri, A.; Sanyal, D. K.; and Mukherjee, I. 2023. CitePrompt: Using Prompts to Identify Citation Intent in Scientific Papers. 2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL), 51-55.
Lauscher, A.; Ko, B. R.; Kuehl, B.; Johnson, S.; Jurgens, D.; Cohan, A.; and Lo, K. 2021. MultiCite: Modeling realistic citations requires moving beyond the single-sentence singlelabel setting. ArXiv, abs/2107.00414.
Lee, Y.; Lee, Y.; Lee, K.; Park, S.; Hwang, D.; Kim, J.; Lee, H. H.; and Lee, M. 2023. QASA: Advanced Question Answering on Scientific Articles. In International Conference on Machine Learning.
Liang, W.; Zhang, Y.; Cao, H.; Wang, B.; Ding, D.; Yang, X.; Vodrahalli, K.; He, S.; Smith, D.; Yin, Y.; McFarland, D. A.; and Zou, J. 2023. Can large language models provide useful feedback on research papers? A large-scale empirical analysis. ArXiv, abs/2310.01783.
Liu, R.; and Shah, N. B. 2023. ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. ArXiv, abs/2306.00622.
Lo, K.; Wang, L. L.; Neumann, M.; Kinney, R.; and Weld, D. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J., eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 4969-4983. Online: Association for Computational Linguistics.
Medic, Z.; and Snajder, J. 2023. Paragraph-level Citation Recommendation based on Topic Sentences as Queries. ArXiv, abs/2305.12190.
Mysore, S.; Cohan, A.; and Hope, T. 2021. Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity. ArXiv, abs/2111.08366.
Ostendorff, M.; Rethmeier, N.; Augenstein, I.; Gipp, B.; and Rehm, G. 2022. Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings. In Conference on Empirical Methods in Natural Language Processing.
Roman, M.; Shahid, A.; Khan, S.; Koubâa, A.; and Yu, L. 2021. Citation Intent Classification Using Word Embedding. IEEE Access, 9: 9982-9995.
Ruggeri, F.; Mesgar, M.; and Gurevych, I. 2022. ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers. ArXiv, abs/2202.06690.
Saikh, T.; Ghosal, T.; Mittal, A.; Ekbal, A.; and Bhattacharyya, P. 2022. ScienceQA: a novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23: 289 - 301.
Singh, A.; D'Arcy, M.; Cohan, A.; Downey, D.; and Feldman, S. 2022. SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. ArXiv, abs/2211.13308.
Touvron, H.; Martin, L.; Stone, K. R.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D. M.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn,</p>
<p>A. S.; Hosseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I. M.; Korenev, A. V.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. ArXiv, abs/2307.09288.
Tunstall, L.; Beeching, E.; Lambert, N.; Rajani, N.; Rasul, K.; Belkada, Y.; Huang, S.; von Werra, L.; Fourrier, C.; Habib, N.; Sarrazin, N.; Sanseviero, O.; Rush, A. M.; and Wolf, T. 2023. Zephyr: Direct Distillation of LM Alignment. arXiv:2310.16944.
Wang, C.; Liu, X.; Yue, Y.; Tang, X.; Zhang, T.; Jiayang, C.; Yao, Y.; Gao, W.; Hu, X.; Qi, Z.; Wang, Y.; Yang, L.; Wang, J.; Xie, X.; Zhang, Z.; and Zhang, Y. 2023a. Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. ArXiv, abs/2310.07521.
Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; ran Yang, H.; Zhang, J.; Chen, Z.-Y.; Tang, J.; Chen, X.; Lin, Y.; Zhao, W. X.; Wei, Z.; and rong Wen, J. 2023b. A Survey on Large Language Model based Autonomous Agents. ArXiv, abs/2308.11432.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; hsin Chi, E. H.; Xia, F.; Le, Q.; and Zhou, D. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. ArXiv, abs/2201.11903.
Wu, P.-C.; Yen, A.-Z.; Huang, H.-H.; and Chen, H.-H. 2022. Incorporating Peer Reviews and Rebuttal CounterArguments for Meta-Review Generation. Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management.
Zhang, Y.; Li, Y.; Cui, L.; Cai, D.; Liu, L.; Fu, T.; Huang, X.; Zhao, E.; Zhang, Y.; Chen, Y.; Wang, L.; Luu, A. T.; Bi, W.; Shi, F.; and Shi, S. 2023a. Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. ArXiv, abs/2309.01219.
Zhang, Y.; Wang, Y.; Wang, K.; Sheng, Q. Z.; Yao, L.; Mahmood, A.; Zhang, W. E.; and Zhao, R. 2023b. When Large Language Models Meet Citation: A Survey. ArXiv, abs/2309.09727.</p>
<h1>Appendix</h1>
<h2>Prompts for different stages of the Workflows</h2>
<h2>1. Motivation Extraction Prompt</h2>
<h2>System Message:</h2>
<p>You are a researcher and trying to understand the following proposal written by another researcher: ${$ proposal $}$</p>
<h2>Human Message:</h2>
<p>Describe in a bulleted list what is not addressed in the current literature which serves as the Motivation behind solving the above research problem proposed in the Proposal. Answer without a heading line and just the bullet points. Each bullet should mention one gap in the literature as a bullet point and not a sentence.</p>
<h2>2. Motivation Question Generation Prompt</h2>
<h2>System Message:</h2>
<p>You are a researcher and trying to understand the following proposal written by another researcher: ${$ proposal $}$</p>
<h2>Human Message:</h2>
<p>Describe in a bulleted list what is not addressed in the current literature which serves as the Motivation behind solving the above research problem proposed in the Proposal. Answer without a heading line and just the bullet points. Each bullet should mention one gap in the literature as a bullet point and not a sentence.</p>
<h2>AI Message:</h2>
<p>{motivation}</p>
<p>Human Message: Convert each of the above bullets in to a binary question. The question should begin with 'Is the research paper'.</p>
<h2>3. Ask Question for Motivation Validation Prompt</h2>
<h2>System Message:</h2>
<p>You are a researcher. You have been given a context, which are paragraphs from a research paper. You have been given a question. Answer the given Question in 'Yes' OR 'No' OR 'Unanswerable'. Answer solely based on the provided context of the research paper. If the question can not be answered with the facts mentioned in the available context or there is any ambiguity in answering the question answer as 'Unanswerable'.
Answer as 'Yes' only when the question can be very clearly answered considering the facts in the research paper provided in the context. Do not repeat the question as the part of the answer.
Provide a concise explanation about how the answer to the question is 'Yes' mentioning the paragraphs used in the context to answer it as 'Yes'. If the answer is 'No' or 'Unanswerable' only output that with NO description or elaboration.</p>
<p>Human Message:
Question: {question}
Research Paper Context: {paper.chunks}</p>
<h1>4. Extract Limitation Prompt</h1>
<h2>System Message:</h2>
<p>You are a researcher. You have been given the following proposal: {proposal}
A different research paper provided in the context already addresses the gap mentioned as the motivation behind the proposal.
${$ descriptions $}$
Human Message:
Research Paper: {paper_chunks}
Identify the limitations or gaps of this research paper which can serve as the new motivation for the proposal. Provide a bulleted list of limitations, where each bullet is concise. Answer WITHOUT a heading line and just the bullet points.</p>
<h2>5. Re-write Research Proposal Prompt</h2>
<h2>System Message:</h2>
<p>You are a researcher and have written a proposal: {proposal}</p>
<h2>Human Message:</h2>
<p>Re-write the proposal by taking into consideration the mentioned gaps in the current literature as the new motivation behind of the problem defined in the proposal.
Answer in a Single detailed paragraph WITHOUT any bullet points or list.
Gaps in the current literature: {limitations}</p>
<h2>6. Research Problem Extraction Prompt</h2>
<h2>System Message:</h2>
<p>You are a researcher and trying to understand the following proposal written by another researcher: {proposal}</p>
<p>Human Message:
What is the problem solved in the proposal?</p>
<h2>7. Similar Problem Generation Prompt</h2>
<h2>System Message:</h2>
<p>You are a researcher and trying to understand the following proposal written by another researcher: {proposal}</p>
<p>Human Message:
What is the problem solved in the proposal?
AI Message:
{problem_statement}
Human Message:
Give me a bulleted list of a more generalised or similar problems to the problem defined in the proposal. Don't give a heading just the answer in a bulleted list.</p>
<h1>8. Sub Problem Generation Prompt</h1>
<h2>System Message:</h2>
<p>You are a researcher and trying to understand the following proposal written by another researcher:
${$ proposal $}$
Human Message:
What is the problem solved in the proposal?
AI Message:
{problem_statement}
Human Message:
Provide a bulleted list of sub-problems or sub-tasks involved to solve the problem. Don't give a heading just the answer in a bulleted list.</p>
<h2>9. Similar and Sub Problem Question Creation Prompt</h2>
<h2>Human Message: <br> {statement}</h2>
<p>For the statement given above generate a question to be posed on a research paper to find out if the paper is proposing an approach or method to perform the task defined by the statement. Start the question with: 'Is the research paper proposing an approach or method to'.</p>
<h2>10. Methodology Extraction Prompt</h2>
<h2>System Message:</h2>
<p>You are a researcher and trying to answer the question posed on a research paper provided as the context.
Research Paper: {paper_chunks}</p>
<h2>Human Message:</h2>
<p>Answer the given Question in 'Yes' OR 'No' OR 'Unanswerable'. Answer solely based on the provided context of the research paper. If the question can not be answered with the facts mentioned in the available context or there is any ambiguity in answering the question, answer as 'Unanswerable'. Answer as 'Yes' only when the question can be very clearly answered considering the facts in the research paper provided in the context. Do not repeat the question as the part of the answer. If the answer to the question is 'Yes', provide detailed approach or methodology to perform the task. If the answer is 'No' or 'Unanswerable' only output that with NO description.</p>
<p>Question: ${$ question $}$</p>
<h2>11. Method Synthesis Prompt</h2>
<h2>System Message:</h2>
<p>You are a researcher and have been given a proposal and the research problem the proposal is trying to solve. You have been given the approaches in the literature trying to solve, similar problems and sub problems or sub tasks of the problem defined in the proposal. Your task is to synthesize and propose a possible set of methods or approaches to solve the problem defined in the proposal.
Proposal: ${$ proposal $}$
Research Problem in the Proposal: {problem}</p>
<h2>Human Message: <br> {method_context $}$</h2>
<p>Based on the above information suggest the top 3 possible methods or approaches to solve the problem defined in the proposal.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Copyright (c) 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ https://elicit.com
${ }^{2}$ https://raxter.io/
${ }^{3}$ https://typeset.io
${ }^{4}$ https://www.litmaps.com
${ }^{5}$ https://iris.ai
${ }^{6}$ https://inciteful.xyz
${ }^{7}$ https://researchrabbitapp.com
${ }^{8}$ https://www.connectedpapers.com
${ }^{9}$ https://scite.ai/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
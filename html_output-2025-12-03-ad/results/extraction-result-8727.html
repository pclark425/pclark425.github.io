<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8727 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8727</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8727</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271924343</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.11841v2.pdf" target="_blank">Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants</a></p>
                <p><strong>Paper Abstract:</strong> Significance Universities primarily evaluate student learning through various course assessments. Our study demonstrates that AI assistants, such as ChatGPT, can answer at least 65.8% of examination questions correctly across 50 diverse courses in the technical and natural sciences. Our analysis demonstrates that these capabilities render many degree programs (and their teaching objectives) vulnerable to potential misuse of these systems. These findings call for attention to assessment design to mitigate the possibility that AI assistants could divert students from acquiring the knowledge and critical thinking skills that university programs are meant to instill.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8727.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8727.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Reflect / Self-Critique Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Reflective / Self-Critique Prompting (reflective prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reflective prompting strategy where the LLM is given its own reasoning/answer and asked to detect inaccuracies and revise the reasoning/answer (generate-then-reflect). Used to refine a prior Chain-of-Thought output or an initial answer and produce a corrected final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two commercial OpenAI assistants evaluated in the paper: GPT-4 (described as the most performant system available to the authors) and GPT-3.5 (a less performant, freely available alternative). The paper does not report model sizes or training corpora and treats them as black-box API models.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Reflect / Self-Critique Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Provide the model with an initial answer and its Chain-of-Thought (CoT) reasoning, then ask it to check that reasoning for inaccuracies and, if found, to revise the reasoning and output a refined final answer. In open-answer cases the prompt asks the model to refine inaccuracies and answer from scratch without mentioning the edit step.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Answering university assessment questions (MCQ and open-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The models were evaluated on a bank of 5,579 textual university assessment questions (MCQs and open-answer) extracted from 50 courses across Bachelor's, Master's, and online programs at EPFL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports that self-reflect (reflective prompting) emerges as the best-performing strategy for open-answer questions for GPT-4 (relative ranking reported in Appendix G.2). Overall aggregated results: GPT-4 majority-vote across strategies = 65.8% average correct; maximum (any strategy) = 85.1% of questions answered correctly across at least one prompting strategy. The paper does not give a single per-method numeric for 'self-reflect' on the whole bank in the main text, but self-reflect is reported as the top open-answer strategy in the per-strategy comparison (Table 10 / Appendix G.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct zero-shot prompting baseline for GPT-4: 55.9% average accuracy (direct zero-shot); Chain-of-Thought zero-shot reported 65.1% (rationalized, non-reflective). These numbers serve as the non-reflective baselines reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: explicit prompt that asks the model to critique and revise its own prior CoT/answer (no special external modules or memory used). Implemented as an extra message in the conversation: {CoT prompt and model output} followed by an instruction to check accuracy and revise if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>The authors report that, among prompting strategies evaluated, self-reflect produced the best performance on open-answer questions for GPT-4 (Appendix G.2 / Table 10), i.e., reflective prompting outperformed zero-shot and some other rationalized strategies on open-answer items; overall aggregated metrics (majority/maximum) show higher performance when combining strategies, and the paper highlights that at least one strategy consistently produced correct answers in most cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes limitations: reflective outputs can be longer and include contextually relevant but unhelpful material; models still struggle on complex mathematical derivations and extended computations even with reflection; grading alignment issues (GPT-4 grader differs from human graders on some examples); the prompting strategies used were non-interactive and standardized (students could do more interactive refinement), so reported improvements may under- or over-estimate what is achievable in more iterative human-in-the-loop settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly against zero-shot, one-shot, expert prompting, chain-of-thought (zero-shot and few-shot), tree-of-thought, and metacognitive prompting. For open-answer questions self-reflect is reported as the top strategy; chain-of-thought variants were stronger for many MCQ tasks (four-shot CoT best for MCQs) while zero-shot did worst overall.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8727.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8727.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metacognitive Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Metacognitive Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that instructs the model to emulate human metacognition by (1) clarifying the question, (2) proposing relevant concepts and a preliminary answer, (3) critically assessing and adjusting that preliminary answer, and (4) confirming and explaining the final response.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metacognitive Prompting Improves Understanding in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box API LLMs (GPT-4 and GPT-3.5) evaluated on the EPFL assessment dataset; authors do not specify parameter counts or pretraining details.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Metacognitive Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single-prompt, multi-step metacognitive procedure embedded in the instruction: the model must (1) clarify its understanding, (2) identify relevant concepts and produce a preliminary answer, (3) evaluate and if necessary re-assess the preliminary answer, and (4) confirm and present the final answer. The reflection is performed within one prompt (internal self-check and revision) rather than as a separate generate-then-revise API call loop.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Answering university assessment questions (MCQ and open-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same EPFL dataset of 5,579 exam/assignment questions across 50 courses, in English and French; both MCQ and open-answer formats were used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported as one of the most effective strategies: metacognitive prompting ranks among top-performing strategies (especially for open-answer questions) in per-strategy comparisons (Appendix G.2 / Table 10). The paper used metacognitive prompting in human grading comparisons and found it among the best strategies evaluated. Exact per-strategy percentages appear in the appendix tables (the main text states metacognitive prompting is among the most effective).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to zero-shot direct prompting (55.9% average for GPT-4) and chain-of-thought zero-shot (65.1%), metacognitive prompting outperforms simple zero-shot in the aggregated analyses; precise numeric deltas per strategy are reported in Appendix G.2/Table 10 rather than the paper body.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: a structured single-prompt that enforces internal multi-step reasoning (clarify, propose, evaluate, confirm). No external memory or auxiliary modules; implemented as the user/system prompt delivered to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Authors report that metacognitive prompting is among top-performing strategies and that it was used in human-grader comparisons; the paper states metacognitive prompting is one of the most effective strategies across courses (Appendix G.2) and human graders evaluated model outputs generated with metacognitive prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The method remains a single-pass prompt (non-interactive) in this study; authors note the broader experimental setup is constrained to non-interactive, published strategies, so more iterative/human-in-loop metacognitive procedures were not explored. Also, models still fail on multi-step mathematical derivations, and metacognitive answers can be verbose and sometimes fail to solve the hardest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to CoT, self-reflect, tree-of-thought, expert prompting, and zero-shot. Metacognitive prompting is among the best strategies; for MCQs four-shot CoT performed best, while for open-answer self-reflect and metacognitive were top-ranked.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8727.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8727.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rationalized prompting strategy that instructs LLMs to generate intermediate reasoning steps ('let's think step by step') before giving a final answer; implemented in zero-shot and few-shot (four-shot) variants in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 and GPT-3.5 as black-box API LLMs; paper reports comparative performance between CoT and other prompting strategies but does not disclose model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Elicit an explicit sequence of intermediate reasoning steps prior to the final answer. Implemented both in zero-shot (instruction only) and few-shot (four demonstrations of rationale) modes; the few-shot demonstrations were generated using GPT-4 and curated by domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Answering university assessment questions (MCQ and open-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The same EPFL dataset; CoT was applied to both MCQs and open-answer questions, with demonstrations selected from similar course clusters and question types.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Chain-of-Thought zero-shot (i.e., asking for a reasoning chain before the answer) gave GPT-4 an average accuracy of 65.1% (reported in main text). The few-shot/four-shot CoT variant is reported as the best-performing strategy for MCQs (Appendix G.2 / Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct zero-shot (no CoT) baseline for GPT-4: 55.9% average accuracy. Thus CoT zero-shot improved over direct zero-shot in the reported aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to force step-by-step reasoning; few-shot CoT uses curated example rationales (generated by GPT-4 and corrected/selected by domain experts) included in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Direct quantitative evidence: CoT zero-shot for GPT-4 = 65.1% vs direct zero-shot 55.9% (main text). Few-shot CoT (four-shot) is cited as the best strategy for MCQs in the per-strategy comparison (Appendix G.2/Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes CoT is sensitive to incorrect reasoning steps (a single incorrect step can mislead the final answer) and that it can produce long justifications which mention incorrect alternatives; CoT helps but does not fully remedy failures on complex multi-step mathematical derivations and numerical calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with zero-shot, few-shot, tree-of-thought, metacognitive and reflective methods; CoT improves over zero-shot, and few-shot CoT is best for MCQs, while reflective strategies outperform CoT on open-answer in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8727.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8727.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought (ToT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative reasoning prompting approach that simulates multiple expert reasoning paths, has them critique each other, allows backtracking, and continues until experts reach consensus on the most likely answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated as one of the prompting strategies in the paper; the authors implemented a ToT-style prompt that instructs the model to emulate three experts who iteratively generate and critique reasoning paths until agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tree-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the model to simulate three experts who each produce a step of reasoning, critique others' steps, backtrack when flaws are found, and iterate until consensus; designed to mitigate CoT sensitivity to single reasoning errors by enabling exploration and critique of multiple reasoning branches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Answering university assessment questions (MCQ and open-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied to the EPFL assessment dataset as one of the rationalized prompting strategies; used the described multi-expert iterative critique prompt per question.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>The paper includes Tree-of-Thought as one of the evaluated strategies in per-strategy comparisons (Appendix G.2 / Table 10). The paper does not highlight ToT as the single best strategy overall; specific per-task numeric results are reported in Appendix tables rather than the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to non-ToT baselines (zero-shot direct 55.9% and CoT zero-shot 65.1% for GPT-4), ToT is reported as an advanced rationalized prompting variant but the paper does not assert a uniform numeric improvement across the full dataset in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering that instructs the model to simulate multiple expert agents, generate multiple reasoning branches, critique and backtrack (implemented entirely via prompts and multi-message API calls, no external search or module).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Tree-of-Thought is included in the strategy set and compared across courses; the paper motivates ToT as an approach to address CoT sensitivity to single-step errors and includes it in empirical comparisons (details in Appendix G.2 / Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper does not report specific ToT failure modes in the main text, but general limitations apply: models still fail on complex symbolic/mathematical derivations and the authors remark that reasoning-based prompts can produce long but ultimately incorrect answers; ToT can be resource- and context-length intensive and was implemented within the study's single-pass, standardized evaluation framework (non-interactive).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against CoT, metacognitive, self-reflect, expert prompting, and direct prompting. CoT and ToT are grouped as rationalized prompting; ToT intended to improve on CoT's sensitivity to errors but the paper reports per-strategy performance in appendices rather than claiming a dominant win for ToT across the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Refine: Iterative Refinement with Self-Feedback. <em>(Rating: 2)</em></li>
                <li>Self-Critique Prompting with Large Language Models for Inductive Instructions. <em>(Rating: 2)</em></li>
                <li>Metacognitive Prompting Improves Understanding in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8727",
    "paper_id": "paper-271924343",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-Reflect / Self-Critique Prompting",
            "name_full": "Self-Reflective / Self-Critique Prompting (reflective prompting)",
            "brief_description": "A reflective prompting strategy where the LLM is given its own reasoning/answer and asked to detect inaccuracies and revise the reasoning/answer (generate-then-reflect). Used to refine a prior Chain-of-Thought output or an initial answer and produce a corrected final answer.",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback.",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5",
            "model_description": "Two commercial OpenAI assistants evaluated in the paper: GPT-4 (described as the most performant system available to the authors) and GPT-3.5 (a less performant, freely available alternative). The paper does not report model sizes or training corpora and treats them as black-box API models.",
            "reflection_method_name": "Self-Reflect / Self-Critique Prompting",
            "reflection_method_description": "Provide the model with an initial answer and its Chain-of-Thought (CoT) reasoning, then ask it to check that reasoning for inaccuracies and, if found, to revise the reasoning and output a refined final answer. In open-answer cases the prompt asks the model to refine inaccuracies and answer from scratch without mentioning the edit step.",
            "task_name": "Answering university assessment questions (MCQ and open-answer)",
            "task_description": "The models were evaluated on a bank of 5,579 textual university assessment questions (MCQs and open-answer) extracted from 50 courses across Bachelor's, Master's, and online programs at EPFL.",
            "performance_with_reflection": "Paper reports that self-reflect (reflective prompting) emerges as the best-performing strategy for open-answer questions for GPT-4 (relative ranking reported in Appendix G.2). Overall aggregated results: GPT-4 majority-vote across strategies = 65.8% average correct; maximum (any strategy) = 85.1% of questions answered correctly across at least one prompting strategy. The paper does not give a single per-method numeric for 'self-reflect' on the whole bank in the main text, but self-reflect is reported as the top open-answer strategy in the per-strategy comparison (Table 10 / Appendix G.2).",
            "performance_without_reflection": "Direct zero-shot prompting baseline for GPT-4: 55.9% average accuracy (direct zero-shot); Chain-of-Thought zero-shot reported 65.1% (rationalized, non-reflective). These numbers serve as the non-reflective baselines reported in the paper.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: explicit prompt that asks the model to critique and revise its own prior CoT/answer (no special external modules or memory used). Implemented as an extra message in the conversation: {CoT prompt and model output} followed by an instruction to check accuracy and revise if needed.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "The authors report that, among prompting strategies evaluated, self-reflect produced the best performance on open-answer questions for GPT-4 (Appendix G.2 / Table 10), i.e., reflective prompting outperformed zero-shot and some other rationalized strategies on open-answer items; overall aggregated metrics (majority/maximum) show higher performance when combining strategies, and the paper highlights that at least one strategy consistently produced correct answers in most cases.",
            "limitations_or_failure_cases": "Paper notes limitations: reflective outputs can be longer and include contextually relevant but unhelpful material; models still struggle on complex mathematical derivations and extended computations even with reflection; grading alignment issues (GPT-4 grader differs from human graders on some examples); the prompting strategies used were non-interactive and standardized (students could do more interactive refinement), so reported improvements may under- or over-estimate what is achievable in more iterative human-in-the-loop settings.",
            "comparison_to_other_methods": "Compared directly against zero-shot, one-shot, expert prompting, chain-of-thought (zero-shot and few-shot), tree-of-thought, and metacognitive prompting. For open-answer questions self-reflect is reported as the top strategy; chain-of-thought variants were stronger for many MCQ tasks (four-shot CoT best for MCQs) while zero-shot did worst overall.",
            "ablation_study_results": null,
            "uuid": "e8727.0",
            "source_info": {
                "paper_title": "Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Metacognitive Prompting",
            "name_full": "Metacognitive Prompting",
            "brief_description": "A prompting method that instructs the model to emulate human metacognition by (1) clarifying the question, (2) proposing relevant concepts and a preliminary answer, (3) critically assessing and adjusting that preliminary answer, and (4) confirming and explaining the final response.",
            "citation_title": "Metacognitive Prompting Improves Understanding in Large Language Models.",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5",
            "model_description": "Black-box API LLMs (GPT-4 and GPT-3.5) evaluated on the EPFL assessment dataset; authors do not specify parameter counts or pretraining details.",
            "reflection_method_name": "Metacognitive Prompting",
            "reflection_method_description": "Single-prompt, multi-step metacognitive procedure embedded in the instruction: the model must (1) clarify its understanding, (2) identify relevant concepts and produce a preliminary answer, (3) evaluate and if necessary re-assess the preliminary answer, and (4) confirm and present the final answer. The reflection is performed within one prompt (internal self-check and revision) rather than as a separate generate-then-revise API call loop.",
            "task_name": "Answering university assessment questions (MCQ and open-answer)",
            "task_description": "Same EPFL dataset of 5,579 exam/assignment questions across 50 courses, in English and French; both MCQ and open-answer formats were used.",
            "performance_with_reflection": "Reported as one of the most effective strategies: metacognitive prompting ranks among top-performing strategies (especially for open-answer questions) in per-strategy comparisons (Appendix G.2 / Table 10). The paper used metacognitive prompting in human grading comparisons and found it among the best strategies evaluated. Exact per-strategy percentages appear in the appendix tables (the main text states metacognitive prompting is among the most effective).",
            "performance_without_reflection": "Compared to zero-shot direct prompting (55.9% average for GPT-4) and chain-of-thought zero-shot (65.1%), metacognitive prompting outperforms simple zero-shot in the aggregated analyses; precise numeric deltas per strategy are reported in Appendix G.2/Table 10 rather than the paper body.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: a structured single-prompt that enforces internal multi-step reasoning (clarify, propose, evaluate, confirm). No external memory or auxiliary modules; implemented as the user/system prompt delivered to the model.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Authors report that metacognitive prompting is among top-performing strategies and that it was used in human-grader comparisons; the paper states metacognitive prompting is one of the most effective strategies across courses (Appendix G.2) and human graders evaluated model outputs generated with metacognitive prompting.",
            "limitations_or_failure_cases": "The method remains a single-pass prompt (non-interactive) in this study; authors note the broader experimental setup is constrained to non-interactive, published strategies, so more iterative/human-in-loop metacognitive procedures were not explored. Also, models still fail on multi-step mathematical derivations, and metacognitive answers can be verbose and sometimes fail to solve the hardest problems.",
            "comparison_to_other_methods": "Compared to CoT, self-reflect, tree-of-thought, expert prompting, and zero-shot. Metacognitive prompting is among the best strategies; for MCQs four-shot CoT performed best, while for open-answer self-reflect and metacognitive were top-ranked.",
            "ablation_study_results": null,
            "uuid": "e8727.1",
            "source_info": {
                "paper_title": "Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) Prompting",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A rationalized prompting strategy that instructs LLMs to generate intermediate reasoning steps ('let's think step by step') before giving a final answer; implemented in zero-shot and few-shot (four-shot) variants in the study.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5",
            "model_description": "GPT-4 and GPT-3.5 as black-box API LLMs; paper reports comparative performance between CoT and other prompting strategies but does not disclose model sizes.",
            "reflection_method_name": "Chain-of-Thought (CoT) Prompting",
            "reflection_method_description": "Elicit an explicit sequence of intermediate reasoning steps prior to the final answer. Implemented both in zero-shot (instruction only) and few-shot (four demonstrations of rationale) modes; the few-shot demonstrations were generated using GPT-4 and curated by domain experts.",
            "task_name": "Answering university assessment questions (MCQ and open-answer)",
            "task_description": "The same EPFL dataset; CoT was applied to both MCQs and open-answer questions, with demonstrations selected from similar course clusters and question types.",
            "performance_with_reflection": "Chain-of-Thought zero-shot (i.e., asking for a reasoning chain before the answer) gave GPT-4 an average accuracy of 65.1% (reported in main text). The few-shot/four-shot CoT variant is reported as the best-performing strategy for MCQs (Appendix G.2 / Table 10).",
            "performance_without_reflection": "Direct zero-shot (no CoT) baseline for GPT-4: 55.9% average accuracy. Thus CoT zero-shot improved over direct zero-shot in the reported aggregate.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering to force step-by-step reasoning; few-shot CoT uses curated example rationales (generated by GPT-4 and corrected/selected by domain experts) included in the prompt.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Direct quantitative evidence: CoT zero-shot for GPT-4 = 65.1% vs direct zero-shot 55.9% (main text). Few-shot CoT (four-shot) is cited as the best strategy for MCQs in the per-strategy comparison (Appendix G.2/Table 10).",
            "limitations_or_failure_cases": "Paper notes CoT is sensitive to incorrect reasoning steps (a single incorrect step can mislead the final answer) and that it can produce long justifications which mention incorrect alternatives; CoT helps but does not fully remedy failures on complex multi-step mathematical derivations and numerical calculations.",
            "comparison_to_other_methods": "Compared with zero-shot, few-shot, tree-of-thought, metacognitive and reflective methods; CoT improves over zero-shot, and few-shot CoT is best for MCQs, while reflective strategies outperform CoT on open-answer in this study.",
            "ablation_study_results": null,
            "uuid": "e8727.2",
            "source_info": {
                "paper_title": "Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Tree-of-Thought (ToT) Prompting",
            "name_full": "Tree-of-Thought Prompting",
            "brief_description": "A deliberative reasoning prompting approach that simulates multiple expert reasoning paths, has them critique each other, allows backtracking, and continues until experts reach consensus on the most likely answer.",
            "citation_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models.",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5",
            "model_description": "Evaluated as one of the prompting strategies in the paper; the authors implemented a ToT-style prompt that instructs the model to emulate three experts who iteratively generate and critique reasoning paths until agreement.",
            "reflection_method_name": "Tree-of-Thought Prompting",
            "reflection_method_description": "Prompt the model to simulate three experts who each produce a step of reasoning, critique others' steps, backtrack when flaws are found, and iterate until consensus; designed to mitigate CoT sensitivity to single reasoning errors by enabling exploration and critique of multiple reasoning branches.",
            "task_name": "Answering university assessment questions (MCQ and open-answer)",
            "task_description": "Applied to the EPFL assessment dataset as one of the rationalized prompting strategies; used the described multi-expert iterative critique prompt per question.",
            "performance_with_reflection": "The paper includes Tree-of-Thought as one of the evaluated strategies in per-strategy comparisons (Appendix G.2 / Table 10). The paper does not highlight ToT as the single best strategy overall; specific per-task numeric results are reported in Appendix tables rather than the main text.",
            "performance_without_reflection": "Compared to non-ToT baselines (zero-shot direct 55.9% and CoT zero-shot 65.1% for GPT-4), ToT is reported as an advanced rationalized prompting variant but the paper does not assert a uniform numeric improvement across the full dataset in the main text.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering that instructs the model to simulate multiple expert agents, generate multiple reasoning branches, critique and backtrack (implemented entirely via prompts and multi-message API calls, no external search or module).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Tree-of-Thought is included in the strategy set and compared across courses; the paper motivates ToT as an approach to address CoT sensitivity to single-step errors and includes it in empirical comparisons (details in Appendix G.2 / Table 10).",
            "limitations_or_failure_cases": "Paper does not report specific ToT failure modes in the main text, but general limitations apply: models still fail on complex symbolic/mathematical derivations and the authors remark that reasoning-based prompts can produce long but ultimately incorrect answers; ToT can be resource- and context-length intensive and was implemented within the study's single-pass, standardized evaluation framework (non-interactive).",
            "comparison_to_other_methods": "Compared against CoT, metacognitive, self-reflect, expert prompting, and direct prompting. CoT and ToT are grouped as rationalized prompting; ToT intended to improve on CoT's sensitivity to errors but the paper reports per-strategy performance in appendices rather than claiming a dominant win for ToT across the dataset.",
            "ablation_study_results": null,
            "uuid": "e8727.3",
            "source_info": {
                "paper_title": "Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-Critique Prompting with Large Language Models for Inductive Instructions.",
            "rating": 2,
            "sanitized_title": "selfcritique_prompting_with_large_language_models_for_inductive_instructions"
        },
        {
            "paper_title": "Metacognitive Prompting Improves Understanding in Large Language Models.",
            "rating": 2,
            "sanitized_title": "metacognitive_prompting_improves_understanding_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.018158249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Could ChatGPT get an Engineering Degree? Evaluating Higher Education Vulnerability to AI Assistants
November 2023</p>
<p>Beatriz Borges 
Negar Foroutan 
Deniz Bayazit 
Anna Sotnikova 
Syrielle Montariol 
Tanya Nazaretzky 
Mohammadreza Banaei 
Alireza Sakhaeirad 
Alexandre Schpfer 
Andrej Janchevski 
Anja Tiede 
Clarence Linden 
Emanuele Troiani 
Francesco Salvi 
Freya Behrens 
Giacomo Orsi 
Giovanni Piccioli 
Hadrien Sevel 
Louis Coulon 
Manuela Pineros-Rodriguez 
Marin Bonnassies 
Pierre Hellich 
Puck Van Gerwen 
Sankalp Gambhir 
Solal Pirelli 
Thomas Blanchard 
Timothe Callens 
Toni Abi Aoun 
Calvino Yannick 
Yuri Alonso 
Cho 
Aleksandra Radenovic 
Alexandre Alahi 
Alexander Mathis 
Anne-Florence Bitbol 
Boi Faltings 
Ccile Hbert 
Devis Tuia 
Franois Marchal 
George Candea 
Jean-Cdric Chappelier 
Nicolas Flammarion 
Jean-Marie Frbringer 
Jean-Philippe Pellet 
Karl Aberer 
Lenka Zdeborov 
Marcel Salath 
Martin Jaggi 
Martin Rajman 
Mathias Payer 
Matthieu Wyart 
Michael Gastpar 
Michele Ceriotti 
Ola Svensson 
Olivier Lvque 
Paolo Ienne 
Rachid Guerraoui 
Robert West 
Sanidhya Kashyap 
Valerio Pi- Azza 
Viesturs Simanis 
Viktor Kuncak 
Volkan Cevher 
Akhil Arora 
Alberto Chiappa 
Antonio Sclocchi 
tienne Bruno 
Florian Hofhammer 
Gabriel Pescia 
Geovani Rizk 
Leello Dadi 
ManoelLucas Stoffl 
Horta Ribeiro 
Matthieu Bovel 
Yueyang Pan 
Could ChatGPT get an Engineering Degree? Evaluating Higher Education Vulnerability to AI Assistants
November 20234EDAC1CDAE5ED7BFD2ACF79052A198C2arXiv:2408.11841v2[cs.CY]LLMEducationLanguage Model HarmsGPT-4Educational Vulnerability
AI assistants are being increasingly used by students enrolled in higher education institutions.While these tools provide opportunities for improved teaching and education, they also pose significant challenges for assessment and learning outcomes.We conceptualize these challenges through the lens of vulnerability, the potential for university assessments and learning outcomes to be impacted by student use of generative AI.We investigate the potential scale of this vulnerability by measuring the degree to which AI assistants can complete assessment questions in standard university-level STEM courses.Specifically, we compile a novel dataset of textual assessment questions from 50 courses at EPFL and evaluate whether two AI assistants, GPT-3.5 and GPT-4 can adequately answer these questions.We use eight prompting strategies to produce responses and find that GPT-4 answers an average of 65.8% of questions correctly, and can even produce the correct answer across at least one prompting strategy for 85.1% of questions.When grouping courses in our dataset by degree program, these systems already pass non-project assessments of large numbers of core courses in various degree programs, posing risks to higher education accreditation that will be amplified as these models improve.Our results call for revising program-level assessment design in higher education in light of advances in generative AI.</p>
<p>Introduction</p>
<p>ChatGPT, a system using a large language model (LLM), GPT-3.5, as its foundation, was released in November 2022 to broad adoption and fanfare, reaching 100M users in its first month of use and remaining in the public discourse to this day.As arguably the most hyped AI system to date, its release has prompted a continuing discussion of societal transformations likely to be induced by AI over the next years and decades.Potential changes in modern educational systems have remained a core topic in this discussion, with early reports highlighting the risk of these AI systems as tools that would allow students to succeed in university coursework without learning the relevant skills those courses are meant to teach.Despite this concern, there has yet to be a comprehensive empirical study of the potential impact of LLMs (and derivative tools) on the assessment methods that educational institutions use to evaluate students.A few studies have explored the interesting sub-task of how well models perform on problems related to topics typically taught in many university courses and aggregated relevant question sets for this purpose [4,25,27,60,71].However, none of these works extrapolate these findings to assess the downstream impact of these tools on degree programs, where the risk of these technologies relative to their pedagogical benefits must actually be measured.</p>
<p>In this work, we conduct a large-scale study across 50 courses from EPFL to measure the current performance of LLMs on higher education course assessments.The selected courses are sampled from 9 Bachelor's, Master's, and Online programs, covering between 33% and 66% of the required courses in these programs.From these courses, we compile a bilingual dataset (English and French) of 5,579 textual open-answer and multiple-choice questions (MCQ).All questions were extracted from real exams, assignments, and practical exercise sessions used to evaluate students in previous years.The course distribution is presented in Figure 1, and the dataset statistics are shown in Table 1 (stratified by particular dataset attributes).</p>
<p>Using this dataset, we subsequently test two commonly-used models, , the system widely considered to be the most performant [70] among public AI assistants, 1 and GPT-3.5 [39], a highly performant freely available system.We generate responses from these systems using a range of prompting strategies [10,36,58,61,62,64,67], each of which varies in complexity, but all of which could easily be applied by a lay practitioner with minimal training in prompt engineering [49].We evaluate these systems using both automatic and manual grading, where manual grading of open-answer questions is performed by the same faculty staff that designed these problems and who have experience grading student answers to them.Subsequently, we conduct a detailed analysis of the generated outputs and their assessment results, considering factors such as the number of courses that would be passed, their distribution across university programs, as well as the effects of the question difficulty and language.</p>
<p>Our results show that AI systems are relatively capable of answering questions used in university assessments.GPT-4 responds correctly to 65.8% of questions when aggregating responses across the different prompting strategies using a simple majority vote (i.e., a knowledge-free setting that assumes the user would use this tool with no subject knowledge).Furthermore, across the eight prompting strategies, GPT-4 can generate at least one correct response for 85.1% of questions (maximum performance), indicating even greater assessment vulnerability in a setting where a user may have enough subject knowledge to select a correct answer even if they cannot produce it.This performance is relatively stable across courses in various scientific disciplines, impacting courses regardless of their subject and size.Importantly, we find that these results indicate that large numbers of university degree programs are highly vulnerable to these tools, with the non-project components of many core courses being passed in multiple degrees offered by our institution.</p>
<p>Finally, we observe that while these systems are capable of reaching passing grades in many university assessments, they struggle with more complex question types where students also tend to perform most poorly.Taken together, these results indicate a possibility that these systems could be used to achieve passing marks in university courses while circumventing the process by which students acquire basic domain knowledge and learn to extend it to more complex problems.We conclude with a discussion on mitigations to university assessment settings, an outlook on how university systems should adapt to the increased use of these tools, and a discussion of limitations of our study, specifically with respect to how it diverges from exact assessment and grading policies at our institution.</p>
<p>Data Collection</p>
<p>We compile a new dataset of assessment questions from 50 courses offered at our institution from both on-campus and online classes.Following data pre-processing and filtering steps, this dataset consists of a total bank of 5,579 textual multiple-choice (MCQ) and openanswer questions in both English and French.These questions span various levels (e.g., Bachelor, Master), and cover a broad spectrum of STEM disciplines, including Computer Science, Mathematics, Biology, Chemistry, Physics, and Material Sciences.Table 1 and Figure 1 provide an overview of the dataset's main statistics and the distribution of questions across different topics.Additionally, we have collected course-specific attributes such as the year when the course is first offered in our institution's degree programs (e.g., Master's year 1), the program designation (e.g., Physics), the language of instruction (e.g., French), and the average student enrollment over recent years.Finally, certain questions have been labeled by the instructor who designed the question with a subjective annotation of the question's difficulty.</p>
<p>Experimental Findings</p>
<p>In our study, we investigate the vulnerability of university programs to generative AI systems using our question bank of 5,579 evaluation questions from 50 courses.We consider two models, GPT-4 and GPT-3.5, selected due to their popularity and usage rates.GPT-4 is considered the most performant model among all publicly accessible LLMs but is only available through a premium subscription, impeding its use by many students.GPT-3.5 is a less performant alternative, but free to use.We generate responses to questions from these models using eight relatively easy-to-apply prompting methods (implementation details are described in Appendix B).For multiple-choice questions, we assess whether a response is correct by comparing the selected choice with the annotated correct answer option.For open-response questions, we use GPT-4 to rate the quality of the response on a four-point scale: Correct, Mostly Correct, Mostly Incorrect, Incorrect, which we map to scores of 1, 0.66, 0.33, and 0, respectively, for calculating performance. 2</p>
<p>Can LLM systems pass university-level courses?</p>
<p>We begin our analysis by assessing model performance in a setting where the user has zero knowledge about the question topic.In the simplest scenarios, where we use the most straightforward prompting strategies, such as directly asking a question (zero-shot) or asking the model to provide a reasoning chain before answering the question (chain-of-thought zero-shot), GPT-4 achieves average accuracies of 55.9% and 65.1%, respectively.However, if the user employed a slightly more complex zero-knowledge strategy, such as majority voting over the eight answers generated by the different prompting strategies, they would receive a correct answer to 65.8% (on average) of questions using GPT-4 (and 52.2% using GPT-3.5).We observe that this performance trend holds regardless of the language of the assessment, with English exhibiting slightly better results than French.Further experimental results for assessments in English and French are detailed in Appendix G. 4.</p>
<p>Beyond overall performance across the question bank, Figure 2 presents the proportion of passed courses for our sample of 50 courses based on varying passing thresholds.Alarmingly, GPT-4 can easily be used to reach a 50% performance threshold (which could be sufficient to pass many courses at various universities) for 89% of courses with MCQ-based evaluations and for 77% of courses for open-answer questions.While not performing quite as well, GPT-3.5, the freely available model, can reach a 50% threshold for 70% of courses with MCQ-based assessments and for 50% of courses with open-answer questions.As passing thresholds may not be set to 50% for all institutions, we vary this threshold and find that GPT-4 still passes 68% of courses at a 60% passing threshold, and 38% of courses at a 70% passing threshold for MCQ.Similar results are found for open-answer questions.</p>
<p>Our results suggest that users with no knowledge of a particular subject could solve enough assessment questions to pass a majority of the courses in our dataset, making a compelling argument that AI assistants could potentially augment student learning as support tools.However, they simultaneously present a credible short-term risk to educational systems if these systems are not adapted to protect against the misuse of these technologies.Finally, we expect this problem to only grow worse over time, as continual model improvements in the years to come will make these tools even more performant in academic contexts.</p>
<p>How do these results affect university programs?</p>
<p>The average performance across courses demonstrates each course's potential vulnerability to generative AI tools, which is particularly important if considerable portions of degree programs can be completed using these tools.To evaluate this program's vulnerability, we aggregate the questions in our datasets according to the study programs in which they are core courses.Specifically, we include four program types: 1st-year Bachelor, Full Bachelor, Full Master, and Online courses.We separate the first year of the Bachelor's degree because, at many institutions (including ours), the first year of the Bachelor's may have a fairly standardized curriculum that serves a special purpose (e.g., replacing or complementing entrance exams).Full Bachelor's and Master's correspond to regular Bachelor's and Master's programs.We also include online courses, as official certifications can often be awarded for completing a sequence of these courses.For each program, our dataset contains a sample of courses that cover from 33% to 66% of the required courses for that program.For more program statistics, see Appendix F.</p>
<p>We consider the same two aggregation strategies across the responses provided by the eight prompting methods: majority vote and maximum performance.For the majority vote, given the eight prompting strategies we have, the final answer to the question is the one that is the most frequent across all strategies.In the maximum performance aggregation, only a single prompting strategy is required to answer correctly for the model to be deemed correct in its response, approximating a pseudo-oracle setting that remains contextually realistic, as a user might be able to distinguish the answer when presented with it, even if they could not find it on their own.</p>
<p>In Table 2, we present the number of courses that would be passed by GPT-4 across the 9 tested degree programs for various course passing thresholds  (i.e., the performance that must be achieved to consider the course passed).Our results show that the general course vulnerability observed in the previous section extends to program vulnerability.At the  = 50% threshold for passing a course, at Bachelor's and 693 Master's questions, respectively, annotated using instructor-reported difficulty levels.(c) 207 questions annotated using Bloom's taxonomy by two researchers in the learning sciences.Across all categorization schemes, GPT-4 performance slightly degrades as the questions become more complex and challenging.Performance is aggregated by the majority vote strategy.Error bars represent 95% confidence intervals using the non-parametric bootstrap with 1000 resamples.</p>
<p>least 83% of courses are passed in each of the evaluated programs.</p>
<p>In certain programs, such as the Physics Bachelor and Computer Science Master, all tested courses are passed.While this number drops as we raise the passing threshold , the maximum performance for each program typically remains above 80%, indicating that a combination of clever prompting and partial subject knowledge may be sufficient to achieve high marks on assessment questions.Topically, we find that the models consistently exhibit lower performance on assessments of courses involving mathematical derivations.Conversely, the model demonstrates strong performance on problems that require generating code of text.For example, models consistently yield high performance in subjects such as Software Engineering and Intro to Machine Learning.This observation is further supported by the difference in performance between Master's and Bachelor's level courses (shown across Figures 3a and 3b).In our dataset, Bachelor's courses feature more mathematical derivations, while Master's courses have more text and code-based problems.In Appendix G.1, we provide further performance comparisons between the courses representing each program.In Appendix G.2, we analyze model performance across all prompting strategies and both question types.</p>
<p>Finally, we highlight that these models are effective in courses that large portions of the student body must take, increasing the overall vulnerability of course programs.Figure 4 demonstrates that some of the largest classes on campus, with upwards of 300 students, are also some of the most vulnerable, with GPT-4 achieving (using the majority vote strategy) an average performance of 69.9% in these classes (while hovering around 60% for other class sizes).This result is particularly problematic because larger courses are often limited in terms of the monitoring and mitigation strategies they can implement due to the number of students.While smaller courses may more easily be able to combat the misuse and unethical use of generative AI tools, larger courses, which are often mandatory for degree completion, must ensure fair and scalable solutions for a larger student population.are capable of solving, as we observe that the performance of these systems does decrease on more challenging assessment questions (Figure 3).We measure the difficulty using a sub-sample of our question bank that is annotated according two different categorizations of their difficulty: (1) instructor-reported question difficulty, a five-point difficulty categorization for Bachelor courses and twopoint for Master's courses provided by the course instructors, and (2) Bloom's taxonomy [9], a six-point scale that measures the cognitive complexity of a question. 3or the instructor-reported difficulty categorization, we collect annotations from course instructors for a subset of 376 questions from the Bachelor's program (n.b., the instructors that designed the questions).The instructor-reported rating ranges from "Very Easy" to "Very Hard" on a 5-point scale.We also collect 693 questions from the Master's program annotated on a 2-point scale, ranging from "Medium" to "Hard" (the original scale was meant to be 3-point, but no instructor reported an "Easy" question).In Figures 3a and 3b, we show the model's performance on questions stratified by their difficulty rating and observe that GPT-4 performs worse on questions that instructors deem harder.For example, in Bachelor courses, there is a 38% difference in accuracy between "Very Easy" and "Very Hard" questions.However, the differences between Bachelor's "Easy" and "Hard" questions or Master's "Medium" and "Hard" questions are only 11.5% and 15.75%, respectively.This pattern is also observed in our assessment of question difficulty performed using Bloom's taxonomy, which classifies educational learning objectives into levels of complexity and specificity: remember, understand, apply, analyze, evaluate, and create.Two researchers in the learning sciences manually annotated 207 questions from our dataset according to Bloom's taxonomy [9].While the taxonomy typically associates questions into six categories, we found that most course assessment questions were covered by the first four categories.The results, grouped by taxonomy category in Figure 3c, illustrate that GPT-4 performance is negatively correlated with the cognitive complexity of the question, with higher performance on lower-level tasks compared to analysis and application tasks.</p>
<p>However, harder assessments may not necessarily be a suitable solution for this vulnerability as they would also likely lead to lower student performance.When comparing the performance of students and GPT-4 on problem sets from a subset of questions for which student performance statistics were collected (Figure 5), we note that the model tends to excel on questions where students also perform well.This pattern perhaps exacerbates fairness as GPT-4 (and similar models) could be used to achieve average results on an assessment while providing few benefits to students who can already complete the easier portion of assessments but struggle with harder questions.Notably, however, we observe that for a subset of problems, the model typically struggles, receiving "Mostly Incorrect" or "Incorrect" marks, while students demonstrate relatively strong performance, with scores ranging from 0.5 to 0.9.These problems typically require mathematical derivations and extensive computations.</p>
<p>Discussion</p>
<p>Summary.In this work, we tested the ability of LLMs to solve assessment questions for a large number of courses from technical and natural sciences academic programs at EPFL.We find that LLMs are generally capable of answering 50-70% (depending on the model) of questions correctly given no subject-related knowledge, and up to 85.1% of questions correctly when some subject-specific knowledge is assumed (i.e., the ability to recognize the correct answer).Most importantly, when considering performance across programs, GPT-4 can achieve greater than 50% performance for 83% to 100% of courses (depending on the program) with an average program pass rate of 91.7%.While a higher per-course passing threshold of 70% would only result in 23% to 50% of courses being passed across our programs (with an average of 37%), this would also lead to higher student fail rates as passing thresholds would similarly affect them.Average student performance for a subset of 197 questions is computed and stratified along 10-point intervals from 0 to 100.The model's performance with the majority vote strategy is assessed by human graders using a 4-point scale.We observe the model typically answers correctly questions that students also excel at.However, there are questions on which the model struggles, but students perform reasonably well.</p>
<p>Given that continuous advancements in LLM technology will likely further improve these LLM performance numbers in the future, we conclude that higher-education assessment schemes are immediately vulnerable to student exploitation of generative AI tools, specifically in the engineering and natural sciences.</p>
<p>Assessment Vulnerability.Our results indicate that the broad availability of generative AI tools should urgently trigger discussion on the design and implementation of assessments.Naturally, our results must be placed in the context where they would normally be observed.In many educational institutions, student assessments are frequently closed-book, thereby precluding the direct use of generative AI tools.Many course assessments (e.g., assignments), though, are completed at home without supervision.In the same vein, most online courses typically evaluate students without any supervised, in-person assessment.In these unsupervised settings, the availability of generative AI tools for aiding in the completion of assessments poses risks for many commonly used student evaluation methods.</p>
<p>One general trend that we observe from our data (Figures 3a, 3b,  and 3c) is that models perform well on basic learning tasks, such as memorizing factual knowledge.In courses where memorization of factual knowledge is a core evaluation objective, students should not be allowed to use these tools in non-proctored settings, and these courses should perhaps return to traditional in-person examination [59].Barring this possibility, in the case of non-proctored assessments, we recommend that their design should not only consider the possibility of assistance from generative AI but actively assume its use.At the very least, assessments should be designed with generative AI in the loop to design AI-adversarial evaluation that remain fair to students.</p>
<p>At the same time, these findings provide an opportunity to improve and diversify student learning through assessments.Students acquire relevant skills when assessments emphasize analytical and applied knowledge settings [69].Rather than using proctored exams, then, or limited practical works, such as assignments, students should be evaluated using assessments requiring a more composite application of course concepts, such as larger class projects.Project settings more closely assess students on problems resembling realworld challenges, would provide students with more opportunities to make problem-solving decisions, such as problem simplification, decomposition, and planning [37], and would mitigate the impact of generative AI tools (Figure 3c).</p>
<p>Education Vulnerability.While our results point to an urgent need to mitigate assessment vulnerabilities in higher education, a longer-term view requires considering how education as a practice should evolve alongside the availability of generative AI tools.Since the release of ChatGPT, ongoing discussions have revolved around this topic with both negative and optimistic views.While numerous studies explore the ways AI can enhance learning, ethical concerns related to plagiarism, biases, and overreliance on technology have also been highlighted [3,12,16,18,30,42,66].</p>
<p>An important dimension of these discussions emphasizes the need to revisit evaluation procedures to ensure that students acquire necessary skills and critical thinking abilities in the face of AI integration [7,20,38,43].For instance, observations from various works (and our study) show that models excel in generating code to solve software problems [26,31,32,55,65].While this capability reduces the burden on professional (and hobbyist) software developers, it also poses a risk for learners by potentially offering shortcuts that impede the acquisition of fundamental coding and engineering skills [17].Coding tools such as GitHub's Copilot or OpenAI's Codex may lead novices to over-rely on auto-suggested solutions.This overreliance may diminish their engagement with computational thinking [7,43], which is arguably the most important skill that is learned in any computer science course or program.</p>
<p>Beyond this example, many studies underscore the significance of adapting course materials and assessments to promote critical thinking, encourage student collaboration, develop practical skills, enhance soft skills, and promote interdisciplinary learning, all with the aim of cultivating graduates equipped with a diverse range of competencies [2,11,15,38].In particular, reinforcing our conclusions above, open-ended assessments are proposed to promote originality and creativity, potentially discouraging reliance on generative AI tools and fostering unique ideas and critical analysis [15,33].Ultimately, these studies suggest the greater risk of generative AI may be its potential to enable the unintentional circumvention of frameworks by which learners are taught the foundations to learn later skills, and that teaching and assessment should be adapted for this risk.</p>
<p>Finally, assuming that students will use and become acquainted with the capabilities of these technologies, we recommend that students should not only be educated on the technical and ethical challenges of generative AI systems, but also on the critical thinking required to successfully engage with such tools [57].One such measure could involve establishing committees for ethical oversight and adding classroom discussions on the use of AI tools.Such discussions would clarify what constitutes plagiarism and address potential ethical concerns, ensuring that students understand the importance of academic integrity and discern the boundaries between legitimate assistance and academic misconduct [2,11,15,17,20,33].</p>
<p>Limitations</p>
<p>While our study offers preliminary insights into the vulnerability of degree programs to student use of AI assistants for assessments, we acknowledge several limitations in our study.</p>
<p>First, our study excluded any multimodal inputs, such as questions containing diagrams, figures, or graphs, which were omitted from our dataset.Approximately 57% of the initially collected data did not qualify for inclusion in the final dataset of 5,579 questions.Consequently, models were solely evaluated with text-only questions.This approach likely resulted in performance outcomes that are higher than what these models would attain when tested on question sets that include these other modalities, though we also note rapid growth in the multimodal capabilities of these models [68].</p>
<p>Simultaneously, our findings might underestimate the performance potential that students could attain through collaboration with these systems.Although we conducted a thorough examination of prompting strategies, our methods are limited by the fact that they (1) rely solely on published prompting strategies, (2) are generally noninteractive, and (3) are tailored for scalability across all questions to facilitate a comprehensive study.Students aiming to address individual questions could devote more time and devise more interactive, less standardized prompting strategies to collaboratively guide the models toward improved solutions.</p>
<p>Finally, we acknowledge certain gaps between our evaluation of AI systems in this study, and how students are normally evaluated in these courses.First, our study standardizes system evaluation across all course assessments, removing course-specific assessment policies for questions.For example, certain courses, beyond not giving points for correct answers to multiple-choice questions, might also penalize incorrect answers more than leaving a question unanswered, while our study simply gives zero points for incorrect answers.Second, our dataset of questions for each course is not necessarily balanced according to a course's grading rubric.As an example, our dataset may contain a balanced mixture of questions from assignments and exams for a particular course, while the overall evaluation of a student in this same course would compute their grade as a 10% mixture of assignments, and 90% mixture of exam questions.Similarly, many courses at our institution also include lab or project components as part of their final grade.Since these parts of the assessment do not have a "correct answer" that can be easily marked, they are not included in our dataset.</p>
<p>As we do not consider these course-specific assessment policies when computing the course pass rates of our tested AI assistants, these design decisions introduce a gap between our evaluation and the actual assessment rubrics by which students are graded in our institution's courses.Despite this divergence, however, we note that other institutions may implement course assessments and grading rubrics in different ways.As a result, while our study is not an exact simulation of our institution's diverse course assessment schemes, it remains a suitable testbed for providing insights into how course assessments are vulnerable to AI assistants, and how this vulnerability would extend to full university programs without mitigations.</p>
<p>Materials and Methods 6.1 Prompting Strategies</p>
<p>To generate answers to questions, we employ various prompting strategies requiring only familiarity with relevant literature and minimal adaptation.We selected eight distinct prompting strategies that we broadly categorize into three types: direct, rationalized, and reflective prompting.Under direct prompting, we use zero-shot, one-shot [10], and expert prompting [64], where models are directly prompted for an answer without encouraging any underlying rationale.For rationalized prompting, three strategies are implemented: zero-shot and four-shot chain-of-thought [62], and tree-of-thought [67] prompting.Here, language models are prompted to generate a rationale before providing the final answer.Lastly, reflective prompting includes selfcritique [36,58] and metacognitive prompting [61], where models are asked to reflect on a previously provided answer and adjust their response according to this reflection.In our experiments, we noted that the prompting strategy substantially influences model performance, with at least one strategy consistently producing the correct answer in the majority of cases.A detailed description of all prompting strategies, along with prompts, is provided in Appendix B.</p>
<p>Evaluation</p>
<p>Below, we outline the grading strategies used to evaluate the model's performance across two question types: multiple-choice (MCQ) and open-answer questions.For MCQ, grading is automated by comparing against the annotated answer.Answers receive a binary score of 0/1 if only one correct option exists, or a proportional score based on the number of correct choices made for multi-answer questions (with no penalty for wrong choices).Appendix C provides more details for grading MCQs.For open-answer questions, we constructed an evaluation pipeline using GPT-4 as a grader [70], which we describe below.For both types of results, we report error bars representing 95% confidence intervals (Figures 3, 4).These intervals were computed using the non-parametric bootstrap with 1000 resamples.We also tasked human experts with independently grading a subset of model responses to measure alignment between model and human grading, and establish a confidence level for model-based grading.</p>
<p>Evaluating Open-Answer Questions.A significant portion of the questions we extracted are open-answer questions, which are challenging to evaluate manually due to the sheer volume of responses (a total of 33,904 answers from 2,119 questions, answered by 2 models using 8 prompting strategies).As a result, we use a state-of-the-art LLM, GPT-4, as a grader.To automate the grading of open answers, we provide the model with the question, the correct solution from an answer sheet of the assessment, and the generated answer text, prompting it to assign a rating based on the quality of the response.We provide the model with a 4-point grading scale: Correct, Mostly Correct, Mostly Incorrect, Incorrect.The model is first tasked with assessing the accuracy and completeness of the answer before assigning the final grade.Although we do not use these interim accuracy and completeness scores, we manually observe that these stages enhance the quality of overall grading.The specific prompting strategy is detailed in Appendix C. 2. As an answer was produced for each question using eight distinct prompting strategies, we obtained eight different answers and corresponding grades.To present a cohesive performance score for both GPT-4 and GPT-3.5 for a given question, we employ two aggregation methods: (1) the maximum approach, which selects the answer with the highest grade for each question as a representation of model performance, and (2) the majority approach, which considers the grade that appears most frequently among the eight prompting strategies.As an example, for a hypothetical question whose generated answers for the eight prompting strategies received 2 Correct, 2 Mostly Correct and 4 Mostly Incorrect grades, the maximum grade would be Correct and the majority grade would be Mostly Incorrect.</p>
<p>Human Grading.To assess how well model grading aligned with human grading on open-answer questions, we enlisted 28 expert annotators from the teaching faculty of 11 courses to evaluate 933 questions.</p>
<p>The courses chosen for expert grading are listed in Appendix C.3.Specifically, we requested graders to assign scores to open-ended responses generated by GPT-4 and GPT-3.5.Human-graded responses for both models were generated using two prompting strategies: zeroshot chain-of-thought prompting [62] (a simple prompting method at the disposal of any student) and metacognitive prompting [61] (one of the most effective strategies across all courses).We anonymized the outputs to prevent graders from knowing which model and prompting strategy they were evaluating.To maintain consistency, we instructed graders to use the same grading scale as GPT-4 direct grading.The number of graders per course varied from 1 to 10, and a total of 3732 answers were evaluated.On average, graders spent approximately 5 minutes assessing each answer.Figure 6 indicates a general alignment between human graders and GPT-4 when categorizing answers into a simplified correct/incorrect quadrant.Out of the examples identified as Correct by graders, the model assigned the same grade to 61% of them.Similarly, for examples graded as Almost Correct by graders, the model's grade matched in 36% of cases.Additionally, in instances where graders labeled examples as Mostly Incorrect, the model's grade aligned with the grader's assessment 65% of the time.However, we note certain discrepancies.For instance, GPT-4 tends to avoid explicitly labeling solutions as Incorrect, instead opting for Mostly Incorrect (i.e., in 74% of cases that humans annotated a solution as Incorrect, the model identified it as Mostly Incorrect), potentially due to the practice of aligning models for harmlessness [6].We find a few instances where the model rates an answer as Correct while humans assign a lower score.</p>
<p>Interestingly, upon comparing average grades assigned by human graders and GPT-4 across 11 courses, we find a difference in average grade of only 2.75%.However, we observe variations between courses, with an average course grade deviation of 8.5% (and the largest deviation for a course being 26%) between human and model graders.Finally, we also note the performance correlation between MCQ and open-answer questions in Figure 2, providing a comparison point for the rationality of our model-based open-answer grading results.While scores for open-answer questions are typically lower than MCQ, the patterns exhibited by both curves are similar across both models.Overall, we note that the grades provided by humans and models are moderately correlated and that the summary statistics across courses tend to have a high correlation.Further details can be found in Appendix C.</p>
<p>A Dataset Collection</p>
<p>Data Statement Our data collection was approved by an institutional review board.Data was voluntarily submitted by members of the Data Consortium and no materials were used without the permission of the data owner.</p>
<p>Data Preprocessing To preprocess our data, we collect assessments from participating faculty, extract questions and answers from these assessments, and standardize them into a uniform format.After compiling an initial question bank from the raw data, we filter unsuitable data points by (1) removing questions that lack the question body or solution, (2) eliminating duplicate questions, and (3) removing questions that require information that cannot be parsed by LLMs in a textual format (e.g., diagrams, images, plots).In cases where a joint context is provided for multiple questions, we augment each question individually with this context.</p>
<p>B Prompting Strategies</p>
<p>Our study's goal is to identify the vulnerability of educational assessments to AI systems.As a result, we select prompting strategies that simulate realistic student use and assess prompting strategies that can be used with minimum effort, requiring only knowledge of the relevant literature and minimal adaptation.We exclude strategies involving training models.Our assessment encompasses three primary categories of prompting strategies: direct prompting, wherein the model is directly prompted to provide an answer; rationalized prompting, which encourages the model to first verbalize reasoning steps before providing a response; and reflective prompting, which prompts the model to reflect on a previously generated response before finalizing an answer.Each prompt is tailored for three scenarios:</p>
<p>(1) MCQs with a single correct answer, (2) MCQs with multiple correct answers, and ( 3) open-answer questions.Below, we outline the strategies used to prompt models to answer questions:</p>
<p>B.1 Direct Prompting</p>
<p>We explore three strategies for direct prompting: zero-shot, one-shot, and expert prompting.All these strategies ask the LLM directly for an answer without encouraging any particular strategy or rationale to arrive at the answer.</p>
<p>Zero-shot Prompting.We ask the model to solve questions without any demonstrations or system role prompts.The instructions vary depending on the type of question: open-answer or multiple-choice (MCQ).Additionally, we differentiate between multiple-choice cases where a single answer is correct and cases where multiple correct answers can be selected.</p>
<p>MCQ single answer: You are given a question followed by the possible answers.Only one answer is correct.Output the correct answer.MCQ multi answer: You are given a question followed by the possible answers.The question can have multiple correct choices.Output all the correct answers.Open answer: Solve the following question:</p>
<p>For MCQs, each answer option is associated with a letter, and the model is expected to provide the letter corresponding to its choice.</p>
<p>One-shot Prompting [10].In this prompting strategy, we instruct the model to solve questions based on a provided example as a demonstration, without any additional system role prompt.Each question is paired with a demonstration that is the most similar to the question being addressed.Specifically, we use the "all-roberta-large-v1" model4 [35] to embed all questions as vectors, and then retrieve the most similar question vector based on cosine similarity to the prompt question vector.We append the corresponding demonstrative example to this retrieved vector to the prompt.The prompt instructions remain the same as for zero-shot prompting.The demonstration is provided to the model in a multi-message setting, mimicking an actual conversation between the user and the assistant.</p>
<p>Expert Prompting [64].In expert prompting, we use the LLM to to simulate the responses of three experts in the field.The model generates answers as if written by these experts, and then we combine their responses using collaborative decision-making, typically through majority voting.This process is represented by using a generic expert defined as the system role, such as "You are a professor of Machine Learning" for questions from, e.g., Machine Learning5 and prompting the model to give us the names of three experts in the field capable of solving the given question using the prompt: System: You are an expert in {course name}.Give an educated guess of the three experts most capable of solving the following question.Only output the name of these three experts as a json format with key as number and value as a name, without any explanation.</p>
<p>Following this, the model adopts the personas of the named experts as its system role to produce an answer, employing the same prompt as used in the zero-shot and one-shot strategies.The answers generated by these personas are then aggregated using a majority voting approach.</p>
<p>B.2 Rationalized Prompting</p>
<p>We explore three strategies for eliciting reasoned answers: zero-shot and four-shot chain-of-thought, and tree-of-thought prompting.Each strategy involves prompting the LLM to generate a rationale before providing a final answer.</p>
<p>Chain-of-Thought Prompting (CoT) [62].In chain-of-thought prompting, we guide the model to generate a sequence of reasoning steps before providing an answer.This approach typically results in more coherent, structured, and accurate responses, as it requires the model to present arguments before delivering the final answer.This behavior is often initiated by an instruction such as "Let's think step by step."For better performance, the model may be given demonstrations that illustrate how to break down a question into multiple reasoning steps.However, manually generating these demonstrations for each course is time-consuming.Therefore, we automatically generate multiple example rationales using GPT-4 for questions from each course.Domain experts then manually select the best chain-ofthought reasoning trace for each question and correct or improve it if necessary.We experiment with two settings: zero-shot (no demonstration) and few-shot (4 demonstrations).For the latter, for each question, we sample 4 demonstrations of the same course cluster (same topic) and the same question type (MCQ or open-answer), ensuring that these demonstrations were different from the question being asked to the model.Sometimes, the total length of the 4 demonstrations exceeds the model's maximum context length.In such cases, we reduced the number of demonstrations to fit within the context limit.Additionally, we provided a system prompt that included the course topic as an extra hint for the model.For each question type, the selected prompts are the following: System: You are an expert in {course name}.MCQ single answer: You are given a question followed by the possible answers.Only one answer is correct.Give a step-by-step reasoning, and then output the correct answer.MCQ multi answer: You are given a question followed by the possible answers.The question can have multiple correct choices.Give a stepby-step reasoning, and then output all the correct answers.Open answer: Solve the following question, by first giving the stepby-step reasoning and then outputting the answer:</p>
<p>The demonstration pairs, which include the question and its reasoning explanation, are provided to the OpenAI API using a multi-message setting similar to the few-shot strategy.</p>
<p>Tree-of-Thought Prompting [67].While chain-of-thought prompting has led to performance improvements in many NLP tasks, it is sensitive to incorrect reasoning steps, as there is no mechanism to assess and fix a reasoning error after it has been made.Tree-of-thought prompting extends chain-of-thought by having the model emulate three subject experts.Each of them must generate a reasoning path and critique the other expert's proposed paths.Then, the model is instructed to simulate a discussion between experts until they reach an agreement and provide a final answer.We use the following prompt to implement Tree-of-Thought:</p>
<p>System: You are an expert in {course name}.Imagine three different experts answering this {question type} question.They will brainstorm the answer step by step reasoning carefully and taking all facts into consideration.All experts will write down one step of their thinking and then share it with the group.They will each critique their response, and all the responses of others.They will check their answer based on science.Then all experts will go on to the next step and write down this step of their thinking.They will keep going through steps until they reach their conclusion taking into account the thoughts of the other experts.If at any time they realize that there is a flaw in their logic, they will backtrack to where that flaw occurred.If any expert realizes they're wrong at any point then they acknowledge this and start another tree of thought.Each expert will assign a likelihood of their current assertion being correct.Continue until the experts agree on the single most likely answer.</p>
<p>B.3 Reflective Prompting</p>
<p>We explore two strategies for reflective prompting: self-critique and metacognitive prompting.Both strategies involve the model reflecting on an answer it previously provided.Based on this reflection, the model then generates a final, improved answer.</p>
<p>Self-Reflect Prompting [36,58].This strategy is performed on in conjunction to CoT to refine the reasoning traces generated by the model.Focusing on MCQ questions, first, we provide the model with a question and its zero-shot CoT response.Then, we prompt the model to revise its reasoning and produce a refined answer.Notably, this refinement process is carried out without any demonstrations.</p>
<p>{CoT prompt and model output} MCQ single answer: Please consider that there is a single correct choice.Is the provided reasoning accurate?If there isn't any inaccuracy, please output "Reasoning is fine."Otherwise, please revise your reasoning and then choose the single correct choice.MCQ multi answer: Please consider that multiple choices can be correct.Is the provided reasoning accurate?If there isn't any inaccuracy, please output "Reasoning is fine."Otherwise, please revise your reasoning and then and then output all the correct choices.Open answer: Assume you got the above answer from a student and you're looking for inaccuracies in either the reasoning or the final response.Try to refine any inaccuracy and answer the question from scratch.Please don't mention in your answer that you're refining a previous answer and write a new answer from scratch.Answer:</p>
<p>Metacognitive Prompting [61].Motivated by the concept of metacognition, this prompt is designed to emulate the human process of introspection and regulation of thinking.To achieve this, the language model is tasked with following a specific procedure akin to human cognitive processes.This involves sequentially: (1) deeply understanding the problem, akin to human comprehension; (2) identifying relevant concepts and formulating a preliminary answer; (3) evaluating and adjusting this preliminary answer if needed; and (4) confirming the final response and presenting it in a specified format.</p>
<p>You have to answer the following {question type} question.{Question text} As you perform this task, follow these steps: 1. Clarify your understanding of the question.2. Make a preliminary identification of relevant concepts and strategies necessary to answer this question, and propose an answer.3. Critically assess your preliminary analysis.If you are unsure about its correctness, try to reassess the problem.4. Confirm your final answer and explain the reasoning behind your choice.</p>
<p>C Evaluation</p>
<p>In this section we describe the methods used for grading MCQs and open answer questions with GPT-4.</p>
<p>C.1 Multiple Choice Scoring</p>
<p>Regardless of the prompting strategy used for MCQs, the model is provided with the list of answer choices, each associated with a letter, and is asked to generate the letter(s) corresponding to the correct answer(s).Therefore, grading MCQs involves extracting the letter(s) indicated in the model's response and comparing them with the correct answer(s) (i.e., ground truth).</p>
<p>This process is straightforward for direct prompting strategies, but more challenging for strategies involving reasoning, such as chain-of-thought, where the model's response may include long explanations that discuss incorrect answers.To ensure consistency in answer extraction across different types of responses, we use an LLM (GPT-3.5)with the following prompt to extract the model's final answer:</p>
<p>{Question prompt} {Model output}</p>
<p>If the above answer does not provide an option, or gives an answer which is not in the options list, you should give the following: {"selection": [None]} Otherwise, please return the answer in a dictionary format, with the key being "selection", and the value is a list that contains the index of letters of all the correct choices, with A being 0, B being 1, and so on:</p>
<p>C.2 Open Answer Direct Grading</p>
<p>For open-answer grading, we compare the performance of GPT-4 as a grader against human graders from the teaching staff of the courses from which the questions originated.</p>
<p>Grading Open Answer Questions.To automatically grade the quality of open answers, the GPT-4 grader model is given the question, the gold solution (extracted from the course materials), and the text of the generated answer, and prompted to assign a rating to the generated answer based on its quality.Rather than asking the model for a single correct or incorrect label, we provide the model with a 4-point grading scale, ranging from correct, almost correct, mostly incorrect, to wrong answer.The full prompt is presented below:</p>
<p>System prompt: You are a teacher of {course name}.You must grade exam questions.</p>
<p>User Prompt: You must rigorously grade an exam question.Please be strict and precise in your assessment, providing reasoning for your assigned grade.Here's the process I'd like you to follow: Carefully read and understand the question.Thoroughly compare the student's answer with the correct golden answer.Evaluate the student's response based on its accuracy and completeness.Deduce a final grade by considering whether the answer is "wrong answer", "mostly incorrect", "almost correct", "correct", along with a clear explanation for your decision.Question: {question} Gold Answer: {gold answer} Student Answer: {model output} format your answer in the following json format, providing a clear and detailed evaluation for each of the two criteria (accuracy and completeness) and finally providing the grade.in the field of grade only write the final grade from the given grading options: {"accuracy": , "completeness": , "grade": }</p>
<p>C.3 Comparing GPT-4 and Human Grading</p>
<p>To better understand GPT4's capabilities as a grader, we compare its grading performance against the human grading scores, using two metrics: Average Grade and Grade Agreement.We recruited 28 graders from 11 of the courses in our dataset and tasked them with providing a general assessment of the quality of 933 responses provided by GPT-3.5 and GPT-4.Similar to GPT-4 as a grader, human graders are asked to use the same 4-point scale to grade model outputs.Given the cost of performing this annotation, we only task graders to mark responses from two prompting strategies, Zero-shot CoT [62] and Metacognitive prompting [61].</p>
<p>Average Grade.To evaluate the similarity between grades given by humans and GPT-4 for each course, we first compare the average grades they provide to responses to questions in each course of our dataset.To quantify the grades given by the model and humans, we map grade ratings to a discrete range between 0 and 1: {correct: 1.0, almost correct: 0.66, mostly incorrect: 0.33, wrong answer: 0.0}.</p>
<p>Table 3 shows the average grades provided by both human graders and GPT-4 to question responses generated by both GPT-4 and GPT-3.5.Two prompting strategies were used: zero-shot CoT and metacognitive prompting.On average, for most of the courses, the model tends to give higher grades compared to human graders, particularly for the zero-shot CoT prompting strategy.Some courses show a significant disparity, with GPT-4 giving much higher grades than humans (e.g., Advanced Computer Architecture and Software Engineering).More rarely, the human graders consistently give higher scores for courses such as Machine Learning for Physicists or, to a lesser extent, Applied Data Analysis.We also observe variations between the two prompting strategies: for Mathematics of Data, humans give higher grades than the model for metacognitive prompting, while the model gives the highest grades for zero-shot CoT.Overall, Statistical Physics 48.9  10.1 53.7  6.9 43.9  11. 4  ) and each student model (GPT-3.5 and GPT-4).Each performance is reported with a 95% confidence interval.</p>
<p>both GPT-4 and human graders tend to give higher grades to GPT-4 answers than GPT-3.5.Despite these differences, these results also indicate that humans and GPT-4 have a similar grading distribution for model responses (particularly for responses to the metacognitive prompting strategy).</p>
<p>Course Name</p>
<p>Pairwise Agreement (%) Zero-Shot CoT Metacognitive GPT-4 GPT-3.5 GPT-4 GPT-3.Agreement.While investigating average grades provides an initial assessment of whether GPT-4's grading distribution generally matches that of humans, it does not give us a comprehensive understanding of the alignment between GPT-4 and the teaching staff's grading, so we now investigate the level of response-level grade agreement between the two.The agreement is defined as the percentage of question responses for which the model and human give the same grade.Table 4 shows the average rate of grade agreement for each course.For each course, student model, and prompting method, we report the exact agreement between human graders and GPT-4 as a grader.The agreement between the model and the human varies for different courses, changing from 18% to 70%, while the average agreement across all courses stays below 50% for both metacognitive and zero-shot CoT. Figure 7 shows the human and GPT-4 assigned grades distribution for both GPT-4 and GPT-3.5 as the student, including the two prompting strategies.We observe that GPT-4 as a grader tends to grade model outputs using the labels almost correct and mostly incorrect far more often than human graders, while rarely identifying a response as wrong answer.In contrast, human graders are more generous at identifying responses as correct, but almost more willing to identify responses as wrong answer.</p>
<p>Human Grader Remarks.During the human grading process, we asked the 28 graders to record their impressions of model answers.Overall, there was a general agreement that the model's responses were satisfactory for straightforward questions, but less so for those requiring logical reasoning or analysis.In the latter cases, it was noted that the model sometimes produced lengthy responses that added contextually relevant information but failed to actually solve the problem.In many instances, graders likened this behavior to students attempting to gain points by including all potentially relevant information related to a question's keywords.This behavior could also be an artifact of the prompting approach, however, as we used metacognitive and chain-of-thought prompting strategies to generate the outputs provided to human graders.While these strategies have the best performance on MCQ, they also tend to produce longer answers to open-ended questions.</p>
<p>Other issues identified include instances of factual inaccuracies (e.g., fabricated references) and contextual inaccuracies (e.g., using concepts unsuitable for the requested analysis).Finally, the model, at times, misunderstood the objective of the question (e.g., providing an implementation-specific answer when a student would instead interpret it as a design question).Regarding mathematical reasoning, apart from the previously mentioned limitations, the models struggled significantly with mathematical derivations requiring multiple steps, demonstrated a flawed understanding of imaginary numbers, and made errors in calculations.</p>
<p>C.4 Automated Grading in Prior Work</p>
<p>A substantial body of research leverages Large Language Models (LLMs) for response evaluation.Traditionally, automated assessment has necessitated high-quality reference data obtained through human grading, which is both costly and time-intensive.Consequently, there has been considerable exploration into the potential of LLMs to serve as evaluators [14].Recent research has found LLMs to be capable of generating quality feedback [13, 36, 41, 50-52, 54, 63], a trend also reflected in investigations into LLM-based evaluation [22,29,34,56,70].Automated solutions for student grading have been explored in the field of learning science, as well, some of which now use LLMs [23].Intelligent Tutoring Systems (ITS), such ALEKS [19], ASSISTments [24], Cognitive Tutor [47], and MATHia [46] are widely employed to automatically assess student performance in closed-ended questioning.These systems cater to several hundred thousand students annually [1,24].Meanwhile, Automated Essay Scoring (AES) platforms such as e-Rater [5], IntelliMetric [48], and Intelligent Essay Assessor [21] have emerged as useful tools for evaluating numerous student essays and responses to open-ended questions each year [8,21,44,48,53].</p>
<p>D OpenAI API Hyperparameters</p>
<p>For both GPT-4 and GPT-3.5, we set temperature=0.8 to increase diversity and encourage more creative responses.To reduce repetitive samples and keep the quality of the generations high, we set presence_penalty=0.5 and frequency_penalty=0.8.These values are chosen based on a human evaluation of the fluency and quality of the responses given a set of questions.For the rest of the hyperparameters, we use their default values.</p>
<p>E Bloom's Taxonomy</p>
<p>Bloom's taxonomy [28] is a framework for categorizing learning objectives and educational items into levels of complexity requiring different cognitive skills.The taxonomy consists of 6 levels, from basic knowledge recall to higher-order critical thinking.The lower levels (remember, understand, and apply) focus on foundational cognitive tasks such as remembering facts and comprehending basic information.As learners progress to higher-level categories, they engage in more complex cognitive tasks.The upper levels (analyze, evaluate, and create) emphasize critical thinking, problem-solving, and creativity.Although Bloom's Taxonomy is widely accepted and used, educators often disagree about the precise definitions of each category.This discrepancy leads to varied interpretations and challenges in categorizing learning objectives and educational items into specific taxonomy levels.This is particularly true when moving between adjacent levels, such as understand and apply.For example, given the following MCQ:</p>
<p>In which of the following cases does JOS acquire the big kernel lock?Options: A. Processor traps in user mode B. Processor traps in kernel mode C. Switching from kernel mode to user mode D. Initialization of application processor On the one hand, to solve this question correctly, it is required to recall specific knowledge (remember) about the circumstances under which JOS acquires the big kernel lock from the lecture or other learning materials.However, it can also be classified as the understand category, as some multiple-choice options act as distractors that test the depth of a student's comprehension of the topic.As a result, the taxonomy has limitations in addressing the complexities of modern learning environments, especially in blended learning where information access and processing diverge from the conventional classroom setting for which Bloom's Taxonomy was crafted.</p>
<p>Despite these ambiguities, Bloom's taxonomy remains a leading categorization scheme of cognitive difficulty in education.In this work, we assign Bloom's taxonomy labels to various questions in our dataset to assess model performance across questions of varying cognitive difficulty.To assign Bloom's meta-labels to questions in our dataset, we tasked two experts in the learning sciences to label 207 randomly-selected English questions with one of the six Bloom categories.They achieved an inter-annotator agreement of 51% on</p>
<p>F Program Statistics</p>
<p>In our work, we study 9 programs from three program levels: Bachelor, Master, and Online.Table 5 shows the number of courses available per program.</p>
<p>G Additional Results</p>
<p>G.1 Individual Course Performance</p>
<p>Table 6 shows GPT-4 performance across all courses for open-answer questions.Table 7 shows GPT-4 performance across all courses for MCQ type of questions.Table 8 show GPT-3.5 performance across all courses for open-answer type of questions.Table 9 show GPT-3.5 performance across all courses for MCQ type of questions.As the exact course names are not important for this analysis, we anonymize course names when presenting results in these Tables.</p>
<p>G.2 Impact of Prompting Strategy</p>
<p>Table 10 shows the average GPT-4 and GPT-3.5 performance for each prompting strategy.GPT-4 outperforms GPT-3.5 across all prompting strategies.When answering MCQs, four-shot CoT [62] emerges as GPT-4's best-performing strategy, while zero-shot achieves the lowest performance.Curiously, the same ranking does not transfer to open-answer questions, where self-reflect [36] emerges as the best strategy, followed by Expert Prompting [64].Zero-shot prompting remains the least performant.However, based on a survey of reports submitted by Masters students for a class project in a Natural Language Processing (NLP) course, we found students to be most likely to use Zero-shot, Expert, and Zero-shot COT prompting, as these are the most intuitive strategies and the ones that require the least amount of preparation work.</p>
<p>G.3 Performance by Language</p>
<p>In our dataset, we have 70.5% of English questions and 29.5% of French questions.Table 11 shows performance by language across models and question types.Table 12 shows GPT-4 performance per language per prompting strategy across all question types.</p>
<p>G.4 Impact of Prompt Language</p>
<p>Tables 11 and 12 show differing performance on English questions compared to French questions.Unfortunately, the subsets of courses in our dataset in English and French mostly do not intersect, precluding a conclusive comparison between these performance measurements.However, given that AI assistants are often predominantly trained on English text data, these results raise a question of whether performance on French questions could be increased further, through creative cross-lingual prompting.Consequently, we explore whether a student user could achieve better performance by varying the language of the prompting instruction.We employ three variations of the metacognitive prompting strategy (which ranks among the top-performing strategies), where we vary the language and the wording of the instruction and the question, as schematized in Figure 8: Vanilla, Language-inverted, and Guided.In the Vanilla setting, we provide the prompt instruction and question in the same language.In the Inverted setting, we provide the instruction and question in different languages.Finally, in the Guided setting, we provide the instruction and question in different languages but clarify in the instruction that the answer should be provided in the same language as the question.We focus on MCQ-based performance to avoid potential language bias from GPT-4 as a grader, assessing the impact of these three variations across all English and French MCQs of our dataset.</p>
<p>As illustrated in Table 13, the average scores for the Vanilla setting are higher for both English and French compared to the languageinverted setting, indicating that instructing the model in the same language as the question leads to higher performance for both models compared to when the question is in a different language.Finally, guiding the model by asking it to reason and answer in the same language as the question, even if the instructions are in another language (i.e., the guided setting), enhances the performance for GPT-4 on French questions, yielding a score equivalent to providing instructions in French.Taken together, our results show that there is little benefit from prompting the model in English (a language that most pretrained models have likely seen more data from) compared to the language of the question.</p>
<p>Fig. 1 .
1
Fig. 1.Overview of Courses.Courses represented in our dataset, grouped by program and degree.Courses may belong to multiple programs, in which case their partition is split into chunks of equal size, with one chunk assigned to each program.</p>
<p>Fig. 2 .
2
Fig. 2. Course Pass Rate of Generative AI Assistants.Proportion of 50 courses that models pass at various performance thresholds.Results are presented independently for multiple-choice (MCQ) and openanswer (Open) question types for both GPT-3.5 and GPT-4.Model responses are aggregated using the majority vote strategy.</p>
<p>Fig. 3 .
3
Fig. 3. Model Performance Stratified by Question Difficulty.(a, b) 376Bachelor's and 693 Master's questions, respectively, annotated using instructor-reported difficulty levels.(c) 207 questions annotated using Bloom's taxonomy by two researchers in the learning sciences.Across all categorization schemes, GPT-4 performance slightly degrades as the questions become more complex and challenging.Performance is aggregated by the majority vote strategy.Error bars represent 95% confidence intervals using the non-parametric bootstrap with 1000 resamples.</p>
<ol>
<li>3
3
More challenging assessments are a half-solution.One possible solution to mitigate assessment vulnerability would be to increase their difficulty beyond what generative AI systems 0-50 50-100 100-150 150-200 200-300</li>
</ol>
<p>Fig. 4 .
4
Fig.4.Course Performance by Course Size.Average course performance of GPT-4 with the majority vote strategy stratified by the course size, measured by the number of enrolled students.GPT-4 successfully answers questions for assessments in some of the largest courses by enrollment, amplifying the potential impact of assessment vulnerability.Error bars represent 95% confidence intervals using the non-parametric bootstrap with 1000 resamples.</p>
<p>Fig. 5 .
5
Fig.5.Comparison of student performance and GPT-4.Average student performance for a subset of 197 questions is computed and stratified along 10-point intervals from 0 to 100.The model's performance with the majority vote strategy is assessed by human graders using a 4-point scale.We observe the model typically answers correctly questions that students also excel at.However, there are questions on which the model struggles, but students perform reasonably well.</p>
<p>Fig. 6 .
6
Fig. 6.Comparison of Human and GPT-4 grading.Average model and human performance for 933 questions and responses from GPT-4 and GPT-3.5 generated with the metacognitive prompting method.</p>
<p>Fig. 7 .
7
Fig. 7. Grade label distributions.Distribution of grades assigned by our grader consortium (blue) and GPT-4 (orange) to the responses provided by GPT-4 and GPT-3.5.</p>
<p>Table 7 .
7
Performance of GPT-4 on MCQs for all courses categorized by prompting strategy.Majority corresponds to the performance of the majority vote aggregation strategy.Max corresponds to the maximum performance (the score when only one prompting strategy is required to return a correct answer for the model get the answer correct).* denotes required courses for a program (applies only for Bachelor and Master programs).</p>
<p>Fig. 8 .
8
Fig.8.The three language-related prompting strategies.Given two languages L1 and L2, and a question in language L1, (1) Vanilla: provides instructions in L1; (2) Language Inverted: provides instructions in L2; (3) Guided: provides instructions in L2, specifying that the question is in L1, and that it should be answered in L1 as well.</p>
<p>Table 1 .
1
Dataset Statistics.
CategoryTotal QuestionsBachelor's courses2,147 (38.5%)LevelMaster's courses1,631 (29.2%)Online programs1,801 (32.3%)LanguageEnglish French3,933 (70.5%) 1,646 (29.5%)Question TypeMCQ Open-Answer3,460 (62%) 2,119 (38%)</p>
<p>Table 2 .
2
Program results.For each program, the first three columns show the percentage of courses for which GPT-4 surpasses the thresholds  = 50, 60, 70% correctly-answered questions using the majority vote strategy."Max" represents the proportion of questions in this degree correctly answered by at least one prompting strategy.Program levels are specified as Bachelor, Master, or Online.Engineering, Chemistry, and Life Science are first-year Bachelor programs.
Program% Courses Passed =50% =60% =70%MaxQuestion CountEngineering80.060.040.00.831,343Chemistry83.366.750.00.851,417Life Science85.771.457.10.851,477Physics Bachelor100.055.633.30.86958CS Bachelor91.766.750.00.871,487CS Master100.083.350.00.871,514Data Science Master90.070.030.00.861,576Physics Online100.063.627.30.84837Life Science Online85.771.457.10.75996</p>
<p>Table 3 . Comparison between human graders and the GPT-4 model across multiple university courses.
3
38.6  4.4 36.4  10.1 45.5  6.3 37.6  10.6 39.9  4.4 Concurrency &amp; Parallel Processing 62.3  14.5 68.4  9.5 62.3  14.5 61.0  8.1 72.8  14.5 68.4  10.5 56.1  15.6 53.8  9.4 The average grades provided by human graders and the GPT-4 model for open-answer questions.Results are presented for two prompting strategies (Zero-shot CoT and Metacognitive
Advanced Computer Architecture50.4  10.5 74.9  6.244.9  11.1 68.8  6.960.3  11.0 73.1  6.242.3  10.5 62.6  6.2Software Engineering62.2  9.085.9  4.847.9  9.572.9  5.366.1  9.984.1  5.849.8  10.5 73.0  5.7Mathematics of Data52.1  13.5 65.4  8.150.2  13.5 56.4  8.194.5  5.568.9  7.376.5  12.7 56.4  9.0ML for Physicists80.8  7.276.7  5.271.7  7.969.1  6.380.4  6.873.9  4.874.4  7.669.5  5.2Semiconductor Properties74.1  15.3 78.6  10.7 66.4  12.2 78.5  12.2 63.4  16.6 66.3  10.6 55.8  15.2 64.8  10.7Applied Data Analysis74.8  13.1 72.3  10.8 58.1  13.1 57.9  8.373.6  13.1 65.1  9.565.2  13.1 55.6  9.6Advanced General Chemistry78.9  8.480.7  6.658.8  10.8 64.0  7.880.8  7.881.4  7.862.3  10.1 62.3  7.8Information &amp; Communication76.6  4.774.3  3.957.6  5.156.2  3.974.3  4.170.9  3.960.3  5.159.5  3.9Analysis I37.2  4.748.7  3.228.8  4.138.4  2.648.6  4.147.1  3.142.9  4.344.5  2.8Average63.5  9.170.9  6.957.7  10.3 60.2  6.768.3  9.467.7  6.956.7  10.5 58.4  6.8</p>
<p>Table 4 .
4
Pairwise agreement (%) between grades provided by human graders and the GPT-4 model as a grader.
5</p>
<p>Table 5 .
5
Program Statistics."Required" shows the ratio of required courses present in our data over the total number of required courses per program."Optional"shows the number of optional courses per program."Total"shows their sum, that is, the total number of courses our dataset covers, per program.thistask.Using a more forgiving fuzzy agreement (which also indicates an agreement if the annotators select adjacent categories) yields an agreement score of 84%.Results for performance stratified by Bloom's taxonomy label can be found in the main article.
ProgramNumber of Courses Required Optional TotalEngineering, 1st year BSc5/12-5Chemistry, 1st year BSc6/9-6Life Science, 1st year BSc7/11-7Physics, BSc8/2619Computer Science, BSc10/21212Computer Science, MSc5/1038Data Science, MSc3/9710Physics, Online--11Life Sciences, Online--8</p>
<p>Table 6 . Performance of GPT-4 on open-answer questions for all courses categorized by prompting strategy.
6
Majority corresponds to the performance of the majority vote aggregation strategy.Max corresponds to the maximum performance (the score when only one prompting strategy is required to return a correct answer for the model get the answer correct).Online courses typically have fewer open-answer questions as most evaluation in online courses is done through MCQA.<em> denotes required courses for a program (applies only for Bachelor and Master programs).
Online Life Sciences #1936.740.466.270.158.959.058.962.758.9 92.6Online Life Sciences #21100.0100.0100.0100.0100.0100.0 100.0100.0100.0 100.0Online Life Sciences #310.00.033.033.033.033.066.033.033.0 66.0Online Life Sciences #4812.412.428.937.328.941.324.837.124.8 57.9Online Life Sciences #5761.757.066.480.771.371.161.766.461.7 85.4Online Life Sciences #6266.583.066.583.0100.083.083.083.083.0 100.0Online Life Sciences #7366.322.055.333.022.022.044.022.022.0 88.7Online Physics #1355.366.0100.0100.077.7100.077.3100.0100.0 100.0Online Physics #2266.549.583.033.066.5100.083.083.066.5 100.0Online Physics #3766.437.961.447.371.071.066.375.961.6 75.9Online Physics #4483.058.349.833.058.091.574.858.066.5 91.5Online Physics #5366.322.055.355.366.366.355.355.355.3 77.3Online Physics #61348.440.868.845.856.063.758.663.761.2 71.4Online Physics #71246.841.344.038.546.960.746.849.541.3 80.2Online Physics #8458.041.349.566.341.349.549.858.049.5 74.8Online Physics #9718.99.428.337.737.737.728.333.033.0 47.1Online Physics #102775.177.686.388.866.487.587.588.886.3 97.5</em>  Physics #1  *  Physics #224 4549.8 58.252.5 64.959.4 66.351.0 57.549.7 52.245.5 53.756.6 73.148.3 57.453.8 71.9 56.0 87.2Physics #3333.033.033.033.055.033.044.044.033.0 66.0Physics #4  *  Physics #5  *  Physics #6  *  Physics #7  *  Physics #8  *  Physics #924 14 68 28 53 47837.2 69.4 66.4 66.4 46.2 56.833.0 69.3 69.7 70.0 46.2 61.251.0 78.2 68.8 59.3 53.7 60.446.8 61.2 68.3 63.9 51.2 61.249.6 60.3 69.3 61.6 40.5 58.041.3 70.8 74.3 77.2 45.5 57.346.9 76.3 66.8 72.4 49.3 61.848.3 61.0 77.7 70.0 57.5 60.844.1 63.5 74.5 89.0 71.3 90.5 68.9 89.1 46.8 71.3 59.0 83.2</p>
<p>Table 8 . Performance of GPT-3.5 on open-answer questions for all courses categorized by prompting strategy.
8
Majority corresponds to the performance of the majority vote aggregation strategy.Max corresponds to the maximum performance (the score when only one prompting strategy is required to return a correct answer for the model get the answer correct).Online courses typically have fewer open-answer questions as most evaluations in online courses are done through MCQA.<em> denotes required courses for a program (applies only for Bachelor and Master programs).
Online Life Sciences #1933.147.847.766.447.958.962.670.155.292.6Online Life Sciences #2166.066.0100.033.066.066.066.033.066.0100.0Online Life Sciences #310.00.033.033.033.033.033.033.033.033.0Online Life Sciences #4837.337.341.337.141.424.824.833.037.166.4Online Life Sciences #5728.352.352.137.947.337.952.156.747.471.1Online Life Sciences #62100.0100.0100.0100.066.583.083.0100.0100.0 100.0Online Life Sciences #7322.044.355.333.066.333.355.355.355.366.3Online Physics #1355.344.077.7100.044.0100.0100.077.7100.0 100.0Online Physics #2249.583.083.066.566.583.066.049.549.5100.0Online Physics #3752.152.052.047.356.733.037.747.142.471.1Online Physics #4458.058.049.833.033.049.549.541.549.866.3Online Physics #5333.022.044.355.333.022.066.355.322.088.7Online Physics #61343.238.150.943.240.745.858.640.745.868.9Online Physics #71241.346.938.538.544.041.338.652.438.571.8Online Physics #8441.533.058.049.549.541.349.858.049.566.5Online Physics #9733.018.937.728.333.018.937.742.437.751.9Online Physics #102768.962.873.987.659.076.380.080.177.796.2</em>  Physics #1  *  Physics #224 4530.3 43.438.6 47.833.0 39.737.2 42.623.4 36.733.0 36.731.7 41.928.9 39.735.8 37.551.0 65.6Physics #3333.033.044.022.044.044.033.044.033.066.0Physics #4  *  Physics #5  *  Physics #6  *  Physics #7  *  Physics #8  *  Physics #924 14 68 28 53 47835.9 57.1 59.4 58.1 38.6 43.233.0 40.0 57.6 62.8 43.0 45.530.3 50.0 59.0 50.9 38.6 42.535.8 64.2 49.1 48.5 38.0 42.539.9 38.6 48.6 45.0 34.9 40.038.5 43.4 54.5 56.9 39.9 42.737.2 50.1 54.6 54.5 37.4 44.638.5 63.8 54.6 43.7 36.8 42.635.8 52.0 54.5 54.5 37.4 42.056.5 79.7 77.7 74.7 58.1 63.7
Analysis of the quality of this automated grading is provided in Appendix C. Importantly, we note that GPT-4 gives slightly higher average grades (on average 2.75%) than humans for responses to a sample of questions graded by both.
More details about Bloom's Taxonomy can be found in Appendix E.
We use the sentence-transformers[45] implementation of this model, available at https://huggingface.co/sentence-transformers/all-roberta-large-v1.
Whenever a prompting strategy makes use of a course name, it is employing the course name rather than the course code (e.g., "Machine learning for physicists" rather than course code like "PHYS 444").
AcknowledgmentsAB gratefully acknowledges the support of the Swiss National Science Foundation (No. 215390), Innosuisse (PFFS-21-29), the EPFL Science Seed Fund, the EPFL Center for Imaging, Sony Group Corporation, and the Allen Institute for AI.PS acknowledges support from the NCCR Catalysis (grant number 180544), a National Centre of Competence in Research funded by the Swiss National Science Foundation.TK is partially funded by the Swiss State Secretariat for Education, Research, and Innovation (No. 591711).13. Performance comparison of GPT-3.5 and GPT-4 across the three different prompting strategies (vanilla, language inverted, and guided), categorized by question language.All scores are provided with 95% confidence interval.
Toward Meta-cognitive Tutoring: A Model of Help Seeking with a Cognitive Tutor. Bruce Vincent Aleven, Ido Mclaren, Kenneth Roll, Koedinger, I. J. Artificial Intelligence in Education. 162006. 01 2006</p>
<p>Generative Artificial Intelligence in Education: From Deceptive to Disruptive. Marc Alier, Francisco Garca-Pealvo, Jorge D Camba, International Journal of Interactive Multimedia and Artificial Intelligence. 2024. 2024</p>
<p>The emergent role of artificial intelligence, natural learning processing, and large language models in higher education and research. Tariq Alqahtani, A Hisham, Mohammed Badreldin, Abdulrahman I Alrashed, Sahar S Alshaya, Alghamdi, Khalid Bin, Shuroug A Saleh, Omar A Alowais, Ishrat Alshaya, Majed S Rahman, Abdulkareem M Al Yami, Albekairy, 10.1016/j.sapharm.2023.05.016Research in Social and Administrative Pharmacy. 192023. 2023</p>
<p>Daman Arora, Himanshu Gaurav Singh, Mausam , arXiv:2305.15074[cs.CL]Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. 2023</p>
<p>Automated Essay Scoring With e-rater. Yigal Attali, Jill Burstein, The Journal of Technology, Learning and Assessment. 42006. Feb. 2006</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, E Perez, Jamie Kerr, Jared Mueller, Jeff Ladish, Kamal Landau, Kamil Ndousse, Liane Lukovsit, Michael Lovitt, Nelson Sellitto, Nicholas Elhage, Schiefer, Nova Noem'i Mercado, Dassarma, arXiv:2212.08073Constitutional AI: Harmlessness from AI Feedback. Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Sam Bowman, Zac Hatfield-Dodds, Benjamin Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom B Brown, Jared Kaplan, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston2022. 2022254823489</p>
<p>Programming Is Hard -Or at. Brett A Becker, Paul Denny, James Finnie-Ansley, Andrew Luxton-Reilly, James Prather, Eddie Antonio Santos, arXiv:2212.01020[cs.HC]Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation. 2022</p>
<p>Automated Evaluation of Writing -50 Years and Counting. Beata Beigman, Klebanov , Nitin Madnani, 10.18653/v1/2020.acl-main.697Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>Taxonomy of Educational Objectives: The Classification of Educational Goals. Number Bd. 1 in Taxonomy of Educational Objectives: The Classification of Educational Goals. B S Bloom, D R Krathwohl, 1956Longmans, Green</p>
<p>Language Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Time to Revisit Existing Student's Performance Evaluation Approach in Higher Education Sector in a New Era of ChatGPT -A Case Study. Iffat Chaudhry, Sayed Sarwary, Ghaleb El-Refae, Habib Chabchoub, 10.1080/2331186X.2023.2210461Cogent Education. 102023. 05 2023</p>
<p>Generative AI, learning and new literacies. Chen, Journal of Educational Technology Development and Exchange. 2023. 2023</p>
<p>Teaching Large Language Models to Self-Debug. Xinyun Chen, Maxwell Lin, Nathanael Schrli, Denny Zhou, International Conference on Learning Representations. 2024</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.8702023870</p>
<p>Chatting and cheating: Ensuring academic integrity in the era of ChatGPT Chatting and cheating: Ensuring academic integrity in the era of ChatGPT. Debby Cotton, Peter Cotton, Reuben Shipway, 10.1080/14703297.2023.2190148Innovations in Education and Teaching International. 61032023. 2023</p>
<p>Academic integrity and artificial intelligence: is ChatGPT hype, hero or heresy?. Geoffrey Currie, 10.1053/j.semnuclmed.2023.04.008Seminars in Nuclear Medicine. 532023. 05 2023</p>
<p>Paul Denny, James Prather, Brett A Becker, James Finnie-Ansley, Arto Hellas, Juho Leinonen, Andrew Luxton-Reilly, Brent N Reeves, arXiv:2306.02608[cs.CY]Eddie Antonio Santos, and Sami Sarsa. 2023. Computing Education in the Era of Generative AI. </p>
<p>Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research. K Yogesh, Nir Dwivedi, Laurie Kshetri, Emma Louise Hughes, Anand Slade, Arpan Jeyaraj, Abdullah M Kumar Kar, Alex Baabdullah, Koohang, Manju Vishnupriya Raghavan, Hanaa Ahuja, Albanna, Ahmad Mousa, Adil S Albashrawi, Janarthanan Al-Busaidi, Yves Balakrishnan, Sriparna Barlette, Indranil Basu, Laurence Bose, Dimitrios Brooks, Lemuria Buhalis, Soumyadeb Carter, Tom Chowdhury, Scott W Crick, Cunningham, H Gareth, Robert M Davies, Rahul Davison, Denis D, Yanqing Dennehy, Rameshwar Duan, Rohita Dubey, John S Dwivedi, Carlos Edwards, Robin Flavin, Varun Gauld, Mei-Chih Grover, Marijn Hu, Paul Janssen, Iris Jones, Sangeeta Junglas, Sascha Khorana, Kai R Kraus, Paul Larsen, Sven Latreille, F Tegwen Laumer, Abbas Malik, Marcello Mardani, Sunil Mariani, Emmanuel Mithas, Jeretta Mogaji, Siobhan O' Horn Nord, Fevzi Connor, Margherita Okumus, Neeraj Pagani, Savvas Pandey, Papagiannidis, O Ilias, Nishith Pappas, Pathak, 10.1016/j.ijinfomgt.2023.102642International Journal of Information Management. Heje, Ramakrishnan Raman, Nripendra P. Rana, Sven-Volker Rehm, Samuel Ribeiro-Navarrete, Alexander Richter, Frantz Rowe, Suprateek Sarker, Bernd Carsten Stahl, Manoj Kumar Tiwari, Wil van der Aalst, Viswanath Venkatesh, Giampaolo Viglia, Michael Wade, Paul Walton, Jochen Wirtz, and Ryan Wright71102642Jan. 2023. 2023Opinion Paper: "So what if ChatGPT wrote it?</p>
<p>The Assessment of Knowledge, in Theory and in Practice. Jean-Claude Falmagne, Eric Cosyn, Jean-Paul Doignon, Nicolas Thiry, Formal Concept Analysis. Rokia Missaoui, Jrg Schmidt, Berlin Heidelberg; Berlin, HeidelbergSpringer2006</p>
<p>The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming. James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew Luxton-Reilly, James Prather, Proceedings of the Evaluating Higher Education Vulnerability to AI Assistants. the Evaluating Higher Education Vulnerability to AI Assistants2022</p>
<p>10.1145/3511861.351186324th Australasian Computing Education Conference (, Virtual Event, Australia,) (ACE '22). New York, NY, USAAssociation for Computing Machinery</p>
<p>The intelligent essay assessor: Applications to educational technology. Peter Foltz, Darrell Laham, T Landauer, Interactive Multimedia Electronic Journal of Computer-Enhanced Learning. 041999. 1999</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166[cs.CL]GPTScore: Evaluate as You Desire. 2023</p>
<p>ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning. Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, Ida Momennejad, arXiv:2309.13701[cs.CL]2023</p>
<p>The ASSISTments Ecosystem: Building a Platform that Brings Scientists and Teachers Together for Minimally Invasive Research on Human Learning and Teaching. Neil T Heffernan, Cristina Lindquist Heffernan, 10.1007/s40593-014-0024-xInternational Journal of Artificial Intelligence in Education. 242014. 01 Dec 2014</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300[cs.CY]Measuring Massive Multitask Language Understanding. 2021</p>
<p>A systematic evaluation of large language models for generating programming code. Wenpin Hou, Zhicheng Ji, arXiv:2403.008942024. 2024</p>
<p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, arXiv:2305.08322[cs.CL]Maosong Sun, and Junxian He. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. </p>
<p>Assessing the inter-rater reliability and accuracy of pharmacy faculty's Bloom's Taxonomy classifications. C Samuel, Adam C Karpen, Welch, 10.1016/j.cptl.2016.08.003Currents in Pharmacy Teaching and Learning. 82016. 2016</p>
<p>Large Language Models Are Stateof-the-Art Evaluators of Translation Quality. Tom Kocmi, Christian Federmann, arXiv:2302.14520[cs.CL]2023</p>
<p>Yunshi Lan, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Weining Qian, Aoying Zhou, arXiv:2401.07518[cs.CL]Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends. 2024</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 3782022. 2022</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Future of education in the era of generative artificial intelligence: Consensus among Chinese scholars on applications of ChatGPT in schools. Ming Liu, Yiling Ren, Lucy Nyagoga, Francis Stonier, Zhongming Wu, Liang Yu, 10.1002/fer3.10Future in Educational Research. 1092023. 2023</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634[cs.CL]G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. 2023</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692[cs.CL]RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019</p>
<p>Self-Refine: Iterative Refinement with Self-Feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Thirty-seventh Annual Conference on Neural Information Processing Systems. 2023</p>
<p>How traditional physics coursework limits problem-solving opportunities. Barron Montgomery, Argenta Price, Carl Wieman, 10.1119/perc.2023.pr.MontgomeryPhysics Education Research Conference. 2023</p>
<p>Raza Nowrozy, David Jam, arXiv:2403.11402[cs.CY]Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT. 2024</p>
<p>OpenAI. 2022. Introducing ChatGPT. </p>
<p>REFINER: Reasoning Feedback on Intermediate Representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, Maltathe Association for Computational Linguistics20241Association for Computational Linguistics</p>
<p>How Machine Learning (ML) is Transforming Higher Education: A Systematic Literature Review. Agostinho Pinto, Antnio Abreu, Eusbio Costa, Jernimo Paiva, 10.55267/iadt.07.13227Journal of Information Systems Engineering and Management. 8211682023. 04 2023</p>
<p>It's Weird That it Knows What I Want": Usability and Interactions with Copilot for Novice Programmers. James Prather, Brent N Reeves, Paul Denny, Brett A Becker, Juho Leinonen, Andrew Luxton-Reilly, Garrett Powell, James Finnie-Ansley, Eddie Antonio Santos, 10.1145/3617367ACM Transactions on Computer-Human Interaction. 312023. Nov. 2023</p>
<p>An automated essay scoring systems: a systematic literature review. Dadi Ramesh, Suresh Kumar, Sanampudi , 10.1007/s10462-021-10068-2Artificial Intelligence Review. 552022. 01 Mar 2022</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>The research behind the Carnegie Learning math series. Steve Ritter, 2011. 2011Carnegie LearningPittsburgh, PA</p>
<p>Carnegie Learning's Cognitive Tutor. Steven Ritter, Stephen E Fancsali, Educational Data Mining. 201530091195</p>
<p>An Evaluation of IntelliMetric. Essay Scoring System. Lawrence Rudner, Veronica Garcia, Catherine Welch, Learning, and Assessment. 4012006. 2006Journal of Technology</p>
<p>Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, arXiv:2402.07927Samrat Sohel Mondal, and Aman Chadha. 2024. A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications. 2024267636769</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, arXiv:2206.05802arXiv:2206.05802Jan Leike. 2022. 2022Jonathan Ward</p>
<p>Training Language Models with Language Feedback. Jrmy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, arXiv:2204.14146arXiv:2204.141462022. 2022</p>
<p>Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, International Conference on Learning Representations. 2023PEER: A Collaborative Language Model</p>
<p>Handbook of Automated Essay Evaluation: Current Applications and New Directions. M D Shermis, J Burstein, 2013Taylor &amp; Francis</p>
<p>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. Niket Tandon, Aman Madaan, Peter Clark, Yiming Yang, 10.18653/v1/2022.findings-naacl.26Findings of the Association for Computational Linguistics: NAACL 2022. Ivan Vladimir, Meza Ruiz, Marine Carpuat, Marie-Catherine de Marneffe; Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. Priyan Vaithilingam, Tianyi Zhang, Elena L Glassman, Chi conference on human factors in computing systems extended abstracts. 2022</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, arXiv:2303.04048[cs.CL]Is ChatGPT a Good NLG Evaluator? A Preliminary Study. 2023</p>
<p>Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving. Karen D Wang, Eric Burkholder, Carl Wieman, Shima Salehi, Nick Haber, arXiv:2310.08773[cs.AI]2023</p>
<p>Self-Critique Prompting with Large Language Models for Inductive Instructions. Rui Wang, Hongru Wang, Fei Mi, Yi Chen, Ruifeng Xu, Kam-Fai Wong, arXiv:2305.13733[cs.CL]2023</p>
<p>Exploring the Role of AI Assistants in Computer Science Education: Methods, Implications, and Instructor Perspectives. Tianjia Wang, Daniel Vargas Daz, Chris Brown, Yan Chen, 10.1109/vl-hcc57772.2023.000182023 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). IEEE2023</p>
<p>Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, Wei Wang, arXiv:2307.10635[cs.CL]SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. 2023</p>
<p>Yuqing Wang, Yun Zhao, arXiv:2308.05342[cs.CL]Metacognitive Prompting Improves Understanding in Large Language Models. 2023</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903[cs.CL]Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2023</p>
<p>Generating Sequences by Learning to Self-Correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, International Conference on Learning Representations. 2023</p>
<p>Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao, arXiv:2305.14688[cs.CL]ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. 2023</p>
<p>A systematic evaluation of large language models of code. Uri Frank F Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine Programming2022</p>
<p>Practical and ethical challenges of large language models in education: A systematic scoping review. Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, Dragan Gaevi, 10.1111/bjet.13370British Journal of Educational Technology. 5512023. Aug. 2023</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.10601[cs.CL]2023</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2311.16502MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. 2023. 2023265466525</p>
<p>A study of the impact of project-based learning on student learning effects: A meta-analysis study. Lu Zhang, Yan Ma, Frontiers in psychology. 1412027282023. 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.06364[cs.CL]AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. 2023</p>
<p>Course Name Prompting Strategy: Zero-Shot CoT Metacognitive Prompting Model: GPT-4 Responses GPT-3.5 Responses GPT-4 Responses GPT-3.5 Responses Grader: Human GPT-4 Human GPT-4 Human GPT-4 Human GPT-4. </p>            </div>
        </div>

    </div>
</body>
</html>
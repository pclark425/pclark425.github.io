<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-232 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-232</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-232</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-13.html">extraction-schema-13</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use memory mechanisms to solve text-based games, including details about the memory architecture, the games played, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-9dc5e34106d82704aa79e30a04f325af66e27103</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9dc5e34106d82704aa79e30a04f325af66e27103" target="_blank">Positive Experience Reflection for Agents in Interactive Text Environments</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the 1st Workshop for Research on Agent Language Models (REALM 2025)</p>
                <p><strong>Paper TL;DR:</strong> This work introduces Sweet&Sour, a novel approach that addresses limitations in existing reflection methods by incorporating positive experiences and managed memory to enrich the context available to the agent at decision time.</p>
                <p><strong>Paper Abstract:</strong> Intelligent agents designed for interactive environments face significant challenges in text-based games, a domain that demands complex reasoning and adaptability. While agents based on large language models (LLMs) using self-reflection have shown promise, they struggle when initially successful and exhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour, a novel approach that addresses these limitations in existing reflection methods by incorporating positive experiences and managed memory to enrich the context available to the agent at decision time. Our comprehensive analysis spans both closed- and open-source LLMs and demonstrates the effectiveness of Sweet&Sour in improving agent performance, particularly in scenarios where previous approaches fall short.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e232.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e232.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use memory mechanisms to solve text-based games, including details about the memory architecture, the games played, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sweet&Sour</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sweet & Sour (Positive Experience Reflection with Managed Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reflection-based LLM agent that augments standard self-reflection with positive (sweet) and negative (sour) experiences and a managed dual-buffer memory to improve decision making in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sweet&Sour</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based actor agent (prompting over a causal LLM) combined with a separate reflection process and a managed memory module. The agent samples actions from a policy implemented via prompting on an LLM (evaluated with GPT-4o, Mistral Large 2, and Llama 3.1 8B). After actions/attempts it runs an explicit reflection step that verbalizes causes of success and failure; reflections are stored in a dual-buffer memory and used to augment context on subsequent decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term and long-term (episodic-style reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Dual-buffer managed memory: a short-term buffer that accumulates per-subgoal reflection tuples and a long-term buffer that persists reflections across attempts. Short-term entries are tuples (reflection_t, o_t, a_t, r_t); on attempt end or task completion short-term entries are moved to long-term; failed-attempt reflections may be added immediately to long-term.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Verbalized reflections (insights), raw recent observations o_t, chosen actions a_t, and received rewards r_t — recorded as tuples representing positive subgoal reflections and failure analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage</strong></td>
                            <td>The agent retrieves stored reflections as additional context for the LLM's reflection/prompting step to shape planning on subsequent attempts (i.e., uses prior successful strategies and failure analyses to inform actions). Managed memory selects and surfaces recent and relevant reflections (both successes and failures) during decision-time prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A science-experiment-oriented interactive text environment with 10 interconnected locations (greenhouse, workshop, etc.), >200 objects, ~25 action templates; tasks require multi-step procedural scientific reasoning (chemistry, electricity, life sciences).</td>
                        </tr>
                        <tr>
                            <td><strong>game_complexity</strong></td>
                            <td>Test set: 30 distinct tasks with up to 10 variations each; average optimal decision depth ≈ 50 steps; large action branching factor due to ~25 action templates and many objects (~200). Success score reported on 0–100 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average success score using Sweet&Sour: Llama 8B: 32.5 (score units 0-100), Mistral Large 2: 44.6, GPT-4o: 54.6 (averaged across all ScienceWorld task variations). Also per-task improvements shown in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation where Sweet&Sour only sampled from failures (i.e., excluded positive experiences) produced lower averages similar to Reflexion: Llama 8B: 24.6, Mistral Large 2: 31.1, GPT-4o: 44.9 (demonstrating the performance drop when positive experiences are not stored/used).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Including positive experiences in memory provides richer decision-time context, helps sustain momentum after early successes (reduces 'tilt'), and yields higher average success scores, especially for smaller models; overall improved planning and generalization across tasks and model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Managed memory that stores both successful (positive) and failed (negative) reflections as short-term then long-term entries improves agent performance across LLM sizes on ScienceWorld; leveraging positive experiences substantially increases scores (notably for smaller LLMs) and outperforms failure-only reflection baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive Experience Reflection for Agents in Interactive Text Environments', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e232.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e232.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use memory mechanisms to solve text-based games, including details about the memory architecture, the games played, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (Language Agents with Verbal Reinforcement Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ReAct-based LLM agent that performs self-reflection by verbalizing reasons for failed attempts and stores those reflections in long-term memory to improve future runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based agent built on the ReAct prompting paradigm that adds a self-reflection loop: after unsuccessful attempts it uses the LLM to verbalize failure causes and proposed corrections, storing these verbal reflections in persistent long-term memory (it iterates across up to four rounds/attempts to accumulate and use these reflections). Evaluated as a baseline in ScienceWorld experiments using multiple LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term persistent episodic memory of reflections</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Persistent history / long-term buffer where reflections from unsuccessful attempts are appended and carried across attempts (described as a persistent history of insights across attempts). Reflexion runs multiple rounds accumulating this long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Textual reflections on unsuccessful attempts: summaries of actions and observations that led to failure, hypothesized reasons for failure, and suggested alternative actions/strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage</strong></td>
                            <td>Reflections stored in long-term memory are provided as additional contextual input to the LLM during subsequent attempts to influence planning and action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same ScienceWorld benchmark: multi-room science experiment tasks with many objects and multi-step procedures; used to evaluate LLM agent planning and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>game_complexity</strong></td>
                            <td>As reported: 30 tasks with up to 10 variations each, ~200 objects, ~25 action templates, average optimal depth ≈ 50 steps; success scored 0–100.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average success score using Reflexion (with its long-term reflection memory): Llama 8B: 21.7, Mistral Large 2: 27.6, GPT-4o: 45.3 (values taken from Table 1 averages across task variations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct baseline (no long-term reflection memory) averages: Llama 8B: 20.5, Mistral Large 2: 24.8, GPT-4o: 36.0 (from Table 1); these serve as a no-memory comparison in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Long-term storage of failure reflections can improve planning across repeated attempts by providing prior failure analyses as context, often yielding better scores than no-reflection baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Focused primarily on failures (not on reinforcing successes), which the paper argues can lead to 'tilt' (loss of momentum) when the agent is initially successful; also reported to have reduced efficacy with smaller LLMs and can get stuck despite accumulated failure reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reflexion's failure-centric long-term memory improves performance relative to non-reflective baselines in many cases, but neglecting positive experiences and relying only on failure reflections can limit sustained performance, especially for tasks where early successes should be reinforced and when using smaller LLMs; Sweet&Sour's inclusion of positive experiences and a managed short/long-term memory outperforms Reflexion across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positive Experience Reflection for Agents in Interactive Text Environments', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
                <li>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-232",
    "paper_id": "paper-9dc5e34106d82704aa79e30a04f325af66e27103",
    "extraction_schema_id": "extraction-schema-13",
    "extracted_data": [
        {
            "name_short": "Sweet&Sour",
            "name_full": "Sweet & Sour (Positive Experience Reflection with Managed Memory)",
            "brief_description": "A reflection-based LLM agent that augments standard self-reflection with positive (sweet) and negative (sour) experiences and a managed dual-buffer memory to improve decision making in text-based games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Sweet&Sour",
            "agent_description": "An LLM-based actor agent (prompting over a causal LLM) combined with a separate reflection process and a managed memory module. The agent samples actions from a policy implemented via prompting on an LLM (evaluated with GPT-4o, Mistral Large 2, and Llama 3.1 8B). After actions/attempts it runs an explicit reflection step that verbalizes causes of success and failure; reflections are stored in a dual-buffer memory and used to augment context on subsequent decisions.",
            "agent_type": "LLM-based",
            "uses_memory": true,
            "memory_type": "short-term and long-term (episodic-style reflections)",
            "memory_structure": "Dual-buffer managed memory: a short-term buffer that accumulates per-subgoal reflection tuples and a long-term buffer that persists reflections across attempts. Short-term entries are tuples (reflection_t, o_t, a_t, r_t); on attempt end or task completion short-term entries are moved to long-term; failed-attempt reflections may be added immediately to long-term.",
            "memory_content": "Verbalized reflections (insights), raw recent observations o_t, chosen actions a_t, and received rewards r_t — recorded as tuples representing positive subgoal reflections and failure analyses.",
            "memory_usage": "The agent retrieves stored reflections as additional context for the LLM's reflection/prompting step to shape planning on subsequent attempts (i.e., uses prior successful strategies and failure analyses to inform actions). Managed memory selects and surfaces recent and relevant reflections (both successes and failures) during decision-time prompting.",
            "game_benchmark": "ScienceWorld",
            "game_description": "A science-experiment-oriented interactive text environment with 10 interconnected locations (greenhouse, workshop, etc.), &gt;200 objects, ~25 action templates; tasks require multi-step procedural scientific reasoning (chemistry, electricity, life sciences).",
            "game_complexity": "Test set: 30 distinct tasks with up to 10 variations each; average optimal decision depth ≈ 50 steps; large action branching factor due to ~25 action templates and many objects (~200). Success score reported on 0–100 scale.",
            "performance_with_memory": "Average success score using Sweet&Sour: Llama 8B: 32.5 (score units 0-100), Mistral Large 2: 44.6, GPT-4o: 54.6 (averaged across all ScienceWorld task variations). Also per-task improvements shown in Table 1.",
            "performance_without_memory": "Ablation where Sweet&Sour only sampled from failures (i.e., excluded positive experiences) produced lower averages similar to Reflexion: Llama 8B: 24.6, Mistral Large 2: 31.1, GPT-4o: 44.9 (demonstrating the performance drop when positive experiences are not stored/used).",
            "has_ablation_study": true,
            "memory_benefits": "Including positive experiences in memory provides richer decision-time context, helps sustain momentum after early successes (reduces 'tilt'), and yields higher average success scores, especially for smaller models; overall improved planning and generalization across tasks and model sizes.",
            "memory_limitations": null,
            "key_findings": "Managed memory that stores both successful (positive) and failed (negative) reflections as short-term then long-term entries improves agent performance across LLM sizes on ScienceWorld; leveraging positive experiences substantially increases scores (notably for smaller LLMs) and outperforms failure-only reflection baselines.",
            "uuid": "e232.0",
            "source_info": {
                "paper_title": "Positive Experience Reflection for Agents in Interactive Text Environments",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (Language Agents with Verbal Reinforcement Learning)",
            "brief_description": "A ReAct-based LLM agent that performs self-reflection by verbalizing reasons for failed attempts and stores those reflections in long-term memory to improve future runs.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "Reflexion",
            "agent_description": "An LLM-based agent built on the ReAct prompting paradigm that adds a self-reflection loop: after unsuccessful attempts it uses the LLM to verbalize failure causes and proposed corrections, storing these verbal reflections in persistent long-term memory (it iterates across up to four rounds/attempts to accumulate and use these reflections). Evaluated as a baseline in ScienceWorld experiments using multiple LLMs.",
            "agent_type": "LLM-based",
            "uses_memory": true,
            "memory_type": "long-term persistent episodic memory of reflections",
            "memory_structure": "Persistent history / long-term buffer where reflections from unsuccessful attempts are appended and carried across attempts (described as a persistent history of insights across attempts). Reflexion runs multiple rounds accumulating this long-term memory.",
            "memory_content": "Textual reflections on unsuccessful attempts: summaries of actions and observations that led to failure, hypothesized reasons for failure, and suggested alternative actions/strategies.",
            "memory_usage": "Reflections stored in long-term memory are provided as additional contextual input to the LLM during subsequent attempts to influence planning and action selection.",
            "game_benchmark": "ScienceWorld",
            "game_description": "Same ScienceWorld benchmark: multi-room science experiment tasks with many objects and multi-step procedures; used to evaluate LLM agent planning and reasoning.",
            "game_complexity": "As reported: 30 tasks with up to 10 variations each, ~200 objects, ~25 action templates, average optimal depth ≈ 50 steps; success scored 0–100.",
            "performance_with_memory": "Average success score using Reflexion (with its long-term reflection memory): Llama 8B: 21.7, Mistral Large 2: 27.6, GPT-4o: 45.3 (values taken from Table 1 averages across task variations).",
            "performance_without_memory": "ReAct baseline (no long-term reflection memory) averages: Llama 8B: 20.5, Mistral Large 2: 24.8, GPT-4o: 36.0 (from Table 1); these serve as a no-memory comparison in the paper.",
            "has_ablation_study": false,
            "memory_benefits": "Long-term storage of failure reflections can improve planning across repeated attempts by providing prior failure analyses as context, often yielding better scores than no-reflection baselines.",
            "memory_limitations": "Focused primarily on failures (not on reinforcing successes), which the paper argues can lead to 'tilt' (loss of momentum) when the agent is initially successful; also reported to have reduced efficacy with smaller LLMs and can get stuck despite accumulated failure reflections.",
            "key_findings": "Reflexion's failure-centric long-term memory improves performance relative to non-reflective baselines in many cases, but neglecting positive experiences and relying only on failure reflections can limit sustained performance, especially for tasks where early successes should be reinforced and when using smaller LLMs; Sweet&Sour's inclusion of positive experiences and a managed short/long-term memory outperforms Reflexion across models.",
            "uuid": "e232.1",
            "source_info": {
                "paper_title": "Positive Experience Reflection for Agents in Interactive Text Environments",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1
        },
        {
            "paper_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "rating": 1
        }
    ],
    "cost": 0.0127295,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Positive Experience Reflection for Agents in Interactive Text Environments</h1>
<p>Philip Lippmann Matthijs T.J. Spaan Jie Yang<br>Delft University of Technology<br>{p.lippmann, m.t.j.spaan, j.yang-3}@tudelft.nl</p>
<h4>Abstract</h4>
<p>Intelligent agents designed for interactive environments face significant challenges in text-based games, a domain that demands complex reasoning and adaptability. While agents based on large language models (LLMs) using self-reflection have shown promise, they struggle when initially successful and exhibit reduced effectiveness when using smaller LLMs. We introduce Sweet\&amp;Sour, a novel approach that addresses these limitations in existing reflection methods by incorporating positive experiences and managed memory to enrich the context available to the agent at decision time. Our comprehensive analysis spans both closed- and opensource LLMs and demonstrates the effectiveness of Sweet\&amp;Sour in improving agent performance, particularly in scenarios where previous approaches fall short.</p>
<h2>1 Introduction</h2>
<p>Intelligent agents, designed to interact with and make decisions in dynamic environments, have become a central focus in AI research, with text-based games (TBGs) emerging as a particularly challenging domain for evaluating these agents' reasoning, adaptability, and learning abilities [2, 21]. Originally popular in the 1970s as text adventure games, ${ }^{1}$ TBGs present players with textual descriptions of environments, requiring them to input natural language commands to achieve objectives [4]. For instance, determining if a metal fork is conductive involves locating the fork, assembling a circuit, and analyzing the result. Navigating TBGs demands that agents exhibit a combination of abilities, including planning, memory retention, spatial reasoning, and common sense knowledge [22].</p>
<p>Previously, deep reinforcement learning and behavior cloning were the primary approaches to develop agents to play TBGs [1, 25]. However, recent research shows that agents based on pretrained large language models (LLMs) are more effective at navigating TBGs [11]. A key factor in their success is the integration of internal reflection to improve planning [24, 9, 7].
Self-reflection, closely related to self-refinement, is a form of reasoning that occurs after receiving binary or scalar feedback from the environment [12]. In this process, the LLM reviews its actions and their outcomes, considering what went wrong and potential ways to improve [23]. By iteratively adjusting its strategy based on verbal reinforcement, conveyed through textual feedback, the agent refines its planning for subsequent attempts [17]. However, reflection also has several limitations, including 1) underwhelming performance when agents are correct initially [10], 2) significantly worse efficacy when using smaller LLMs [11], and 3) dependence on external feedback [29].
Our Contributions In this work we conduct a comprehensive analysis of LLM-based agents employing reflection approaches in TBGs and evaluate their performance across closed- and open-source LLMs. To address the limitations of poor performance when agents are initially successful and the diminished efficacy of smaller LLMs, we propose Sweet\&amp;Sour to leverage positive experiences and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of used prompting methods to play ScienceWorld. ReAct introduces a THINK action to explicitly reason regarding the next step. Reflexion leverages self-reflection across attempts to learn from unsuccessful tries and stores these in memory. Sweet&amp;Sour not only performs self-reflection after failures but also after each completed sub goal, making its reflection instantly available.</p>
<p>managed memory to create a richer context for self-reflection. Our findings demonstrate that our method improves the performance of agents using reflection, particularly in scenarios where they previously struggled, enabling more robust and generalizable learning across tasks and model sizes.</p>
<h2>2 Methodology</h2>
<p><strong>Background</strong> Assuming an LLM behaving as an actor model as part of our agent – <em>i.e.</em> generating actions based on the current state and policy, analogous to traditional policy-based RL setups – we sample an action $a_t$ from the current policy $\pi_{\theta}$ at time $t$ and receive an observation from the environment $o_t$. When a game begins, the agent makes its first observation $o_0$ at time step $t = 0$. This first observation differs from subsequent ones, as it consists of the goal description $d$, as well as an analysis of the starting room (<em>i.e.</em> the output of the "look around" command). Subsequently, the agent can perform an action $a_t$ at each time step and receives a corresponding observation $o_t$ from the environment. The agent has an inventory $i_t$ in which to store items. Each task consists of a number of sub tasks (such as finding a key object), the completion of which grants the agent a sparse reward, which adds to its current reward $r_t$. The game continues until the agent has achieved the goal outlined in $d$ and receives the full reward as final score, or the maximum number of steps – which we set to 150 – is reached, in which case $r_t$ will become the final score. A detailed problem formulation and assumptions we make are given in appendix A.</p>
<p><strong>Self-Reflection</strong> Reflection occurs in addition to the acting LLM. Here, the agent reviews the $a_t$ and $o_t$ associated with previous unsuccessful attempts to verbalize the reason for failure. This process typically involves maintaining a persistent history of insights gained across attempts, which the LLM uses as additional context for its reflections to improve future decision making for the next attempt [17]. However, since other self-reflection methods focus on learning from failures [16, 28, 30, 8, 27], they overlook the importance of reinforcing successful behaviors in a similar way.</p>
<p><strong>Sweet&amp;Sour</strong> To address the limitations of existing reflection methods, we enhance self-reflection by incorporating both positive (<em>sweet</em>) and negative (<em>sour</em>) experiences into the reflection process. This enables the agent to learn effectively from successful actions by reinforcing strategies that lead to positive outcomes while still learning from failures. When the current policy is achieving rewards, we query the agent to extrapolate from it, encouraging the agent to verbalize what made its current policy successful and what can be generalized from this. This is visualized in figure 1 and an example of this is shown in appendix B. Our method is broadly applicable to agents in interactive text environments.</p>
<p>with feedback using self-reflection, including those that build additional complexity on top of the core reflection loop, such as grounding [11] or gradient learning [27]. We leave the study of these additional use cases for future work.</p>
<p>Previous works store their reflections gained from unsuccessful attempts in something akin to a longterm memory to make them available to the agent across attempts [8, 17]. Instead, to complement Sweet\&amp;Sour, we propose a managed memory approach to store and retrieve relevant reflections. This is implemented using a dual-buffer structure, where experiences are stored in two categories: short-term memory and long-term memory, based on their outcome (success or failure) and recency. Initially, if sub goals are reached, short-term memories of the made reflections are stored in a temporary buffer. Each short-term memory consists of a tuple $\left(\right.$ reflection $\left.<em t="t">{t}, o</em>\right)$. Once a task is completed or an attempt ends, all short-term memories are moved to long-term memory. Failed attempts' reflections are immediately added to long-term memory for the next attempt and short-term memory collection ends.}, a_{t}, r_{t</p>
<h1>3 Experiments</h1>
<p>Data \&amp; Environment We use the ScienceWorld benchmark [21], which provides a versatile setting for evaluating agents in science experiment tasks across 10 interconnected locations, such as a greenhouse and a workshop, with over 200 objects and 25 action templates, generating a vast and dynamic search space. We use the test set for our evaluation, which provides up to 10 variations of each of the 30 distinct tasks. These interactive tasks cover various topics, including chemistry and electricity, and have an average optimal decision depth of 50 steps. An example task is shown in appendix C. For details of all tasks and the environment, we refer to [21]. We elect to use ScienceWorld instead of previous interactive text environment benchmarks such as TWC [13] and ALFWorld [18], due to their relative simplicity for current LLM-based agents. We measure performance using the success score, which is always between 0 and 100. Completing a task implies completing every sub task, receiving the full reward, and thus a score of 100 .
Baselines CALM [25] is a reranking method that integrates a deep reinforced relevance network (DRRN) [5] with a causal language model fine-tuned using oracle trajectories. The causal language model acquires task- and environment-specific knowledge through imitation learning, while DRRN trains a policy network to rerank the language model's predictions. We use ReAct [26] as our baseline LLM-based agent. ReAct composes useful information at each time step by reasoning over the current context (e.g. decomposing task or common sense knowledge query) and carries it forward to the context of the following time step. This reasoning action does not affect the environment and may be considered few-shot in-context learning. To contextualize our work, we compare our method against Reflexion [17], an agent built on ReAct that employs a self-reflection mechanism to iteratively improve its performance across rounds upon encountering failure based on feedback from the environment. As such, it runs over up to four rounds as it builds up its long-term memory. For all agents, we evaluate their performance using LLMs of different sizes and complexities to assess the performance of each method across varying computational resources. In descending order of parameter count, we select GPT-4o (gpt-4o-2024-08-06)[15], Mistral Large 2 (mistral-large-2407), and Llama 3.1 8B (llama-3.1-8b-instruct)[3], accessing each through its respective APIs.
Main Results The results are shown in table 1. We find that Sweet\&amp;Sour outperforms the baseline methods across all LLMs, setting the highest average score at 54.6 using GPT-4o. The performance gap between Sweet\&amp;Sour and the other methods widens for smaller models with a lower parameter count. For instance, it achieves 44.6 compared to Reflexion's 27.6 on Mistral Large 2, and 32.5 compared to 21.7 on Llama 8B - indicating that our method is more suitable for scenarios with limited computational resources. When we modify our method to only sample from failures, performance drops significantly to a level similar to Reflexion - scores decrease to 24.6, 31.1, and 44.9 for Llama 8B, Mistral Large 2, and GPT-4o, respectively. As such, incorporating positive experiences indeed leads to better reflections, mimicking how humans learn from both positive and negative experiences, resulting in improved decision-making.
Anti-Tilt In highly challenging tasks, such as 1-1 and 8-2, all methods tend to struggle, while in simpler tasks, most methods succeed based on the LLM's inherent capabilities alone. However, medium-difficulty tasks, such as 3-2 and 3-3, reveal a critical performance gap between our method and previous approaches. We note that this occurs because traditional methods fail to reflect on early</p>
<p>Table 1: Results on the ScienceWorld benchmark. For each method, we use GPT-4o (GPT), Mistral Large 2 (ML2), and Llama 8B (L8B). Each value is an average of across all task variations.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>CALM</th>
<th>ReAct</th>
<th></th>
<th></th>
<th>Reflexion</th>
<th></th>
<th></th>
<th>Sweet\&amp;Sour (ours)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CALM</td>
<td>L8B</td>
<td>ML2</td>
<td>GPT</td>
<td>L8B</td>
<td>ML2</td>
<td>GPT</td>
<td>L8B</td>
<td>ML2</td>
<td>GPT</td>
</tr>
<tr>
<td>1-1 (Boil)</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>3.8</td>
<td>0.0</td>
<td>0.0</td>
<td>5.1</td>
<td>0.0</td>
<td>7.2</td>
<td>9.6</td>
</tr>
<tr>
<td>1-2 (Melt)</td>
<td>0.0</td>
<td>8.4</td>
<td>10.3</td>
<td>11.8</td>
<td>0.0</td>
<td>0.0</td>
<td>10.0</td>
<td>11.4</td>
<td>12.1</td>
<td>12.8</td>
</tr>
<tr>
<td>1-3 (Freeze)</td>
<td>0.0</td>
<td>1.5</td>
<td>0.0</td>
<td>8.1</td>
<td>0.0</td>
<td>2.3</td>
<td>8.3</td>
<td>2.4</td>
<td>3.1</td>
<td>8.9</td>
</tr>
<tr>
<td>1-4 (Change state)</td>
<td>0.0</td>
<td>1.0</td>
<td>4.7</td>
<td>10.0</td>
<td>0.0</td>
<td>0.0</td>
<td>4.2</td>
<td>1.7</td>
<td>2.9</td>
<td>9.2</td>
</tr>
<tr>
<td>2-1 (Thermometer)</td>
<td>1.0</td>
<td>5.1</td>
<td>7.8</td>
<td>7.7</td>
<td>3.4</td>
<td>4.2</td>
<td>7.6</td>
<td>7.8</td>
<td>9.7</td>
<td>10.9</td>
</tr>
<tr>
<td>2-2 (Melting)</td>
<td>1.0</td>
<td>6.7</td>
<td>6.3</td>
<td>5.9</td>
<td>3.3</td>
<td>3.3</td>
<td>26.2</td>
<td>7.9</td>
<td>36.8</td>
<td>46.0</td>
</tr>
<tr>
<td>2-3 (Melting)</td>
<td>5.0</td>
<td>9.1</td>
<td>11.8</td>
<td>23.4</td>
<td>13.2</td>
<td>14.7</td>
<td>22.6</td>
<td>15.2</td>
<td>29.0</td>
<td>38.3</td>
</tr>
<tr>
<td>3-1 (Power 1)</td>
<td>7.0</td>
<td>18.8</td>
<td>24.6</td>
<td>57.2</td>
<td>21.2</td>
<td>51.5</td>
<td>78.4</td>
<td>28.6</td>
<td>75.4</td>
<td>81.1</td>
</tr>
<tr>
<td>3-2 (Power 2)</td>
<td>2.0</td>
<td>10.2</td>
<td>24.7</td>
<td>55.6</td>
<td>9.5</td>
<td>11.9</td>
<td>24.7</td>
<td>23.3</td>
<td>44.5</td>
<td>58.0</td>
</tr>
<tr>
<td>3-3 (Conductivity 1)</td>
<td>2.0</td>
<td>52.4</td>
<td>51.7</td>
<td>73.0</td>
<td>9.2</td>
<td>25.8</td>
<td>72.1</td>
<td>59.1</td>
<td>69.2</td>
<td>75.7</td>
</tr>
<tr>
<td>3-4 (Conductivity 2)</td>
<td>10.0</td>
<td>54.2</td>
<td>64.9</td>
<td>89.7</td>
<td>35.4</td>
<td>41.6</td>
<td>75.1</td>
<td>62.7</td>
<td>60.3</td>
<td>67.3</td>
</tr>
<tr>
<td>4-1 (Find 1)</td>
<td>54.0</td>
<td>17.3</td>
<td>18.7</td>
<td>27.5</td>
<td>44.6</td>
<td>48.1</td>
<td>62.3</td>
<td>41.7</td>
<td>71.7</td>
<td>74.2</td>
</tr>
<tr>
<td>4-2 (Find 2)</td>
<td>10.0</td>
<td>69.1</td>
<td>71.6</td>
<td>80.3</td>
<td>68.4</td>
<td>75.7</td>
<td>87.3</td>
<td>76.8</td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td>4-3 (Find 3)</td>
<td>8.0</td>
<td>21.3</td>
<td>42.8</td>
<td>47.7</td>
<td>18.4</td>
<td>16.5</td>
<td>17.3</td>
<td>20.9</td>
<td>21.5</td>
<td>34.3</td>
</tr>
<tr>
<td>4-4 (Find 4)</td>
<td>2.0</td>
<td>15.7</td>
<td>15.2</td>
<td>19.3</td>
<td>39.6</td>
<td>46.6</td>
<td>100.0</td>
<td>55.1</td>
<td>87.8</td>
<td>100.0</td>
</tr>
<tr>
<td>5-1 (Grow plant)</td>
<td>4.0</td>
<td>10.8</td>
<td>10.8</td>
<td>10.0</td>
<td>7.2</td>
<td>7.2</td>
<td>7.9</td>
<td>14.2</td>
<td>14.6</td>
<td>17.4</td>
</tr>
<tr>
<td>5-2 (Grow fruit)</td>
<td>3.0</td>
<td>18.1</td>
<td>18.5</td>
<td>19.2</td>
<td>30.8</td>
<td>51.4</td>
<td>34.6</td>
<td>51.5</td>
<td>55.6</td>
<td>60.2</td>
</tr>
<tr>
<td>6-1 (Chemistry 1)</td>
<td>6.0</td>
<td>37.8</td>
<td>42.9</td>
<td>58.6</td>
<td>27.1</td>
<td>29.7</td>
<td>70.2</td>
<td>37.9</td>
<td>61.1</td>
<td>70.2</td>
</tr>
<tr>
<td>6-2 (Chemistry 2)</td>
<td>3.0</td>
<td>25.0</td>
<td>27.1</td>
<td>50.6</td>
<td>14.4</td>
<td>28.0</td>
<td>69.8</td>
<td>27.2</td>
<td>51.9</td>
<td>83.1</td>
</tr>
<tr>
<td>6-3 (Chemistry 3)</td>
<td>6.0</td>
<td>14.4</td>
<td>17.5</td>
<td>39.7</td>
<td>38.9</td>
<td>31.1</td>
<td>16.7</td>
<td>45.3</td>
<td>53.7</td>
<td>61.5</td>
</tr>
<tr>
<td>7-1 (Lifespan 1)</td>
<td>10.0</td>
<td>37.0</td>
<td>41.7</td>
<td>60.0</td>
<td>75.0</td>
<td>75.0</td>
<td>100.0</td>
<td>75.0</td>
<td>88.2</td>
<td>100.0</td>
</tr>
<tr>
<td>7-2 (Lifespan 2)</td>
<td>4.0</td>
<td>50.5</td>
<td>50.7</td>
<td>67.5</td>
<td>60.0</td>
<td>71.9</td>
<td>81.4</td>
<td>70.5</td>
<td>77.0</td>
<td>80.0</td>
</tr>
<tr>
<td>7-3 (Lifespan 3)</td>
<td>4.0</td>
<td>33.7</td>
<td>38.2</td>
<td>50.0</td>
<td>29.5</td>
<td>33.7</td>
<td>75.0</td>
<td>51.1</td>
<td>54.2</td>
<td>84.6</td>
</tr>
<tr>
<td>8-1 (Identify life 1)</td>
<td>0.0</td>
<td>5.1</td>
<td>18.9</td>
<td>25.3</td>
<td>1.7</td>
<td>1.7</td>
<td>3.4</td>
<td>11.1</td>
<td>10.3</td>
<td>14.2</td>
</tr>
<tr>
<td>8-2 (Identify life 2)</td>
<td>0.0</td>
<td>6.4</td>
<td>7.4</td>
<td>8.0</td>
<td>7.4</td>
<td>8.0</td>
<td>8.0</td>
<td>5.0</td>
<td>7.4</td>
<td>7.4</td>
</tr>
<tr>
<td>9-1 (Measure angle)</td>
<td>0.0</td>
<td>28.5</td>
<td>33.0</td>
<td>42.5</td>
<td>56.9</td>
<td>55.1</td>
<td>57.1</td>
<td>68.4</td>
<td>70.3</td>
<td>75.0</td>
</tr>
<tr>
<td>9-2 (Friction 1)</td>
<td>3.0</td>
<td>14.5</td>
<td>22.6</td>
<td>43.1</td>
<td>23.4</td>
<td>29.3</td>
<td>100.0</td>
<td>33.3</td>
<td>36.7</td>
<td>62.0</td>
</tr>
<tr>
<td>9-3 (Friction 2)</td>
<td>2.0</td>
<td>2.9</td>
<td>14.5</td>
<td>42.8</td>
<td>1.3</td>
<td>33.6</td>
<td>59.6</td>
<td>7.2</td>
<td>51.9</td>
<td>63.1</td>
</tr>
<tr>
<td>10-1 (Genetics 1)</td>
<td>2.0</td>
<td>25.7</td>
<td>27.3</td>
<td>26.4</td>
<td>5.6</td>
<td>9.8</td>
<td>50.4</td>
<td>38.9</td>
<td>48.6</td>
<td>78.8</td>
</tr>
<tr>
<td>10-2 (Genetics 2)</td>
<td>2.0</td>
<td>13.2</td>
<td>19.1</td>
<td>17.2</td>
<td>6.2</td>
<td>21.5</td>
<td>22.7</td>
<td>23.6</td>
<td>24.0</td>
<td>54.8</td>
</tr>
<tr>
<td>Average</td>
<td>5.07</td>
<td>20.5</td>
<td>24.8</td>
<td>36.0</td>
<td>21.7</td>
<td>27.6</td>
<td>45.3</td>
<td>32.5</td>
<td>44.6</td>
<td>54.6</td>
</tr>
</tbody>
</table>
<p>successes, leaving them with less contextual understanding to carry momentum forward, leading to a sustained performance decline or "tilt." By contrast, we theorize that Sweet\&amp;Sour's reflection on both successes and failures provides a richer context, enabling it to sustain and build upon initial successes. For instance, on task 3-2, Sweet\&amp;Sour achieves a score of 68.0 with GPT-4o, significantly outperforming Reflexion's 24.7 while beating ReAct's 55.6 by a smaller margin, indicating that Reflexion, although it is overall more capable, got stuck despite its self-reflection.</p>
<h1>4 Conclusion</h1>
<p>In an attempt to improve agent performance and reduce sensitivity to the LLM used, our study embarks on an analysis of reflection mechanisms for LLM-based agents. Existing work focuses on learning from failures only. Although these methods improve planning by analyzing past mistakes, they struggle when agents are initially successful and when using smaller LLMs. The primary contribution of our work is Sweet\&amp;Sour, a novel reflection technique for LLM-based agents in TBGs that leverages positive experiences to improve agent self-reflection. Our comprehensive analysis demonstrates the effectiveness of Sweet\&amp;Sour in enhancing agent adaptability and reasoning, particularly in challenging situations where previous approaches struggled.
Limitations Despite promising results, our work has limitations. LLMs do not provide guarantees regarding their reasoning capabilities. Additionally, our evaluation is conducted using a single environment, which, while comprehensive, does not cover all types of interactive scenarios. We leave the exploration of additional environments to future work.</p>
<p>Acknowledgments</p>
<p>This work was supported by an Oracle for Research Grant Award, as well as SURF Grant EINF-8535.</p>
<h2>References</h2>
<p>[1] Prithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1358. URL https://aclanthology. org/N19-1358.
[2] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018.
[3] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet,</p>
<p>Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.
[4] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903-7910, 2020.
[5] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, Berlin, Germany, Au-</p>
<p>gust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1153. URL https://aclanthology.org/P16-1153.
[6] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1153. URL https://aclanthology.org/P16-1153.
[7] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu. A survey on large language model-based game agents, 2024. URL https://arxiv. org/abs/2404.02039.
[8] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender ai agent: Integrating large language models for interactive recommendations, 2024. URL https://arxiv.org/abs/2308.16505.
[9] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey, 2024. URL https://arxiv.org/abs/2402.02716.
[10] Yanhong Li, Chenghao Yang, and Allyson Ettinger. When hindsight is not 20/20: Testing limits on reflective thinking in large language models, 2024. URL https://arxiv.org/abs/2404. 09129 .
[11] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. ArXiv preprint, abs/2305.17390, 2023. URL https://arxiv.org/abs/2305.17390.
[12] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S37hOerQLB.
[13] Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. Proceedings of the AAAI Conference on Artificial Intelligence, 35(10):9018-9027, May 2021. doi: 10.1609/aaai.v35i10.17090. URL https://ojs.aaai.org/index.php/AAAI/ article/view/17090.
[14] Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for textbased games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1001. URL https : //aclanthology.org/D15-1001.
[15] OpenAI. Gpt-4 technical report, 2023.
[16] Matthew Renze and Erhan Guven. Self-reflection in llm agents: Effects on problem-solving performance, 2024. URL https://arxiv.org/abs/2405.06682.
[17] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.
[18] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. URL https://arxiv.org/abs/2010.03768.
[19] Matthijs TJ Spaan. Partially observable markov decision processes. In Reinforcement learning: State-of-the-art, pages 387-414. Springer, 2012.</p>
<p>[20] Ruo Yu Tao, Marc-Alexandre Côté, Xingdi Yuan, and Layla El Asri. Towards solving text-based games by producing adaptive action spaces, 2018.
[21] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. ScienceWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11279-11298, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.775. URL https://aclanthology.org/2022.emnlp-main. 775.
[22] Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Dayiheng Liu, Yike Guo, and Jie Fu. Interactive natural language processing, 2023. URL https://arxiv.org/abs/2305.13246.
[23] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, 2024. URL https://arxiv.org/abs/2302.01560.
[24] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey, 2023. URL https://arxiv.org/abs/2309.07864.
[25] Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore: Language models for action generation in text-based games. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8736-8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704. URL https://aclanthology.org/2020.emnlp-main. 704.
[26] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL https: //arxiv.org/abs/2210.03629.
[27] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh R N, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil L Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=KOZu91CzbK.
[28] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, and Yaodong Yang. Proagent: Building proactive cooperative agents with large language models, 2024. URL https://arxiv.org/abs/2308.11339.
[29] Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. Self-contrast: Better reflection through inconsistent solving perspectives, 2024. URL https://arxiv.org/abs/2401.02009.
[30] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and optimization, 2024. URL https://arxiv.org/abs/2402.17574.</p>
<h1>A Background</h1>
<p>One may consider every TBG to be a partially observable Markov decision process (POMDP) [19] where the environment state is never observed directly. This POMDP may be formalized as $\langle S, T, A, \Omega, R, \gamma\rangle$, where $\gamma \in[0,1]$ denotes the reward discount factor. $S$ denotes the set of states $s$ that contain the internal information of the game - such as objects found throughout the game or the player's location - not all of which may be visible to the agent at any given time. $A$ denotes the action space made up of individual text actions $a$ issued by the player. $\Omega$ denotes the observation function. Further, $o \in O$ denotes the observations made by the player. The observation $o_{t}$ of the agent at time $t$ depends on the current state $s_{t}$, as well as the previous action $a_{t-1}$, which may be formalized as $\Omega\left(o_{t} \mid s_{t}, a_{t-1}\right)$. Seeing as the agent can only observe and interact with the environment of a TBG via natural language, each observation is composed of a sequence of tokens $o_{t}=\left{\hat{o}<em t="t">{t}^{1}, \cdots, \hat{o}</em>}^{N}\right}$, as are their actions $a_{t}=\left{\hat{a<em t="t">{t}^{1}, \cdots, \hat{a}</em>\right}$.
In the context of TBGs, an action $a_{t}$ is considered admissible at a state $s_{t}$ if it is capable of changing the game's state, i.e., if it can lead to a transition to a new state $s_{t+1}$ that is different from the current state $s_{t}$. The environment's state transition is modeled through a probabilistic function $T\left(s_{t+1} \mid s_{t}, a_{t}\right)$. Traditionally, admissible actions in state $s_{t}$ could deterministically lead to a new state $s_{t+1}$. However, we use a more general approach where all actions, whether admissible or not, are included in the state transition function. Non-admissible actions, which do not lead to a change in the game's state, result in a transition back to the original state $s_{t}$ with probability 1 . In contrast, admissible actions lead to different states with their own probability. The admissible action set is bound to be significant for quests with a sufficiently large branching factor. While admissible, many action candidates are bound to be suboptimal.
The reward $r$ received by the agent - the discounted sum of which, $\mathbb{E}\left[\sum_{t} \gamma^{t} r_{t}\right]$, it aims to maximize - are obtained by $r_{t}=R\left(s_{t}, a_{t}\right)$. In practice, TBGs typically provide sparse positive rewards for completing subgoals and advancing towards completing the game.
A TBG's interpreter can accept any text sequence, but will only recognize text that follows a certain structure. Typically, commands take the form of either a single keyword (such as "look") or a combination involving verbs, objects, and occasionally prepositions. Previous works on TBGs made the assumption that we have access to a predefined set of all admissible actions at each game state and must select the correct action to progress [14, 6]. This is consistent with how some text adventure games are played in the real world [20]. In line with more recent works [11], we make the assumption that we have access to a number of action templates (e.g. connect A to B, pick up C) and subsequently ask the agent to generate the actions as tokens using these templates. This leads to a more challenging action generation process for the agent.}^{M</p>
<h2>B LLM Reflection</h2>
<p>After successfully completing a sub task, the agent is prompted to reflect on its most recent observations and identify the key factors that contributed to its success. This reflection process encourages the agent to verbalize the steps and strategies that led to the desired outcome, helping it create a plan that can be stored in managed memory for future use. A similar procedure is applied in cases of failure. When a task is unsuccessful, the agent is instructed to reflect on alternative actions it could have taken and to devise a revised plan for the next attempt, ensuring continuous learning and improvement. This plan is also stored in memory.
An example trajectory of a ScienceWorld task where the agent reflects on positive and negative experiences is shown in Figure 2. At the end of this trajectory, we show for a single command what a reflection would look like for a successful or unsuccessful choice, in this case picking up an animal. For the unsuccessful case (red), where the agent does not pick up an animal, we reflect on whether another choice would have constituted an animal and thus resulted in a reward. For the successful case where a sub goal is reached (yellow), we reflect on what made the current actions successful and subsequently commit this to memory.</p>
<h1>ScienceWorld Task 8-1</h1>
<h2>Task Description:</h2>
<p>Your task is to find a(n) animal. First, focus on the thing. Then, move it to the red box in the kitchen.</p>
<blockquote>
<p>look around
This room is called the hallway. In it, you see:</p>
</blockquote>
<ul>
<li>a substance called air</li>
<li>a picture</li>
<li>the agent</li>
</ul>
<p>You also see:</p>
<ul>
<li>A door to the greenhouse (open)</li>
<li>A door to the living room (open)</li>
<li>A door to the art studio (open)</li>
<li>A door to the kitchen (open)</li>
<li>A door to the bedroom (open)</li>
<li>A door to the workshop (open)<blockquote>
<p>open door to greenhouse
The door is already open.
$&gt;$ go to greenhouse
You move to the greenhouse.
$&gt;$ open door to outside
The door is already open.
$&gt;$ go to outside
You move to the outside.
$&gt;$ look around
This outside location is called the outside. Here, you see:</p>
</blockquote>
</li>
<li>a substance called air</li>
<li>an axe</li>
<li>a substance called wood</li>
<li>the ground</li>
<li>a fire pit (containing nothing)</li>
<li>a fountain (containing a substance called water)</li>
<li>a butterfly egg</li>
<li>a blue jay egg</li>
<li>a dove egg</li>
<li>the agent</li>
</ul>
<p>You also see:</p>
<ul>
<li>A door to the kitchen (open)</li>
<li>A door to the foundry (open)</li>
<li>A door to the greenhouse (open)</li>
</ul>
<h2>Case #1: Failure</h2>
<p>$&gt;$ focus on axe
You focus on the axe.
$&gt;$ pick up axe
You move the axe to the inventory.
Reflection(I failed because I assumed eggs are not animals. My plan was to go outside to find animals. I tried to pick up an axe. An alternative course of action would have been to go outside and pick up an egg, as it might be considered an animal.)
$\rightarrow$ Managed Memory</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Case</span><span class="w"> </span><span class="err">\#</span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="nx">Success</span>
<span class="p">&gt;</span><span class="w"> </span><span class="nx">focus</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">dove</span><span class="w"> </span><span class="nx">egg</span>
<span class="nx">You</span><span class="w"> </span><span class="nx">focus</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">dove</span><span class="w"> </span><span class="nx">egg</span><span class="p">.</span>
<span class="p">&gt;</span><span class="w"> </span><span class="nx">pick</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">dove</span><span class="w"> </span><span class="nx">egg</span>
<span class="nx">You</span><span class="w"> </span><span class="nx">move</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">dove</span><span class="w"> </span><span class="nx">egg</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">inventory</span><span class="p">.</span>
<span class="nx">Reflection</span><span class="p">(</span><span class="nx">Picking</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">egg</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">correct</span><span class="w"> </span><span class="kd">choice</span><span class="p">.</span><span class="w"> </span><span class="nx">Even</span><span class="w"> </span><span class="nx">though</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">animal</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">egg</span><span class="p">,</span>
<span class="nx">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">still</span><span class="w"> </span><span class="nx">considered</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">animal</span><span class="p">.</span><span class="w"> </span><span class="nx">My</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">go</span><span class="w"> </span><span class="nx">outside</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">find</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">animal</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">wild</span><span class="p">.</span>
<span class="nx">Items</span><span class="w"> </span><span class="nx">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">axe</span><span class="p">,</span><span class="w"> </span><span class="nx">wood</span><span class="p">,</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">landscape</span><span class="w"> </span><span class="nx">elements</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">related</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">animals</span><span class="p">.)</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">rightarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Managed</span><span class="w"> </span><span class="nx">Memory</span>
</code></pre></div>

<p>Figure 2: Example of reflection in the 2-1 Task, where the agent needs to find something.</p>
<h1>C Example ScienceWorld Task</h1>
<p>In this section, we provide a successfully completed task, a variation of Task 1-1, which concerns itself with boiling a substance, in this case water, to change its state. Once the agent has viewed its surroundings and moved to the kitchen, it collects the necessary items and begins its experiment by boiling the water in a pot on the stove. Finally, the agent examines steam and completes the task (highlighted in green). The example is truncated to improve readability. The trajectory is shown in Figure 3.</p>
<h2>ScienceWorld Task 1-1</h2>
<h2>Task Description:</h2>
<p>Your task is to boil water. For compounds without a boiling point, combusting the substance is also acceptable. First, focus on the substance. Then, take actions that will cause it to change its state of matter.</p>
<blockquote>
<p>look around
This room is called the hallway. In it, you see:</p>
</blockquote>
<ul>
<li>a picture</li>
<li>a substance called air</li>
<li>the agent</li>
<li>A door to the green house (open)</li>
<li>A door to the living room (open)</li>
<li>A door to the art studio (open)</li>
<li>A door to the kitchen (open)</li>
<li>A door to the bedroom (open)</li>
<li>A door to the workshop (open)<blockquote>
<p>open door to kitchen
The door is already open.
$&gt;$ go to kitchen
You move to the kitchen.
$&gt;$ look around
This room is called the kitchen. In it, you see:</p>
</blockquote>
</li>
<li>a substance called soap</li>
<li>a painting</li>
<li>a thermometer reading $10^{\circ} \mathrm{C}$</li>
<li>a counter with various items (banana, potato, red apple, orange)</li>
<li>a sink (off, empty)</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">glass</span><span class="w"> </span><span class="n">cup</span><span class="w"> </span><span class="p">(</span><span class="n">empty</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">chair</span><span class="w"> </span><span class="p">(</span><span class="n">empty</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">freezer</span><span class="w"> </span><span class="p">(</span><span class="n">closed</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">lighter</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">stopwatch</span><span class="w"> </span><span class="p">(</span><span class="n">deactivated</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">fridge</span><span class="w"> </span><span class="p">(</span><span class="n">closed</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">cupboard</span><span class="w"> </span><span class="p">(</span><span class="n">closed</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">stove</span><span class="w"> </span><span class="p">(</span><span class="n">turned</span><span class="w"> </span><span class="n">off</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">glass</span><span class="w"> </span><span class="n">jar</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">sodium</span><span class="w"> </span><span class="n">chloride</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">various</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">items</span><span class="w"> </span><span class="p">(</span><span class="n">air</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">agent</span><span class="p">,</span><span class="w"> </span><span class="n">etc</span><span class="o">.</span><span class="p">)</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">thermometer</span>
<span class="n">You</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">inventory</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">open</span><span class="w"> </span><span class="n">cupboard</span>
<span class="n">The</span><span class="w"> </span><span class="n">cupboard</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">open</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">You</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">inventory</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">sink</span>
<span class="n">You</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">sink</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">activate</span><span class="w"> </span><span class="n">sink</span>
<span class="n">The</span><span class="w"> </span><span class="n">sink</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">activated</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">deactivate</span><span class="w"> </span><span class="n">sink</span>
<span class="n">The</span><span class="w"> </span><span class="n">sink</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">deactivated</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">You</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">inventory</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">focus</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">You</span><span class="w"> </span><span class="n">focus</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">water</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">stove</span>
<span class="n">You</span><span class="w"> </span><span class="n">move</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">stove</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">activate</span><span class="w"> </span><span class="n">stove</span>
<span class="n">The</span><span class="w"> </span><span class="n">stove</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">activated</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">inventory</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">The</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="n">measures</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">temperature</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">13</span><span class="w"> </span><span class="n">degrees</span><span class="w"> </span><span class="n">Celsius</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">A</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="n">called</span><span class="w"> </span><span class="n">water</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">inventory</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">The</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="n">measures</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">temperature</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">28</span><span class="w"> </span><span class="n">degrees</span><span class="w"> </span><span class="n">Celsius</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">A</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="n">called</span><span class="w"> </span><span class="n">water</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">inventory</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">The</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="n">measures</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">temperature</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">58</span><span class="w"> </span><span class="n">degrees</span><span class="w"> </span><span class="n">Celsius</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">A</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="n">called</span><span class="w"> </span><span class="n">water</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">inventory</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">The</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="n">measures</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">temperature</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">78</span><span class="w"> </span><span class="n">degrees</span><span class="w"> </span><span class="n">Celsius</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">A</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="n">called</span><span class="w"> </span><span class="n">water</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">inventory</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">metal</span><span class="w"> </span><span class="n">pot</span>
<span class="n">The</span><span class="w"> </span><span class="n">thermometer</span><span class="w"> </span><span class="n">measures</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">temperature</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">98</span><span class="w"> </span><span class="n">degrees</span><span class="w"> </span><span class="n">Celsius</span><span class="o">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">steam</span>
<span class="n">A</span><span class="w"> </span><span class="n">substance</span><span class="w"> </span><span class="n">called</span><span class="w"> </span><span class="n">steam</span><span class="o">.</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="n">completed</span><span class="o">.</span>
</code></pre></div>

<p>Figure 3: Successful sequence of events in the 1-1 Task, where the agent needs to boil water.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Try it yourself: https://www.microsoft.com/en-us/research/project/textworld/try-it/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-985 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-985</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-985</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-75e4b78e42ecd445934399fddca073f86c1c2e68</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/75e4b78e42ecd445934399fddca073f86c1c2e68" target="_blank">Efficient Intervention Design for Causal Discovery with Latents</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The notion of p-colliders, that are colliders between pair of nodes arising from a specific type of conditioning in the causal graph, is introduced, and an upper bound on the number of interventions as a function of the maximum number of $p-collider between any two nodes in the causality graph is provided.</p>
                <p><strong>Paper Abstract:</strong> We consider recovering a causal graph in presence of latent variables, where we seek to minimize the cost of interventions used in the recovery process. We consider two intervention cost models: (1) a linear cost model where the cost of an intervention on a subset of variables has a linear form, and (2) an identity cost model where the cost of an intervention is the same, regardless of what variables it is on, i.e., the goal is just to minimize the number of interventions. Under the linear cost model, we give an algorithm to identify the ancestral relations of the underlying causal graph, achieving within a $2$-factor of the optimal intervention cost. This approximation factor can be improved to $1+\epsilon$ for any $\epsilon > 0$ under some mild restrictions. Under the identity cost model, we bound the number of interventions needed to recover the entire causal graph, including the latent variables, using a parameterization of the causal graph through a special type of colliders. In particular, we introduce the notion of $p$-colliders, that are colliders between pair of nodes arising from a specific type of conditioning in the causal graph, and provide an upper bound on the number of interventions as a function of the maximum number of $p$-colliders between any two nodes in the causal graph.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e985.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e985.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSMATRIX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SSMATRIX (Strongly Separating Matrix Construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-adaptive algorithm that constructs a strongly separating binary matrix (intervention design) to recover the ancestral graph under a linear intervention cost model; provides a 2-approximation to the optimal intervention cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SSMATRIX</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Constructs a binary matrix U (rows = observed variables, columns = interventions) that is strongly separating: for every pair of nodes there exist two columns distinguishing them. The algorithm relaxes to unique rows, assigns low-weight binary vectors to high-cost nodes, reserves columns as 'row-weight indicators' and guesses the number of unit-weight rows to tighten the assignment; returns intervention sets corresponding to U. Key algorithmic components: combinatorial encoding of variables into intervention patterns, greedy/guessing strategy for number of weight-1 rows, and conversion from unique-row assignment to a strongly separating matrix while controlling per-node intervention cost.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Structural Causal Model (observed V + latents L)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch, non-adaptive intervention setting in the SCM framework with observable variables V and latent variables L; interventions are designed upfront (non-sequential) and applied to generate interventional distributions queried via conditional independence (CI) tests.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Designs interventions (strongly separating sets) so each variable pair is separable under interventions, ensuring latent-induced spurious dependencies can be disambiguated via CI-tests; effectively isolates variables to avoid latent confounding interfering with ancestral recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders / inducing paths producing spurious statistical dependencies between observed variables (collider/latent-induced associations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses CI-tests on interventional distributions produced by the constructed interventions; the strongly separating property guarantees existence of an intervene-vs-not-intervene pair for every node pair to detect true ancestral relations vs latent-induced dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutes spurious edges by intervening on the appropriate set so that if dependence disappears under the intervention pattern predicted by the matrix, the dependence was due to latent-induced paths rather than an ancestor relation; uses transitive closure after CI-tests to reconstruct Anc(G).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Non-adaptive combinatorial experimental design: construct m interventions up-front using the strongly separating matrix encoding (with m >= O(log n)); columns correspond to which variables to intervene on.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Provable guarantee: the returned intervention design achieves total linear intervention cost c_U <= 2 * c_OPT for m >= gamma * log n (gamma constant, proof uses combinatorial bounds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Handled via parameter τ (number of p-colliders between a pair); algorithm itself does not enumerate a fixed number of distractors but the recovery guarantee holds for arbitrary latent structure when using separating sets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A combinatorial non-adaptive design (SSMATRIX) suffices to separate pairs of observed variables so that CI-tests under those interventions can distinguish ancestral relations from latent-induced dependencies; this achieves a 2-approximation to minimum linear intervention cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e985.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e985.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ε-SSMATRIX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ε-SSMATRIX ((1+ε)-approximation Strongly Separating Matrix)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modification of combinatorial strongly-separating construction that, under mild assumptions (bounds on m and cost ratios), gives a (1+ε)-approximation to minimum linear intervention cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ε-SSMATRIX</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Adapts the Kruskal-Katona based construction (originating from Hyttinen et al.) to the weighted-cost setting by ordering rows of a strongly separating matrix by increasing weight and assigning them to nodes by decreasing cost; combines combinatorial set-system design with greedy assignment to match high-cost nodes to low-weight intervention patterns. Requires bounds on c_max (cost ratio) relative to combinatorial coefficients to guarantee (1+ε)-approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Structural Causal Model (observed V + latents L)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-adaptive batch intervention design setting in SCM; uses an offline construction of m interventions then queries CI-oracle to recover ancestral relations.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Same separating-set approach as SSMATRIX but optimized assignment of low-weight intervention patterns to high-cost variables to reduce total cost while retaining the ability to disambiguate latent-induced spurious dependence via interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders / collider-induced spurious associations (primitive inducing paths).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>CI-tests on interventional distributions produced by the constructed interventions (same mechanism as SSMATRIX).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Interventions chosen to separate variables so dependence disappearing under designed interventions refutes causal ancestry in favor of latent-induced dependence; theoretical cost bounds show near-optimal intervention budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Non-adaptive, combinatorial design using Kruskal-Katona-type set families and greedy cost-aware assignment; no sequential adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Provable guarantee: under assumptions (c_max <= ε n / (3 * binom(m, t)) and ranges for m), the algorithm returns a strongly separating matrix with cost <= (1+ε) * c_OPT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Implicitly handled through τ in downstream full-recovery algorithms; ε-SSMATRIX targets ancestral recovery, not explicit distractor counts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>By combining combinatorial set-system constructions with cost-aware assignment, one can approach optimal intervention cost (1+ε) for ancestral graph recovery even with latent confounders, under mild technical assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e985.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e985.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Strongly Separating Set System</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strongly Separating Set System / Matrix</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combinatorial design of intervention sets (or binary matrix) such that for every ordered pair of variables there exist interventions that include one and exclude the other, enabling disambiguation of ancestral relations under interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Strongly Separating Set System</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A family S = {S_1,...,S_m} of subsets of V is strongly separating if for every distinct u,v there exist S_i and S_j with u in S_i\S_j and v in S_j\S_i. In matrix form, rows are variables and columns interventions; the property guarantees interventions that isolate each variable relative to any other variable so CI-tests on appropriate interventions reveal ancestral relations even with latents. Used as the core combinatorial object for designing interventions (SSMATRIX and ε-SSMATRIX).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Structural Causal Model (observational + interventional queries)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-adaptive batch experimental design where the set system determines which variables are intervened in each experiment; interventions are costly and counted under linear or identity cost models.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>By guaranteeing distinct inclusion/exclusion patterns for each pair, the set system ensures that latent-induced dependencies (which can be broken by intervening on one side) can be separated and identified via CI-tests; effectively a combinatorial way to neutralize spurious dependence caused by latents.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounding and inducing paths that create spurious dependencies between observed nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Existence of columns isolating u vs v ensures you can test dependence under intervention patterns that isolate potential latent paths; detection is via CI-tests on the interventional distributions corresponding to those columns.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If dependence vanishes under the intervention pattern that includes u but not v (or vice versa), the method refutes direct ancestry and implicates latent-induced dependence. Repeated across pairs, transitive closure yields Anc(G).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Design a set system meeting the strongly separating property (binary encoding, Kruskal-Katona constructions, greedy cost-aware assignment) and perform all interventions non-adaptively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Guarantees sufficiency and necessity: any set of interventions that recovers Anc(G) must be strongly separating; constructions yield provable approximation guarantees for cost (2-approx and (1+ε)-approx under conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Not a numeric count; the property holds irrespective of number of latents, but cost guarantees depend on m and combinatorial parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Strongly separating set systems are the correct combinatorial object for designing non-adaptive interventions that allow recovery of the ancestral graph in the presence of latents; they are both necessary and sufficient for ancestral recovery via CI-tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e985.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e985.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>p-colliders</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>p-colliders (parent-proximal colliders)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A restricted collider notion: a node v_k is a p-collider for pair (v_i, v_j) if it is a collider on some path between them and is a parent of or has a descendant that is a parent of v_i or v_j; used to parameterize complexity of latent structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>p-colliders</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Defines P_ij as the set of p-colliders between v_i and v_j; p-colliders capture the minimal collider nodes that need to be intervened upon to break primitive inducing paths or dependencies caused by latents. The paper introduces τ-causal graphs (at most τ p-colliders per pair) and uses this parameter to bound number of interventions required to recover full graph with latents.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Structural Causal Model with latent variables (each latent affects at most two observed variables)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static causal graph ensembles (synthetic graphs used in analysis/experiments); recovery algorithms aim to hit P_ij sets via interventions to neutralize collider-induced dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Targets p-colliders explicitly: intervention sets are designed (randomized sampling) to include P_ij for each pair so that collider-induced spurious dependence is removed and true causal edges can be tested/refuted.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Collider activation and latent-induced primitive inducing paths causing spurious associations between observed variables.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Design interventions that include P_ij (or likely supersets); then use CI-tests or distribution comparison to detect whether dependence remains, indicating latent vs direct relation.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Intervene on p-colliders (and parents when necessary); if dependence between pair vanishes under do(P_ij ∪ appropriate parents) then the edge is refuted as direct ancestry and indicates either non-parent or latent structure; used in lemmas to distinguish parent edges vs latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Randomized non-adaptive sampling of interventions, each variable included independently with probability 1 - 1/τ', repeated O(τ log n) times, to ensure with high probability that each P_ij is covered by some intervention set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Leads to main full-recovery bound: RecoverG + Latents algorithms reconstruct full graph with O(n τ log n + n log n) interventions with probability 1 - O(1/n^2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Parameter τ bounds number of p-colliders between any pair; experiments injected latents at ~5% of pairs to measure τ empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Parameterizing graphs by number of p-colliders (τ) yields tighter intervention complexity bounds in many regimes; randomized interventions aimed at hitting p-colliders let the algorithm detect and refute spurious collider/latent-induced signals efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e985.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e985.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecoverG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RecoverG (observable graph recovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Algorithm to recover the observable directed graph G from the ancestral graph Anc(G) by distinguishing direct parent edges from indirect ancestry using interventions that include p-colliders and CI-tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RecoverG</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given Anc(G) and a collection A_tau of randomized interventions (each variable included independently with prob 1 - 1/τ'), for each candidate edge v_i -> v_j in Anc(G) the algorithm considers interventions that include v_i but not v_j and tests whether v_i and v_j remain dependent conditional on Anc(v_j) \ {v_i} under those interventions; if dependence persists in all such interventions, reports an edge. Relies on Lemma showing conditioning on Anc(v_j) \ {v_i} and intervening on P_ij separates non-parent ancestry.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Structural Causal Model (non-adaptive interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-adaptive, randomized batch interventions constructed to probabilistically cover p-colliders for each node pair; experiments assume access to CI-oracle to test conditional independence under interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Uses randomized interventions that (with high probability) include all p-colliders P_ij for the pair; by intervening on these colliders and conditioning on Anc(v_j) \ {v_i}, latent-induced and indirect paths are broken so direct parenthood can be detected.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Indirect ancestral paths and latent-induced dependencies (primitive inducing paths) causing spurious dependence between potential parent-child pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>CI-tests on interventional distributions produced by sampled sets A_t that include P_ij and exclude v_j; if independence holds under these conditions, v_i is not a parent of v_j.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If v_i becomes independent of v_j after intervening on P_ij ∪ Anc(v_j)\{v_i}, then v_i is refuted as a parent of v_j; otherwise the edge is retained.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Randomized coverage: sample O(τ log n) intervention sets with inclusion probability 1 - 1/τ' to probabilistically hit each P_ij, then evaluate CI-tests per candidate edge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Proposition: RecoverG recovers the observable graph using O(τ log n + log n) interventions with probability ≥ 1 - 1/n^2 ( τ' = max{τ,2} ).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Relies on τ (max p-colliders per pair); complexity scales linearly with τ.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>By sampling randomized intervention sets to cover p-colliders and using CI-tests conditional on Anc(v_j) \ {v_i}, the algorithm can reliably distinguish direct parent edges from indirect/latent-induced ancestry with O(τ log n) interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e985.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e985.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LatentsNEDGES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LatentsNEDGES (detect latents for non-adjacent pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Algorithm to detect latent variables that affect non-adjacent pairs of observable nodes by intervening on p-colliders and conditioning on parents of the pair, using CI-tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LatentsNEDGES</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Starting from observable graph G, for each non-adjacent pair v_i, v_j the algorithm intervenes on randomized sets designed to include P_ij and conditions on Pa(v_i) and Pa(v_j); if independence is not achieved under those interventions, a latent affecting both nodes is inferred. The method uses randomized sampling to ensure P_ij inclusion with high probability and then CI-tests to detect residual dependence indicative of latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Structural Causal Model with latent variables</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-adaptive randomized interventions; relies on CI-oracle; assumes each latent affects at most two observed variables.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicitly targets p-colliders (and conditions on parents) to neutralize indirect paths; residual dependence after such interventions signals presence of a latent confounder between the pair.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders connecting non-adjacent observables, inducing spurious dependence even after conditionalization on observable parents.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>CI-tests under interventions that include P_ij and conditioning on parents; lack of conditional independence implies a latent between the pair.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If independence is achieved after intervening on P_ij and conditioning, then latent is refuted; otherwise a latent is declared between the pair.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Randomized interventions repeated O(τ^2 log n + log n) times to ensure coverage for non-adjacent pairs; then per-pair CI-tests with parent conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Proposition: LatentsNEDGES recovers all latents affecting non-adjacent pairs with O(τ^2 log n + log n) interventions, with probability ≥ 1 - 1/n^2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Handles up to τ p-colliders per pair; experimental injection of latents in experiments ~5% of pairs used to evaluate τ empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Intervening on p-colliders and conditioning on parents suffices to detect latents affecting non-adjacent pairs; randomized coverage yields high-probability guarantees with intervention complexity polynomial in τ and log n.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e985.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e985.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LatentsWEdges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LatentsWEdges (detect latents for adjacent pairs / do-see tests)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Algorithm to detect latent confounders between adjacent observable pairs (v_i -> v_j) using distribution-comparison (do-see) tests under interventions that include p-colliders plus parents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LatentsWEdges</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each directed edge v_i -> v_j in G, consider randomized intervention sets B that contain v_i but not v_j; remove v_i to produce B\{v_i} and perform distribution-testing comparisons between Pr[v_j | v_i, Pa(v_j), do(Pa(v_i) ∪ B\{v_i})] and Pr[v_j | Pa(v_j), do({v_i} ∪ Pa(v_i) ∪ B\{v_i})]. Using a DT-oracle (distribution equality tester), inequality indicates a latent l_ij affecting both v_i and v_j. Requires intervening on P_ij and Pa(v_i) to block indirect paths.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Structural Causal Model with latent variables and access to distribution-testing oracle</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-adaptive randomized intervention sets designed to include p-colliders and parents; assumes access to a DT-oracle that tests whether two conditional interventional distributions are identical (do-see test).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Blocks collider and parent-mediated paths via interventions and compares conditional distributions with/without intervening on v_i to detect latent confounder influence that cannot be explained by direct edge.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders that simultaneously parent both endpoints of an edge, causing observational equality of 'see' and 'do' distributions to fail or hold in characteristic ways.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Distribution testing (do-see): compare conditional distributions under two interventions (with and without do(v_i)) while controlling for parents and sampled set B; inequality signals latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If distributions are identical under the two interventions, then no latent is inferred; otherwise a latent connecting the pair is declared (do-see acts as refutation test for absence/presence of latent).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Randomized interventions (O(n τ log n + n log n) overall) to probabilistically include required p-colliders plus parents; per-edge distribution comparisons via DT-oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Proposition: LatentsWEdges recovers all latents affecting adjacent node pairs with O(n τ log n + n log n) interventions with probability ≥ 1 - 1/n^2; combined with other steps yields full recovery bound in Theorem 4.12.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Handles up to τ p-colliders per pair; complexity linear in n and τ.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Do-see distribution comparisons combined with interventions that include p-colliders and parents provide a principled test to detect latent confounders between adjacent nodes where CI-tests alone are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e985.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e985.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CI-oracle & DT-oracle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Independence Oracle and Distribution Testing (do-see) Oracle</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assumed oracles: CI-oracle answers conditional independence queries in interventional distributions; DT-oracle tests equality of conditional distributions under different interventions (do-see comparisons). Both are used as primitives by algorithms to detect and refute spurious signals caused by latents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CI-oracle and DT-oracle</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>CI-oracle: given v_i, v_j, Z, S returns whether v_i ⟂ v_j | Z under do(S). DT-oracle: given conditional distributions under two interventions, returns whether they are identical (used to implement do-see tests). These oracles are treated as primitives (can be implemented by statistical tests with sample complexity results cited). Algorithms use CI-oracle to test d-separation under interventions and DT-oracle to compare 'see' vs 'do' distributions to detect latents on adjacent edges.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interventional query model (access to interventional samples / testers)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Abstract oracle model (assumed available); in practice realized by statistical conditional independence tests and distribution closeness testers with sample complexity bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Provide detection primitives: CI-oracle detects changes in dependence structure under interventions (revealing latent-induced dependencies), DT-oracle detects distributional differences caused by latent confounders in do-see comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounding (inducing paths), distributional shifts under interventions that reveal confounding, collider-activation signals.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>CI-oracle: independence tests under do(S) with conditioning sets; DT-oracle: two-sample distribution test comparing Pr[v_j | v_i, Z, do(S)] vs Pr[v_j | Z, do(S ∪ {v_i})].</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If CI-oracle reports independence under targeted intervention patterns, direct ancestry is refuted; if DT-oracle reports equality of distributions in do-see, no latent is inferred (refutation of latent presence).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Used as evaluation primitives after non-adaptive intervention design (i.e., oracles are queried for specified interventions and conditioning sets defined by the set-system/randomized sampling designs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Oracles are assumed to be implementable with sublinear-in-domain sample complexity (citations given); theoretical recovery guarantees of algorithms are conditioned on accurate oracle responses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Treating CI and distribution-comparison tests as oracles enables principled non-adaptive algorithms to detect and refute spurious signals from latents; do-see tests are essential for detecting latents on adjacent edges where CI-tests alone cannot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e985.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e985.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Randomized Intervention Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Randomized sampling of intervention sets for p-collider coverage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A randomized non-adaptive strategy that constructs O(τ log n) intervention sets by including each variable independently with probability 1 - 1/τ' to probabilistically hit all p-colliders for every pair with high probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Randomized Intervention Sampling (1 - 1/τ' inclusion)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Constructs T = O(τ' log n) sets A_t by independently including each variable with prob 1 - 1/τ' (τ' = max{τ,2}). This balances coverage and economy: high inclusion probability increases chance that P_ij ⊆ A_t for any fixed pair, while repetition ensures some set covers P_ij with high probability. Used in RecoverG, LatentsNEDGES, LatentsWEdges to ensure existence of interventions that include required p-colliders while excluding the tested node.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Non-adaptive interventional SCM experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch randomized experimental design; interventions constructed once and applied to produce distributions used for CI and DT oracles.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Probabilistic coverage: by sampling sets with high inclusion probability, the scheme ensures that for each pair the set of p-colliders is included in at least one intervention set, thereby neutralizing collider-induced spurious signals when testing edges.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Collider and latent-induced dependencies (primitive inducing paths) that require hitting small sets (P_ij) to break.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Guarantees existence of an intervention set that includes P_ij (and excludes the target node) so that CI or DT tests can detect/refute spurious dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>When a sampled intervention contains P_ij and the conditioning set is applied, observed independence/refutation indicates the prior dependence was due to those colliders / latents rather than direct causation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Non-adaptive repeated random sampling with parameters scaled by τ and log n to achieve high-probability coverage for all pairs concurrently.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Used to achieve high-probability recovery: RecoverG etc. achieve correctness with probability ≥ 1 - O(1/n^2) using O(n τ log n + n log n) interventions overall when combined with other steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Designed for up to τ p-colliders per pair; uses τ' = max(τ,2) in sampling probability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Randomized non-adaptive sampling is an effective practical strategy to ensure coverage of p-colliders across all pairs simultaneously, enabling CI/DT-based detection of latent-induced spurious signals with provable sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Intervention Design for Causal Discovery with Latents', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Experimental design for learning causal graphs with latent variables <em>(Rating: 2)</em></li>
                <li>Cost-optimal learning of causal graphs <em>(Rating: 2)</em></li>
                <li>Experiment selection for causal discovery <em>(Rating: 2)</em></li>
                <li>Two optimal strategies for active learning of causal models from interventional data <em>(Rating: 1)</em></li>
                <li>Ancestral graph markov models <em>(Rating: 1)</em></li>
                <li>Causality: Models, Reasoning and Inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-985",
    "paper_id": "paper-75e4b78e42ecd445934399fddca073f86c1c2e68",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "SSMATRIX",
            "name_full": "SSMATRIX (Strongly Separating Matrix Construction)",
            "brief_description": "A non-adaptive algorithm that constructs a strongly separating binary matrix (intervention design) to recover the ancestral graph under a linear intervention cost model; provides a 2-approximation to the optimal intervention cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SSMATRIX",
            "method_description": "Constructs a binary matrix U (rows = observed variables, columns = interventions) that is strongly separating: for every pair of nodes there exist two columns distinguishing them. The algorithm relaxes to unique rows, assigns low-weight binary vectors to high-cost nodes, reserves columns as 'row-weight indicators' and guesses the number of unit-weight rows to tighten the assignment; returns intervention sets corresponding to U. Key algorithmic components: combinatorial encoding of variables into intervention patterns, greedy/guessing strategy for number of weight-1 rows, and conversion from unique-row assignment to a strongly separating matrix while controlling per-node intervention cost.",
            "environment_name": "Structural Causal Model (observed V + latents L)",
            "environment_description": "Batch, non-adaptive intervention setting in the SCM framework with observable variables V and latent variables L; interventions are designed upfront (non-sequential) and applied to generate interventional distributions queried via conditional independence (CI) tests.",
            "handles_distractors": true,
            "distractor_handling_technique": "Designs interventions (strongly separating sets) so each variable pair is separable under interventions, ensuring latent-induced spurious dependencies can be disambiguated via CI-tests; effectively isolates variables to avoid latent confounding interfering with ancestral recovery.",
            "spurious_signal_types": "Latent confounders / inducing paths producing spurious statistical dependencies between observed variables (collider/latent-induced associations).",
            "detection_method": "Uses CI-tests on interventional distributions produced by the constructed interventions; the strongly separating property guarantees existence of an intervene-vs-not-intervene pair for every node pair to detect true ancestral relations vs latent-induced dependence.",
            "downweighting_method": null,
            "refutation_method": "Refutes spurious edges by intervening on the appropriate set so that if dependence disappears under the intervention pattern predicted by the matrix, the dependence was due to latent-induced paths rather than an ancestor relation; uses transitive closure after CI-tests to reconstruct Anc(G).",
            "uses_active_learning": false,
            "inquiry_strategy": "Non-adaptive combinatorial experimental design: construct m interventions up-front using the strongly separating matrix encoding (with m &gt;= O(log n)); columns correspond to which variables to intervene on.",
            "performance_with_robustness": "Provable guarantee: the returned intervention design achieves total linear intervention cost c_U &lt;= 2 * c_OPT for m &gt;= gamma * log n (gamma constant, proof uses combinatorial bounds).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Handled via parameter τ (number of p-colliders between a pair); algorithm itself does not enumerate a fixed number of distractors but the recovery guarantee holds for arbitrary latent structure when using separating sets.",
            "key_findings": "A combinatorial non-adaptive design (SSMATRIX) suffices to separate pairs of observed variables so that CI-tests under those interventions can distinguish ancestral relations from latent-induced dependencies; this achieves a 2-approximation to minimum linear intervention cost.",
            "uuid": "e985.0",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "ε-SSMATRIX",
            "name_full": "ε-SSMATRIX ((1+ε)-approximation Strongly Separating Matrix)",
            "brief_description": "A modification of combinatorial strongly-separating construction that, under mild assumptions (bounds on m and cost ratios), gives a (1+ε)-approximation to minimum linear intervention cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "ε-SSMATRIX",
            "method_description": "Adapts the Kruskal-Katona based construction (originating from Hyttinen et al.) to the weighted-cost setting by ordering rows of a strongly separating matrix by increasing weight and assigning them to nodes by decreasing cost; combines combinatorial set-system design with greedy assignment to match high-cost nodes to low-weight intervention patterns. Requires bounds on c_max (cost ratio) relative to combinatorial coefficients to guarantee (1+ε)-approximation.",
            "environment_name": "Structural Causal Model (observed V + latents L)",
            "environment_description": "Non-adaptive batch intervention design setting in SCM; uses an offline construction of m interventions then queries CI-oracle to recover ancestral relations.",
            "handles_distractors": true,
            "distractor_handling_technique": "Same separating-set approach as SSMATRIX but optimized assignment of low-weight intervention patterns to high-cost variables to reduce total cost while retaining the ability to disambiguate latent-induced spurious dependence via interventions.",
            "spurious_signal_types": "Latent confounders / collider-induced spurious associations (primitive inducing paths).",
            "detection_method": "CI-tests on interventional distributions produced by the constructed interventions (same mechanism as SSMATRIX).",
            "downweighting_method": null,
            "refutation_method": "Interventions chosen to separate variables so dependence disappearing under designed interventions refutes causal ancestry in favor of latent-induced dependence; theoretical cost bounds show near-optimal intervention budgets.",
            "uses_active_learning": false,
            "inquiry_strategy": "Non-adaptive, combinatorial design using Kruskal-Katona-type set families and greedy cost-aware assignment; no sequential adaptation.",
            "performance_with_robustness": "Provable guarantee: under assumptions (c_max &lt;= ε n / (3 * binom(m, t)) and ranges for m), the algorithm returns a strongly separating matrix with cost &lt;= (1+ε) * c_OPT.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Implicitly handled through τ in downstream full-recovery algorithms; ε-SSMATRIX targets ancestral recovery, not explicit distractor counts.",
            "key_findings": "By combining combinatorial set-system constructions with cost-aware assignment, one can approach optimal intervention cost (1+ε) for ancestral graph recovery even with latent confounders, under mild technical assumptions.",
            "uuid": "e985.1",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Strongly Separating Set System",
            "name_full": "Strongly Separating Set System / Matrix",
            "brief_description": "A combinatorial design of intervention sets (or binary matrix) such that for every ordered pair of variables there exist interventions that include one and exclude the other, enabling disambiguation of ancestral relations under interventions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Strongly Separating Set System",
            "method_description": "A family S = {S_1,...,S_m} of subsets of V is strongly separating if for every distinct u,v there exist S_i and S_j with u in S_i\\S_j and v in S_j\\S_i. In matrix form, rows are variables and columns interventions; the property guarantees interventions that isolate each variable relative to any other variable so CI-tests on appropriate interventions reveal ancestral relations even with latents. Used as the core combinatorial object for designing interventions (SSMATRIX and ε-SSMATRIX).",
            "environment_name": "Structural Causal Model (observational + interventional queries)",
            "environment_description": "Non-adaptive batch experimental design where the set system determines which variables are intervened in each experiment; interventions are costly and counted under linear or identity cost models.",
            "handles_distractors": true,
            "distractor_handling_technique": "By guaranteeing distinct inclusion/exclusion patterns for each pair, the set system ensures that latent-induced dependencies (which can be broken by intervening on one side) can be separated and identified via CI-tests; effectively a combinatorial way to neutralize spurious dependence caused by latents.",
            "spurious_signal_types": "Latent confounding and inducing paths that create spurious dependencies between observed nodes.",
            "detection_method": "Existence of columns isolating u vs v ensures you can test dependence under intervention patterns that isolate potential latent paths; detection is via CI-tests on the interventional distributions corresponding to those columns.",
            "downweighting_method": null,
            "refutation_method": "If dependence vanishes under the intervention pattern that includes u but not v (or vice versa), the method refutes direct ancestry and implicates latent-induced dependence. Repeated across pairs, transitive closure yields Anc(G).",
            "uses_active_learning": false,
            "inquiry_strategy": "Design a set system meeting the strongly separating property (binary encoding, Kruskal-Katona constructions, greedy cost-aware assignment) and perform all interventions non-adaptively.",
            "performance_with_robustness": "Guarantees sufficiency and necessity: any set of interventions that recovers Anc(G) must be strongly separating; constructions yield provable approximation guarantees for cost (2-approx and (1+ε)-approx under conditions).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Not a numeric count; the property holds irrespective of number of latents, but cost guarantees depend on m and combinatorial parameters.",
            "key_findings": "Strongly separating set systems are the correct combinatorial object for designing non-adaptive interventions that allow recovery of the ancestral graph in the presence of latents; they are both necessary and sufficient for ancestral recovery via CI-tests.",
            "uuid": "e985.2",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "p-colliders",
            "name_full": "p-colliders (parent-proximal colliders)",
            "brief_description": "A restricted collider notion: a node v_k is a p-collider for pair (v_i, v_j) if it is a collider on some path between them and is a parent of or has a descendant that is a parent of v_i or v_j; used to parameterize complexity of latent structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "p-colliders",
            "method_description": "Defines P_ij as the set of p-colliders between v_i and v_j; p-colliders capture the minimal collider nodes that need to be intervened upon to break primitive inducing paths or dependencies caused by latents. The paper introduces τ-causal graphs (at most τ p-colliders per pair) and uses this parameter to bound number of interventions required to recover full graph with latents.",
            "environment_name": "Structural Causal Model with latent variables (each latent affects at most two observed variables)",
            "environment_description": "Static causal graph ensembles (synthetic graphs used in analysis/experiments); recovery algorithms aim to hit P_ij sets via interventions to neutralize collider-induced dependencies.",
            "handles_distractors": true,
            "distractor_handling_technique": "Targets p-colliders explicitly: intervention sets are designed (randomized sampling) to include P_ij for each pair so that collider-induced spurious dependence is removed and true causal edges can be tested/refuted.",
            "spurious_signal_types": "Collider activation and latent-induced primitive inducing paths causing spurious associations between observed variables.",
            "detection_method": "Design interventions that include P_ij (or likely supersets); then use CI-tests or distribution comparison to detect whether dependence remains, indicating latent vs direct relation.",
            "downweighting_method": null,
            "refutation_method": "Intervene on p-colliders (and parents when necessary); if dependence between pair vanishes under do(P_ij ∪ appropriate parents) then the edge is refuted as direct ancestry and indicates either non-parent or latent structure; used in lemmas to distinguish parent edges vs latent confounding.",
            "uses_active_learning": false,
            "inquiry_strategy": "Randomized non-adaptive sampling of interventions, each variable included independently with probability 1 - 1/τ', repeated O(τ log n) times, to ensure with high probability that each P_ij is covered by some intervention set.",
            "performance_with_robustness": "Leads to main full-recovery bound: RecoverG + Latents algorithms reconstruct full graph with O(n τ log n + n log n) interventions with probability 1 - O(1/n^2).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Parameter τ bounds number of p-colliders between any pair; experiments injected latents at ~5% of pairs to measure τ empirically.",
            "key_findings": "Parameterizing graphs by number of p-colliders (τ) yields tighter intervention complexity bounds in many regimes; randomized interventions aimed at hitting p-colliders let the algorithm detect and refute spurious collider/latent-induced signals efficiently.",
            "uuid": "e985.3",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "RecoverG",
            "name_full": "RecoverG (observable graph recovery)",
            "brief_description": "Algorithm to recover the observable directed graph G from the ancestral graph Anc(G) by distinguishing direct parent edges from indirect ancestry using interventions that include p-colliders and CI-tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "RecoverG",
            "method_description": "Given Anc(G) and a collection A_tau of randomized interventions (each variable included independently with prob 1 - 1/τ'), for each candidate edge v_i -&gt; v_j in Anc(G) the algorithm considers interventions that include v_i but not v_j and tests whether v_i and v_j remain dependent conditional on Anc(v_j) \\ {v_i} under those interventions; if dependence persists in all such interventions, reports an edge. Relies on Lemma showing conditioning on Anc(v_j) \\ {v_i} and intervening on P_ij separates non-parent ancestry.",
            "environment_name": "Structural Causal Model (non-adaptive interventions)",
            "environment_description": "Non-adaptive, randomized batch interventions constructed to probabilistically cover p-colliders for each node pair; experiments assume access to CI-oracle to test conditional independence under interventions.",
            "handles_distractors": true,
            "distractor_handling_technique": "Uses randomized interventions that (with high probability) include all p-colliders P_ij for the pair; by intervening on these colliders and conditioning on Anc(v_j) \\ {v_i}, latent-induced and indirect paths are broken so direct parenthood can be detected.",
            "spurious_signal_types": "Indirect ancestral paths and latent-induced dependencies (primitive inducing paths) causing spurious dependence between potential parent-child pairs.",
            "detection_method": "CI-tests on interventional distributions produced by sampled sets A_t that include P_ij and exclude v_j; if independence holds under these conditions, v_i is not a parent of v_j.",
            "downweighting_method": null,
            "refutation_method": "If v_i becomes independent of v_j after intervening on P_ij ∪ Anc(v_j)\\{v_i}, then v_i is refuted as a parent of v_j; otherwise the edge is retained.",
            "uses_active_learning": false,
            "inquiry_strategy": "Randomized coverage: sample O(τ log n) intervention sets with inclusion probability 1 - 1/τ' to probabilistically hit each P_ij, then evaluate CI-tests per candidate edge.",
            "performance_with_robustness": "Proposition: RecoverG recovers the observable graph using O(τ log n + log n) interventions with probability ≥ 1 - 1/n^2 ( τ' = max{τ,2} ).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Relies on τ (max p-colliders per pair); complexity scales linearly with τ.",
            "key_findings": "By sampling randomized intervention sets to cover p-colliders and using CI-tests conditional on Anc(v_j) \\ {v_i}, the algorithm can reliably distinguish direct parent edges from indirect/latent-induced ancestry with O(τ log n) interventions.",
            "uuid": "e985.4",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "LatentsNEDGES",
            "name_full": "LatentsNEDGES (detect latents for non-adjacent pairs)",
            "brief_description": "Algorithm to detect latent variables that affect non-adjacent pairs of observable nodes by intervening on p-colliders and conditioning on parents of the pair, using CI-tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LatentsNEDGES",
            "method_description": "Starting from observable graph G, for each non-adjacent pair v_i, v_j the algorithm intervenes on randomized sets designed to include P_ij and conditions on Pa(v_i) and Pa(v_j); if independence is not achieved under those interventions, a latent affecting both nodes is inferred. The method uses randomized sampling to ensure P_ij inclusion with high probability and then CI-tests to detect residual dependence indicative of latent confounding.",
            "environment_name": "Structural Causal Model with latent variables",
            "environment_description": "Non-adaptive randomized interventions; relies on CI-oracle; assumes each latent affects at most two observed variables.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicitly targets p-colliders (and conditions on parents) to neutralize indirect paths; residual dependence after such interventions signals presence of a latent confounder between the pair.",
            "spurious_signal_types": "Latent confounders connecting non-adjacent observables, inducing spurious dependence even after conditionalization on observable parents.",
            "detection_method": "CI-tests under interventions that include P_ij and conditioning on parents; lack of conditional independence implies a latent between the pair.",
            "downweighting_method": null,
            "refutation_method": "If independence is achieved after intervening on P_ij and conditioning, then latent is refuted; otherwise a latent is declared between the pair.",
            "uses_active_learning": false,
            "inquiry_strategy": "Randomized interventions repeated O(τ^2 log n + log n) times to ensure coverage for non-adjacent pairs; then per-pair CI-tests with parent conditioning.",
            "performance_with_robustness": "Proposition: LatentsNEDGES recovers all latents affecting non-adjacent pairs with O(τ^2 log n + log n) interventions, with probability ≥ 1 - 1/n^2.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Handles up to τ p-colliders per pair; experimental injection of latents in experiments ~5% of pairs used to evaluate τ empirically.",
            "key_findings": "Intervening on p-colliders and conditioning on parents suffices to detect latents affecting non-adjacent pairs; randomized coverage yields high-probability guarantees with intervention complexity polynomial in τ and log n.",
            "uuid": "e985.5",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "LatentsWEdges",
            "name_full": "LatentsWEdges (detect latents for adjacent pairs / do-see tests)",
            "brief_description": "Algorithm to detect latent confounders between adjacent observable pairs (v_i -&gt; v_j) using distribution-comparison (do-see) tests under interventions that include p-colliders plus parents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LatentsWEdges",
            "method_description": "For each directed edge v_i -&gt; v_j in G, consider randomized intervention sets B that contain v_i but not v_j; remove v_i to produce B\\{v_i} and perform distribution-testing comparisons between Pr[v_j | v_i, Pa(v_j), do(Pa(v_i) ∪ B\\{v_i})] and Pr[v_j | Pa(v_j), do({v_i} ∪ Pa(v_i) ∪ B\\{v_i})]. Using a DT-oracle (distribution equality tester), inequality indicates a latent l_ij affecting both v_i and v_j. Requires intervening on P_ij and Pa(v_i) to block indirect paths.",
            "environment_name": "Structural Causal Model with latent variables and access to distribution-testing oracle",
            "environment_description": "Non-adaptive randomized intervention sets designed to include p-colliders and parents; assumes access to a DT-oracle that tests whether two conditional interventional distributions are identical (do-see test).",
            "handles_distractors": true,
            "distractor_handling_technique": "Blocks collider and parent-mediated paths via interventions and compares conditional distributions with/without intervening on v_i to detect latent confounder influence that cannot be explained by direct edge.",
            "spurious_signal_types": "Latent confounders that simultaneously parent both endpoints of an edge, causing observational equality of 'see' and 'do' distributions to fail or hold in characteristic ways.",
            "detection_method": "Distribution testing (do-see): compare conditional distributions under two interventions (with and without do(v_i)) while controlling for parents and sampled set B; inequality signals latent confounding.",
            "downweighting_method": null,
            "refutation_method": "If distributions are identical under the two interventions, then no latent is inferred; otherwise a latent connecting the pair is declared (do-see acts as refutation test for absence/presence of latent).",
            "uses_active_learning": false,
            "inquiry_strategy": "Randomized interventions (O(n τ log n + n log n) overall) to probabilistically include required p-colliders plus parents; per-edge distribution comparisons via DT-oracle.",
            "performance_with_robustness": "Proposition: LatentsWEdges recovers all latents affecting adjacent node pairs with O(n τ log n + n log n) interventions with probability ≥ 1 - 1/n^2; combined with other steps yields full recovery bound in Theorem 4.12.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Handles up to τ p-colliders per pair; complexity linear in n and τ.",
            "key_findings": "Do-see distribution comparisons combined with interventions that include p-colliders and parents provide a principled test to detect latent confounders between adjacent nodes where CI-tests alone are insufficient.",
            "uuid": "e985.6",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "CI-oracle & DT-oracle",
            "name_full": "Conditional Independence Oracle and Distribution Testing (do-see) Oracle",
            "brief_description": "Assumed oracles: CI-oracle answers conditional independence queries in interventional distributions; DT-oracle tests equality of conditional distributions under different interventions (do-see comparisons). Both are used as primitives by algorithms to detect and refute spurious signals caused by latents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "CI-oracle and DT-oracle",
            "method_description": "CI-oracle: given v_i, v_j, Z, S returns whether v_i ⟂ v_j | Z under do(S). DT-oracle: given conditional distributions under two interventions, returns whether they are identical (used to implement do-see tests). These oracles are treated as primitives (can be implemented by statistical tests with sample complexity results cited). Algorithms use CI-oracle to test d-separation under interventions and DT-oracle to compare 'see' vs 'do' distributions to detect latents on adjacent edges.",
            "environment_name": "Interventional query model (access to interventional samples / testers)",
            "environment_description": "Abstract oracle model (assumed available); in practice realized by statistical conditional independence tests and distribution closeness testers with sample complexity bounds.",
            "handles_distractors": true,
            "distractor_handling_technique": "Provide detection primitives: CI-oracle detects changes in dependence structure under interventions (revealing latent-induced dependencies), DT-oracle detects distributional differences caused by latent confounders in do-see comparisons.",
            "spurious_signal_types": "Latent confounding (inducing paths), distributional shifts under interventions that reveal confounding, collider-activation signals.",
            "detection_method": "CI-oracle: independence tests under do(S) with conditioning sets; DT-oracle: two-sample distribution test comparing Pr[v_j | v_i, Z, do(S)] vs Pr[v_j | Z, do(S ∪ {v_i})].",
            "downweighting_method": null,
            "refutation_method": "If CI-oracle reports independence under targeted intervention patterns, direct ancestry is refuted; if DT-oracle reports equality of distributions in do-see, no latent is inferred (refutation of latent presence).",
            "uses_active_learning": null,
            "inquiry_strategy": "Used as evaluation primitives after non-adaptive intervention design (i.e., oracles are queried for specified interventions and conditioning sets defined by the set-system/randomized sampling designs).",
            "performance_with_robustness": "Oracles are assumed to be implementable with sublinear-in-domain sample complexity (citations given); theoretical recovery guarantees of algorithms are conditioned on accurate oracle responses.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Treating CI and distribution-comparison tests as oracles enables principled non-adaptive algorithms to detect and refute spurious signals from latents; do-see tests are essential for detecting latents on adjacent edges where CI-tests alone cannot.",
            "uuid": "e985.7",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Randomized Intervention Sampling",
            "name_full": "Randomized sampling of intervention sets for p-collider coverage",
            "brief_description": "A randomized non-adaptive strategy that constructs O(τ log n) intervention sets by including each variable independently with probability 1 - 1/τ' to probabilistically hit all p-colliders for every pair with high probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Randomized Intervention Sampling (1 - 1/τ' inclusion)",
            "method_description": "Constructs T = O(τ' log n) sets A_t by independently including each variable with prob 1 - 1/τ' (τ' = max{τ,2}). This balances coverage and economy: high inclusion probability increases chance that P_ij ⊆ A_t for any fixed pair, while repetition ensures some set covers P_ij with high probability. Used in RecoverG, LatentsNEDGES, LatentsWEdges to ensure existence of interventions that include required p-colliders while excluding the tested node.",
            "environment_name": "Non-adaptive interventional SCM experiments",
            "environment_description": "Batch randomized experimental design; interventions constructed once and applied to produce distributions used for CI and DT oracles.",
            "handles_distractors": true,
            "distractor_handling_technique": "Probabilistic coverage: by sampling sets with high inclusion probability, the scheme ensures that for each pair the set of p-colliders is included in at least one intervention set, thereby neutralizing collider-induced spurious signals when testing edges.",
            "spurious_signal_types": "Collider and latent-induced dependencies (primitive inducing paths) that require hitting small sets (P_ij) to break.",
            "detection_method": "Guarantees existence of an intervention set that includes P_ij (and excludes the target node) so that CI or DT tests can detect/refute spurious dependencies.",
            "downweighting_method": null,
            "refutation_method": "When a sampled intervention contains P_ij and the conditioning set is applied, observed independence/refutation indicates the prior dependence was due to those colliders / latents rather than direct causation.",
            "uses_active_learning": false,
            "inquiry_strategy": "Non-adaptive repeated random sampling with parameters scaled by τ and log n to achieve high-probability coverage for all pairs concurrently.",
            "performance_with_robustness": "Used to achieve high-probability recovery: RecoverG etc. achieve correctness with probability ≥ 1 - O(1/n^2) using O(n τ log n + n log n) interventions overall when combined with other steps.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Designed for up to τ p-colliders per pair; uses τ' = max(τ,2) in sampling probability.",
            "key_findings": "Randomized non-adaptive sampling is an effective practical strategy to ensure coverage of p-colliders across all pairs simultaneously, enabling CI/DT-based detection of latent-induced spurious signals with provable sample complexity.",
            "uuid": "e985.8",
            "source_info": {
                "paper_title": "Efficient Intervention Design for Causal Discovery with Latents",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Experimental design for learning causal graphs with latent variables",
            "rating": 2
        },
        {
            "paper_title": "Cost-optimal learning of causal graphs",
            "rating": 2
        },
        {
            "paper_title": "Experiment selection for causal discovery",
            "rating": 2
        },
        {
            "paper_title": "Two optimal strategies for active learning of causal models from interventional data",
            "rating": 1
        },
        {
            "paper_title": "Ancestral graph markov models",
            "rating": 1
        },
        {
            "paper_title": "Causality: Models, Reasoning and Inference",
            "rating": 1
        }
    ],
    "cost": 0.02237,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Efficient Intervention Design for Causal Discovery with Latents</h1>
<p>Raghavendra Addanki* Shiva Prasad Kasiviswanathan ${ }^{\dagger}$ Andrew McGregor ${ }^{\ddagger}$<br>Cameron Musco ${ }^{\S}$</p>
<h2>Abstract</h2>
<p>We consider recovering a causal graph in presence of latent variables, where we seek to minimize the cost of interventions used in the recovery process. We consider two intervention cost models: (1) a linear cost model where the cost of an intervention on a subset of variables has a linear form, and (2) an identity cost model where the cost of an intervention is the same, regardless of what variables it is on, i.e., the goal is just to minimize the number of interventions. Under the linear cost model, we give an algorithm to identify the ancestral relations of the underlying causal graph, achieving within a 2 -factor of the optimal intervention cost. This approximation factor can be improved to $1+\epsilon$ for any $\epsilon&gt;0$ under some mild restrictions. Under the identity cost model, we bound the number of interventions needed to recover the entire causal graph, including the latent variables, using a parameterization of the causal graph through a special type of colliders. In particular, we introduce the notion of $p$-colliders, that are colliders between pair of nodes arising from a specific type of conditioning in the causal graph, and provide an upper bound on the number of interventions as a function of the maximum number of $p$-colliders between any two nodes in the causal graph.</p>
<h2>1 Introduction</h2>
<p>Causality has long been a key tool in studying and analyzing various behaviors in fields such as genetics, psychology, and economics [Pearl, 2009]. Causality also plays a pivotal role in helping us build systems that can understand the world around us, and in turn, in helping us understand the behavior of machine learning systems deployed in the real world. Although the theory of causality has been around for more than three decades, for these reasons it has received increasing attention in recent years. In this paper, we study one of the fundamental problems of causality: causal discovery. In causal discovery, we want to learn all the causal relations existing between variables (nodes of the causal graph) of our system. It has been shown that, under certain assumptions, observational data alone only lets us recover the existence of a causal relationship between two variables, but not the direction of all relationships. To recover the directions of causal edges, we use the notion of an intervention described in the Structural Causal Models (SCM) framework introduced by [Pearl, 2009].</p>
<p>An intervention requires us to fix a subset of variables to a set of values, inducing a new distribution on the free variables. Such a system manipulation is generally expensive and thus there has been significant interest in trying to minimize the number of interventions and their cost in causal discovery. In a general cost model, intervening on any subset of variables has a cost associated with</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>it, and the goal is to identify all causal relationships and their directions while minimizing the total cost of interventions applied. This captures the fact that some interventions are more expensive than others. For example, in a medical study, intervening on certain variables might be impractical or unethical. In this work, we study two simplifications of this general cost model. In the linear cost model, each variable has an intervention cost, and the cost of an intervention on a subset of variables is the sum of costs for each variable in the set [Kocaoglu et al., 2017a; Lindgren et al., 2018]. In the identity cost model, every intervention has the same cost, regardless of what variables it contains and therefore minimizing intervention cost is the same as minimizing the number of interventions [Kocaoglu et al., 2017b].</p>
<p>As is standard in the causality literature, we assume that our causal relationship graph satisfies the causal Markov condition and faithfulness [Spirtes et al., 2000]. We assume that faithfulness holds both in the observational and interventional distributions following [Hauser and Bühlmann, 2014]. As is common, we also assume that we are given access to an oracle that can check if two variables are independent, conditioned on a subset of variables. We discuss this assumption in more detail in Section 2. Unlike much prior work, we do not make the causal sufficiency assumption: that there are no unobserved (or latent) variables in the system. Our algorithms apply to the causal discovery problem with the existence of latent variables.</p>
<p>Results. Our contributions are as follows. Let $\mathcal{G}$ be a causal graph on both observable variables $V$ and latent variables $L$. A directed edge $(u, v)$ in $\mathcal{G}$ indicates a causal relationship from $u$ to $v$. Let $G$ be the induced subgraph of $\mathcal{G}$ on the $n$ observable variables (referred to as observable graph). See Section 2 for a more formal description.</p>
<p>Linear Cost Model. In the linear cost model, we give an algorithm that given $m=\Omega(\log n)$ where $n$ is the number of observed variables, outputs a set of $m$ interventions that can be used to recover all ancestral relations of the observable graph $G$. ${ }^{1}$ We show that cost of interventions generated by the algorithm is at most twice the cost of the optimum set of interventions for this task. Our result is based on a characterization that shows that generating a set of interventions sufficient to recover ancestral relations is equivalent to designing a strongly separating set system (Def. 2.2). We show how to design such a set system with at most twice the optimum cost based on a greedy algorithm that constructs intervention sets which includes a variable with high cost in the least number of sets possible.</p>
<p>In the special case when each variable has unit intervention cost [Hyttinen et al., 2013a] give an exact algorithm to recover ancestral relations in $G$ with minimal total cost. Their algorithm is based on the Kruskal-Katona theorem in combinatorics [Kruskal, 1963; Katona, 1966]. We show that a modification of this approach yields a $(1+\epsilon)$-approximation algorithm in the general linear cost model for any $0&lt;\epsilon \leq 1$, under mild assumptions on $m$ and the maximum intervention cost on any one variable.</p>
<p>The linear cost model was first considered in [Kocaoglu et al., 2017a] and studied under the causal sufficiency (no latents) assumption. [Lindgren et al., 2018] showed given the essential graph of a causal graph, the problem of recovering $G$ with optimal cost under the linear cost model is NPhard. To the best of our knowledge, our result is the first to minimize intervention cost under the popular linear cost model in the presence of latents, and without the assumption of unit intervention cost on each variable.</p>
<p>We note that, while we give a 2-approximation for recovering ancestral relations in $G$, under the linear cost model, there seems to be no known characterization of the optimal intervention sets needed to recover the entire causal graph $\mathcal{G}$, making it hard to design a good approximation here.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Tackling this problem in the linear cost model is an interesting direction for future work.
Identity Cost Model. In the identity cost model, where we seek to just minimize the number of interventions, recovering ancestral relations in $G$ with minimum cost becomes trivial (see Section 4). Thus, in this case, we focus on algorithms that recover the causal graph $\mathcal{G}$ completely. We start with the notion of colliders in causal graphs [Pearl, 2009]. Our idea is to parametrize the causal graph in terms of a specific type of colliders that we refer to as $p$-colliders (Def. 4.2). Intuitively, a node $v_{k}$ is $p$-collider for a pair of nodes $\left(v_{i}, v_{j}\right)$ if a) it is a collider on a path between $v_{i}$ and $v_{j}$ and b) at least one of the parents $v_{i}, v_{j}$ is a descendent of $v_{k}$. If the graph $\mathcal{G}$ has at most $\tau p$-colliders between every pair of nodes, then our algorithm uses at most $O(n \tau \log n+n \log n)$ interventions. We also present causal graph instances where any non-adaptive algorithm requires $\Omega(n)$ interventions.</p>
<p>The only previous bound on recovering $\mathcal{G}$ in this setting utilized $O\left(\min \left{d \log ^{2} n, \ell\right}+d^{2} \log n\right)$ interventions where $d$ is the maximum (undirected) node degree and $\ell$ is the length of the longest directed path of the causal graph [Kocaoglu et al., 2017b]. Since we use a different parameterization of the causal graph, a direct comparison with this result is not always possible. We argue that a parameterization in terms of $p$-colliders is inherently more "natural" as it takes the directions of edges in $\mathcal{G}$ into account whereas the maximum degree does not. The presence of a single high-degree node can make the number of interventions required by existing work extremely high, even if the overall causal graph is sparse. In this case, the notion of $p$-colliders is a more global characterization of a causal graph. See Section 5 for a more detailed discussion of different parameter regimes under which our scheme provides a better bound. We also experimentally show that our scheme achieves a better bound over [Kocaoglu et al., 2017b] in some popular random graph models.</p>
<h1>1.1 Other Related Work</h1>
<p>Broadly, the problem of causal discovery has been studied under two different settings. In the first, one assumes causal sufficiency, i.e., that there are no unmeasured (latent) variables. Most work in this setting focuses on recovering causal relationships based on just observational data. Examples include algorithms like $I C$ [Pearl, 2009] and $P C$ [Spirtes et al., 2000]. Much work has focused on understanding the limitations and assumptions underlying these algorithms [Hauser and Bühlmann, 2014; Hoyer et al., 2009; Heinze-Deml et al., 2018; Loh and Bühlmann, 2014; Hoyer et al., 2009; Shimizu et al., 2006]. It is well-known, that to disambiguate a causal graph from its equivalence class, interventional, rather than just observational data is required [Hauser and Bühlmann, 2012; Eberhardt and Scheines, 2007; Eberhardt, 2007]. In particular, letting $\chi(\mathcal{G})$ be the chromatic number of $G, \Theta(\log \chi(\mathcal{G}))$ interventions are necessary and sufficient for recovery under the causal sufficiency assumption [Hauser and Bühlmann, 2014]. Surprising connections have been found [Hyttinen et al., 2013a; Katona, 1966; Mao-Cheng, 1984] between combinatorial structures and causality. Using these connections, much recent work has been devoted to minimizing intervention cost while imposing constraints such as sparsity or different costs for different sets of nodes [Shanmugam et al., 2015; Kocaoglu et al., 2017a; Lindgren et al., 2018].</p>
<p>In many cases, causal sufficiency is too strong an assumption: it is often contested if the behavior of systems we observe can truly be attributed to measured variables [Pearl, 2000; Bareinboim and Pearl, 2016]. In light of this, many algorithms avoiding the causal sufficiency assumption, such as IC* [Verma and Pearl, 1992] and FCI [Spirtes et al., 2000], have been developed. The above algorithms only use observational data. However, there is a growing interest in optimal intervention design in this setting [Silva et al., 2006; Hyttinen et al., 2013b; Parviainen and Koivisto, 2011]. We contribute to this line of work, focusing on minimizing the intervention cost required to recover the full intervention graph, or its ancestral graph, in the presence of latents.</p>
<h1>2 Preliminaries</h1>
<p>Notation. Following the SCM framework introduced by Pearl [2009], we represent the set of random variables of interest by $V \cup L$ where $V$ represents the set of endogenous (observed) variables that can be measured and $L$ represents the set of exogenous (latent) variables that cannot be measured. We define a directed graph on these variables where an edge corresponds to a causal relation between the corresponding variables. The edges are directed with an edge $\left(v_{i}, v_{j}\right)$ meaning that $v_{i} \rightarrow v_{j}$. As is common, we assume that all causal relations that exist between random variables in $V \cup L$ belong to one of the two categories : (i) $E \subseteq V \times V$ containing causal relations between the observed variables and (ii) $E_{L} \subseteq L \times V$ containing relations of the form $l \rightarrow v$ where $l \in L, v \in V$. Thus, the full edge set of our causal graph is denoted by $\mathcal{E}=E \cup E_{L}$. We also assume that every latent $l \in L$ influences exactly two observed variables i.e., $(l, u),(l, v) \in E_{L}$ and no other edges are incident on $l$ following [Kocaoglu et al., 2017b]. We let $\mathcal{G}=\mathcal{G}(V \cup L, \mathcal{E})$ denote the entire causal graph and refer to $G=G(V, E)$ as the observable graph.</p>
<p>Unless otherwise specified a path between two nodes is a undirected path. For every observable $v \in V$, let the parents of $v$ be defined as $\operatorname{Pa}(v)={w \mid w \in V$ and $(w, v) \in E}$. For a set of nodes $S \subseteq V, \operatorname{Pa}(S)=\cup_{v \in S} \operatorname{Pa}(v)$. If $v_{i}, v_{j} \in V$, we say $v_{j}$ is a descendant of $v_{i}$ (and $v_{i}$ is an ancestor of $v_{j}$ ) if there is a directed path from $v_{i}$ to $v_{j} . \operatorname{Anc}(v)={w \mid w \in V$ and $v$ is a descendant of $w}$. We let $\operatorname{Anc}(G)$ denote the ancestral graph ${ }^{2}$ of $G$ where an edge $\left(v_{i}, v_{j}\right) \in \operatorname{Anc}(G)$ if and only if there is a directed path from $v_{i}$ to $v_{j}$ in $G$. One of our primary interests is in recovering $\operatorname{Anc}(G)$ using a minimal cost set of interventions.</p>
<p>Using Pearl's do-notation, we represent an intervention on a set of variables $S \subseteq V$ as do $(S=s)$ for a value $s$ in the domain of $S$ and the joint probability distribution on $V \cup L$ conditioned on this intervention by $\operatorname{Pr}[\cdot \mid d o(S)]$.</p>
<p>We assume that there exists an oracle that answers queries such as "Is $v_{i}$ independent of $v_{j}$ given $Z$ in the interventional distribution $\operatorname{Pr}[\cdot \mid \operatorname{do}(S=s)]$ ?"</p>
<p>Assumption 2.1 (Conditional Independence (CI)-Oracle). Given any $v_{i}, v_{j} \in V$ and $Z, S \subseteq V$ we have an oracle that tests whether $v_{i} \Perp v_{j} \mid Z, \operatorname{do}(S=s)$.</p>
<p>Such conditional independence tests have been widely investigated with sublinear (in domain size) bounds on the sample size needed for implementing this oracle [Canonne et al., 2018; Zhang et al., 2011].</p>
<p>Intervention Cost Models. We study the causal discovery problem under two cost models:</p>
<ol>
<li>
<p>Linear Cost Model. In this model, each node $v \in V$ has a different cost $c(v) \in \mathbb{R}^{+}$and the cost of intervention on a set $S \subset V$ is defined as $\sum_{v \in S} c(v)$ (akin to [Lindgren et al., 2018]). That is, interventions that involve a larger number of, or more costly nodes, are more expensive. Our goal is to find an intervention set $\mathcal{S}$ minimizing $\sum_{S \in \mathcal{S}} \sum_{v \in S} c(v)$. We constrain the number of interventions to be upper bounded by some budget $m$. Without such a bound, we can observe that for ancestral graph recovery, the optimal intervention set is $\mathcal{S}=\left{\left{v_{1}\right},\left{v_{2}\right}, \ldots,\left{v_{n}\right}\right}$ with cost $\sum_{v \in V} c(v)$ as intervention on every variable is necessary, as we need to account for the possibility of latent variables (See Lemma 3.1 for more details). The optimality of $\mathcal{S}$ here follows from a characterization of any feasible set system we establish in Lemma 3.1.
<sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
</li>
<li>
<p>Identity Cost Model. As an intervention on a set of variables requires controlling the variables, and generating a new distribution, we want to use as few interventions as possible. In this cost model, an intervention on any set of observed variables has unit cost (no matter how many variables are in the set). We assume that for any intervention, querying the CI-oracle comes free of cost. This model is akin to the model studied in Kocaoglu et al. [2017b].</p>
</li>
</ol>
<p>Causal Discovery Goals. We will study two variations of the causal discovery problem. In the first, we aim to recover the ancestral graph $\operatorname{Anc}(G)$, which contains all the causal ancestral relationships between the observable variables $V$. In the second, our goal is to recover all the causal relations in $\mathcal{E}$, i.e., learn the entire causal graph $\mathcal{G}(V \cup L, \mathcal{E})$. We aim to perform both tasks using a set of intervention sets $\mathcal{S}=\left{S_{1}, \ldots, S_{m}\right}$ (each $S_{i} \subseteq V$ ) with minimal cost, with our cost models defined above.</p>
<p>For ancestral graph recovery, we will leverage a simple characterization of when a set of interventions $\mathcal{S}=\left{S_{1}, \ldots, S_{m}\right}$ is sufficient to recover $\operatorname{Anc}(G)$. In particular, $\mathcal{S}$ is sufficient if it is a strongly separating set system [Kocaoglu et al., 2017b].</p>
<p>Definition 2.2 (Strongly Separating Set System). A collection of subsets $\mathcal{S}=\left{S_{1}, \cdots, S_{m}\right}$ of the ground set $V$ is a strongly separating set system if for every distinct $u, v \in V$ there exists $S_{i}$ and $S_{j}$ such that $u \in S_{i} \backslash S_{j}$ and $v \in S_{j} \backslash S_{i}$.</p>
<p>Ancestral graph recovery using a strongly separating set system is simple: we intervene on each of the sets $S_{1}, \ldots, S_{m}$. Using CI-tests we can identify for every pair of $v_{i}$ and $v_{j}$, if there is a path from $v_{i}$ to $v_{j}$ or not in $G$ using the intervention corresponding to $S \in \mathcal{S}$ with $v_{i} \in S$ and $v_{j} \notin S$. We add an edge to $\operatorname{Anc}(G)$ if the test returns dependence. Finally, we take the transitive closure and output the resulting graph as $\operatorname{Anc}(G)$. In Lemma 3.1, we show that in fact being strongly separating is necessary for any set of interventions to be used to identify $\operatorname{Anc}(G)$.</p>
<h1>3 Linear Cost Model</h1>
<p>We begin with our results on recovering the ancestral graph $\operatorname{Anc}(G)$ in the linear cost model. Recall that, given a budget of $m$ interventions, our objective is to find a set of interventions $\mathcal{S}=$ $\left{S_{1}, S_{2}, \cdots S_{m}\right}$ that can be used to identify $\operatorname{Anc}(G)$ while minimizing $\sum_{S \in \mathcal{S}} \sum_{v \in S} c(v)$.</p>
<p>As detailed in Section 2, a strongly separating set system is sufficient to recover the ancestral graph. We show that it also necessary: a set of interventions to discover $\operatorname{Anc}(G)$ must be a strongly separating set system (Definition 2.2). See proof in Appendix A.</p>
<p>Lemma 3.1. Suppose $\mathcal{S}=\left{S_{1}, S_{2}, \cdots, S_{m}\right}$ is a collection of subsets of $V$. For a given causal graph $G$ if $\operatorname{Anc}(G)$ is recovered using CI-tests by intervening on the sets $S_{i} \in \mathcal{S}$. Then, $\mathcal{S}$ is a strongly separating set system.</p>
<p>Given this characterization, the problem of constructing the ancestral graph $\operatorname{Anc}(G)$ with minimum linear cost reduces to that of constructing a strongly separating set system with minimum cost. In developing our algorithm for finding such a set system, it will be useful to represent a set system by a binary matrix, with rows corresponding to observable variables $V$ and columns corresponding to interventions (sets $S_{1}, \ldots, S_{m}$ ).</p>
<p>Definition 3.2 (Strongly Separating Matrix). Matrix $U \in{0,1}^{n \times m}$ is a strongly separating matrix if $\forall i, j \in[n]$ there exists $k, k^{\prime} \in[m]$ such that $U(i, k)=1, U(j, k)=0$ and $U\left(i, k^{\prime}\right)=$ $0, U\left(j, k^{\prime}\right)=1$.</p>
<p>Note that given a strongly separating set system $\mathcal{S}$, if we let $U$ be the matrix where $U(i, k)=1$ if $v_{i} \in S_{k}$ and 0 otherwise, $U$ will be a strongly separating matrix. The other direction is also true. Let $U(j)$ denote the $j$ th row of $U$. Using Definition 3.2 and above connection between recovering $\operatorname{Anc}(G)$ and strongly separating set system, we can reformulate the problem at hand as:</p>
<p>$$
\begin{aligned}
&amp; \min <em j="1">{U} \sum</em> \
&amp; \text { s.t. } U \in{0,1}^{n \times m} \text { is a strongly separating matrix. }
\end{aligned}
$$}^{n} c\left(v_{j}\right) \cdot|U(j)|_{1</p>
<p>We can thus view our problem as finding an assignment of vectors in ${0,1}^{m}$ (i.e., rows of $U$ ) to nodes in $V$ that minimizes (1). Throughout, we will call $|U(j)|_{1}$ the weight of row $U(j)$, i.e., the number of 1 s in that row. It is easy to see that $m \geq \log n$ is necessary for a feasible solution to exist as each row must be distinct.</p>
<p>We start by giving a 2-approximation algorithm for (1). In Section 3.2, we show how to obtain an improved approximation under certain assumptions.</p>
<h1>3.1 2-approximation Algorithm</h1>
<p>In this section, we present an algorithm (Algorithm SSMATRIX) that constructs a strongly separating matrix (and a corresponding intervention set) which minimizes (1) to within a 2-factor of the optimum. Missing details from section are collected in Appendix A.1.
Outline. Let $U_{\mathrm{OPT}}$ denote a strongly separating matrix minimizing (1). Let $c_{\mathrm{OPT}}=\sum_{j=1}^{n} c\left(v_{j}\right)\left|U_{\mathrm{OPT}}(j)\right|<em _mathrm_OPT="\mathrm{OPT">{1}$ denote the objective value achieved by this optimum $U</em>$, as we have only relaxed the constraint in (1).}}$. We start by relaxing the constraint on $U$ so that it does not need to be strongly separating, but just must have unique rows, where none of the rows is all zero. In this case, we can optimize (1) very easily. We simply take the rows of $U$ to be the $n$ unique binary vectors in ${0,1}^{m} \backslash\left{0^{m}\right}$ with lowest weights. That is, $m$ rows will have weight $1,\binom{m}{2}$ will have weight 2 , etc. We then assign the rows to the nodes in $V$ in descending order of their costs. So the $m$ nodes with the highest costs will be assigned the weight 1 rows, the next $\binom{m}{2}$ assigned weight 2 rows, etc. The cost of this assignment is only lower than $c_{\mathrm{OPT}</p>
<p>We next convert this relaxed solution into a valid strongly separating matrix. Given $m+\log n$ columns, we can do this easily. Since there are $n$ nodes, in the above assignment, all rows will have weight at most $\log n$. Let $\bar{U} \in{0,1}^{m+\log n}$ have its first $m$ columns equal to those of $U$. Additionally, use the last $\log n$ columns as 'row weight indicators': if $|U(j)|_{1}=k$ then set $\bar{U}(j, m+$ $k)=1$. We can see that $\bar{U}$ is a strongly separating matrix. If two rows have different weights $k, k^{\prime}$ in $\bar{U}$, then the last $\log n$ columns ensure that they satisfy the strongly separating condition. If they have the same weight in $\bar{U}$, then they already satisfy the condition, as to be unique in $U$ they must have a at least 2 entries on which they differ.</p>
<p>To turn the above idea into a valid approximation algorithm that outputs $\bar{U}$ with just $m$ (not $m+\log n$ ) columns, we argue that we can 'reserve' the last $\log n$ columns of $\bar{U}$ to serve as weight indicator columns. We are then left with just $m-\log n$ columns to work with. Thus we can only assign $m-\log n$ weight 1 rows, $\binom{m-\log n}{2}$ weight 2 rows, etc. Nevertheless, if $m \geq \gamma \log n$ (for a constant $\gamma&gt;1$ ), this does not affect the assignment much: for any $i$ we can still 'cover' the $\binom{m}{i}$ weight $i$ rows in $U$ with rows of weight $\leq 2 i$. Thus, after accounting for the weight indicator columns, each weight $k$ row in $U$ has weight $\leq 2 k+1$ in $\bar{U}$. Overall, this gives us a 3 -approximation algorithm: when $k$ is 1 the weight of a row may become as large as 3 .</p>
<p>To improve the approximation to a 2-approximation we guess the number of weight 1 vectors $a_{1}$ in the optimum solution $U_{\mathrm{OPT}}$ and assign the $a_{1}$ highest cost variables to weight 1 vectors, achieving</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="n">\(1</span><span class="w"> </span><span class="n">\operatorname{SSMATRIX}(V,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="n">\)</span>
<span class="n">\(c_{U_{\min</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\infty\)</span>
<span class="k">for</span><span class="w"> </span><span class="n">\(a_{1}</span><span class="w"> </span><span class="n">\in\{0,1,</span><span class="w"> </span><span class="n">\cdots,</span><span class="w"> </span><span class="mh">2</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mh">3</span><span class="n">\}\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span><span class="n">\(U</span><span class="w"> </span><span class="n">\in\{0,1\}^{n</span><span class="w"> </span><span class="n">\times</span><span class="w"> </span><span class="n">m</span><span class="p">}</span><span class="n">\)</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">initialized</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">zeros</span>
<span class="w">    </span><span class="n">Assign</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">highest</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="n">\(a_{1}\)</span><span class="w"> </span><span class="n">nodes</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">unit</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">vectors</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">\(U(i,</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="o">=</span><span class="mh">1</span><span class="n">\)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">\(i</span><span class="w"> </span><span class="n">\leq</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="mh">1</span><span class="p">}</span><span class="n">\)</span>
<span class="w">    </span><span class="n">Set</span><span class="w"> </span><span class="n">\(m^{\prime}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">m</span><span class="o">-</span><span class="n">a_</span><span class="p">{</span><span class="mh">1</span><span class="p">}</span><span class="n">\)</span>
<span class="w">    </span><span class="n">Mark</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">vectors</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="mh">1</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">\(\{0,1\}^{m^{\prime}-\log</span><span class="w"> </span><span class="n">n</span><span class="p">}</span><span class="n">\)</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">available</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">unassigned</span><span class="w"> </span><span class="n">\(v_{i}</span><span class="w"> </span><span class="n">\in</span><span class="w"> </span><span class="n">V\)</span><span class="w"> </span><span class="p">(</span><span class="n">in</span><span class="w"> </span><span class="n">decreasing</span><span class="w"> </span><span class="n">order</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">cost</span><span class="p">)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">Set</span><span class="w"> </span><span class="n">\(U\left(i,\left(a_{1}+1\right):</span><span class="w"> </span><span class="n">m</span><span class="o">-</span><span class="n">\log</span><span class="w"> </span><span class="n">n\right)\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">smallest</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">vector</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">\(\{0,1\}^{m^{\prime}-\log</span><span class="w"> </span><span class="n">n</span><span class="p">}</span><span class="n">\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">make</span><span class="w"> </span><span class="n">this</span>
<span class="w">    </span><span class="n">vector</span><span class="w"> </span><span class="n">unavailable</span><span class="p">.</span><span class="w"> </span><span class="n">Let</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">assigned</span><span class="w"> </span><span class="n">vector</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">\(k\)</span>
<span class="w">        </span><span class="n">Set</span><span class="w"> </span><span class="p">&#39;</span><span class="n">row</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">indicator</span><span class="p">&#39;</span><span class="w"> </span><span class="n">\(U\left(i,</span><span class="w"> </span><span class="n">m</span><span class="o">^</span><span class="p">{</span><span class="n">\prime}-\log</span><span class="w"> </span><span class="n">n</span><span class="o">+</span><span class="n">k\right)=1\)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">objective</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">\(U\)</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">\(c_{U}\)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">\(c_{U}&lt;c_{U_{\min</span><span class="w"> </span><span class="p">}}</span><span class="n">\)</span><span class="w"> </span><span class="n">then</span>
<span class="w">            </span><span class="n">\(c_{U_{\min</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">c_</span><span class="p">{</span><span class="n">U</span><span class="p">},</span><span class="w"> </span><span class="n">U_</span><span class="p">{</span><span class="n">\min</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">U\)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="n">Return</span><span class="w"> </span><span class="n">\(U_{\min}\)</span>
</code></pre></div>

<p>optimal cost for these variables. There are $O(m)$ possible values for $a_{1}$ and so trying all guesses is still efficient. We then apply our approximation algorithm to the remaining $m-a_{1}$ available columns of $U$ and $n-a_{1}$ variables. Since no variables are assigned weight 1 in this set, we achieve a tighter 2-approximation using our approach. The resulting matrix has the form:</p>
<p>$$
U=\left(\begin{array}{c|ccc}
\mathbb{I}<em 1="1">{a</em> &amp; 0 &amp; 0 \
\hline 0 &amp; C_{1} &amp; M_{1} \
\hline 0 &amp; C_{2} &amp; M_{2} \
\hline \vdots &amp; \vdots &amp; \vdots
\end{array}\right)
$$}</p>
<p>where $\mathbb{I}<em 1="1">{a</em>$ are length $\log n$ binary vectors with 1's in the $w$ th column. The entire approach is presented in Algorithm SSMATRIX and a proof of the approximation bound in Theorem 3.3 is present in Appendix A.1.}}$ is the $a_{1} \times a_{1}$ identity matrix, the rows of $C_{w}$ are all weight $w$ binary vectors of length $m-\log n-a_{1}$, and the rows of $M_{w</p>
<p>Theorem 3.3. Let $m \geq \gamma \log n$ for constant $\gamma&gt;1$ and $U$ be the strongly separating matrix returned by SSMATRIX. ${ }^{2}$ Let $c_{U}=\sum_{j=1}^{n} c\left(v_{j}\right)|U(j)|<em U="U">{1}$. Then, $c</em>$.} \leq 2 \cdot c_{\mathrm{OPT}}$, where $c_{\mathrm{OPT}}$ is the objective value associated with optimum set of interventions corresponding to $U_{\mathrm{OPT}</p>
<p>Using the interventions from the matrix $U$ returned by Algorithm SSMATRIX, we obtain a cost within twice the optimum for recovering $\operatorname{Anc}(G)$.</p>
<h1>$3.2(1+\epsilon)$-approximation Algorithm</h1>
<p>In [Hyttinen et al., 2013a], the authors show how to construct a collection $\mathcal{A}$ of $m$ strongly separating intervention sets with minimum average set size, i.e., $\sum_{A \in \mathcal{A}}|A| / m$. This is equivalent to minimizing the objective (1) in the linear cost model when the cost of intervening on any node equals 1. In this section, we analyze an adaptation of their algorithm to the general linear cost model, and obtain a $(1+\epsilon)$-approximation for any given $0&lt;\epsilon \leq 1$, an improvement over the 2 -approximation of Section 3.1. Our analysis requires mild restrictions on the number of interventions and an upper bound on</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the maximum cost. The algorithm will not depend on $\epsilon$ but these bounds will. Missing details from this section are collected in Appendix A.2.
Algorithm $\epsilon$-SSMATRIX Outline. The famous Kruskal-Katona theorem in combinatorics forms the basis of the scheme presented in [Hyttinen et al., 2013a] for minimizing the average size of the intervention sets. To deal with with varying costs of node interventions, we augment this approach with a greedy strategy. Let $\mathcal{A}$ denote a set of $m$ interventions sets over the nodes $\left{v_{1}, v_{2} \cdots, v_{n}\right}$ obtained using the scheme from [Hyttinen et al., 2013a]. Construct a strongly separating matrix $\widetilde{U}$ from $\mathcal{A}$ with $\widetilde{U}(i, j)=1$ iff $v_{i} \in A_{j}$ for $A_{j} \in \mathcal{A}$. Let $\zeta$ denote the ordering of rows of $\widetilde{U}$ in the increasing order of weight. Our Algorithm $\epsilon$-SSMATRIX outputs the strongly separating matrix $U$ where, for every $i \in[n], U(i)=\widetilde{U}(\zeta(i))$ and the $i$ th row of $U$ corresponds to the node with $i$ th largest cost.</p>
<p>Let $c_{\max }=\max <em i="i">{v</em>\right) / \min } \in V} c\left(v_{i<em i="i">{v</em>\right)$ be the ratio of maximum cost to minimum cost of nodes in $V$. For ease of analysis, we assume that the cost of any node is least 1.
Theorem 3.4. Let $U$ be the strongly separating matrix returned by $\epsilon$-SSMATRIX. If $c_{\max } \leq \frac{\epsilon n}{3\binom{m}{t}}$ for $0&lt;\epsilon \leq 1$ where $\binom{m}{k-1}&lt;n \leq\binom{ m}{k}$ and $t=\lfloor k-\epsilon k / 3\rfloor$, then,} \in V} c\left(v_{i</p>
<p>$$
c_{U}:=\sum_{j=1}^{n} c\left(v_{j}\right)|U(j)|<em _mathrm_OPT="\mathrm{OPT">{1} \leq(1+\epsilon) \cdot c</em>
$$}</p>
<p>where $c_{\mathrm{OPT}}$ is the objective value associated with optimum set of interventions corresponding to $U_{\mathrm{OPT}}$.
Proof. Suppose the optimal solution $U_{\mathrm{OPT}}$ includes $a_{q}^{<em>}$ vectors of weight $q$. Let $S$ be the $a_{1}^{</em>}+a_{2}^{<em>}+\ldots+$ $a_{t}^{</em>}$ nodes with highest cost in $U_{\mathrm{OPT}}$. Since $a_{q}^{*} \leq\binom{ m}{q}$, it immediately follows that $|S| \leq \sum_{i=q}^{t}\binom{m}{q}$. However, a slightly tighter analysis (see Lemma A.10) implies $|S| \leq\binom{ m}{t}$. Let $c_{\mathrm{OPT}}(S)$ be the total contribution of the nodes in $S$ to $c_{\mathrm{OPT}}$. Let $c_{U}(S)$ denote the sum of contribution of the nodes in $S$ to $c_{U}$ for the matrix $U$ returned by $\epsilon$-SSMATRIX. Let $\bar{k}<em n="n">{|S|}$ and $\bar{k}</em>}$ be the average of the smallest $|S|$ and $n$ respectively of the vector weights assigned by the algorithm. It is easy to observe that $\bar{k<em n="n">{|S|} \leq \bar{k}</em>$.</p>
<p>$$
\begin{aligned}
c_{U}(S) &amp; =\sum_{v_{i} \in S} c\left(v_{i}\right)|U(i)|<em _max="\max">{1} \leq c</em>|U(i)|} \sum_{v_{i} \in S<em _max="\max">{1} \
&amp; =c</em>} \bar{k<em _max="\max">{|S|}|S| \leq c</em>} \bar{k<em _S_="|S|">{|S|}\binom{m}{t} \leq \epsilon \bar{k}</em> n / 3
\end{aligned}
$$</p>
<p>As every node in $V \backslash S$ receives weight at least $t=k-\epsilon k / 3$ in $U_{\mathrm{OPT}}$ and at most $k$ in $U$ returned by $\epsilon$-SSMATRIX, we have $c_{U}(V \backslash S) \leq \frac{c_{\mathrm{OPT}}(V \backslash S)}{1-\epsilon / 3}$. Now, we give a lower bound on the cost of the optimum solution $c_{\mathrm{OPT}}(V)$. We know that when costs of all the nodes are 1 , then $\epsilon$ SSMATRIXachieves optimum cost denoted by $c_{\mathrm{OPT}}^{\prime}(V)$ (see Appendix A. 2 for more details). As all the nodes of $V$ have costs more than 1 , we have:</p>
<p>$$
c_{\mathrm{OPT}}(V) \geq c_{\mathrm{OPT}}^{\prime}(V)=\bar{k}<em _S_="|S|">{n} \cdot n \geq \bar{k}</em> \cdot n
$$</p>
<p>Hence,</p>
<p>$$
\frac{c_{U}(V)}{c_{\mathrm{OPT}}(V)} \leq \frac{c_{U}(S)}{\bar{k}<em U="U">{|S|} n}+\frac{c</em> \leq 1+\epsilon
$$}(V \backslash S)}{c_{\mathrm{OPT}}(V \backslash S)} \leq \frac{\epsilon}{3}+\frac{1}{1-\epsilon / 3</p>
<p>This completes the proof.</p>
<p>By bounding the binomial coefficients in Thm. 3.4, we obtain the following somewhat easier to interpret corollary:</p>
<p>Corollary 3.5. If $c_{\text {max }} \leq(\epsilon / 6) n^{\Omega(\epsilon)}$ and either a) $n^{\epsilon / 6} \geq m \geq\left(2 \log <em 1="1">{2} n\right)^{c</em>&gt;1$ or b) $4 \log }}$ for some constant $c_{1<em 2="2">{2} n \leq m \leq c</em> \log <em 2="2">{2} n$ for some constant $c</em>$ then the Algorithm $\epsilon$-SSMATRIX returns an $(1+\epsilon)$-approximation.</p>
<h1>4 Identity Cost Model</h1>
<p>In this section, we consider the identity cost model, where the cost of intervention for any subset of variables is the same. Our goal is to construct the entire causal graph $\mathcal{G}$, while minimizing the number of interventions. Our algorithm is based on parameterizing the causal graph based on a specific type of collider structure.</p>
<p>Before describing our algorithms, we recall the notion of $d$-separation and introduce this specific type of colliders that we rely on. Missing details from section are collected in Appendix B.
Colliders. Given a causal graph $\mathcal{G}(V \cup L, \mathcal{E})$, let $v_{i}, v_{j} \in V$ and a set of nodes $Z \subseteq V$. We say $v_{i}$ and $v_{j}$ are $d$-separated by $Z$ if and only if every undirected path $\pi$ between $v_{i}$ and $v_{j}$ is blocked by $Z$. A path $\pi$ between $v_{i}$ and $v_{j}$ is blocked by $Z$ if at least one of the following holds.</p>
<p>Rule 1: $\pi$ contains a node $v_{k} \in Z$ such that the path $\pi=v_{i} \cdots \rightarrow v_{k} \rightarrow \cdots v_{j}$ or $v_{i} \cdots \leftarrow v_{k} \leftarrow$ $\cdots v_{j}$.</p>
<p>Rule 2: $\pi=v_{i} \cdots \rightarrow v_{k} \leftarrow \cdots v_{j}$ contains a node $v_{k}$ and both $v_{k} \notin Z$ and no descendant of $v_{k}$ is in $Z$.</p>
<p>Lemma 4.1. [Pearl, 2009] If $v_{i}$ and $v_{j}$ are d-separated by $Z$, then $v_{i} \Perp v_{j} \mid Z$.
For the path $\pi=v_{i} \cdots \rightarrow v_{k} \leftarrow \cdots v_{j}$ between $v_{i}$ and $v_{j}, v_{k}$ is called a collider as there are two arrows pointing towards it. We say that $v_{k}$ is a collider for the pair $v_{i}$ and $v_{j}$, if there exists a path between $v_{i}$ and $v_{j}$ for which $v_{k}$ is a collider. As shown by Rule 2, colliders play an important role in $d$-separation. We give a more restrictive definition for colliders that we will rely on henceforth.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: $v_{k}$ is a $p$-collider for $v_{i}, v_{j}$ as it has a path to $v_{p}$, a parent of $v_{j}$.</p>
<p>Definition 4.2 ( $p$-colliders). Given a causal graph $\mathcal{G}\left(V \cup L, E \cup E_{L}\right)$. Consider $v_{i}, v_{j} \in V$ and $v_{k} \in V$. We say $v_{k}$ is a $p$-collider for the pair $v_{i}$ and $v_{j}$, if there exists a path $v_{i} \cdots \rightarrow v_{k} \leftarrow \cdots v_{j}$ in $\mathcal{G}$ and either $v_{k} \in \operatorname{Pa}\left(v_{i}\right) \cup \operatorname{Pa}\left(v_{j}\right)$ or has at least one descendant in $\operatorname{Pa}\left(v_{i}\right) \cup \operatorname{Pa}\left(v_{j}\right)$. Let $P_{i j} \subset V$ denote all the $p$-colliders between $v_{i}$ and $v_{j}$.</p>
<p>Intervening on p-colliders essentially breaks down all the primitive inducing paths. Primitive inducing paths are those whose endpoints cannot be separated by any conditioning [Richardson et al., 2002]. Now, between every pair of observable variables, we can define a set of $p$-colliders as above. Computing $P_{i j}$ for the pair of variables $v_{i}$ and $v_{j}$ explicitly requires the knowledge of $\mathcal{G}$, however as we show below we can use randomization to overcome this issue. The following parameterization of a causal graph will be useful in our discussions.</p>
<p>Definition 4.3 ( $\tau$-causal graph). A causal graph $\mathcal{G}(V \cup L, \mathcal{E})$ is a $\tau$-causal graph if for every pair of nodes in $V$ the number of $p$-colliders is at most $\tau$, i.e., $v_{i}, v_{j} \in V(i \neq j)$, we have $\left|P_{i j}\right| \leq \tau$.</p>
<p>Note that every causal graph is at most $n-2$-causal. In practice, we expect $\tau$ to be significantly smaller. Given a causal graph $\mathcal{G}$, it is easy to determine the minimum values of $\tau$ for which it is $\tau$-causal, as checking for $p$-colliders is easy. Our algorithm recovers $\mathcal{G}$ with number of interventions that grow as a function of $\tau$ and $n$.
Outline of our Approach. Let $\mathcal{G}$ be a $\tau$-causal graph. As in [Kocaoglu et al., 2017b], we break our approach into multiple steps. Firstly, we construct the ancestral graph $\operatorname{Anc}(G)$ using the strongly separating set system (Definition 2.2) idea detailed in Section 2. For example, a strongly separating set system can be constructed with $m=2 \log n$ interventions by using the binary encoding of the numbers $1, \cdots, n$ [Kocaoglu et al., 2017b]. After that the algorithm has two steps. In the first step, we recover the observable graph $G$ from $\operatorname{Anc}(G)$. In the next step, after obtaining the observable graph, we identify all the latents $L$ between the variables in $V$ to construct $\mathcal{G}$. In both these steps, an underlying idea is to construct intervention sets with the aim of making sure that all the $p$-colliders between every pair of nodes is included in at least one of the intervention sets. As we do not know the graph $\mathcal{G}$, we devise randomized strategies to hit all the $p$-colliders, whilst ensuring that we do not create a lot of interventions.</p>
<p>A point to note is that, we design the algorithms to achieve an overall success probability of $1-O\left(1 / n^{2}\right)$, however, the success probability can be boosted to any $1-O\left(1 / n^{c}\right)$ for any constant $c$, by just adjusting the constant factors (see for example the proof of Lemma B.2). Also for simplicity of discussion, we assume that we know $\tau$. However as we discuss in Appendix B this assumption can be easily removed with an additional $O(\log \tau)$ factor.</p>
<h1>4.1 Recovering the Observable Graph</h1>
<p>$\operatorname{Anc}(G)$ encodes all the ancestral relations on observable variables $V$ of the causal graph $G$. To recover $G$ from $\operatorname{Anc}(G)$, we want to differentiate whether $v_{i} \rightarrow v_{j}$ represents an edge in $G$ or a directed path going through other nodes in $G$. We use the following observation, if $v_{i}$ is a parent of $v_{j}$, the path $v_{i} \rightarrow v_{j}$ is never blocked by any conditioning set $Z \subseteq V \backslash\left{v_{i}\right}$. If $v_{i} \notin \operatorname{Pa}\left(v_{j}\right)$, then we show that we can provide a conditioning set $Z$ in some interventional distribution $S$ such that $v_{i} \Perp v_{j} \mid Z, \operatorname{do}(S)$. For every pair of variables that have an edge in $\operatorname{Anc}(G)$, we design conditioning sets in Algorithm 2 that blocks all the paths between them.</p>
<p>Let $v_{i} \in \operatorname{Anc}\left(v_{j}\right) \backslash \operatorname{Pa}\left(v_{j}\right)$. We argue that conditioning on $\operatorname{Anc}\left(v_{j}\right) \backslash\left{v_{i}\right}$ in do $\left(v_{i} \cup P_{i j}\right)$ blocks all the paths from $v_{i}$ to $v_{j}$. The first simple observation, from $d$-separation is that if we take a path that has no $p$-colliders between $v_{i}$ to $v_{j}$ (a $p$-collider free path) then it is blocked by conditioning on $\operatorname{Anc}\left(v_{j}\right) \backslash\left{v_{i}\right}$ i.e., $v_{i} \Perp v_{j} \mid \operatorname{Anc}\left(v_{j}\right) \backslash\left{v_{i}\right}$.</p>
<p>The idea then will be to intervene on colliders $P_{i j}$ to remove these dependencies between $v_{i}$ and $v_{j}$ as shown by the following lemma.</p>
<p>Lemma 4.4. Let $v_{i} \in \operatorname{Anc}\left(v_{j}\right) . v_{i} \Perp v_{j} \mid \operatorname{do}\left(v_{i} \cup P_{i j}\right), \operatorname{Anc}\left(v_{j}\right) \backslash\left{v_{i}\right}$ iff $v_{i} \notin \operatorname{Pa}\left(v_{j}\right)$.</p>
<p>From Lemma 4.4, we can recover the edges of the observable graph $G$ provided we know the $p$-colliders between every pair of nodes. However, since the set of $p$-colliders is unknown without the knowledge of $\mathcal{G}$, we construct multiple intervention sets by independently sampling every variable with some probability. This ensures that there exists an intervention set $S$ such that $\left{v_{i}\right} \cup P_{i j} \subseteq S$ and $v_{j} \notin S$ with high probability.</p>
<p>Formally, let $A_{t} \subseteq V$ for $t \in\left{1,2, \cdots, 72 \tau^{\prime} \log n\right}$ be constructed by including every variable $v_{i} \in V$ with probability $1-1 / \tau^{\prime}$ where $\tau^{\prime}=\max {\tau, 2}$. Let $\mathcal{A}<em 1="1">{\tau}=\left{A</em>$.}, \cdots, A_{72 \tau^{\prime} \log n}\right}$ be the collection of the set $A_{t}$ 's. Algorithm 2 uses the interventions in $\mathcal{A}_{\tau</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">RecoverG</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Anc</span><span class="p">}(</span><span class="nx">G</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">tau</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">E</span><span class="p">=</span><span class="err">\</span><span class="nx">phi</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">rightarrow</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Anc</span><span class="p">}(</span><span class="nx">G</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Let</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">}=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">A</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">tau</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">such</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">A</span><span class="p">,</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">notin</span><span class="w"> </span><span class="nx">A</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="k">forall</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="k">not</span><span class="w"> </span><span class="err">\</span><span class="nx">models</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Anc</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">backslash</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">do</span><span class="p">}(</span><span class="nx">A</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">E</span><span class="p">=</span><span class="nx">E</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">E</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Proposition 4.5. Let $\mathcal{G}\left(V \cup L, E \cup E_{L}\right)$ be a $\tau$-causal graph with observable graph $G(V, E)$. There exists a procedure to recover the observable graph using $O(\tau \log n+\log n)$ many interventions with probability at least $1-1 / n^{2}$.</p>
<p>Lower Bound. Complementing the above result, the following proposition gives a lower bound on the number of interventions by providing an instance of a $O(n)$-causal graph such that any nonadaptive algorithm requires $\Omega(n)$ interventions for recovering it. The lower bound comes because of the fact that the algorithm cannot rule out the possibility of latent.</p>
<p>Proposition 4.6. There exists a graph causal $\mathcal{G}\left(V \cup L, E \cup E_{L}\right)$ such that every non-adaptive algorithm requires $\Omega(n)$ many interventions to recover even the observable graph $G(V, E)$ of $\mathcal{G}$.</p>
<h1>4.2 Detecting the Latents</h1>
<p>We now describe algorithms to identify latents that effect the observable variables $V$ to learn the entire causal graph $\mathcal{G}\left(V \cup L, E \cup E_{L}\right)$. We start from the observable graph $G(V, E)$ constructed in the previous section. Our goal will be to use the fact that $\mathcal{G}$ is a $\tau$-causal graph, which means that $\left|P_{i j}\right| \leq \tau$ for every pair $v_{i}, v_{j}$. Since we assumed that each latent variable (in $L$ ) effects at most two observable variables (in $V$ ), we can split the analysis into two cases: a) pairs of nodes in $G$ without an edge (non-adjacent nodes) and b) pairs of nodes in $G$ with a direct edge (adjacent). In Algorithm LatentsNEDGES(Appendix B), we describe the algorithm for identifying the latents effecting pairs of non-adjacent nodes. The idea is to block the paths by conditioning on parents and intervening on $p$-colliders. We use the observation that for any non-adjacent pair $v_{i}, v_{j}$ an intervention on the set $P_{i j}$ and conditioning on the parents of $v_{i}$ and $v_{j}$ will make $v_{i}$ and $v_{j}$ independent, unless there is a latent between them.</p>
<p>Proposition 4.7. Let $\mathcal{G}\left(V \cup L, E \cup E_{L}\right)$ be a $\tau$-causal graph with observable graph $G(V, E)$. Algorithm LatentsNEDGESwith $O\left(\tau^{2} \log n+\log n\right)$ interventions recovers all latents effecting pairs of non-adjacent nodes in the observable graph $G$ with probability at least $1-1 / n^{2}$.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="nx">LatentsWEdges</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">V</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">L</span><span class="p">,</span><span class="w"> </span><span class="nx">E</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">E_</span><span class="p">{</span><span class="nx">L</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">tau</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Consider</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">edge</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">rightarrow</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">E</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Let</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">}=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">B</span><span class="w"> </span><span class="err">\</span><span class="nx">backslash</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">B</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">tau</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">s</span><span class="p">.</span><span class="nx">t</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">B</span><span class="p">,</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">notin</span><span class="w"> </span><span class="nx">B</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="k">forall</span><span class="w"> </span><span class="nx">B</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Pr</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Pa</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">do</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Pa</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">B</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="w"> </span><span class="err">\</span><span class="nx">neq</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Pr</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Pa</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">do</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Pa</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">B</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">L</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">L</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">l_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">E_</span><span class="p">{</span><span class="nx">L</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">E_</span><span class="p">{</span><span class="nx">L</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">l_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">l_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">v_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">V</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">L</span><span class="p">,</span><span class="w"> </span><span class="nx">E</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="w"> </span><span class="nx">E_</span><span class="p">{</span><span class="nx">L</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Latents Affecting Adjacent Nodes in $G$. Suppose we have an edge $v_{i} \rightarrow v_{j}$ in $G(V, E)$ and we want to detect whether there exists a latent $l_{i j}$ that effects both of them. Here, we cannot block the edge path $v_{i} \rightarrow v_{j}$ by conditioning on any $Z \subseteq V$ in any given interventional distribution $\operatorname{do}(S)$ where $S$ does not contain $v_{j}$. However, intervening on $v_{j}$ also disconnects $v_{j}$ from its latent parent. Therefore, CI-tests are not helpful. Hence, we make use of another test called do-see test [Kocaoglu et al., 2017b], that compares two probability distributions. We assume there exists an oracle that answers whether two distributions are the same or not. This is a well-studied problem with sublinear (in domain size) bound on the sample size needed for implementing this oracle [Chan et al., 2014].</p>
<p>Assumption 4.8 (Distribution Testing (DT)-Oracle). Given any $v_{i}, v_{j} \in V$ and $Z, S \subseteq V$ tests whether two distributions $\operatorname{Pr}\left[v_{j} \mid v_{i}, Z\right.$, do $\left.\left.(S)\right]$ and $\operatorname{Pr}\left[v_{j} \mid Z\right.\right.$, do $\left.\left(S \cup\left{v_{i}\right}\right)\right]$ are identical or not.</p>
<p>The intuition of the do-see test is as follows: if $v_{i}$ and $v_{j}$ are the only two nodes in the graph $G$ with $v_{i} \rightarrow v_{j}$, then, $\operatorname{Pr}\left[v_{j} \mid v_{i}\right]=\operatorname{Pr}\left[v_{j} \mid \operatorname{do}\left(v_{i}\right)\right]$ iff there exists no latent that effects both of them. This follows from the conditional invariance principle [Bareinboim et al., 2012] (or page 24, property 2 in [Pearl, 2009]). Therefore, the presence or absence of latents can be established by invoking a DT-oracle.</p>
<p>As we seek to minimize the number of interventions, our goal is to create intervention sets that contain $p$-colliders between every pair of variables that share an edge in $G$. However, in Lemmas $4.9,4.10$ we argue that it is not sufficient to consider interventions with only $p$-colliders. We must also intervene on $P a\left(v_{i}\right)$ to detect a latent between $v_{i} \rightarrow v_{j}$. The main idea behind LatentsWEdges is captured by the following two lemmas.</p>
<p>Lemma 4.9 (No Latent Case). Suppose $v_{i} \rightarrow v_{j} \in G$ and $v_{i}, v_{j} \notin B$, and $P_{i j} \subseteq B$ then, $\operatorname{Pr}\left[v_{j} \mid\right.$ $\left.v_{i}, \operatorname{Pa}\left(v_{j}\right), \operatorname{do}\left(\operatorname{Pa}\left(v_{i}\right) \cup B\right)\right]=\operatorname{Pr}\left[v_{j} \mid \operatorname{Pa}\left(v_{j}\right), \operatorname{do}\left(\left{v_{i}\right} \cup \operatorname{Pa}\left(v_{i}\right) \cup B\right)\right]$ if there is no latent $l_{i j}$ with $v_{i} \leftarrow l_{i j} \rightarrow v_{j}$.
Lemma 4.10 (Latent Case). Suppose $v_{i} \rightarrow v_{j} \in G$ and $v_{i}, v_{j} \notin B$, and $P_{i j} \subseteq B$, then, $\operatorname{Pr}\left[v_{j} \mid\right.$ $\left.v_{i}, \operatorname{Pa}\left(v_{j}\right), \operatorname{do}\left(\operatorname{Pa}\left(v_{i}\right) \cup B\right)\right] \neq \operatorname{Pr}\left[v_{j} \mid \operatorname{Pa}\left(v_{j}\right), \operatorname{do}\left(\left{v_{i}\right} \cup \operatorname{Pa}\left(v_{i}\right) \cup B\right)\right]$ if there is a latent $l_{i j}$ with $v_{i} \leftarrow l_{i j} \rightarrow v_{j}$.</p>
<p>From Lemmas 4.9, 4.10, we know that to identify a latent $l_{i j}$ between $v_{i} \rightarrow v_{j}$, we must intervene on all the $p$-colliders between them with $P a\left(v_{i}\right) \cup\left{v_{i}\right}$. To do this, we again construct random intervention sets. Let $B_{t} \subseteq V$ for $t \in\left{1,2, \cdots, 72 \tau^{\prime} \log n\right}$ be constructed by including every variable $v_{i} \in V$ with probability $1-1 / \tau^{\prime}$ where $\tau^{\prime}=\max {\tau, 2}$. Let $\mathcal{B}<em 1="1">{\tau}=\left{B</em>}, \cdots, B_{72 \tau^{\prime} \log n}\right}$ be the collection of the sets. Consider a pair $v_{i} \rightarrow v_{j}$. To obtain the interventions given by the above lemmas, we iterate over all sets in $\mathcal{B<em i="i">{\tau}$ and identify all the sets containing $v</em>}$, but not $v_{j}$. From these sets, we remove $v_{i}$ to obtain $\mathcal{B<em i="i">{i j}$. These new interventions are then used in LatentsWEdges to perform the required distribution tests using a DT-oracle on the interventions $B \cup P a\left(v</em>$. We can show:}\right)$ and $B \cup P a\left(v_{i}\right) \cup\left{v_{i}\right}$ for every $B \in \mathcal{B}_{i j</p>
<p>Proposition 4.11. Let $\mathcal{G}\left(V \cup L, E \cup E_{L}\right)$ be a $\tau$-causal graph with observable graph $G(V, E)$.
LatentsWEdges with $O(n \tau \log n+n \log n)$ interventions recovers all latents effecting pairs of adjacent nodes in the observable graph $G$ with probability at least $1-1 / n^{2}$.</p>
<p>Putting it all Together. Using Propositions 4.5, 4.7, and 4.11, we get the following result. Note that $\tau \leq n-2$.</p>
<p>Theorem 4.12. Given access to a $\tau$-causal graph $\mathcal{G}=\mathcal{G}\left(V \cup L, E \cup E_{L}\right)$ through Conditional Independence (CI) and Distribution Testing (DT) oracles, Algorithms RecoverG, LatentsNEDges, and LatentsWEdgesput together recovers $\mathcal{G}$ with $O(n \tau \log n+n \log n)$ interventions, with probability at least $1-O\left(1 / n^{2}\right)($ where $|V|=n)$.</p>
<h1>5 Experiments</h1>
<p>In this section, we compare the total number of interventions required to recover causal graph $\mathcal{G}$ parameterized by $p$-colliders (See Section 4) vs. maximum degree utilized by [Kocaoglu et al., 2017b].</p>
<p>Since the parameterization of these two results are different, a direct comparison between them is not always possible. If $\tau=o\left(d^{2} / n\right)$, we use fewer interventions than Kocaoglu et al. [2017b] for recovering the causal graph. Roughly, for any $0 \leq \epsilon \leq 1$, (a) when $\tau<n^{\epsilon}, d>n^{(1+\epsilon) / 2}$, our bound is better, (b) when $\tau&gt;n^{\epsilon}, \tau<d\<n^{(1+\epsilon) / 2}$, then we can identify latents using the algorithms of Kocaoglu et al. [2017b] after using our algorithm for observable graph recovery, and (c) when $\tau>d>n^{\epsilon}, d&lt;n^{(1+\epsilon) / 2}$, the bound in Kocaoglu et al. [2017b] is better.</p>
<p>In this section, our main motivation is to show that $p$-colliders can be a useful measure of complexity of a graph. As discussed in Section 1, even few nodes of high degree could make $d^{2}$ quite large.
Setup. We demonstrate our results by considering sparse random graphs generated from the families of: (i) Erdös-Rényi random graphs $G(n, c / n)$ for constant $c$, (ii) Random bipartite graphs generated using $G\left(n_{1}, n_{2}, c / n\right)$ model, with partitions $L, R$ and edges directed from $L$ to $R$, (iii) Random directed trees with degrees of nodes generated from power law distribution. In each of the graphs that we consider, we include latent variables by sampling $5 \%$ of $\binom{n}{2}$ pairs and adding a latent between them.
Finding $p$-colliders. Let $\mathcal{G}$ contain observable variables and the latents. To find $p$-colliders between every pair of observable nodes of $\mathcal{G}$, we enumerate all paths between them and check if any of the observable nodes on a path can be a possible $p$-collider. As this became practically infeasible for larger values of $n$, we devised an algorithm that runs in polynomial time (in the size of the graph) by constructing an appropriate flow network and finding maximum flow in this network. Please refer to Appendix C for more details.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of $\tau$ vs. maximum degree in sparse random bi-partite graphs.
Results. In our plots (Figure 2), we compare the maximum undirected degree $(d)$ with the maximum number of $p$-colliders between any pair of nodes (which defines $\tau$ ). We ran each experiment 10 times and plot the mean value along with one standard deviation error bars.</p>
<p>For random bipartite graphs, that can be used to model causal relations over time, we use equal partition sizes $n_{1}=n_{2}=n / 2$ and plot the results for $\mathcal{G}(n / 2, n / 2, c / n)$ for constant $c=5$. We observe that the behaviour is uniform for small constant values of $c$. In Figure 2, we observe that the maximum number of $p$-colliders $(\tau)$ is close to zero for all values of $n$ while the values of $d^{2} / n$ using the mean value of $d$, is significantly higher. So, in the range considered our algorithms use fewer interventions. We show similar results for other random graphs in Appendix C.</p>
<p>Therefore, we believe that minimizing the number of interventions based on the notion of $p$ colliders is a reasonable direction to consider.</p>
<h1>6 Concluding Remarks</h1>
<p>We have studied how to recover a causal graph in presence of latents while minimizing the intervention cost. In the linear cost setting, we give a 2 -approximation algorithm for ancestral graph recovery. This approximation factor can be improved to $(1+\epsilon)$ under some additional assumptions. Removing these assumptions would be an interesting direction for future work. In the identity cost setting, we give a randomized algorithm to recover the full causal graph, through a novel characterization based on $p$-colliders. In this setting, understanding the optimal intervention cost is open, and an important direction for research.</p>
<p>While we focus on non-adaptive settings, where all the interventions are constructed at once in the beginning, an adaptive (sequential) setting has received recent attention [He and Geng, 2008; Shanmugam et al., 2015], and is an interesting direction in both our cost models.</p>
<h2>Acknowledgements</h2>
<p>The first two named authors would like to thank Nina Mishra, Yonatan Naamad, MohammadTaghi Hajiaghayi and Dominik Janzing for many helpful discussions during the initial stages of this project. This work was partially supported by NSF grants CCF-1934846, CCF-1908849, and CCF-1637536.</p>
<h1>References</h1>
<p>Lada A Adamic and Bernardo A Huberman. Power-law distribution of the world wide web. Science, 287(5461):2115-2115, 2000.</p>
<p>Elias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences, 113(27):7345-7352, 2016.</p>
<p>Elias Bareinboim, Carlos Brito, and Judea Pearl. Local characterizations of causal bayesian networks. In Graph Structures for Knowledge Representation and Reasoning, pages 1-17. Springer, 2012 .</p>
<p>Clément L Canonne, Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Testing conditional independence of discrete distributions. In 2018 Information Theory and Applications Workshop (ITA), pages 1-57. IEEE, 2018.</p>
<p>Siu-On Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for testing closeness of discrete distributions. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 1193-1203. SIAM, 2014.</p>
<p>Frederick Eberhardt. Causation and intervention. PhD Thesis, Carnegie Mellon University, 2007.
Frederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosophy of Science, 74(5):981-995, 2007.</p>
<p>Alain Hauser and Peter Bühlmann. Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. Journal of Machine Learning Research, 13(Aug): $2409-2464,2012$.</p>
<p>Alain Hauser and Peter Bühlmann. Two optimal strategies for active learning of causal models from interventional data. International Journal of Approximate Reasoning, 55(4):926-939, 2014.</p>
<p>Yang-Bo He and Zhi Geng. Active learning of causal networks with intervention experiments and optimal designs. Journal of Machine Learning Research, 9(Nov):2523-2547, 2008.</p>
<p>Christina Heinze-Deml, Marloes H Maathuis, and Nicolai Meinshausen. Causal structure learning. Annual Review of Statistics and Its Application, 5:371-391, 2018.</p>
<p>Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pages 689-696, 2009.</p>
<p>Antti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. Experiment selection for causal discovery. The Journal of Machine Learning Research, 14(1):3041-3071, 2013a.</p>
<p>Antti Hyttinen, Patrik O Hoyer, Frederick Eberhardt, and Matti Jarvisalo. Discovering cyclic causal models with latent variables: A general sat-based procedure. arXiv preprint arXiv:1309.6836, 2013b.</p>
<p>Stasys Jukna. Extremal combinatorics: with applications in computer science. Springer Science \&amp; Business Media, 2011.</p>
<p>Gyula Katona. On separating systems of a finite set. Journal of Combinatorial Theory, 1(2): $174-194,1966$.</p>
<p>Ákos Kisvölcsey. Flattening antichains. Combinatorica, 1(26):65-82, 2006.
Murat Kocaoglu, Alex Dimakis, and Sriram Vishwanath. Cost-optimal learning of causal graphs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 18751884. JMLR. org, 2017a.</p>
<p>Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Experimental design for learning causal graphs with latent variables. In Advances in Neural Information Processing Systems, pages $7018-7028,2017 \mathrm{~b}$.</p>
<p>Joseph B Kruskal. The number of simplices in a complex. Mathematical Optimization Techniques, 10:251-278, 1963.</p>
<p>Erik Lindgren, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath. Experimental design for cost-aware learning of causal graphs. In Advances in Neural Information Processing Systems, pages 5279-5289, 2018.</p>
<p>Po-Ling Loh and Peter Bühlmann. High-dimensional learning of linear causal networks via inverse covariance estimation. The Journal of Machine Learning Research, 15(1):3065-3105, 2014.</p>
<p>Florence Jessie MacWilliams and Neil James Alexander Sloane. The theory of error-correcting codes, volume 16. Elsevier, 1977.</p>
<p>Cai Mao-Cheng. On separating systems of graphs. Discrete Mathematics, 49(1):15-20, 1984.
Pekka Parviainen and Mikko Koivisto. Ancestor relations in the presence of unobserved variables. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 581-596. Springer, 2011.</p>
<p>Judea Pearl. Causality: models, reasoning and inference, volume 29. Springer, 2000.
Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge university press, 2009.
Thomas Richardson, Peter Spirtes, et al. Ancestral graph markov models. The Annals of Statistics, 30(4):962-1030, 2002.</p>
<p>Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath. Learning causal graphs with small interventions. In Advances in Neural Information Processing Systems, pages $3195-3203,2015$.</p>
<p>Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, and Antti Kerminen. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003-2030, 2006.</p>
<p>Ricardo Silva, Richard Scheine, Clark Glymour, and Peter Spirtes. Learning the structure of linear latent variable models. Journal of Machine Learning Research, 7(Feb):191-246, 2006.</p>
<p>Peter Spirtes, Clark N Glymour, Richard Scheines, David Heckerman, Christopher Meek, Gregory Cooper, and Thomas Richardson. Causation, prediction, and search. MIT press, 2000.</p>
<p>Thomas Verma and Judea Pearl. An algorithm for deciding if a set of observed independencies has a causal explanation. In Uncertainty in artificial intelligence, pages 323-330. Elsevier, 1992.</p>
<p>Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Kernel-based conditional independence test and application in causal discovery. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 804-813. AUAI Press, 2011.</p>
<h1>Appendix</h1>
<h2>A Missing Details from Section 3</h2>
<p>Lemma A. 1 (Lemma 3.1 Restated). Suppose $\mathcal{S}=\left{S_{1}, S_{2}, \cdots, S_{m}\right}$ is a collection of subsets of $V$. For a given causal graph $G$ if $\operatorname{Anc}(G)$ is recovered using CI-tests by intervening on the sets $S_{i} \in \mathcal{S}$. Then, $\mathcal{S}$ is a strongly separating set system.</p>
<p>Proof. Suppose $\mathcal{S}$ is not a strongly separating set system. If there exists a pair of nodes $\left(v_{i}, v_{j}\right)$ such that every set $S_{k} \in \mathcal{S}$ contains none of them, then, we cannot recover the edge between these two nodes as we are not intervening on either $v_{i}$ or $v_{j}$ and the results of an independence test $v_{i} \Perp v_{j}$ might not be correct due to the presence of a latent variable $l_{i j}$ between them. Now, consider the case when only one of them is present in the set system. Let $\left(v_{i}, v_{j}\right)$ be such that $\forall S_{k}: S_{k} \cap\left{v_{i}, v_{j}\right}=\left{v_{i}\right} \Rightarrow v_{i} \in S_{k}, v_{j} \notin S_{k}$. We choose our graph $G_{i j}$ to have two components $\left{v_{i}, v_{j}\right}$ and $V \backslash\left{v_{i}, v_{j}\right}$; and include the edge $v_{j} \rightarrow v_{i}$ in it. Our algorithm will conclude from CI-test $v_{i} \Perp v_{j} \mid \operatorname{do}\left(S_{k}\right)$ that $v_{i}$ and $v_{j}$ are independent. However, it is possible that $v_{i} \Perp v_{j}$ because of a latent $l_{i j}$ between $v_{i}$ and $v_{j}$, but $v_{i} \Perp v_{j} \mid \operatorname{do}\left(S_{k}\right)$ as intervening on $v_{i}$ disconnects the $l_{i j} \rightarrow v_{i}$ edge. Therefore, our algorithm cannot distinguish the two cases $v_{j} \rightarrow v_{i}$ and $v_{i} \leftarrow l_{i j} \rightarrow v_{j}$ without intervening on $v_{j}$. For every $\mathcal{S}$ that is not a strongly separating set system, we can provide a $G_{i j}$ such that by intervening on sets in $\mathcal{S}$, we cannot recover $\operatorname{Anc}\left(G_{i j}\right)$ correctly.</p>
<h2>A. 1 Missing Details from Section 3.1</h2>
<p>We first argue that the matrix returned by Algorithm SSMATRIX is indeed a strongly separating matrix.</p>
<p>Lemma A.2. The matrix $U$ returned by Algorithm SSMATRIX is a strongly separating matrix.
Proof. Consider any two nodes $v_{i}, v_{j}$ with corresponding row vectors $U(i)$ and $U(j)$. Suppose $|U(i)|<em 1="1">{1}=|U(j)|</em>$.</p>
<p>By construction, $U(i) \neq U(j)$ they will differ in at least one coordinate. However, they have equal weights, so, there must exist one more coordinate such that the strongly separating condition holds. If $U(i)$ and $U(j)$ have weights $r^{i} \neq r^{j}$, then, $U\left(i, m^{\prime}-\log n+r^{i}\right)=U\left(j, m^{\prime}-\log n+r^{j}\right)=1$ and $U\left(i, m^{\prime}-\log n+r^{j}\right)=U\left(j, m^{\prime}-\log n+r^{i}\right)=0$ by construction outlined in the Algorithm SSMATRIX. This proves that the matrix $U$ returned is a strongly separating matrix.</p>
<p>The following inequalities about Algorithm SSMATRIX will be useful in analyzing its performance.</p>
<p>Lemma A.3. . For $m \geq 66 \log n$ and $m^{\prime}$ as defined in Algorithm SSMATRIX, we have the following
(a) $\sum_{t=1}^{\log n}\binom{m^{\prime}-\log n}{t} \geq n$ (i.e., there are enough vectors of weight $\leq \log n$ only using $m^{\prime}-\log n$ columns to assign a unique vector to each variable).
(b) Let $i^{<em>}$ be the smallest integer s.t. $\sum_{t=1}^{i^{</em>}}\binom{m^{\prime}}{t} \geq n$. Then, $\sum_{t=1}^{2 i-1}\binom{m^{\prime}-\log n}{t} \geq \sum_{t=1}^{i}\binom{m^{\prime}}{t}$ for all $i \in\left{2, \ldots, i^{*}\right}$.</p>
<p>Proof. Let $m \geq 66 \log n$. From Algorithm SSMATRIX, we have $m^{\prime}=m-a_{1}$ for all guesses $1 \leq$ $a_{1} \leq \frac{2 m}{3}$. By reserving the last " $\log n$ " columns in Algorithm SSMATRIX, we want to make sure that $m^{\prime}-\log n$ can fully cover $n$ nodes with weight at most $\log n$. We have :</p>
<p>$$
\begin{aligned}
m^{\prime}=m-a_{1} \geq \frac{m}{3} &amp; \geq 22 \log n \text { and } \
\sum_{t=1}^{\log n}\binom{m^{\prime}-\log n}{t} \geq\binom{ m^{\prime}-\log n}{\log n} &amp; \geq\left(\frac{21 \log n}{\log n}\right)^{\log n}&gt;n
\end{aligned}
$$</p>
<p>Moving onto Part (b). Let $i^{<em>}$ be the minimum value of $i$ such that $\sum_{t=1}^{i^{</em>}}\binom{m^{\prime}}{i} \geq n$. Consider $i$ such that $2 \leq i \leq i^{*}$ :</p>
<p>$$
\sum_{t=1}^{2 i-1}\binom{m^{\prime}-\log n}{t} \geq\binom{ m^{\prime}-\log n}{2 i-1}
$$</p>
<p>Consider now the right hand side:</p>
<p>$$
\sum_{t=1}^{i}\binom{m^{\prime}}{t} \leq \sum_{t=0}^{i}\binom{m^{\prime}}{t} \leq \sum_{t=0}^{i} \frac{m^{\prime t}}{t!} \leq \sum_{t=0}^{i} \frac{i^{t}}{t!}\left(\frac{m^{\prime}}{i}\right)^{t} \leq \mathrm{e}^{i}\left(\frac{m^{\prime}}{i}\right)^{i}
$$</p>
<p>We inductively show that for all $i \geq 2$</p>
<p>$$
\frac{\left(\frac{e m^{\prime}}{i}\right)^{i}}{\binom{m^{\prime}-\log n}{2 i-1}} \leq 1
$$</p>
<p>Let $i=2$. For $m \geq \frac{m^{\prime}}{3} \geq 50\left(\frac{22}{21}\right)^{3}$ we have</p>
<p>$$
\frac{\left(e m^{\prime} / 2\right)^{2}}{\binom{m^{\prime}-\log n}{3}} \leq \frac{\left(e m^{\prime} / 2\right)^{2}}{\left(\frac{m^{\prime}-\log n}{3}\right)^{3}} \leq 50\left(\frac{22}{21}\right)^{3} \frac{m^{\prime 2}}{m^{\prime 3}} \leq 1
$$</p>
<p>Assume the inequality is correct for some $i&gt;2$. Now, we show that it must also hold for $i+1$.</p>
<p>$$
\frac{\left(\frac{e m^{\prime}}{i+1}\right)^{i+1}}{\binom{m^{\prime}-\log n}{2 i+1}}=\frac{\left(\frac{e m^{\prime}}{i+1}\right)^{i} \frac{e m^{\prime}}{i+1}\binom{m^{\prime}-\log n}{2 i-1}}{\binom{m^{\prime}-\log n}{2 i-1}\binom{m^{\prime}-\log n}{2 i+1}} \leq \frac{\left(\frac{e m^{\prime}}{i}\right)^{i} \frac{e m^{\prime}}{i+1}\binom{m^{\prime}-\log n}{2 i-1}}{\binom{m^{\prime}-\log n}{2 i-1}\binom{m^{\prime}-\log n}{2 i+1}} \leq \frac{\frac{e m^{\prime}}{i+1}\binom{m^{\prime}-\log n}{2 i-1}}{\binom{m^{\prime}-\log n}{2 i+1}}
$$</p>
<p>For ease of notation, denote $a=m^{\prime}-\log n \geq m^{\prime}\left(1-\frac{1}{22}\right) \geq 21 \log n$.
Consider the binary entropy function $H(x)=-x \log x-(1-x) \log (1-x)$. For $x \in\left[\frac{2 i-1}{a}, \frac{2 i+1}{a}\right]$, $H(x)$ is an increasing function. For some value of $x$ in the range we have :</p>
<p>$$
\begin{gathered}
\frac{H\left(\frac{2 i+1}{a}\right)-H\left(\frac{2 i-1}{a}\right)}{\frac{2 i+1}{a}-\frac{2 i-1}{a}}=H^{\prime}(x)=\log \left(\frac{1}{x}-1\right) \geq \log \left(\frac{a}{2 i-1}-1\right) \
\Longrightarrow H\left(\frac{2 i+1}{a}\right)-H\left(\frac{2 i-1}{a}\right) \geq \frac{2}{a} \log \left(\frac{a}{2 i-1}-1\right)
\end{gathered}
$$</p>
<p>Now, consider the fraction</p>
<p>$$
\binom{a}{2 i-1} /\binom{a}{2 i+1}
$$</p>
<p>Using the bound from (MacWilliams and Sloane [1977], Page 309)</p>
<p>$$
\begin{aligned}
&amp; \sqrt{\frac{a}{8 b(a-b)}} 2^{m H(b / a)} \leq\binom{a}{b} \leq \sqrt{\frac{a}{2 \pi b(a-b)}} 2^{m H(b / a)}, \
&amp; \binom{a}{2 i-1} /\binom{a}{2 i+1} &amp; \leq \sqrt{8(2 i+1)(a-2 i-1) / 2 \pi(2 i-1)(a-2 i+1)} / 2^{a H\left(\frac{2 i+1}{a}\right)-H\left(\frac{2 i-1}{a}\right)} \
&amp; \leq \sqrt{20 / 3 \pi} / 2^{a H\left(\frac{2 i+1}{a}\right)-H\left(\frac{2 i-1}{a}\right)} \
&amp; \leq \sqrt{20 / 3 \pi} / 2^{2 \log \left(\frac{a}{2 i-1}-1\right)} \
&amp; =\frac{\sqrt{20 / 3 \pi}}{\left(\frac{a}{2 i-1}-1\right)^{2}}
\end{aligned}
$$</p>
<p>Combining the above, we have :</p>
<p>$$
\begin{aligned}
\frac{\left(\frac{e m^{\prime}}{i+1}\right)^{i+1}}{\binom{m^{\prime}-\log n}{2 i+1}} &amp; \leq \frac{\frac{m^{\prime}}{i+1} \sqrt{20 e^{2} / 3 \pi}}{\left(\frac{a}{2 i-1}-1\right)^{2}} \
&amp; \leq \frac{4 m^{\prime} i \sqrt{20 e^{2} / 3 \pi}}{(a-2 i)^{2}} \
&amp; \leq \frac{4 m^{\prime} \log n \sqrt{20 e^{2} / 3 \pi}}{m^{\prime 2}(1-3 / 22)^{2}}=\frac{4 \log n \sqrt{20 e^{2} / 3 \pi}}{m^{\prime}(1-3 / 22)^{2}} \leq \frac{21.2 \log n}{m^{\prime}} \leq 1
\end{aligned}
$$</p>
<p>Therefore, we have for all $i \geq 2$</p>
<p>$$
\begin{gathered}
\left(\frac{e m^{\prime}}{i}\right)^{i} /\binom{m^{\prime}-\log n}{2 i-1} \leq 1 \
\Longrightarrow \sum_{t=1}^{i}\binom{m^{\prime}}{t} \leq\left(\frac{e m^{\prime}}{i}\right)^{i} \leq\binom{ m^{\prime}-\log n}{2 i-1} \leq \sum_{t=1}^{2 i-1}\binom{m^{\prime}-\log n}{t}
\end{gathered}
$$</p>
<p>Let $c_{U}=\sum_{j=1}^{n} c\left(v_{j}\right)|U(j)|<em _mathrm_OPT="\mathrm{OPT">{1}$ be value of objective for the matrix $U$ returned by Algorithm SSMATRIX.
Consider $U</em>$.}}$, and let $V_{\mathrm{OPT}}^{(1)}$ represent all nodes that are assigned weight 1 in it (nodes which have only one 1 in their row). Let $c_{\mathrm{OPT}}^{(1)}$ denote the sum of cost of the nodes in $V_{\mathrm{OPT}}^{(1)}$. In our Algorithm SSMATRIX, we maintain a guess for the size of $V_{\mathrm{OPT}}^{(1)}$ as $a_{1}$. We want to guess the exact value of $\left|V_{O P T}^{(1)}\right| \leq m$. However, we only guess $a_{1}$ until $\frac{2 m}{3}$, so that the remaining columns can be used to obtain a valid separating matrix (for each of our guesses) as observed in Lemma A.3. We show that the cost contribution of nodes in $V_{\mathrm{OPT}}^{(1)}$ (by allowing this slack in our guesses) due to Algorithm SSMATRIXis not far away from $c_{\mathrm{OPT}}^{(1)</p>
<p>First, we show that for any weight $i \geq 2$ node in $U_{\mathrm{OPT}}$, the output $U$ of Algorithm SSMATRIX assigns vectors with weight at most $2 i$ and for a weight 1 node, we show that the weight assigned by $U$ is at most 3 .</p>
<p>Lemma A.4. Algorithm SSMATRIX assigns a weight of</p>
<p>(a) at most 3 for a weight 1 node in $U_{\mathrm{OPT}}$.
(b) at most $2 i$ for a node of weight $i$ in $U_{\mathrm{OPT}}$ for $i \geq 2$.</p>
<p>Proof. (a) Let $V$ denote sorted (in the decreasing order of cost) order of nodes. Suppose we assign unique length- $m$ vectors starting from weight 1 to the nodes in the order $V$. Let the assignment of vectors be denoted by $\widetilde{U}$. It is easy to observe that this described assignment $\widetilde{U}$ is not a strongly separating matrix. However, any strongly separating matrix $U$ is such that the vector assigned to any node $v_{i}$ in $U$ has weight at least that in $\widetilde{U}$ i.e., $|U(i)|<em 1="1">{1} \geq|\widetilde{U}(i)|</em>(i)\right|}$. As $U$ can be any strongly separating matrix, it also holds for $U_{\mathrm{OPT}}$ giving us $\left|U_{\mathrm{OPT}<em 1="1">{1} \geq|\widetilde{U}(i)|</em>$.</p>
<p>The number of weight 1 nodes possible in the assignment $\widetilde{U}$ is $\binom{m}{1}$ and therefore, $\left|V_{\mathrm{OPT}}^{(1)}\right| \leq m$. Consider all the nodes of weight $\leq 3$ in $U$ assigned by Algorithm SSMATRIX. After discarding the first $m^{\prime}=m-a_{1}$ columns assuming our guess $a_{1}$ in the current iteration, $U$ starts assigning vectors with weight 1 in the remaining $m^{\prime}-\log n$ while setting a 'row weight indicator bit' in the last $\log n$ columns. In order to obtain nodes of weight $\leq 3$, in $U$, we include vectors of weight $\leq 2$ in the $m^{\prime}-\log n$ columns. Therefore, total number of such nodes is $a_{1}+\binom{m^{\prime}-\log n}{1}+\binom{m^{\prime}-\log n}{2}$.</p>
<p>$$
\begin{aligned}
a_{1}+\binom{m^{\prime}-\log n}{1}+\binom{m^{\prime}-\log n}{2} &amp; \geq\binom{m^{\prime}-\log n}{1}+\binom{m^{\prime}-\log n}{2} \
&amp; \geq m^{\prime}-\log n+\left(\frac{m^{\prime}-\log n}{2}\right)^{2} \quad\left(\text { using }\binom{m^{\prime}}{k} \geq\left(\frac{m^{\prime}}{k}\right)^{k}\right) \
&amp; \geq m \geq\left|V_{\mathrm{OPT}}^{(1)}\right| . \quad \text { (using } m^{\prime} \geq \frac{m}{3} \text { and } m \geq 66 \log n \text { ) }
\end{aligned}
$$</p>
<p>Therefore, every weight 1 node in $U_{\mathrm{OPT}}$ is covered by a vector in $U$ with weight $\leq 3$.
(b) First we argue that using an appropriate $m^{\prime}$, we can give a construction of $\widetilde{U} \in{0,1}^{n \times m^{\prime}}$ (similar to case (a)) such that weight of node $v_{j}$ in $\widetilde{U}$ is at most the weight in $U_{\mathrm{OPT}}$ for all nodes of weight more than 2 in $U_{\mathrm{OPT}}$. Let $m^{\prime}=m-\frac{2 m}{3}$. In other words, we are considering the guess $a_{1}=\frac{2 m}{3}$. As our algorithm $U$ considers all the guesses and returns $U$ with the lowest cost, arguing that our lemma holds for this guess is sufficient. For this value of $m^{\prime}$, let $\widetilde{U}$ be constructed using vectors from ${0,1}^{m^{\prime}}$ in the increasing order of weight, starting with weight 1 .</p>
<p>When $m^{\prime}=\frac{m}{3}$, it is possible that a node in $U_{\mathrm{OPT}}$ can be assigned a vector of weight 1 from ${0,1}^{m^{\prime}}$ (this can happen when $\left|V_{\mathrm{OPT}}^{(1)}\right| \geq \frac{2 m}{3}$ ). As $\widetilde{U}$ assigns weights in the increasing order occupying the entire $m^{\prime}$ columns, it will not result in a strongly separating matrix. Therefore, any node $v_{j}$ with weight $i \geq 2$ in $U_{\mathrm{OPT}}$, will be assigned a weight of at most $i$ in $\widetilde{U}$.</p>
<p>We know that the number of vectors of weight at most $i$ in $\widetilde{U}$ is equal to $\sum_{t=1}^{i}\binom{m^{\prime}}{t}$ and number of vectors with weight at most $2 i-1$ using $m^{\prime}-\log n$ columns of $U$ is equal to $\sum_{t=1}^{2 i-1}\binom{m^{\prime}-\log n}{t}$. As Lemma A. 3 holds for all guesses of $a_{1}$, we have $\sum_{t=1}^{i}\binom{m^{\prime}}{t} \leq \sum_{t=1}^{2 i-1}\binom{m^{\prime}-\log n}{t}$ for all $i \geq 2$. Using induction, we can observe that $v_{j}$ is assigned a vector in ${0,1}^{m^{\prime}-\log n}$ with weight at most $2 i-1$. As $U$ obtained from Algorithm SSMATRIX mimics the construction used in $\widetilde{U}$ over $m^{\prime}-\log n$ columns, we have that weight of node $v_{j}$ in $U$ using $m^{\prime}-\log n$ columns is at most $2 i-1$. Combining it with the 'row weight indicator' bit we set to 1 in the last $\log n$ columns gives us the lemma.</p>
<p>In our next lemma shows that the sum of contribution of the nodes in $V_{\mathrm{OPT}}^{(1)}$ to $c_{U}$ is at most twice that of $c_{\mathrm{OPT}}^{(1)}$. Combining this with Lemma A.4, we show that $U$ achieves a 2-approximation.</p>
<p>Lemma A.5. Let $c_{U}^{(1)}=\sum_{v_{i} \in V_{\mathrm{OPT}}^{(1)}} c\left(v_{i}\right)|U(i)|<em U="U">{1}$ for the matrix $U$ returned by Algorithm SSMATRIX, then $c</em>$.}^{(1)} \leq 2 c_{O P T}^{(1)</p>
<p>Proof. Suppose $a_{1}$ represents our guess for the number of weight 1 vectors and $a_{1}^{<em>}$ represent the number of weight 1 vectors in $U_{\mathrm{OPT}}$ i.e, $\left|V_{\mathrm{OPT}}^{(1)}\right|=a_{1}^{</em>}$. In Algorithm SSMATRIX, we use the following bounds for our guess $0 \leq a_{1} \leq 2 m / 3$. If $a_{1}^{*} \leq \frac{2 m}{3}$, then it would have been one of our guesses. As we take minimum among all the guesses, we have $c_{U}^{(1)}=c_{O P T}^{(1)}$ in such a case.</p>
<p>Consider the case when $a_{1}^{<em>}&gt;\frac{2 m}{3}$. Let $V_{\mathrm{OPT}}^{(1)}=\left{v_{1}, v_{2}, \cdots v_{a_{1}^{</em>}}\right}$ represent an ordering of nodes in the decreasing ordering of cost that are assigned weight 1 in $U_{\mathrm{OPT}}$. Consider the contribution of only weight 1 nodes to $c_{\mathrm{OPT}}$. We have</p>
<p>$$
c_{\mathrm{OPT}}^{(1)}=\sum_{k=1}^{a_{1}^{*}} c\left(v_{k}\right) \geq \sum_{k=1}^{2 m / 3} \frac{2 m}{3} c\left(v_{k}\right) \geq \frac{2 m}{3} c\left(v_{2 m / 3}\right)
$$</p>
<p>We will look at the case when our guess $a_{1}$ reaches $a_{1}=\frac{2 m}{3}$ and argue about the cost for this particular value of $a_{1}$. As we are taking minimum over all the guesses, we are only going to do better and our approximation ratio will only be better. Among the nodes $\left{v_{1}, v_{2}, \cdots v_{a_{1}^{<em>}}\right}$ first $\frac{2 m}{3}$ nodes would be assigned weight 1 by $U$. From Lemma A.4, we have that for the remaining $a_{1}^{</em>}-\frac{2 m}{3}$ nodes, Algorithm SSMATRIX might assign a weight 2 or weight 3 vector in $U$.</p>
<p>$$
\begin{aligned}
c_{U}^{(1)} &amp; \leq \sum_{i=1}^{2 m / 3} c\left(v_{i}\right)+3 \sum_{j=2 m / 3+1}^{a_{1}^{<em>}} c\left(v_{j}\right)=\sum_{i=1}^{a_{1}^{</em>}} c\left(v_{i}\right)+2 \sum_{j=2 m / 3+1}^{a_{1}^{<em>}} c\left(v_{j}\right) \
&amp; \leq \sum_{i=1}^{a_{1}^{</em>}} c\left(v_{i}\right)+2\left(a_{1}^{<em>}-\frac{2 m}{3}\right) c\left(v_{2 m / 3+1}\right) \
&amp; \leq c_{O P T}^{(1)}+2\left(a_{1}^{</em>}-\frac{2 m}{3}\right) c\left(v_{2 m / 3}\right) \
&amp; \leq c_{O P T}^{(1)}+\frac{2 m}{3} c\left(v_{2 m / 3}\right) \
&amp; \leq 2 c_{O P T}^{(1)}
\end{aligned}
$$</p>
<p>(since $c\left(v_{2 m / 3}\right) \geq c\left(v_{2 m / 3+1}\right)$ )
(since $a_{1}^{*} \leq m$ )
This completes the proof of the lemma.
Theorem A. 6 (Theorem 3.3 Restated). Let $m \geq 66 \log n$ and $U$ be the strongly separating matrix returned by Algorithm SSMATRIX. Let $c_{U}=\sum_{j=1}^{n} c\left(v_{j}\right)|U(j)|_{1}$. Then,</p>
<p>$$
c_{U} \leq 2 \cdot c_{\mathrm{OPT}}
$$</p>
<p>where $c_{\mathrm{OPT}}$ is the objective value associated with optimum set of interventions corresponding to $U_{\mathrm{OPT}}$.</p>
<p>Proof. From Lemma A.2, we know that matrix returned by Algorithm SSMATRIX given by $U$ with $\operatorname{cost} c_{U}$ is a strongly separating matrix. Consider a strongly separating matrix $U_{\mathrm{OPT}}$ that achieves optimum objective value $c_{\mathrm{OPT}}$. Let $V_{\mathrm{OPT}}^{(1)}$ represent all nodes that are assigned weight 1 in $U_{\mathrm{OPT}}$. Let $c_{U}^{(1)}$ denote the cost of nodes in $V_{\mathrm{OPT}}^{(1)}$ using $U$ returned by Algorithm SSMATRIX and $c_{\mathrm{OPT}}^{(1)}$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ In our proof, $\gamma=66$ but this can likely be decreased.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4386 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4386</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4386</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-277781479</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.10284v3.pdf" target="_blank">Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol</a></p>
                <p><strong>Paper Abstract:</strong> Literature review tables are essential for summarizing and comparing collections of scientific papers. We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers. Building on recent work (Newman et al., 2024), we extend prior approaches to address real-world complexities through a combination of LLM-based methods and human annotations. Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers). To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios. Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements. Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4386.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4386.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IterativeBatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Batch-based Tabular Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step LLM-driven pipeline that (A) extracts concise key-information paragraphs from each paper's full text, (B) partitions those summaries into small batches, and (C) iteratively performs paper selection and schema refinement across batches to produce a literature-review table; the process revisits papers multiple times and finally verifies cell values against full text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Iterative Batch-based Tabular Generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three-stage approach: (A) Key information extraction — an LLM is prompted with each paper's title, abstract, and full text plus the user demand to produce a concise paragraph preserving potentially relevant details; (B) Paper batching — summaries are split into small batches (batch size=4 in experiments) to keep prompts manageable; (C) Paper selection & schema refinement — starting from an empty schema/table, the LLM sequentially processes each batch to decide inclusion/exclusion of papers, refine/add/remove/modify columns, and insert new rows for newly included papers; steps B and C are iterated k times (k=5 in experiments) with randomized batching between iterations; after iterations, the LLM revisits selected papers' full texts to verify cell values.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Multiple backbone LLMs in experiments: GPT-4o, GPT-4o-mini, LLAMA-3.3-70B, Mistral-Large-123B, DeepSeek-V3-685B (the method itself is LLM-agnostic; LLaMA-3.3 used in some computational-cost runs)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based summarization/key-information extraction from full text (prompted paragraph extraction) combined with embedding-based pre-filtering for candidate retrieval (Sentence-BERT used in retrieval simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative multi-batch schema induction and refinement with repeated comparisons across different paper subsets; incremental table construction and merging of schema edits across iterations, followed by full-text verification of selected rows</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the ARXIV2TABLE benchmark (1,957 tables, 7,158 papers total); per-table paper counts vary (dataset contains small to moderate per-table sizes; batching used with size=4 and distractor candidates up to 10 per table during simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature (benchmark derived from computer-science literature review tables on arXiv)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured literature-review tables (schema + cell values) summarizing multiple papers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Paper selection recall; schema precision/recall/F1; unary (single-cell) precision/recall/F1; pairwise-cell precision/recall/F1; evaluation via LLM-synthesized binary QA pairs (yes/no) produced from ground-truth and answered against generated tables to compute recall/precision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported consistent improvement over baselines across backbone models (higher paper-selection recall and higher precision/recall/F1 for schema and unary/pairwise metrics). Computational-cost numbers (LLaMA-3.3 backend): 100% generation success rate, ~118K average tokens per table, ~194s average runtime per table (Table 6). Exact per-model metric tables are in the paper (e.g., improved average F1 and higher recall across models).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Baseline 1 (one-step generation), Baseline 2 (per-document table generation + merge), and Newman et al. (two-stage: schema from titles/abstracts then value fill from full text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Across all backbone models and evaluation criteria, the iterative batch method consistently outperforms the listed baselines (higher paper recall and F1 on unary and pairwise metrics); specifics are reported in Table 2 of the paper (method produces the best scores within each backbone in the experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative batching plus repeated schema refinement improves selection of relevant papers and alignment of schema/values versus single-pass or per-document methods; extracting key-information paragraphs (not relying solely on abstracts) reduces context-window issues and preserves full-text details; multiple randomized iterations (k up to 5) steadily improve metrics before potential overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Still challenged by distractor papers — selection recall is imperfect (many distractors remain included); pairwise relationship preservation remains substantially harder than unary value extraction; iterative overfitting can reduce precision after many iterations; computational cost is higher than simplest baselines though within acceptable latency.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Experiments show gains with larger LLMs (performance improves when scaling from 70B to 123B in open-source models), and iterative refinement (increasing k up to ~4 improves performance, k=5 was chosen as optimal in paper due to diminishing returns/overfitting thereafter). Number of input papers is handled via batching (batch size 4) and iterative revisits—no claim of unlimited scaling, but method controls context-window costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4386.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4386.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARXIV2TABLE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ARXIV2TABLE benchmark and pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A realistic benchmark and task formulation for LLM-based literature-review table generation that (1) replaces concise table captions with abstract user-demand prompts, (2) injects semantically similar distractor papers into candidate pools, and (3) evaluates generated tables by LLM-synthesized QA utilization (schema, unary, pairwise).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ARXIV2TABLE (benchmark + evaluation pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Construction pipeline: rewrite original table captions into abstract user-demand intents using GPT-4o; simulate retrieval by selecting distractor candidates via Sentence-BERT ranking (combining similarity to user demand and to referenced papers) and human-verified exclusion labels; evaluation pipeline uses LLM (GPT-4o) to synthesize binary QA pairs from ground-truth tables and to answer them against generated tables to produce recall/precision metrics across schema, unary, and pairwise dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o used for user-demand generation and QA-synthesis in benchmark curation and evaluation; experiments use multiple backbone LLMs (GPT-4o, GPT-4o-mini, LLAMA-3.3-70B, Mistral-123B, DeepSeek-V3-685B) to evaluate generation methods on this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Candidate retrieval simulated with Sentence-BERT embeddings and similarity scoring; LLMs are used both to filter/select relevant papers and to extract key information from full text via prompted summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based schema induction and value generation (single-shot, per-document, and iterative-batch strategies explored); evaluation uses LLM-synthesized QA pairs to measure usable information overlap rather than only embedding similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Benchmark contains 1,957 tables and 7,158 referenced papers total; per-table numbers vary (distractor candidates: top-10 per table, final distractor counts randomly sampled between [m,10]).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature (arXiv-sourced literature review tables)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured literature-review tables (schema + cell values) used as outputs for generation systems and as ground-truth for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LLM-synthesized QA-based utilization metrics: recall for ground-truth content retained (schema, unary, pairwise); precision by reversing QA generation/answering; classic metrics reported as precision/recall/F1 for schema, unary, and pairwise value overlap; also paper-selection recall.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Benchmarking shows that both proprietary and open-weight LLMs struggle on the more realistic task (distractors + abstract user demands); reported example numbers: many models produce paper-selection recall in the ~60–75% range, and the proposed method improves these numbers; expert validation shows >98% acceptance rate for synthesized QA pairs and >97% agreement between humans and LLM answers in sampled evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Provides stronger realism vs prior benchmarks (e.g., ARXIVDIGESTABLES/Newman et al. 2024) by adding distractors and using abstract user demands; used to compare one-step, per-document, Newman et al. two-stage, and the iterative batch method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>When evaluated on ARXIV2TABLE, the iterative batch method and other approaches achieved different trade-offs; overall the paper reports their method yields higher recall/F1 than baselines across backbones (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Realistic evaluation (distractors + under-specified intents) exposes limitations of LLMs in literature-table generation; LLM-synthesized QA pairs provide a scalable, high-agreement proxy for human evaluation; schema generation from abstracts can miss full-text details so full-text-based extraction is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Use of GPT-4o for benchmark curation and QA synthesis introduces potential data-contamination and model-bias concerns; human annotation required to verify distractors is expensive; benchmark derived from computer-science tables so broader-domain generality needs verification.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Benchmark construction and evaluation can be resource-intensive when relying on proprietary LLMs; evaluation pipeline scales to many tables but curation at large scale may be limited by reliance on expensive LLM calls (paper suggests open-source replication as future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4386.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4386.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArXivDigestables (Newman)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ArXIVDIGESTABLES / Arxivdigestables: Synthesizing scientific literature into tables using language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LLM-based pipeline that streamlines schema and value generation for literature-review tables via a sequential two-stage process: selecting relevant papers and generating schema (titles/abstracts), then filling table values by looping through selected papers' full texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Arxivdigestables: Synthesizing scientific literature into tables using language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Newman et al. two-stage method (ArXIVDIGESTABLES)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage LLM pipeline: stage 1 — select relevant papers from candidates (titles/abstracts) and induce a schema; stage 2 — iterate through selected papers' full texts to fill table rows according to the induced schema. Schema induction uses titles and abstracts which can miss full-text-only aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in detail in this paper; in the present work Newman et al.'s method is implemented/evaluated across the same backbone LLMs used in experiments (e.g., LLaMA-3.3, Mistral, DeepSeek, GPT-4o variants) as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Schema generated from titles/abstracts; values filled from full text via LLM prompting (document-grounded generation)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Sequential pipeline: schema induction followed by per-document value extraction under the induced schema</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied per-table on the ARXIV2TABLE candidate sets (per-table sizes vary); used as a baseline across the dataset of 1,957 tables</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature (arXiv tables)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature-review tables (schema + filled values)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same ARXIV2TABLE QA-based evaluation: paper selection recall; schema/unary/pairwise precision/recall/F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported as a competitive baseline but generally outperformed by the proposed iterative-batch method across backbone LLMs (detailed numbers in Table 2 of the paper). Example: Newman et al. two-stage shows higher schema recall than some one-step baselines but lower overall F1 vs the iterative-batch approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against one-step and per-document baselines and the new iterative batch method in experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Newman et al. method generally exceeded naive one-step baselines on some schema metrics but was outperformed by the iterative-batch approach overall, especially on unary/pairwise F1, per the paper's Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Two-stage design reduces prompt-length issues and separates schema induction from value extraction, but schema induction based only on titles/abstracts can miss full-text details and reduce alignment with ground-truth tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Schema generation from abstracts may overlook full-text aspects; susceptible to distractors if retrieval is noisy; can produce mismatched granularity across columns.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Addresses context-window issues by splitting schema induction from full-text filling; still limited by quality of initial retrieval and by model context constraints during the filling stage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4386.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4386.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline1_OneStep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline 1 — One-step Table Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A straightforward approach that prompts an LLM with all available candidate papers R and the user demand p and asks it in a single pass to select relevant papers and output a complete table with schema and filled values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Baseline 1 (one-step generation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Single-round prompting: provide all candidate documents and the user demand to the LLM and request selection plus full table induction (schema+values) in one conversation. This approach can exceed LLM context windows when tables/paper texts are large.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied across the same backbone LLMs in the paper's experiments (e.g., GPT-4o, GPT-4o-mini, LLAMA-3.3, Mistral-123B, DeepSeek-V3) as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Direct prompting over concatenated inputs (full text/abstracts) to extract values and choose relevant papers</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single-step synthesis: simultaneous selection and table generation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Varies per-table; when many papers included this baseline often fails due to context-window limits (experiments vary retrieval sizes between 2 and 100 in pilot studies)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature (ARXIV2TABLE)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured tables (schema + values)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same ARXIV2TABLE metrics (paper recall; schema/unary/pairwise P/R/F1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Per-paper experiments show baseline 1 suffers from prompt-length/context issues and generally underperforms compared to iterative methods (specific scores in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly against Baseline 2, Newman et al., and IterativeBatch in the paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Performs worse on large inputs due to context-window failures; lower average F1 and higher failure rate for large tables compared to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple and conceptually direct but impractical for larger tables because of context-length constraints; leads to incomplete generations when prompts exceed model windows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Excessively long prompts/contexts cause generation failures or degraded selection/induction quality; not robust to distractor-heavy candidate pools.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Does not scale well with number of papers or full-text size; success rate drops as input size grows (paper reports failures of Baseline 1 in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4386.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4386.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline2_PerDoc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline 2 — Per-document Processing + Merge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that processes each document individually with an LLM to decide inclusion and generate a per-document row, then merges per-document schemas via exact string matching to form the final table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Baseline 2 (per-document processing and merge)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For each candidate document, the LLM is asked whether it should be included given the user demand; if included, the model generates a table (row) for that document. Final table is produced by merging schemas of all individual rows using exact string matching, and copying values into the merged schema, which often yields a sparse table.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Evaluated across backbone LLMs in the experiments (GPT-4o, GPT-4o-mini, LLAMA-3.3, Mistral-Large, DeepSeek-V3)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Per-document LLM prompting for inclusion decision and per-document value extraction</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Row-wise generation followed by schema merging via exact string matching</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates at the smaller T2 scale (paper selection stage); number depends on candidate set C per table (varies across ARXIV2TABLE entries)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Merged literature-review table (often sparse due to inconsistent per-document schemas)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ARXIV2TABLE QA-based metrics (paper recall; schema/unary/pairwise P/R/F1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Tends to achieve higher schema recall because it produces many columns (greater chance of overlap), but results in sparser tables and potential omissions, giving mixed F1 performance relative to other baselines (detailed numbers in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Baseline 1, Newman et al., and IterativeBatch in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Often better schema recall than one-step methods but worse overall utility due to sparsity and inconsistent granularity, and outperformed by the iterative-batch approach on averaged metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Per-document processing reduces context-window issues but produces inconsistent schemas across rows requiring merging, leading to sparsity and potential information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Schema inconsistency and sparsity across rows; per-document prompts may not expose broader cross-paper comparative aspects, reducing comparative utility.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Handles larger numbers of documents better than one-step approach because each document is small, but merging many per-document schemas can become unwieldy and sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4386.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4386.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScidaSynth</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scidasynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that frames interactive structured knowledge extraction and synthesis from multiple scientific articles using large language models (title appears in bibliography); mentioned as related work on LLM-based scientific synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scidasynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scidasynth</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Not described in detail in this paper; referenced as prior work addressing interactive structured extraction and synthesis from scientific literature using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature (general)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured knowledge artifacts (implicit from title: extraction + synthesis output)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4386.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4386.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-Tuple-Table</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-tuple-table: Towards information integration in text-to-table generation via global tuple extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced method that targets information integration for text-to-table generation by extracting global tuples, cited as related work on tabular induction from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-tuple-table: Towards information integration in text-to-table generation via global tuple extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Text-tuple-table</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a text-to-table approach that uses global tuple extraction to integrate information when generating tables from text; the present paper notes such methods are related but not directly applicable under differing assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>global tuple extraction (per reference title/description)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>information integration into tabular form</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Text-to-table generation (general NLP tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured tables (from text)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4386.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4386.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-to-Table</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-to-table: A new way of information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier line of work on inducing tables from text (cited) that frames table induction as a general text-to-table generation problem; referenced in related work as part of broader literature on tabular induction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-to-table: A new way of information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Text-to-table</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced prior work positioning table induction as sequence-to-sequence text-to-table generation and information extraction; not used or analyzed experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>sequence-to-sequence / text-to-table extraction (per reference)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General text-to-table information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Tables (from textual documents)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4386.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4386.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IntentAwareSchema</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Setting the table with intent: Intent-aware schema generation and editing for literature review tables</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited (to-appear) work that focuses on intent-aware schema generation/editing for literature review tables, noted as related to user-intent-driven schema induction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Setting the table with intent: Intent-aware schema generation and editing for literature review tables</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Intent-aware schema generation (Padmakumar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced approach that targets schema generation/editing conditioned on user intent to improve literature-review table construction; cited as complementary prior work emphasizing intent in schema induction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>intent-conditioned schema induction (per citation)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>schema editing and generation guided by user intent</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature review table generation (computer science / general academic domains)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Schemas and edited table structures</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Arxivdigestables: Synthesizing scientific literature into tables using language models <em>(Rating: 2)</em></li>
                <li>Scidasynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model <em>(Rating: 2)</em></li>
                <li>Text-tuple-table: Towards information integration in text-to-table generation via global tuple extraction <em>(Rating: 2)</em></li>
                <li>Text-to-table: A new way of information extraction <em>(Rating: 1)</em></li>
                <li>Setting the table with intent: Intent-aware schema generation and editing for literature review tables <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4386",
    "paper_id": "paper-277781479",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "IterativeBatch",
            "name_full": "Iterative Batch-based Tabular Generation",
            "brief_description": "A multi-step LLM-driven pipeline that (A) extracts concise key-information paragraphs from each paper's full text, (B) partitions those summaries into small batches, and (C) iteratively performs paper selection and schema refinement across batches to produce a literature-review table; the process revisits papers multiple times and finally verifies cell values against full text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Iterative Batch-based Tabular Generation",
            "system_description": "Three-stage approach: (A) Key information extraction — an LLM is prompted with each paper's title, abstract, and full text plus the user demand to produce a concise paragraph preserving potentially relevant details; (B) Paper batching — summaries are split into small batches (batch size=4 in experiments) to keep prompts manageable; (C) Paper selection & schema refinement — starting from an empty schema/table, the LLM sequentially processes each batch to decide inclusion/exclusion of papers, refine/add/remove/modify columns, and insert new rows for newly included papers; steps B and C are iterated k times (k=5 in experiments) with randomized batching between iterations; after iterations, the LLM revisits selected papers' full texts to verify cell values.",
            "llm_model_used": "Multiple backbone LLMs in experiments: GPT-4o, GPT-4o-mini, LLAMA-3.3-70B, Mistral-Large-123B, DeepSeek-V3-685B (the method itself is LLM-agnostic; LLaMA-3.3 used in some computational-cost runs)",
            "extraction_technique": "LLM-based summarization/key-information extraction from full text (prompted paragraph extraction) combined with embedding-based pre-filtering for candidate retrieval (Sentence-BERT used in retrieval simulation)",
            "synthesis_technique": "Iterative multi-batch schema induction and refinement with repeated comparisons across different paper subsets; incremental table construction and merging of schema edits across iterations, followed by full-text verification of selected rows",
            "number_of_papers": "Applied to the ARXIV2TABLE benchmark (1,957 tables, 7,158 papers total); per-table paper counts vary (dataset contains small to moderate per-table sizes; batching used with size=4 and distractor candidates up to 10 per table during simulation)",
            "domain_or_topic": "Computer science literature (benchmark derived from computer-science literature review tables on arXiv)",
            "output_type": "Structured literature-review tables (schema + cell values) summarizing multiple papers",
            "evaluation_metrics": "Paper selection recall; schema precision/recall/F1; unary (single-cell) precision/recall/F1; pairwise-cell precision/recall/F1; evaluation via LLM-synthesized binary QA pairs (yes/no) produced from ground-truth and answered against generated tables to compute recall/precision",
            "performance_results": "Reported consistent improvement over baselines across backbone models (higher paper-selection recall and higher precision/recall/F1 for schema and unary/pairwise metrics). Computational-cost numbers (LLaMA-3.3 backend): 100% generation success rate, ~118K average tokens per table, ~194s average runtime per table (Table 6). Exact per-model metric tables are in the paper (e.g., improved average F1 and higher recall across models).",
            "comparison_baseline": "Compared against Baseline 1 (one-step generation), Baseline 2 (per-document table generation + merge), and Newman et al. (two-stage: schema from titles/abstracts then value fill from full text)",
            "performance_vs_baseline": "Across all backbone models and evaluation criteria, the iterative batch method consistently outperforms the listed baselines (higher paper recall and F1 on unary and pairwise metrics); specifics are reported in Table 2 of the paper (method produces the best scores within each backbone in the experiments).",
            "key_findings": "Iterative batching plus repeated schema refinement improves selection of relevant papers and alignment of schema/values versus single-pass or per-document methods; extracting key-information paragraphs (not relying solely on abstracts) reduces context-window issues and preserves full-text details; multiple randomized iterations (k up to 5) steadily improve metrics before potential overfitting.",
            "limitations_challenges": "Still challenged by distractor papers — selection recall is imperfect (many distractors remain included); pairwise relationship preservation remains substantially harder than unary value extraction; iterative overfitting can reduce precision after many iterations; computational cost is higher than simplest baselines though within acceptable latency.",
            "scaling_behavior": "Experiments show gains with larger LLMs (performance improves when scaling from 70B to 123B in open-source models), and iterative refinement (increasing k up to ~4 improves performance, k=5 was chosen as optimal in paper due to diminishing returns/overfitting thereafter). Number of input papers is handled via batching (batch size 4) and iterative revisits—no claim of unlimited scaling, but method controls context-window costs.",
            "uuid": "e4386.0",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ARXIV2TABLE",
            "name_full": "ARXIV2TABLE benchmark and pipeline",
            "brief_description": "A realistic benchmark and task formulation for LLM-based literature-review table generation that (1) replaces concise table captions with abstract user-demand prompts, (2) injects semantically similar distractor papers into candidate pools, and (3) evaluates generated tables by LLM-synthesized QA utilization (schema, unary, pairwise).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ARXIV2TABLE (benchmark + evaluation pipeline)",
            "system_description": "Construction pipeline: rewrite original table captions into abstract user-demand intents using GPT-4o; simulate retrieval by selecting distractor candidates via Sentence-BERT ranking (combining similarity to user demand and to referenced papers) and human-verified exclusion labels; evaluation pipeline uses LLM (GPT-4o) to synthesize binary QA pairs from ground-truth tables and to answer them against generated tables to produce recall/precision metrics across schema, unary, and pairwise dimensions.",
            "llm_model_used": "GPT-4o used for user-demand generation and QA-synthesis in benchmark curation and evaluation; experiments use multiple backbone LLMs (GPT-4o, GPT-4o-mini, LLAMA-3.3-70B, Mistral-123B, DeepSeek-V3-685B) to evaluate generation methods on this benchmark.",
            "extraction_technique": "Candidate retrieval simulated with Sentence-BERT embeddings and similarity scoring; LLMs are used both to filter/select relevant papers and to extract key information from full text via prompted summarization.",
            "synthesis_technique": "LLM-based schema induction and value generation (single-shot, per-document, and iterative-batch strategies explored); evaluation uses LLM-synthesized QA pairs to measure usable information overlap rather than only embedding similarity.",
            "number_of_papers": "Benchmark contains 1,957 tables and 7,158 referenced papers total; per-table numbers vary (distractor candidates: top-10 per table, final distractor counts randomly sampled between [m,10]).",
            "domain_or_topic": "Computer science literature (arXiv-sourced literature review tables)",
            "output_type": "Structured literature-review tables (schema + cell values) used as outputs for generation systems and as ground-truth for evaluation",
            "evaluation_metrics": "LLM-synthesized QA-based utilization metrics: recall for ground-truth content retained (schema, unary, pairwise); precision by reversing QA generation/answering; classic metrics reported as precision/recall/F1 for schema, unary, and pairwise value overlap; also paper-selection recall.",
            "performance_results": "Benchmarking shows that both proprietary and open-weight LLMs struggle on the more realistic task (distractors + abstract user demands); reported example numbers: many models produce paper-selection recall in the ~60–75% range, and the proposed method improves these numbers; expert validation shows &gt;98% acceptance rate for synthesized QA pairs and &gt;97% agreement between humans and LLM answers in sampled evaluations.",
            "comparison_baseline": "Provides stronger realism vs prior benchmarks (e.g., ARXIVDIGESTABLES/Newman et al. 2024) by adding distractors and using abstract user demands; used to compare one-step, per-document, Newman et al. two-stage, and the iterative batch method.",
            "performance_vs_baseline": "When evaluated on ARXIV2TABLE, the iterative batch method and other approaches achieved different trade-offs; overall the paper reports their method yields higher recall/F1 than baselines across backbones (see Table 2).",
            "key_findings": "Realistic evaluation (distractors + under-specified intents) exposes limitations of LLMs in literature-table generation; LLM-synthesized QA pairs provide a scalable, high-agreement proxy for human evaluation; schema generation from abstracts can miss full-text details so full-text-based extraction is necessary.",
            "limitations_challenges": "Use of GPT-4o for benchmark curation and QA synthesis introduces potential data-contamination and model-bias concerns; human annotation required to verify distractors is expensive; benchmark derived from computer-science tables so broader-domain generality needs verification.",
            "scaling_behavior": "Benchmark construction and evaluation can be resource-intensive when relying on proprietary LLMs; evaluation pipeline scales to many tables but curation at large scale may be limited by reliance on expensive LLM calls (paper suggests open-source replication as future work).",
            "uuid": "e4386.1",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ArXivDigestables (Newman)",
            "name_full": "ArXIVDIGESTABLES / Arxivdigestables: Synthesizing scientific literature into tables using language models",
            "brief_description": "A prior LLM-based pipeline that streamlines schema and value generation for literature-review tables via a sequential two-stage process: selecting relevant papers and generating schema (titles/abstracts), then filling table values by looping through selected papers' full texts.",
            "citation_title": "Arxivdigestables: Synthesizing scientific literature into tables using language models",
            "mention_or_use": "use",
            "system_name": "Newman et al. two-stage method (ArXIVDIGESTABLES)",
            "system_description": "Two-stage LLM pipeline: stage 1 — select relevant papers from candidates (titles/abstracts) and induce a schema; stage 2 — iterate through selected papers' full texts to fill table rows according to the induced schema. Schema induction uses titles and abstracts which can miss full-text-only aspects.",
            "llm_model_used": "Not specified in detail in this paper; in the present work Newman et al.'s method is implemented/evaluated across the same backbone LLMs used in experiments (e.g., LLaMA-3.3, Mistral, DeepSeek, GPT-4o variants) as a baseline.",
            "extraction_technique": "Schema generated from titles/abstracts; values filled from full text via LLM prompting (document-grounded generation)",
            "synthesis_technique": "Sequential pipeline: schema induction followed by per-document value extraction under the induced schema",
            "number_of_papers": "Applied per-table on the ARXIV2TABLE candidate sets (per-table sizes vary); used as a baseline across the dataset of 1,957 tables",
            "domain_or_topic": "Computer science literature (arXiv tables)",
            "output_type": "Literature-review tables (schema + filled values)",
            "evaluation_metrics": "Same ARXIV2TABLE QA-based evaluation: paper selection recall; schema/unary/pairwise precision/recall/F1",
            "performance_results": "Reported as a competitive baseline but generally outperformed by the proposed iterative-batch method across backbone LLMs (detailed numbers in Table 2 of the paper). Example: Newman et al. two-stage shows higher schema recall than some one-step baselines but lower overall F1 vs the iterative-batch approach.",
            "comparison_baseline": "Compared against one-step and per-document baselines and the new iterative batch method in experiments reported in this paper.",
            "performance_vs_baseline": "Newman et al. method generally exceeded naive one-step baselines on some schema metrics but was outperformed by the iterative-batch approach overall, especially on unary/pairwise F1, per the paper's Table 2.",
            "key_findings": "Two-stage design reduces prompt-length issues and separates schema induction from value extraction, but schema induction based only on titles/abstracts can miss full-text details and reduce alignment with ground-truth tables.",
            "limitations_challenges": "Schema generation from abstracts may overlook full-text aspects; susceptible to distractors if retrieval is noisy; can produce mismatched granularity across columns.",
            "scaling_behavior": "Addresses context-window issues by splitting schema induction from full-text filling; still limited by quality of initial retrieval and by model context constraints during the filling stage.",
            "uuid": "e4386.2",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Baseline1_OneStep",
            "name_full": "Baseline 1 — One-step Table Generation",
            "brief_description": "A straightforward approach that prompts an LLM with all available candidate papers R and the user demand p and asks it in a single pass to select relevant papers and output a complete table with schema and filled values.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Baseline 1 (one-step generation)",
            "system_description": "Single-round prompting: provide all candidate documents and the user demand to the LLM and request selection plus full table induction (schema+values) in one conversation. This approach can exceed LLM context windows when tables/paper texts are large.",
            "llm_model_used": "Applied across the same backbone LLMs in the paper's experiments (e.g., GPT-4o, GPT-4o-mini, LLAMA-3.3, Mistral-123B, DeepSeek-V3) as a baseline",
            "extraction_technique": "Direct prompting over concatenated inputs (full text/abstracts) to extract values and choose relevant papers",
            "synthesis_technique": "Single-step synthesis: simultaneous selection and table generation",
            "number_of_papers": "Varies per-table; when many papers included this baseline often fails due to context-window limits (experiments vary retrieval sizes between 2 and 100 in pilot studies)",
            "domain_or_topic": "Computer science literature (ARXIV2TABLE)",
            "output_type": "Structured tables (schema + values)",
            "evaluation_metrics": "Same ARXIV2TABLE metrics (paper recall; schema/unary/pairwise P/R/F1)",
            "performance_results": "Per-paper experiments show baseline 1 suffers from prompt-length/context issues and generally underperforms compared to iterative methods (specific scores in Table 2).",
            "comparison_baseline": "Compared directly against Baseline 2, Newman et al., and IterativeBatch in the paper's evaluations.",
            "performance_vs_baseline": "Performs worse on large inputs due to context-window failures; lower average F1 and higher failure rate for large tables compared to other methods.",
            "key_findings": "Simple and conceptually direct but impractical for larger tables because of context-length constraints; leads to incomplete generations when prompts exceed model windows.",
            "limitations_challenges": "Excessively long prompts/contexts cause generation failures or degraded selection/induction quality; not robust to distractor-heavy candidate pools.",
            "scaling_behavior": "Does not scale well with number of papers or full-text size; success rate drops as input size grows (paper reports failures of Baseline 1 in some cases).",
            "uuid": "e4386.3",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Baseline2_PerDoc",
            "name_full": "Baseline 2 — Per-document Processing + Merge",
            "brief_description": "A baseline that processes each document individually with an LLM to decide inclusion and generate a per-document row, then merges per-document schemas via exact string matching to form the final table.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Baseline 2 (per-document processing and merge)",
            "system_description": "For each candidate document, the LLM is asked whether it should be included given the user demand; if included, the model generates a table (row) for that document. Final table is produced by merging schemas of all individual rows using exact string matching, and copying values into the merged schema, which often yields a sparse table.",
            "llm_model_used": "Evaluated across backbone LLMs in the experiments (GPT-4o, GPT-4o-mini, LLAMA-3.3, Mistral-Large, DeepSeek-V3)",
            "extraction_technique": "Per-document LLM prompting for inclusion decision and per-document value extraction",
            "synthesis_technique": "Row-wise generation followed by schema merging via exact string matching",
            "number_of_papers": "Operates at the smaller T2 scale (paper selection stage); number depends on candidate set C per table (varies across ARXIV2TABLE entries)",
            "domain_or_topic": "Computer science literature",
            "output_type": "Merged literature-review table (often sparse due to inconsistent per-document schemas)",
            "evaluation_metrics": "ARXIV2TABLE QA-based metrics (paper recall; schema/unary/pairwise P/R/F1)",
            "performance_results": "Tends to achieve higher schema recall because it produces many columns (greater chance of overlap), but results in sparser tables and potential omissions, giving mixed F1 performance relative to other baselines (detailed numbers in Table 2).",
            "comparison_baseline": "Compared against Baseline 1, Newman et al., and IterativeBatch in experiments.",
            "performance_vs_baseline": "Often better schema recall than one-step methods but worse overall utility due to sparsity and inconsistent granularity, and outperformed by the iterative-batch approach on averaged metrics.",
            "key_findings": "Per-document processing reduces context-window issues but produces inconsistent schemas across rows requiring merging, leading to sparsity and potential information loss.",
            "limitations_challenges": "Schema inconsistency and sparsity across rows; per-document prompts may not expose broader cross-paper comparative aspects, reducing comparative utility.",
            "scaling_behavior": "Handles larger numbers of documents better than one-step approach because each document is small, but merging many per-document schemas can become unwieldy and sparse.",
            "uuid": "e4386.4",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ScidaSynth",
            "name_full": "Scidasynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model",
            "brief_description": "A referenced work that frames interactive structured knowledge extraction and synthesis from multiple scientific articles using large language models (title appears in bibliography); mentioned as related work on LLM-based scientific synthesis.",
            "citation_title": "Scidasynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model",
            "mention_or_use": "mention",
            "system_name": "Scidasynth",
            "system_description": "Not described in detail in this paper; referenced as prior work addressing interactive structured extraction and synthesis from scientific literature using LLMs.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Scientific literature (general)",
            "output_type": "Structured knowledge artifacts (implicit from title: extraction + synthesis output)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4386.5",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Text-Tuple-Table",
            "name_full": "Text-tuple-table: Towards information integration in text-to-table generation via global tuple extraction",
            "brief_description": "A referenced method that targets information integration for text-to-table generation by extracting global tuples, cited as related work on tabular induction from text.",
            "citation_title": "Text-tuple-table: Towards information integration in text-to-table generation via global tuple extraction",
            "mention_or_use": "mention",
            "system_name": "Text-tuple-table",
            "system_description": "Referenced as a text-to-table approach that uses global tuple extraction to integrate information when generating tables from text; the present paper notes such methods are related but not directly applicable under differing assumptions.",
            "llm_model_used": null,
            "extraction_technique": "global tuple extraction (per reference title/description)",
            "synthesis_technique": "information integration into tabular form",
            "number_of_papers": null,
            "domain_or_topic": "Text-to-table generation (general NLP tasks)",
            "output_type": "Structured tables (from text)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4386.6",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Text-to-Table",
            "name_full": "Text-to-table: A new way of information extraction",
            "brief_description": "An earlier line of work on inducing tables from text (cited) that frames table induction as a general text-to-table generation problem; referenced in related work as part of broader literature on tabular induction.",
            "citation_title": "Text-to-table: A new way of information extraction",
            "mention_or_use": "mention",
            "system_name": "Text-to-table",
            "system_description": "Referenced prior work positioning table induction as sequence-to-sequence text-to-table generation and information extraction; not used or analyzed experimentally in this paper.",
            "llm_model_used": null,
            "extraction_technique": "sequence-to-sequence / text-to-table extraction (per reference)",
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "General text-to-table information extraction",
            "output_type": "Tables (from textual documents)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4386.7",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "IntentAwareSchema",
            "name_full": "Setting the table with intent: Intent-aware schema generation and editing for literature review tables",
            "brief_description": "A cited (to-appear) work that focuses on intent-aware schema generation/editing for literature review tables, noted as related to user-intent-driven schema induction.",
            "citation_title": "Setting the table with intent: Intent-aware schema generation and editing for literature review tables",
            "mention_or_use": "mention",
            "system_name": "Intent-aware schema generation (Padmakumar et al.)",
            "system_description": "Referenced approach that targets schema generation/editing conditioned on user intent to improve literature-review table construction; cited as complementary prior work emphasizing intent in schema induction.",
            "llm_model_used": null,
            "extraction_technique": "intent-conditioned schema induction (per citation)",
            "synthesis_technique": "schema editing and generation guided by user intent",
            "number_of_papers": null,
            "domain_or_topic": "Literature review table generation (computer science / general academic domains)",
            "output_type": "Schemas and edited table structures",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": null,
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4386.8",
            "source_info": {
                "paper_title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Arxivdigestables: Synthesizing scientific literature into tables using language models",
            "rating": 2,
            "sanitized_title": "arxivdigestables_synthesizing_scientific_literature_into_tables_using_language_models"
        },
        {
            "paper_title": "Scidasynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model",
            "rating": 2,
            "sanitized_title": "scidasynth_interactive_structured_knowledge_extraction_and_synthesis_from_scientific_literature_with_large_language_model"
        },
        {
            "paper_title": "Text-tuple-table: Towards information integration in text-to-table generation via global tuple extraction",
            "rating": 2,
            "sanitized_title": "texttupletable_towards_information_integration_in_texttotable_generation_via_global_tuple_extraction"
        },
        {
            "paper_title": "Text-to-table: A new way of information extraction",
            "rating": 1,
            "sanitized_title": "texttotable_a_new_way_of_information_extraction"
        },
        {
            "paper_title": "Setting the table with intent: Intent-aware schema generation and editing for literature review tables",
            "rating": 2,
            "sanitized_title": "setting_the_table_with_intent_intentaware_schema_generation_and_editing_for_literature_review_tables"
        }
    ],
    "cost": 0.020431500000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol
1 Aug 2025</p>
<p>Weiqi Wang wwangbw@cse.ust.hk 
Center for Speech and Language Processing
Johns Hopkins University
BaltimoreMDUSA</p>
<p>Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>Jiefu Ou 
Center for Speech and Language Processing
Johns Hopkins University
BaltimoreMDUSA</p>
<p>Yangqiu Song yqsong@cse.ust.hk 
Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>Benjamin Van Durme vandurme@jhu.edu 
Center for Speech and Language Processing
Johns Hopkins University
BaltimoreMDUSA</p>
<p>Daniel Khashabi danielk@jhu.edu 
Center for Speech and Language Processing
Johns Hopkins University
BaltimoreMDUSA</p>
<p>Daya Guo 
Dejian Yang 
Haowei Zhang 
Junxiao Song 
Ruoyu Zhang 
Runxin Xu 
Qihao Zhu 
Shirong Ma 
Peiyi Wang 
Xiao Bi 
Xiaokang Zhang 
Xingkai Yu 
Yu Wu 
Z F Wu 
Zhibin Gou 
Zhihong Shao 
Zhuoshu Li 
Ziyi Gao 
Aixin Liu 
Bing Xue 
Bingxuan Wang 
Bochao Wu 
Bei Feng 
Chengda Lu 
Chenggang Zhao 
Chengqi Deng 
Chenyu Zhang 
Chong Ruan 
Damai Dai 
Deli Chen 
Dongjie Ji 
Erhang Li 
Fangyun Lin 
Fucong Dai 
Fuli Luo 
Guangbo Hao 
Guanting Chen 
Guowei Li 
Han Bao 
Hanwei Xu 
Haocheng Wang 
Honghui Ding 
Huajian Xin 
Huazuo Gao 
Hui Qu 
Hui Li 
Jianzhong Guo 
Jiashi Li 
Jiawei Wang 
Jingchang Chen 
Jingyang Yuan 
Junjie Qiu 
Junlong Li 
J L Cai 
Jiaqi Ni 
Jian Liang 
Kai Dong 
Kai Hu 
Kaige Gao 
Kang Guan 
Kexin Huang 
Kuai Yu 
Lean Wang 
Lecong Zhang 
Liang Zhao 
Litong Wang 
Liyue Zhang 
Lei Xu 
Leyi Xia 
Mingchuan Zhang 
Minghua Zhang 
Minghui Tang 
Meng Li 
Miaojun Wang 
Mingming Li 
Ning Tian 
Panpan Huang 
Peng Zhang 
Qiancheng Wang 
Qinyu Chen 
Qiushi Du 
Ruiqi Ge 
Ruisong Zhang 
Ruizhe Pan 
Runji Wang 
Ruyi J Chen 
R L Jin 
Shanghao Lu 
Shangyan Zhou 
Shanhuang Chen 
Shengfeng Ye 
Shiyu Wang 
Shuiping Yu 
Shunfeng Zhou 
Shuting Pan 
S S Li 
Shuang Zhou 
Shaoqing Wu 
Tao Yun 
Tian Pei 
Tianyu Sun 
T Wang 
Wangding Zeng 
Wanjia Zhao 
Wen Liu 
Wenfeng Liang 
Wenjun Gao 
Wenqin Yu 
Wentao Zhang 
W L Xiao 
Wei An 
Xiaodong Liu 
Xiaohan Wang 
Xiaokang Chen 
Xiaotao Nie 
Xin Cheng 
Xin Liu 
Xin Xie 
Xingchao Liu 
Xinyu Yang 
Xinyuan Q Li 
Xuecheng Su 
Xuheng Lin 
Xiangyue Jin 
Xiaojin Shen 
Xiaosha Chen 
Xiaowen Sun 
Xiaoxi- Ang Wang 
Xinnan Song 
Xinyi Zhou 
Xianzu Wang 
Xinxia Shan 
Yao K Li 
Yaohui Q Wang 
Y X Wei 
Yang Zhang 
Yanhong Xu 
Yao Zhao 
Yaofeng Sun 
Yi Yu 
Yichao Zhang 
Yifan Shi </p>
<p>Yiliang Xiong
Ying He, Yishi Piao, Yisong Wang, Yiyang Ma, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yuheng Zou, Yu-jia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuyang Zhou, Y. X. Zhu, Yanhong XuYixuan Tan, Yiyuan Liu, Yue Gong, Yuxuan Liu</p>
<p>Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol
1 Aug 2025063B8A06EC360A1375A60EC4A17D62CAarXiv:2504.10284v3[cs.CL]
Literature review tables are essential for summarizing and comparing collections of scientific papers.We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers.Building on recent work(Newman et al., 2024), we extend prior approaches to address realworld complexities through a combination of LLM-based methods and human annotations.Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers).To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios.Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements.Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.</p>
<p>Introduction</p>
<p>Literature review tables play a crucial role in scientific research by organizing and summarizing large amounts of information from selected papers into a concise and comparable format (Russell et al., 1993).At the core of these tables are the schema and values that define their structure, where schema refers to the categories or aspects used to summarize different papers and values correspond to the specific information extracted from each paper.A well-defined schema allows each work to be represented as a row of values, enabling structured and transparent comparisons across different studies.</p>
<p>(T2) Paper Selection (T3) Table Induction: Schema + Value Generation</p>
<p>Generate a table that analyzes various image-guided fashion retrieval methods, highlighting the evolution of network architectures, the datasets they were evaluated on, the evaluation criteria used, and the specific loss functions applied.The table should provide a detailed comparison of these aspects to facilitate understanding of differences across the listed papers.</p>
<p>(T1) Candidate paper retrieval IR User intent prompt inferred table</p>
<p>Figure 1: Overview of our proposed task: Given a user's demand, a simulated information retrieval (IR) engine first retrieves semantically relevant papers.Then, a language model further filters them and induces the table's corresponding schema and values to satisfy the user's demand.The grayed region indicates the scope covered by our method and benchmark (ARXIV2TABLE).</p>
<p>With recent advancements in large language models (LLMs; OpenAI, 2025b;DeepSeek-AI et al., 2025), several studies (Newman et al., 2024;Dagdelen et al., 2024;Sun et al., 2024) have explored generating literature review tables by prompting LLMs with a set of pre-selected papers and the table's caption.While these efforts represent meaningful progress, we argue that the existing task definition and evaluation protocols are somewhat unrealistic, thus hindering the practical applicability of generation methods.</p>
<p>First, existing pipelines assume that all provided papers are relevant and should be included in the table.However, in real-world scenarios, distractor papers-those that are irrelevant or contain limited useful information-are common (OpenAI, 2025a).Models should be able to identify and filter out such papers before table construction.Additionally, current pipelines use the ground-truth table's descriptive caption as the objective for generation.These captions often lack sufficient context, making it difficult for LLMs to infer an appropriate schema, or they may inadvertently reveal the schema and values, leading to biased evaluations.</p>
<p>In this paper, we introduce our task, as illustrated in Figure 1, which improves upon previous task definitions through two key adaptations.First, our pilot study shows that LLMs struggle to retrieve relevant papers from large corpora.To benchmark this, we introduce distractor papers by selecting them based on semantic similarity to papers in the ground-truth table.LLMs must first determine which papers should be included before generating the table.Second, we replace table captions with abstract user demands that describe the goal of curating the table, making the task more aligned with real-world scenarios.We build upon the ARX-IVDIGESTABLES (Newman et al., 2024) dataset and construct a sibling benchmark through human annotation to verify the selected distractors, comprising 1,957 tables and 7,158 papers.</p>
<p>Meanwhile, current evaluation methods rely on static semantic embeddings to estimate schema overlap between generated and ground-truth tables and require human annotations to assess the quality of unseen schemas and values.However, semantic embeddings struggle to capture nuanced, context-specific variations due to their reliance on pre-trained representations, while human annotation is costly and time-consuming.Moreover, the most effective table generation approaches define schemas primarily based on paper abstracts.This method risks missing important aspects present in the full text, leading to loosely defined schemas with inconsistent granularity.</p>
<p>To address these issues, we propose an annotation-free evaluation framework that instructs an LLM to synthesize QA pairs based on the ground-truth table and assess the generated table by answering these questions.These QA pairs evaluate table content overlap across three dimensions: schema-level, single-cell, and pairwise-cell comparisons.Additionally, we introduce a novel table generation method that batches input papers, iteratively refining paper selection and schema definition by revisiting each paper multiple times.Extensive experiments using five LLMs demonstrate that they struggle with both selecting relevant papers and generating high-quality tables, while our method significantly improves performance on both fronts.Expert validation further confirms the reliability of our QA-synthetic evaluations.</p>
<p>In summary, our contributions are threefold: (1) We introduce an improved task definition for literature review tabular generation, benchmarking it in a more realistic scenario by incorporating dis-tractor papers and replacing table captions with abstract user demands; (2) We propose an annotationfree evaluation framework that leverages LLMgenerated QA pairs to assess schema-level, singlecell, and pairwise-cell content overlap, addressing the limitations of static semantic embeddings and human evaluation; and (3) We develop a novel iterative batch-based table generation method that processes input papers in batches, refining schema definition and paper selection iteratively.</p>
<p>To the best of our knowledge, we are the first to introduce a task that simulates real-world use cases of scientific tabular generation by incorporating user demands and distractor papers, providing a more robust assessment of LLMs in this domain.</p>
<p>Related Works</p>
<p>Scientific literature tabular generation Prior works primarily attempt to generate scientific tables through two stages: schema induction and value extraction.For schema induction, early methods like entity-based table generation (Zhang and Balog, 2018) focused on structured input, while recent work has explored schema induction from user queries (Wang et al., 2024) and comparative aspect extraction (Hashimoto et al., 2017).For value extraction, various approaches such as documentgrounded question-answering (Kwiatkowski et al., 2019;Dasigi et al., 2021;Lee et al., 2023), aspectbased summarization (Ahuja et al., 2022), and document summarization (DeYoung et al., 2021;Lu et al., 2020) have been proposed to extract relevant information.Beyond these methods, several datasets have been introduced to support scientific table-related tasks, such as TableBank (Li et al., 2020), SciGen (Moosavi et al., 2021), and Sc-iTabQA (Lu et al., 2023).Recently, Newman et al. (2024) proposed streamlining schema and value generation with LLMs sequentially and curated a large-scale benchmark for evaluation.However, all these methods assume a clean and fully relevant set of papers and rely on predefined captions or abstract-based schemas, which risk missing key details.In contrast, we argue for an evaluation approach where candidate papers include tangentially relevant or distracting papers, aligning more closely with real-world literature review workflows (Padmakumar et al., 2025).(Li et al., 2023;Wu et al., 2022) or as a question-answering problem (Sundar et al., 2024;Tang et al., 2023).Similar to these works, our framework is capable of better handling both structured and distractive input for real-world literature review and knowledge synthesis.</p>
<p>Task Definition</p>
<p>We first define a pipeline consisting of three subtasks that extend prior definitions and better capture the real-world usage of literature review tabular generation.For all the following tasks, we are given a user demand prompt p, which specifies the intended purpose of creating the table.(T1) Candidate Paper Retrieval: We begin with a given universe of papers (e.g., the content of Google Scholar or arXiv) from which relevant papers need to be identified.Given a large collection, the goal is to use a search engine (IR) to retrieve a subset of candidate papers C := {d i } M i=1 of size M , which may include distractor papers-i.e., papers that resemble the user demand prompt but do not fully satisfy the requirement.(T2) Paper Selection: Given C, the second subtask is to select the relevant subset of size m (m &lt; M ): R := {d i } m i=1 ⊆ C, which best aligns with the user demand p. T2 differs from T1 in scale.Due to the large scale of T1, IR engines must optimize for recall, ensuring that as many relevant papers as possible are retrieved.However, T2 operates at a smaller scale, where precision is the priority, as it focuses on filtering out distractors and selecting only the most relevant papers.(T3) Table Induction: Given the selected papers R, the objective is to generate a table with m rows and N columns, where N ≥ 2 (i.e., no singlecolumn tables).Each row r i ∈ {r 1 , r 2 , . . ., r m } corresponds to a unique input document d i ∈ R, and each column c j ∈ {c 1 , c 2 , . . ., c N } represents a unique aspect of the documents.We refer to these N columns as the schema of the table and the N × m cells as the values of the table.The value of each cell is derived from its respective document according to the aspect defined by the corresponding column.</p>
<p>ARXIV2TABLE Construction</p>
<p>We then construct ARXIV2TABLE based on the ARXIVDIGESTABLES dataset which consists of literature tables (extracted from computer science papers) and their corresponding captions.We filter out tables that are structurally incomplete or lack full text for all referenced papers.As a result, we are left with 1,957 tables (with captions) which have rows referring to 7,158 papers.Our construction involves three pillars: user demand inference ( §4.1), a simulated paper retrieval ( §4.2) and evaluation through utilization ( §4.3).</p>
<p>Constructing User Demand Prompts</p>
<p>The first step is to collect user demands p that explicitly describe the desired table (can be understood without the table content) and do not reveal the table's schema or specific values.</p>
<p>Table captions are not appropriate prompts</p>
<p>While the input dataset contains one caption per table, collected from arXiv papers, these captions are meant to complement tables rather than fully describe them.As a result, they are generally concise.For example, a table caption might read: "Performance comparison of different approaches," which is too vague to understand without seeing the table.Consequently, using table captions as prompts may not yield a well-defined task.A more contextually self-contained rewritten user demand might instead be: "Draft a table that compares different knowledge editing methods, focusing on their performance on QA datasets."</p>
<p>Our prompt construction To address this issue, we propose rewriting the captions of literature review tables into abstract yet descriptive user intentions using LLMs.We guide GPT-4o with a prompt (see §A) that first explains the task to the LLM, specifying that the user demand should be sufficiently contextualized to clearly state the table's purpose while avoiding the inclusion or direct description of column names or specific values.GPT-4o is then expected to infer the user demand for the given table and</p>
<p>Paper Retrieval Simulation</p>
<p>The unreliability of paper retrieval Next, we approach the first subtask, candidate paper retrieval, by conducting a pilot study to assess whether LMs can reliably retrieve relevant papers from a large corpus.For each table, we employ a Sentence-BERT (Reimers and Gurevych, 2019) encoder as a retrieval engine, selecting papers from the entire corpus based on the highest similarity between the table's user demand and each paper's title and abstract.We vary the number of retrieved papers between 2 and 100 and plot the precision and recall of retrieval against the ground-truth papers in the original table (Figure 3).We observe consistently low precision and recall across different retrieval sizes, highlighting the challenge of retrieving relevant papers from a noisy corpus.This demonstrates that the first subtask is non-trivial and may introduce noise into subtask T2.However, various information retrieval engines, such as Google Scholar and Semantic Scholar, can replace LMs in this subtask.Thus, we decide to simulate T1 by manually adding noisy distractor papers into C to construct R, ensuring a noisy input for T2.This allows us to focus on evaluating LLMs' capabilities in the T2 and T3 subtasks.table but exhibit key differences that fail to meet the user demand.To select such candidates, we adopt a retrieve-then-annotate approach.First, we use a SentenceBERT encoder F to obtain embeddings for (1) the user demand F (p) and (2) all papers in the corpus {F (d i ) | d i ∈ C}.Each paper's embedding is computed by encoding the concatenation of its title and abstract.We then rank all papers d i / ∈ R based on the average of two cosine similarities: (1) the similarity between the candidate and the user demand, and (2) the average similarity between the candidate and each referenced paper:</p>
<p>Similarity-based paper retrieval
s(di) = cos(F (di), F (p)) + 1 m m j=1 cos(F (di), F (du j )).
Higher values of s(d i ) indicate stronger semantic relevance, and we select the top 10 ranked papers for each table as its distractor candidates.</p>
<p>Candidates verification via human annotation</p>
<p>After selecting these candidates, we conduct human annotations to verify whether they should indeed be excluded from the table.Given that annotating these tables requires expert knowledge in computer science, we recruit seven postgraduate students with research experience in the field as annotators.To ensure they are well-prepared for the task, the annotators undergo rigorous training, including pilot annotation exams.Their task is to make a binary decision on whether a given distractor paper-based on its title, abstract, user demand, the ground-truth table, and the titles and abstracts of all referenced papers-should be included in the table.Each table contains annotations for 10 papers, with each distractor paper initially assigned to two randomly selected annotators.If both annotators agree on the label, it is finalized.Otherwise, two additional annotators review the paper until a consensus is reached.In the first round, the inter-annotator agreement (IAA) is 94% based on pairwise agreement, and the Fleiss' Kappa (Fleiss, 1971) score is 0.73, indicating a substantial level of agreement (Landis and Koch, 1977).Finally, for each table, we randomly select a number of distractor papers between [m, 10].</p>
<p>Evaluation via LLM-based Utilization</p>
<p>After constructing the benchmark, we propose evaluating the quality of generated tables from a utilization perspective to address the challenge of aligning schemas and values despite potential differences in phrasing.This is achieved by synthesizing QA pairs based on the ground-truth table and using the generated table to answer them, or vice versa.The flexibility of this QA synthesis allows us to evaluate multiple dimensions of the table while ensuring a structured and scalable assessment.An overview with running examples is shown in Figure 4.</p>
<p>Dimensions of evaluating a table with QAs</p>
<p>We introduce three key aspects for evaluating a table in terms of its usability: (1) Schema: whether a specific column is included in the generated schema, (2) Unary Value: whether a particular cell from the ground-truth table appears in the generated table, (3) Pairwise Value: whether relationships between two cells remain consistent in the generated table .Recall evaluation We guide GPT-4o in generating binary QA pairs based on the ground-truth table .For the first two aspects, we generate QA pairs for all columns and cells, whereas for the third, we randomly sample 10 cell pairs per table and synthesize them into QA pairs.We then prompt GPT-4o to answer these questions based on the generated table, providing yes/no responses.If the answer cannot be found, the model is instructed to respond with no," and vice versa for yes."The ratio of "yes" answers indicates how well the generated table preserves the schema, individual values, and pairwise relationships.This represents the recall of the ground-truth table, measuring how much original information is retained in the generated table.</p>
<p>Precision evaluation To additionally evaluate precision, we reverse the process: instead of gen-
Incorrect! Correct! Correct!
The ratio of "correct" indicates the recall.</p>
<p>Tabular Generation Methodologies</p>
<p>We explore a range of methods to evaluate on our proposed task, starting from several baselines inspired by prior work ( §5.1) and then our proposed approach ( §5.2).</p>
<p>Baseline Methods</p>
<p>We first introduce three methods for generating literature review tables to evaluate their performance on our task and use them as baselines for our proposed method.For easy reference, these methods are termed numerically.First, Baseline 1 generates the table in a one-step process.It takes all available papers R and the user demand p as input, and the model is asked to select all relevant papers and output a table with a welldefined schema and filled values in a single round of conversation.However, this method struggles with extremely long prompts that exceed the LLMs' context window when generating large tables.</p>
<p>To address this issue, Baseline 2 processes papers individually.For each document, the model decides whether it should be included based on the user demand.If included, the model generates a table for that document.After processing all documents, the final table is created by merging the schemas of all individual tables using exact string matching and copying the corresponding values.While this approach reduces the input prompt length, it results in highly sparse tables due to inconsistent schema across papers and the potential omission of relevant information when individual papers lack sufficient context to define comprehensive table aspects.</p>
<p>To overcome both issues, Newman et al. (Newman et al., 2024) introduces a two-stage process.In the first stage, the model selects papers relevant to the user demand based on their titles and abstracts, then generates a corresponding schema.In the second stage, the model loops through the selected papers and fills in the respective rows based on the full text of each document.A minor drawback of this method is that the schema is generated solely from titles and abstracts, which may overlook details present only in the full text.Note that this method is the strongest recent baseline for scientific tabular generation while other text-to-table methods (Deng et al., 2024b) are not directly applicable due to different assumptions.</p>
<p>Iterative Batch-based Tabular Generation</p>
<p>Then, we introduce our proposed method for generating literature review tables.Our approach consists of three steps: (A) key information extraction, (B) paper batching, and (C) paper selection and schema refinement, where the latter two steps can be iterated multiple times.</p>
<p>(A) Key Information Extraction Processing multiple papers simultaneously using their full text often results in excessively long prompts that exceed the LLMs' context window.To address this, we first shorten each paper by instructing the LLM to extract key information from the full text that is relevant to the user's requirements.Notably, we do not rely solely on the abstract, as important details often appear in the full text but are omitted from the abstract.For each paper, we provide the LLM with its title, abstract, and full text, along with the user's request, and ask it to generate a concise paragraph that preserves all potentially relevant details.These summary paragraphs serve as condensed representations of the papers for subsequent processing.</p>
<p>(B) Paper Batching Next, we divide all key information paragraphs into smaller batches.Processing too many papers at once negatively affects the model's performance (as demonstrated by the comparison of Baseline 1 in Table 2), whereas batching facilitates more efficient comparisons within each batch.For simplicity, we set a batch size of 4 and randomly partition R into |R| 4 batches.</p>
<p>(C) Paper Selection and Schema Refinement</p>
<p>We initialize an empty schema and table, then sequentially process each batch with the LLM by providing it with the user's request and summaries of batched papers.The LLM is instructed to (1) decide whether each paper should be included or removed based on its key information and (2) refine the schema based on the current batch of papers.Schema refinement involves adding or removing specific columns or modifying existing values to align with different formats.For new papers that are not deemed suitable for inclusion yet are not in the current table, we also prompt the LLM to insert a new row according to the refined schema.This ensures that the table remains dynamically structured, continuously adapting to new information while maintaining consistency across batches.</p>
<p>Afterward, we iterate steps B and C for k iterations.Here k is a hyper-parameter and we set k = 5 in our experiments.The rationale is that multiple iterations allow the schema and table contents to progressively improve, ensuring better alignment with user demands.In each iteration, the batches are newly randomized so that each paper is compared with different subsets, enabling more robust decision-making and reducing bias from specific batch compositions.This iterative refinement also mitigates errors from earlier batches by revisiting and adjusting prior decisions based on newly processed information.After completing all iterations, we individually prompt the LLM to revisit the full text of the selected papers to verify the values, thereby completing the tabular generation process.</p>
<p>6 Experiments and Analyses</p>
<p>Experiment Setup</p>
<p>To demonstrate the generalizability of our method and evaluations, we conduct experiments using two proprietary and three open-source LLMs as backbone model representatives: GPT-4o (OpenAI, 2024b), GPT-4o-mini (OpenAI, 2024a) 3.3 (70B; Dubey et al., 2024), and Mistral-Large (123B; Mistral-AI, 2024).We apply all baseline methods and our proposed method to each model and use our evaluation framework to assess the quality of the generated tables based on our benchmark, focusing on four aspects: paper selection (Paper), schema content overlap (Schema), singlecell value overlap (Unary Value), and comparisons across cells (Pairwise Value).For paper selection, we use recall as the metric to measure the number of ground-truth papers successfully selected.For the latter three tasks, we report precision (P), recall (R), and F1 scores (F1), as explained in §4.3.</p>
<p>Main Evaluation Results</p>
<p>We report the main evaluation results in Table 2 and summarize our key findings as follows.A visual comparison of model-wise performance across methods is also provided in Figure 6.</p>
<p>(1) All methods and models struggle to distinguish relevant papers from distractors.For example, even with their best-performing methods, LLAMA-3.3 and GPT-4o achieve only 65.4% and 71.3% recall on average, respectively.This indicates that a significant number of distractor papers are still being included in the generated tables.Additionally, we observe that processing papers individually or using only abstracts for inclusion decisions yields better performance than concatenating full texts.This suggests that excessively long prompts may weaken LLMs' ability to make accurate inclusion decisions for each paper.</p>
<p>(2) Aligning generated schemas with the groundtruth table remains challenging.Among the baselines, the second method consistently achieves higher recall (e.g., 69.3% with LLAMA-3.3),primarily because it generates a larger number of columns, leading to more overlaps with the groundtruth schema.However, other methods exhibit significantly lower recall, indicating that LLMs still struggle to generate meaningful columns that align well with the ground-truth structure.</p>
<p>(3) While unary values are well preserved, pairwise comparisons suffer substantial losses.Most methods, especially our proposed approach, extract unary values with relatively high F1 scores.However, extracting and maintaining pairwise relationships remains challenging.This trend is consistent across different models, suggesting that while individual entries are correctly identified, capturing the relationships between them remains difficult.The significant gap highlights the challenge of preserving complex relational comparisons within the generated tables.</p>
<p>(4) Our proposed method improves performance across all aspects and models.Across all backbone models and evaluation criteria, our method consistently outperforms the baselines.For example, it achieves the highest recall and F1 scores for both unary and pairwise metrics, regardless of model size.This demonstrates that our approach not only enhances overall performance but also provides a more robust solution for handling distractor paper selection and precise table generation.</p>
<p>(5) Larger models lead to better performance.</p>
<p>For the three open-source LLMs, we observe a clear trend that increasing the model size improves performance across all aspects when using the same method.For instance, with our approach, scaling from 70B to 123B parameters leads to consistent improvements in most aspects and metrics, reinforcing the importance of stronger generative capabilities in addressing this task.</p>
<p>Ablation Study on Iteration Number</p>
<p>We further study the impact of the number of iterations, k, in our proposed method to illustrate the importance of refining the schema and table contents over multiple iterations using different batches of papers.As described in §5.2, we perform one round of paper selection and schema refinement five times to achieve optimal performance.</p>
<p>In this section, we analyze this process by studying the model's performance across previous rounds.</p>
<p>We select GPT-4o as the backbone model and visualize changes in the F1 scores for schema, unary value, pairwise comparison overlap, and their average, by applying the same evaluation protocol to the generated tables across iterations ranging from 1 (the first cycle) to 5.</p>
<p>The results are plotted in Figure 5.We observe that during the first four iterations, performance steadily improves across all aspects, demonstrating the effectiveness of iteratively refining paper selection and table schema through multiple iterations and comparisons between different subsets of papers.At the fifth iteration, however, the improvement slows down, and in some cases, performance even decreases.One possible reason is that the table starts overfitting by including additional values that do not appear in the ground-truth table, reducing precision and leading to lower F1 scores.Considering the overall performance, k = 5 is supported as the optimal number of iterations.</p>
<p>,WHUDWLRQV 3HUIRUPDQFH 6FKHPD 8QDU\9DOXH</p>
<p>3DLUZLVH9DOXH $YHUDJH</p>
<p>Figure 5: Ablation study on the number of iterations for our iterative batch-based table generation method.</p>
<p>Validation of Utilization-Based Evaluation</p>
<p>To verify the reliability of synthesizing QA pairs using LLMs for evaluating tabular data, we conduct two complementary expert assessments.First, we invited the authors (as domain experts) to manually inspect a random sample of 200 QA pairs-spanning schema-level, unary value, and pairwise value comparisons.Annotators were asked to assess (1) whether each QA pair is firmly grounded in the source table, and (2) whether the LLM's answer is correct based on the generated target table.As shown in Table 3, the expert acceptance rates exceed 98% in all categories, confirming the quality of the synthesized QA pairs.Second, we conducted an additional human study to assess whether our LLM-based evaluation aligns with human judgment across different generation methods.For each method, we sampled 300 QA pairs, answered them using both LLMs and human annotators, and measured the agreement rate.As shown in Table 4, LLM and human "yes" response rates are highly consistent, with over 97% agreement across all methods.These results reinforce the robustness of our evaluation framework, demonstrating that LLM-synthesized QA pairs provide a scalable and trustworthy proxy for human judgment in assessing semantically diverse tabular outputs.Specifically, these results indicate that the high agreement is not driven by an inherent bias of LLMs toward their own generated QA pairs.Table 4: Comparison between GPT-4o and human annotators on 300 QA pairs.We report the proportion of "yes" answers by each and their overall agreement.</p>
<p>Conclusion</p>
<p>In this work, we introduce an improved literature review table generation task that incorporates distractor papers and replaces table captions with abstract user demands to better align with real-world scenarios, and curated an associated benchmark.Additionally, we propose an annotation-free evaluation framework using LLM-synthesized QA pairs and a novel method to enhance table generation.</p>
<p>Our experiments show that current LLMs and existing methods struggle with our task, while our approach significantly improves performance.We envision that our work paves the way for more au-tomated and scalable literature review table generation, ultimately facilitating the efficient synthesis of scientific knowledge in large-scale applications.</p>
<p>Limitations</p>
<p>A minor limitation is that our work uses ARXIVDI-GESTABLES as the source of literature review tables for subsequent data reconstruction.However, Newman et al. (2024) have included their pipeline for scalably extracting literature review tables from scientific papers, thus resolving the data reliance gap.Beyond the computer science domain, our formulation and methodology are readily applicable to other scientific fields such as medicine, physics, and social sciences, where structured comparisons across publications are equally valuable.Moreover, the core task-generating structured tables from noisy, unstructured input with under-specified intent-extends naturally to real-world applications like news fact aggregation, personalized knowledge card generation, and structured database population from web or legal documents.Another limitation of our work is its reliance on GPT-4o, a proprietary LLM, for benchmark curation and subsequent evaluation, which may introduce several issues.First, it raises concerns about data contamination (Deng et al., 2024a;Dong et al., 2024), as the model may generate user demands (during benchmark curation) and synthesis evaluation questions (when evaluating a generated table against the ground truth) that are similar to its training data, potentially leading to inflated performance in table generation.A data provenance check (Longpre et al., 2024) can be further implemented to address this issue.Second, the benchmark and evaluation process may inherit the internal knowledge or semantic distribution biases of GPT-4o, which could skew the evaluation of other LLMs and reduce the generalizability of our findings.Lastly, a minor issue is scalability, as curating larger datasets using a proprietary model can be resource-intensive and may limit accessibility when extending our framework to other literature or domains.Future work can explore the use of open-source LLMs to replicate the entire process for convenient adaptation to other tabular datasets.</p>
<p>Ethics Statement</p>
<p>The ARXIVDIGESTABLES (Newman et al., 2024) dataset used in our work is shared under the Open Data Commons License, which grants us access to it and allows us to improve and redistribute it for research purposes.Regarding language models, we access all open-source LMs via the Hugging Face Hub (Wolf et al., 2020) and proprietary GPT models through their official API1 .The number of these models, if available, is marked in Table 2.All associated licenses for these models permit user access for research purposes, and we commit to following all terms of use.</p>
<p>When prompting GPT-4o to generate user demands and synthetic QA questions, we explicitly state in the prompt that the LLM should not generate any content that contains personal privacy violations, promotes violence, racial discrimination, hate speech, sexual, or self-harm contents.We also manually inspect a random sample of 100 data entries generated by GPT-4o for offensive content, and none are detected.Therefore, we believe that our dataset is safe and will not yield any negative or harmful impact.</p>
<p>Our human annotations are conducted by recruiting five graduate-level students who have sufficient experience in data collection for training large language models.They are proficient in English, primarily from Asia, and are paid above the minimum wage in their local jurisdictions.They receive thorough training on the task and are reminded to have a clear understanding of the task instructions before proceeding to annotation.The high level of inter-agreement also confirms the quality of our annotation.The expert annotators have agreed to participate as their contribution to the paper without receiving any compensation.</p>
<p>Appendices A Implementation Details</p>
<p>In this section, we provide additional implementation details about our benchmark curation and evaluation pipeline, including the prompt we used and the models we accessed.</p>
<p>A.1 Prompts Used</p>
<p>We first introduce the prompt used to construct the ARXIV2TABLE benchmark, as explained in Section 4. The main step involves prompting LLM is to collect user demands that describe the purpose of creating the table while remaining contextually self-contained and not revealing the actual schema or values of the table.We use the following prompt to instruct GPT-4o in generating these user demands.The distribution of number of papers per table in ARXIV2TABLE is shown in Figure 7.</p>
<p>A.2 Evaluation Implementations</p>
<p>We access all open-source LLMs via the Hugging Face library (Wolf et al., 2020).The models used are meta-llama/Llama-3.3-70B-Instruct,mistralai/Mistral-Large-Instruct-2411, and deepseek-ai/DeepSeek-V3.</p>
<p>For GPT models, we access them via the official OpenAI Batch API 2 .The models used are gpt-4o-mini-2024-07-18 and gpt-4o-2024-08-06.</p>
<p>Note that the DeepSeek model family has a context window limit of 64K tokens, whereas the others have a limit of 128K tokens.The generation temperature is set to 0.5 for all experiments.All experiments are repeated twice and the average performance is reported.</p>
<p>2 https://platform.openai.com/docs/guides/batch1XPEHURI5HIHUHQFH3DSHUVSHU7DEOH )UHTXHQF\RI7DEOHV</p>
<p>A.3 Computational Cost Comparisons</p>
<p>To assess the efficiency and scalability of our iterative batch-based method, we report computational statistics in Table 6.Each method was run using the same LLaMA-3.3model backend.We measure three aspects: (1) generation success rate, defined as the proportion of prompts yielding complete tables within the context window, (2) average token usage per table, and (3) average runtime per table.Our method achieves a 100% success rate, outperforming the baselines that occasionally fail due to context limitations or prompt instability.While our runtime is moderately longer than Baseline 1 and Baseline 2, it remains comparable to Newman et al. and stays well within acceptable latency for practical usage.Furthermore, token usage remains controlled, confirming that our iterative approach does not incur excessive computational cost despite its multi-step structure.These results demonstrate that our method offers a favorable trade-off between performance and efficiency.</p>
<p>B Annotation Details</p>
<p>To ensure the high quality of our human annotations, we implement strict quality control measures.First, we select only postgraduate students with research experience in computer science to ensure they are familiar with relevant topics.All selected annotators undergo qualification rounds, and we invite only those who demonstrate satisfactory performance to serve as our main annotators.</p>
<p>For each task, we provide workers with comprehensive task explanations in layman's terms to enhance their understanding.Additionally, we offer detailed definitions and multiple examples for each choice to help annotators make informed decisions.Each entry requires the worker to provide a binary vote on whether the paper should be excluded or not.Our annotation interface is shown in Figure 8.</p>
<p>To ensure comprehension, we require annotators to confirm that they have thoroughly read the instructions by ticking a checkbox before starting the annotation task.We also manually monitor the performance of annotators throughout the annotation process and provide feedback based on common errors.Spammers or underperforming workers are disqualified.As described in Section 4.2, the interannotator agreement supports the quality of our collected annotations.Table 8 illustrates schema, unary value, and pairwise value questions designed to assess the quality of generated tables, ensuring alignment with ground-truth information.The results reveal that this QA-based evaluation effectively quantifies schema retention, individual value accuracy, and consistency in relationships, providing a structured approach for benchmarking table generation models.</p>
<p>C Case Studies</p>
<p>In addition, we present two pairs of ground-truth and generated tables as examples for a case study on table generation, as shown in Table 9.From these tables, we observe that the generation process is capable of incorporating many useful columns, thereby enriching the available information.For instance, in the first example, the generated table introduces new columns such as Number of Images, Number of Subjects, and Avg.Images per Subject, which add valuable quantitative insights beyond the original ground truth table.However, it is also evident that some columns present in the ground truth, like the Evaluation Metric, are not fully covered in the generated version.In the second example, the user demand for detailed descriptions has led to a generated table with numerous specific columns, including ID, Method Used, Performance Metric, and Results Achieved.Although these additional details enhance the descriptive quality of the table, they also suggest a potential issue: the need for further polishing and refinement of the user demand to balance detail with clarity.</p>
<p>Original Table Caption</p>
<p>User Demand</p>
<p>Comparison of Trajectory and Path Planing Approach</p>
<p>Generate a table that compares different trajectory and path planning approaches, focusing on their collision avoidance techniques, benefits, limitations, and applicable scenarios.The table should include detailed columns to capture these aspects for each method mentioned in the relevant papers.</p>
<p>Publications with deep-learning focused sampling methods.We cluster the papers based on the space the sample through and how the samples are evaluated.Some approaches further consider an optional refinement stage.</p>
<p>Create a table that categorizes publications focused on deep-learning-based sampling methods for grasp detection, organizing them by the space in which samples are generated, the evaluation criteria used, and whether a refinement stage is included.The table should provide a comprehensive yet concise overview of the methodological variations and enhancements across different papers.</p>
<p>Categorization of textual explanation methods.</p>
<p>Create a table that categorizes the methods used for providing textual explanations in visual question answering systems, focusing on the types of texts generated and the reasoning processes employed.The table should use succinct columns to differentiate between these methodological aspects for each paper.</p>
<p>Metadata of the three benchmarks that we focus on.XSumSota is a combined benchmark of cite:1400aac and cite:d420ef8 for summaries generated by the state-of-the-art summarization models.</p>
<p>Create a table that details the metadata for three summarization benchmarks, focusing on the composition of annotators, the dataset sizes for validation and testing, and the distribution of positive and negative evaluations.The table should provide a comprehensive comparison across these aspects for each benchmark.</p>
<p>Review of open access ground-based forest datasets</p>
<p>Create a table that reviews various open-access forest datasets, focusing on the publication and data recording years, types of data collected, and their applicability to specific forestryrelated tasks.The table should offer a concise summary of each dataset's attributes, including the number of classification categories and geographical location.</p>
<p>Comparison of existing consistencytype models.</p>
<p>Create a table that compares different models focusing on their purpose, the trajectory they follow, the main objects they equate, and their methodological approach.The table should provide detailed insights into how each model addresses consistency issues, drawing from specified papers.</p>
<p>Figure 6 :
6
Figure 6: Average performance scores of four backbone LLMs across four different methods.The comparison highlights the consistent improvement of our proposed method over existing baselines and prior work.</p>
<p>Figure 7 :
7
Figure 7: Distribution of number of papers in each table.</p>
<p>Figure 8 :
8
Figure 8: The annotation interface we used for collecting the gold labels for distractor papers.</p>
<p>Table induction for general domains Other than the scientific domain, table induction is also widely studied as text-to-table generation.Prior works attempt this as a sequence-to-sequence task</p>
<p>Table 1 :
1
Moving forward, we associate distractor paper candidates with each table to simulate a potentially noisy document pool before constructing the table.Ideally, distractor candidates should be semantically related to the 1XPEHURI5HWULHYHG3DSHUV 5HWULHYDO3UHFLVLRQ 5HWULHYDO5HFDOO Figure 3: Precision and recall curves for different numbers of retrieved papers.Overlap statistics between prompts (the original caption or our constructed user demand) and table content (schema or values).#Table: Number (and %) of tables with at least one token from table content overlapping with the prompt.#Tokens: Average count of overlapping tokens between table content and prompt.
PromptContent #Table ↓ #Tokens ↓CaptionSchema 101 (5.2%) Value 46 (2.4%)1.2 1.3User DemandSchema 14 (0.7%) Value 8 (0.4%)1.0 1.0</p>
<p>Table Generated Table 18 table schema :
Generated18schema
Is
CBFIR NetworksDatasetsMetricsLossGround-truthGAN CN-LexNet ResNet-v2DARN Shopping100K DeepFashionRecall@1 Recall@20 Recall@1,10 BCE Loss TL, AL CL, TLunary (cell) values:pairwise comparisons:DatasetIs CL, TL the lossIs ResNet using moreincluded in thefunction for paperevaluation metricstable schema?CN-LexNet?than GAN?Backbone Model LossesAttributes DatasetsGANTL+ALShapeDARN ColorCNLexNetCL+TLVariousConsumer-to-ShopResNetLandmark VariousDeepFashion</p>
<p>Table 2 :
2
, DeepSeek-V3 (685B;DeepSeek-AI et al., 2024), LLAMA-Tabular evaluation results (%) of five LLMs on the ARXIV2TABLE.The best performances within each backbone are underlined and the best among all backbones are bold-faced.Avg refers to averaging three F1 scores.
Backbone ModelMethodPaperSchemaUnary ValuePairwise ValueAvgRecallPRF1PRF1PRF1Baseline 152.831.3 37.7 34.2 29.6 40.4 34.2 28.4 31.8 30.0 32.8LLAMA-3.3 (70B)Baseline 2 Newman et al.65.4 61.926.7 69.3 38.5 17.0 56.8 26.2 11.2 22.5 15.0 26.6 36.4 40.5 38.3 32.8 44.5 37.8 29.5 30.2 29.8 35.3Ours69.341.9 55.4 47.7 43.1 62.6 51.1 36.4 46.9 41.0 46.6Baseline 154.733.1 34.5 33.8 31.6 30.4 31.0 15.5 24.7 19.0 27.9Mistral-Large (123B)Baseline 2 Newman et al.66.8 67.927.4 65.0 38.5 22.7 47.4 30.7 17.8 30.7 22.6 30.6 39.9 41.6 40.7 34.7 46.3 39.7 29.9 35.1 32.3 37.6Ours71.345.4 56.7 50.4 43.3 61.5 50.8 42.0 49.2 45.3 48.8Baseline 157.538.7 41.7 40.1 32.5 43.8 37.3 28.7 31.8 30.1 35.8DeepSeek-V3 (685B)Baseline 2 Newman et al.69.8 70.934.9 69.0 46.4 27.1 55.5 36.4 25.7 32.7 28.8 37.2 39.4 44.2 41.7 36.6 49.2 42.0 33.3 36.5 34.8 39.5Ours74.339.6 56.9 46.7 47.7 65.2 55.1 40.4 49.8 44.6 48.8Baseline 155.932.0 35.7 33.7 28.9 39.3 33.3 25.0 31.0 27.7 31.6GPT-4o-miniBaseline 2 Newman et al.68.2 69.331.5 67.7 43.0 27.7 50.8 35.9 21.6 28.3 24.5 34.5 40.3 45.9 42.9 38.3 47.5 42.4 35.0 37.8 36.3 40.5Ours72.646.5 59.7 52.3 49.0 66.7 56.5 43.5 51.9 47.3 52.0Baseline 158.535.8 43.2 39.2 36.9 41.8 39.2 29.0 34.7 31.6 36.7GPT-4oBaseline 2 Newman et al.70.2 71.334.2 68.0 45.5 27.9 56.0 37.2 19.4 33.6 24.6 35.8 45.0 47.9 46.4 38.7 49.8 43.6 36.9 40.0 38.4 42.8Ours74.651.5 59.4 55.2 46.1 66.7 54.5 45.9 55.7 50.3 53.3</p>
<p>Table 3 :
3
Expert acceptance rate for the synthesized QA pairs sampled from our evaluations.</p>
<p>Given a literature review table, along with its caption, you are tasked with writing a user demand or intention for the creator of this table. The user demand should be written as though you are instructing an AI system to generate the table. Avoid directly mentioning column names in the table itself, but instead, focus on explaining why the table is needed and what information it should contain. You may include a description of the table's structure, whether it requires detailed or summarized columns. Additionally, infer the user's intentions from the titles of the papers the table will include. Limit each user demand to 1-2 sentences. Examples of good user demands are: I need a table that outlines how each study conceptualizes the problem, categorizes the task, describes the data analyzed, and summarizes the main findings. The table should have detailed columns for each of these aspects. Generate a detailed table comparing the theoretical background, research methodology, and key results of these papers. You can use several columns to capture these aspects for each paper. I want to create a table that summarizes the datasets used to evaluate different GNN models, focusing on the common features and characteristics found across the papers listed below. The table should have concise columns to highlight these dataset attributes. Now, write a user demand for the table below. The caption of the table is "<CAPTION>". The table looks like this: <TABLE> The following papers are included in the table: <PAPER-1> . . . <PAPER-N> Write the user demand for this table. Do not include the column names in the user demand. Write a concise and clear user demand covering the function, topic, and structure of the table with one or two sentences. The user demand is:</p>
<p>Then, for synthesizing QA pairs from a table, we use the following prompt to guide GPT-4o in generating some QA pairs with answers:
You</p>
<p>will evaluate the quality of a generated table by comparing it against a ground-truth table. The goal is to assess whether the generated table correctly retains the schema, individual values, and pairwise relationships. This is achieved by generating targeted QA pairs based on the ground-truth table and answering them using the generated table. Step 1: QA Pair Generation Based on the Ground-Truth Table Generate binary (Yes/No) QA pairs focusing on three aspects: Schema QA Pairs: Check whether a specific column from the ground-truth table appears in the generated table schema. Example: Is Dataset included in the table schema? Unary Value QA Pairs: Check whether a specific cell value from the ground-truth table is present in the generated table. Example: Is CL, TL the loss function for paper CN-LexNet? Pairwise Value QA Pairs: Check whether a relationship between two values remains consistent in the generated table. Example: Is ResNet-v2 using more evaluation metrics than GAN? For Schema and Unary Value, generate a QA pair for every column and every cell, respectively. For Pairwise Value, randomly sample 10 pairs per table and construct the corresponding QA pairs.</p>
<p>Table 5 :
5
Summary statistics of the ARXIV2TABLE benchmark.We report aggregate values for the number of papers, columns, and distractor papers per table.
Step 2:</p>
<p>Answering QA Pairs Using the Generated Table After generating the QA pairs, answer them using the generated table. Provide only "yes" or "no" responses: If the information is present in the generated table, respond with "yes." If the information is missing or different, respond with "no." Your task is to generate the QA pairs based on the ground-truth table and then answer them based on the generated table. Now, begin by generating the QA pairs.</p>
<p>Table 6 :
6
Computational cost and efficiency metrics across different generation methods using LLaMA-3.3.
MethodSuccess Rate #Tokens Avg. RuntimeBaseline 148.2%128K37Baseline 298.2%167K118Newman et al.99.7%110K208Ours100.0%118K194
We report the generation success rate, average token usage, and average runtime (s) per table.</p>
<p>Table 7 presents randomly sampled examples of original table captions alongside their improved user demands, demonstrating how refining vague captions enhances specificity and ensures more structured table generation.The findings highlight that well-defined user demands help capture key aspects of table construction, leading to more informative and targeted tabular representations.</p>
<p>Table 7 :
7
Randomly sampled examples of the original captions and their corresponding improved user demands.Most captions are relatively short and may be vague without the full table's content.
SchemaUnary ValuePairwise ValueIs Dataset included in the table schema?Is CL, TL the loss function for paper CN-Is ResNet-v2 using more evaluation met-LexNet?rics than GAN?Is Model Architecture included in the tableIs GPT-4o the model used for multimodalDoes GPT-4o have a larger parameter sizeschema?understanding?than LLaMA-2?Is Training Dataset included in the tableIs ImageNet the dataset used for trainingIs ResNet trained on more samples thanschema?ResNet?EfficientNet?Is Performance Metric included in the ta-Is BLEU-4 the evaluation metric for MT-Does BERT outperform LSTM on BLEU-ble schema?BERT?4 score?Is Activation Function included in the tableIs ReLU the activation function used inIs GELU smoother than ReLU in functionschema?Transformer?continuity?Is Optimization Algorithm included in theIs Adam the optimizer used for trainingDoes Adam converge faster than SGD fortable schema?BERT?BERT training?Is Pretraining Task included in the tableIs Masked Language Modeling the pre-Does BERT use a more complex pretrain-schema?training task for BERT?ing strategy than GPT?Is Hyperparameter included in the tableIs the learning rate set to 0.001 for trainingDoes ViT use a higher learning rate thanschema?ViT?ResNet?Is Hardware Accelerator included in theIs TPU used for training T5?Do TPUs provide faster training thantable schema?GPUs for T5?</p>
<p>Table 8 :
8
Randomly sampled examples of schema, unary value, and pairwise value questions used to evaluate the quality of generated tables.Each row contains three related questions derived from the same table.</p>
<p>https://platform.openai.com/
Shuo Zhang and Krisztian Balog. 2018. On-the-fly table generation. In The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018, pages 595-604. ACM.
AcknowledgmentsWe thank the JHU CLSP community for discussions and inspiration and HKUST KnowComp community for helping with data annotations.The authors of this paper were supported by the NSFC Fund (U20B2053) from the NSFC of China, the RIF (R6020-19 and R6021-20), the GRF (16211520 and 16205322) from RGC of Hong Kong and ONR grant (N0001424-1-2089).
ASPECTNEWS: aspect-oriented summarization of news documents. Ojas Ahuja, Jiacheng Xu, Akshay Gupta, Kevin Horecka, Greg Durrett ; Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z Z Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang, 10.18653/V1/2022.ACL-LONG.449arXiv:2501.12948Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational Linguistics2022. 2025PreprintDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</p>
<p>. Deepseek-Ai , Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J L Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R J Chen, R L Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S S Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, T Shuting Pan, Tao Wang, Yun, 10.48550/ARXIV.2412.1943719437Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. 2024. Deepseek-v3 technical report. CoRR, abs/2412</p>
<p>Investigating data contamination in modern benchmarks for large language models. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan, 10.18653/V1/2024.NAACL-LONG.482Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024a. June 16-21, 20241NAACL 2024</p>
<p>Text-tuple-table: Towards information integration in text-to-table generation via global tuple extraction. Zheye Deng, Chunkit Chan, Weiqi Wang, Yuxi Sun, Wei Fan, Tianshi Zheng, Yauwai Yim, Yangqiu Song, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024b. 2024. November 12-16, 2024</p>
<p>Ms\ˆ2: Multidocument summarization of medical studies. Jay Deyoung, Iz Beltagy, Madeleine Van Zuylen, Bailey Kuehl, Lucy Lu, Wang , 10.18653/V1/2021.EMNLP-MAIN.594Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican Republic2021</p>
<p>Generalization or memorization: Data contamination and trustworthy evaluation for large language models. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, Ge Li, 10.18653/V1/2024.FINDINGS-ACL.716Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 2024and virtual meeting</p>
<p>. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Rozière, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, David Livshits, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Dieuwke Perino, Egor Hupkes, Ehab Lakomkin, Elina Albadawy, Emily Lobanova, Eric Michael Dinan, Filip Smith, Frank Radenovic, Gabriel Zhang, Gabrielle Synnaeve, Georgia Lee, Graeme Lewis Anderson, Grégoire Nail, Guan Mialon, Guillem Pang, Hailey Cucurell, Hannah Nguyen, Hu Korevaar, Hugo Xu, Iliyan Touvron, Zarov, Arrieta Imanol, Isabel M Ibarra, Ishan Kloumann, Ivan Misra, Jana Evtimov, Jason Vranes, Jay Park, Jeet Mahadeokar, Jelmer Shah, Jennifer Van Der Linde, Jenny Billock, Jenya Hong, Jeremy Lee, Jianfeng Fu, Jianyu Chi, Jiawen Huang, Jie Liu, Jiecao Wang, Joanna Yu, Joe Bitton, Jongsoo Spisak, Joseph Park, Joshua Rocca, Joshua Johnstun, Junteng Saxe, Jia, 10.48550/ARXIV.2407.21783Jan Geffert,Kartikeya UpasaniJade Copet, Jaewon Lee; Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stoneand et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783</p>
<p>Measuring nominal scale agreement among many raters. Joseph L Fleiss, Psychological bulletin. 7653781971</p>
<p>Automatic generation of review matrices as multi-document summarization of scientific papers. Hayato Hashimoto, Kazutoshi Shinoda, Hikaru Yokono, Akiko Aizawa, Proceedings of the 2nd Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2017) co-located with the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017). the 2nd Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2017) co-located with the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017)Tokyo, Japan2017. August 11, 2017CEUR Workshop Proceedings</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P Parikh, Chris Alberti, Danielle Epstein, 10.1162/TACL_A_00276Trans. Assoc. Comput. Linguistics. Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov72019</p>
<p>The measurement of observer agreement for categorical data. Richard Landis, Gary G Koch, biometrics. 1977</p>
<p>QASA: advanced question answering on scientific articles. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, Moontae Lee, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202of Proceedings of Machine Learning Research</p>
<p>Tablebank: Table benchmark for image-based table detection and recognition. Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, Zhoujun Li, Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020. The 12th Language Resources and Evaluation Conference, LREC 2020Marseille, FranceEuropean Language Resources Association2020. May 11-16, 2020</p>
<p>A sequenceto-sequence&amp;set model for text-to-table generation. Tong Li, Zhihao Wang, Liangying Shao, Xuling Zheng, Xiaoli Wang, Jinsong Su, 10.18653/V1/2023.FINDINGS-ACL.330Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023</p>
<p>Position: Data authenticity, consent, &amp; provenance for AI are all broken: what will it take to fix them?. Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, Katy Ilonka Gero, Alex Pentland, Jad Kabbara, Fortyfirst International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024</p>
<p>SCITAB: A challenging benchmark for compositional reasoning and claim verification on scientific tables. Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, Min-Yen Kan, 10.18653/V1/2023.EMNLP-MAIN.483Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Multixscience: A large-scale dataset for extreme multidocument summarization of scientific articles. Yao Lu, Yue Dong, Laurent Charlin, 10.18653/V1/2020.EMNLP-MAIN.648Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020. 2020. November 16-20, 2020</p>
<p>Scigen: a dataset for reasoning-aware text generation from scientific tables. Mistral-Ai , Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 20212024. 2021. December 2021Mistral AI Blog. Nafise Sadat Moosavi, Andreas Rücklé, Dan Roth, and Iryna Gurevych</p>
<p>Arxivdigestables: Synthesizing scientific literature into tables using language models. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Daniel S Weld, Joseph Chee Chang, Kyle Lo, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. 2024. November 12-16, 2024</p>
<p>Gpt-4o mini: advancing cost-efficient intelligence. 2024aOpenAIOpenAI</p>
<p>Hello gpt-4o. OpenAI. OpenAI. 2025a. Introducing deep research. 2024bOpenAI BlogOpenAI</p>
<p>Openai o3-mini. 2025bOpenAI BlogOpenAI</p>
<p>Setting the table with intent: Intent-aware schema generation and editing for literature review tables. Joseph Chee Vishakh Padmakumar, Kyle Chang, Doug Lo, Aakanksha Downey, Naik, 2025To appear</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, 10.18653/V1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>The cost structure of sensemaking. M Daniel, Mark Russell, Peter Stefik, Stuart K Pirolli, Card, 10.1145/169059.169209Human-Computer Interaction, INTERACT '93, IFIP TC13 International Conference on Human-Computer Interaction. Amsterdam, The NetherlandsACM1993. April 1993jointly organised with ACM Conference on Human Aspects in Computing Systems CHI'93</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, 10.1609/AAAI.V38I17.29872Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. Vancouver, CanadaAAAI Press2024. February 20-27, 20242014</p>
<p>Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. Anirudh Sundar, Christopher Richardson, Larry Heck, 10.48550/ARXIV.2309.08963CoRR, abs/2309.089632024. 2023Struc-bench: Are large language models really good at generating complex structured data?</p>
<p>Scidasynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model. Xingbo Wang, Samantha L Huey, Rui Sheng, Saurabh Mehta, Fei Wang, 10.48550/ARXIV.2404.13765CoRR, abs/2404.137652024</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, 10.18653/V1/2020.EMNLP-DEMOS.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -Demos. the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -DemosOnlineAssociation for Computational Linguistics2020. November 16-20, 2020</p>
<p>Textto-table: A new way of information extraction. Xueqing Wu, Jiacheng Zhang, Hang Li, 10.18653/V1/2022.ACL-LONG.180Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>            </div>
        </div>

    </div>
</body>
</html>
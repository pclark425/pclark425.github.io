<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6451 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6451</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6451</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-268357759</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.06221v1.pdf" target="_blank">TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision</a></p>
                <p><strong>Paper Abstract:</strong> Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise. Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6451.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6451.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TRAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thought Retrieval and Aligned Decision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent framework that augments few-shot in-context LLM decision making with a step-wise, thought-based retrieval memory and a temporally aligned demonstration augmentation to tolerate imperfect thoughts and reduce irrelevant prompt noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TRAD</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>TRAD first prepares a 'thought-enhanced' memory by labeling each expert demonstration step with a pseudo-golden Chain-of-Thought. At inference it (1) optionally retrieves trajectories by task meta-data to generate a current-step thought, (2) performs step-wise retrieval from the thought-enhanced memory using the generated thought as a dense embedding query, and (3) forms action-prediction prompts by temporally expanding retrieved steps, marking relative order ([Step -1, 0, +1]) and aligning history to provide context for the LLM's action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (ALFWorld experiments), GPT-3.5-turbo (Mind2Web experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld (text-based householding game) and Mind2Web (web navigation/text-based HTML interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval memory (thought-enhanced memory storing step-level demonstrations and thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Each memory entry is an expert trajectory step augmented with a pseudo-golden 'thought' (a textual Chain-of-Thought abstraction), plus the associated observation, action, and task instruction; keys are the thought texts (encoded into embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Offline preprocessing: a small set of human-labeled thought examples are used as few-shot prompts to an LLM which labels (writes) pseudo-golden thoughts for all demonstration steps; memory is static at inference (no learned write network reported).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Two-stage retrieval: (A) optional trajectory-wise retrieval by task meta-data to provide context for generating a current-step thought; (B) step-wise dense retrieval using embeddings (e.g., Sentence-BERT or DPR encoders) of the generated thought as query and cosine similarity nearest-neighbor search over precomputed thought embeddings, collecting top-K steps from different trajectories. Retrieved steps are then temporally expanded and order-marked before prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Tuning-free few-shot in-context prompting (no fine-tuning); expert trajectories collected offline and used as demonstration memory.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ALFWorld: task success rate (SR); Mind2Web: element accuracy (Ele.Acc), step success rate (Step SR), trajectory success rate (SR); real-world deployment: Step SR and SR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld: average success rate 96.77% (TRAD using thought-enhanced memory and aligned decision). Mind2Web: reported overall improvements over Synapse of +2.1% in element accuracy, +1.4% in step SR, +0.5% in trajectory SR. Real-world deployment: Step SR improved from 90.2% to 98.1% and SR from 65.0% to 92.5% when using TRAD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Ablations examined components of the aligned decision module and retrieval hyperparameters: (1) Removing temporal expansion (TE) causes the largest drop — TE is critical to tolerate imperfect inference-time thoughts; (2) Relative order mark (ROM) and history alignment (HA) help particularly on out-of-distribution/generalization subsets (Cross-Website, Cross-Domain) but may be neutral or slightly harmful on in-distribution tasks; (3) Temporal expansion horizon: forward (future) expansion generally helps more than backward expansion, with diminishing or negative returns for very large horizons; best reported configuration was forward expansion F=2, backward B=0 for Mind2Web; (4) Retrieval size K has a mild effect and TRAD advantage over Synapse is stable for K in {1,2,3,4}; (5) TRAD w/o TE (i.e., thought retrieval without temporal expansion) still outperforms simple ReAct baselines but lags the full TRAD.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Dependence on quality of thoughts (requires LLM that can produce meaningful abstractions); trade-off in temporal expansion (more expansion increases tolerance to imperfect thoughts but also adds noisy irrelevant context); still constrained by LLM context limits; imperfect thoughts can harm retrieval if not mitigated — aligned decision reduces but does not eliminate this issue.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Prepare pseudo-golden thoughts for memory steps via few-shot LLM labeling; use dense embedding retrieval on thought texts (Sentence-BERT/DPR) with cosine similarity to achieve precise step-level retrieval; prefer limited forward temporal expansion (e.g., +2) to tolerate imperfect thoughts while avoiding excessive noise; include relative order marks and limited history alignment when generalization to OOD tasks is required; keep K (retrieved demonstrations) modest (1–4) as results are stable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6451.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6451.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synapse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline retrieval-augmented LLM agent that retrieves full expert trajectories by task meta-data and uses them as in-context examples for the LLM to make decisions; used in this paper as the main state-of-the-art baseline for trajectory-level retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Synapse (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieves relevant expert trajectories using task meta-data and prompts the LLM with whole trajectories (trajectory-wise retrieval) as few-shot examples for decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used on Mind2Web and ALFWorld in comparisons (as implemented by authors of TRAD based on the Synapse method).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>trajectory-wise external memory (stores whole expert trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Full trajectories (sequences of observations, actions) indexed by task meta-data; no per-step Chain-of-Thoughts prepared in this paper's usage of Synapse (contrasted with TRAD's thought-enhanced memory).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Not described in detail here (retrieval over stored trajectories by task meta-data); treated as an external database of expert trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Trajectory-level retrieval by task meta-data (as used by Zheng et al. [46]; exact retrieval implementation not detailed in this paper), then prompting with full retrieved trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Tuning-free in-context retrieval + prompting (as used as baseline in TRAD experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same metrics as TRAD comparisons: ALFWorld success rate; Mind2Web element accuracy, step SR, SR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as baseline: on ALFWorld Synapse ~89.55% average success rate (paper reports TRAD 96.77% vs Synapse 89.55%); on Mind2Web Synapse is the main baseline and TRAD reports modest but consistent improvements over Synapse (e.g., TRAD improvements of +2.1% Ele.Acc, +1.4% Step SR, +0.5% SR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>In TRAD's analysis, trajectory-wise retrieval (Synapse-style) can produce plausible-but-misleading examples when task meta-data matches but underlying state-transition dynamics differ; prompting with full trajectories increases prompt length and irrelevant context which can harm LLM decision making and exceed context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Trajectory-wise retrieval yields plausible but irrelevant examples when tasks require different state-transition dynamics (plausible examples can mislead agents); long prompts from full trajectories cause context-window issues and add irrelevant information that LLMs can copy, harming performance; not tolerant to domain shifts when retrieved trajectories are superficially similar but not structurally relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>When using trajectory-wise retrieval, be cautious of plausible examples and context length; the paper suggests step-level retrieval using richer local abstractions (thoughts) is preferable for sequential decision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6451.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6451.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reason+Act)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM agent prompting style that interleaves reasoning (Chain-of-Thought style 'thought' outputs) and concrete actions, used in this paper as a baseline (with fixed or random demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct (baseline variants: Fixed, Random, Relevant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ReAct produces stepwise thoughts and actions in a reason-then-act loop. In this paper it is evaluated with (a) fixed human-written trajectories as demonstrations, (b) random demonstrations, or (c) random steps from Synapse-retrieved trajectories (to match prompt formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld and Mind2Web as baselines in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>no explicit external memory (uses fixed in-prompt demonstrations or randomly sampled in-context examples rather than an indexed memory store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Tuning-free few-shot in-context prompting with Chain-of-Thought style outputs (no memory index used in standard ReAct as evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ALFWorld success rate; Mind2Web metrics (Ele.Acc, Step SR, SR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>ReAct with fixed demonstrations performed worse on some ALFWorld subtasks (e.g., Put), showing that simply providing fixed or random demonstrations is insufficient when task dynamics differ; ReAct combined with Synapse trajectories (Synapse+ReAct) improved results but still underperformed TRAD.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Using fixed demonstrations can lead to incorrect actions when demonstrations are not closely relevant; single-step prompting in ReAct (without history) may be insufficient for complex or OOD tasks; ReAct is sensitive to the quality and relevance of demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Combine reasoning outputs with retrieval of relevant demonstrations; ensure demonstrations are step-relevant (not merely trajectory-wise) and consider temporal context when tasks require history dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>Mind2Web: Towards a Generalist Agent for the Web <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 2)</em></li>
                <li>Generative Agents: Interactive Simulacra of Human Behavior <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6451",
    "paper_id": "paper-268357759",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "TRAD",
            "name_full": "Thought Retrieval and Aligned Decision",
            "brief_description": "An LLM-agent framework that augments few-shot in-context LLM decision making with a step-wise, thought-based retrieval memory and a temporally aligned demonstration augmentation to tolerate imperfect thoughts and reduce irrelevant prompt noise.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
            "agent_name": "TRAD",
            "agent_description": "TRAD first prepares a 'thought-enhanced' memory by labeling each expert demonstration step with a pseudo-golden Chain-of-Thought. At inference it (1) optionally retrieves trajectories by task meta-data to generate a current-step thought, (2) performs step-wise retrieval from the thought-enhanced memory using the generated thought as a dense embedding query, and (3) forms action-prediction prompts by temporally expanding retrieved steps, marking relative order ([Step -1, 0, +1]) and aligning history to provide context for the LLM's action prediction.",
            "model_name": "GPT-4 (ALFWorld experiments), GPT-3.5-turbo (Mind2Web experiments)",
            "model_size": null,
            "benchmark_name": "ALFWorld (text-based householding game) and Mind2Web (web navigation/text-based HTML interaction)",
            "memory_used": true,
            "memory_type": "external retrieval memory (thought-enhanced memory storing step-level demonstrations and thoughts)",
            "memory_representation": "Each memory entry is an expert trajectory step augmented with a pseudo-golden 'thought' (a textual Chain-of-Thought abstraction), plus the associated observation, action, and task instruction; keys are the thought texts (encoded into embeddings).",
            "memory_update_mechanism": "Offline preprocessing: a small set of human-labeled thought examples are used as few-shot prompts to an LLM which labels (writes) pseudo-golden thoughts for all demonstration steps; memory is static at inference (no learned write network reported).",
            "memory_retrieval_method": "Two-stage retrieval: (A) optional trajectory-wise retrieval by task meta-data to provide context for generating a current-step thought; (B) step-wise dense retrieval using embeddings (e.g., Sentence-BERT or DPR encoders) of the generated thought as query and cosine similarity nearest-neighbor search over precomputed thought embeddings, collecting top-K steps from different trajectories. Retrieved steps are then temporally expanded and order-marked before prompting.",
            "training_method": "Tuning-free few-shot in-context prompting (no fine-tuning); expert trajectories collected offline and used as demonstration memory.",
            "evaluation_metric": "ALFWorld: task success rate (SR); Mind2Web: element accuracy (Ele.Acc), step success rate (Step SR), trajectory success rate (SR); real-world deployment: Step SR and SR.",
            "performance_with_memory": "ALFWorld: average success rate 96.77% (TRAD using thought-enhanced memory and aligned decision). Mind2Web: reported overall improvements over Synapse of +2.1% in element accuracy, +1.4% in step SR, +0.5% in trajectory SR. Real-world deployment: Step SR improved from 90.2% to 98.1% and SR from 65.0% to 92.5% when using TRAD.",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "ablation_findings": "Ablations examined components of the aligned decision module and retrieval hyperparameters: (1) Removing temporal expansion (TE) causes the largest drop — TE is critical to tolerate imperfect inference-time thoughts; (2) Relative order mark (ROM) and history alignment (HA) help particularly on out-of-distribution/generalization subsets (Cross-Website, Cross-Domain) but may be neutral or slightly harmful on in-distribution tasks; (3) Temporal expansion horizon: forward (future) expansion generally helps more than backward expansion, with diminishing or negative returns for very large horizons; best reported configuration was forward expansion F=2, backward B=0 for Mind2Web; (4) Retrieval size K has a mild effect and TRAD advantage over Synapse is stable for K in {1,2,3,4}; (5) TRAD w/o TE (i.e., thought retrieval without temporal expansion) still outperforms simple ReAct baselines but lags the full TRAD.",
            "reported_limitations": "Dependence on quality of thoughts (requires LLM that can produce meaningful abstractions); trade-off in temporal expansion (more expansion increases tolerance to imperfect thoughts but also adds noisy irrelevant context); still constrained by LLM context limits; imperfect thoughts can harm retrieval if not mitigated — aligned decision reduces but does not eliminate this issue.",
            "best_practices_recommendations": "Prepare pseudo-golden thoughts for memory steps via few-shot LLM labeling; use dense embedding retrieval on thought texts (Sentence-BERT/DPR) with cosine similarity to achieve precise step-level retrieval; prefer limited forward temporal expansion (e.g., +2) to tolerate imperfect thoughts while avoiding excessive noise; include relative order marks and limited history alignment when generalization to OOD tasks is required; keep K (retrieved demonstrations) modest (1–4) as results are stable.",
            "uuid": "e6451.0",
            "source_info": {
                "paper_title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Synapse",
            "name_full": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
            "brief_description": "A baseline retrieval-augmented LLM agent that retrieves full expert trajectories by task meta-data and uses them as in-context examples for the LLM to make decisions; used in this paper as the main state-of-the-art baseline for trajectory-level retrieval.",
            "citation_title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
            "mention_or_use": "use",
            "paper_title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
            "agent_name": "Synapse (baseline)",
            "agent_description": "Retrieves relevant expert trajectories using task meta-data and prompts the LLM with whole trajectories (trajectory-wise retrieval) as few-shot examples for decision making.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Used on Mind2Web and ALFWorld in comparisons (as implemented by authors of TRAD based on the Synapse method).",
            "memory_used": true,
            "memory_type": "trajectory-wise external memory (stores whole expert trajectories)",
            "memory_representation": "Full trajectories (sequences of observations, actions) indexed by task meta-data; no per-step Chain-of-Thoughts prepared in this paper's usage of Synapse (contrasted with TRAD's thought-enhanced memory).",
            "memory_update_mechanism": "Not described in detail here (retrieval over stored trajectories by task meta-data); treated as an external database of expert trajectories.",
            "memory_retrieval_method": "Trajectory-level retrieval by task meta-data (as used by Zheng et al. [46]; exact retrieval implementation not detailed in this paper), then prompting with full retrieved trajectories.",
            "training_method": "Tuning-free in-context retrieval + prompting (as used as baseline in TRAD experiments).",
            "evaluation_metric": "Same metrics as TRAD comparisons: ALFWorld success rate; Mind2Web element accuracy, step SR, SR.",
            "performance_with_memory": "Reported as baseline: on ALFWorld Synapse ~89.55% average success rate (paper reports TRAD 96.77% vs Synapse 89.55%); on Mind2Web Synapse is the main baseline and TRAD reports modest but consistent improvements over Synapse (e.g., TRAD improvements of +2.1% Ele.Acc, +1.4% Step SR, +0.5% SR).",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "ablation_findings": "In TRAD's analysis, trajectory-wise retrieval (Synapse-style) can produce plausible-but-misleading examples when task meta-data matches but underlying state-transition dynamics differ; prompting with full trajectories increases prompt length and irrelevant context which can harm LLM decision making and exceed context limits.",
            "reported_limitations": "Trajectory-wise retrieval yields plausible but irrelevant examples when tasks require different state-transition dynamics (plausible examples can mislead agents); long prompts from full trajectories cause context-window issues and add irrelevant information that LLMs can copy, harming performance; not tolerant to domain shifts when retrieved trajectories are superficially similar but not structurally relevant.",
            "best_practices_recommendations": "When using trajectory-wise retrieval, be cautious of plausible examples and context length; the paper suggests step-level retrieval using richer local abstractions (thoughts) is preferable for sequential decision tasks.",
            "uuid": "e6451.1",
            "source_info": {
                "paper_title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reason+Act)",
            "brief_description": "An LLM agent prompting style that interleaves reasoning (Chain-of-Thought style 'thought' outputs) and concrete actions, used in this paper as a baseline (with fixed or random demonstrations).",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "mention_or_use": "use",
            "paper_title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
            "agent_name": "ReAct (baseline variants: Fixed, Random, Relevant)",
            "agent_description": "ReAct produces stepwise thoughts and actions in a reason-then-act loop. In this paper it is evaluated with (a) fixed human-written trajectories as demonstrations, (b) random demonstrations, or (c) random steps from Synapse-retrieved trajectories (to match prompt formatting).",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "ALFWorld and Mind2Web as baselines in comparisons",
            "memory_used": false,
            "memory_type": "no explicit external memory (uses fixed in-prompt demonstrations or randomly sampled in-context examples rather than an indexed memory store)",
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Tuning-free few-shot in-context prompting with Chain-of-Thought style outputs (no memory index used in standard ReAct as evaluated).",
            "evaluation_metric": "ALFWorld success rate; Mind2Web metrics (Ele.Acc, Step SR, SR).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "ablation_findings": "ReAct with fixed demonstrations performed worse on some ALFWorld subtasks (e.g., Put), showing that simply providing fixed or random demonstrations is insufficient when task dynamics differ; ReAct combined with Synapse trajectories (Synapse+ReAct) improved results but still underperformed TRAD.",
            "reported_limitations": "Using fixed demonstrations can lead to incorrect actions when demonstrations are not closely relevant; single-step prompting in ReAct (without history) may be insufficient for complex or OOD tasks; ReAct is sensitive to the quality and relevance of demonstrations.",
            "best_practices_recommendations": "Combine reasoning outputs with retrieval of relevant demonstrations; ensure demonstrations are step-relevant (not merely trajectory-wise) and consider temporal context when tasks require history dependence.",
            "uuid": "e6451.2",
            "source_info": {
                "paper_title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
            "rating": 2,
            "sanitized_title": "synapse_trajectoryasexemplar_prompting_with_memory_for_computer_control"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Mind2Web: Towards a Generalist Agent for the Web",
            "rating": 2,
            "sanitized_title": "mind2web_towards_a_generalist_agent_for_the_web"
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "Generative Agents: Interactive Simulacra of Human Behavior",
            "rating": 1,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        }
    ],
    "cost": 0.014974749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision
10 Mar 2024</p>
<p>Ruiwen Zhou 
Ying Wen ying.wen@sjtu.edu.cn </p>
<p>Shanghai Jiao Tong University Shanghai
China Yingxuan Yang</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Muning Wen</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Shanghai Jiao Tong University Shanghai
Wenhao WangChina</p>
<p>China Pacific Insurance Shanghai
China</p>
<p>Chunling Xi</p>
<p>China Pacific Insurance Shanghai
China</p>
<p>Guoqiang Xu</p>
<p>China Pacific Insurance Shanghai
China</p>
<p>Yong Yu</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Weinan Zhang</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision
10 Mar 202490F64BF8808BD31A107C0E1C182A221DarXiv:2403.06221v1[cs.AI]Large Language ModelLLM AgentSequential Decision MakingLLM ReasoningInformation Retrieval
Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability.Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples.Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks.However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context.In this paper, we propose a novel framework (TRAD) to address these issues.TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise.Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise.Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization.Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.Our codes are available at: https://github.com/skyriver-2000/TRAD-Official.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) [3,32] have achieved remarkable success on various tasks like question answering [45], chatbot [20], code synthesis [24], text ranking [7], table-based reasoning [43], and retrieval query expansion [17] due to their wide knowledge and excellent ability of text understanding and generation.Recently, a series of works have attempted to build powerful agents based on LLMs for various sequential decision-making tasks, including text-based games [41], online shopping [40], web navigation [4], and information retrieval [48].</p>
<p>Among existing LLM agents, some are trained with large-scale expert data by supervised fine-tuning (SFT) [8,9,18], while some are tuning-free and utilize in-context learning (ICL) with few expert demonstration examples [13,34,42,46].In this paper, we focus the scope on tuning-free ICL methods, as they are highly costeffective and can seamlessly generalize to different tasks using only a small amount of expert samples.Most existing ICL-based agents are prompted with expert trajectories carefully selected by human [28,38,42], which work well when few expert trajectories are available.However, when we have access to a large dataset of expert trajectories or an expert policy, the automatic and personalized selection of expert trajectories for each task instruction becomes necessary, and can have an essential influence on task performance.</p>
<p>Recently, Zheng et al. [46] study the problem of demonstration selection and propose Synapse, which retrieves relevant expert trajectories by task meta-data, and then prompts LLMs with these retrieved trajectories.Synapse performs well on computer control tasks (MiniWob++ [27]) and web navigation tasks (Mind2Web [4]).Nevertheless, retrieving and prompting with complete trajectories can be problematic in the following three aspects.</p>
<p>Plausible examples.Sometimes generalization to data from various domains can be critical.For example, in cross-website and cross-domain subsets of Mind2Web, agents operate on websites unseen in the training set, i.e., memory.In this case, retrieving trajectories with only task meta-data is very likely to provide plausible examples, which share similar task instructions to the current one but require totally different solutions.As shown by experiments in [46], plausible examples provide no more information than random examples and can usually mislead LLM agents to wrong decisions.</p>
<p>Thought-Enhanced Memory Thought</p>
<p>I am now in/on: armchair 1 Critical objects I have found: pillow 1 (armchair 1) pillow 2 (armchair 1) Objects I have taken: None Now I have found both pillows in/on armchair 1. Next, I need to take the first pillow and then put it in/on the sofa …… Figure 1: An overall illustration of TRAD agent (on ALFWorld [30] enviroment).TRAD first pre-processes expert trajectories, labeling each step with high-quality thoughts.At inference time, TRAD first conducts thought retrieval, which generates thought with trajectory-wise retrieved demonstrations as the query and keys for a more precise step-wise demonstration retrieval.Given the retrieved steps, TRAD employs aligned decision module to complement their temporally neighboring steps and corresponding position information (Fig. 2).Finally, the next action is generated according to the enhanced demonstration.</p>
<p>Context limit of LLMs.When facing tasks with long horizons and complex observations, prompting with complete trajectories will result in input sequences longer than the allowed length of LLMs.Synapse thus has to reduce the number of trajectory examples or even fail to complete the task directly.Though some long-context LLMs can receive very long prompts, the performance can be harmed due to the issue of long-term forgetting [31].</p>
<p>Irrelevant information in prompts.LLMs are found sensitive to their prompts, and can easily copy their recent input [11,22].The decision at the current timestep can be related to very few steps in a retrieved trajectory, while other steps do not provide any helpful information.Therefore, irrelevant steps will have unpredictable effects on the decision of LLM agents.As shown by our experiments, they negatively impact the performance most of the time.</p>
<p>To address the problems of trajectory-wise retrieval and prompting, we delve into step-wise demonstration retrieval and prompting.We discover that, via demonstrating with relevant steps, the input context of the LLM agent can be significantly reduced.Thus, the issue of context limit and irrelevant information can be alleviated.Therefore, the critical part is to retrieve step demonstrations that are truly relevant and helpful.To achieve this, we utilize step-bystep reasoning, i.e.Chain-of-Thought technique [38], to abstract the state at each timestep as retrieval queries and keys.The generated thoughts can involve historical information or future plans, which is more specific with state transitions and helpful in reducing plausible examples.</p>
<p>In this paper, we propose Thought Retrieval and Aligned Decision (TRAD), a novel framework that achieves step-wise demonstration retrieval via thought matching and enhances the context for action prediction with temporally neighboring steps and their order information.Our contribution can be summarized in four-folds:</p>
<p>• We propose a thought retrieval method, where we label thoughts for expert demonstration steps in advance with an LLM, prompt LLM agents to reason at inference time, and achieve step-wise retrieval by a similarity search on thought.To the best of our knowledge, this is the first work that enables the LLM agent with thought retrieval techniques for sequential decision-making.• Based on the thought retrieval operation, we further propose an aligned decision method, where we supply the retrieved steps with their temporal neighbors to overcome imperfect thoughts and enhance task-relevant information.• We conduct extensive experiments and analysis on Mind2Web [4] tasks and ALFWorld [30], showing that TRAD achieves state-ofthe-art (SoTA) performance compared to existing works.TRAD brings a 2.99% improvement over the strongest baseline (93.78% → 96.77%) to the success rate (SR) on ALFWorld.On Mind2Web, TRAD improves element accuracy, step SR, and SR remarkably over the powerful Synapse agent [46] by 2.1%, 1.4%, and 0.5%.</p>
<p>• We have deployed TRAD to the real-world robotic process automation scenarios of a global business insurance company, where TRAD enables the LLM agent to significantly improve the success rate in a bunch of practical tasks.In average, TRAD raises step SR from 90.2% to 98.1% and SR from 65.0% to 92.5%.</p>
<p>RELATED WORK 2.1 LLM Agents</p>
<p>In recent years, there has been a rapidly growing trend to utilize pre-trained LLMs as the central controller to obtain human-level decision-making capabilities [35].Among these works: Nakano et al. [18] fine-tune the GPT-3 [3] model for question answering in a text-based web browsing environment.Yao et al. [40] develop WebShop, a simulated e-commerce website environment, and finetune a BERT [5] model with imitation learning and reinforcement learning.Yao et al. [42] insert a reasoning section between observation input and action output, significantly improving the performance on ALFWorld [30] and WebShop [40] tasks.Shinn et al. [28] further improve over [42] via verbally reflecting on linguistic task feedback signals.Schick et al. [26] teach LLMs to use external tools via simple APIs in a self-supervised learning way.Park et al. [21] introduce Generative Agents, extending LLMs with natural language memories and retrieving them dynamically to plan behavior.Wang et al. [37] propose DEPS, an interactive planning approach, which facilitates better error correction by integrating a description of the plan execution process and an explanation of failure feedback.Wang et al. [34] employ an exploration curriculum, a growing skill library, and a novel iterative prompting mechanism, leading to better proficiency in playing Minecraft.Deng et al. [4] construct the Mind2Web dataset from real-world webpages, which consists of three subsets requiring different degrees of generalization, and compare the performance of imitation learning and few-shot inference.</p>
<p>As can be seen above, most existing LLM agents focus on: 1) improving task performance by direct fine-tuning [4,18,40]; 2) enhancing planning or reasoning by explicitly prompting [28,37,42]; 3) extending the application with an external memory or tool library [21,26,34].However, providing more relevant information in prompts, as a fundamental way to elicit better task understanding, does not receive sufficient attention.When near-optimal demonstrations are accessible, selecting few-shot demonstrations properly can be a simple yet very effective way to improve task performance, which is investigated in our work.</p>
<p>In-Context Example Selection</p>
<p>LLMs have been shown excellence of few-shot learning [3], and the selection of in-context examples can yield a significant improvement on the overall performance.Liu et al. [16] first propose to retrieve the -nearest neighbors (-NN) of the input as in-context examples, and achieve improvement over random retrieval baselines.Rubin et al. [25] select relevant samples with an encoder trained with label similarity, and obtain better performance over BM25 and pre-trained encoder baselines.Zhang et al. [44] consider selecting and labeling unlabeled examples as demonstrations to achieve the best performance, and view this problem as a sequential decision making task to solve by reinforcement learning.Wu et al. [39] further select examples in a subset recalled from -NN search via minimizing the entropy of output.</p>
<p>IRCoT [33] should be the most relevant work to ours, which retrieves relevant documents with reasoning steps on questionanswering tasks.However, their method consists of retrieving with a complete historical trajectory and accumulating retrieved trajectories over time, which are not transferable to complex sequential decision-making tasks, and we propose a method different from theirs in that: (i) Our method focuses on both providing more relevant demonstrations and reducing irrelevant context for sequential decision-making tasks, while theirs is limited to question-answering tasks and only addresses the first issue.(ii) Our method retrieves completely different steps across timesteps and complements the retrieval results with temporal information, while theirs only accumulates relevant documents at every reasoning step and heuristically cuts off the earliest ones to fit in the context limit of LLMs.(iii) Our method prepares pseudo-golden thoughts for expert trajectories in the memory to enable retrieval with trajectories without thoughts, and utilizes single-step thoughts as both queries and keys for precise retrieval, while theirs uses thoughts only as queries with raw documents as keys.</p>
<p>The selection of in-context examples has been studied thoroughly for non-sequential tasks like question answering and sentiment analysis.However, for sequential decision-making tasks, how to select the examples to improve the overall performance remains unclear.Zheng et al. [46] propose a trajectory-wise retrieval solution, while a more precise step-wise solution is still desired as discussed in Section 1, which motivates our work.</p>
<p>LLM Planning and Reasoning</p>
<p>Our work proposes to use thought, which can be viewed as a general abstraction of the current state, as queries and keys for retrieval.Nevertheless, plans, code comments, and any other text that extracts comprehensive information about the current state can serve as an alternative.Therefore, we particularly review some remarkable reasoning and planning works based on LLMs, and most of them are complementary to our work.</p>
<p>Wei et al. [38] first introduce the concept of Chain-of-Thought (CoT) by providing with explicit step-by-step reasoning process in example outputs improving performance on arithmetic, commonsense, and symbolic reasoning tasks.Wang et al. [36] further find that a single reasoning path can be sub-optimal, and propose selfconsistency to address this problem by sampling multiple reasoning paths.For efficient yet flexible search of reasoning paths, Yao et al. [41] apply tree search with self-evaluation to find globally excellent thoughts.Besta et al. [2] later extend the tree-search structure to a graph search for even better flexibility and overall performance.</p>
<p>The works mentioned above consider problems that are nonsequential or solvable by a single complete reasoning path after receiving the input.For harder sequential decision-making problems: Zhou et al. [47] introduce least-to-most prompting to solve hard problems by decomposing the problem and solving sub-problems sequentially.ReAct proposed by Yao et al. [42] interacts with the environment in a reason-then-act style, which enriches the context for action prediction.Code-as-Policies [14] writes executable codes</p>
<p>Demonstrations (For Decision)</p>
<p>Task: put two pillow in armchair.</p>
<p>Input (Observation)</p>
<p>You pick up the pillow 1 from the sofa 1.</p>
<p>Output (Action)</p>
<p>go to armchair 2</p>
<p>Timestep</p>
<p>Input (Observation)</p>
<p>On the armchair 2, you see ……</p>
<p>Output (Action)</p>
<p>put pillow 1 in/on armchair 2</p>
<p>Timestep + 1</p>
<p>Input (Observation)</p>
<p>On the sofa 1, you see a pillow 1, and a pillow 2.</p>
<p>Output (Action)</p>
<p>take pillow 1 from armchair 1 The aligned decision method consists of three sub-processes to the retrieved step demonstrations and prompting: 1) Temporal Expansion: Collect at most  previous steps and  subsequent steps for each retrieved step, and transform each step into a sequence of length  +  + 1 from   −  to   +  ; 2) Relative Order Mark: For each step in one demonstration step sequence, we label its relative position to the retrieved step in this sequence, i.e., the previous one (  − 1) with [Step -1] and the next one (  + 1) with [Step 1]; 3) History Alignment: For the current episode, we complement current observation (and thought, optional) with  +  previous steps to enrich information and align with demonstrations.</p>
<p>for embodied control by hierarchically expanding undefined programs, which can be viewed as implicit reasoning or CoT process.Liu et al. [15] propose to incorporate the strength of classical planners by translating the original problem into a PDDL [1] problem to solve by classical planners.Hao et al. [10] and Ding et al. [6] share a similar insight that reasoning can be implemented indeed by planning, where [10] use LLMs as world models and [6] conduct MCTS for thought generation with a light-weight extra network.</p>
<p>To summarize, LLM planning and reasoning have continuously received huge attention from researchers in recent years.This makes our work flexible and improvable with more powerful planning and reasoning methods in the future.</p>
<p>THE TRAD FRAMEWORK</p>
<p>As discussed in Section 1, trajectory-wise retrieving and prompting lead to issues of plausible examples, LLM context limits, and irrelevant information.To resolve these issues, we propose a novel method called Thought Retrieval and Aligned Decision (TRAD), as illustrated in Fig. 1.Our TRAD agent utilizes thought, which is obtained by reasoning about its current state, to retrieve similar steps from expert trajectories, and is then complemented with steps temporally correlated to the retrieved ones and their temporal position information to predict the action.Formally, our TRAD agent can be summarized in one equation:
𝜋 𝑇 𝑅𝐴𝐷 (𝑎 𝑡 |𝜉, 𝑜 0:𝑡 , 𝑎 0:𝑡 −1 ) = LLM(AD(TR(𝜏 𝑡 , M), 𝜉, 𝑜 0:𝑡 , 𝑎 0:𝑡 −1 )) ,
where  is the current task,  0: and  0: −1 are historical observations and actions,   is the thought generated by LLM about the current state, TR and AD denote our thought retrieval and aligned decision modules, and M refers to the thought-enhanced memory.We will present each module of TRAD in the following subsections.</p>
<p>Thought Preparation</p>
<p>Most expert trajectories, collected by either human or other expert agents, do not contain their reasoning process.Therefore, before we utilize thoughts for retrieval, we should prepare thoughts for each demonstration step in the memory.Specifically, we start from a small subset of expert demonstrations and provide thoughts written by human experts for each step in it.Given this small subset as fewshot examples in prompts, we can query LLMs to label thoughts for a large memory.Although ground-truth actions are not accessible at inference time, we can prompt LLMs with them to generate thoughts of higher quality.In this way, LLMs produce pseudogolden thoughts consistent with expert actions, and we obtain a thought-enhanced memory M supporting both trajectory-wise retrieval with task meta-data and step-wise retrieval with thoughts.</p>
<p>Thought Retrieval</p>
<p>Given pseudo-golden thoughts for all steps in the memory, which can serve as keys for step-wise similarity search, we now present our thought retrieval method to select relevant demonstrations at inference time.To be specific, we first conduct trajectory-wise demonstration retrieval as in [46] for thought generation.With these trajectory demonstrations, at each timestep  we prompt the LLM to generate a thought   for step-wise retrieval.Note that this process does not directly effects decision-making, hence it can be further simplified if necessary and the issues mentioned in Section 1 will not impact the agent severely.</p>
<p>With the thought   , which can be viewed as an abstraction, about current state, we conduct dense retrieval to find relevant steps in the thought-enhance memory M.Here any encoder pre-trained on a large corpus for retrieval, e.g., Sentence-BERT [23] and DPR [12], can be utilized to encode the query thought and key thoughts into dense vectors.Using a cosine similarity between the query and keys, we then collect top- relevant steps that belong to mutually different trajectories and their corresponding task instructions.</p>
<p>Table 1: Success Rate of Different Methods on 6 Types of ALFWorld Tasks.We compare TRAD with ReAct [42], Synapse [46], and their strong combination.TRAD significantly outperforms all baselines in terms of overall performance, achieves the best performance in 5 out of 6 types of task, and shows a decent performance on Heat task.The improvement of TRAD over all baselines on overall performance is statistically significant (measured by student's t-test at  &lt; 0.05).0.9583±0.00000.9630±0.05241.0000±0.00000.8986±0.02051.0000±0.00000.9804±0.02770.9677±0.0141</p>
<p>Method</p>
<p>Aligned Decision</p>
<p>Now we have relevant demonstration steps from thought retrieval.</p>
<p>However, the query thought can be imperfect due to the lack of expert action information at inference time.As we will show by ablation experiments in Section 4.4, directly using these steps to form single-step demonstrations does not provide satisfactory performance, which is similar to the plausible example issue of trajectorywise retrieval.Therefore, we propose an aligned decision method to incorporate more information during the decision-making process.</p>
<p>Aligned decision complements LLM agents with steps temporally correlated to the retrieved ones and their temporal position information.As illustrated in Fig. 2, the aligned decision method can be decomposed into following three sub-processes.</p>
<p>Temporal expansion.For each retrieved step, we first expand it into a step sequence involving  previous steps and  subsequent steps.When the number of previous or subsequent steps is smaller than  or  , we simply take all previous or subsequent steps.This transforms each retrieved step into at most ( + 1 +  ) temporally successive steps, allowing LLM agents to correct their imperfect thoughts by looking at more related steps at decision-making time.</p>
<p>Relative order mark.Given  expanded step sequences by temporal expansion, we insert a mark for each step (including the retrieved ones) indicating the relative position w.r.t.its corresponding retrieved step, and incorporate this rule of mark in the prompt for decision.For example, the last step before the retrieved one will be marked as [Step -1], the retrieved step as [Step 0], and the first step after the retrieved one as [ Step 1].This provides temporal information about the ( + 1 +  ) ×  demonstration steps, and promotes more accurate demonstration following.</p>
<p>History alignment.Sometimes the optimal policy to a task, like ALFWorld, can be history-dependent, hence using single-step input for action prediction is unreasonable.Since we aim to reduce input content for less forgetting and noise, we should neither use all historical observations and actions.Moreover, even if we include previous actions as auxiliary information, there exists a mismatch where expert demonstrations are given as sequences of length  + 1 +  while current input is a single step.We thus propose to insert at most  +  previous input-output pairs (i.e.  − (+ ): −1 ,   − (+ ): −1 ) before current input   , transforming current input into a similar sequence to demonstrations.</p>
<p>EXPERIMENTS</p>
<p>In this section, we aim to study the following research questions:</p>
<p>RQ1 How does TRAD perform against existing SoTA methods?RQ2 Does thought retrieval help to reduce irrelevant context and improve the overall performance?RQ3 Does aligned decision help to supply information when generalization is important?RQ4 Diving into aligned decision, are all temporal expansion (TE), relative order mark (ROM), and history alignment (HA) necessary for improvement?RQ5 How will the performance and advantage of TRAD be effected by critical hyper-parameters?</p>
<p>Experiment Setup</p>
<p>To answer the above research questions, we conduct extensive experiments on ALFWorld [30] and Mind2Web [4] tasks.For each task, we introduce the details of evaluation as follows.</p>
<p>ALFWorld [30] is a text-based game aligned with ALFRED [29] benchmark.It involves 6 types of tasks where an agent must take a series of actions (e.g.go to shelf 1, take vase 2 from shelf 1, put vase 2 in/on cabinet 5) to achieve a high-level goal given by a natural language instruction (e.g.put some vase on a cabinet).This environment is challenging in three aspects: 1) Agent should determine likely places of a householding object and explore them one by one to find such object; 2) Agent should understand the usage of some objects like microwaves, fridges, and desklamps; 3) Some tasks can take an agent more than 30 steps to solve, requiring substantial long-term memorization.Following Shridhar et al. [30], we evaluate on the subset of 134 out-of-distribution tasks, comparing the task success rates of TRAD to ReAct [42] and Synapse [46] (without state abstraction as observations are short).As ReAct and Synapse has provided sufficiently strong performances, we do not include more complex reasoning and planning baselines and corresponding variants of TRAD due to our API cost limit.Note that the original ReAct uses fixed but not retrieved trajectories as demonstrations, hence we test two ReAct baselines to eliminate such an effect:</p>
<p>• ReAct (Fixed) uses fixed human-written trajectories as demonstrations; • ReAct (Random) randomly samples trajectories from the memory as demonstrations.</p>
<p>For fair comparison, TRAD uses thoughts in exactly the same format as ReAct, and shares a consistent memory of expert trajectories with Synapse.We also add a strong baseline (Synapse+ReAct)  [4] find that the cross-website and cross-domain subsets are significantly harder due to the need for generalization to unseen websites.Since Mind2Web was introduced only about half a year ago, there is a lack of suitable baseline algorithms, and thus we compare our TRAD agent to Synapse [46] and ReAct [42].Following Zheng et al. [46], we evaluate on all 3 subsets, comparing the element accuracy (Ele.Acc), step success rate (Step SR), and trajectory success rate (SR).For fair comparison, we follow [46] and summarize observations into 5 web elements with the pre-trained element ranker provided by [4] for all methods.Since the observations are still very complex on Mind2Web, including thoughts for every step in trajectories is not available, hence: 1) we do not include a Synapse + ReAct baseline; 2) TRAD generates thoughts and predicts actions by a single-step prompt with the current observation and previous actions (without previous observations).To eliminate the effect of prompting style and reasoning, we build two ReAct baselines using the same format of prompt as TRAD:</p>
<p>• ReAct (Random), for which we prompt ReAct with completely random demonstration steps.• ReAct (Relevant), for which we prompt ReAct with demonstrate steps randomly chosen from trajectories retrieved by Synapse.</p>
<p>We do not include the ReAct (Fixed) baseline as it is hard to write or pick demonstrations commonly helpful for such diverse test sets.We also provide the results of the simplest MindAct [4] baseline without reasoning and retrieval for completeness.On Mind2Web, all methods are built with GPT-3.5-turbo and 3 in-context examples.</p>
<p>Evaluation on ALFWorld</p>
<p>The success rate of each method tested on ALFWorld is shown in Tab. 1. Generally, our TRAD agent achieves an average success rate of 96.77%, significantly outperforming ReAct (∼90%), Synapse (89.55%), and even their strong combination (93.78%).It is also worth noting that the worst trial of TRAD among 3 random seeds achieves a success rate of 94.8%, outperforming the best trial produced by any other method (94.0%).Down to the success rate on each type of task, we observe that the success rate of each method varies more on the simplest Put task and the hardest PutTwo task.We discuss the results of these two tasks respectively as follows:</p>
<p>• On the simplest Put task, ReAct performs even more poorly than other harder tasks.We find that the two vital reasons for ReAct's failure on Put task are incorrect location and usage of objects, e.g.trying to put an object in a closed safe.As this issue can be alleviated through a combination with Synapse, the necessity of retrieving relevant demonstrations thus justified.• TRAD achieves the largest improvement on the hardest PutTwo task.PutTwo requires to correct the locations of two objects and a comprehensive understanding of its task process.Since TRAD's outstanding performance on this hardest task is obtained from a reduced input context at decision-making time, we can conclude that step-wise thought retrieval is helpful by reducing the noise of irrelevant steps and finding relevant examples more precisely.</p>
<p>Evaluation on Mind2Web</p>
<p>To verify the capability of TRAD under more realistic scenarios, we compare TRAD to ReAct and the current SoTA method, Synapse, on the Mind2Web benchmark, and the results are shown in Tab. 2. We also include the results of Synapse without retrieval here to better illustrate the effect of different retrieval methods.Generally, TRAD achieves the highest performance in terms of all 3 metrics averaged on 3 subsets.Considering that the trajectorylevel retrieval of Synapse only brings marginal boosts on Cross-Task and Cross-Website subsets, and even slightly impacts the performance on the Cross-Domain subset, our TRAD method can be thus justified in two aspects:</p>
<p>• By reducing input context and utilizing step-wise relevant demonstrations, our step-wise thought retrieval helps more than the trajectory-wise retrieval with task meta-data in Synapse to improve on the simplest Cross-Task subset.• By eliminating plausible examples and complementing temporal correlated steps, aligned decision helps to improve on the two harder subsets, especially the most out-of-distribution Cross-Domain subset.</p>
<p>Furthermore, we observe that the two ReAct baselines perform poorly on this task, which indicates that: • The thoughts generated by GPT-3.5-turbo on Mind2Web tasks are not sufficient for LLM agents to infer the correct action.• The single-step prompting style which removes previous observations does not benefit overall performance.</p>
<p>On the contrary, TRAD utilizes these imperfect thoughts for retrieval rather than direct decision-making, and is complemented with temporally correlated steps via aligned decision.Therefore, TRAD is not negatively impacted by the imperfect thoughts, but transforms them into helpful information.</p>
<p>Before we start the study on detailed design and hyper-parameter choices of TRAD, we can summarize our performance evaluation on ALFWorld and Mind2Web benchmarks and answer the first three research questions as follows.</p>
<p>Answer to RQ1: On both householding (ALFWorld) and web navigation (Mind2Web) tasks, TRAD significantly outperforms curernt SoTA methods and becomes the new SoTA method.</p>
<p>Answer to RQ2: On ALFWorld benchmark, Synapse + ReAct generates thoughts in exactly the same way with our TRAD, and uses entire relevant trajectories (more information than TRAD) as demonstrations for action prediction.However, TRAD shows obvious advantage over this baseline.Therefore, we can conclude that TRAD benefits from more relevant demonstrations and less irrelevant input context brought by thought retrieval.</p>
<p>Answer to RQ3: On Mind2Web benchmark, TRAD achieves the most improvement over Synapse on the Cross-Domain subset which requires the most generalization.Therefore, we can tell that the aligned decision method complements critical information for decisionmaking on unseen input.</p>
<p>Ablation Studies</p>
<p>We have verified the effectiveness of TRAD on two different scenarios, i.e., automatic householding and web navigation.Next, we are to examine the effect of each module in TRAD.Due to our limited budget for API usage, all ablation studies are conducted on the Mind2Web benchmark with GPT-3.5-turbo.</p>
<p>4.4.1</p>
<p>The Effect of Aligned Decision.First, we study the effect of macro building blocks of TRAD.Since eliminating thought retrieval will disable aligned decision at the same time and break the framework fundamentally, we do not remove the thought retrieval module, but ablate each component of aligned decision, i.e., temporal expansion (TE), relative order mark (ROM), and history alignment (HA), and compare the corresponding performances.The results are shown in Tab. 3.</p>
<p>From Tab. 3, we observe that the performance without each component varies differently on the simplest Cross-Task subset and the two harder subsets:</p>
<p>• On the harder Cross-Website and Cross-Domain subsets, the elimination of all three modules in aligned decision results in a significant performance drop, and the effect of temporal expansion is the most significant.This is intuitive, since only retrieved steps are provided to the agent without TE, and thus the agent becomes more vulnerable to imperfect thoughts.• On the simplest Cross-Task subset, however, history alignment and relative order mark are not that helpful and even cause performance drop.As discussed earlier (Section 1 and Section 3.3), when the issue of plausible examples is not severe, reducing context and prompting with the most relevant demonstration becomes the dominant factor of performance boost.Therefore, only temporal expansion remains beneficial for recovering from imperfect thoughts, while the other two components lead to sub-optimal performance.</p>
<p>Generally, the aligned decision method provides more information about the source trajectories of retrieved steps and the current trajectory, and helps especially for scenarios where generalization is essential.We can now summarize these observations and answer the fourth research question.</p>
<p>Answer to RQ4: Among the sub-processes in aligned decision, 1) temporal expansion provides tolerance for imperfect thoughts and improves the overall performance of TRAD consistently; 2) relative order mark and history alignment complement TRAD with temporal information about the trajectories of retrieved steps and the current trajectory, which serve as useful context for out-of-distribution decision-making but may become less useful for in-distribution decision-making.</p>
<p>The Effect of Expansion</p>
<p>Steps  and  .Next we vary a critical hyper-parameter, the number of temporal expansion steps, and investigate how the overall performance will change accordingly.To avoid an expensive grid search on  and  , we consider only one-side expansion by varying  or  from 0 to 4 with the other set to 0. The results over all 3 subsets are shown in Fig. 3.</p>
<p>From Fig. 3, we can have the following observations: • Both forward expansion ( &gt; 0) and backward expansion ( &gt; 0) achieve improvement compared to no expansion ( =  = 0).This justifies our design of aligned decision.Forward expansion ( &gt; 0) generally provides more improvement than backward expansion ( &gt; 0) over no expansion ( =  = 0) and the Synapse baseline. or  does not help more when they are sufficiently large.</p>
<p>• Either forward expansion or backward expansion does not benefit from increasing a large enough  or  further.This proves our hypothesis that irrelevant context too far from the current state is of little value and even noisy.• Generally, forward expansion performs better than backward expansion when varying  and .The reason for this phenomenon might be that historical information has been incorporated in thoughts and thus future information helps more.• TRAD achieves its best performance when  = 2 and  = 0, and consistently outperforms Synapse with forward expansion.</p>
<p>4.4.3</p>
<p>The Effect of Demonstration Amount .Finally, we look into a common yet important hyper-parameter, the number of retrieved demonstrations , and see how the advantage of TRAD over the baseline (Synapse) will change given different  ∈ {1, 2, 3, 4, 5}.We show the results over all 3 subsets in Fig. 4. Note that the trajectorywise prompting in Synapse frequently exceeds the context limit when  = 5, and thus we omit this result. has a mild effect on the performance of TRAD and Synapse, and the advantage of TRAD over Synapse remains stable when  varies.</p>
<p>From Fig. 4, we see that  has a mild effect on the performance of TRAD and Synapse, and that the advantage of TRAD over Synapse consistently remains for all  ∈ {1, 2, 3, 4}.</p>
<p>With results in Section 4.4.2 and Section 4.4.3,we now respond to our last research question.</p>
<p>Answer to RQ5: The performance and advantage of TRAD generally remains stable with different hyper-parameter choices, i.e., temporal expansion steps, number of retrieved demonstrations.Its performance and advantage only degrade when using long backward extension, which is possibly due to the fact that historical information has already been incorporated in thoughts and does not provide further help for decision-making.</p>
<p>Case Studies</p>
<p>At the end of this section, we present some representative trajectories or steps, where we can intuitively learn the advantages of TRAD.We show two cases produced by Synapse and our TRAD agent on the cross-domain subset of Mind2Web in Fig. 5, to demonstrate: 1) the difference between task meta-data retrieval and thought retrieval; 2) the reason for retrieval rather than direct prediction with thought and the tolerance for imperfect thoughts.</p>
<p>In Fig. 5a, the trajectory-wise retrieval of Synapse is obviously problematic, which only considers "search" in task instructions and the retrieved trajectories are completely irrelevant to the current one.However, when we use these irrelevant demonstrations for thought production and conduct thought retrieval afterwards, the retrieved demonstrations become much more relevant as they all relate to baby (toddler) and reflect the process of interacting with navigation links or buttons to unfold invisible web pages during web browsing.With the demonstrations from thought retrieval, TRAD is capable of making the correct decision.</p>
<p>In Fig. 5b, both Synapse and TRAD seem to retrieve relevant examples trying to find something in New York, but if we examine the trajectories retrieved by task meta-data, 2/3 of them fulfill the condition "New York" by clicking some link or button rather than typing in a text box.Unfortunately, the correct action under the current state is typing, not clicking, and thus Synapse fails to type the correct content.On the contrary, TRAD learns to type the correct content "New York" into the text box, even if its thought is incorrect.This also validates our hypothesis that using thought for retrieval instead of prediction helps to correct imperfect thoughts.</p>
<p>REAL-WORLD DEPLOYMENT OF TRAD</p>
<p>Since Dec. 2023, we have deployed our TRAD agent to automate some real-world office tasks in a mainstream insurance company, which owns a global business with approximately 170 million customers worldwide.We select 4 different websites and collect 100 expert trajectories for some representative tasks on each website as our memory.For evaluation, we collect 20 unseen tasks on each website, using step success rate (Step SR) and trajectory success rate (SR) as evaluation metrics.Tasks involve filling in insurance inquiry forms, implementing advanced information retrieval, etc.Since the websites are complex and contain thousands of web elements, prompting with complete trajectories is not available, hence we only consider single-step prompting with historical actions as auxiliary information.To verify the effectiveness of TRAD, we use two different ReAct agents that the company has attempted as our baseline:</p>
<p>• ReAct-RD: randomly selects expert steps in random trajectories as demonstrations.• ReAct-RV: randomly selects expert steps in relevant trajectories retrieved by task instruction as demonstrations.</p>
<p>To be specific, the difference between TRAD and ReAct-RV is using thought for a second-time step retrieval and the aligned decision module.To further investigate the effect of thought retrieval and aligned decision, we also deploy a TR agent which removes our aligned decision method, namely the TRAD w/o TE baseline in Tab. 3. We list the results in Tab. 4. As can be seen in Tab. 4, TRAD achieves the best performance on all 4 websites, showing its advantage can remain when deployed to real-world scenarios.Moreover, we observe that TRAD w/o TE baseline also outperforms both ReAct agents, but exhibits noticeable disadvantages compared to the complete TRAD agents.This justifies our design of both thought retrieval and aligned decision.</p>
<p>Inference efficiency of TRAD.At inference time, our TRAD agent only introduces little extra time consumption in thought retrieval compared to ReAct.We profile the inference process of TRAD and ReAct on all websites and tasks, and in average TRAD takes only 11.7% more time than ReAct-RD, which indicates that our method achieves improvement without much sacrifice on efficiency.</p>
<p>DISCUSSIONS 6.1 Limitations of TRAD</p>
<p>Although TRAD exhibits excellent performances over a diverse set of tasks, it still has limitations like dependence on high-quality thought and trade-off between information and noise in temporal expansion, and we briefly discuss about them here.</p>
<p>6.1.1Dependence on high-quality thought.TRAD alleviates the issue of imperfect thoughts by its aligned decision module, but its capability still depends heavily on the quality of thoughts and the capability of backbone LLM.To make such a step-wise retrievalaugmented method work well, the abstraction of current state is critical since it serves as the query and key for retrieval, hence the LLM used to build a TRAD agent should at least have a decent understanding of the task.</p>
<p>6.1.2Trade-off in temporal expansion.TRAD expects to keep relevant information but reduce irrelevant input context by step-wise thought retrieval, while preserving some chance for correcting imperfect thoughts by temporal expansion.Here exists a trade-off: a longer temporal expansion brings not only more tolerance to imperfect thoughts, but also more irrelevant noise in demonstrations.This trade-off requires careful consideration for different tasks.</p>
<p>Future Directions</p>
<p>While ablation studies have been conducted to justify our design of TRAD, there are some promising ideas worth study which can probably improve TRAD further.We leave them as future works, and discuss them as follows.</p>
<p>6.2.1 Better Demonstrations For Reasoning.TRAD currently employs relevant trajectories or randomly-chosen steps from them as demonstrations to generate thoughts, which still suffers from the issues discussed in Section 1 to some extent.Therefore, modifications can be made to generate thoughts of higher quality, and thus improve the overall performance of TRAD.6.2.2 Better Representations For Retrieval.As we have discussed in Section 2.3, TRAD can utilize any other methods to obtain a comprehensive abstraction of the current state in a sequential decisionmaking task, which can possibly serve as better queries and keys for the step-wise demonstration retrieval.Therefore, TRAD can be combined with more powerful LLM planning and reasoning methods and even dense abstractions produced by LLMs pre-trained on domain-specific data like [8].</p>
<p>CONCLUSIONS</p>
<p>In this work, we propose a novel LLM agent augmented by step-wise demonstration retrieval (TRAD) for sequential decision-making tasks.TRAD first retrieves relevant step demonstrations by its thought about current state, and then complements temporally correlated steps for more informative action prediction.Extensive experiments are conducted on two different sequential decisionmaking tasks to validate the effectiveness of our solution, and thorough ablation studies justify the design choice and stability of our method.We further present the results from real-world deployment of our method, showing its value in real-world applications.</p>
<p>For the Put task we show part of $Demo 1 here.</p>
<p>You are in the middle of a room .Looking quickly around you , you see a cabinet 4 , a cabinet 3 , a cabinet 2 , a cabinet 1 , a countertop 1 , a garbagecan 1 , a handtowelholder 2 , a handtowelholder 1 , a sinkbasin 2 , a sinkbasin 1 , a toilet 1 , a toiletpaperhanger 1 , and a towelholder 1.Your task is to : put some spraybottle on toilet .act : go to cabinet 1 think : I am now in / on : the middle of a room Critical objects I have found : None Objects I have taken : None To solve the task , I need to find and take a sparybottle , then put it on toilet .First I need to find a spraybottle .A spraybottle is more likely to appear in cabinet (1 -4) , countertop (1) , toilet (1) , sinkbasin (1 -2) , garbagecan (1) .I can check one by one , starting with cabinet 1. obs : On the cabinet 1 , you see a cloth 1 , a soapbar 1 , a soapbottle 1. act : go to cabinet 2 think : I am now in / on : cabinet 1 Critical objects I have found : None Objects I have taken : None In cabinet 1 , there is no spraybottle .Next , I need to check cabinet 2. ...... obs : On the toilet 1 , you see a soapbottle 2. act : put spraybottle 2 in / on toilet 1 think : I am now in / on : toilet 1 Critical objects I have found : spraybottle 2 ( cabinet 2) Objects I have taken : spraybottle 2 Now I am on the toilet 1.Next , I need to put spraybottle 2 in / on it .obs : You put the spraybottle 2 in / on the toilet 1.</p>
<p>A.1.2 Thought generation.</p>
<p>You are an agent to interact with a household to solve a task .You will be given a task where you need to put an ( two ) object ( s ) to a target either directly or after an operation .Each time you first think about your current situation , then output an action , and wait for next observation .Here is your action space : * go to target : Move to the target , and you will observe what is in / on the target or know it is closed or opened .<em> open target : Open the target when it is closed , and you will observe what is in / on the target .Only cabinets , drawers , fridges , safes , and microwaves can be opened .</em> take object from target : Take the object from the target when the object is in / on the target .You can take only one object at the same time .For the Put task we show part of a typical $Demo here.</p>
<p>You are in the middle of a room .Looking quickly around you , you see a cabinet 1 , a cabinet 2 , a cabinet 3 , a cabinet 4 , a countertop 1 , a garbagecan 1 , a handtowelholder 1 , a handtowelholder 2 , a sinkbasin 1 , a sinkbasin 2 , a toilet 1 , a toiletpaperhanger 1 , and a towelholder 1.Your task is to : put some soapbottle on toilet .&gt; think : I am now in / on : the middle of a room Critical objects I have found : None Objects I have taken : None To solve the task , I need to find and take a soapbottle , then put it on the toilet .First , I need to find a soapbottle .A soapbottle is more likely to appear in cabinets (1 -4) , countertop (1) , or sinkbasin (1 -2) .I will start by checking the garbagecan 1 , although it 's less likely to find a soapbottle there , it 's still possible .&gt; act : go to garbagecan 1 On the garbagecan 1 , you see a soapbottle 1. &gt; think : I am now in / on : garbagecan 1 Critical objects I have found : soapbottle 1 ( garbagecan 1) Objects I have taken : None I have found a soapbottle (1) in the garbagecan 1.Next , I need to take it .&gt; act : take soapbottle 1 from garbagecan 1 You pick up the soapbottle 1 from the garbagecan 1. &gt; think : I am now in / on : garbagecan 1 Critical objects I have found : soapbottle 1 ( garbagecan 1) Objects I have taken : soapbottle 1 Now I have taken the soapbottle (1) .Next , I need to go to and put it on the toilet 1.</p>
<p>A.2 Prompts on Mind2Web</p>
<p>On Mind2Web we generally follow prompts in Synapse [46].</p>
<p>A.2.1 Thought preparation.</p>
<p>You are a large language model trained to navigate the web .You will be given a task , an observation , and your previous actions , and each time you should output the next action and wait for the next observation .Here is the action space : 1. <code>CLICK [ id ]</code>: Click on an HTML element with its id .We show one demonstration here:</p>
<p>Task : Find JetBlue press releases for the year 2020 Trajectory : obs : <code>&lt; html &gt; &lt;jb -app &gt; &lt;jb -tab -panel tabpanel &gt; &lt;div &gt; &lt; div combobox &gt; &lt;jb -type -ahead -input &gt; &lt; label &gt; From &lt;/ label &gt; &lt; input id =908 text columbus port columbus intl apt , /&gt; &lt;/ jb -type -ahead -input &gt; &lt;/ div &gt; &lt; div combobox &gt; &lt;jb -type -ahead -input &gt; &lt; label &gt; To &lt;/ label &gt; &lt; input id =927 text / &gt; &lt;/ jb -type -ahead -input &gt; &lt;/ div &gt; &lt;/ div &gt; &lt;/ jb -tab -panel &gt; &lt;jbfooter contentinfo &gt; &lt;div &gt; &lt;a id =1554 &gt; Investor Relations &lt;jb -icon img external link should open in / &gt; &lt;/a &gt; &lt;a id =107 &gt; Press Room &lt;jb -icon img external link should open in / &gt; &lt;/a &gt; &lt;/ div &gt; &lt;/ jb -footer &gt; &lt;div &gt; &lt; textarea text / &gt; &lt;div &gt; 250 characters remaining &lt;/ div &gt; &lt; button id =1911 &gt; Submit &lt;/ button &gt; &lt;/ div &gt; &lt;/ jb -app &gt; &lt;/ html &gt;à ct :</code>CLICK [107] <code>([ link ] Press RoomExternal Link should open in a new windo ... -&gt; CLICK ) obs :</code>&lt; html &gt; &lt; main main &gt; &lt; span &gt; <a id =4386 > View all releases </a > <div > <div > <a download > </a > <a id =5509 print > &lt; span &gt; Print Core Overhead Bins &lt;/ span &gt; </a > &lt;/ div &gt; &lt; div &gt; <a download > </a > <a id =6087 print > &lt; span &gt; Print Welcome Kiosk ( JFK ) &lt;/ span &gt; </a > &lt;/ div &gt; <div > <a download > </a > <a id =6614 print > &lt; span &gt; Print Core Overhead Bins &lt;/ span &gt; </a > &lt;/ div &gt; <div > <a download > </a > <a id =7192 print > &lt; span &gt; Print Welcome Kiosk ( JFK ) &lt;/ span &gt; </a > &lt;/ div &gt; &lt;/ div &gt; &lt;/ span &gt; &lt;/ main &gt; &lt;/ html &gt;à ct : `CLICK A.2.2 Thought generation.</p>
<p>You are a large language model trained to navigate the web .You will be given a task , an observation , and your previous actions .We show part of a typical $Demo here.</p>
<p>Task : Find cheapest cars available at San Francisco Airport for a day .Trajectory : obs : <code>&lt; html &gt; &lt; main &gt; &lt;div &gt; &lt; label id =132 &gt; Pick -up location &lt;/ label &gt; &lt; input id =372 pick -up location / &gt; &lt;/ div &gt; &lt;div &gt; &lt;div &gt; &lt;a id =859 &gt; &lt; span &gt; San Francisco &lt;/ span &gt; &lt;/a &gt; &lt;a id =896 &gt; &lt; span &gt; San Francisco Airport &lt;/ span &gt; &lt;/a &gt; &lt;/ div &gt; &lt; button id =1137 button &gt; &lt;div &gt; Airports &lt;/ div &gt; &lt;/ button &gt; &lt;/ div &gt; &lt;/ main &gt; &lt;/ html &gt;p revious actions : obs :</code>&lt; html &gt; &lt; body &gt; <a id =1885 > &lt; span &gt; Car rental &lt;/ span &gt; </a > <div > <div > &lt; label &gt; Pick -up location &lt;/ label &gt; &lt; input id =1994 pick -up location san francisco airport , us ( sfo ) / &gt; &lt;/ div &gt; <div > &lt; button id =2006 button tue , mar 28 &gt; <div > <div > Pick -up date &lt;/ div &gt; <div > Tue , Mar 28 &lt;/ div &gt; &lt;/ div &gt; &lt;/ button &gt; &lt; button id =2131 button fri , mar 31 &gt; &lt; div &gt; <div > Drop -off date &lt;/ div &gt; <div > Fri , Mar 31 &lt;/ div &gt; &lt;/ div &gt; &lt;/ button &gt; &lt; button id =2251 button &gt; <div > Search &lt;/ div &gt; &lt;/ button &gt; &lt;/ div &gt; &lt;/ div &gt; &lt;/ body &gt; &lt;/ html &gt;p revious actions : div ] Fri , Mar 31 -&gt; CLICK ) to select the drop -off date as " Fri , Mar 31" obs : <code>&lt; html &gt; &lt; body &gt; &lt;a id =5677 &gt; &lt; span &gt; Car rental &lt;/ span &gt; &lt;/a &gt; &lt;div &gt; &lt;div &gt; &lt; label &gt; Pick -up location &lt;/ label &gt; &lt; input id =5786 pick -up location san francisco airport , us ( sfo ) / &gt; &lt;/ div &gt; &lt;div &gt; &lt; table grid &gt; &lt; td gridcell &gt; &lt; span id =5951 checkbox 29 march 2023 &gt; &lt; span &gt; 29 &lt;/ span &gt; &lt;/ span &gt; &lt;/ td &gt; &lt;/ table &gt; &lt;div &gt; &lt; button id =6248 button fri , mar 31 &gt; &lt;div &gt; &lt;div &gt; Drop -off date &lt;/ div &gt; &lt;div &gt; Fri , Mar 31 &lt;/ div &gt; &lt;/ div &gt; &lt;/ button &gt; &lt; select id =6264 dropoff -time &gt; &lt; option 00:00 &gt; Midnight &lt;/ option &gt; &lt; option 00:30 &gt; 12:30 AM &lt;/ option &gt; &lt; option 01:00 &gt; 1:00 AM &lt;/ option &gt; &lt; option 01:30 &gt; 1:30 AM &lt;/ option &gt; &lt; option 02:00 &gt; 2:00 AM &lt;/ option &gt; &lt; option 02:30 &gt; 2:30 AM &lt;/ option &gt; &lt; option 03:00 &gt; 3:00 AM &lt;/ option &gt; &lt; option 03:30 &gt; 3:30 AM &lt;/ option &gt; &lt; option 04:00 &gt; 4:00 AM &lt;/ option &gt; &lt; option 04:30 &gt; 4:30 AM &lt;/ option &gt; &lt; option 05:00 &gt; 5:00 AM &lt;/ option &gt; &lt; option 05:30 &gt; 5:30 AM &lt;/ option &gt; &lt; option 06:00 &gt; 6:00 AM &lt;/ option &gt; &lt; option 06:30 &gt; 6:30 AM &lt;/ option &gt; &lt; option 07:00 &gt; 7:00 AM &lt;/ option &gt; &lt; option 07:30 &gt; 7:30 AM &lt;/ option &gt; &lt; option 08:00 &gt; 8:00 AM &lt;/ option &gt; &lt; option 08:30 &gt; 8:30 AM &lt;/ option &gt; &lt; option 09:00 &gt; 9:00 AM &lt;/ option &gt; &lt; option 09:30 &gt; 9:30 AM &lt;/ option &gt; &lt; option 10:00 true &gt; 10:00 AM &lt;/ option &gt; &lt; option 10:30 &gt; 10:30 AM &lt;/ option &gt; &lt; option 11:00 &gt; 11:00 AM &lt;/ option &gt; &lt; option 11:30 &gt; 11:30 AM &lt;/ option &gt; &lt; option 12:00 &gt; Noon &lt;/ option &gt; &lt;/ select &gt; &lt;/ div &gt; &lt; button id =6369 button &gt; &lt;div &gt; Search &lt;/ div &gt; &lt;/ button &gt; &lt;/ div &gt; &lt;/ div &gt; &lt;/ body &gt; &lt;/ html &gt;p revious actions :</code>CLICK A.2.3 Action prediction.We use the same sentence as in ALFWorld to tell LLM about the relative order mark.</p>
<p>You are a large language model trained to navigate the web .You will be given a task , an observation , and your previous actions .Each time you should output the next action and wait for the next observation .Here is the action space : 1. <code>CLICK [ id ]</code>: Click on an HTML element with its id .We show the format of demonstrations here:</p>
<p>Task : Look for a job opening in sales in San Fransisco , and if found , apply for the job .obs : <code>&lt; html &gt; &lt; body &gt; &lt;div &gt; &lt; nav navigation &gt; &lt; ul menubar &gt; &lt;li &gt; &lt; button id =8372 menuitem &gt; &lt; span &gt; Research &lt;/ span &gt; &lt;/ button &gt; &lt; div menu &gt; &lt;/ div &gt; &lt;/ li &gt; &lt;/ ul &gt; &lt;/ nav &gt; &lt; nav cargurus corporate information navigation &gt; &lt; ul menu &gt; &lt;a id =8721 menuitem olink &gt; Our Team &lt;/a &gt; &lt;a id =8015 menuitem olink &gt; Careers &lt;/a &gt; &lt;/ ul &gt; &lt;/ nav &gt; &lt;/ div &gt; &lt;ul &gt; &lt;a id =9178 our team &gt; Our Team &lt;/a &gt; &lt;a id =9208 careers &gt; Careers &lt;/a &gt; &lt;/ ul &gt; &lt;/ body &gt; &lt;/ html &gt;p revious actions :</code>CLICK [117] <code>([ link ] Our Team -&gt; CLICK ) act :</code>CLICK [8015] `([ menuitem ] olink -&gt; CLICK )</p>
<p>The input are presented in the same format as demonstrations, except that they have no ground-truth actions.</p>
<p>B FULL EXPERIMENT RESULTS B.1 The Effect of 𝐹 and 𝐵</p>
<p>We list the results of varying subsequent step number  and previous step number  of temporal expansion on each subset and over all 3 subsets of the Mind2Web benchmark in Fig. 6.</p>
<p>B.2 The Effect of 𝐾</p>
<p>We list the results of varying retrieval size  on each subset and over all 3 subsets of the Mind2Web benchmark in Fig. 7.</p>
<p>[Step - 1 ][[Step 1 ] 2 Retrieved ( 1 )[Step 1 ]
11211
On the sofa 1, you see …… think: …… act: take pillow 1 from sofa 1 Step 0] You pick up the pillow 1 from the sofa 1. think: I am now in/on: sofa 1 Critical objects I have found: pillow 1 (sofa 1) pillow 2 (sofa 1) Objects I have taken: pillow 1 Now I have taken the first pillow (1).Next, I need to go to an armchair and put it there …… act: go to armchair 2 On the armchair 2, you see …… think: …… act: put pillow 1 in/on armchair Task: put two pillow in sofa [Step -1] On the armchair 2, …… [Step 0] You put the pillow 2 …… On the sofa 1, …… Retrieved (2) Thought I am now in/on armchair 1 Critical objects I have found: pillow 1 (armchair 1) pillow 2 (armchair 1) Objects I have taken: pillow 1 Now I have taken the first pillow (1).Next, I need to go to the sofa and put it there …… taken the first pillow (1).Next, I need to go to an armchair and put it there …… Thought I am now in/on: armchair 2 Critical objects I have found: pillow 1 (sofa 1) pillow 2 (sofa 1, taken and put in/on armchair 2) Objects I have taken: None Now I have put the first pillow in armchair 2. Next, I need to go back to sofa 1 and take the second pillow (1).</p>
<p>1 Timestep − 1 Figure 2 :
112
Figure 2: An illustration of our aligned decision method, where  =  = 1 and the -th retrieved step is at time   in its trajectory.The aligned decision method consists of three sub-processes to the retrieved step demonstrations and prompting: 1) Temporal Expansion: Collect at most  previous steps and  subsequent steps for each retrieved step, and transform each step into a sequence of length  +  + 1 from   −  to   +  ; 2) Relative Order Mark: For each step in one demonstration step sequence, we label its relative position to the retrieved step in this sequence, i.e., the previous one (  − 1) with [Step -1] and the next one (  + 1) with [Step 1]; 3) History Alignment: For the current episode, we complement current observation (and thought, optional) with  +  previous steps to enrich information and align with demonstrations.</p>
<p>Figure 3 :
3
Figure3: The effect of varying subsequent steps  and previous steps  on Mind2Web benchmark.Solid lines correspond to the performance metrics of TRAD given different  and , and the dashed lines correspond to the Synapse baseline.Forward expansion ( &gt; 0) generally provides more improvement than backward expansion ( &gt; 0) over no expansion ( =  = 0) and the Synapse baseline. or  does not help more when they are sufficiently large.</p>
<p>Figure 4 :
4
Figure 4: The effect of varying the number of retrieved demonstrations  on Mind2Web benchmark.Solid lines correspond to the performance metrics of TRAD given different , and the dashed lines correspond to the Synapse baseline.has a mild effect on the performance of TRAD and Synapse, and the advantage of TRAD over Synapse remains stable when  varies.</p>
<p>New York City) to search for the pollutants in New York City.…… Therefore, next I have to: <code>TYPE [23127] [New York]</code>([textbox] Location-Search -&gt; TYPE: New York) to specify the location as New York.…… Therefore, next I have to: <code>TYPE [9606] [New York]( [searchbox] Search by city -&gt; TYPE: New York) to specify the location as New York City.Thought-Enhanced Memory …… Therefore, next I have to:</code>TYPE [6394] <a href="[searchbox] Find a Location -&gt; TYPE: NEW YORK">NEW YORK</a> to specify the location as New York.Predicted Action: TYPE [143] [New York] (√)</p>
<p>Figure 5 :
5
Figure5: Comparison between Synapse trajectory-wise retrieval with task meta-data and TRAD step-wise retrieval with thought.(a) The trajectory-wise retrieval of Synapse only considers "search" in task instructions and the retrieved trajectories are completely irrelevant.However, by generating thoughts with these irrelevant trajectories, thought retrieval finds more relevant step-wise demonstrations related to baby (toddler) and navigation.(b) The trajectory-wise retrieval of Synapse retrieves plausible examples which do not type in a text box with task meta-data.Although thoughts are imperfect, thought retrieval finds more relevant demonstrations and TRAD learns to input "New York".</p>
<ul>
<li>put object in / on target : Put an object you have taken / picked up in / on the target .You should go to the target in your last action .You can put no matter there are other objects in / on the target or not .<em> clean object with target : Clean an object you have taken / picked up with the target .The target should be a sinkbasin .You should go to the target in your last action .You can clean no matter there are other objects in / on the target or not .</em> heat object with target : Heat an object you have taken / picked up with the target .The target should be a microwave .You should go to the target in your last action .You can heat no matter there are other objects in / on the target or not .<em> cool object with target : Cool an object you have taken / picked up with the target .The target should be a fridge .You should go to the target in your last action .You can cool no matter there are other objects in / on the target or not .</em> use object : Use the object .The object should be a desklamp .You should be in / on a place where the object appears .* look : Look around and see what you are facing .Only look when nothing happens .When you cannot find some object , remember to check less possible places that you have not visited , or even impossible places like garbagecan .Here are two examples .</li>
</ul>
<blockquote>
<p>act : go to toilet 1 On the toilet 1 , you see a cloth 1. &gt; think : I am now in / on : toilet 1 Critical objects I have found : soapbottle 1 ( garbagecan 1) Objects I have taken : soapbottle 1 Now I am at the toilet 1.Next , I need to put the soapbottle 1 on it .&gt; act : put soapbottle 1 in / on toilet 1 A.1.3Action prediction.For action prediction we use almost the same prompt as thought generation.For TRAD we only use  + 1 +  steps in each demonstration, and we add the following sentence to tell LLM the meaning of relative order mark: The mark [ Step $i ] in expert examples indicates a coarse relative position of expert demonstration steps to your situation .For example , [ Step -1] means the last step , [ Step 0] means the current step , and [ Step 1] means the next step .</p>
</blockquote>
<p>2 .
2
<code>TYPE [ id ] [ value ]</code>: Type a string into the element with the id .3. <code>SELECT [ id ] [ value ]</code>: Select a value for an HTML element by its id .Now you are given some expert demonstrations and reasons for their actions , follow these examples and give your reason for the given action .Note that you should take all previous actions into reasoning , and not take the current action as what you have done .</p>
<p>[4386] <code>([ link ] View All Releases \ uedbe -&gt; CLICK ) obs :</code>&lt; html &gt; &lt; main main &gt; <div > <div > &lt; label id =8053 &gt; Select year : &lt;/ label &gt; &lt; select id =7685 &gt; &lt; option 2023 true &gt; 2023 &lt;/ option &gt; &lt; option 2022 &gt; 2022 &lt;/ option &gt; &lt; option 2021 &gt; 2021 &lt;/ option &gt; &lt; option 2020 &gt; 2020 &lt;/ option &gt; &lt; option 2019 &gt; 2019 &lt;/ option &gt; &lt; option 2018 &gt; 2018 &lt;/ option &gt; &lt; option 2017 &gt; 2017 &lt;/ option &gt; &lt; option 2016 &gt; 2016 &lt;/ option &gt; &lt; option 2015 &gt; 2015 &lt;/ option &gt; &lt; option 2014 &gt; 2014 &lt;/ option &gt; &lt; option 2013 &gt; 2013 &lt;/ option &gt; &lt; option 2012 &gt; 2012 &lt;/ option &gt; &lt; option 2011 &gt; 2011 &lt;/ option &gt; &lt; option 2010 &gt; 2010 &lt;/ option &gt; &lt; option 2009 &gt; 2009 &lt;/ option &gt; &lt; option 2008 &gt; 2008 &lt;/ option &gt; &lt; option 2007 &gt; 2007 &lt;/ option &gt; &lt; option 2006 &gt; 2006 &lt;/ option &gt; &lt; option 2005 &gt; 2005 &lt;/ option &gt; &lt; option 2004 &gt; 2004 &lt;/ option &gt; &lt; option 2003 &gt; 2003 &lt;/ option &gt; &lt; option 2002 &gt; 2002 &lt;/ option &gt; &lt;/ select &gt; &lt;/ div &gt; <div > &lt; label &gt; Category : &lt;/ label &gt; &lt; select id =7691 &gt; &lt; option true &gt; All Latest News &lt;/ option &gt; &lt; option press -release &gt; Press Releases &lt;/ option &gt; &lt; option articles &gt; Articles &lt;/ option &gt; &lt;/ select &gt; &lt;/ div &gt; <div > &lt; label &gt; Search : &lt;/ label &gt; &lt; input id =8124 text newssearch / &gt; &lt; button id =8126 submit &gt; &lt; span &gt; Search &lt;/ span &gt; &lt;/ button &gt; &lt;/ div &gt; &lt;/ div &gt; &lt;/ main &gt; &lt;/ html &gt;à ct : <code>SELECT [7685] [2020]</code>([ combobox ] Select year : -&gt; SELECT : 2020) reason : I have to find : JetBlue press releases for the year 2020 Now I have done : <code>CLICK [107]</code>([ link ] Press RoomExternal Link should open in a new windo ... -&gt; CLICK ) <code>CLICK [4386]</code>([ link ] View All Releases \ uedbe -&gt; CLICK ) to show all releases Therefore , next I have to : <code>SELECT [7685] [2020]</code>([ combobox ] Select year : -&gt; SELECT : 2020) due to the condition `for the year 2020 The input are presented in the same format as demonstrations without human-written reasons.</p>
<p>[896] <code>([ link ] San Francisco Airport -&gt; CLICK )</code>CLICK [2131] `([ div ] Fri , Mar 31 -&gt; CLICK ) reason : I have to Find cheapest cars available at San Francisco Airport for a day .</p>
<p>2 .
2
<code>TYPE [ id ] [ value ]</code>: Type a string into the element with the id .3. <code>SELECT [ id ] [ value ]</code>: Select a value for an HTML element by its id .Now you are given some expert demonstrations , follow these demonstrations and make your decision .The mark [ Step $i ] indicates a coarse relative position of expert demonstration steps to your situation .For example , [ Step -1] means the last step , [ Step 0] means the current step , and [ Step 1] means the next step .Note that you should take all previous actions into reasoning .In your output , the action should be quoted by a pair of '`'.</p>
<p>Figure 6 :
6
Figure 6: The effect of varying subsequent steps  and previous steps  on Mind2Web benchmark.Solid lines correspond to the performance metrics of TRAD given different  and , and the dashed lines correspond to the Synapse baseline.Forward expansion ( &gt; 0) generally provides more improvement than backward expansion ( &gt; 0) over no expansion ( =  = 0) and the Synapse baseline. or  does not help more when they are sufficiently large.</p>
<p>Figure 7 :
7
Figure 7: The effect of varying the number of retrieved demonstrations  on Mind2Web benchmark.Solid lines correspond to the performance metrics of TRAD given different , and the dashed lines correspond to the Synapse baseline. has a mild effect on the performance of TRAD and Synapse, and the advantage of TRAD over Synapse remains stable when  varies.</p>
<p>Table 2 :
2
Results (%) of all methods on Mind2Web benchmark.TRAD achieves the best overall performances and the most improvement on the two harder subsets, especially the most out-of-distribution Cross-Domain subset.The improvement of TRAD over all baselines on three overall metrics is statistically significant (measured by student's t-test with  &lt; 0.01).
MethodCross-Task Ele. Acc Step SR SRCross-Website Ele. Acc Step SR SRCross-Domain Ele. Acc Step SR SRAll Ele. Acc Step SR SRMindAct20.317.40.819.316.20.621.018.61.020.618.00.9ReAct (Random)31.024.71.625.719.10.627.922.91.828.322.71.6ReAct (Relevant)31.326.01.226.720.50.628.023.11.628.523.41.4Synapse w/o Retrieval33.128.93.227.822.11.130.026.51.430.426.41.7Synapse34.430.62.028.823.41.129.425.91.630.426.61.6TRAD (Ours)35.230.83.630.424.00.632.028.02.032.528.02.1combining the trajectory-level retrieval in Synapse and the reason-ing in ReAct. On ALFWorld, all methods are built with GPT-4 [19]and 2 in-context examples.
[4]d2Web[4]is an HTML-based web navigation benchmark collected from real-world webpages, involving various tasks such as searching, trip booking, social network subscription, etc.It contains 3 subsets, i.e., cross-task, cross-website, cross-domain.This environment is challenging in two aspects: 1) Existing LLM agents can hardly understand HTML input well; 2) Unseen tasks and websites can require substantial generalization.Deng et al.</p>
<p>Table 3 :
3
Results (%) of ablation studies on Mind2Web benchmark.TE builds the basic structure of aligned decision and is thus critical for performance boost on all three subsets.HA and ROM work well to promote generalization on the two harder Cross-Website and Cross-Domain subsets but provide little help on the Cross-Task subset.The improvement of TRAD over all ablation baselines on Ele.Acc and Step SR is statistically significant (measured by student's t-test with  &lt; 0.05).
MethodCross-Task Ele. Acc Step SR SRCross-Website Ele. Acc Step SR SRCross-Domain Ele. Acc Step SR SRAll Ele. Acc Step SR SRTRAD w/o TE34.228.41.227.420.40.629.124.01.430.024.51.3TRAD w/o HA36.231.14.028.322.20.629.424.91.830.825.92.1TRAD w/o ROM35.730.53.628.922.30.631.527.21.932.127.22.0TRAD (Ours)35.230.83.630.424.00.632.028.02.032.528.02.1</p>
<p>Table 4 :
4
Evaluation results on real-world websites from a mainstream global business insurance company.
MethodReAct-RD ReAct-RVTRTRAD (Ours)Website 1Step SR0.8430.8260.9410.950(form filling)SR0.5000.4500.8000.800Website 2Step SR0.9410.9370.9580.974(advanced IR)SR0.9000.8500.8500.900Website 3Step SR0.9620.9871.0001.000(advanced IR)SR0.8500.8000.8501.000Website 4Step SR0.8200.8600.8451.000(form filling)SR0.3500.3500.4001.000AverageStep SR SR0.891 0.6500.902 0.6130.936 0.7250.981 0.925</p>
<p>Select a value for an HTML element by its id .Now you are given some expert demonstrations , follow these examples and conduct reasoning about your situation .
$Demo 1$Demo 2$Demo 3$Input
Each time you should output the next action and wait for the next observation .Here is the action space : 1. <code>CLICK [ id ]</code>: Click on an HTML element with its id .2. <code>TYPE [ id ][ value ]</code>: Type a string into the element with the id .3.<code>SELECT [ id ] [ value ]</code>:</p>
<p><code>([ link ] San Francisco Airport -&gt; CLICK ) to select the pick -up location as San Francisco Airport .
reason : I have to find :the cheapest carsavailable at San Francisco Airportfor a dayNow I have done :NoneTherefore , next I have to :</code>CLICK [896]</p>
<p><code>([ link ] San Francisco Airport -&gt; CLICK ) due to the condition</code>at San Francisco Airport Therefore , next I have to : <code>CLICK [2131]</code>([
<code>CLICK [896]</code>([ link ] San Francisco Airport -&gt; CLICK )reason : I have to find :the cheapest carsavailable at San Francisco Airportfor a dayNow I have done :`CLICK [896]
ACKNOWLEDGMENTSThe Shanghai Jiao Tong University team is partially supported by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62322603, 62076161).A PROMPT LIBRARY A.1 Prompts on ALFWorldALFWorld includes 6 different types of task, and we only present the prompt for the Put task here.A.1.1 Thought preparation.We write thoughts for the same demonstration ($Demo 1 and $Demo 2) as the first two in ReAct[42]and use them for thought preparation.You are an agent to interact with a household to solve a task .You will be given a task where you need to put an ( two ) object ( s ) to a target either directly or after an operation .Each time you first think about your current situation , then output an action , and wait for next observation .Here is your action space : * go to target : Move to the target , and you will observe what is in / on the target or know it is closed or opened .
Pddl| the planning domain definition language. Constructions Aeronautiques, Adele Howe, Craig Knoblock, Drew Isi, Ashwin Mcdermott, Manuela Ram, Daniel Veloso, David Weld, Sri Wilkins, Anthony Barrett, Dave Christianson, 1998. 1998Technical Report</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, arXiv:2308.096872023. 2023arXiv preprint</p>
<p>Language Models are Few-Shot Learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Proceedings of the 34th Advances in Neural Information Processing Systems (NeurIPS). the 34th Advances in Neural Information Processing Systems (NeurIPS)Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Ilya Sutskever, and Dario Amodei</p>
<p>Mind2Web: Towards a Generalist Agent for the Web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, Proceedings of the 37th Advances in Neural Information Processing Systems (NeurIPS). the 37th Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Everything of thoughts: Defying the law of penrose triangle for thought generation. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang, arXiv:2311.042542023. 2023arXiv preprint</p>
<p>ExaRanker: Synthetic Explanations Improve Neural Rankers. Fernando Ferraretto, Thiago Laitz, Roberto Lotufo, Rodrigo Nogueira, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)2023</p>
<p>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust, Proceedings of The 12th International Conference on Learning Representations (ICLR). The 12th International Conference on Learning Representations (ICLR)2024</p>
<p>Understanding HTML with Large Language Models. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust, Findings of the Association for Computational Linguistics (EMNLP). 2023</p>
<p>Reasoning with Language Model is Planning with World Model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>The Curious Case of Neural Text Degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, Proceedings of the 8th International Conference on Learning Representations (ICLR). the 8th International Conference on Learning Representations (ICLR)2020</p>
<p>Dense Passage Retrieval for Open-Domain Question Answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Language Models can Solve Computer Tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Proceedings of the 37th Advances in Neural Information Processing Systems (NeurIPS). the 37th Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>Code as Policies: Language Model Programs for Embodied Control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, Proceedings of 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023 IEEE International Conference on Robotics and Automation (ICRA)2023</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477LLM+P: Empowering large language models with optimal planning proficiency. 2023. 2023arXiv preprint</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What Makes Good In-Context Examples for GPT-3?. 2021. 2021arXiv preprint</p>
<p>Generative Relevance Feedback with Large Language Models. Iain Mackie, Shubham Chatterjee, Jeffrey Dalton, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)2023</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted question-answering with human feedback. 2021. 2021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Proceedings of the 36th Advances in Neural Information Processing Systems (NeurIPS). the 36th Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST). the 36th Annual ACM Symposium on User Interface Software and Technology (UIST)2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 2019. 2019</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023. 2023arXiv preprint</p>
<p>Learning To Retrieve Prompts for In-Context Learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies (NAACL-HLT2022</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Proceedings of the 37th Advances in Neural Information Processing Systems (NeurIPS). the 37th Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>World of Bits: An Open-Domain Platform for Web-Based Agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, Proceedings of the 34th International Conference on Machine Learning (ICML). the 34th International Conference on Machine Learning (ICML)201770</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Proceedings of the 37th Advances in Neural Information Processing Systems (NeurIPS). the 37th Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew J Hausknecht, Proceedings of 9th International Conference on Learning Representations (ICLR). 9th International Conference on Learning Representations (ICLR)2021</p>
<p>The LongChat Team. 2023. How Long Can Open-Source LLMs Truly Promise on Context Length?. </p>
<p>Edouard Grave, and Guillaume Lample. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023. 2023arXiv preprint</p>
<p>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023. 2023arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023. 2023arXiv preprint</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The 11th International Conference on Learning Representations. 2023ICLR</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, Proceedings of the 37th Advances in Neural Information Processing Systems (NeurIPS). the 37th Advances in Neural Information Processing Systems (NeurIPS)2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th Advances in Neural Information Processing Systems (NeurIPS). the 36th Advances in Neural Information Processing Systems (NeurIPS)2022</p>
<p>Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>Web-Shop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Proceedings of 36th Conference on Neural Information Processing Systems (NeurIPS). 36th Conference on Neural Information Processing Systems (NeurIPS)2022</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, Proceedings of 37th Conference on Neural Information Processing Systems (NeurIPS). 37th Conference on Neural Information Processing Systems (NeurIPS)2023</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, Proceedings of The 11th International Conference on Learning Representations (ICLR). The 11th International Conference on Learning Representations (ICLR)2023</p>
<p>Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)2023</p>
<p>Active Example Selection for In-Context Learning. Yiming Zhang, Shi Feng, Chenhao Tan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models. Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Chi, Denny Quoc V Le, Zhou, Proceedings of The 12th International Conference on Learning Representations (ICLR). The 12th International Conference on Learning Representations (ICLR)2024</p>
<p>Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An, Proceedings of 12th International Conference on Learning Representations (ICLR). 12th International Conference on Learning Representations (ICLR)2024</p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, The 11th International Conference on Learning Representations (ICLR). 2023</p>
<p>Large language models for information retrieval: A survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen, arXiv:2308.071072023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9741 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9741</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9741</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-279070345</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.23824v2.pdf" target="_blank">Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models have sparked interest in utilizing them to aid the peer review process of scientific publication amid the peer review crisis. However, having AI models generate full reviews in the same way as human reviewers risks exacerbating the irresponsible use of LLM-generated reviews. As an alternative, we propose adopting LLMs as manuscript quality checkers. We introduce several baseline approaches and an extendable automatic evaluation framework using top reasoning LLMs as judges to tackle the difficulty of recruiting domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we validated our proposed methods with several leading reasoning LLMs from multiple vendors and assessed their performance and API costs for identifying critical errors and unsoundness problems in scientific papers. o3 exhibited the best problem identification performance among all models at a modest cost. This paper provides insights into document-based scientific understanding/reasoning and lays a foundation for future applications. Our dataset, code, and model outputs are publicly available.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9741.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9741.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic evaluation pipeline using LLMs as judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation pipeline that uses multiple top-performing reasoning LLMs as independent judges to determine whether an LLM checker's reported problems match gold error descriptions; majority (or unanimous) affirmative votes indicate a hit and are used to compute hit rate and precision estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (Gemini 2.5 Pro, Gemini 2.5 Flash, o3, o4-mini, Claude 3.7 Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Closed-source reasoning-focused LLMs from Google (Gemini 2.5 Pro/Flash), OpenAI (o3, o4-mini) and Anthropic (Claude 3.7 Sonnet) accessed via vendor APIs with differing PDF ingestion pipelines and configurable 'thinking' budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics, Physics, Computer Science (scientific manuscripts / peer review)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated LLM-as-judge: m reasoning LLMs independently judge each problem submission against the gold retraction comment; for n_j>1 the judge's most self-consistent response is used; a checker is counted as a 'hit' on a paper if it receives majority (or unanimous for stricter scoring) affirmative judge votes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary match to gold error description as judged by LLM judges (used to compute Hit Rate at k and precision estimates); judges default to 'No' unless certain (prompted).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on the WITHDRARXIV-CHECK dataset (created from WITHDRARXIV), using a held-out test set of 245 withdrawn arXiv papers with author retraction comments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The multi-judge automatic evaluation was feasible and revealed inter-judge preference differences (e.g., Gemini 2.5 Pro more lenient than o3); requiring unanimous affirmative votes reduced apparent scores, demonstrating resistance to false positives in automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Circularity risk (evaluating LLMs with LLM judges), judge bias/leniency differences across vendors, single-pass judge responses in experiments (n_j=1), and lack of human calibration may reduce reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Designed to reduce dependence on costly human experts; authors recommend human experts remain central—LLM-as-judge is a pragmatic proxy but not a replacement for human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multiple judges from different vendors/settings to reduce single-judge bias; default judges to 'No' unless certain; consider requiring unanimous/majority votes for stricter evaluation; calibrate judges with domain experts when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9741.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9741.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input-baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline ingestion approaches: PDF attachment, OCR text, LaTeX script</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three baseline ways to provide paper content to checkers: (1) upload PDF as attachment (vendor-specific PDF pipelines), (2) embed OCR-extracted text into prompts, and (3) provide LaTeX source in the prompt; each has trade-offs in fidelity to math, images, and vendor preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (Gemini 2.5 Pro, Gemini 2.5 Flash, o3, o4-mini, Claude 3.7 Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>See description above; vendors differ in how they preprocess PDFs — some use pure vision, others include extracted text plus page images.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific manuscripts rich in mathematics and physics (arXiv domains)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare model performance (hit rate, precision, token usage, cost) when the same model ingests papers via different pipelines (PDF vs LaTeX; OCR suggested for other domains).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Hit Rate at k (HR@k), Average Precision (AP@k), token usage, and estimated API cost per paper; qualitative observations (e.g., ingestion failures, OCR math errors).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>WITHDRARXIV-CHECK (LaTeX scripts available for a subset; PDF used otherwise).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PDF-vs-LaTeX effects differed by vendor: Gemini's hit rates decreased when switching to LaTeX (likely information loss), OpenAI o-series models' hit rates were stable or slightly increased (possibly better LaTeX familiarity), Claude performed poorly on PDFs but improved on LaTeX, suggesting vendor-specific ingestion issues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LaTeX approach omits images (images ignored in experiments) and may lose page/section metadata; OCR can corrupt math and lead to lower performance; vendor PDF pipelines are black-box and vary considerably.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Real-world users typically upload PDFs; LaTeX and OCR baselines allow more controlled cross-vendor comparisons than raw PDFs which confound preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Test both PDF and unified-text (OCR/LaTeX) pipelines; for math-heavy domains prefer LaTeX when available but ensure images/figure referencing are handled; report token usage and cost for practical deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9741.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9741.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HR@k / MHR@k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hit Rate at k (HR@k) and Mean Hit Rate at k (MHR@k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary detection metric: the proportion of test papers where an LLM checker produced at least one problem submission that judges deemed an exact match to the gold error, reported for up to k returned problems; MHR@k averages across repeated checker runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (Gemini 2.5 Pro, Gemini 2.5 Flash, o3, o4-mini, Claude 3.7 Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>See above; models were allowed up to k problems per paper (k=5 in experiments) and tested n_c times (n_c=1 in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific manuscript error detection (math/physics-focused arXiv withdrawals)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>An LLM checker's submission is judged against gold retraction description; if any submission matches, the paper is a 'hit'; HR@k is fraction of test-papers hit; MHR@k averages across multiple runs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>HR@k quantifies recall-like coverage of critical errors at a given budget k; dependence on k is explicitly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Test split of WITHDRARXIV-CHECK (245 papers); experiments used k=5.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>HR@5 was a principal reported metric; OpenAI o3 achieved the highest estimated hit rate in experiments (exact HR numbers vary by ingestion pipeline and judges).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>HR@k depends strongly on k and on judge behavior; ambiguous gold retraction comments can make exact-match judging conservative; multi-judge requirements reduce measured HR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Unlike full-text human review comparisons, HR@k focuses narrowly on detection of the single gold critical error per withdrawn paper; it's a pragmatic proxy for recall of critical problem detection.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report HR@k across multiple k values; use multiple judges and quorum rules to reduce false positives; accompany HR with precision estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9741.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9741.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AP@k / MAP@k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average Precision at k (AP@k) and Mean Average Precision at k (MAP@k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Precision-oriented metrics estimating the proportion of LLM-identified problem submissions that are true positives as judged by LLM judges; AP@k applied when each checker run is single (n_c=1), MAP@k averages across multiple runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>multiple (Gemini 2.5 Pro, Gemini 2.5 Flash, o3, o4-mini, Claude 3.7 Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>See above.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific manuscripts (error/unsoundness detection)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Each reported problem submission is judged by multiple LLM judges; submissions receiving majority (or unanimous) affirmative votes are counted as true positives to compute AP@k/MAP@k.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision estimate (proportion of positive predictions that are true positives) summarized as AP@k / MAP@k.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>WITHDRARXIV-CHECK test set; precision evaluated under PDF-based approach in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Larger models (Gemini 2.5 Pro and o3) had higher estimated average precision than smaller siblings; Gemini 2.5 Pro's estimated precision exceeded o3's, attributed to more cautious reporting. Total number of judged true-positive problem submissions were: o3 found 350, Gemini Pro 293, Claude 149.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No gold-standard human-based precision labels; LLM-judge subjectivity influences precision estimates; precision could not be computed for checkers that returned zero problems for a paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides a quantitative proxy for real-world usability (false-alarm rates) where human verification cost is high, but less reliable than human-curated precision labels.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Estimate precision using multiple judges and require affirmative votes; prefer conservative judge prompts; supplement with human expert calibration where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9741.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9741.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WITHDRARXIV-CHECK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WITHDRARXIV-CHECK (derived withdrawn-arXiv dataset for error detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cleaned subset (1,225 cases) of the WITHDRARXIV retraction dataset containing withdrawn arXiv papers with retraction comments indicating factual/methodological/critical errors, with a held-out test set of 245 papers used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>N/A (dataset used to evaluate LLM checkers)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>arXiv papers spanning Math, Physics, Computer Science and others (dataset biased toward math and physics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Gold error descriptions taken from authors' retraction comments were used as target annotations; LLM checkers' reported problems were matched to these gold errors via LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Presence/absence of exact-match error detection (hit) and judged true-positive submissions (precision).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Derived from WITHDRARXIV (Rao et al. 2024) and filtered automatically and manually to create WITHDRARXIV-CHECK (1,225 cases; 245 test papers).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Served as the benchmark for validating baseline checkers; allowed measurement of HR@k and AP@k across ingestion pipelines and LLM checkers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retraction comments are often terse or ambiguous; some retraction reasons not detectable from manuscript alone; possible incorrect author retraction reasons; dataset concentrated in math/physics and historical papers which may overlap with LLM training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Affords scalable automated benchmarking but lacks human-curated fine-grained annotations; complementary to smaller human-annotated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use WITHDRARXIV-CHECK for initial scaling experiments but supplement with human expert annotations, domain diversification, and post-hoc validation to address ambiguous retraction comments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9741.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9741.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Empirical findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical model comparison and observations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental findings comparing multiple reasoning LLM checkers, ingestion pipelines, token/cost footprints, and judge behaviors, highlighting performance and vendor-specific quirks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Gemini 2.5 Pro, Gemini 2.5 Flash, o3 (medium), o4-mini (medium), Claude 3.7 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>See above; models accessed via vendor APIs with specific thinking/temperature settings (detailed in Appendix B of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics, Physics, Computer Science (arXiv manuscript error detection)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Each model tested as a checker (n_c=1) returning up to k=5 problems; two LLM judges (Gemini 2.5 Pro and o3) independently judged submissions (n_j=1); primary metrics HR@5 and AP@5 reported along with token usage and estimated API cost per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>HR@5 (primary), AP@5 (precision estimate), token usage, estimated cost; also qualitative judge behavior (leniency/strictness) observed.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>WITHDRARXIV-CHECK test split (245 papers), k=5, m=2 judges, n_c=n_j=1.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>OpenAI o3 had the best problem-identification performance and a modest cost; o3 found 350 judged true-positive problem submissions, Gemini 2.5 Pro found 293, Claude found 149. Gemini Pro exhibited higher estimated precision than o3 (more cautious). Claude often returned no problems on PDFs (64.9% of test papers) but improved on LaTeX inputs, indicating ingestion issues. Token/cost footprints varied widely across vendors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Results constrained by single-run evaluations (n_c=1), single-judge-run (n_j=1), closed-source models only, and domain bias; judge behavior differences complicate direct model ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>No large-scale human expert evaluation was done due to cost; authors stress LLM checkers should augment (not replace) human reviewers and recommend human validation of identified problems.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report both hit-rate and precision estimates, include token/ cost accounting, test multiple ingestion pipelines, use multiple judges and stricter quorum rules for more conservative scoring, and prioritize human calibration for final validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>WithdrarXiv: A large-scale dataset for retraction study <em>(Rating: 2)</em></li>
                <li>When AI co-scientists fail: SPOT-a benchmark for automated verification of scientific research <em>(Rating: 2)</em></li>
                <li>Automatically evaluating the paper reviewing capability of large language models <em>(Rating: 1)</em></li>
                <li>Can large language models provide useful feedback on research papers? a large-scale empirical analysis <em>(Rating: 1)</em></li>
                <li>Automated scholarly paper review: Concepts, technologies, and challenges <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9741",
    "paper_id": "paper-279070345",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "LLM-as-judge framework",
            "name_full": "Automatic evaluation pipeline using LLMs as judges",
            "brief_description": "An automatic evaluation pipeline that uses multiple top-performing reasoning LLMs as independent judges to determine whether an LLM checker's reported problems match gold error descriptions; majority (or unanimous) affirmative votes indicate a hit and are used to compute hit rate and precision estimates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple (Gemini 2.5 Pro, Gemini 2.5 Flash, o3, o4-mini, Claude 3.7 Sonnet)",
            "llm_description": "Closed-source reasoning-focused LLMs from Google (Gemini 2.5 Pro/Flash), OpenAI (o3, o4-mini) and Anthropic (Claude 3.7 Sonnet) accessed via vendor APIs with differing PDF ingestion pipelines and configurable 'thinking' budgets.",
            "scientific_domain": "Mathematics, Physics, Computer Science (scientific manuscripts / peer review)",
            "evaluation_method": "Automated LLM-as-judge: m reasoning LLMs independently judge each problem submission against the gold retraction comment; for n_j&gt;1 the judge's most self-consistent response is used; a checker is counted as a 'hit' on a paper if it receives majority (or unanimous for stricter scoring) affirmative judge votes.",
            "evaluation_criteria": "Binary match to gold error description as judged by LLM judges (used to compute Hit Rate at k and precision estimates); judges default to 'No' unless certain (prompted).",
            "benchmark_or_dataset": "Evaluated on the WITHDRARXIV-CHECK dataset (created from WITHDRARXIV), using a held-out test set of 245 withdrawn arXiv papers with author retraction comments.",
            "results_summary": "The multi-judge automatic evaluation was feasible and revealed inter-judge preference differences (e.g., Gemini 2.5 Pro more lenient than o3); requiring unanimous affirmative votes reduced apparent scores, demonstrating resistance to false positives in automated evaluation.",
            "limitations_or_challenges": "Circularity risk (evaluating LLMs with LLM judges), judge bias/leniency differences across vendors, single-pass judge responses in experiments (n_j=1), and lack of human calibration may reduce reliability.",
            "comparison_to_human_or_traditional": "Designed to reduce dependence on costly human experts; authors recommend human experts remain central—LLM-as-judge is a pragmatic proxy but not a replacement for human validation.",
            "recommendations_or_best_practices": "Use multiple judges from different vendors/settings to reduce single-judge bias; default judges to 'No' unless certain; consider requiring unanimous/majority votes for stricter evaluation; calibrate judges with domain experts when possible.",
            "uuid": "e9741.0",
            "source_info": {
                "paper_title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Input-baselines",
            "name_full": "Baseline ingestion approaches: PDF attachment, OCR text, LaTeX script",
            "brief_description": "Three baseline ways to provide paper content to checkers: (1) upload PDF as attachment (vendor-specific PDF pipelines), (2) embed OCR-extracted text into prompts, and (3) provide LaTeX source in the prompt; each has trade-offs in fidelity to math, images, and vendor preprocessing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple (Gemini 2.5 Pro, Gemini 2.5 Flash, o3, o4-mini, Claude 3.7 Sonnet)",
            "llm_description": "See description above; vendors differ in how they preprocess PDFs — some use pure vision, others include extracted text plus page images.",
            "scientific_domain": "Scientific manuscripts rich in mathematics and physics (arXiv domains)",
            "evaluation_method": "Compare model performance (hit rate, precision, token usage, cost) when the same model ingests papers via different pipelines (PDF vs LaTeX; OCR suggested for other domains).",
            "evaluation_criteria": "Hit Rate at k (HR@k), Average Precision (AP@k), token usage, and estimated API cost per paper; qualitative observations (e.g., ingestion failures, OCR math errors).",
            "benchmark_or_dataset": "WITHDRARXIV-CHECK (LaTeX scripts available for a subset; PDF used otherwise).",
            "results_summary": "PDF-vs-LaTeX effects differed by vendor: Gemini's hit rates decreased when switching to LaTeX (likely information loss), OpenAI o-series models' hit rates were stable or slightly increased (possibly better LaTeX familiarity), Claude performed poorly on PDFs but improved on LaTeX, suggesting vendor-specific ingestion issues.",
            "limitations_or_challenges": "LaTeX approach omits images (images ignored in experiments) and may lose page/section metadata; OCR can corrupt math and lead to lower performance; vendor PDF pipelines are black-box and vary considerably.",
            "comparison_to_human_or_traditional": "Real-world users typically upload PDFs; LaTeX and OCR baselines allow more controlled cross-vendor comparisons than raw PDFs which confound preprocessing.",
            "recommendations_or_best_practices": "Test both PDF and unified-text (OCR/LaTeX) pipelines; for math-heavy domains prefer LaTeX when available but ensure images/figure referencing are handled; report token usage and cost for practical deployment.",
            "uuid": "e9741.1",
            "source_info": {
                "paper_title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "HR@k / MHR@k",
            "name_full": "Hit Rate at k (HR@k) and Mean Hit Rate at k (MHR@k)",
            "brief_description": "Primary detection metric: the proportion of test papers where an LLM checker produced at least one problem submission that judges deemed an exact match to the gold error, reported for up to k returned problems; MHR@k averages across repeated checker runs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple (Gemini 2.5 Pro, Gemini 2.5 Flash, o3, o4-mini, Claude 3.7 Sonnet)",
            "llm_description": "See above; models were allowed up to k problems per paper (k=5 in experiments) and tested n_c times (n_c=1 in this work).",
            "scientific_domain": "Scientific manuscript error detection (math/physics-focused arXiv withdrawals)",
            "evaluation_method": "An LLM checker's submission is judged against gold retraction description; if any submission matches, the paper is a 'hit'; HR@k is fraction of test-papers hit; MHR@k averages across multiple runs.",
            "evaluation_criteria": "HR@k quantifies recall-like coverage of critical errors at a given budget k; dependence on k is explicitly reported.",
            "benchmark_or_dataset": "Test split of WITHDRARXIV-CHECK (245 papers); experiments used k=5.",
            "results_summary": "HR@5 was a principal reported metric; OpenAI o3 achieved the highest estimated hit rate in experiments (exact HR numbers vary by ingestion pipeline and judges).",
            "limitations_or_challenges": "HR@k depends strongly on k and on judge behavior; ambiguous gold retraction comments can make exact-match judging conservative; multi-judge requirements reduce measured HR.",
            "comparison_to_human_or_traditional": "Unlike full-text human review comparisons, HR@k focuses narrowly on detection of the single gold critical error per withdrawn paper; it's a pragmatic proxy for recall of critical problem detection.",
            "recommendations_or_best_practices": "Report HR@k across multiple k values; use multiple judges and quorum rules to reduce false positives; accompany HR with precision estimates.",
            "uuid": "e9741.2",
            "source_info": {
                "paper_title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "AP@k / MAP@k",
            "name_full": "Average Precision at k (AP@k) and Mean Average Precision at k (MAP@k)",
            "brief_description": "Precision-oriented metrics estimating the proportion of LLM-identified problem submissions that are true positives as judged by LLM judges; AP@k applied when each checker run is single (n_c=1), MAP@k averages across multiple runs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "multiple (Gemini 2.5 Pro, Gemini 2.5 Flash, o3, o4-mini, Claude 3.7 Sonnet)",
            "llm_description": "See above.",
            "scientific_domain": "Scientific manuscripts (error/unsoundness detection)",
            "evaluation_method": "Each reported problem submission is judged by multiple LLM judges; submissions receiving majority (or unanimous) affirmative votes are counted as true positives to compute AP@k/MAP@k.",
            "evaluation_criteria": "Precision estimate (proportion of positive predictions that are true positives) summarized as AP@k / MAP@k.",
            "benchmark_or_dataset": "WITHDRARXIV-CHECK test set; precision evaluated under PDF-based approach in experiments.",
            "results_summary": "Larger models (Gemini 2.5 Pro and o3) had higher estimated average precision than smaller siblings; Gemini 2.5 Pro's estimated precision exceeded o3's, attributed to more cautious reporting. Total number of judged true-positive problem submissions were: o3 found 350, Gemini Pro 293, Claude 149.",
            "limitations_or_challenges": "No gold-standard human-based precision labels; LLM-judge subjectivity influences precision estimates; precision could not be computed for checkers that returned zero problems for a paper.",
            "comparison_to_human_or_traditional": "Provides a quantitative proxy for real-world usability (false-alarm rates) where human verification cost is high, but less reliable than human-curated precision labels.",
            "recommendations_or_best_practices": "Estimate precision using multiple judges and require affirmative votes; prefer conservative judge prompts; supplement with human expert calibration where possible.",
            "uuid": "e9741.3",
            "source_info": {
                "paper_title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "WITHDRARXIV-CHECK",
            "name_full": "WITHDRARXIV-CHECK (derived withdrawn-arXiv dataset for error detection)",
            "brief_description": "A cleaned subset (1,225 cases) of the WITHDRARXIV retraction dataset containing withdrawn arXiv papers with retraction comments indicating factual/methodological/critical errors, with a held-out test set of 245 papers used in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "N/A (dataset used to evaluate LLM checkers)",
            "llm_description": null,
            "scientific_domain": "arXiv papers spanning Math, Physics, Computer Science and others (dataset biased toward math and physics).",
            "evaluation_method": "Gold error descriptions taken from authors' retraction comments were used as target annotations; LLM checkers' reported problems were matched to these gold errors via LLM judges.",
            "evaluation_criteria": "Presence/absence of exact-match error detection (hit) and judged true-positive submissions (precision).",
            "benchmark_or_dataset": "Derived from WITHDRARXIV (Rao et al. 2024) and filtered automatically and manually to create WITHDRARXIV-CHECK (1,225 cases; 245 test papers).",
            "results_summary": "Served as the benchmark for validating baseline checkers; allowed measurement of HR@k and AP@k across ingestion pipelines and LLM checkers.",
            "limitations_or_challenges": "Retraction comments are often terse or ambiguous; some retraction reasons not detectable from manuscript alone; possible incorrect author retraction reasons; dataset concentrated in math/physics and historical papers which may overlap with LLM training data.",
            "comparison_to_human_or_traditional": "Affords scalable automated benchmarking but lacks human-curated fine-grained annotations; complementary to smaller human-annotated benchmarks.",
            "recommendations_or_best_practices": "Use WITHDRARXIV-CHECK for initial scaling experiments but supplement with human expert annotations, domain diversification, and post-hoc validation to address ambiguous retraction comments.",
            "uuid": "e9741.4",
            "source_info": {
                "paper_title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Empirical findings",
            "name_full": "Empirical model comparison and observations",
            "brief_description": "Experimental findings comparing multiple reasoning LLM checkers, ingestion pipelines, token/cost footprints, and judge behaviors, highlighting performance and vendor-specific quirks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Gemini 2.5 Pro, Gemini 2.5 Flash, o3 (medium), o4-mini (medium), Claude 3.7 Sonnet",
            "llm_description": "See above; models accessed via vendor APIs with specific thinking/temperature settings (detailed in Appendix B of the paper).",
            "scientific_domain": "Mathematics, Physics, Computer Science (arXiv manuscript error detection)",
            "evaluation_method": "Each model tested as a checker (n_c=1) returning up to k=5 problems; two LLM judges (Gemini 2.5 Pro and o3) independently judged submissions (n_j=1); primary metrics HR@5 and AP@5 reported along with token usage and estimated API cost per paper.",
            "evaluation_criteria": "HR@5 (primary), AP@5 (precision estimate), token usage, estimated cost; also qualitative judge behavior (leniency/strictness) observed.",
            "benchmark_or_dataset": "WITHDRARXIV-CHECK test split (245 papers), k=5, m=2 judges, n_c=n_j=1.",
            "results_summary": "OpenAI o3 had the best problem-identification performance and a modest cost; o3 found 350 judged true-positive problem submissions, Gemini 2.5 Pro found 293, Claude found 149. Gemini Pro exhibited higher estimated precision than o3 (more cautious). Claude often returned no problems on PDFs (64.9% of test papers) but improved on LaTeX inputs, indicating ingestion issues. Token/cost footprints varied widely across vendors.",
            "limitations_or_challenges": "Results constrained by single-run evaluations (n_c=1), single-judge-run (n_j=1), closed-source models only, and domain bias; judge behavior differences complicate direct model ranking.",
            "comparison_to_human_or_traditional": "No large-scale human expert evaluation was done due to cost; authors stress LLM checkers should augment (not replace) human reviewers and recommend human validation of identified problems.",
            "recommendations_or_best_practices": "Report both hit-rate and precision estimates, include token/ cost accounting, test multiple ingestion pipelines, use multiple judges and stricter quorum rules for more conservative scoring, and prioritize human calibration for final validation.",
            "uuid": "e9741.5",
            "source_info": {
                "paper_title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "WithdrarXiv: A large-scale dataset for retraction study",
            "rating": 2,
            "sanitized_title": "withdrarxiv_a_largescale_dataset_for_retraction_study"
        },
        {
            "paper_title": "When AI co-scientists fail: SPOT-a benchmark for automated verification of scientific research",
            "rating": 2,
            "sanitized_title": "when_ai_coscientists_fail_spota_benchmark_for_automated_verification_of_scientific_research"
        },
        {
            "paper_title": "Automatically evaluating the paper reviewing capability of large language models",
            "rating": 1,
            "sanitized_title": "automatically_evaluating_the_paper_reviewing_capability_of_large_language_models"
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis",
            "rating": 1,
            "sanitized_title": "can_large_language_models_provide_useful_feedback_on_research_papers_a_largescale_empirical_analysis"
        },
        {
            "paper_title": "Automated scholarly paper review: Concepts, technologies, and challenges",
            "rating": 1,
            "sanitized_title": "automated_scholarly_paper_review_concepts_technologies_and_challenges"
        }
    ],
    "cost": 0.0112357,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation
7 Jul 2025</p>
<p>Tianmai M Zhang tianmai@uw.edu 
University of Washington</p>
<p>Neil F Abernethy neila@uw.edu 
University of Washington</p>
<p>Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation
7 Jul 202589F1725D50DEF98300BAB28069EAD7B6arXiv:2505.23824v2[cs.CL]
Recent advancements in large language models have sparked interest in utilizing them to aid the peer review process of scientific publication amid the peer review crisis.However, having AI models generate full reviews in the same way as human reviewers risks exacerbating the irresponsible use of LLM-generated reviews.As an alternative, we propose adopting LLMs as manuscript quality checkers.We introduce several baseline approaches and an extendable automatic evaluation framework using top reasoning LLMs as judges to tackle the difficulty of recruiting domain experts for manual evaluation.Utilizing papers withdrawn from arXiv, we validated our proposed methods with several leading reasoning LLMs from multiple vendors and assessed their performance and API costs for identifying critical errors and unsoundness problems in scientific papers.o3 exhibited the best problem identification performance among all models at a modest cost.This paper provides insights into document-based scientific understanding/reasoning and lays a foundation for future applications.Our dataset, code, and model outputs are publicly available.</p>
<p>Introduction</p>
<p>Recent advancements in the domain intelligence of large language models (LLMs) have initiated interest in utilizing them to aid the peer review process of scientific publication, especially in consideration of the peer review crisis due to the skyrocketing number of paper submissions in recent years (Kim et al., 2025).Researchers have reported receiving reviews that have likely been written with LLMs, and concern is growing over whether and how AI can be responsibly applied to aid peer review (Naddaf, 2025;Kwon, 2025).The extensive knowledge and high efficiency of LLMs seem promising for streamlining laborious peer review, but irresponsible uses of LLM-generated reviews could significantly undermine trust in the long-established peer review process and publisher credibility.</p>
<p>Several studies have formally explored and evaluated the quality of LLM-generated reviews.An early study by Liang et al. (2024) reported substantial overlap between GPT-generated reviews and human reviews, and participant-reported helpfulness of GPT-generated feedback.Later studies (Du et al., 2024;Zhou et al., 2024;Shin et al., 2025) revealed defects in LLM-generated reviews, such as superficial comments and a lack of criticism or novelty assessment.Other studies (D'Arcy et al., 2024;Gao et al., 2024;Taechoyotin et al., 2024;Tan et al., 2024;Tyser et al., 2024;Yu et al., 2024;Zhu et al., 2025) developed technical methods to improve LLM review generation.However, all of these studies focused on the scenario where LLMs generate full reviews in the same way as human reviewers, which risks exacerbating the irresponsible use of LLM-generated reviews.The most common evaluation method in these studies was comparing LLM-generated reviews with human reviews, either manually or computationally.</p>
<p>Inspired by the Black Spatula Project1 that seeks to identify errors in scientific papers, we propose using LLMs as manuscript quality checkers rather than efficient reviewers who write full reviews.In this way, LLMs would no longer be competing with human reviewers.Instead, LLMs would complete necessary sub-tasks, thereby saving reviewers' time.This could allow human reviewers to focus more on leveraging their domain expertise to evaluate important aspects of the manuscript, such as completeness, coherence, novelty, and significance.</p>
<p>In this work, we consider the identification of critical errors and unsoundness problems that may invalidate the conclusions of a paper, a key subtask in peer review, as the main goal of an LLM manuscript checker.We present an extensible framework including several baseline approaches and an automatic evaluation pipeline, which also supports other related tasks.Utilizing papers withdrawn from arXiv, we validate our proposed methods with several top-performing reasoning LLMs and assess their performance and costs to inform future research and applications.</p>
<p>Methods</p>
<p>Dataset</p>
<p>We utilized WITHDRARXIV (Rao et al., 2024), a large-scale dataset of papers withdrawn from arXiv by September 2024, along with associated retraction comments from authors and well-defined retraction categories.The most common retraction category, "factual/methodological/other critical errors in manuscript", contains 6,018 candidate cases with critical errors that would potentially invalidate study conclusions, such as flawed experimental designs, incorrect data analyses, and proof/lemma errors (Rao et al., 2024).</p>
<p>Since retraction comments were typically short and did not necessarily contain clear mentions of errors (e.g., "This paper has been withdrawn due to some mistakes"), we further filtered the dataset with the help of LLMs.Specifically, de-identified retraction comments were first provided to Gemini 2.5 Flash (preview-04-17) to determine whether each retraction comment clearly specified the error.This step resulted in a subset of 2,190 cases.Our manual review further excluded cases that (1) were incorrectly identified during LLM screening;</p>
<p>(2) belonged to different versions of the same paper; (3) were not in English; (4) suspiciously used exactly the same retraction reason ("The author has withdrawn this paper due to a crucial sign error in equation 1") that seems to have been a template provided by arXiv in early years; (5) contained problems that were unlikely to be detectable from the manuscript alone (e.g., an error in a core reference, a bug in code, unreliable raw data, problems revealed by new observations, etc.).We also corrected mistakenly redacted theorem names in the retraction comments.The final dataset, named WITHDRARXIV-CHECK, contains 1,225 cases in total.</p>
<p>We randomly sampled 20% of the dataset (245 cases) as the test set for evaluation experiments.The remaining 80% (980 cases) of the dataset was set aside for training and validation, although these latter two steps were not considered in this work whose main objective was to establish baseline ap- proaches and evaluation methods.Dataset characteristics are provided in Table 1.</p>
<p>Baseline Approaches</p>
<p>We propose three baselines for LLMs to perform quality checks based on different approaches to ingest the papers: (1) paper PDF as an attachment, (2) optical character recognition (OCR) results of the PDF in the prompt, and (3) LaTeX script of the paper in the prompt.The first approach simulates the common real-world scenario wherein users upload the PDF file of a paper as an attachment and ask an LLM to read it.However, the PDF preprocessing pipeline (and its performance) likely varies by LLM vendor.</p>
<p>The other two approaches based on OCR or La-TeX help decouple PDF preprocessing steps from LLM inference by providing the same input to all models, thereby enabling fairer comparisons of their competencies.However, LLM performance under the OCR approach could be limited considerably by OCR errors, especially when these errors appear in key math formulae, which are potentially more difficult to accurately transcribe.The LaTeXbased approach could perfectly retain the original math formulae, albeit with the inclusion of LaTeX markup, but may also introduce additional noise and information loss (e.g., page and section number).This method is also restricted to papers with available LaTeX source scripts.</p>
<p>Considering the nature of the dataset, we evaluated the PDF-based approach and the LaTeX-based approach in this work.For the small proportion of papers without available LaTeX scripts (Table 1), we resorted to utilizing the problems identified by the same model through the PDF-based approach.Images were ignored when using LaTeX scripts in the absence of a simple method to provide LLMs with images and correct image referencing at the same time4 .For future work on papers in other scientific domains where PDF is the standard format for paper dissemination, we recommend testing the OCR-based approach as one of the baselines.</p>
<p>In our experiments, both approaches utilized the same simplistic, general task instruction (Appendix A).In short, LLMs were instructed to produce a list of up to k problems or errors that are the most critical in a given paper.The prompt was not customized for our dataset that is rich in math and physics papers.Each LLM checker was tested n c (c for checker) times with each paper in consideration of potential variations in outputs.</p>
<p>Evaluation</p>
<p>Considering the daunting cost of recruiting domain experts to manually evaluate LLM-identified scientific errors, we propose an automatic evaluation pipeline to streamline the process.Inspired by LLM-as-a-judge and the idea of LLMs -You Can't Please Them All5 , we utilized m top-performing LLMs to serve as judges.Ideally, LLM judges should be from different LLM vendors to maximize preference diversity.Each LLM judge independently evaluates an LLM checker's problem submissions one by one for n j (j for judge) times to determine whether they contain an exact match to the gold error description from the authors.For n j &gt; 1, the most self-consistent answer of the judge is taken as its final decision.If an LLM checker receives a majority of (or all, for a stricter evaluation) affirmative votes from LLM judges, it is deemed to have made a hit on a paper.LLM checkers were primarily evaluated by their hit rates on test papers.Since hit rate is likely associated with k, the number of problems/errors allowed in generation, we report this metric as the Hit Rate at k (HR@k).For n c &gt; 1 (i.e., each LLM checker is tested on a paper more than once), the metric becomes the Mean Hit Rate at k (MHR@k).</p>
<p>Another metric of interest from the application perspective is the proportion of true positives among all positive predictions, i.e., precision.In our task, all LLM-identified problems or errors are positive predictions, and an LLM checker with a higher precision would be more usable in realworld application.We again take the LLMs-asjudges approach, where each judge independently assesses n j times whether an individual problem submission is a true positive based on the paper in PDF, and a submission receiving a majority of (or all) affirmative votes is considered a true positive.In this way, we are able to obtain a rough estimate of the actual precision of LLM-identified problems from a paper.Please note that there is no gold standard for precision evaluation in our experiment, and that a case in which an LLM checker found no problem is skipped because a precision value cannot be calculated.LLM checkers were evaluated by their Average Precision (AP@ k) on test papers if n c = 1, or Mean Average Precision (MAP@ k) if n c &gt; 1. Prompts for the judges can be found in Appendix A.</p>
<p>Experiment Setup</p>
<p>In this work, we took k = 5, n c = n j = 1, and m = 2, i.e., each LLM checker was tested once with each paper and was allowed to report up to 5 problems, and 2 LLMs served as judges, each judging a problem submission once.</p>
<p>The following reasoning LLMs were tested as paper quality checkers: Google's Gemini 2.5 Pro (preview-05-06) and Gemini 2.5 Flash (preview-04-17); OpenAI's o3 (2025-04-16) and o4-mini (2025-04-16); Anthropic's Claude 3.7 Sonnet (20250219).</p>
<p>The two LLM judges are Gemini 2.5 Pro (preview-06-05, for its better performance on related benchmarks than preview-05-06) and o3 (2025-04-16).We initially planned for m = 3 with Claude 3.7 Sonnet as the last judge, but its overly low hit rate under the PDF-based approach indicates that it might not qualify for serving as a judge in this task.Under m = 2, both judges must vote affirmatively to confirm a hit or a true positive.</p>
<p>LLMs were accessed via their official APIs using Python.Model parameters used are provided in Appendix B. Reasoning effort or thinking budget was kept as the default or automatic setting if applicable.LLMs were not given access to any tools, including web search since LLMs might find the retraction comments online.</p>
<p>Results</p>
<p>Table 2 provides the main results assessing LLM quality checkers.Besides numbers of identified problems and performance metrics, we recorded token usage to inform future work that seeks to apply LLMs at a larger scale.Average costs of reviewing a paper under each pipeline-LLM combination were estimated based on the standard API pricing 6 of the LLM vendors in June 2025.</p>
<p>Compared to the Gemini family, OpenAI's oseries models always tended to make full use of the 5 problem slots allowed.o3 achieved the highest estimated hit rate among all models at a modest cost.After switching to LaTeX, the hit rates of Gemini models decreased, likely due to the information loss.In contrast, the hit rates of OpenAI o-series models remained around the same or slightly increased, suggesting resistance to format change or a higher familiarity with LaTeX obtained in its training processes.When papers are provided as PDFs, Claude 3.7 Sonnet found no problem in 64.9% of test papers, leading to a low hit rate compared to other reasoning models.Interestingly, both its number of identified problems and hit rate increased after switching to LaTeX, suggesting potential obstacles in Claude's PDF ingestion pipeline or scientific understanding via PDF.</p>
<p>Precision was assessed only under the PDFbased approach due to both its proximity to realworld usage and the availability of all information (especially figures and page/section numbers) in PDFs.Similar to the trend in hit rate results, larger 6 On June 10, OpenAI dropped the price of o3 API by 80%.models (Gemini 2.5 Pro and o3) received higher average precision estimates than their lightweight siblings.Gemini 2.5 Pro had a higher estimated precision than o3, likely due to its cautiousness in reporting problems.In terms of the total number of reported problems that were deemed true positives by both judges, the o3 checker found 350, Gemini Pro found 293, and Claude found only 149.This again suggests that o3 was the most competent problem finder in our evaluation, although additional verification steps are certainly necessary in future attempts to improve its precision.</p>
<p>Large differences in the input token usage between each model family reflect distinct PDF ingestion pipelines of the three LLM vendors.Unlike Gemini models that understand PDFs purely with vision7 , OpenAI and Anthropic provide their LLMs with both the extracted text and an image of each page 8,9 .It remains unclear why Claude has an exceptionally high input token usage for PDF files, and why o4-mini sometimes used slightly more input tokens than o3.After switching to unified La-TeX scripts, all models used the same magnitude of input tokens, although Claude still consumed more.Regarding thinking token usage, Gemini models notably spent several times as many thinking tokens as o3 and o4-mini under both approaches, but this potentially overthinking behavior did not result in higher hit rates.</p>
<p>Additional evaluation results by individual judges are shown in Table 3 in the Appendix.Compared to performance scores determined by a single judge, final scores reasonably dropped due to the difficulty of receiving affirmative votes from both judges, which demonstrates some resistance of our multi-judge approach to potential false positives in automatic evaluation.The Gemini 2.5 Pro judge is consistently more lenient than o3, sometimes making hallucinatory assumptions about the relationship between a problem submission and the gold error description.For example, when comparing an LLM checker's report of a missing factor in an unnumbered equation after a specific sentence and a retraction comment regarding an error in equation 13, the Gemini 2.5 Pro judge said it is "extremely likely" that the erroneous equation is equation 13.In our tests with different judge instructions, Gemini 2.5 Pro was also far less responsive than o3 to important instructions such as "Default your answer to 'No' and only give 'Yes' if you are certain".The phrase "exactly the same problem" in the prompt for judges to determine hits is particularly necessary for Gemini 2.5 Pro, without which it would give an even higher proportion of affirmative votes, although the trend in hit rate results remained the same.These observations reveal preference differences between models from different vendors and thus highlight the necessity of adopting multiple judges or model settings instead of a single judge while using the LLM-as-a-judge approach.</p>
<p>Discussion</p>
<p>This work introduces and validates a framework for automatic evaluation of LLMs for scientific quality checking.Evaluation results revealed impressive performance of leading reasoning LLMs in reviewing scientific papers for critical errors and unsoundness problems.We believe that our proposed design, after careful improvement and domain expansion, has the potential to serve as a new benchmark for document-based scientific understanding and reasoning.Our design is also generalizable to the detection of many other types of errors or problems that may appear in scientific papers, such as data errors, content inconsistencies, unmet publication requirements, and undeclared limitations, as long as gold standard annotations are available.</p>
<p>The idea of using natural language processing (NLP) for detecting flaws in scientific papers is, in fact, not completely new (Kuznetsov et al., 2024).Previous studies have explored the application of NLP in the detection of statistical reporting inconsistencies (Nuijten and Polanin, 2020), mathematical or conceptual errors (Liu and Shah, 2023), and inaccurate citations (Sarol et al., 2024;Zhang and Abernethy, 2024).There are also attempts in the industry to develop LLM-driven tools for similar purposes (Naddaf, 2025).The core novelty of our work is that we present a reusable framework for the formal evaluation of LLM capabilities in a wide range of specialized tasks related to scientific quality checking using real, full papers.</p>
<p>In this work, we only assessed the performance of the most simplistic approach-providing an LLM with a paper and tasking it with finding problems.More complex approaches are likely to demonstrate further gains in LLM performance.For example, one can further customize the prompt (e.g., by considering the scientific field a paper belongs to), design a propose-then-verify workflow, expand task input (e.g., by including supplementary materials and references of a paper, or additional domain knowledge through retrieval from a knowledge base) and tools (e.g., code execution), fine-tune models for specific reviewing tasks, or adopt multi-agent collaboration.</p>
<p>One potential concern over the validity of our evaluation results is that LLMs might have seen the test papers (or other versions of them) in their training data.However, recent studies on data contamination and membership inference attacks (Duan et al., 2024;Fu et al., 2025) suggest that LLMs are unlikely to memorize individual instances from pre-training data due to the combination of few training epochs and enormous corpora.Reasoning LLMs also experienced complex fine-tuning and post-training processes, which could further obfuscate memorized information.Nevertheless, it would be beneficial to evaluate LLMs on papers that are unlikely to be included in the training phase, such as those published after the knowledge cutoff dates of LLMs.</p>
<p>Copyright restrictions pose a key challenge to the automatic review of scientific papers in multiple fields at scale.In this work, we circumvent this challenge by utilizing publicly available arXiv papers.In real-world applications of LLMs, researchers must consider confidentiality and legal risks before sending private, unpublished manuscripts or copyrighted papers to public interfaces provided by LLM service vendors.We also advise developers who wish to apply LLM review at scale to refrain from posting LLM-generated reviews online before performing careful examination for false positives.</p>
<p>Furthermore, we would like to restate our standpoint that human experts should always be at the center of peer review.Although our results demonstrate seemingly impressive capabilities of LLMs in finding critical problems in papers in the domains considered, it should not be interpreted as meaning that LLMs at the current stage are broadly competent to replace human reviewers.Instead, journal publishers and conference organizers may consider incorporating LLM quality checkers into initial assessments of manuscripts (Bauchner and Rivara, 2024), thereby reducing the burden on reviewers.</p>
<p>There can also be other ways of leveraging LLMs for peer review.For example, Kim et al. (2025) proposed presenting LLM-generated reviews alongside human reviews to "potentially" discourage irresponsible human reviewers.However, these diverse approaches would likely necessitate separate lines of sociotechnical research with different task formulations, methodologies, and cost-effectiveness.We advocate that LLM-aided review processes should be implemented ethically and transparently (Lin et al., 2023;Zhuang et al., 2025), in cooperation with the wider scientific community to ensure that they result in a net increase in trust in the scientific literature.This is especially important at a time when AI tools are displacing human employees and the trust in science is at risk.</p>
<p>Limitation</p>
<p>As a proof-of-concept study, this work has several limitations.First, only closed-source reasoning LLMs were evaluated.Future work may consider comparing different PDF preprocessing pipelines and open-source LLMs.Second, our evaluation metrics may suffer inaccuracies due to (1) ambiguities in paper authors' retraction comments (e.g., "withdrawn due to a crucial error in Lemma 2", without specification of the error, (2) automatic evaluation utilizing LLMs without human involvement, and (3) our settings that each LLM checker was tested only once per paper and each LLM judge graded each submission only once.Nevertheless, a higher score under parallel evaluation still reasonably indicates better performance of an LLM checker.We expect gradual improvements in the reliability of the LLMs-as-judges approach as lead-ing models continue to evolve.Third, the impacts of experiment parameters or alternate prompts were not formally investigated or optimized.In addition, our results based on a dataset rich in math and physics papers published in the past may not generalize well to papers in other scientific domains or future papers.Last, since we were unable to recruit domain experts for manual annotation, it remains possible that some cases in the dataset contain problems that could not be detected based on the manuscript alone, or that the authors gave inaccurate reasons for retraction, which may pose an upper limit for model performance.If conditions permit, future work should also consider involving domain experts in calibrating the LLM judges to resolve potential biases caused by circular evaluation of LLMs using LLMs.</p>
<p>A concurrent work on arXiv by Son et al. ( 2025) also utilizes the WITHDRARXIV dataset for automatic error detection in scientific papers.Unlike their work which focuses on benchmarking LLMs, our work approaches the same topic more from an application perspective.Their main metrics overlap substantially with our hit rate, although there is a difference in the meaning of k.Other key differences include: • Son et al. present a small but better annotated dataset of latest papers designed solely for onetime benchmarking, whereas our dataset is much larger and contains more papers from the past, also including a training set which may directly benefit future work.• They normalized all papers using OCR, whereas we assessed LLMs with papers in PDF and La-TeX formats.• They used GPT-4.1 to determine whether an LLM's error submissions match the gold error annotation, whereas we adopted multiple reasoning LLMs as judges.• They treated all LLM-identified problems that did not directly match the gold error description as wrong answers, whereas we allowed some flexibility through LLM reasoning and attempted to assess the precision of these additional answers.These factors could explain the gap between their and our evaluation results.The strengths of their study include the use of domain experts for further quality control of their benchmark dataset, inclusion of open-source LLMs in evaluation, and a study of variations in LLM outputs via resampling.The two efforts are complementary, and both report the leading performance of o3.</p>
<p>Data Availability</p>
<p>Our WITHDRARXIV-CHECK dataset, experiment code, and model outputs (including thinking outputs if available) are available on Github 10 .Readers may use the dataset for further experiments, reproduce the numbers in our tables, or perform a closer inspection of the model outputs to obtain further insights into LLM behaviors.We welcome discussions, collaborations, and funding for future work.</p>
<p>Location: {location} Explanation: {explanation} I checked the paper and noticed that the authors have the following retraction comment: {retraction_comment} Is my colleague referring to exactly the same problem mentioned in the retraction comment?Your final answer should be "Yes" or "No".Default your answer to "No" and only give "Yes" if you are certain.You may explain your decision but please be concise.</p>
<p>Prompt for LLM judges to determine true positives: My colleague was reading this paper and said there is a critical problem in it, as described below: Problem: {problem} Location: {location} Explanation: {explanation} Is this problem a true problem or a false alarm?Please be careful because I don't want to get the authors into trouble by mistake.In your final answer, clearly indicate "Yes, it is a true problem" or "No, it is a false alarm".Make your best decision if you are unsure.You may explain your decision but please be concise.</p>
<p>{paper_attachment}</p>
<p>B Model Parameters</p>
<p>Gemini 2.5 Pro and Gemini 2.5 Flash: thinking budget: default (automatic) include thoughts: True tools: [] temperature: 0 seed: 42 o3 and o4-mini: reasoning effort: defaults to "medium" reasoning summary: "auto" tools: [] temperature and seed: not supported Claude 3.7 Sonnet: max tokens: 16,000 thinking type: "enabled" thinking budget: 14,000 tools: [] temperature: 1 (required for thinking) seed: not supported</p>
<p>Table 1 :
1
Dataset characteristics.
TrainTestSample size980245Time span, n (%)2007-2012155 (16%) 32 (13%)2013-2018487 (50%) 114 (47%)2019-2024338 (34%) 99 (40%)Main subject, n (%)Math492 (50%) 128 (52%)Physics 2256 (26%) 70 (29%)Computer Science 196 (20%) 37 (15%)Others 336 (4%)10 (4%)Page countMedian1414[Min, Max][1, 156][2, 136]LaTeX script available, n (%)-216 (88%)</p>
<p>Table 2 :
2
Checker Model # Prob.Found Performance (%) Avg.Token Usage (/paper) Cost Est.Numbers of identified problems, performance metrics, token usage, and estimated of LLM checkers.
Avg. Q1, Q3 HR@5 AP@5Input Think Output ($/paper)PDF as an attachmentGemini 2.5 Pro3.33, 439.235.24,678 14,2288810.157Gemini 2.5 Flash3.83, 538.423.34,6788,7136440.033o3 (medium)4.85, 548.229.516,594 3,1527290.064o4-mini (medium) 4.75, 538.026.717,760 3,5827010.038Claude 3.7 Sonnet 1.60, 411.036.443,357 1,6303110.159LaTeX script in promptGemini 2.5 Pro3.33, 436.3-20,644 18,0691,0330.217Gemini 2.5 Flash3.63, 534.7-20,644 13,2476670.052o3 (medium)4.85, 550.6-21,990 3,1569270.077o4-mini (medium) 4.55, 538.8-22,287 3,4216850.043Claude 3.7 Sonnet 3.41, 524.1-28,284 2,7015150.133
https://the-black-spatula-project.github.io
 Contains physics, cond-mat, quant-ph, astro-ph, etc. <br />
Contains stat, q-bio, eess, econ, and q-fin.
o-series models cannot read images in base64 format.
https://www.kaggle.com/competitions/ llms-you-cant-please-them-all
https://ai.google.dev/gemini-api/docs/ document-processing
https://platform.openai.com/docs/guides/ pdf-files?api-mode=responses
https://docs.anthropic.com/en/docs/ build-with-claude/pdf-support
AcknowledgmentsWe thank community members of the Black Spatula Project for their inspiring discussions, especially Ugo Dos Reis for highlighting the WITH-DRARXIV dataset in December 2024, Delip Rao for instructions on how to use this dataset, and Casey Wimsatt and David Reinstein for constructive feedback on early versions of this paper.The authors also thank Yuxin Xu for her help with the API.A Prompt TemplatesPrompt for LLM quality checkers:Please check the attached paper for critical errors and unsoundness problems that would invalidate the conclusions.You can ignore minor issues (e.g, typos and formatting errors) and limitations that have been properly acknowledged.In your final output, give me up to {k} most critical problems as a JSON object using the following schema: Entry = {"Problem": str, "Location": str, "Explanation": str}, Return: list
Use of artificial intelligence and the future of peer review. Howard Bauchner, Frederick P Rivara, 10.1093/haschl/qxae058Health Affairs Scholar. 252024</p>
<p>MARG: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024Preprint</p>
<p>LLMs assist NLP researchers: Critique paper (meta-)reviewing. Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Ranran Haoran, Vipul Zhang, Yinghui Gupta, Tao Li, Fei Li, Qin Wang, Tianlin Liu, Pengzhi Liu, Congying Gao, Chen Xia, Cheng Xing, Zhaowei Jiayang, Ying Wang, Raj Sanjay Su, Ruohao Shah, Jing Guo, Haoran Gu, Kangda Li, Zihao Wei, Lu Wang, Surangika Cheng, Meng Ranathunga, Jie Fang, Fei Fu, Ruihong Liu, Eduardo Huang, Yixin Blanco, Rui Cao, Philip S Zhang, Wenpeng Yu, Yin, 10.18653/v1/2024.emnlp-main.292Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Does data contamination detection work (well) for LLMs? a survey and evaluation on detection assumptions. Anshuman Michael Duan, Niloofar Suri, Sewon Mireshghallah, Weijia Min, Luke Shi, Yulia Zettlemoyer, Yejin Tsvetkov, David Choi, Hannaneh Evans, Hajishirzi, Findings of the Association for Computational Linguistics: NAACL 2025. Albuquerque, New MexicoAssociation for Computational Linguistics2024. 202510Ozlem Uzuner, Meliha Yetisgen, and Fei Xia</p>
<p>Reviewer2: Optimizing review generation through prompt generation. Zhaolin Gao, Kianté Brantley, Thorsten Joachims, arXiv:2402.108862024Preprint</p>
<p>Position: The AI conference peer review crisis demands author feedback and reviewer rewards. Jaeho Kim, Yunseok Lee, Seulki Lee, Proceedings of the 42nd International Conference on Machine Learning. the 42nd International Conference on Machine LearningVancouver, CanadaPMLR2025</p>
<p>What can natural language processing. Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Margot Mausam, Aurélie Mieskes, Danish Névéol, Lizhen Pruthi, Roy Qu, Noah A Schwartz, Thamar Smith, Jingyan Solorio, Xiaodan Wang, Zhu, arXiv:2405.065632024Nihar B. Shah, and Iryna GurevychAnna RogersPreprint</p>
<p>Is it ok for AI to write science papers? Nature survey shows researchers are split. Diana Kwon, 10.1038/d41586-025-01463-8Nature. 6412025</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Scott Daniel, Yian Smith, Yin, A Daniel, James Mc-Farland, Zou, 10.1056/AIoa2400196NEJM AI. 812024</p>
<p>Automated scholarly paper review: Concepts, technologies, and challenges. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi, 10.1016/j.inffus.2023.101830Information Fusion. 982023. 101830</p>
<p>ReviewerGPT? an exploratory study on using large language models for paper reviewing. Ryan Liu, Nihar B Shah, arXiv:2306.006222023Preprint</p>
<p>AI is transforming peer review -and many scientists are worried. Nature, 639. Michèle B. Nuijten and Joshua R. Polanin. 2020. "statcheck": automatically detect statistical reporting inconsistencies to increase reproducibility of metaanalyses. Miryam Naddaf, 10.1002/jrsm.1408Research Synthesis Methods. 5112025</p>
<p>WithdrarXiv: A large-scale dataset for retraction study. Delip Rao, Jonathan Young, Thomas Dietterich, Chris Callison-Burch, arXiv:2412.037752024Preprint</p>
<p>Assessing citation integrity in biomedical publications: corpus annotation and NLP models. Janina Maria, Shufan Sarol, Shruthan Ming, Jodi Radhakrishna, Halil Schneider, Kilicoglu, 10.1093/bioinformatics/btae420Bioinformatics. 7402024</p>
<p>Automatically evaluating the paper reviewing capability of large language models. Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim, arXiv:2502.170862025Preprint</p>
<p>When AI co-scientists fail: SPOT-a benchmark for automated verification of scientific research. Guijin Son, Jiwoo Hong, Honglu Fan, Heejeong Nam, Hyunwoo Ko, Seungwon Lim, Jinyeop Song, Jinha Choi, Gonçalo Paulo, Youngjae Yu, Stella Biderman, arXiv:2505.118552025Preprint</p>
<p>MAMORX: Multi-agent multi-modal scientific review generation with external knowledge. Pawin Taechoyotin, Guanchao Wang, Tong Zeng, Bradley Sides, Daniel Acuna, Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges. Vancouver, Canada2024</p>
<p>Peer review as a multi-turn and long-context dialogue with role-based interactions. Cheng Tan, Dongxin Lyu, Siyuan Li, Zhangyang Gao, Jingxuan Wei, Siqi Ma, Zicheng Liu, Stan Z Li, arXiv:2406.056882024Preprint</p>
<p>Dov Te'eni, and Iddo Drori. 2024. AI-driven review systems: Evaluating LLMs in scalable and bias-aware academic reviews. Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, Madeleine Udell, arXiv:2408.10365Preprint</p>
<p>Automated peer reviewing in paper SEA: Standardization, evaluation, and analysis. Jianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong, Long Zeng, Ren-Jing Cui, Chengcheng Han, Qiushi Sun, Zhiyong Wu, Yunshi Lan, Xiang Li, 10.18653/v1/2024.findings-emnlp.595Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Detecting reference errors in scientific literature with large language models. M Tianmai, Neil F Zhang, Abernethy, arXiv:2411.061012024Preprint</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025Preprint</p>
<p>Large language models for automated scholarly paper review: A survey. Zhenzhen Zhuang, Jiandong Chen, Hongfeng Xu, Yuwen Jiang, Jialiang Lin, 10.1016/j.inffus.2025.103332Information Fusion. 1242025. 103332</p>            </div>
        </div>

    </div>
</body>
</html>
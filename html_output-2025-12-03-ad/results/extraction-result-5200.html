<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5200 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5200</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5200</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-271050100</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.01495v2.pdf" target="_blank">Re-ReST: Reflection-Reinforced Self-Training for Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a reflector to refine low-quality generated samples during self-training. The reflector takes the agent’s output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6% on HotpotQA and 28.4% on AlfWorld, and Re-ReST further boosting performance by 2.0% and 14.1%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5200.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5200.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Re-ReST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection-Reinforced Self-Training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-training framework for language agents that uses a separately trained reflection model (reflector) plus environmental feedback to refine low-quality model-generated trajectories, enriching pseudo-labeled training data and improving downstream agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13B / Llama-3-8B / Llama-2-7B / CodeLlama (varies by task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLMs used as the agent and as the reflector; sizes reported include Llama-2-13B, Llama-3-8B, Llama-2-7B; CodeLlama used for some code experiments. Models are fine-tuned with LoRA in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Re-ReST (reflector-assisted generate-then-reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-model pipeline: agent M generates multiple trajectories; environment E evaluates each sample and provides feedback (binary success and/or error info); a separate reflector R (an LLM) consumes the task, the agent's prior generation, and E(x,ŷ) to generate a corrected trajectory. High-scoring original and corrected trajectories are added to self-training data. Reflector is trained on pairs of incorrect→correct samples (plus zero-shot generated data). Reflection is used during training with oracle/environmental feedback; during inference the reflector is not required, but a test-time variant uses reflector outputs combined via self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (HotpotQA, ALFWorld, MBPP code gen, GQA visual programming, text-to-image)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA (HotpotQA), long-horizon text-based sequential decision-making (ALFWorld), programming/code generation (MBPP), visual programming (GQA), and text-to-image generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>HotpotQA: Re-ReST further improves self-training by +2.0% EM (paper reports Re-ReST boost of 2.0%); ALFWorld: Re-ReST yields +14.1% absolute success-rate uplift over self-training; MBPP (code generation): Re-ReST sample accuracy reported as 77.3% (table entries), improving over self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>HotpotQA: self-training improved base EM from 20.0% to 27.6% (Llama-2) and from 30.0% to 34.4% (Llama-3); ALFWorld: self-training raised success from 8.9% to 37.3%; MBPP: self-training raised sample accuracy from 48.6% to 54.5% (zero-shot baseline 48.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative gains across tasks: self-training alone produced substantial gains (e.g., +7.6% EM on HotpotQA for Llama-2, +28.4% absolute on ALFWorld). Adding reflector-generated data (Re-ReST) produced additional gains (reported +2.0% on HotpotQA, +14.1% on ALFWorld) and larger increases in usable pseudo-labeled training instances; MBPP shows large improvement in pass/sample-accuracy when reflector-generated corrected programs are included. Ablations show (1) reflector finds correct solutions more efficiently than simply increasing agent sampling, and (2) training the reflector improves correction ability relative to using a pretrained LLM zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires high-quality ground-truth/environmental feedback during training (oracle/unit-tests/execution results); study limited to a single reflection iteration per sample for efficiency; test-time reflection with actual oracle feedback is often unavailable—paper proposes but only approximates it via self-consistency; risk of amplifying model biases through self-training; effectiveness depends on reflector training (zero-shot reflection helps but trained reflector performs better).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5200.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5200.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Training (agent-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Self-Training without Reflector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard self-training where the agent samples multiple outputs, scores them with a selection function, and fine-tunes on high-scoring self-generated trajectories; used as the baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13B / Llama-3-8B / Llama-2-7B / CodeLlama (varies by task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same open-source LLMs as used for Re-ReST but without the reflector; models sample k outputs per input (k typically = 3 in main experiments) and high-scoring samples are used for finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>generate-only self-training</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each unlabeled instance, sample k generations from the agent, evaluate with environment/metric, retain generations that exceed a threshold to form the pseudo-labeled training set, and then finetune the agent on those samples.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA, ALFWorld, MBPP, GQA, text-to-image (same set as Re-ReST experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-hop QA, sequential decision-making, code generation, visual programming, and image generation tasks used to evaluate Re-ReST.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>HotpotQA: Base→self-training (Llama-2) 20.0% → 27.6% EM (+7.6%); Llama-3 30.0% → 34.4% EM. ALFWorld: base 8.9% → self-training 37.3% ( +28.4% ). MBPP: zero-shot 48.6% → self-training 54.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper reports consistent gains from agent-only self-training across tasks (numerical improvements above), establishing self-training as a strong baseline and showing that self-generated trajectories can bootstrap agent performance without external stronger-model distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sampling more model outputs increases solved instances but remains less effective than reflector-corrected data; simply increasing k (number of samples) plateaus and cannot match Re-ReST results because sample quality—not just quantity—matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5200.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5200.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-time reflection + self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inference-time reflection combined with self-consistency decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to apply the trained reflector at inference without oracle feedback by running reflection on sampled agent outputs and aggregating answers via self-consistency voting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-based agent and trained reflector (same families as above)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent model generates multiple samples; trained reflector produces corrections for each sampled output at inference (but without ground-truth feedback); final answer selected by majority vote over combined set (agent outputs + reflected outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>inference-time reflection with self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample multiple agent outputs (e.g., 3), apply reflector to each output (without oracle feedback), then aggregate agent and reflector outputs using self-consistency majority voting to select final answer; compared against standard self-consistency (e.g., 6 agent samples).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA (reported), general method applicable to other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering evaluated with exact-match metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>HotpotQA EM (example reported in paper Table 5): Base (no S.C.) 27.6% EM; Self-consistency (6 agent samples) 30.8% EM; Self-consistency (3 agent + 3 reflector samples) 32.0% EM; Oracle (3 agent + 3 reflectors with ground-truth feedback) 36.8% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Self-consistency baseline (6 agent samples): 30.8% EM; base agent without self-consistency: 27.6% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Using reflector outputs combined with self-consistency (3 agent + 3 reflector) outperforms plain self-consistency with 6 agent samples (32.0% vs 30.8% EM), showing that the reflector can improve inference-time performance even without oracle feedback when used within a self-consistency aggregation framework.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reflection at test time without oracle feedback may fail in general (citing Huang et al. 2024); paper mitigates this via self-consistency but still notes that access to high-quality feedback is often unavailable and fully reliable inference-time reflection is challenging. Reported improvement is modest relative to oracle-feedback case.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5200.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5200.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot reflector (pretrained LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot reflection using an off-the-shelf pretrained LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an unfinetuned pretrained LLM as a reflector to perform self-correction on agent outputs without specialized reflector training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained LLM (unspecified; paper tests zero-shot reflector behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generic pretrained LLM used as reflector without further fine-tuning for the reflection task; used for ablation to compare against a trained reflector.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>zero-shot reflection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Feed agent generation plus environment feedback to a pretrained LLM and ask it to correct the output without any additional finetuning on reflection-specific data.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (HotpotQA, ALFWorld, MBPP experiments used ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as main experiments to probe reflector behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper reports that zero-shot reflection (using an off-the-shelf LLM) can improve agent performance relative to no reflection, but explicitly finds that training the reflector further improves correction ability and yields better downstream performance (see Table 5 and ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero-shot reflectors are less effective than a reflector trained specifically on agent incorrect→correct pairs; prior work and the paper note that without high-quality feedback pre-trained LLMs struggle to reliably self-correct complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5200.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5200.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (Shinn et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that uses verbalized reflections as reinforcement signals to improve agent performance, primarily an inference-time agent reflection approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion (Shinn et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reflexion approach (applied to LLM agents in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent-centric verbal reinforcement learning method where the agent reflects on failures and uses verbal feedback to guide future trials; primarily operates at inference-time rather than as a training-time self-training augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative verbal reflection (Reflexion)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agent generates a trial, receives environment feedback, generates verbal reflection about failure modes, and uses those reflections to reboot and guide subsequent trials; iterative trial-and-error with verbalization.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prior works; cited as related (e.g., ALFWorld experiments in reflexion literature)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Text-based sequential decision-making and interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in related work as an inference-time reflection/self-correction technique; paper contrasts Re-ReST by noting Re-ReST uses reflection during training (with oracle feedback) and does not require the reflector at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes prior inference-time reflection methods (including Reflexion) often require access to high-quality environmental feedback at test time and can fail without it.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5200.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5200.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (Madaan et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior iterative refinement method where an LLM critiques and refines its own outputs via internal evaluation steps, used mainly at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine (Madaan et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Refine approach (applied to general LLMs in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Iterative pipeline where a single LLM repeatedly generates, critiques, and refines its output using its internal critique mechanism without external oracle feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-critique / iterative self-refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates an output, internally critiques or scores it, then re-generates a refined output guided by the critique; often repeated iteratively at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General reasoning and generation tasks (cited as related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various generative and reasoning benchmarks explored in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Listed as related work; paper emphasizes that Re-ReST differs by using reflection to augment training data with oracle feedback and by training a separate reflector.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper cites Huang et al. (2024) and others to caution that self-refinement without high-quality external feedback can degrade performance for complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5200.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5200.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Huang et al. 2024 (self-correction failure)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models cannot self-correct reasoning yet (Huang et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior finding cited by the paper that LLMs struggle to self-correct complex reasoning tasks reliably without high-quality external feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models cannot self-correct reasoning yet</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prior analysis across LLMs (citation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Empirical study arguing that LLMs require high-quality external ground-truth feedback to perform reliable self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>analysis of self-correction limitations</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Demonstrates that LLMs fail to self-correct multi-step reasoning reliably in the absence of high-quality environmental feedback; motivates Re-ReST's use of oracle feedback during training.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Analysis across reasoning tasks (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Studies complex reasoning and self-correction capability of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Not an improvement method; cited as evidence that external feedback is essential for effective reflection/self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Used by paper to justify requiring environmental feedback during training; highlights failure modes of inference-time reflection without oracle feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Self-Refining Large Language Models (example works on reflection) <em>(Rating: 1)</em></li>
                <li>Reinforced Self-Training (ReST) for language modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5200",
    "paper_id": "paper-271050100",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Re-ReST",
            "name_full": "Reflection-Reinforced Self-Training",
            "brief_description": "A self-training framework for language agents that uses a separately trained reflection model (reflector) plus environmental feedback to refine low-quality model-generated trajectories, enriching pseudo-labeled training data and improving downstream agent performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13B / Llama-3-8B / Llama-2-7B / CodeLlama (varies by task)",
            "model_description": "Open-source LLMs used as the agent and as the reflector; sizes reported include Llama-2-13B, Llama-3-8B, Llama-2-7B; CodeLlama used for some code experiments. Models are fine-tuned with LoRA in experiments.",
            "reflection_method_name": "Re-ReST (reflector-assisted generate-then-reflect)",
            "reflection_method_description": "Two-model pipeline: agent M generates multiple trajectories; environment E evaluates each sample and provides feedback (binary success and/or error info); a separate reflector R (an LLM) consumes the task, the agent's prior generation, and E(x,ŷ) to generate a corrected trajectory. High-scoring original and corrected trajectories are added to self-training data. Reflector is trained on pairs of incorrect→correct samples (plus zero-shot generated data). Reflection is used during training with oracle/environmental feedback; during inference the reflector is not required, but a test-time variant uses reflector outputs combined via self-consistency.",
            "num_iterations": 1,
            "task_name": "Multiple (HotpotQA, ALFWorld, MBPP code gen, GQA visual programming, text-to-image)",
            "task_description": "Multi-hop QA (HotpotQA), long-horizon text-based sequential decision-making (ALFWorld), programming/code generation (MBPP), visual programming (GQA), and text-to-image generation.",
            "performance_with_reflection": "HotpotQA: Re-ReST further improves self-training by +2.0% EM (paper reports Re-ReST boost of 2.0%); ALFWorld: Re-ReST yields +14.1% absolute success-rate uplift over self-training; MBPP (code generation): Re-ReST sample accuracy reported as 77.3% (table entries), improving over self-training.",
            "performance_without_reflection": "HotpotQA: self-training improved base EM from 20.0% to 27.6% (Llama-2) and from 30.0% to 34.4% (Llama-3); ALFWorld: self-training raised success from 8.9% to 37.3%; MBPP: self-training raised sample accuracy from 48.6% to 54.5% (zero-shot baseline 48.6%).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative gains across tasks: self-training alone produced substantial gains (e.g., +7.6% EM on HotpotQA for Llama-2, +28.4% absolute on ALFWorld). Adding reflector-generated data (Re-ReST) produced additional gains (reported +2.0% on HotpotQA, +14.1% on ALFWorld) and larger increases in usable pseudo-labeled training instances; MBPP shows large improvement in pass/sample-accuracy when reflector-generated corrected programs are included. Ablations show (1) reflector finds correct solutions more efficiently than simply increasing agent sampling, and (2) training the reflector improves correction ability relative to using a pretrained LLM zero-shot.",
            "limitations_or_failure_cases": "Requires high-quality ground-truth/environmental feedback during training (oracle/unit-tests/execution results); study limited to a single reflection iteration per sample for efficiency; test-time reflection with actual oracle feedback is often unavailable—paper proposes but only approximates it via self-consistency; risk of amplifying model biases through self-training; effectiveness depends on reflector training (zero-shot reflection helps but trained reflector performs better).",
            "uuid": "e5200.0",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Training (agent-only)",
            "name_full": "Agent Self-Training without Reflector",
            "brief_description": "Standard self-training where the agent samples multiple outputs, scores them with a selection function, and fine-tunes on high-scoring self-generated trajectories; used as the baseline in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13B / Llama-3-8B / Llama-2-7B / CodeLlama (varies by task)",
            "model_description": "Same open-source LLMs as used for Re-ReST but without the reflector; models sample k outputs per input (k typically = 3 in main experiments) and high-scoring samples are used for finetuning.",
            "reflection_method_name": "generate-only self-training",
            "reflection_method_description": "For each unlabeled instance, sample k generations from the agent, evaluate with environment/metric, retain generations that exceed a threshold to form the pseudo-labeled training set, and then finetune the agent on those samples.",
            "num_iterations": null,
            "task_name": "HotpotQA, ALFWorld, MBPP, GQA, text-to-image (same set as Re-ReST experiments)",
            "task_description": "Same multi-hop QA, sequential decision-making, code generation, visual programming, and image generation tasks used to evaluate Re-ReST.",
            "performance_with_reflection": "",
            "performance_without_reflection": "HotpotQA: Base→self-training (Llama-2) 20.0% → 27.6% EM (+7.6%); Llama-3 30.0% → 34.4% EM. ALFWorld: base 8.9% → self-training 37.3% ( +28.4% ). MBPP: zero-shot 48.6% → self-training 54.5%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Paper reports consistent gains from agent-only self-training across tasks (numerical improvements above), establishing self-training as a strong baseline and showing that self-generated trajectories can bootstrap agent performance without external stronger-model distillation.",
            "limitations_or_failure_cases": "Sampling more model outputs increases solved instances but remains less effective than reflector-corrected data; simply increasing k (number of samples) plateaus and cannot match Re-ReST results because sample quality—not just quantity—matters.",
            "uuid": "e5200.1",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Test-time reflection + self-consistency",
            "name_full": "Inference-time reflection combined with self-consistency decoding",
            "brief_description": "A method to apply the trained reflector at inference without oracle feedback by running reflection on sampled agent outputs and aggregating answers via self-consistency voting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-based agent and trained reflector (same families as above)",
            "model_description": "Agent model generates multiple samples; trained reflector produces corrections for each sampled output at inference (but without ground-truth feedback); final answer selected by majority vote over combined set (agent outputs + reflected outputs).",
            "reflection_method_name": "inference-time reflection with self-consistency",
            "reflection_method_description": "Sample multiple agent outputs (e.g., 3), apply reflector to each output (without oracle feedback), then aggregate agent and reflector outputs using self-consistency majority voting to select final answer; compared against standard self-consistency (e.g., 6 agent samples).",
            "num_iterations": 1,
            "task_name": "HotpotQA (reported), general method applicable to other tasks",
            "task_description": "Multi-hop question answering evaluated with exact-match metric.",
            "performance_with_reflection": "HotpotQA EM (example reported in paper Table 5): Base (no S.C.) 27.6% EM; Self-consistency (6 agent samples) 30.8% EM; Self-consistency (3 agent + 3 reflector samples) 32.0% EM; Oracle (3 agent + 3 reflectors with ground-truth feedback) 36.8% EM.",
            "performance_without_reflection": "Self-consistency baseline (6 agent samples): 30.8% EM; base agent without self-consistency: 27.6% EM.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Using reflector outputs combined with self-consistency (3 agent + 3 reflector) outperforms plain self-consistency with 6 agent samples (32.0% vs 30.8% EM), showing that the reflector can improve inference-time performance even without oracle feedback when used within a self-consistency aggregation framework.",
            "limitations_or_failure_cases": "Reflection at test time without oracle feedback may fail in general (citing Huang et al. 2024); paper mitigates this via self-consistency but still notes that access to high-quality feedback is often unavailable and fully reliable inference-time reflection is challenging. Reported improvement is modest relative to oracle-feedback case.",
            "uuid": "e5200.2",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Zero-shot reflector (pretrained LLM)",
            "name_full": "Zero-shot reflection using an off-the-shelf pretrained LLM",
            "brief_description": "Using an unfinetuned pretrained LLM as a reflector to perform self-correction on agent outputs without specialized reflector training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pretrained LLM (unspecified; paper tests zero-shot reflector behavior)",
            "model_description": "A generic pretrained LLM used as reflector without further fine-tuning for the reflection task; used for ablation to compare against a trained reflector.",
            "reflection_method_name": "zero-shot reflection",
            "reflection_method_description": "Feed agent generation plus environment feedback to a pretrained LLM and ask it to correct the output without any additional finetuning on reflection-specific data.",
            "num_iterations": null,
            "task_name": "Multiple (HotpotQA, ALFWorld, MBPP experiments used ablations)",
            "task_description": "Same tasks as main experiments to probe reflector behavior.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Paper reports that zero-shot reflection (using an off-the-shelf LLM) can improve agent performance relative to no reflection, but explicitly finds that training the reflector further improves correction ability and yields better downstream performance (see Table 5 and ablations).",
            "limitations_or_failure_cases": "Zero-shot reflectors are less effective than a reflector trained specifically on agent incorrect→correct pairs; prior work and the paper note that without high-quality feedback pre-trained LLMs struggle to reliably self-correct complex reasoning.",
            "uuid": "e5200.3",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflexion (Shinn et al. 2023)",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "Prior work that uses verbalized reflections as reinforcement signals to improve agent performance, primarily an inference-time agent reflection approach.",
            "citation_title": "Reflexion (Shinn et al., 2023)",
            "mention_or_use": "mention",
            "model_name": "Reflexion approach (applied to LLM agents in original work)",
            "model_description": "Agent-centric verbal reinforcement learning method where the agent reflects on failures and uses verbal feedback to guide future trials; primarily operates at inference-time rather than as a training-time self-training augmentation.",
            "reflection_method_name": "iterative verbal reflection (Reflexion)",
            "reflection_method_description": "Agent generates a trial, receives environment feedback, generates verbal reflection about failure modes, and uses those reflections to reboot and guide subsequent trials; iterative trial-and-error with verbalization.",
            "num_iterations": null,
            "task_name": "Prior works; cited as related (e.g., ALFWorld experiments in reflexion literature)",
            "task_description": "Text-based sequential decision-making and interactive tasks.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned in related work as an inference-time reflection/self-correction technique; paper contrasts Re-ReST by noting Re-ReST uses reflection during training (with oracle feedback) and does not require the reflector at inference.",
            "limitations_or_failure_cases": "Paper notes prior inference-time reflection methods (including Reflexion) often require access to high-quality environmental feedback at test time and can fail without it.",
            "uuid": "e5200.4",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Refine (Madaan et al. 2023)",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "Prior iterative refinement method where an LLM critiques and refines its own outputs via internal evaluation steps, used mainly at inference time.",
            "citation_title": "Self-Refine (Madaan et al., 2023)",
            "mention_or_use": "mention",
            "model_name": "Self-Refine approach (applied to general LLMs in original work)",
            "model_description": "Iterative pipeline where a single LLM repeatedly generates, critiques, and refines its output using its internal critique mechanism without external oracle feedback.",
            "reflection_method_name": "self-critique / iterative self-refinement",
            "reflection_method_description": "Model generates an output, internally critiques or scores it, then re-generates a refined output guided by the critique; often repeated iteratively at inference.",
            "num_iterations": null,
            "task_name": "General reasoning and generation tasks (cited as related work)",
            "task_description": "Various generative and reasoning benchmarks explored in prior work.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Listed as related work; paper emphasizes that Re-ReST differs by using reflection to augment training data with oracle feedback and by training a separate reflector.",
            "limitations_or_failure_cases": "Paper cites Huang et al. (2024) and others to caution that self-refinement without high-quality external feedback can degrade performance for complex reasoning tasks.",
            "uuid": "e5200.5",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Huang et al. 2024 (self-correction failure)",
            "name_full": "Large language models cannot self-correct reasoning yet (Huang et al., 2024)",
            "brief_description": "Prior finding cited by the paper that LLMs struggle to self-correct complex reasoning tasks reliably without high-quality external feedback.",
            "citation_title": "Large language models cannot self-correct reasoning yet",
            "mention_or_use": "mention",
            "model_name": "Prior analysis across LLMs (citation)",
            "model_description": "Empirical study arguing that LLMs require high-quality external ground-truth feedback to perform reliable self-correction.",
            "reflection_method_name": "analysis of self-correction limitations",
            "reflection_method_description": "Demonstrates that LLMs fail to self-correct multi-step reasoning reliably in the absence of high-quality environmental feedback; motivates Re-ReST's use of oracle feedback during training.",
            "num_iterations": null,
            "task_name": "Analysis across reasoning tasks (cited)",
            "task_description": "Studies complex reasoning and self-correction capability of LLMs.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Not an improvement method; cited as evidence that external feedback is essential for effective reflection/self-correction.",
            "limitations_or_failure_cases": "Used by paper to justify requiring environmental feedback during training; highlights failure modes of inference-time reflection without oracle feedback.",
            "uuid": "e5200.6",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Self-Refining Large Language Models (example works on reflection)",
            "rating": 1,
            "sanitized_title": "selfrefining_large_language_models_example_works_on_reflection"
        },
        {
            "paper_title": "Reinforced Self-Training (ReST) for language modeling",
            "rating": 1,
            "sanitized_title": "reinforced_selftraining_rest_for_language_modeling"
        }
    ],
    "cost": 0.015112249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Re-ReST: Reflection-Reinforced Self-Training for Language Agents
7 Jul 2024</p>
<p>Zi-Yi Dou zdou@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Cheng-Fu Yang cfyang@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Xueqing Wu xueqing.wu@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Kai-Wei Chang kwchang@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Nanyun Peng 
University of California
Los Angeles</p>
<p>Re-ReST: Reflection-Reinforced Self-Training for Language Agents
7 Jul 20242E6AAF0D4101471B9C254A871B0504AEarXiv:2406.01495v2[cs.CL]
Finetuning language agents with reasoningaction trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical.In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations.Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks.To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a reflector to refine low-quality generated samples during self-training.The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples.This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples.We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-toimage generation.The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6% on HotpotQA and 28.4% on AlfWorld, and Re-ReST further boosting performance by 2.0% and 14.1%, respectively.Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training.Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work.Our code is released at https://github.com/PlusLabNLP/Re-ReST.</p>
<p>Introduction</p>
<p>Large language models (LLMs) (Kenton and Toutanova, 2019;Touvron et al., 2023;Achiam (Chen et al., 2023;Yin et al., 2024) distill knowledge from stronger models (e.g., GPT-4) to weaker ones (e.g., Llama-2).In contrast, we adopt self-training and improve it with reflection to improve agents more autonomously, which reduces reliance on external propriety models and maintains a fully open-source framework.et al., 2023) have demonstrated potential in interacting with external environments and addressing practical interactive tasks, resulting in a new class -language agents (Nakano et al., 2021;Yao et al., 2022).Finetuning LLMs for agentic tasks has proven effective, yet existing works rely on data generated by stronger models (e.g., GPT-4) (Chen et al., 2023;Yin et al., 2024), which are not always available (e.g., to improve the strongest model).</p>
<p>Among the potential techniques to improve agents (Ouyang et al., 2022;Wang et al., 2023b;Li et al., 2024;Chen et al., 2024), self-training holds promise for enhancing agent performance for challenging agentic tasks.The self-training process typically involves refining the model by generating samples, assessing their quality through rewards, and updating the model by training on high-quality samples.Compared with existing agent training methods (Chen et al., 2023;Yin et al., 2024), selftraining can autonomously improve agents and reduce the discrepancy between the agent's training data and its original predictions.Additionally, as in Figure 1, self-training can potentially allow for the development of performant agents within a fully open-source framework, without relying on closedsource, proprietary models.Given these benefits, we propose to investigate the use of self-training in language agents in this paper.</p>
<p>Figure 2: An overview of our Re-ReST method.Our approach incorporates self-training in language agent tasks by sampling multiple outputs from an agent and using positive samples for training.To enhance the effectiveness of self-training in language agents, we introduce a reflector mechanism.If a sample is incorrect, the reflector adjusts the agent's output based on environmental feedback.The corrected sample is then incorporated into the training data, thereby improving the overall self-training process.</p>
<p>However, one significant challenge for applying self-training in language agent tasks lies in the acquisition of high-quality samples to achieve good performance.Specifically, self-training requires a substantial amount of high-quality samples, while relying solely on model-generated samples can be inefficient, particularly for language agent tasks that demand multi-step reasoning and long-horizon planning.As a result, it is challenging to obtain good samples solely through sampling.Moreover, the common practice of discarding low-quality samples neglects their potential for improvement and effective utilization, thus limiting the overall efficacy of self-training methods.</p>
<p>To address these issues, we propose Reflection-Reinforced Self-Training (Re-ReST), which enhances the self-training algorithm using a reflection model.Re-ReST incorporates a reflector during self-training, which improves sample quality by utilizing environmental feedback such as execution successes and unit test outcomes.Specifically, the reflector transforms lower-quality samples into higher-quality ones, leveraging the capability of LLMs to self-improve when provided with accurate ground-truth feedback (Huang et al., 2024).Consequently, it enriches the training dataset, enabling more effective bootstrapping.After training, only the agent model is used for inference, ensuring no additional computational burden during testing.Unlike existing self-reflection methods (Madaan et al., 2023;Shinn et al., 2023;Pan et al., 2023), Re-ReST only requires access to feedback during training, not during inference, making our setting more realistic and practical.</p>
<p>We conduct extensive experiments with opensource LLMs across a wide range of tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation.Our results first demonstrate the potential of self-training in language agent tasks, showing improvements over few-shot baselines in long-horizon planning tasks, with gains of 7.6% on HotpotQA and 28.4% on AlfWorld.By incorporating Re-ReST, we further enhance performance significantly by 2.0% and 14.1% on HotpotQA and AlfWorld, respectively, achieving results better or comparable to models relying on commercial APIs.Ablation studies confirm the efficiency of the reflection model in generating high-quality self-training samples.Furthermore, we explore using our reflection model during inference with self-consistency decoding, which improves the model performance while alleviating the need for ground-truth feedback required by previous work (Huang et al., 2024).Additionally, we demonstrate the application of our method in preference optimization objectives.</p>
<p>2 Method: Re-ReST Self-Training.Formally, given a dataset U = {x i } N i=1 , self-training begins by using a base model M to generate a pseudo-label ŷi = M(x i ) for each instance x i ∈ U .Subsequently, a subset of {(x i , ŷi )} N i=1 is selected based on a scoring function, and M is finetuned on this selected subset.For language agents, we define the label y as a trajectory comprising interleaved thoughts and actions, as described in ReAct (Yao et al., 2022).We propose adopting the self-training paradigm by training language agents with their self-generated thought-action trajectories.</p>
<p>Overview of Re-ReST.Obtaining high-quality samples through self-sampling can be challenging, particularly for complex language agent tasks.To address this issue, we introduce Re-ReST, which aims to enhance the pseudo-label generation pro-cess in self-training for language agents.As illustrated in Figure 2, we propose improving lowquality samples using a reflection model with external feedback.We then enrich the self-training data by incorporating these corrected generations.This process generates high-quality samples efficiently by correcting low-quality ones with ground-truth feedback during training.</p>
<p>Components</p>
<p>Our method involves two models, including a language agent M that generates text and actions, and a reflection model R that improves a low-quality sample.The reflection model R has access to an external environment E that can provide external feedback to a generated sample (e.g.numerical scores and/or verbal error information).We illustrate each of these modules in the following part.</p>
<p>Language Agent.The language agent M is built upon a large language model (LLM) that is trained or prompted to generate thoughts and actions given a task.Formally, given an instance x i , the agent M generates its output ŷ ∼ M(y|x) containing its actions.The agent can first generate its reasoning traces before outputting its actions, which has been demonstrated to improve the model performance and interpretability (Yao et al., 2022).</p>
<p>Reflector.The reflection model R is also instantiated as an LLM, the goal of which is to improve the language agent's generations given external feedback.We assume that during training, an external environment E can evaluate a generated sample and provide feedback E(x, ŷ) to the agent.The feedback can be a binary success status and/or error information.For example, in code generation tasks, the environment can execute the model-generated code on unit tests, providing information on whether the code has syntax errors and whether it can pass the unit tests.</p>
<p>Having access to such an environment is important in our setting, as it has been shown that an LLM cannot perform self-correction without highquality external feedback (Huang et al., 2024).The reflection model generates a corrected sample ỹ ∼ R(y|x, ŷ, E(x, ŷ)) given the task information x, the agent generation ŷ, and the environmental feedback E(x, ŷ).It can optionally first state its reasoning process (e.g., which specific actions could be corrected) before generating the corrected answer.)The use of the reflection model can improve self-training by finding good solutions efficiently because of the additional information provided (i.e., the agent's previous trial and the environmental feedback.)We do not share the model parameters between the agent and reflector in this paper.</p>
<p>Data Generation</p>
<p>We then describe how we generate self-training data for the language agent M. The data generation process involves two steps, including the initial generation step with the language agent itself and the reflection step with the reflector, and we obtain the agent-generated dataset D M and reflectorgenerated dataset D R from the two steps.</p>
<p>Initial Generation.As in the standard setup, given an instance x, we sample k generations {ŷ j } k j=1 from the current language agent model ŷj ∼ M(y|x).Then, the environment E scores the generation and provides feedback E(x, ŷj )).If the score exceeds a threshold, we add the instance to (x, ŷj ) to the training data D M .In practice, we observe that setting k = 3 achieves a good balance between efficiency and effectiveness.</p>
<p>Reflection with Environmental Feedback.The initial generation step only relies on the agent model M itself to generate data.For a sampled generation ŷj , if the score does not pass the threshold, we will feed it to the reflection model for refinement.The reflector takes as inputs the task information x, the agent's prior generation ŷj , and the environmental feedback E(x, ŷj )), and then generates the corrected sample ỹj ∼ R(x, ŷj , E(x, ŷj )).The corrected sample ỹj will also be evaluated by the environment and we will add it to the reflectorgenerated training dataset D R if its score exceeds the threshold.While the reflection procedure can be iteratively applied multiple times as per Shinn et al. (2023), in this study, we limit this process to a single iteration for the sake of efficiency.This means that each generated sample ŷj is allowed a maximum of one refined counterpart ỹj .</p>
<p>Model Training and Inference</p>
<p>We first train the reflector R parameterized by θ R and then use the trained reflector to generate the reflection data D R .Afterward, we combine D R and the agent's self-generated data D M to train the agent model M parameterized by θ M .</p>
<p>Reflector Training.While base LLMs can perform self-reflection or self-correction without any finetuning given ground-truth feedback (Shinn et al., 2023), we propose to further improve its reflection ability with the self-generated data.First, from the initial generation step, we obtain multiple generations {y j } k j=1 from the agent model M. For each correct generation y w and incorrect generation y l with its environmental feedback E(x, ŷl ) in {y j } k j=1 , we will add the instance ⟨x, y l , E(x, ŷl ), y w ⟩ to the agent-generated dataset D R M for reflector training.In addition, the reflector generates its self-training dataset in a zero-shot manner D R R similar to the agent initial generation step.Combining the two generated datasets, we train the reflector on D R M ∪ D R R with the standard maximum log-likelihood objective first before generating the training data D R for the language agent:
LMLE(θR) = −E (x,y l ,y w )∼D R M ∪D R R log p θ R (y w |x, y l ).
(1)</p>
<p>Language Agent Training.After we have the base language agent to generate the self-training data D M and the improved reflector to generate the reflector-generated data D R , we train the language agent jointly on D M ∪ D R :
L M LE (θ M ) = −E (x,y)∼D M ∪D R log p θ M (y|x).
(2) Besides the maximum log-likelihood objective, because the reflection training and data generation process involves the use of preference pairs, it is natural to use preference optimization objectives such as DPO (Rafailov et al., 2023) for training, which we will discuss in the experiment section.</p>
<p>Inference.During inference, accessing highquality environmental feedback is often challenging, which can cause inference-time self-reflection algorithms to fail (Huang et al., 2024).Therefore, we only have the agent M directly output generations without the reflector during inference.This approach eliminates the need for feedback and avoids any additional computational overhead.A potential method to integrate the reflector into the inference process involves first training a scorer to evaluate the agent's output.If the score falls below a certain threshold, self-correction can then be performed, which we leave as a future direction.Additionally, we propose performing reflection regardless of environmental feedback and employing self-consistency to derive the final results from both the agent's outputs and the reflector's outputs, as shown in the experiment section.</p>
<p>Experiments</p>
<p>We experiment with multi-hop reasoning, sequential decision-making, code generation, visual question answering, and text-to-image generation.We present the experimental settings and results for each task.In all our experiments, we advocate for the use of open-source models and aim to avoid black-box, closed-source commercial models whenever possible.</p>
<p>Multi-Hop Reasoning</p>
<p>Dataset.We use the HotpotQA dataset (Yang et al., 2018), a well-established question-answering dataset featuring multi-hop reasoning and knowledge retrieval.It is constructed based on Wikipedia and an agent needs to retrieve and reason over multiple supporting documents to answer a question.We sample 5,000 training instances randomly for self-training and 500 instances from the development set for evaluation as in Chen et al. (2023).</p>
<p>Model Setup.We build both the agent model and the reflector upon the Llama-2-13B and Llama-3-8B models (Touvron et al., 2023).Note that different from previous work (Shinn et al., 2023;Chen et al., 2023;Yin et al., 2024), we do not employ a stronger language model such as GPT-3.5/4for data generation or self-reflection, ensuring that the models do not benefit from knowledge distillation.Following Shinn et al. (2023), we use the ReAct (Yao et al., 2022) method where at each step, the agent model first generates its thoughts and then performs an action.The action is chosen from (1) Search[entity], which searches the exact entity on Wikipedia, (2) Lookup[keyword], which localizes a keyword in the retrieved passages, and ( 3) Finish[answer], which returns the answer and finishes the task.We use a free Wikipedia API1 for passage retrieval and keyword lookup.</p>
<p>Training and Evaluation Setup.We use 2-shot prompting for few-shot agent and reflector data generation as in Shinn et al. (2023).For each training instance, the agent model samples 3 generations.The generation is evaluated with the exact match metric (i.e., if the generated answer is exactly the same as the ground-truth answer).The retrieval and evaluation results are given to the reflector as the environmental feedback for self-correction.We use Low-Rank Adaptation (LoRA) (Hu et al., 2022) for training the language models for efficiency.The agent and reflector models are trained for 3 epochs with a learning rate of 3e-4.</p>
<p>Main Results.We list the main results in Table 1.As shown in the table, self-training can significantly improve the model performance from an EM score of 20.0% to 27.6% for Llama-2 and from 30.0% to 34.4% for Llama-3.However, only 37.1% and 48.3% of the training instances are correctly solved by the agent model and are used for self-training respectively.By integrating our reflector model into the process, the agent can solve more training instances and thus have more data for training the agent model, increasing the EM scores significantly.In addition to our implemented models, following previous work (FireAct (Chen et al., 2023) and LUMOS (Yin et al., 2024)) that use GPT-3.5/4for data generation and model finetuning, we employ GPT-4 to generate 0.5k instances and first train the agents with the GPT-4 generated data before selftraining.Results demonstrate that 1) self-training is a stronger baseline than FireAct under a fair setting where the same QA tool is used; 2) we can achieve comparable or better performance of our model than these methods, even though both of them use strong knowledge retrieval models (i.e., SerpAPI2 for FireAct and GPT-4 for LUMOS), which are costly and non-scalable.By contrast, we use the free Wikipedia API.</p>
<p>Sequential Decision-Making</p>
<p>Dataset.We also assess the proposed approach on sequential decision-making using ALF-World (Shridhar et al., 2021).ALFWorld comprises a collection of text-based settings designed to test an agent's ability to complete multi-step tasks across diverse interactive environments.Following Yao et al. (2022); Shinn et al. (2023), we operate under the assumption that the agents are devoid of any access to successful trajectories, relying solely on a binary indicator of task success or failure.Our evaluation encompasses testing the agent across 134 previously unseen environments, spanning six diverse tasks.These tasks range from locating concealed items and transporting objects to interacting with objects using other items.</p>
<p>Model Setup.We build the agent and the reflector upon the Llama2-7b (Touvron et al., 2023).At each step, the agent can either contemplate its next move or generate admissible actions for execution as in Yao et al. (2022).Following the heuristics outlined by Shinn et al. (2023), we trigger the reflector model for self-reflection if the agent repeats an action with the same response over three cycles, or if it performs over 30 actions in an environment.</p>
<p>Training and Evaluation Setup.We use oneshot prompting instead of the two-shot prompting in Shinn et al. (2023) for the models so that we can better fit a trajectory into the context window of Llama-2.We train the agent and reflector models on the collected trajectories for 2 epochs with a learning rate of 2e-5 using LoRA.</p>
<p>Results.As shown in Table 2, it is evident that the base Llama model faces challenges in adapting to the experimental environment, but self-training can significantly improve the model performance.A significant point to highlight is that the model operates without access to complete trajectories during the experiment.Despite this limitation, it demonstrates a notable improvement in performance within unseen environments-increasing the success rate from 8.9% to 37.3% through the utilization of self-augmented trajectories.Furthermore, the implementation of the reflector contributes a 14.1% uplift in success rates, which affirms the efficacy of our proposed method.</p>
<p>Programming: Code Generation and</p>
<p>Visual Question Answering Dataset.For code generation, we experiment with the Python code writing task on MBPP (Austin et al., 2021) and visual programming on GQA (Hudson and Manning, 2019) For GQA, following Surís et al. (2023), we build the agent by providing a pre-defined set of visual APIs (e.g.object detection) and prompt the model to generate code using the APIs.</p>
<p>Training and Evaluation Setup.For MBPP, we use zero-shot and three-shot prompting for zeroshot agent and reflector data generation.For GQA, we follow the prompt in Surís et al. (2023) for the model for sample generation.For both datasets, the agent model samples 3 generations per training instance as before.We do not use the provided ground truths for MBPP training for consistency with the other experimental settings.The agent and reflector models are trained for 3 epochs with a learning rate of 3e-4 using LoRA.The BLIP-2 generated results are treated as the environmental feedback for the reflector.We do not use zero-shot reflection results to train the reflector because LLMs cannot perform this task without finetuning.The agent and reflector are trained for 2 epochs with a learning rate of 1e-5 using LoRA.(Rafailov et al., 2023), and integrating DPO into our method can generally improve the model performance.</p>
<p>Results. As in</p>
<p>Results. As shown in</p>
<p>Analysis</p>
<p>Re-ReST v.s.Self-Training with More Samples.</p>
<p>We investigate if we can simply sample more generations from the language agent for self-training and achieve comparable performance with our reflectoraugmented method.Specifically, we try to sample k generations for each instance, where k is set to 1, 2, 3, 4, 5, 6, and use the generated samples for self-training.As shown in Figure 3, if we keep sampling more generations from the language agent, the agent can indeed solve more instances and we can obtain an increasing amount of data for self-training.However, 1) the number of solved instances is still lower than the number of reflectorsolved instances, demonstrating that the reflector can find the correct solutions more efficiently than sampling; 2) the model performance is not always improved with more training data and it cannot outperform our method even when trained with more generated samples, indicating that the quality of the self-training data is also important and our reflector can generate training data effectively for the agent.</p>
<p>Effect of Training the Reflector.</p>
<p>As illustrated, we propose to first train the reflector before using it to generate the self-training data.In this part, we investigate if we can use the reflector to perform self-correction in a zero-shot manner and then train the language agent.As in Table 5, we find that while the reflector can perform self-correction without any finetuning and improve the performance of the language agent, further improvements can be made if we specifically train the model for selfcorrection, demonstrating the effectiveness of our proposed reflector training strategy.</p>
<p>Test-Time Reflection without Ground-Truth Feedback.Previously, our reflector functions only during training and is not used during inference because it is often impossible to obtain ground-truth feedback, which is required for reflection methods to work (Huang et al., 2024).In this section, we propose employing selfconsistency (Wang et al., 2023a) to enable test-time reflection and address this limitation.Selfconsistency is a decoding technique that combines multiple model predictions by sampling various reasoning paths and then selecting the most consistent answer through a majority vote.This approach allows us to apply the reflector during inference.Specifically, we sample multiple answers from our model and perform reflection on each output, regardless of correctness.We then aggregate all the answers using self-consistency.As in Table 6, integrating our reflector with self-consistency (3 agent samples and 3 reflection samples) achieves improvements over baseline (self-consistency with 6 model samples).This demonstrates the potential application of our method during inference, overcoming the current limitation of requiring groundtruth feedback for reflection methods.</p>
<p>Re-ReST with Direct Preference Optimization.</p>
<p>Our reflector turns incorrect samples into correct ones, naturally making negative-positive pairs suitable for preference optimization objectives such as DPO.In this part, we investigate the application of DPO in our method.As in Table 7, integrating DPO into our method can generally improve or achieve comparable performance with training models only with supervised training on positive samples, indicating our compatibility with DPO.</p>
<p>Conclusion</p>
<p>Our study studies the applications of self-training in language agents and improves it with Reflection-Reinforced Self-Training (Re-ReST), an approach that efficiently obtains high-quality samples for self-training with a reflector.Our experiments demonstrate that Re-ReST outperforms selftraining methods across various tasks, confirming the efficiency and effectiveness of incorporating a reflection mechanism.Within the proposed framework, in the future, we can improve the reflection mechanism and develop better training paradigms for the agent and reflector.</p>
<p>Limitations</p>
<p>Our approach is predicated on the availability of ground-truth feedback during the training process.While this assumption holds true for many language agent tasks, it presents challenges when applied to broader contexts.Specifically, acquiring accurate ground-truth feedback can be difficult in diverse, real-world scenarios.This limitation underscores a key aspect of our study: it is primarily concentrated on language agent tasks, thereby neglecting the potential applications and implications within the broader scope of general language modeling.This suggests the need for future research to explore and address the complexities of applying our methods to general language modeling tasks, where ground-truth feedback may not be as readily accessible or reliable.Another potential risk of the method is that through self-training, the biases encoded in LLMs can be amplified, and careful calibrations should be conducted before the deployment of our method.</p>
<p>A Related Work</p>
<p>In this section, we first overview the research progress in language agents, then briefly describe self-training and self-correction methods for improving language agents.We also summarize the major differences between our work and previous language agent methods in Table 8.</p>
<p>Language Agents.Language agents refer to language models that interact with the world in general.It has been demonstrated that LLMs can perform actions by generating specific commands (Nakano et al., 2021;Huang et al., 2022;Ahn et al., 2022) and calling external tool APIs (Lu et al., 2023;Schick et al., 2023;Gou et al., 2024).By integrating the model reasoning and acting abilities, ReAct (Yao et al., 2022) asks an LLM to first generate reasoning traces and then act accordingly, which is then improved by follow-up works through inference-time techniques such as reflection (Shinn et al., 2023) and planning (Yao et al., 2023;Yang et al., 2023).Recently, finetuning agents (Chen et al., 2023;Yin et al., 2024) have attracted attention from the research community.However, most of the existing works attempt to distill knowledge from a relatively strong LLM (e.g., GPT-4) to a weaker LLM (e.g., LLaMa-2).By contrast, our work bootstraps a language agent's performance by utilizing its own reflective ability without using external models.</p>
<p>Self-Training for Language Models.Various self-training algorithms have been proposed to improve language models (He et al., 2019;Huang et al., 2023;Dong et al., 2023;Gulcehre et al., 2023;Yuan et al., 2024), with the general idea being to improve models with self-generated samples in an unsupervised or semi-supervised manner.He et al. ( 2019) is one early work in applying self-training to generative language models and points out the importance of introducing noises during pseudo-label generation to increase the sample diversity.In the large language model era, Gulcehre et al. (2023) propose Reinforced Self-Training (ReST), where they use a scoring function to select self-generated samples and augment the training data.Similarly, Yuan et al. (2024) proposes self-rewarding that scores samples with the LLM itself and trains the model with direct preference optimization (DPO) (Rafailov et al., 2023) on the scored samples.Self-training has also been employed to improve the chain-of-thought reason-ing (Nye et al., 2022;Wei et al., 2022) ability of LLMs (Uesato et al., 2022).For example, Zelikman et al. (2022) propose to ask an LLM to generate rationales given questions and improve the LLM with its own generated reasoning.Re-ReST falls under the self-training paradigm, and different from previous work, our aim is to generate useful samples efficiently for self-training.</p>
<p>Self-Reflection/Self-Correction for Language Models.Several works have used LLMs to reflect on their generations with internal or external feedback and correct their errors (Welleck et al., 2023;Wang et al., 2023c;Shinn et al., 2023;Madaan et al., 2023;Kim et al., 2024;Ji et al., 2024).A majority of this line of research is focused on improving LLMs during inference.For example, Self-Refine (Madaan et al., 2023) proposes to have LLMs iteratively evaluate their generations, based on which they improve their generations.Similarly, Shinn et al. (2023) use LLM agents to reflect on its generations and their environment feedback, then guide the next generation with the generated verbal feedback.As pointed out by Huang et al. (2024), high-quality external feedback is essential for these self-correction models, without which existing techniques actually decrease model performance.However, such high-quality feedback is often unavailable during the test time, thus we propose to use Re-ReST only during training and perform corrections with oracle feedback from environments, ensuring its effectiveness in correcting the model generations.In addition, during the test time, the corrected generations are distilled into the language model, thus directly generating the answer without introducing overhead during inference.</p>
<p>B Prompts</p>
<p>Work</p>
<p>Agent Training</p>
<p>Agent Reflection Finetuning GPT-Free w/ G.T. Feedback w/o G.T. Feedback FireAct (Chen et al., 2023) ✓ ✗ ✗ ✗ LUMOS (Yin et al., 2024) ✓ ✗ ✗ ✗ Reflexion (Shinn et al., 2023) ✗ ✗ ✓ ✗ Self-Refine (Madaan et al., 2023)
✗ ✗ ✓ ✗ Re-ReST ✓ ✓ ✓ ✓
Table 8: Comparisons with previous language agent methods.We propose to finetune LLMs for language agent tasks with self-generated data, while previous work such as FireAct and LUMOS rely on stronger LLMs such as GPT-4 to perform knowledge distillation.In addition, we propose to use the agent's reflection ability to improve the self-training efficiency, where the reflection can function both with and without ground-truth feedback, addressing the limitation of previous agent reflection methods (Shinn et al., 2023;Madaan et al., 2023;Huang et al., 2024).</p>
<p>Prompt Solve a question answering task with interleaving Thought, Action, Observation steps.Thought can reason about the current situation, and Action can be three types:</p>
<p>(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists.If not, it will return some similar entities to search.</p>
<p>(2) Lookup[keyword], which returns the next sentence containing keyword in the last passage successfully found by Search.</p>
<p>(3) Finish [answer], which returns the answer and finishes the task.You may take as many steps as necessary.</p>
<p>{In-context examples} Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?Thought 1: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.The implementation failed the test cases where the input integers are 1 and 2. The issue arises because the code does not add the two integers together, but instead subtracts the second integer from the first.To fix this issue, we should change the operator from '-' to '+' in the return statement.This will ensure that the function returns the correct output for the given input.</p>
<p>[improved impl]: "'python def add(a: int, b: int): """ Given integers a and b, return the total value of a and b. """ return a + b "'"' ... {In-context examples} You are in the middle of a room.Looking quickly around you, you see a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 1, a garbagecan 1, a handtowelholder 2, a handtowelholder 1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1.Your task is to: put some spraybottle on toilet.&gt; think: To solve the task, I need to find and take a sparybottle, then put it on toilet.OK. &gt; think: First I need to find a spraybottle.A spraybottle is more likely to appear in cabinet (1-4), countertop (1), toilet (1), sinkbasin (1-2), garbagecan (1).I can check one by one, starting with cabinet 1. OK. &gt; go to cabinet 1 On the cabinet 1, you see a cloth 1, a soapbar 1, a soapbottle 1. &gt; go to cabinet 2 The cabinet 2 is closed.&gt; open cabinet 2 You open the cabinet 2. The cabinet 2 is open.In it, you see a candle 1, and a spraybottle 2. &gt; think: Now I find a spraybottle (2).Next, I need to take it.OK &gt; take spraybottle 2 from cabinet 2 You pick up the spraybottle 2 from the cabinet 2. &gt; think: Now I take a spraybottle (2).Next, I need to put it in/on toilet 1. OK. &gt; go to toilet 1 On the toilet 1, you see a soapbottle 2. &gt; put spraybottle 2 in/on toilet 1 You put the spraybottle 2 in/on the toilet 1.</p>
<p>User {Input task and previous trial}</p>
<p>User {Input question} Here is the task: You are in the middle of a room.Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.Your task is to: examine the bowl with the desklamp.{Reflection Results} think: I was stuck in a loop in which I continually picked up the alarmclock 1 instead of turning on the desklamp.</p>
<p>Figure 1 :
1
Figure 1: Previous agent training methods(Chen et al., 2023;Yin et al., 2024) distill knowledge from stronger models (e.g., GPT-4) to weaker ones (e.g., Llama-2).In contrast, we adopt self-training and improve it with reflection to improve agents more autonomously, which reduces reliance on external propriety models and maintains a fully open-source framework.</p>
<p>Table 15 :
15
Example Prompt Template on the ALFWorld dataset.A prompt includes (a) {In-context example} which is a complete trajectory from a successful trial.(b) {Input question} describes the initial environment and the instruction of the task, and (c) {Reflection Results} encapsulates the self-reflection results from the reflector model.</p>
<p>Table 1 :
1
On HotpotQA, our method enables a better usage of the training data compared with self-training and improves self-training for LLama-2/3-based agents.Also, adding only 0.5k GPT-generated data enables our agents with the free Wikipedia API to achieve comparable or better performance than methods with commercial APIs.
. The MBPP
Roziere et al. (2023)23), the agent model is given the unit test cases during code generation.Similarly, the reflection model is given the agent generation and its unit test results as the environmental feedback, and then generates a corrected version.</p>
<p>Table 2 :
2
Results on the ALFWorld dataset.Re-ReST substantially increases the sampling accuracy and outperforms self-training in terms of success rate even upon employing a reflector.</p>
<p>Table 4 :
4
Table 3, for MBPP, because CodeLlama is trained on a large amount of code Re-ReST can outperform self-training in text-to-image generation when applied to VPGen and evaluated with VPEval (Cho et al., 2023) on multiple dimensions.
ModelMBPP Sample Acc.P@1Sample Acc.Score GQAZero-Shot-48.6-40.9Self-Training66.954.544.741.9Re-ReST77.356.455.742.6Table 3: Re-ReST improves self-training on code gen-eration and visual programming tasks.generation corpus, the base CodeLlama model canachieve a decent performance without any fine-tuning. The high pass rate results in many of thetraining instances being used for self-training. Af-ter self-training on the MBPP training data, themodel performance can be improved from 48.6%to 54.5%. The reflector model can generate moreself-training data and the pass rate can be improvedwith the reflector-generated data. For GQA, simi-lar improvements can be seen, indicating that ourmethod is also applicable in visual programming.3.4 Text-to-Image Generation
Cho et al. (2023)onduct experiments in text-toimage generation.Specifically, we use the dataset constructed byCho et al. (2023).Their dataset evaluates the model's generated images in multiple dimensions and has training data for the spatial, Figure 3: In self-training, increasing the number of generations per instance initially improves model performance, but this effect plateaus.Additionally, both model performance and the number of solved training instances are lower than with Re-ReST, indicating our reflector can efficiently and effectively generate highquality self-training data.scale,andcount dimensions.For each dimension, the evaluation set consists of 1,000 instances.The training dataset consists of 36,920/18,200/1,560 instances for the spatial/scale/count dimensions.Training and Evaluation Setup.We use VP-Gen to perform inference on their training data, and evaluate the generations using VPEval(Cho</p>
<p>Table 5 :
5
While directly using a pretrained LLM as our reflector improves self-training, training the reflector specifically for self-correction further improves the agent performance.
ModelEMBase27.6S.C. (6 agents)30.8S.C. (3 agents + 3 reflectors)32.0Oracle (3 agents + 3 reflectors)36.8
(Li et al., 2023a)a)k relies on ground-truth feedback for test-time reflection (Oracle).In contrast, we propose to use self-consistency(Wang et al., 2023a)to enable our reflector to be applied during inference without ground-truth feedback and achieve improvements, demonstrating the potential of applying our method during the test time.etal., 2023).Specifically, during evaluation, a visual question answering model (BLIP-2(Li et al., 2023a)) is used to determine if the generated images correctly capture the input text information.</p>
<p>Table 4
4, our method con-
training is applied, further improved significantly with our Re-ReST method across all the dimensions.The results demonstrate promising applications of our model in the multimodal generation domain with a language agent as a backend.</p>
<p>Table 7 :
7
Our method is compatible with direct preference optimization (DPO)</p>
<p>Table 9 :
9
Prompt template for the HotpotQA agent.{In-context examples} {Input question} Prompt Solve a question answering task with a reflection on a previous trial and interleaving Thought, Action, Observation steps.Reflection can reason on a previous trial that failed and develop a new strategy to avoid repeating the same mistake.Thought can reason about the current situation, and Action can be three types: (1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists.If not, it will return some similar entities to search.The Deliberate Stranger is a book about American serial killer Ted Bundy written by Seattle Times reporter Richard W. Larsen that was published in 1980.The book spawned a television miniseries of the same title, starring Mark Harmon as Bundy, that aired on NBC on May 4, 1986.Thought 2: The subject of the book, Ted Bundy, was executed in 1989.Action 2: Finish[1989] Reflection: I should have searched for Ted Bundy first before I finished answering the question.Thought 1: I need to first search The Deliberate Stranger and find its subject, and then find out which year the subject was executed.Action 1: Search[The Deliberate Stranger] Observation 1: The Deliberate Stranger is a book about American serial killer Ted Bundy written by Seattle Times reporter Richard W. Larsen that was published in 1980.The book spawned a television miniseries of the same title, starring Mark Harmon as Bundy, that aired on NBC on May 4, 1986.Thought 2: The subject of The Deliberate Stranger is Ted Bundy.I need to search for Ted Bundy and find out which year he was executed.
(2) Lookup[keyword], which returns the next sentence containing keyword in the last passage successfullyfound by Search.(3) Finish[answer], which returns the answer and finishes the task.You need to reflect on the previous trial and then restart a new trial.{In-context examples}Question: When was the subject of the book The Deliberate Stranger executed?Previous Trial:Thought 1: I need to search The Deliberate Stranger and find when the subject was executed.Action 1: Search[The Deliberate Stranger]Observation 1: MarieBurr', 'Murder of Georgann Hawkins', 'Zac Efron']Thought 3: I need to change the keyword for search.Action 3: Search[Ted Bundy executed]Observation 3: Theodore Robert Bundy (Cowell; November 24, 1946 January 24, 1989) was an Americanserial killer who kidnapped, raped and murdered dozens of young women and girls during the 1970s andpossibly earlier. After more than a decade of denials, he confessed to 30 murders committed in seven statesbetween 1974 and 1978. His true victim total is unknown.Bundy often employed charm to disguise hismurderous intent when kidnapping victims, and extended this tactic vis-vis law enforcement, the media andthe criminal justice system to maintain his claims of innocence...Thought 4: Ted Bundy was executed on January 24, 1989.Action 4: Finish[January 24, 1989]...UserQuestion: {Input question and previous trial}
Action 2: Search[Ted Bundy] Observation 2: Could not find [Ted Bundy].Similar: ['Ted Bundy', 'Ted Bundy (film)', 'Ted Bundy: American Boogeyman', 'Conversations with a Killer: The Ted Bundy Tapes', 'Murder of Rita Curran', 'Ted Bundy: Falling for a Killer', 'Extremely Wicked, Shockingly Evil and Vile', 'Disappearance of Ann</p>
<p>Table 10 :
10
Prompt template for the HotpotQA reflector.{In-context examples} {Input question and previous trial}
PromptYou are an expert Python programmer, and here is your task: {input task}Your code should pass these tests:{unit tests}
Your code should start with a [PYTHON] tag and end with a [/PYTHON] tag.</p>
<p>Table 11 :
11
Prompt template for the MBPP agent.{unit tests} {input task} Prompt You are an AI Python assistant.You will be given the user input, your past incorrect function implementation, and a series of unit tests.Write your reflection on the function implementation and correct your implementation (copy the function signature and its docstring).
{In-context examples}[previous impl]:"'pythondef add(a: int, b: int):"""Given integers a and b, return the total value of a and b."""return a -b"'[unit test results from previous impl]:Tested passed:Tests failed:assert add(1, 2) == 3 # output: -1assert add(1, 2) == 4 # output: -1[reflection on previous impl]:</p>
<p>Table 12 :
12
Prompt template for the MBPP reflector.{In-context examples} {Input task and previous trial} Prompt class ImagePatch: """A Python class containing a crop of an image centered around a particular object, as well as relevant information.Methods --find(object_name: str)-&gt;List[ImagePatch] Returns a list of new ImagePatch objects containing crops of the image centered around any objects found in the image matching the object_name.simple_query(question: str=None)-&gt;str Returns the answer to a basic question asked about the image.If no question is provided, returns the answer to "What is this?".exists(object_name: str)-&gt;bool Returns True if the object specified by object_name is found in the image, and False otherwise.verify_property(property: str)-&gt;bool Returns True if the property is met, and False otherwise.best_text_match(string1: str, string2: str)-&gt;str Returns the string that best matches the image.crop(left: int, lower: int, right: int, upper: int)-&gt;ImagePatch Returns a new ImagePatch object containing a crop of the image at the given coordinates.
"""{Detailed API definition}{In-context examples}{Input question}</p>
<p>Table 13 :
13
Prompt template for the GQA agent.Full prompt is released in https://github.com/cvlab-columbia/viper/blob/main/prompts/benchmarks/gqa.prompt.{Detailed API definition} {Incontext examples} {Input question} Prompt Interact with a household to solve a task.Here is an example.</p>
<p>https://python.langchain.com/docs/integrations/tools/wikipedia
https://serpapi.com/
Prompt I am writing code to handle visual question answering tasks by calling computer vision APIs.My code is wrong, and I hope you can help correct it.{Input question and previous trial}Your response should start with your reasoning and analysis.Then, you should write the correct code wrapped in <code>python and</code>.The correct code should be a function with signature `def exe-cute_command(image) -&gt; str:-Below are the available APIs and some example usages: ```python class ImagePatch:"""A Python class containing a crop of an image centered around a particular object, as well as relevant information.Methods
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Program synthesis with large language models. 2021arXiv preprint</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, FireAct: Toward language agent fine-tuning. 2023arXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, 2024arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Visual programming for text-to-image generation and evaluation. Jaemin Cho, Abhay Zala, Mohit Bansal, 2023NeurIPS</p>
<p>RAFT: Reward ranked finetuning for generative foundation model alignment. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Tong Shum Kashun, Zhang, 2023TMLR</p>
<p>ToRA: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, 2024ICLR</p>
<p>Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Reinforced Self-Training (ReST) for language modeling. 2023arXiv preprint</p>
<p>Revisiting self-training for neural sequence generation. Junxian He, Jiatao Gu, Jiajun Shen, Marc'aurelio Ranzato, ICLR. 2019</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR. 2022</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, EMNLP. 2023</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 2024ICLR</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, ICML. 2022</p>
<p>GQA: A new dataset for real-world visual reasoning and compositional question answering. Drew A Hudson, Christopher D Manning, CVPR. 2019</p>
<p>Aligner: Achieving efficient alignment through weak-to-strong correction. Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Yaodong Yang, 2024arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , 2019</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, 2024NeurIPS</p>
<p>BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML. 2023a</p>
<p>Self-alignment with instruction backtranslation. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason E Weston, Mike Lewis, ICLR. 2024</p>
<p>GLIGEN: Open-set grounded text-to-image generation. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, Yong Jae Lee, CVPR. 2023b</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, 2023NeurIPS</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, 2023NeurIPS</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, WebGPT: Browser-assisted questionanswering with human feedback. 2021arXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Deep Learning for Code Workshop. 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, 2022NeurIPS</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , 2023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, 2023NeurIPS</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, Code Llama: Open foundation models for code. 2023arXiv preprint</p>
<p>ToolFormer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023NeurIPS</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023NeurIPS</p>
<p>AlfWorld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, 2021ICLR</p>
<p>ViperGPT: Visual inference via python execution for reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, ICCV. 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv preprintet al. 2023.</p>
<p>Solving math word problems with process-and outcomebased feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, 2022arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, ICLR. 2023a</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, ACL. 2023b</p>
<p>Enable language models to implicitly learn self-improvement from data. Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji, 2023carXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, 2022NeurIPS</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, ICLR. 2023</p>
<p>Lacma: Language-aligning contrastive learning with meta-actions for embodied instruction following. Cheng-Fu Yang, Yen-Chun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan, Yu-Chiang Frank, Wang , Kai-Wei Chang, arXiv:2310.123442023arXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, EMNLP. 2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, 2023NeurIPS</p>
<p>ReAct: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, ICLR. 2022</p>
<p>LUMOS: Learning agents with unified data, modular design, and open-source llms. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Choi, Lin Yuchen, 2024ACL</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, Self-rewarding language models. 2024arXiv preprint</p>
<p>STaR: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, 2022NeurIPS</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3525 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3525</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3525</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-b4013141cceb937c46ea5f84f8c06f6bf1215106</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b4013141cceb937c46ea5f84f8c06f6bf1215106" target="_blank">PRover: Proof Generation for Interpretable Reasoning over Rules</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs, and learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm.</p>
                <p><strong>Paper Abstract:</strong> Recent work by Clark et al. (2020) shows that transformers can act as 'soft theorem provers' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PROVER generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PROVER obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for 'depth 5', indicating significant scope for future work. Our code and models are publicly available at this https URL</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3525.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3525.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRoVeR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRoVeR: Proof Generation for Interpretable Reasoning over Rules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interpretable transformer-based system that jointly answers binary questions over natural-language rule-bases and generates corresponding directed-graph proofs (nodes = facts/rules/NAF, edges = rule applications) using a RoBERTa encoder plus QA, node and edge modules, constrained training, and ILP-based inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (fine-tuned with PRoVeR architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a pretrained RoBERTa-large transformer as a shared encoder; adds three heads: QA (binary classification on [CLS]), Node classifier (per-fact/rule/NAF presence), and Edge classifier (pairwise edge presence/direction using concatenation + difference of node embeddings). Trained with joint binary cross-entropy losses and constrained label masking; inference enforces global constraints via an ILP (max-flow connectivity + structural constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Rule-based natural-language theorem proving (RuleTakers-style: DU0-DU5, Birds-Electricity, ParaRules)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary question-answering and proof-generation over contexts consisting of natural-language facts and rules (including negation-as-failure). Task requires applying rules to derive or fail-to-derive statements; proofs are represented as directed graphs up to depth 5.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Joint fine-tuning of RoBERTa-large with three supervised heads (QA, Node, Edge); semantic masking of impossible edge labels during training (reduces label space); a learned NAF node computed from [CLS]; ILP inference to maximize edge posterior subject to structural constraints (no fact->fact, no rule->fact, connectivity via max-flow), ensuring globally valid proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On DU5 test: QA overall 99.3% (per-depth QA in Table 1: D0 100, D1 99.0, D2 98.8, D3 99.1, D4 98.8, D5 99.3). Proof metrics on DU5 (PRoVeR): Node Accuracy 89.2%, Edge Accuracy 87.5%, Proof Accuracy 87.1%, Full Accuracy 87.1%. Proof accuracy drops with depth (depth 5 PA = 65.1%). Zero-shot/out-of-distribution and paraphrase (ParaRules) results: PRoVeR attains high proof accuracy on ParaRules (≈95% in combined DU3+ParaRules setting) and shows strong zero-shot QA (the paper reports up to +6% QA improvement on some zero-shot evaluations). Data-efficiency: with 40% of DU5 training data PRoVeR achieves QA ≈97.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared against RuleTakers (fine-tuned RoBERTa baseline): DU5 overall QA reported 99.2% for RuleTakers (Table 1). RuleTakers does not generate explicit proofs (it used a post-hoc leave-one-out critical-sentence procedure for explanations). On some zero-shot splits RuleTakers QA reported ~98.8% (varies by split in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>PRoVeR matches or slightly improves QA vs RuleTakers (overall +0.1% on DU5) while additionally producing interpretable proofs (PA 87.1%). PRoVeR shows larger gains in generalization: when trained on lower-depth questions PRoVeR achieves up to +15% absolute improvement in QA on higher-depth test questions compared to the baseline in some settings; up to +6% QA improvement in some zero-shot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Proof generation is substantially harder than QA: edge prediction is the bottleneck and performance degrades with proof depth (PA falls to 65% at depth 5). The model tends to underestimate necessary nodes (42% of node-set errors are subsets of gold; 25% are supersets). Some failure modes are due to missing/incorrect edges rather than answer prediction (answers are almost always correct when the proof is correct). The model collapses multiple NAF nodes into a single learned NAF representation (limitation tied to dataset labeling). Producing exact proofs for deeper chains remains challenging and scope for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations: 'No NAF' (random NAF embeddings) causes a 5–6% drop in node and edge accuracy; removing constraints during training and inference ('Unconstrained Train + No ILP') causes ~5–6% drop in edge/proof accuracy; applying ILP at inference only ('UT + ILP') slightly recovers proof accuracy; removing connectivity constraint has marginal effect (model already predicts connected graphs in most cases; 57 examples disconnected without it). NAF prediction itself is highly accurate (≈95%). Edge directionality is learned nearly perfectly (only 1 example where direction alone caused proof failure in the analyzed depth-5 subset). Error breakdown for depth-5: on average PRoVeR predicts 6 of 7 edges correctly. Simpler lexical baselines (e.g., Random Forest on overlap features) give much lower edge accuracy (~47%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3525.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3525.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RuleTakers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RuleTakers (Transformers as soft reasoners over language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior benchmark and baseline showing that pretrained transformers (RoBERTa) can be fine-tuned to answer binary questions over natural-language rule-bases, characterized as 'soft theorem provers'; used in this paper as the primary baseline for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (fine-tuned baseline used in RuleTakers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned RoBERTa model used to classify the truth of statements given concatenated context+question input; produces high QA accuracy but does not natively produce structured proof graphs (used leave-one-out critical-sentence analysis for post-hoc explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Rule-based natural-language theorem proving (RuleTakers datasets / DU0-DU5, Birds-Electricity, ParaRules)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary QA over sets of facts and rules expressed in natural language; measures ability to derive statements through multi-step rule application up to certain depths; intended to probe compositional reasoning of transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning a pretrained transformer (RoBERTa) on the binary QA task; explanation via post-hoc leave-one-out 'critical sentence' identification (remove each sentence and observe answer flip).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported QA on DU5 test (Table 1) overall ≈99.2% (per-depth examples: D0 100, D1 98.4, D2 98.4, D3 98.8, D4 99.2, D5 99.8 in Table 1 rows shown). On zero-shot/paraphrase splits RuleTakers also attains high QA but PRoVeR claims improvements on some zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>N/A (this is the baseline used by PRoVeR). PRoVeR matches or slightly exceeds RuleTakers QA and additionally provides explicit proof graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not generate explicit structured proofs; its post-hoc leave-one-out critical-sentence method fails in presence of negation and cannot capture the full chain of reasoning. In the paper's direct comparison on the no-negation subset, RuleTakers' critical-sentence identification had accuracy 74.5% (high precision 98.7%, recall 86.9%, F1 92.4).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>RuleTakers analysis in this paper is limited to being a QA baseline and to the leave-one-out critical-sentence comparison; the paper uses RuleTakers' behavior as a reference to show that adding proof supervision (as in PRoVeR) produces better generalization and interpretable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3525.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3525.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa: A robustly optimized BERT pretraining approach (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained transformer encoder (BERT-family) used as the shared encoder backbone in PRoVeR and as the basis of the RuleTakers baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized bert pretraining approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained masked-language-model transformer (improved BERT training recipes); used here as the encoder whose token embeddings are pooled to produce node embeddings and [CLS] summarization; fine-tuned with task-specific heads (QA, Node, Edge).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Backbone encoder for rule-based natural-language reasoning tasks (DU datasets, ParaRules, Birds-Electricity)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Provides contextual embeddings for concatenated facts/rules and question; supports downstream supervised heads to perform binary QA and to predict proof-graph nodes and edges.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning with joint supervision on multiple heads; augmented with structural constraints during training (masking impossible edges) and ILP at inference to enforce global proof validity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As a backbone, fine-tuned RoBERTa-large in PRoVeR achieves high QA accuracy (PRoVeR QA overall 99.3% on DU5) and supports proof generation with PA ≈87.1% (when used with PRoVeR modules and constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Not applicable as a backbone; paper shows that adding proof supervision (node/edge labels and ILP constraints) on top of a RoBERTa backbone improves generalization and data efficiency for QA compared to training only QA head (e.g., near-perfect QA with 40% data).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Pretrained encoder alone does not produce structured proofs; success depends on added supervised modules and constrained inference. Computationally heavy (training PRoVeR takes multiple hours per epoch on a V100).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper's ablations demonstrate that the architectural additions and constraints (NAF module, edge masking, ILP) materially affect proof metrics even when the same RoBERTa backbone is used: removing NAF or constraints reduces node/edge/proof accuracies by several percent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language. <em>(Rating: 2)</em></li>
                <li>NLProlog: Reasoning with weak unification for question answering in natural language. <em>(Rating: 1)</em></li>
                <li>Learning a SAT solver from single-bit supervision. <em>(Rating: 1)</em></li>
                <li>An experimental study of formula embeddings for automated theorem proving in first-order logic. <em>(Rating: 1)</em></li>
                <li>RoBERTa: A robustly optimized bert pretraining approach. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3525",
    "paper_id": "paper-b4013141cceb937c46ea5f84f8c06f6bf1215106",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "PRoVeR",
            "name_full": "PRoVeR: Proof Generation for Interpretable Reasoning over Rules",
            "brief_description": "An interpretable transformer-based system that jointly answers binary questions over natural-language rule-bases and generates corresponding directed-graph proofs (nodes = facts/rules/NAF, edges = rule applications) using a RoBERTa encoder plus QA, node and edge modules, constrained training, and ILP-based inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large (fine-tuned with PRoVeR architecture)",
            "model_description": "Uses a pretrained RoBERTa-large transformer as a shared encoder; adds three heads: QA (binary classification on [CLS]), Node classifier (per-fact/rule/NAF presence), and Edge classifier (pairwise edge presence/direction using concatenation + difference of node embeddings). Trained with joint binary cross-entropy losses and constrained label masking; inference enforces global constraints via an ILP (max-flow connectivity + structural constraints).",
            "model_size": "355M",
            "reasoning_task_name": "Rule-based natural-language theorem proving (RuleTakers-style: DU0-DU5, Birds-Electricity, ParaRules)",
            "reasoning_task_description": "Binary question-answering and proof-generation over contexts consisting of natural-language facts and rules (including negation-as-failure). Task requires applying rules to derive or fail-to-derive statements; proofs are represented as directed graphs up to depth 5.",
            "method_or_intervention": "Joint fine-tuning of RoBERTa-large with three supervised heads (QA, Node, Edge); semantic masking of impossible edge labels during training (reduces label space); a learned NAF node computed from [CLS]; ILP inference to maximize edge posterior subject to structural constraints (no fact-&gt;fact, no rule-&gt;fact, connectivity via max-flow), ensuring globally valid proofs.",
            "performance": "On DU5 test: QA overall 99.3% (per-depth QA in Table 1: D0 100, D1 99.0, D2 98.8, D3 99.1, D4 98.8, D5 99.3). Proof metrics on DU5 (PRoVeR): Node Accuracy 89.2%, Edge Accuracy 87.5%, Proof Accuracy 87.1%, Full Accuracy 87.1%. Proof accuracy drops with depth (depth 5 PA = 65.1%). Zero-shot/out-of-distribution and paraphrase (ParaRules) results: PRoVeR attains high proof accuracy on ParaRules (≈95% in combined DU3+ParaRules setting) and shows strong zero-shot QA (the paper reports up to +6% QA improvement on some zero-shot evaluations). Data-efficiency: with 40% of DU5 training data PRoVeR achieves QA ≈97.8%.",
            "baseline_performance": "Compared against RuleTakers (fine-tuned RoBERTa baseline): DU5 overall QA reported 99.2% for RuleTakers (Table 1). RuleTakers does not generate explicit proofs (it used a post-hoc leave-one-out critical-sentence procedure for explanations). On some zero-shot splits RuleTakers QA reported ~98.8% (varies by split in paper tables).",
            "improvement_over_baseline": "PRoVeR matches or slightly improves QA vs RuleTakers (overall +0.1% on DU5) while additionally producing interpretable proofs (PA 87.1%). PRoVeR shows larger gains in generalization: when trained on lower-depth questions PRoVeR achieves up to +15% absolute improvement in QA on higher-depth test questions compared to the baseline in some settings; up to +6% QA improvement in some zero-shot evaluations.",
            "limitations_or_failures": "Proof generation is substantially harder than QA: edge prediction is the bottleneck and performance degrades with proof depth (PA falls to 65% at depth 5). The model tends to underestimate necessary nodes (42% of node-set errors are subsets of gold; 25% are supersets). Some failure modes are due to missing/incorrect edges rather than answer prediction (answers are almost always correct when the proof is correct). The model collapses multiple NAF nodes into a single learned NAF representation (limitation tied to dataset labeling). Producing exact proofs for deeper chains remains challenging and scope for future work.",
            "ablation_or_analysis": "Ablations: 'No NAF' (random NAF embeddings) causes a 5–6% drop in node and edge accuracy; removing constraints during training and inference ('Unconstrained Train + No ILP') causes ~5–6% drop in edge/proof accuracy; applying ILP at inference only ('UT + ILP') slightly recovers proof accuracy; removing connectivity constraint has marginal effect (model already predicts connected graphs in most cases; 57 examples disconnected without it). NAF prediction itself is highly accurate (≈95%). Edge directionality is learned nearly perfectly (only 1 example where direction alone caused proof failure in the analyzed depth-5 subset). Error breakdown for depth-5: on average PRoVeR predicts 6 of 7 edges correctly. Simpler lexical baselines (e.g., Random Forest on overlap features) give much lower edge accuracy (~47%).",
            "uuid": "e3525.0",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RuleTakers",
            "name_full": "RuleTakers (Transformers as soft reasoners over language)",
            "brief_description": "A prior benchmark and baseline showing that pretrained transformers (RoBERTa) can be fine-tuned to answer binary questions over natural-language rule-bases, characterized as 'soft theorem provers'; used in this paper as the primary baseline for QA.",
            "citation_title": "Transformers as soft reasoners over language.",
            "mention_or_use": "use",
            "model_name": "RoBERTa (fine-tuned baseline used in RuleTakers)",
            "model_description": "Fine-tuned RoBERTa model used to classify the truth of statements given concatenated context+question input; produces high QA accuracy but does not natively produce structured proof graphs (used leave-one-out critical-sentence analysis for post-hoc explanations).",
            "model_size": null,
            "reasoning_task_name": "Rule-based natural-language theorem proving (RuleTakers datasets / DU0-DU5, Birds-Electricity, ParaRules)",
            "reasoning_task_description": "Binary QA over sets of facts and rules expressed in natural language; measures ability to derive statements through multi-step rule application up to certain depths; intended to probe compositional reasoning of transformers.",
            "method_or_intervention": "Fine-tuning a pretrained transformer (RoBERTa) on the binary QA task; explanation via post-hoc leave-one-out 'critical sentence' identification (remove each sentence and observe answer flip).",
            "performance": "Reported QA on DU5 test (Table 1) overall ≈99.2% (per-depth examples: D0 100, D1 98.4, D2 98.4, D3 98.8, D4 99.2, D5 99.8 in Table 1 rows shown). On zero-shot/paraphrase splits RuleTakers also attains high QA but PRoVeR claims improvements on some zero-shot settings.",
            "baseline_performance": null,
            "improvement_over_baseline": "N/A (this is the baseline used by PRoVeR). PRoVeR matches or slightly exceeds RuleTakers QA and additionally provides explicit proof graphs.",
            "limitations_or_failures": "Does not generate explicit structured proofs; its post-hoc leave-one-out critical-sentence method fails in presence of negation and cannot capture the full chain of reasoning. In the paper's direct comparison on the no-negation subset, RuleTakers' critical-sentence identification had accuracy 74.5% (high precision 98.7%, recall 86.9%, F1 92.4).",
            "ablation_or_analysis": "RuleTakers analysis in this paper is limited to being a QA baseline and to the leave-one-out critical-sentence comparison; the paper uses RuleTakers' behavior as a reference to show that adding proof supervision (as in PRoVeR) produces better generalization and interpretable outputs.",
            "uuid": "e3525.1",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RoBERTa-large",
            "name_full": "RoBERTa: A robustly optimized BERT pretraining approach (large)",
            "brief_description": "A large pretrained transformer encoder (BERT-family) used as the shared encoder backbone in PRoVeR and as the basis of the RuleTakers baseline.",
            "citation_title": "RoBERTa: A robustly optimized bert pretraining approach.",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Pretrained masked-language-model transformer (improved BERT training recipes); used here as the encoder whose token embeddings are pooled to produce node embeddings and [CLS] summarization; fine-tuned with task-specific heads (QA, Node, Edge).",
            "model_size": "355M",
            "reasoning_task_name": "Backbone encoder for rule-based natural-language reasoning tasks (DU datasets, ParaRules, Birds-Electricity)",
            "reasoning_task_description": "Provides contextual embeddings for concatenated facts/rules and question; supports downstream supervised heads to perform binary QA and to predict proof-graph nodes and edges.",
            "method_or_intervention": "Fine-tuning with joint supervision on multiple heads; augmented with structural constraints during training (masking impossible edges) and ILP at inference to enforce global proof validity.",
            "performance": "As a backbone, fine-tuned RoBERTa-large in PRoVeR achieves high QA accuracy (PRoVeR QA overall 99.3% on DU5) and supports proof generation with PA ≈87.1% (when used with PRoVeR modules and constraints).",
            "baseline_performance": null,
            "improvement_over_baseline": "Not applicable as a backbone; paper shows that adding proof supervision (node/edge labels and ILP constraints) on top of a RoBERTa backbone improves generalization and data efficiency for QA compared to training only QA head (e.g., near-perfect QA with 40% data).",
            "limitations_or_failures": "Pretrained encoder alone does not produce structured proofs; success depends on added supervised modules and constrained inference. Computationally heavy (training PRoVeR takes multiple hours per epoch on a V100).",
            "ablation_or_analysis": "Paper's ablations demonstrate that the architectural additions and constraints (NAF module, edge masking, ILP) materially affect proof metrics even when the same RoBERTa backbone is used: removing NAF or constraints reduces node/edge/proof accuracies by several percent.",
            "uuid": "e3525.2",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language.",
            "rating": 2
        },
        {
            "paper_title": "NLProlog: Reasoning with weak unification for question answering in natural language.",
            "rating": 1
        },
        {
            "paper_title": "Learning a SAT solver from single-bit supervision.",
            "rating": 1
        },
        {
            "paper_title": "An experimental study of formula embeddings for automated theorem proving in first-order logic.",
            "rating": 1
        },
        {
            "paper_title": "RoBERTa: A robustly optimized bert pretraining approach.",
            "rating": 1
        }
    ],
    "cost": 0.016968,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PRoVer: Proof Generation for Interpretable Reasoning over Rules</h1>
<p>Swarnadeep Saha Sayan Ghosh Shashank Srivastava Mohit Bansal<br>UNC Chapel Hill<br>{swarna, sayghosh, ssrivastava, mbansal}@cs.unc.edu</p>
<h4>Abstract</h4>
<p>Recent work by Clark et al. (2020) shows that transformers can act as "soft theorem provers" by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PRoVer generates proofs with an accuracy of $87 \%$, while retaining or improving performance on the QA task, compared to RuleTakers (up to $6 \%$ improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to $15 \%$ improvement). Third, PRoVer obtains near perfect QA accuracy of $98 \%$ using only $40 \%$ of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to $65 \%$ for "depth 5", indicating significant scope for future work. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Developing systems that can understand and reason over explicitly provided knowledge has been a fundamental goal of AI (Newell and Simon, 1956). Owing to the challenges posed in reasoning over formal representations (Musen and Van Der Lei,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Block diagram showing that PRoVer is a closer linguistic analog of formal reasoning.</p>
<p>1988), and backed by the recent successes of transformers (Vaswani et al., 2017) in NLP, Clark et al. (2020) propose a new version of the problem by replacing the formal representations of rule-bases with natural language (English). Specifically, their task requires predicting the truth value of a statement by reasoning over a set of facts and rules, all expressed in natural language. Figure 2 shows some examples of the task. Clark et al. (2020) propose RuleTakers, a fine-tuned RoBERTa model (Liu et al., 2019b) to show that transformers can act as "soft theorem provers" by predicting the final answer in such reasoning-based problems with high accuracy.</p>
<p>We argue that to use transformers for natural language reasoning reliably, they should be able to generate proofs that provide rationales for the predicted answer. Proof generation is vital for emulating formal reasoning but also for moving towards human-interpretable models that alleviate concerns about the black-box nature of deep architectures (Rudin, 2019). Towards this, we present PRoVer, a transformer-based model that jointly answers questions over natural language rule-bases and generates corresponding proofs. Figure 1 illustrates our method as a closer linguistic analog</p>
<p>of formal reasoning, as it generates proofs along with answers. However, unlike formal reasoners, PRoVER can operate on natural language text that provides the underlying theory, rather than rely on formal logical representations. Such methods that combine interpretability and flexibility in reasoning can have wide applications across domains.</p>
<p>PROVER's architecture consists of three modules that together generate answers along with proofs. In this work, proofs are represented as directed graphs consisting of the relevant rules and facts needed to prove or disprove the question statement. Section 3.1 contains details of this representation. A QA module predicts a binary answer for the question, a node module chooses which rules and facts are part of the proof, and an edge module predicts the presence and the direction of the edges between the chosen nodes. Model training minimizes a joint cross-entropy loss over the three modules. To guide the model to predict edges between valid nodes only, we enforce global constraints on the structure of the proof during training, by masking out labels for impossible edges, resulting in a more efficient learning problem. PRoVER generates valid proofs during inference by solving an ILP over the edge potentials, subject to multiple semantic constraints, such as ensuring proof graph connectivity. Our contributions are:</p>
<ul>
<li>We present PRoVER, an interpretable joint model that learns to reason over natural language rule-bases and generate corresponding proofs.</li>
<li>PRoVER performs similarly or improves upon state-of-the-art QA accuracy for the task, with up to $6 \%$ improvement on zero-shot evaluation, and generates exact proofs at $87 \%$ accuracy. Unlike RuleTakers, it does not require additional finetuning on the RACE (Lai et al., 2017) dataset.</li>
<li>PRoVER demonstrates significantly better generalization. When trained on lower depth questions, it shows better QA accuracy (up to 15\%) on higher depth ones.</li>
</ul>
<h2>2 Related Work</h2>
<p>Our work is related to multiple bodies of previous work in NLP and formal reasoning.</p>
<p>QA and NLI: The rule reasoning task is related to reasoning tasks that have been proposed recently. These include tasks in the bAbI dataset (Weston et al., 2015), synthetically generated probe tasks (Richardson et al., 2020) or reading comprehension tasks in datasets such as QuaRTz (Tafjord et al.,
2019) and ROPES (Lin et al., 2019). Unlike our task, most of these require reasoning over implicit rules, the focus being on language understanding and one step of rule application. Multi-hop QA datasets like HotpotQA (Yang et al., 2018) require multiple reasoning steps, but the inference rules needed are again implicitly inferred, rather than explicitly provided. Our task also bears similarity with Natural Language Inference (MacCartney and Manning, 2014), but NLI also allows unsupported inferences by filling gaps in explicitly stated knowledge (Dagan et al., 2013).</p>
<p>Formal Reasoning and Neural Theorem Proving: Semantic parsing (Zettlemoyer and Collins, 2005; Berant et al., 2013; Berant and Liang, 2014) of multi-sentence texts into logical forms has proved to be challenging, restricting the application of semantic parsers to formal reasoning systems (Kamath and Das, 2019). PRoVER bypasses this expensive and error-prone process and attempts to solve the problem in an end-to-end manner, without any intermediate logical representations.</p>
<p>Our approach is conceptually similar to a body of work on Neural Theorem Proving (Weber et al., 2019) that has focused on developing theorem provers by combining reasoning from symbolic techniques with the possibility of differentiable learning from neural networks. These include neuro-symbolic methods for table comprehension (Neelakantan et al., 2016), executing basic compositional programs (Reed and de Freitas, 2016), SAT solving (Selsam et al., 2019), formula embedding (Abdelaziz et al., 2020), approximate (DNF) model counting (Abboud et al., 2020), etc. However, PRoVER diverges from these in working with free-form natural language input to generate proofs similar to formal reasoners.</p>
<p>Model Interpretability: PRoVER follows a significant body of previous work on developing interpretable neural models for NLP tasks to foster explainability. Several approaches have focused on formalizing the notion of interpretability (Rudin, 2019; Doshi-Velez and Kim, 2017; Hase and Bansal, 2020), tweaking features for local model interpretability (Ribeiro et al., 2016, 2018) and exploring interpretability in latent spaces (Joshi et al., 2018; Samangouei et al., 2018). Our work can be seen as generating explanations in the form of proofs for an NLP task. While there has been prior work on generating natural language explana-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Diagram showing two rule-bases with rules, facts, questions, answers and proofs. PROVER answers all the questions correctly and also generates all the corresponding proofs accurately in the above scenarios.
tions for multiple NLP tasks, including NLI (Camburu et al., 2018), commonsense reasoning (Rajani et al., 2019; Zhang et al., 2020) and generic text classification tasks (Liu et al., 2019a), our novelty lies in generating compositional explanations consisting of proof graphs that detail the chain of reasoning, starting from language. We use a maxflow ILP formulation for checking proof graph connectivity (Even and Tarjan, 1975). Multiple approaches for NLP tasks such as sentiment analysis and content selection (Pang and Lee, 2004; Barzilay and Lapata, 2005; Bansal et al., 2008) have been framed as optimal flow problems on graphs.</p>
<p>Program Synthesis with Transformers: Existing works show that transformers already capture some knowledge from pre-training for algorithm emulation (Talmor et al., 2019) or can be fine-tuned for tasks like semantic parsing (He and Choi, 2020), translation (Wang et al., 2019), symbolic integration (Lample and Charton, 2020) and mathematics (Saxton et al., 2019). In our work, we also employ a transformer-based pre-trained language model (RoBERTa (Liu et al., 2019b)) but for the downstream task of rule-based reasoning.</p>
<h2>3 Method</h2>
<p>Each input to PROVER is a context $C$ (consisting of facts $F$ and rules $R$ ) and a question $Q$, about the context. PROVER predicts the answer $A \in$
{True, False $}$ and generates a proof $\mathcal{P}$.</p>
<h3>3.1 Proof Representation</h3>
<p>A proof, $\mathcal{P}=(\mathcal{N}, \mathcal{E})$, is a directed graph with nodes $n \in \mathcal{N}$ and edges $e \in \mathcal{E}$. Each node is either a fact $f \in F$, a rule $r \in R$ or a special $N A F$ node (Negation As Failure, as described below). Edges in the proof are directed either from a fact (or $N A F$ ) to a rule or from a rule to another rule. These indicate that a fact (or $N A F$ ) is consumed by a rule, or the output of a rule (a new fact) is consumed by another rule, respectively. We use these constraints both during PROVER's training and inference, as described later in the paper. Formally, we have:</p>
<p>$$
\begin{gathered}
\mathcal{P}=(\mathcal{N}, \mathcal{E}) \
\mathcal{N} \subseteq R \cup F \cup N A F \
\mathcal{E} \subseteq \mathcal{N} \times \mathcal{N}
\end{gathered}
$$</p>
<p>Figure 2 shows examples of two contexts (consisting of facts and rules), five questions about the contexts, along with their answers and proofs. Each proof has a depth ( $Q_{1}$ 's proof has a depth of 1 ). The maximum proof depth in all the datasets considered in this work (Clark et al., 2020) is 5. Proofs in the datasets are of three types:</p>
<p>Successful proof with NAF: The proof of $Q_{1}$ in Figure 2 is one such such example. $F_{2}$ acts on $R_{4}$ to prove that "The wire is conducting." and hence the answer is false.</p>
<p>Successful proof with NAF: Given a statement $s, N A F$ in logic programming is a non-monotonic inference rule used to derive "not $s$ " (negation of the statement) from failure to derive $s$. Hence, a proof may contain $N A F$ node(s), representing the truthfulness of the negation of statement(s) that cannot be proved using the set of rules. Consider the proofs for $Q_{4}$ and $Q_{5}$ where the $N A F$ node in $Q_{4}$ represents "The bald eagle is not kind.".</p>
<p>Failed proof: This happens when a statement cannot be derived using the given rule-base and the shallowest branch of the proof tree that fails is shown. $Q_{3}$ 's proof in Figure 2 is an example as "The radio is playing." cannot be proved.</p>
<p>Note that a proof can have edges between two rules in both directions. E.g., consider the edges $R_{4} \rightarrow R_{5}$ and $R_{5} \rightarrow R_{4}$ in $Q_{5}$ 's proof in Figure 2. A node can have more than two incoming edges the node $R_{6}$ in $Q_{2}$ has three incoming edges from $R_{1}, R_{3}$, and $R_{4}$.</p>
<h3>3.2 Task Description</h3>
<p>Each training example is a tuple $\left(C_{i}::=\right.$ $\left.\left{F_{i}, R_{i}\right}, Q_{i}, A_{i}, \mathcal{P}_{i}\right)$ consisting of a context (set of rules and facts), a question, the corresponding answer, and a proof. Generating a proof graph requires (1) identifying the nodes (set of relevant facts, $N A F$ and rules) that are part of the proof, (2) identifying the edges connecting these nodes, and (3) verifying a set of global constraints such as proof connectivity that ensure a valid proof.</p>
<p>For the first, we predict a binary label over each rule, fact and $N A F$ denoting their presence or absence in the proof. For the second, we also predict binary labels denoting the presence or absence of each edge. For the third, we enforce constraints during both training and inference (Section 3.4). During training, we mask out the edge labels ${ }^{2}$ corresponding to (1) self-loops, (2) edges between absent nodes, and (3) edges between facts to facts and rules to facts. This enforces a semantic constraint that the set of candidate edges in the ensuing proof is consistent with the chosen set of nodes, and also simplifies the learning problem, since a smaller number of edges need to be labeled.</p>
<h3>3.3 PROVER: Joint QA and Proof Generation Model</h3>
<p>Figure 3 shows the architecture of PROVER, built on top of RoBERTa (Liu et al., 2019b). Our model</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>consists of three modules: (1) QA module, (2) Node module, and (3) Edge module. The QA module is exactly the same as the RuleTakers model (Clark et al., 2020), thus allowing us to directly evaluate the effectiveness of our node and edge modules. The input to RoBERTa is the concatenation of the context and the question, separated by the $[S E P]$ tokens. The context is represented by concatenating the text consisting of facts and rules. Formally, if the rules and facts are denoted by $\left{R F_{i}\right}_{i=1}^{k}$ and the question by $Q$, the input is</p>
<p>$$
[C L S]\left{R F_{i}\right}_{i=1}^{k}[S E P][S E P] Q[S E P]
$$</p>
<p>QA Module: The output of RoBERTa contains an embedding for each token in the context and a global embedding corresponding to the $[C L S]$ token. The QA classification head $H_{Q A}$ is a sequence of two linear layers with dropout probability of $p$. Formally, if $t_{[C L S]}$ denotes the $[C L S]$ token embedding, we obtain the class-wise probability values $P_{Q A}$ using the softmax function $\sigma$.</p>
<p>$$
P_{Q A}=\sigma\left(H_{Q A}\left(t_{[C L S]}\right)\right)
$$</p>
<p>Node Module: Let $\left{w_{j}^{(i)}\right}<em i="i">{j=1}^{m}$ denotes the $m$ tokens corresponding to $R F</em>\right}}$. Assuming the corresponding RoBERTa embeddings are denoted by $\left{t_{w_{j}^{(i)}<em F__i="F_{i" R="R">{j=1}^{m}$, we learn a representation $t</em>$, by performing a mean pooling $M P$ of the constituent token embeddings.}}$ for each $R F_{i</p>
<p>$$
t_{R F_{i}}=M P\left(\left{t_{w_{j}^{(i)}}\right}_{j=1}^{m}\right)
$$</p>
<p>We also learn a representation of the $N A F$ node $t_{N A F}$ as a linear transformation on $t_{[C L S]}$. Note that due to the self-attention layers of RoBERTa, $t_{[C L S]}$ summarizes the set of all derivable facts given the context and the question. We want the $N A F$ node to encode information about all facts containing negation (e.g.,"The bald eagle is not kind" in $Q_{4}$ 's proof of Figure 2) in the context. These are taken as true as their positive counterparts ("The bald eagle is kind") are non-derivable given the context. Thus, if a statement $s$ cannot be derived from the facts and the rules in a context, the $N A F$ node should infer that "not $s$ " is true. We model this notion of the negation of all unprovable statements (given a context) by learning $N A F$ as a function of everything provable in the context, encoded by the $t_{[C L S]}$ embedding. ${ }^{3}$</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Architecture diagram of PROVER. The presence and absence of nodes/edges are labeled by 1 and 0 respectively while -100 represents masked out edges.</p>
<p>The node classifier $H_{N o d e}$ has a similar architecture to the QA classifier and predicts a presence and absence probability score for each node.</p>
<p>$$
P_{N o d e}=\sigma\left(H_{N o d e}\left(\left{t_{R F_{i}}\right}<em A="A" F="F" N="N">{i=1}^{k}, t</em>\right)\right)
$$</p>
<p>Edge Module: Now, given the representations of each fact, rule and $N A F$, we learn a representation for each edge between these. Formally, we define the edge embedding $t_{\left(R F_{i}, R F_{j}\right)}$ from node $R F_{i}$ to node $R F_{j}$ by concatenating their individual embeddings $t_{R F_{i}}$ and $t_{R F_{j}}$ with their element-wise difference (which gives the directionality vector).</p>
<p>$$
t_{\left(R F_{i}, R F_{j}\right)}=\left[t_{R F_{i}}, t_{R F_{j}},\left(t_{R F_{j}}-t_{R F_{i}}\right)\right]
$$</p>
<p>The above formulation also helps learn separate representations for edges $R F_{i} \rightarrow R F_{j}$ and $R F_{j} \rightarrow$ $R F_{i}$. This is essential for our task as a proof can have edges between two rules in both directions. In Section 4.7, we see that this formulation leads to a near perfect empirical performance in predicting the directionality of edges. The edge classifier $H_{E d g e}$ outputs probability scores representing the presence and absence of each edge.</p>
<p>$$
\begin{array}{r}
P_{E d g e}=\sigma\left(H_{E d g e}\left(\left{t_{\left(R F_{i}, R F_{j}\right)}\right}<em F__k_1="F_{k+1" R="R">{i, j=1}^{k+1}\right)\right) \
t</em>
\end{array}
$$}}=t_{N A F</p>
<p>We train our model by using binary cross-entropy loss for each of the three modules. Formally, if</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$L_{Q A}, L_{N o d e}$ and $L_{E d g e}$ denote the three losses, the overall loss $L$ is given by:</p>
<p>$$
L=L_{Q A}+L_{N o d e}+L_{E d g e}
$$</p>
<h3>3.4 ILP Inference for Global Constraints</h3>
<p>As mentioned previously, during inference, we enforce additional constraints on the structure of the predicted proof graph. For this, we frame inference as Integer Linear Program (ILP) optimization, which we describe next. We follow the generative process of a graph wherein the nodes are defined first, followed by the edges on that set of nodes. Thus, we fix the nodes first based on the predictions by the node module of our model and maximize a global score over the set of edges only. This reduces the large search space and ensures that all constraints can be expressed as linear expressions.
Proof Connectivity Formulation: An important constraint is to ensure that the predicted proof graphs are connected. ${ }^{4}$ To check if a proof graph $\mathcal{P}$ is connected, we define an augmented graph $\mathcal{P}<em _aug="{aug" _text="\text">{\text {aug }}=\left(\mathcal{N}</em>$ to the sink.}}, \mathcal{E}_{\text {aug }}\right)$ with two added nodes "source" and "sink". We add an edge from the source to any one of the nodes $x$ in $\mathcal{P}$. We also define edges from all nodes in $\mathcal{P</p>
<p>$$
\mathcal{N}_{\text {aug }}=\mathcal{N} \cup{\text { source }, \text { sink }}
$$</p>
<p>$\mathcal{E}<em _aug="{aug" _text="\text">{\text {aug }}=\mathcal{E} \cup{($ source, $x)} \cup{(n, \operatorname{sink}) \forall n \in \mathcal{N}}$
Having defined $\mathcal{P}</em>$ to a maximum flow problem}}$, we can reduce the graph connectivity in $\mathcal{P</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>(Leighton and Rao, 1999) in $\mathcal{P}<em _m_="(m," n_="n)">{\text {aug }}$ (Even and Tarjan, 1975). For this, we define the capacity variable $c</em>$, as follows.}$ for each edge, $m \rightarrow n$ in $P_{\text {aug }</p>
<p>$$
\begin{gathered}
c_{(\text {source }, x)}=|\mathcal{N}| \text { and } c_{(x, \text { source })}=0 \
\forall n \in \mathcal{N}, c_{(n, \text { sink })}=1 \text { and } c_{(\text {sink }, n)}=0 \
\forall m, n \in \mathcal{N}, c_{(m, n)}=|\mathcal{N}| \
c_{(m, n)}=0 \text { if } m \notin \mathcal{N} \text { or } n \notin \mathcal{N}
\end{gathered}
$$</p>
<p>Now, there can be a maximum total flow of $|\mathcal{N}|$ from the source to the sink, if and only if the graph is connected. We use this flow formulation to provide additional constraints for our ILP inference procedure that ensure connectivity of proof graphs.</p>
<p>Final Optimization Problem: Our maximization objective, subject to the connectivity constraint and all other constraints (that ensure a valid proof) is as follows. Let $\phi_{(m, n)}$ represent the probability that an edge $m \rightarrow n$ is present, as predicted by PROVER. We want to infer $0 / 1$ assignments for our optimization variables $e_{(m, n)}$ (a value of 1 means the edge is part of the proof, while 0 means it is not) such that the following objective is maximized:</p>
<p>$$
\begin{gathered}
\underset{e_{(m, n)}, f_{(m, n)}}{\operatorname{argmax}} \sum_{m, n, m \neq n}\left(\phi_{(m, n)} e_{(m, n)}+\right. \
\left.\left(1-\phi_{(m, n)}\right)\left(1-e_{(m, n)}\right)\right)
\end{gathered}
$$</p>
<p>subject to constraints:</p>
<p>$$
\begin{gathered}
\forall m, n \in F \cup R \cup N A F, e_{(m, n)} \in{0,1} \
e_{(m, n)}=0, \text { if } m \notin \mathcal{N} \text { or } n \notin \mathcal{N} \
e_{(m, n)}=0, \text { if } m \in F \text { and } n \in F \
e_{(m, n)}=0, \text { if } m \in R \text { and } n \in F \
\forall m, n \in \mathcal{N}<em _m_="(m," n_="n)">{\text {aug }}, 0 \leq f</em> \
\forall n \in \mathcal{N}} \leq c_{(m, n)<em _in="\in" _mathcal_E="\mathcal{E" m:_m_="m:(m," n_="n)">{\text {aug }}, \sum</em><em _m_="(m," n_="n)">{\text {aug }}} f</em> \
=\sum_{o:(n, o) \in \mathcal{E}<em _n_="(n," o_="o)">{\text {aug }}} f</em> \
f_{(\text {source }, x)}=|\mathcal{N}| \
\forall m, n \in \mathcal{N}<em _m_="(m," n_="n)">{\text {aug }}, e</em> \
-\left(f_{(m, n)} /|\mathcal{N}|\right) \geq 0
\end{gathered}
$$}+e_{(n, m)</p>
<p>Note that $\mathcal{N}, F$, and $R$ refer to the set of predicted nodes (from the model), the set of facts, and the set of rules, respectively. Equations 2, 3 and 4 ensure that edges are present only when the corresponding nodes are present and that there are no edges between two facts and from a rule to a
fact. Next, to ensure proof connectivity, we first define the flow constraints in Equations 5 and 6 constrained by the flow variables $f_{(m, n)}$ for each edge $m \rightarrow n$. These maintain the capacity constraints (the flow at each edge should be less than its capacity) and the flow conservation constraints (the total flow through the incoming edges at a node is equal to the total flow through the outgoing edges). Equation 7 ensures connectivity in the proof graph, by enforcing the total flow to be $|\mathcal{N}|$. Finally, we ensure that the proof connectivity is checked on the valid edges only (which are part of the proof) through the last constraint, since a max-flow of $|\mathcal{N}|$ is achievable for any connected graph.</p>
<h2>4 Experiments</h2>
<p>Our experiments evaluate the effectiveness of PROVER (PR), our joint QA, and proof model against RuleTakers (RT). Details of our experimental setup are in the appendix.</p>
<h3>4.1 Datasets and Evaluation Metrics</h3>
<p>We conduct experiments on all the three sets of datasets introduced in Clark et al. (2020) and consisting of gold answers and proofs. Further details of the datasets can be found in the appendix.
DU0-DU5: The first set consists of five datasets, each containing 100 k questions with theories in synthetic language and requiring reasoning paths up to depth $D(D=0,1,2,3,5)$. We refer to these datasets as DU0, DU1, DU2, DU3 and DU5, where DU stands for "Depth Upto".
Birds-Electricity: It consists of two test-only datasets of 5 k samples used to evaluate the out-of-distribution performance of the models.
ParaRules: ParaRules consists of 40k questions against 2 k theories expressed in paraphrased natural language, obtained through crowdsourcing.</p>
<p>We evaluate QA performance through accuracy. For proofs, we introduce three metrics: (1) Node Accuracy (NA): Fraction of examples where the predicted node set matches exactly with the gold node set, (2) Edge Accuracy (EA): Fraction of examples where the predicted edge set match exactly with the gold set, and (3) Proof Accuracy (PA): Fraction of examples where the generated proof matches exactly with the gold proof. For examples with multiple gold proofs, we give credit if the prediction matches exactly with any one of the proofs. We also evaluate Full Accuracy (FA), denoting the fraction of samples where both the answer and the</p>
<table>
<thead>
<tr>
<th>D</th>
<th>Cnt</th>
<th>QA</th>
<th></th>
<th>NA</th>
<th>EA</th>
<th>PA</th>
<th>FA</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>RT</td>
<td>PR</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>0</td>
<td>6299</td>
<td>$\mathbf{1 0 0}$</td>
<td>$\mathbf{1 0 0}$</td>
<td>98.6</td>
<td>98.5</td>
<td>98.4</td>
<td>98.4</td>
</tr>
<tr>
<td>1</td>
<td>4434</td>
<td>98.4</td>
<td>$\mathbf{9 9 . 0}$</td>
<td>93.3</td>
<td>95.1</td>
<td>93.2</td>
<td>93.1</td>
</tr>
<tr>
<td>2</td>
<td>2915</td>
<td>98.4</td>
<td>$\mathbf{9 8 . 8}$</td>
<td>85.9</td>
<td>84.8</td>
<td>84.8</td>
<td>84.8</td>
</tr>
<tr>
<td>3</td>
<td>2396</td>
<td>98.8</td>
<td>$\mathbf{9 9 . 1}$</td>
<td>82.3</td>
<td>80.5</td>
<td>80.5</td>
<td>80.5</td>
</tr>
<tr>
<td>4</td>
<td>2134</td>
<td>$\mathbf{9 9 . 2}$</td>
<td>98.8</td>
<td>77.7</td>
<td>72.5</td>
<td>72.5</td>
<td>72.4</td>
</tr>
<tr>
<td>5</td>
<td>2003</td>
<td>$\mathbf{9 9 . 8}$</td>
<td>99.3</td>
<td>76.0</td>
<td>65.1</td>
<td>65.1</td>
<td>65.1</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>All</td>
<td>20192</td>
<td>99.2</td>
<td>$\mathbf{9 9 . 3}$</td>
<td>89.2</td>
<td>87.5</td>
<td>87.1</td>
<td>87.1</td>
</tr>
</tbody>
</table>
<p>Table 1: QA comparison between RT and PR for varying depths along with node, edge, proof and full accuracy for PRoVER on DU5. Cnt $=$ Sample Count. proof are exactly correct.</p>
<h3>4.2 QA and Proof Results for Varying Depths</h3>
<p>We first train and evaluate PRoVER on the train and test splits of the DU5 dataset, and compare its QA performance with RuleTakers for questions of varying depths (D). Table 1 shows these results and the proof-related metrics for PRoVER. The corresponding validation set results can be found in the appendix. Overall, and at each depth, PRoVER matches the QA performance of RuleTakers. PRoVER is also able to generate exact proofs fairly accurately at $87 \%$. Perhaps unsurprisingly, we find that edge prediction is a harder task than node prediction, and performance worsens with increasing depth due to an increasingly large number of edges to be labeled. The proof accuracy matches the edge accuracy at each depth, suggesting that proofs are almost always correct if the edges are correct. Similarly, the full accuracy matches the proof accuracy, showing that the predicted answer is almost always correct when the corresponding proof is correct. This points to an interesting observation - QA is easier than node prediction, which in turn is easier than edge prediction. All the datasets experimented with exhibit this behavior, as we also describe later. Proof generation becomes harder with increasing depth (and hence, more nodes and edges), as the exact proof generation accuracy drops to $65 \%$ for depth 5 . On analyzing further, we find that on average, PRoVER correctly predicts 6 out of 7 edges present in a depth 5 proof. Overall, PRoVER is interpretable yet efficient, as it generates proofs fairly accurately without any loss in QA performance.</p>
<h3>4.3 Zero-Shot Evaluation</h3>
<p>Following previous work (Clark et al., 2020), we now test the out-of-distribution performance of</p>
<p>|  | Cnt | QA |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---</p>
<table>
<thead>
<tr>
<th style="text-align: center;">D</th>
<th style="text-align: center;">Cnt</th>
<th style="text-align: center;">QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NA</th>
<th style="text-align: center;">EA</th>
<th style="text-align: center;">PA</th>
<th style="text-align: center;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">PR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2968</td>
<td style="text-align: center;">$\mathbf{9 9 . 8}$</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.4</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2406</td>
<td style="text-align: center;">$\mathbf{9 9 . 3}$</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">97.3</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1443</td>
<td style="text-align: center;">$\mathbf{9 8 . 2}$</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">88.7</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1036</td>
<td style="text-align: center;">$\mathbf{9 6 . 7}$</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">89.9</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">142</td>
<td style="text-align: center;">$\mathbf{9 0 . 1}$</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">8008</td>
<td style="text-align: center;">$\mathbf{9 8 . 8}$</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">95.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of models trained on DU3 and ParaRules training sets and tested on ParaRules test set.
and $6 \%$, respectively. On DU3, both models show high and comparable performance. PRoVER's superior generalization ability can be attributed to the extra training supervision incorporated in the form of proofs and an inductive bias for making proof-based predictions. While proof construction for supervised training is expensive, PRoVER's superior QA results on out-of-distribution data (Table 2) and higher depth questions is a potential first step to showing that limited proof supervision can still lead to effective generalization.</p>
<h3>4.5 Varying Training Data Size</h3>
<p>We explore varying the amount of training data from 10 k to 30 k to all the examples ( 70 k ) in DU5. As shown in Table 4, when trained with only $40 \%$ of the data, PRoVER obtains a near-perfect QA accuracy of $97.8 \%$. Thus, for QA, PRoVER's joint training with proofs can compensate for the lack of training data. Proof generation, however, is much harder and with increased training data, the rate of increase in proof accuracy is much more gradual.</p>
<h3>4.6 Evaluation on Complex Language</h3>
<p>We also test PRoVER's ability to generate proofs for more human-like natural language theories. More details on the ParaRules dataset can be found in the appendix. Following Clark et al. (2020), we train a model by combining the DU3 and ParaRules training partitions and test on the ParaRules test partition. Table 3 again shows that PRoVER matches the QA performance of RuleTakers, and also generates proofs with a high accuracy of $95 \%$. Following previous trends, the proof accuracy drops as the depth increases, and QA performance is higher than for node prediction, which in turn is higher than for edge prediction.</p>
<h3>4.7 Ablation and Error Analysis</h3>
<p>Table 5 analyzes the effectiveness of the individual components of PRoVER through an ablation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Count</th>
<th style="text-align: left;">QA</th>
<th style="text-align: left;">NA</th>
<th style="text-align: left;">EA</th>
<th style="text-align: left;">PA</th>
<th style="text-align: left;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">10 k</td>
<td style="text-align: left;">87.1</td>
<td style="text-align: left;">48.1</td>
<td style="text-align: left;">44.7</td>
<td style="text-align: left;">44.0</td>
<td style="text-align: left;">42.7</td>
</tr>
<tr>
<td style="text-align: left;">30 k</td>
<td style="text-align: left;">97.8</td>
<td style="text-align: left;">77.9</td>
<td style="text-align: left;">73.2</td>
<td style="text-align: left;">72.5</td>
<td style="text-align: left;">72.4</td>
</tr>
<tr>
<td style="text-align: left;">70 k</td>
<td style="text-align: left;">99.3</td>
<td style="text-align: left;">89.2</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">87.1</td>
<td style="text-align: left;">87.1</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of PRoVER models trained with varying amount of training data on DU5. Count $=$ Number of training examples.
study. These ablated variants also provide natural baselines for our proof-related results. Specifically, we train and test the following models on DU5: (1) QA+Node: We train a model consisting of only the QA and Node modules; (2) No NAF: We train a model using random NAF embeddings; (3) Unconstrained Train (UT) + No ILP: We remove constraints both during training and inference; (4) Unconstrained Train (UT) + ILP: We remove constraints only during training; (5) No Connectivity: Finally, we train a model where we only remove the connectivity constraint during inference. More details about these models in appendix.</p>
<p>The QA accuracy is mostly unaffected in all our models and all but "No NAF" have similar node accuracy. The "No NAF" model does not learn a representation for NAF, leading to 5-6\% drop in both node and edge accuracy. The 5-6\% drop in edge and proof accuracy for the "Unconstrained Train + No ILP" model, compared to PRoVER, shows that removing constraints results in a harder learning problem and the model fails to automatically learn all the constraints. The proof accuracy improves slightly when we add constraints only during inference ("Unconstrained Train + ILP"). The connectivity constraint provides only marginal improvement as our model mostly predicts connected proofs without any explicit supervision. Specifically, only 57 examples have disconnected proofs without this constraint. The overall PRoVER model outperforms all variants in full accuracy.</p>
<p>To better understand the loss of accuracy for higher depth proofs, we perform error analysis of PRoVER for the depth 5 subset of DU5. We find that our NAF learning module is highly accurate PRoVER correctly predicts NAF in a proof $95 \%$ of the time. Among all examples with incorrectly predicted node sets, $42 \%$ are such that the predicted set is a subset of the gold set while for $25 \%$ examples, it is a superset, demonstrating that our model tends to underestimate the number of essential rules and facts. PRoVER almost perfectly identifies the direction of edges. We find only 1 example where the proof is incorrect solely due to the incorrect</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">QA</th>
<th style="text-align: left;">NA</th>
<th style="text-align: center;">EA</th>
<th style="text-align: center;">PA</th>
<th style="text-align: center;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QA+N+E (PR)</td>
<td style="text-align: left;">99.3</td>
<td style="text-align: left;">89.2</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">QA+N</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">88.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">QA (RT)</td>
<td style="text-align: left;">99.2</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">No NAF</td>
<td style="text-align: left;">$\mathbf{9 9 . 5}$</td>
<td style="text-align: left;">83.1</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">81.7</td>
</tr>
<tr>
<td style="text-align: left;">UT + No ILP</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">$\mathbf{9 0 . 1}$</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: left;">UT + ILP</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">$\mathbf{9 0 . 1}$</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">82.8</td>
</tr>
<tr>
<td style="text-align: left;">No Connectivity</td>
<td style="text-align: left;">99.3</td>
<td style="text-align: left;">89.2</td>
<td style="text-align: center;">$\mathbf{8 7 . 8}$</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">87.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation studies of PROVER showing the importance of each component and constraints.
identification of directionality. Further, $21 \%$ of the incorrectly predicted edges are subsets of the gold sets, while $35 \%$ are supersets.</p>
<h2>5 Discussion and Future Work</h2>
<p>Graph-based Explanations: While we have presented PROVER as a model that can emulate formal reasoning, it has further potential use as an explanation generation system. PROVER generates compositional explanations in the form of graphs and QA systems, in general, can potentially benefit from generating such graphical explanations. For example, in multi-hop QA tasks, the node module can choose all the relevant sentences in the context and the edge module can identify the flow of information between these to arrive at the answer (in the presence of task-specific constraints). Graphical explanations, in contrast to natural language ones, are more structured and can allow explicit modeling of causality (and are easier to evaluate, as opposed to free-form natural language generation). We hope that PROVER will encourage further work towards developing interpretable NLP models with structured explanations.</p>
<p>QA and Proof Consistency: Currently, PROVER predicts the answer and generates the proof by jointly optimizing the QA, node and edge modules using a shared RoBERTa model. Another modeling choice could explicitly condition the QA module on the node and edge modules so that the answer is predicted from the proof. We empirically verify the consistency between the predicted answer and the generated proof by showing that the full accuracy matches the proof accuracy. However, in scenarios where questions have open-ended answers, generating answer from a 'proof' in a consistent manner needs more exploration. PROVER's constraints like ensuring connectivity are necessary constraints for generating valid proofs for any graph-based
explanation generation system. However, other tasks may require imposing additional constraints to ensure valid explanations.PROVER's inference mechanism can be extended to incorporate these.</p>
<h2>Broader Implications in Formal Logic:</h2>
<p>PROVER's framework is not conceptually constrained to a particular logic fragment. PROVER uses the idea that applying a rule to fact(s) can produce new fact(s). All logic fragments from formal logic fit this idea and may only differ in the nature of the graphs generated. For a fact "Robin is a bird" and a rule with universal quantification "All birds can fly", PROVER's graph will have an edge from the fact to the rule to generate "Robin can fly". We experiment with datasets which already contain negations in facts. While these datasets currently do not contain disjunctions, our graphical representations of proofs allow an easy extension in such scenarios. E.g., if there is a disjunction rule "If X or Y then Z" instead of a conjunction rule "If X and Y then Z", only the shape of the graph changes. In the former, Z is proved by either an edge from X or from Y to the rule, while in the latter, both edges have to be necessarily present. Inferences over modals like "might" and disjunction rules like "If X then Y or Z" will mean that both the answer and the proof will be probabilistic. In such scenarios, PROVER's unweighted proof graphs can be extended to weighted ones to represent this probabilistic nature.</p>
<h2>6 Conclusion</h2>
<p>We introduce PROVER, an interpretable joint model that answers binary questions over natural language rule-bases and generates corresponding proofs. The proofs are generated through the node and edge modules of the model in the presence of multiple global constraints during training and ILP inference. Our model improves state-of-theart QA accuracy in the zero-shot scenario by $6 \%$ and generates proofs accurately. PROVER also generalizes much better to higher depth questions with up to $15 \%$ absolute improvement in QA performance over RuleTakers. PROVER's modeling is relatively generic, and similar proof generation methods can be explored in traditional multi-hop QA tasks. PROVER can also be a helpful aid to formal reasoners in scenarios where rules are fuzzy and creating rule-bases in a formal language is tedious or infeasible.</p>
<h2>Acknowledgements</h2>
<p>We thank the reviewers for their helpful feedback. This work was supported by DARPA MCS Grant N66001-19-2-4031, NSF-CAREER Award 1846185, DARPA YFA17-D17AP00022, ONR Grant N00014-18-1-2871, Microsoft Investigator Fellowship, and Munroe \&amp; Rebecca Cobey Fellowship. The views in this article are those of the authors and not the funding agency.</p>
<h2>References</h2>
<p>Ralph Abboud, Ismail Ilkan Ceylan, and Thomas Lukasiewicz. 2020. Learning to reason: Leveraging neural networks for approximate DNF counting. In Proceedings of the 34th AAAI Conference on Artificial Intelligence.</p>
<p>Ibrahim Abdelaziz, Veronika Thost, Maxwell Crouse, and Achille Fokoue. 2020. An experimental study of formula embeddings for automated theorem proving in first-order logic. arXiv preprint arXiv:2002.00423.</p>
<p>Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In COLING 2008: Companion Volume: Posters, pages $15-18$.</p>
<p>Regina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 331-338. Association for Computational Linguistics.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533-1544.</p>
<p>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14151425 .</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems, pages 9539-9549.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In International Joint Conference on Artificial Intelligence.</p>
<p>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1-220.</p>
<p>Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608.</p>
<p>Shimon Even and R Endre Tarjan. 1975. Network flow and testing graph connectivity. SIAM journal on computing, 4(4):507-518.</p>
<p>Peter Hase and Mohit Bansal. 2020. Evaluating explainable AI: Which algorithmic explanations help users predict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Han He and Jinho Choi. 2020. Establishing strong baselines for the new decade: Sequence tagging, syntactic and semantic parsing with bert. In The Thirty-Third International Flairs Conference.</p>
<p>Shalmali Joshi, Oluwasanmi Koyejo, Been Kim, and Joydeep Ghosh. 2018. xGEMs: Generating examplars to explain black-box models. arXiv preprint arXiv:1806.08867.</p>
<p>Aishwarya Kamath and Rajarshi Das. 2019. A survey on semantic parsing. In Automated Knowledge Base Construction (AKBC).</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785-794.</p>
<p>Guillaume Lample and François Charton. 2020. Deep learning for symbolic mathematics. In 8th International Conference on Learning Representations, ICLR 2020. OpenReview.net.</p>
<p>Tom Leighton and Satish Rao. 1999. Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms. Journal of the ACM (JACM), 46(6):787-832.</p>
<p>Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gardner. 2019. Reasoning over paragraph effects in situations. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 5862, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Hui Liu, Qingyu Yin, and William Yang Wang. 2019a. Towards explainable NLP: A generative explanation framework for text classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5570-5581.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.</p>
<p>RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Bill MacCartney and Christopher D Manning. 2014. Natural logic and natural language inference. In Computing meaning, pages 129-147. Springer.</p>
<p>Mark A Musen and Johan Van Der Lei. 1988. Of brittleness and bottlenecks: Challenges in the creation of pattern-recognition and expert-system models. In Machine Intelligence and Pattern Recognition, volume 7, pages 335-352. Elsevier.</p>
<p>Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. 2016. Neural programmer: Inducing latent programs with gradient descent. In 4th International Conference on Learning Representations, ICLR 2016, Conference Track Proceedings.</p>
<p>Allen Newell and Herbert Simon. 1956. The logic theory machine-a complex information processing system. IRE Transactions on information theory, 2(3):61-79.</p>
<p>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942.</p>
<p>Scott E. Reed and Nando de Freitas. 2016. Neural programmer-interpreters. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should I trust you?": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135-1144.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision modelagnostic explanations. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Kyle Richardson, Hai Hu, Lawrence S Moss, and Ashish Sabharwal. 2020. Probing natural language inference models through semantic fragments. In Thirty-Fourth AAAI Conference on Artificial Intelligence.</p>
<p>Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206-215.</p>
<p>Pouya Samangouei, Ardavan Saeedi, Liam Nakagawa, and Nathan Silberman. 2018. ExplainGAN: Model explanation via decision boundary crossing transformations. In Proceedings of the European Conference on Computer Vision (ECCV), pages 666-681.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations.</p>
<p>Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. 2019. Learning a SAT solver from single-bit supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. 2019. QuaRTz: An open-domain dataset of qualitative relationship questions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5941-5946, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2019. oLMpics-on what language model pre-training captures. arXiv preprint arXiv:1912.13283.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810-1822, Florence, Italy. Association for Computational Linguistics.</p>
<p>Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, and Tim Rocktäschel. 2019. NLProlog: Reasoning with weak unification for question answering in natural language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6151-6161, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, UAI'05, page 658-666. AUAI Press.</p>
<p>Hongming Zhang, Xinran Zhao, and Yangqiu Song. 2020. WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<h2>A Appendix</h2>
<h2>A. 1 Experimental Setup</h2>
<p>We build our model on top of the Hugging Face Transformers library (Wolf et al., 2019). ${ }^{5}$ All hyperparamters are chosen based on the best validation set performance (Full Accuracy) of the corresponding dataset. We use RoBERTa-large (Liu et al., 2019b) as the pre-trained Language Model and all our models are trained using a batch size of 8 and a maximum sequence length of 300 . We train the models for a maximum of 5 epochs using an initial learning rate of $10^{-5}$, with linear decay and a weight decay of 0.1 . The dropout probability is chosen to be 0.1 . The random seed used in all the experiments is 42 . Each epoch of PROVER takes 2.5 hours to run on one V100 Volta GPU. The total number of parameters of PROVER is similar to that of RoBERTa-large ( 355 M ). Batch size and learning rate are manually tuned in the range ${8,16}$ and $\left{10^{-5}, 2 * 10^{-5}\right}$ respectively. The ILP is modeled using PuLP. ${ }^{6}$ Proofs in the datasets are represented as bracketed strings, which are pre-processed into graph representations consisting of unique nodes and edges. The maximum number of facts and rules corresponding to a context is $25 .{ }^{7}$</p>
<h2>A. 2 Dataset Details</h2>
<p>Below we briefly describe the three sets of datasets we conduct experiments on. ${ }^{8}$ Each dataset has a</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>train, validation and test split, except for the zeroshot test-only one. Further details about these can be found in Clark et al. (2020).</p>
<p>DU0-DU5: The first set consists of five datasets, each containing 100k questions with theories in synthetic language and requiring reasoning paths up to depth $D(D=0,1,2,3,5)$. For example, $D=0$ means the true facts can be proved by simple lookup in the context. The samples are randomly split 70/10/20 into train/dev/test partitions such that there is no overlap of theories between the partitions.</p>
<p>Birds-Electricity: The second set consists of two test-only datasets used to evaluate robustness and out-of-distribution performance of the models. The contexts are about birds and an electric circuit, and consist of 5 k samples in total. The vocabulary of entities, attributes and predicates, apart from is () are all new at test time.</p>
<p>ParaRules: The final dataset, ParaRules consists of 40k questions against 2 k theories expressed in paraphrased natural language, obtained through crowdsourcing. While the previous datasets contain synthetic language, ParaRules tests the models' ability to reason over more human-like paraphrased language.</p>
<h2>A. 3 QA and Proof Results for Varying Depths</h2>
<p>Table 7 shows the DU5 validation set performance of PROVER trained on the training split of DU5. PROVER obtains a near perfect QA accuracy and a proof accuracy of $88 \%$. While the QA accuracy remains equally high at all depths, the proof accuracy drops with increasing depth. Full accuracy matches the proof accuracy, demonstrating consistency between the predicted answers and generated proofs.</p>
<h2>A. 4 Generalization to Higher Depths</h2>
<p>In Table 6, we provide detailed results of PROVER's generalization ability to higher depth questions. Specifically, we evaluate four models, trained on the training splits of DU0, DU1, DU2 and DU3 and tested on the validation and test splits of DU5. We have shown previously that PROVER does significantly better than RuleTakers (Clark et al., 2020) on QA generalization. The proofs, however, do not generalize that well. Note that depth 0 proofs are rather simple (consisting of a</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">QA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">NA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">EA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">PA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">FA</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
</tr>
<tr>
<td style="text-align: left;">DU0</td>
<td style="text-align: right;">68.3</td>
<td style="text-align: right;">68.7</td>
<td style="text-align: right;">45.3</td>
<td style="text-align: right;">46.0</td>
<td style="text-align: right;">49.3</td>
<td style="text-align: right;">49.5</td>
<td style="text-align: right;">43.8</td>
<td style="text-align: right;">44.4</td>
<td style="text-align: right;">42.3</td>
<td style="text-align: right;">42.8</td>
</tr>
<tr>
<td style="text-align: left;">DU1</td>
<td style="text-align: right;">73.2</td>
<td style="text-align: right;">73.7</td>
<td style="text-align: right;">66.4</td>
<td style="text-align: right;">66.3</td>
<td style="text-align: right;">64.5</td>
<td style="text-align: right;">64.3</td>
<td style="text-align: right;">63.9</td>
<td style="text-align: right;">63.8</td>
<td style="text-align: right;">61.8</td>
<td style="text-align: right;">61.9</td>
</tr>
<tr>
<td style="text-align: left;">DU2</td>
<td style="text-align: right;">89.3</td>
<td style="text-align: right;">89.6</td>
<td style="text-align: right;">76.6</td>
<td style="text-align: right;">76.4</td>
<td style="text-align: right;">73.1</td>
<td style="text-align: right;">73.1</td>
<td style="text-align: right;">72.6</td>
<td style="text-align: right;">72.6</td>
<td style="text-align: right;">72.3</td>
<td style="text-align: right;">72.3</td>
</tr>
<tr>
<td style="text-align: left;">DU3</td>
<td style="text-align: right;">98.3</td>
<td style="text-align: right;">98.6</td>
<td style="text-align: right;">85.5</td>
<td style="text-align: right;">85.0</td>
<td style="text-align: right;">79.9</td>
<td style="text-align: right;">79.5</td>
<td style="text-align: right;">79.4</td>
<td style="text-align: right;">79.1</td>
<td style="text-align: right;">79.4</td>
<td style="text-align: right;">79.1</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance of PROVER trained on the training splits of DU0, DU1, DU2 and DU3 and tested on the validation and test splits of DU5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">D</th>
<th style="text-align: right;">Cnt</th>
<th style="text-align: right;">QA</th>
<th style="text-align: right;">NA</th>
<th style="text-align: right;">EA</th>
<th style="text-align: right;">PA</th>
<th style="text-align: right;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: right;">3116</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">98.7</td>
<td style="text-align: right;">98.6</td>
<td style="text-align: right;">98.5</td>
<td style="text-align: right;">98.5</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: right;">2304</td>
<td style="text-align: right;">98.8</td>
<td style="text-align: right;">92.5</td>
<td style="text-align: right;">94.9</td>
<td style="text-align: right;">92.2</td>
<td style="text-align: right;">92.2</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: right;">1436</td>
<td style="text-align: right;">99.2</td>
<td style="text-align: right;">86.1</td>
<td style="text-align: right;">85.6</td>
<td style="text-align: right;">85.6</td>
<td style="text-align: right;">85.6</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: right;">1165</td>
<td style="text-align: right;">98.7</td>
<td style="text-align: right;">85.1</td>
<td style="text-align: right;">82.8</td>
<td style="text-align: right;">82.8</td>
<td style="text-align: right;">82.8</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: right;">1041</td>
<td style="text-align: right;">98.8</td>
<td style="text-align: right;">81.2</td>
<td style="text-align: right;">76.9</td>
<td style="text-align: right;">76.9</td>
<td style="text-align: right;">76.9</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: right;">990</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">78.3</td>
<td style="text-align: right;">67.4</td>
<td style="text-align: right;">67.4</td>
<td style="text-align: right;">67.4</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: right;">10068</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">90.0</td>
<td style="text-align: right;">88.6</td>
<td style="text-align: right;">88.0</td>
<td style="text-align: right;">88.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of PROVER trained on the training split of DU5 and tested on the validation split of DU5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">D</th>
<th style="text-align: right;">Cnt</th>
<th style="text-align: right;">QA</th>
<th style="text-align: right;">NA</th>
<th style="text-align: right;">EA</th>
<th style="text-align: right;">PA</th>
<th style="text-align: right;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: right;">1485</td>
<td style="text-align: right;">99.9</td>
<td style="text-align: right;">99.6</td>
<td style="text-align: right;">99.7</td>
<td style="text-align: right;">99.5</td>
<td style="text-align: right;">99.5</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: right;">1180</td>
<td style="text-align: right;">99.7</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">99.5</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">99.3</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: right;">727</td>
<td style="text-align: right;">99.4</td>
<td style="text-align: right;">91.5</td>
<td style="text-align: right;">91.5</td>
<td style="text-align: right;">91.5</td>
<td style="text-align: right;">91.3</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: right;">524</td>
<td style="text-align: right;">98.5</td>
<td style="text-align: right;">92.0</td>
<td style="text-align: right;">90.3</td>
<td style="text-align: right;">90.3</td>
<td style="text-align: right;">90.3</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">87.6</td>
<td style="text-align: right;">72.8</td>
<td style="text-align: right;">72.8</td>
<td style="text-align: right;">72.8</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: right;">4004</td>
<td style="text-align: right;">99.6</td>
<td style="text-align: right;">96.8</td>
<td style="text-align: right;">96.2</td>
<td style="text-align: right;">96.1</td>
<td style="text-align: right;">96.0</td>
</tr>
</tbody>
</table>
<p>Table 8: PROVER results on the ParaRules validation set after training on DU3+ParaRules training splits.
single fact) and a model trained on only such proofs, unsuprisingly, fails to generate proofs for higher depth questions. However, the proof results start improving as the model gets trained on more complex proofs and reaches an accuracy of $79 \%$, when trained on DU3 questions.</p>
<h2>A. 5 Evaluation on Complex Language</h2>
<p>In Table 8, we report the ParaRules validation set results of PROVER trained on the combination of DU3 and ParaRules training splits (following previous work (Clark et al., 2020)). ParaRules is created by first separating the fact groups (a fact group is the set of all facts in the theory concerning a particular person) and the rules from a theory and then asking crowdworkers to paraphrase these in their own words. For example, a fact group "Alan is blue. Alan is rough. Alan is young.", may be reworded into "Alan is on the young side, but rough.</p>
<p>He often feels rather blue.". Thus, unlike the previous datasets where the proof graphs are composed of facts and rules, ParaRules proofs are composed of fact groups and rules. ${ }^{9}$ PROVER obtains high QA and proof accuracy on complex humanparapharsed rule-bases, showing good generalization on such language. However, the proof accuracy again drops as the depth of the questions increases.</p>
<h2>A. 6 Ablation Models and Simpler Baselines</h2>
<p>We provide brief descriptions of our ablation models. These are (1) QA+Node: We model PROVER consisting of only the QA and Node modules. Since there is no edge module, this model does not require any constrained training or inference; (2) No NAF: We train a model using random NAF embeddings with no learning. This helps us understand the effectiveness of our NAF learning; (3) Unconstrained Train + No ILP: Through this model, we study the effectiveness of our global constraints. Specifically, no edges are masked for training and during inference, the edge labels are predicted based on the model's probability scores only; (4) Unconstrained Train + ILP: Here the constraints are employed only during inference. Note that the reverse configuration, constrained training without ILP inference, is not included as the edge logits for the masked out labels would be random (since they are not learned). (5) No Connectivity: Finally, we train a model where we only remove the connectivity constraint during ILP optimization, keeping everything else same.</p>
<p>We also experiment with simpler baselines for edge prediction like training a Random Forest with lexical features (BLEU scores, length difference,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>word overlap, etc.) and this obtains a much lower edge accuracy of $47 \%$. This fails primarily because (1) proof graphs can contain NAF which account for $9 \%$ of the data and edges from it cannot be learned without learning a latent representation; (2) overlap features are mostly symmetric and hence are not enough for learning directionality; (3) there is lack of overall context information.</p>
<h1>A. 7 Critical Sentence Identification</h1>
<p>Clark et al. (2020) provide an initial solution towards generating explanations for the predicted answers by using a post-hoc method - they remove each fact or rule from the theory and check if the predicted answer changes with the new theory. They define all such rules and facts which flip the answer as critical sentences. If an example has multiple gold proofs, a critical sentence is one which is present in all the proofs. We argue that this leave-one-out analysis is not ideal for multiple reasons - (1) This does not work if the theory has negations, (2) This only predicts the presence or absence of rules and facts, and does not look at the entire chain of reasoning, which our model achieves through the edge module. In our final ex-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RuleTakers</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">$\mathbf{9 8 . 7}$</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">92.4</td>
</tr>
<tr>
<td style="text-align: left;">PROVER</td>
<td style="text-align: center;">$\mathbf{7 8 . 1}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of critical sentence identification on the No Negation subset of DU5 test set.
periment, we still apply the leave-one-out-strategy on the no-negation subset of the DU5 test set for a direct comparison with RuleTakers. As shown in Table 9, our model identifies the exact critical sentences in an example in $78 \%$ of the cases, a $4 \%$ improvement over RuleTakers.</p>
<h2>A. 8 Proofs Generated by PROVER</h2>
<p>In Figure 5, we show two rule-bases, one about electric circuits and another about birds from the Birds-Electricity dataset. PROVER not only answers the questions correctly but also generates the proofs accurately. These proofs are complex because of the presence of NAF and also the long chains of reasoning needed in the inference process. Figure 6 shows three more accurate proofs generated by PROVER for three questions from the DU5 datset.</p>
<h2>Facts :</h2>
<p>$\mathbf{F}<em 2="2">{1}$ : The circuit has the battery. $\mathbf{F}</em>$ : The circuit has the bell.}$ : The switch is on. $\mathbf{F}_{3</p>
<h2>Rules :</h2>
<p>$\mathbf{R}<em 2="2">{1}$ : If the circuit has the battery then the circuit is powered.
$\mathbf{R}</em>$ : If the circuit does not have the battery then the circuit is dead.
$\mathbf{R}<em 4="4">{3}$ : If the circuit is dead then the bell is not ringing.
$\mathbf{R}</em>$ : If the circuit is dead then the radio is not playing.
$\mathbf{R}<em 1="1">{5}$ : If the circuit is dead then the light bulb is not glowing.
$\mathbf{Q}</em>$ : The current runs through the circuit. [ Answer : T]
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h2>Facts :</h2>
<p>$\mathbf{F}<em 2="2">{1}$ : Arthur is a bird. $\mathbf{F}</em>$ : Arthur is not wounded.
$\mathbf{F}<em 4="4">{3}$ : Bill is an ostrich. $\mathbf{F}</em>$ : Colin is a bird.
$\mathbf{F}<em 6="6">{5}$ : Colin is wounded. $\mathbf{F}</em>$ : Dave is not an ostrich.
$\mathbf{F}<em 3="3">{7}$ : Dave is wounded.
<img alt="img-4.jpeg" src="img-4.jpeg" />
$\mathbf{Q}</em>$ : Colin is not abnormal. [ Answer : F]</p>
<h2>Rules :</h2>
<p>$\mathbf{R}<em 7="7">{6}$ : If the circuit has the switch and the switch is on then the circuit is complete.
$\mathbf{R}</em>$ : If the circuit does not have the switch then the circuit is complete.
$\mathbf{R}<em 9="9">{8}$ : If the circuit is powered and the circuit is complete then the current runs through the circuit.
$\mathbf{R}</em>$ : If the current runs through the circuit and the circuit has the light bulb then the light bulb is glowing.
$\mathbf{R}<em 11="11">{10}$ : If the current runs through the circuit and the circuit has the bell then the bell is ringing.
$\mathbf{R}</em>$ : If the current runs through the circuit and the circuit has the radio then the radio is playing.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h2>Rules :</h2>
<p>$\mathbf{R}<em 2="2">{1}$ : If someone is an ostrich then they are a bird.
$\mathbf{R}</em>$ : If someone is an ostrich then they are abnormal.
$\mathbf{R}<em 4="4">{3}$ : If someone is an ostrich then they cannot fly.
$\mathbf{R}</em>$ : If someone is a bird and wounded then they are abnormal.
$\mathbf{R}<em 6="6">{5}$ : If someone is wounded then they cannot fly.
$\mathbf{R}</em>$ : If someone is a bird and not abnormal then they can fly.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5: Examples of proofs generated by PROVER for four questions on two rule-bases about electric circuits and birds from the Birds-Electricity dataset. PROVER not only answers the questions correctly but also accurately predicts the long reasoning chains with multiple branches.</p>
<h2>Facts :</h2>
<p>$\mathbf{F}<em 2="2">{1}$ : The bear visits the tiger. $\mathbf{F}</em>$ : The cat is kind.
$\mathbf{F}<em 4="4">{3}$ : The mouse is green. $\mathbf{F}</em>$ : The mouse is kind.
$\mathbf{F}<em 6="6">{5}$ : The mouse sees the tiger. $\mathbf{F}</em>$ : The tiger is rough.
$\mathbf{F}_{7}$ : The tiger visits the cat.</p>
<h2>Rules :</h2>
<p>$\mathbf{R}<em 2="2">{1}$ : If something visits the bear then it sees the bear.
$\mathbf{R}</em>$ : If something sees the bear then the bear likes the cat.
$\mathbf{R}<em 4="4">{3}$ : If something visits the cat then the cat visits the bear.
$\mathbf{R}</em>$ : If something sees the bear and the bear likes the cat then it is cold.
$\mathbf{R}<em 6="6">{5}$ : Cold things are rough.
$\mathbf{R}</em>$ : If something is green and it likes the tiger then the tiger visits the mouse.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Examples of proofs generated by PROVER for three questions on a rule-base from the DU5 dataset. The proof corresponding to the last question is a failed case.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ The original ParaRules dataset released by Clark et al. (2020) has proofs for the unparaphrased theories (consisting of facts and rules). Using the mapping from a fact to the corresponding fact group, we replace the fact nodes in the proof graph with the corresponding fact group nodes. Note that this is done to report proof accuracy for this dataset as well.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Proofs are directed graphs. We check connectivity in the equivalent undirected graphs.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>